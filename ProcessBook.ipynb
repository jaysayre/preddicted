{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "# Final Project\n",
      "\n",
      "*Due Thursday, December 12, 11:59pm*\n",
      "\n",
      "<img src=\"http://media.merchantcircle.com/37277172/seo-graph-up_full.jpeg\">\n",
      "\n",
      "<br>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "\n",
      "import json\n",
      "\n",
      "import numpy as np\n",
      "import networkx as nx\n",
      "import requests\n",
      "from pattern import web\n",
      "import matplotlib.pyplot as plt\n",
      "\n",
      "# set some nicer defaults for matplotlib\n",
      "from matplotlib import rcParams\n",
      "\n",
      "#these colors come from colorbrewer2.org. Each is an RGB triplet\n",
      "dark2_colors = [(0.10588235294117647, 0.6196078431372549, 0.4666666666666667),\n",
      "                (0.8509803921568627, 0.37254901960784315, 0.00784313725490196),\n",
      "                (0.4588235294117647, 0.4392156862745098, 0.7019607843137254),\n",
      "                (0.9058823529411765, 0.1607843137254902, 0.5411764705882353),\n",
      "                (0.4, 0.6509803921568628, 0.11764705882352941),\n",
      "                (0.9019607843137255, 0.6705882352941176, 0.00784313725490196),\n",
      "                (0.6509803921568628, 0.4627450980392157, 0.11372549019607843),\n",
      "                (0.4, 0.4, 0.4)]\n",
      "\n",
      "rcParams['figure.figsize'] = (10, 6)\n",
      "rcParams['figure.dpi'] = 150\n",
      "rcParams['axes.color_cycle'] = dark2_colors\n",
      "rcParams['lines.linewidth'] = 2\n",
      "rcParams['axes.grid'] = False\n",
      "rcParams['axes.facecolor'] = 'white'\n",
      "rcParams['font.size'] = 14\n",
      "rcParams['patch.edgecolor'] = 'none'\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "### Project Outline\n",
      "\n",
      "This project is the final project for the course CS109 - data science at Harvard University ( cs109.org ). \n",
      "\n",
      "One of the visualizations in one of our homeworks got posted to reddit and <a href='http://www.reddit.com/r/dataisbeautiful/comments/1q7b3s/voting_relationships_between_senators_in_the/'>achieved a really high score</a>. Due to this attention, almost all big media web sites like yahoo covered the visualization that shows the bipartisanship in the US senate.\n",
      "\n",
      "<img src='https://d1b10bmlvqabco.cloudfront.net/attach/hhuzz8a158o7j0/gxt34glxwec2jd/ho053e5eanau/Screenshot_20131114_10.14.33.png'>\n",
      "\n",
      "We found it to be really interesting that a single post can generate such a huge media attention and wondered whether you can predict such success. Since reddit is split up into so called subreddits that have own communities we wanted to take a subset of those and investigate what makes a post successful.\n",
      "\n",
      "There are two types of posts to reddit - so called self posts that are text only and link posts that have no text but a link behind them. In this project we will focus only on text posts that have no links."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Finding subreddits\n",
      "\n",
      "There are more then <a href ='http://metareddit.com/'>278,000 subreddits</a> on reddit. Each subreddits has its own rules and community which means that they need different approaches to predict how successful a post is. Depending on the size of the community, a different score or number of comments will count as success and a subreddit dedicated to jokes will have different indicators of success than a subreddit dedicated to science.\n",
      "\n",
      "We wanted to compile a list of roughly 10 text based subreddits that have a large community and cover diverse topics. We found most of the subreddits we will look at <a href='http://www.reddit.com/r/AskReddit/comments/1hhvxy/what_are_your_favorite_textbased_subreddits/'> here </a>.\n",
      "\n",
      "This is our list:\n",
      "    \n",
      "- <b>explainlikeimfive</b>, a subreddit where people ask questions and get answers that could be adressed at 5 year olds\n",
      "- <b>AskReddit</b>, a subreddit where people ask questions about anything. This is probably the largest and most actice subreddit on reddit\n",
      "- <b>TalesFromTechsupport</b>, a subreddit where people working in tech support tell stories from their job\n",
      "- <b>talesFromRetail</b>, which originated from talesfromtechsupport. People there work in retail and tell stories from their job.\n",
      "- <b>pettyrevenge</b>, where people tell stories about revenges they got\n",
      "- <b>askhistorians</b>, where you can ask historians about anything related to history\n",
      "- <b>askscience</b>, where you can ask scientists about anything related to science\n",
      "- <b>tifu</b>, which is short for \"today i f\\*\\*\\*ed up\" and where people tell stories how they did\n",
      "- <b>nosleep</b>, where people tell scary stories\n",
      "- <b>jokes</b>, a subreddit dedicated to jokes\n",
      "- <b>atheism</b>, where people who are atheists discuss religious topics \n",
      "- <b>politics</b>, a subreddit about political news and discussions"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "## Getting data\n",
      "\n",
      "Reddit offers an API to access their site which can be found <a href='http://www.reddit.com/dev/api'>here</a>. We used it to download all possible data sets for our list of subreddits. Reddit allows to access the top 1,000 posts for each of the following lists: Top (all / week / day), new and hot. \n",
      "Downloading the posts gave us a list of 44,000 submissions."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Adding to our data\n",
      "\n",
      "We downloaded all the posts we want to look at but there are still important information missing.\n",
      "\n",
      "***Part 1 - merging files***\n",
      "\n",
      "Since we download data from different subreddits and only get 1,000 entries at a time we create a ton of .csv files. The first step is to merge them by opening one after the other and saving them into a big table.\n",
      "\n",
      "***Part 2 - downloading extra information***\n",
      "\n",
      "In the original download you get a lot of fields at once but we still missed some information. The API delivers only the name of the original poster. We also wanted to look at the comment and link karma of the user, not only the score of the post. That's why we needed to expand our table by this information\n",
      "\n",
      "***Part 3 - downloading comments***\n",
      "\n",
      "For each one of our 44,000 submissions we also want to predict and look at the scores of comments. For this we need to download all comments. Using the reddit API we can get the top 200 comments for each post. We did this and merged all comments of one subreddit into one file."
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "##Cleaning the data\n",
      "\n",
      "The task that everyone hates is cleaning the data. Even though the reddit API works well, you need to work on the raw data. \n",
      "\n",
      "First of all we don't want to look at moderator posts because they stand out and are successful by nature. There is no reason to let those posts influence our function. \n",
      "\n",
      "Additionally the API sometimes will return text in number fields, null values, wrong post ID's or have symbols that can't be represented in UTF-8. We need to filter all of them out"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "---\n",
      "*css tweaks in this cell*\n",
      "<style>\n",
      "div.text_cell_render {\n",
      "    line-height: 150%;\n",
      "    font-size: 110%;\n",
      "    width: 800px;\n",
      "    margin-left:50px;\n",
      "    margin-right:auto;\n",
      "    }\n",
      "</style>"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}