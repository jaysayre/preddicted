{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "from collections import Counter"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\" #Fill in your own\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "bigcsv = csvfiles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 63
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(bigcsv, encoding='utf-8') # Top all is our training data set\n",
      "df['up/down'] = df['upvotes'].astype(float)/df['downvotes'].astype(float) # Reddit fuzzes this so... "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topcomments=float(max(df['comments']))\n",
      "topsscore=float(max(df['score']))\n",
      "leastcontro = max(df['up/down'])\n",
      "# This needs to be improved. Sticking with it simply for testing purposes\n",
      "df['mymetric'] = (((df['comments'].astype(float)/topcomments)*0.10)+((df['score'].astype(float)/topsscore)*0.85)+((df['up/down']/leastcontro)*0.05))**(0.30)\n",
      "df['nrmscore'] = (df['score'].astype(float)/topsscore)**(0.30)\n",
      "bigdf = df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df[df['subreddit'] == 'AskReddit']\n",
      "df2 = df[df['type'] == 'top_week']\n",
      "df = df[df['type'] == 'top_all']\n",
      "df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<pre>\n",
        "&lt;class 'pandas.core.frame.DataFrame'&gt;\n",
        "Int64Index: 993 entries, 0 to 26940\n",
        "Data columns (total 15 columns):\n",
        "author        992  non-null values\n",
        "comments      993  non-null values\n",
        "downvotes     993  non-null values\n",
        "id            993  non-null values\n",
        "karma         993  non-null values\n",
        "link_karma    993  non-null values\n",
        "score         993  non-null values\n",
        "selftext      579  non-null values\n",
        "subreddit     993  non-null values\n",
        "title         993  non-null values\n",
        "type          993  non-null values\n",
        "upvotes       993  non-null values\n",
        "up/down       993  non-null values\n",
        "mymetric      993  non-null values\n",
        "nrmscore      993  non-null values\n",
        "dtypes: float64(9), object(6)\n",
        "</pre>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 66,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 993 entries, 0 to 26940\n",
        "Data columns (total 15 columns):\n",
        "author        992  non-null values\n",
        "comments      993  non-null values\n",
        "downvotes     993  non-null values\n",
        "id            993  non-null values\n",
        "karma         993  non-null values\n",
        "link_karma    993  non-null values\n",
        "score         993  non-null values\n",
        "selftext      579  non-null values\n",
        "subreddit     993  non-null values\n",
        "title         993  non-null values\n",
        "type          993  non-null values\n",
        "upvotes       993  non-null values\n",
        "up/down       993  non-null values\n",
        "mymetric      993  non-null values\n",
        "nrmscore      993  non-null values\n",
        "dtypes: float64(9), object(6)"
       ]
      }
     ],
     "prompt_number": 66
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So a few things about the way Reddit generates its metrics. First, I highly encourage that you read [this article on how Reddit ranks posts](http://amix.dk/blog/post/19588). Second, Reddit \"fuzzes\" the upvotes and downvotes so spambots can't manipulate the forum easily, so while the score is accurate, the number of upvotes and downvotes is not. For reference, $score = upvotes - downvotes$. Therefore, it must be that reddit adds/subtracts some unknown constant $k$ to the number of upvotes and downvotes.\n",
      "\n",
      "Currently, I've simply computed up/down as a measure of whether or not a post is controversial, but mathematically we may want to talk about methods to try to normalize this figure (if such a method exists).\n",
      "\n",
      "Third, I've found the actual paper that the Stanford researchers produced, and [it's worth a read over](http://i.stanford.edu/~julian/pdfs/icwsm13.pdf)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It's important in cross validation that the sets are disjoint, so we are removing duplicates\n",
      "dfids = list(df['id'])\n",
      "df2ids = list(df2['id'])\n",
      "\n",
      "dupids = []\n",
      "for redditid in dfids:\n",
      "    if redditid in df2ids:\n",
      "        dupids.append(redditid)\n",
      "\n",
      "#This part is slightly overengineered, but the motivation behind it is that we didn't want to simply strip out the \n",
      "#posts from other data set at will. Instead, we are splitting the duplicates in half and assigning them to one of the data sets\n",
      "#to avoid some sort of possible bias.\n",
      "if len(dupids)%2 != 0:\n",
      "    a = len(dupids)/2\n",
      "    a = a+1\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "else: \n",
      "    a = len(dupids)/2\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "    \n",
      "if np.random.randint(2) == 0:\n",
      "    df=df[df['id'].apply(lambda x: x in dup1) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup2) == False]\n",
      "else: \n",
      "    df=df[df['id'].apply(lambda x: x in dup2) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup1) == False]\n",
      "\n",
      "#df['mymetric'] = df['score'] + (df['comments'].astype(float)/2)*0.30 - ((df['up/down']/20)*(0.30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>author</th>\n",
        "      <th>comments</th>\n",
        "      <th>downvotes</th>\n",
        "      <th>id</th>\n",
        "      <th>karma</th>\n",
        "      <th>link_karma</th>\n",
        "      <th>score</th>\n",
        "      <th>selftext</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>title</th>\n",
        "      <th>type</th>\n",
        "      <th>upvotes</th>\n",
        "      <th>up/down</th>\n",
        "      <th>mymetric</th>\n",
        "      <th>nrmscore</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0  </th>\n",
        "      <td>             lieface</td>\n",
        "      <td>  8435</td>\n",
        "      <td>  5998</td>\n",
        "      <td> zzz5p</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 1989</td>\n",
        "      <td> For me its that Im happyI laugh and joke and s...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>               Whats a huge lie you tell everyday </td>\n",
        "      <td> top_all</td>\n",
        "      <td>  7987</td>\n",
        "      <td> 1.331611</td>\n",
        "      <td> 0.565861</td>\n",
        "      <td> 0.570535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10 </th>\n",
        "      <td>           Yossarian</td>\n",
        "      <td>  3615</td>\n",
        "      <td>  6196</td>\n",
        "      <td> zyyjj</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 2298</td>\n",
        "      <td> EDIT Thank you Now that Im up here Id like to ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> What is the most genuinely useful subreddit yo...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  8494</td>\n",
        "      <td> 1.370884</td>\n",
        "      <td> 0.576411</td>\n",
        "      <td> 0.595795</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>53 </th>\n",
        "      <td>          kiltedfrog</td>\n",
        "      <td>  9066</td>\n",
        "      <td> 12638</td>\n",
        "      <td> zrotp</td>\n",
        "      <td>  2363</td>\n",
        "      <td>    58</td>\n",
        "      <td> 2115</td>\n",
        "      <td> Two drums and a cymbal fall off a cliffDuh dum...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>                Whats the best clean joke you know</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 14753</td>\n",
        "      <td> 1.167352</td>\n",
        "      <td> 0.576621</td>\n",
        "      <td> 0.581146</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>65 </th>\n",
        "      <td>    BitBulletBarrage</td>\n",
        "      <td>  2581</td>\n",
        "      <td> 22187</td>\n",
        "      <td> zpf3j</td>\n",
        "      <td>    55</td>\n",
        "      <td>     1</td>\n",
        "      <td> 1969</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Reddit why is there not a button on the side o...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 24156</td>\n",
        "      <td> 1.088746</td>\n",
        "      <td> 0.548896</td>\n",
        "      <td> 0.568808</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>91 </th>\n",
        "      <td> browneisthenewblack</td>\n",
        "      <td>  2740</td>\n",
        "      <td> 14983</td>\n",
        "      <td> zkln1</td>\n",
        "      <td>    63</td>\n",
        "      <td>    95</td>\n",
        "      <td> 2397</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Why is Ziploc yet to partner with cereal compa...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 17380</td>\n",
        "      <td> 1.159981</td>\n",
        "      <td> 0.581301</td>\n",
        "      <td> 0.603382</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>121</th>\n",
        "      <td>             mizuhri</td>\n",
        "      <td> 16759</td>\n",
        "      <td> 10950</td>\n",
        "      <td> zdzdj</td>\n",
        "      <td> 16707</td>\n",
        "      <td>  4128</td>\n",
        "      <td> 2021</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>    The year is  what doesnt exist anymore and why</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 12971</td>\n",
        "      <td> 1.184566</td>\n",
        "      <td> 0.588356</td>\n",
        "      <td> 0.573273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>125</th>\n",
        "      <td>   steelpantherrocks</td>\n",
        "      <td>  4197</td>\n",
        "      <td>  8483</td>\n",
        "      <td> zcp8d</td>\n",
        "      <td>   627</td>\n",
        "      <td> 13919</td>\n",
        "      <td> 2374</td>\n",
        "      <td>  hours after original post with a little more ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Not only did Geico not save me  or more in  mi...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 10857</td>\n",
        "      <td> 1.279854</td>\n",
        "      <td> 0.583164</td>\n",
        "      <td> 0.601639</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>150</th>\n",
        "      <td>         alexiagrace</td>\n",
        "      <td>  2582</td>\n",
        "      <td>  9230</td>\n",
        "      <td> z80hb</td>\n",
        "      <td>  2336</td>\n",
        "      <td> 13129</td>\n",
        "      <td> 2641</td>\n",
        "      <td> Edit Whoa I went to sleep and when I woke up t...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Someone recently told me The way you talk to y...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 11871</td>\n",
        "      <td> 1.286132</td>\n",
        "      <td> 0.597478</td>\n",
        "      <td> 0.621187</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>174</th>\n",
        "      <td>              ananci</td>\n",
        "      <td>  2171</td>\n",
        "      <td> 14329</td>\n",
        "      <td> z2rhj</td>\n",
        "      <td>  3134</td>\n",
        "      <td>   191</td>\n",
        "      <td> 2160</td>\n",
        "      <td> Just a bit under a year ago the a good portion...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Just a few months ago Reddit was boycotting Go...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 16489</td>\n",
        "      <td> 1.150743</td>\n",
        "      <td> 0.562659</td>\n",
        "      <td> 0.584828</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>181</th>\n",
        "      <td>            damarust</td>\n",
        "      <td>   541</td>\n",
        "      <td> 12285</td>\n",
        "      <td> z177g</td>\n",
        "      <td>  1968</td>\n",
        "      <td>  3566</td>\n",
        "      <td> 3248</td>\n",
        "      <td> I just thought about it You could choose the d...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Would Reddit want a flashback feature added to...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 15533</td>\n",
        "      <td> 1.264387</td>\n",
        "      <td> 0.630580</td>\n",
        "      <td> 0.660962</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 57,
       "text": [
        "                  author  comments  downvotes     id  karma  link_karma  \\\n",
        "0                lieface      8435       5998  zzz5p      0           0   \n",
        "10             Yossarian      3615       6196  zyyjj      0           0   \n",
        "53            kiltedfrog      9066      12638  zrotp   2363          58   \n",
        "65      BitBulletBarrage      2581      22187  zpf3j     55           1   \n",
        "91   browneisthenewblack      2740      14983  zkln1     63          95   \n",
        "121              mizuhri     16759      10950  zdzdj  16707        4128   \n",
        "125    steelpantherrocks      4197       8483  zcp8d    627       13919   \n",
        "150          alexiagrace      2582       9230  z80hb   2336       13129   \n",
        "174               ananci      2171      14329  z2rhj   3134         191   \n",
        "181             damarust       541      12285  z177g   1968        3566   \n",
        "\n",
        "     score                                           selftext  subreddit  \\\n",
        "0     1989  For me its that Im happyI laugh and joke and s...  AskReddit   \n",
        "10    2298  EDIT Thank you Now that Im up here Id like to ...  AskReddit   \n",
        "53    2115  Two drums and a cymbal fall off a cliffDuh dum...  AskReddit   \n",
        "65    1969                                                NaN  AskReddit   \n",
        "91    2397                                                NaN  AskReddit   \n",
        "121   2021                                                NaN  AskReddit   \n",
        "125   2374   hours after original post with a little more ...  AskReddit   \n",
        "150   2641  Edit Whoa I went to sleep and when I woke up t...  AskReddit   \n",
        "174   2160  Just a bit under a year ago the a good portion...  AskReddit   \n",
        "181   3248  I just thought about it You could choose the d...  AskReddit   \n",
        "\n",
        "                                                 title     type  upvotes  \\\n",
        "0                  Whats a huge lie you tell everyday   top_all     7987   \n",
        "10   What is the most genuinely useful subreddit yo...  top_all     8494   \n",
        "53                  Whats the best clean joke you know  top_all    14753   \n",
        "65   Reddit why is there not a button on the side o...  top_all    24156   \n",
        "91   Why is Ziploc yet to partner with cereal compa...  top_all    17380   \n",
        "121     The year is  what doesnt exist anymore and why  top_all    12971   \n",
        "125  Not only did Geico not save me  or more in  mi...  top_all    10857   \n",
        "150  Someone recently told me The way you talk to y...  top_all    11871   \n",
        "174  Just a few months ago Reddit was boycotting Go...  top_all    16489   \n",
        "181  Would Reddit want a flashback feature added to...  top_all    15533   \n",
        "\n",
        "      up/down  mymetric  nrmscore  \n",
        "0    1.331611  0.565861  0.570535  \n",
        "10   1.370884  0.576411  0.595795  \n",
        "53   1.167352  0.576621  0.581146  \n",
        "65   1.088746  0.548896  0.568808  \n",
        "91   1.159981  0.581301  0.603382  \n",
        "121  1.184566  0.588356  0.573273  \n",
        "125  1.279854  0.583164  0.601639  \n",
        "150  1.286132  0.597478  0.621187  \n",
        "174  1.150743  0.562659  0.584828  \n",
        "181  1.264387  0.630580  0.660962  "
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "title = list(df['title']) + list(df2['title'])\n",
      "vectorizer.fit(title)\n",
      "\n",
      "def category(x, df, num=20):\n",
      "    size = len(df)\n",
      "    blocksize = size/num\n",
      "    for i in range(num):\n",
      "        blockmax = max(sorted(df['score'])[blocksize*i:blocksize*(i+1)])        \n",
      "        if x < blockmax:\n",
      "            return i+1\n",
      "    return num\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "#X = vectorizer.transform(title)\n",
      "#Y = np.array(scores)\n",
      "x_train = vectorizer.transform(df['title'])\n",
      "x_test = vectorizer.transform(df2['title'])\n",
      "score = [20 for i in df['score']]\n",
      "score2 = [category(i, df2) for i in df2['score']]\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training accuracy is 1.0\n"
       ]
      }
     ],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test accuracy is 0.0587044534413\n"
       ]
      }
     ],
     "prompt_number": 62
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#words = vectorizer2.get_feature_names()\n",
      "#words\n",
      "#diag_words = np.eye(len(words))\n",
      "#probword = pd.DataFrame(clf.predict_proba(diag_words))\n",
      "#probword.rename(columns={0: 'rotten', 1: 'fresh'}, inplace=True)\n",
      "#probword['words'] = words\n",
      "#print \"Top 10 Rotten words are\"\n",
      "#probword.sort([3], ascending=False)\n",
      "#probword['words'].head(20)\n",
      "\n",
      "#print \"\\n Top 10 Fresh words are\"\n",
      "#print probword.sort(['fresh'], ascending=False)[0:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 455
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So while I have quite figured out how to get this to work properly, it's pretty obvious from doing a quick Naive Bayes fit that this method isn't going that work, regardless of what parameters we pick -- I don't really know though so you (whoever you are?) should fucking check me on this. It could work-- idk. Alternately, could just be this subreddit - I don't fucking know.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "#apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from myalchemy import MyAlchemy\n",
      "p= MyAlchemy(apikey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 22
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dftitles = list(df['title'])\n",
      "df2titles = list(df2['title'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dftitles[5]\n",
      "print p.run_method(dftitles[5], 'concepts')\n",
      "print p.run_method(dftitles[5], 'keywords')\n",
      "print p.run_method(dftitles[5], 'category')\n",
      "#print p.run_method(dftitles[5], 'sentiment')\n",
      "print p.run_method(dftitles[5], 'entities')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Can creatures that are small see even smaller creatures ie bacteria because they are closer in size\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'0.934117', u'smaller creatures'), (u'0.701291', u'bacteria'), (u'0.530302', u'size')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'unknown', u'english', u'0.400001', u'OK')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "categories = []\n",
      "#for i in range(len(dftitles)):\n",
      "for i in range(4, 30):\n",
      "    categories.append(p.run_method(dftitles[i], 'category')[0])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 39
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "categories"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 40,
       "text": [
        "[u'unknown',\n",
        " u'unknown',\n",
        " u'unknown',\n",
        " u'unknown',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'unknown',\n",
        " u'health',\n",
        " u'unknown',\n",
        " u'unknown',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'business',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'science_technology',\n",
        " u'health',\n",
        " u'arts_entertainment',\n",
        " u'unknown',\n",
        " u'business',\n",
        " u'science_technology',\n",
        " u'unknown',\n",
        " u'unknown',\n",
        " u'unknown']"
       ]
      }
     ],
     "prompt_number": 40
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see here that perhaps just using the titles might not give us enough information to accurately ascertain what the posts are about."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "subreddit = 'explainlikeimfive'\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")\n",
      "\n",
      "topids = list(df['id'])\n",
      "\n",
      "endat = 150\n",
      "commentlen = []\n",
      "#for topid in topids:\n",
      "for i in range(endat):\n",
      "    try:\n",
      "        commentlen.append(len(list(get_comments(subreddit, topids[i], 'top', user_agent)['comment'])[0]))\n",
      "    except:\n",
      "        print topids[i]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15u5d0\n",
        "s8l4e"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids.index('15u5d0')\n",
      "topids.index('s8l4e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "122"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(commentlen)\n",
      "\n",
      "topscores = list(df['score'])[:50] + list(df['score'])[51:122] + list(df['score'])[123:endat]\n",
      "topmetrics = list(df['mymetric'])[:50] + list(df['mymetric'])[51:122] + list(df['mymetric'])[123:endat]\n",
      "topups = list(df['upvotes'])[:50] + list(df['upvotes'])[51:122] + list(df['upvotes'])[123:endat]\n",
      "topcont = list(df['up/down'])[:50] + list(df['up/down'])[51:122] + list(df['up/down'])[123:endat]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "148\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There appears to be some minor correlation between the length of the top comment and popularity of the post, but it appears like this effect is on the decline over time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.stats import pearsonr\n",
      "\n",
      "print pearsonr(commentlen, topscores)\n",
      "print pearsonr(commentlen, topmetrics)\n",
      "print pearsonr(commentlen, topups)\n",
      "print pearsonr(commentlen, topcont)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.44383859210747884, 1.6035273359197743e-08)\n",
        "(0.44319470358692481, 1.6912587281428895e-08)\n",
        "(0.25094344719532163, 0.0020954876387472149)\n",
        "(0.041284452559721747, 0.61834237353945753)\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#following code will create a counter dictionary will all keywords in titles for a single subreddit\n",
      "\n",
      "def get_keywords(subreddit):\n",
      "    returndict = Counter()\n",
      "    for a in subreddit.title:\n",
      "        keywords = p.run_method(a, 'keywords')\n",
      "        if keywords:\n",
      "            for b in keywords:\n",
      "                returndict[b[1]] = returndict[b[1]] + 1\n",
      "    return returndict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_keywords = get_keywords(pd.read_csv(csvfiles[13], encoding='utf-8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-38-d70f96484ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-35-20ccc0388b59>\u001b[0m in \u001b[0;36mget_keywords\u001b[1;34m(subreddit)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreturndict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keywords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/j/Dropbox/College/Data Science/Project/intelligent-dolphins/myalchemy.py\u001b[0m in \u001b[0;36mrun_method\u001b[1;34m(self, comment, whichtoget, otherparams)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0malchemybase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__look_at\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhichtoget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlencode\u001b[1;34m(query, doseq)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}