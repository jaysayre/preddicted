{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "\n",
      "\n",
      "# Not our code here - credit to the CS109 psets.\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\" #Fill in your own\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "bigcsv = csvfiles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(bigcsv, encoding='utf-8') # Top all is our training data set\n",
      "print len(df)\n",
      "df['up/down'] = df['upvotes'].astype(float)/df['downvotes'].astype(float) # Reddit fuzzes this so... "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "44261\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topcomments=float(max(df['comments']))\n",
      "topsscore=float(max(df['score']))\n",
      "leastcontro = max(df['up/down'])\n",
      "# This needs to be improved. Sticking with it simply for testing purposes\n",
      "df['mymetric'] = (((df['comments'].astype(float)/topcomments)*0.10)+((df['score'].astype(float)/topsscore)*0.85)+((df['up/down']/leastcontro)*0.05))**(0.30)\n",
      "df['nrmscore'] = (df['score'].astype(float)/topsscore)**(0.30)\n",
      "bigdf = df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df[df['subreddit'] == 'AskReddit']\n",
      "df2 = df[df['type'] == 'top_week']\n",
      "df = df[df['type'] == 'top_all']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So a few things about the way Reddit generates its metrics. First, I highly encourage that you read [this article on how Reddit ranks posts](http://amix.dk/blog/post/19588). Second, Reddit \"fuzzes\" the upvotes and downvotes so spambots can't manipulate the forum easily, so while the score is accurate, the number of upvotes and downvotes is not. For reference, $score = upvotes - downvotes$. Therefore, it must be that reddit adds/subtracts some unknown constant $k$ to the number of upvotes and downvotes.\n",
      "\n",
      "Currently, I've simply computed up/down as a measure of whether or not a post is controversial, but mathematically we may want to talk about methods to try to normalize this figure (if such a method exists).\n",
      "\n",
      "Third, I've found the actual paper that the Stanford researchers produced, and [it's worth a read over](http://i.stanford.edu/~julian/pdfs/icwsm13.pdf)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It's important in cross validation that the sets are disjoint, so we are removing duplicates\n",
      "dfids = list(df['id'])\n",
      "df2ids = list(df2['id'])\n",
      "\n",
      "dupids = []\n",
      "for redditid in dfids:\n",
      "    if redditid in df2ids:\n",
      "        dupids.append(redditid)\n",
      "\n",
      "#This part is slightly overengineered, but the motivation behind it is that we didn't want to simply strip out the \n",
      "#posts from other data set at will. Instead, we are splitting the duplicates in half and assigning them to one of the data sets\n",
      "#to avoid some sort of possible bias.\n",
      "if len(dupids)%2 != 0:\n",
      "    a = len(dupids)/2\n",
      "    a = a+1\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "else: \n",
      "    a = len(dupids)/2\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "    \n",
      "if np.random.randint(2) == 0:\n",
      "    df=df[df['id'].apply(lambda x: x in dup1) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup2) == False]\n",
      "else: \n",
      "    df=df[df['id'].apply(lambda x: x in dup2) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup1) == False]\n",
      "\n",
      "#df['mymetric'] = df['score'] + (df['comments'].astype(float)/2)*0.30 - ((df['up/down']/20)*(0.30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>author</th>\n",
        "      <th>comments</th>\n",
        "      <th>downvotes</th>\n",
        "      <th>id</th>\n",
        "      <th>score</th>\n",
        "      <th>selftext</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>time_created</th>\n",
        "      <th>title</th>\n",
        "      <th>type</th>\n",
        "      <th>upvotes</th>\n",
        "      <th>karma</th>\n",
        "      <th>link_karma</th>\n",
        "      <th>up/down</th>\n",
        "      <th>mymetric</th>\n",
        "      <th>nrmscore</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>194 </th>\n",
        "      <td>  shennyepeldon</td>\n",
        "      <td>  6088</td>\n",
        "      <td>  8722</td>\n",
        "      <td> 1mw2mo</td>\n",
        "      <td> 2562</td>\n",
        "      <td> I do not want to sound cliche or almighty but ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1379851998</td>\n",
        "      <td>  year old redditors what advicetips would you ...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 11284</td>\n",
        "      <td>  1473</td>\n",
        "      <td>     2</td>\n",
        "      <td> 1.293740</td>\n",
        "      <td> 0.600205</td>\n",
        "      <td> 0.615639</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>508 </th>\n",
        "      <td>       thesouls</td>\n",
        "      <td>  1510</td>\n",
        "      <td> 13287</td>\n",
        "      <td> 11xk2i</td>\n",
        "      <td> 2105</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1350970194</td>\n",
        "      <td> A friend said this Presidential candidates sho...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 15392</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 1.158426</td>\n",
        "      <td> 0.556802</td>\n",
        "      <td> 0.580401</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>577 </th>\n",
        "      <td>        iamriot</td>\n",
        "      <td>  3053</td>\n",
        "      <td>  8358</td>\n",
        "      <td> 13jgci</td>\n",
        "      <td> 2125</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1353458946</td>\n",
        "      <td> A guy on a motorcycle knocked on my window at ...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 10483</td>\n",
        "      <td> 20355</td>\n",
        "      <td> 46079</td>\n",
        "      <td> 1.254247</td>\n",
        "      <td> 0.562361</td>\n",
        "      <td> 0.582050</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>810 </th>\n",
        "      <td>       Taodeist</td>\n",
        "      <td>  6092</td>\n",
        "      <td>  3911</td>\n",
        "      <td> 142uxm</td>\n",
        "      <td> 2426</td>\n",
        "      <td> I admit I had one when he asked me Back when I...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1354320555</td>\n",
        "      <td> A man once told me if you want a good story ju...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  6337</td>\n",
        "      <td> 31023</td>\n",
        "      <td>  7143</td>\n",
        "      <td> 1.620302</td>\n",
        "      <td> 0.591215</td>\n",
        "      <td> 0.605647</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1051</th>\n",
        "      <td>       maestoso</td>\n",
        "      <td>  5597</td>\n",
        "      <td> 10932</td>\n",
        "      <td> 11c3rs</td>\n",
        "      <td> 3300</td>\n",
        "      <td> KEEP THE COMMENTS COMINGIm reading them all  I...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1349996855</td>\n",
        "      <td> A really rich guy I know told me that when neg...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 14232</td>\n",
        "      <td>  5769</td>\n",
        "      <td>   363</td>\n",
        "      <td> 1.301866</td>\n",
        "      <td> 0.643361</td>\n",
        "      <td> 0.664212</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1232</th>\n",
        "      <td>   TheMediaSays</td>\n",
        "      <td>  4747</td>\n",
        "      <td>  5913</td>\n",
        "      <td> 1qoyn2</td>\n",
        "      <td> 3009</td>\n",
        "      <td> Places where the writers clearly did not think...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1384526912</td>\n",
        "      <td> A ume all of world history is a movie What are...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  8922</td>\n",
        "      <td> 51130</td>\n",
        "      <td>    97</td>\n",
        "      <td> 1.508879</td>\n",
        "      <td> 0.625071</td>\n",
        "      <td> 0.646069</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1596</th>\n",
        "      <td>       YogaButt</td>\n",
        "      <td>  7798</td>\n",
        "      <td> 10675</td>\n",
        "      <td> 1r034d</td>\n",
        "      <td> 2118</td>\n",
        "      <td> SERIOUS replies onlyEdit Thanks for up voting ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1384892372</td>\n",
        "      <td> Alien abductees of reddit or people who have c...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 12793</td>\n",
        "      <td>    13</td>\n",
        "      <td>   223</td>\n",
        "      <td> 1.198407</td>\n",
        "      <td> 0.573798</td>\n",
        "      <td> 0.581474</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1670</th>\n",
        "      <td>      redditeej</td>\n",
        "      <td> 27724</td>\n",
        "      <td> 20088</td>\n",
        "      <td> 1336qo</td>\n",
        "      <td> 2856</td>\n",
        "      <td> Ok ill go first I cant help but smell my finge...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1352760475</td>\n",
        "      <td> Alright Men of Reddit what are some things you...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 22944</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 1.142174</td>\n",
        "      <td> 0.660006</td>\n",
        "      <td> 0.636033</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1673</th>\n",
        "      <td> simplybrowsing</td>\n",
        "      <td> 21970</td>\n",
        "      <td> 14404</td>\n",
        "      <td> 134hhp</td>\n",
        "      <td> 2023</td>\n",
        "      <td> for me its being extra horny in the morning wh...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1352820135</td>\n",
        "      <td> Alright weve discovered what all Men of Reddit...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 16427</td>\n",
        "      <td>  2522</td>\n",
        "      <td>  1565</td>\n",
        "      <td> 1.140447</td>\n",
        "      <td> 0.600358</td>\n",
        "      <td> 0.573523</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1700</th>\n",
        "      <td>   Imperishable</td>\n",
        "      <td>  2205</td>\n",
        "      <td>  3823</td>\n",
        "      <td> 12egel</td>\n",
        "      <td> 2132</td>\n",
        "      <td> If thats the case its both depre ing and relie...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> 1351696224</td>\n",
        "      <td> Am I still being an idiot Im  now and looking ...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  5955</td>\n",
        "      <td>  1145</td>\n",
        "      <td>    33</td>\n",
        "      <td> 1.557677</td>\n",
        "      <td> 0.560697</td>\n",
        "      <td> 0.582624</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "              author  comments  downvotes      id  score  \\\n",
        "194    shennyepeldon      6088       8722  1mw2mo   2562   \n",
        "508         thesouls      1510      13287  11xk2i   2105   \n",
        "577          iamriot      3053       8358  13jgci   2125   \n",
        "810         Taodeist      6092       3911  142uxm   2426   \n",
        "1051        maestoso      5597      10932  11c3rs   3300   \n",
        "1232    TheMediaSays      4747       5913  1qoyn2   3009   \n",
        "1596        YogaButt      7798      10675  1r034d   2118   \n",
        "1670       redditeej     27724      20088  1336qo   2856   \n",
        "1673  simplybrowsing     21970      14404  134hhp   2023   \n",
        "1700    Imperishable      2205       3823  12egel   2132   \n",
        "\n",
        "                                               selftext  subreddit  \\\n",
        "194   I do not want to sound cliche or almighty but ...  AskReddit   \n",
        "508                                                 NaN  AskReddit   \n",
        "577                                                 NaN  AskReddit   \n",
        "810   I admit I had one when he asked me Back when I...  AskReddit   \n",
        "1051  KEEP THE COMMENTS COMINGIm reading them all  I...  AskReddit   \n",
        "1232  Places where the writers clearly did not think...  AskReddit   \n",
        "1596  SERIOUS replies onlyEdit Thanks for up voting ...  AskReddit   \n",
        "1670  Ok ill go first I cant help but smell my finge...  AskReddit   \n",
        "1673  for me its being extra horny in the morning wh...  AskReddit   \n",
        "1700  If thats the case its both depre ing and relie...  AskReddit   \n",
        "\n",
        "      time_created                                              title  \\\n",
        "194     1379851998   year old redditors what advicetips would you ...   \n",
        "508     1350970194  A friend said this Presidential candidates sho...   \n",
        "577     1353458946  A guy on a motorcycle knocked on my window at ...   \n",
        "810     1354320555  A man once told me if you want a good story ju...   \n",
        "1051    1349996855  A really rich guy I know told me that when neg...   \n",
        "1232    1384526912  A ume all of world history is a movie What are...   \n",
        "1596    1384892372  Alien abductees of reddit or people who have c...   \n",
        "1670    1352760475  Alright Men of Reddit what are some things you...   \n",
        "1673    1352820135  Alright weve discovered what all Men of Reddit...   \n",
        "1700    1351696224  Am I still being an idiot Im  now and looking ...   \n",
        "\n",
        "         type  upvotes  karma  link_karma   up/down  mymetric  nrmscore  \n",
        "194   top_all    11284   1473           2  1.293740  0.600205  0.615639  \n",
        "508   top_all    15392      0           0  1.158426  0.556802  0.580401  \n",
        "577   top_all    10483  20355       46079  1.254247  0.562361  0.582050  \n",
        "810   top_all     6337  31023        7143  1.620302  0.591215  0.605647  \n",
        "1051  top_all    14232   5769         363  1.301866  0.643361  0.664212  \n",
        "1232  top_all     8922  51130          97  1.508879  0.625071  0.646069  \n",
        "1596  top_all    12793     13         223  1.198407  0.573798  0.581474  \n",
        "1670  top_all    22944      0           0  1.142174  0.660006  0.636033  \n",
        "1673  top_all    16427   2522        1565  1.140447  0.600358  0.573523  \n",
        "1700  top_all     5955   1145          33  1.557677  0.560697  0.582624  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "title = list(df['title']) + list(df2['title'])\n",
      "vectorizer.fit(title)\n",
      "\n",
      "def category(x, df, num=2):\n",
      "    size = len(df)\n",
      "    blocksize = size/num\n",
      "    for i in range(num):\n",
      "        blockmax = max(sorted(df['score'])[blocksize*i:blocksize*(i+1)])        \n",
      "        if x < blockmax:\n",
      "            return i+1\n",
      "    return num\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "#X = vectorizer.transform(title)\n",
      "#Y = np.array(scores)\n",
      "x_train = vectorizer.transform(df['title'])\n",
      "x_test = vectorizer.transform(df2['title'])\n",
      "score = [category(i, df2) for i in df['score']]\n",
      "score2 = [category(i, df2) for i in df2['score']]\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training accuracy is 1.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test accuracy is 0.517730496454\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#words = vectorizer2.get_feature_names()\n",
      "#words\n",
      "#diag_words = np.eye(len(words))\n",
      "#probword = pd.DataFrame(clf.predict_proba(diag_words))\n",
      "#probword.rename(columns={0: 'rotten', 1: 'fresh'}, inplace=True)\n",
      "#probword['words'] = words\n",
      "#print \"Top 10 Rotten words are\"\n",
      "#probword.sort([3], ascending=False)\n",
      "#probword['words'].head(20)\n",
      "\n",
      "#print \"\\n Top 10 Fresh words are\"\n",
      "#print probword.sort(['fresh'], ascending=False)[0:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So while I have quite figured out how to get this to work properly, it's pretty obvious from doing a quick Naive Bayes fit that this method isn't going that work, regardless of what parameters we pick -- I don't really know though so you should check me on this. It could work-- idk. Alternately, could just be this subreddit.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "#apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from myalchemy import MyAlchemy\n",
      "p = MyAlchemy(apikey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dftitles = list(df['title'])\n",
      "df2titles = list(df2['title'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dftitles[5]\n",
      "print p.run_method(dftitles[5], 'concepts')\n",
      "print p.run_method(dftitles[5], 'keywords')\n",
      "print p.run_method(dftitles[5], 'category')\n",
      "#print p.run_method(dftitles[5], 'sentiment')\n",
      "print p.run_method(dftitles[5], 'entities')\n",
      "print len(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "A ume all of world history is a movie What are the biggest plotholes\n",
        "[(u'0.857188', u'History of the world'), (u'0.837988', u'Ibn Khaldun')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'0.930601', u'biggest plotholes'), (u'0.875235', u'ume'), (u'0.7165', u'world history'), (u'0.539622', u'movie')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'arts_entertainment', u'english', u'0.570921', u'OK')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[(u'1', u'0.33', u'world history', u'FieldTerminology')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "982\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Below is the testing I did for building a model off of concepts. I tried everything from concepts to categories to entities, comparing weekly and all time, even just comparing All Time and it yielded little to no results. Even with concepts, which are far more broad than keywords, there is just way too much difference between posts and no specific concept makes anything more or less popular, so there are no trends of any statistical significance. Additionally, Alchemy only allows for 1000 calls per day which is a massive limitation (I pretty much used up all of our 3,000 limit across 3 accounts just for testing) so running it on any even decently large data set would be unweildy - it is both slow and yields too few results (for free) for us to really do anything. So here again, it looks like we'll have to abandon ship with trying to bag-of-words this model.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "categories = []\n",
      "concepts, concepts2 = [], []\n",
      "for i in range(30):\n",
      "    conceptlist = p.run_method(dftitles[i], 'concepts')\n",
      "    for c in conceptlist:\n",
      "        concepts.append(c[1])\n",
      "        \n",
      "for i in range(30):\n",
      "    conceptlist = p.run_method(df2titles[i], 'concepts')\n",
      "    for c in conceptlist:\n",
      "        concepts2.append(c[1])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print concepts\n",
      "print \"--------\"\n",
      "print concepts2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'Gerontology', u'Ageing', u'Old age', u'2006 albums', u'Laptop', u'Internet', u'Problem solving', u'World Wide Web', u'Manchester City F.C.', u'Mobile phone', u'Wi-Fi', u'Man', u'Guy', u'2004 albums', u'History of the world', u'Ibn Khaldun', u'Alien abduction', u'Man', u'Gender', u'Leonard Cohen', u'Western culture', u'Puerto Rico', u'United States', u'Latin America', u'U.S. state', u'Territories of the United States', u'Native Americans in the United States', u'Christopher Columbus', u'Spanish language', u'Illegal drug trade', u'Drug', u'2007 albums', u'High school', u'College', u'Draw-A-Person Test', u'The Front Page', u'University', u'Education', u'Yolanda Adams', u'Knowledge', u'Ralph Waldo Emerson', u'Family', u'The Red Chord', u'Plane', u'Education', u'English-language films', u'2006 albums', u'Political terms', u'WALL-E', u'The Nature Conservancy', u'Statistics', u'Arnold Schwarzenegger', u'Taste', u'Internet pornography', u'Internet']\n",
        "--------\n",
        "[u'Vector space', u'John Carpenter', u'Old Testament', u'Pennsylvania', u'E-mail', u'Culture', u'Roswell UFO Incident', u'Theodore Roosevelt', u'Barack Obama', u'President of the United States', u'Hotel', u'United States', u'Country music', u'Old-time music', u'Rock and roll', u'Mother', u'Father', u'Parent', u'Family', u'Question', u'Adolescence', u'Pumpkin pie', u'Human sexuality', u'Life', u'Look', u'Norman Rockwell', u'Mika Nakashima', u'Stan Lee', u'2006 albums', u'Second language', u'Cognition', u'Homosexuality', u'Bisexuality', u'Sexual orientation', u'Heterosexuality', u'Street dance', u'2000s American television series', u'1999 in film', u'Time', u'Pixar', u'American films', u'Plus One', u'Christmas', u\"St Hilda's College, Oxford\"]\n"
       ]
      }
     ],
     "prompt_number": 19
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "vectorizer.fit(concepts)\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "X = vectorizer.transform(concepts)\n",
      "Y = np.array(df['score'][0:55])\n",
      "'''\n",
      "x_train = vectorizer.transform(concepts)\n",
      "x_test = vectorizer.transform(concepts2)\n",
      "score = df['score']\n",
      "score2 = df2['score']\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 20,
       "text": [
        "\"\\nx_train = vectorizer.transform(concepts)\\nx_test = vectorizer.transform(concepts2)\\nscore = df['score']\\nscore2 = df2['score']\\ny_train = np.array(score)\\ny_test = np.array(score2)\\n\""
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(Y)\n",
      "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)\n",
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "55\n",
        "Training accuracy is 1.0\n",
        "Test accuracy is 0.0\n"
       ]
      }
     ],
     "prompt_number": 22
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Below is what I believe to be our best chance of predicting popularity (based on above average ratings) for a weekly post.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "title = list(dftitles) + list(df2titles)\n",
      "vectorizer.fit(title)\n",
      "\n",
      "def category(x, df, num=2):\n",
      "    size = len(df)\n",
      "    blocksize = size/num\n",
      "    for i in range(num):\n",
      "        blockmax = max(sorted(df['score'])[blocksize*i:blocksize*(i+1)])        \n",
      "        if x < blockmax:\n",
      "            return i+1\n",
      "    return num\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "#X = vectorizer.transform(title)\n",
      "#Y = np.array(scores)\n",
      "x_train = vectorizer.transform(dftitles)\n",
      "x_test = vectorizer.transform(df2titles)\n",
      "score = [1 if i > np.mean(df['score']) else 0 for i in df['score']]\n",
      "score2 = [1 if i > np.mean(df2['score']) else 0 for i in df2['score']]\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 23
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 24,
       "text": [
        "\"\\nvectorizer2 = CountVectorizer(min_df=0.001)\\ntitle2 = df2['title']\\nvectorizer2.fit(title2)\\nX2 = vectorizer2.transform(title2)\\nY2 = np.array(df2['score'])\\n\""
       ]
      }
     ],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training accuracy is 0.849287169043\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test accuracy is 0.750759878419\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calibration_plot(clf, X, Y):\n",
      "    #Number of bins and array of bin edges from 0 to .95\n",
      "    N = 20\n",
      "    bins = [float(i)/N for i in range(N)]\n",
      "    unpopular, popular = zip(*clf.predict_proba(X))\n",
      "    \n",
      "    #Use nmupy digitize to separate data into bins and make 2D array for bins\n",
      "    digitized = np.digitize(popular, bins)\n",
      "    results = [[] for x in xrange(N)] \n",
      "    for i in range(len(digitized)):\n",
      "        results[digitized[i]-1].append(Y[i])\n",
      "    \n",
      "    #Calculate the y error values\n",
      "    yr = []\n",
      "    for i in results:\n",
      "        total = len(i)\n",
      "        frac = np.mean(i)\n",
      "        err = ((frac*(1-frac))/total)**.5\n",
      "        yr.append(err)\n",
      "    \n",
      "    #Calculate the \"Fresh\" fractions and plot them\n",
      "    means = [np.mean(x) for x in results]\n",
      "    fig1=plt.figure()\n",
      "    #Line y=x\n",
      "    x = np.array(range(0,2))\n",
      "    #Format the plot\n",
      "    plt.plot(x,x, c='r', label='Expectation')\n",
      "    plt.errorbar(bins,means,yerr=yr, c='black', label='Model with error', ecolor='b')\n",
      "    plt.axes().set_title('Popularity Fraction vs P(Popular)')\n",
      "    plt.xlabel('Popularity Probability') \n",
      "    plt.ylabel('Fraction Popular')\n",
      "    plt.legend(loc = 0).draw_frame(False)\n",
      "    remove_border()\n",
      "    plt.show()\n",
      "    \n",
      "    #Use the fresh array to plot histogram\n",
      "    fig2=plt.figure()\n",
      "    n, b, p = plt.hist(popular, N, label='examples', edgecolor='black')\n",
      "    #Annotate the bins with appropriate values\n",
      "    for i in range(len(bins)):\n",
      "        plt.annotate(int(n[i]), (.005+float(i)/N,n[i]+20))\n",
      "    #Format the plot\n",
      "    plt.axes().set_title('Number of Examples per Bin')\n",
      "    plt.xlabel('Bins') \n",
      "    plt.ylabel('Examples')\n",
      "    plt.legend(loc = 0).draw_frame(False)\n",
      "    remove_border()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 27
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calibration_plot(clf, x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get feature names and diagonalize them\n",
      "mywords = vectorizer.get_feature_names()\n",
      "diag = np.eye(len(mywords))\n",
      "print diag\n",
      "\n",
      "#Calculate the rotten and fresh word probabilities \n",
      "#and create a new, sorted DataFrame for them\n",
      "print len(zip(*clf.predict_proba(diag)))\n",
      "print popular\n",
      "data = pd.DataFrame({'words': mywords, 'punpopular': unpopular, 'ppopular': popular})\n",
      "sort = data.sort('ppopular', ascending=False).copy()\n",
      "\n",
      "print 'Top 10 \"Popular\" Words:'\n",
      "print\n",
      "for i in sort[:10].index:\n",
      "    print \"The word \\\"\",sort.words[i],\"\\\" has probability\", sort.ppopular[i], \"of being popular\"\n",
      "print\n",
      "print 'Top 10 \"Unpopular\" Words:'\n",
      "print\n",
      "for i in sort[:-11:-1].index:\n",
      "    print \"The word \\\"\",sort.words[i],\"\\\" has probability\", sort.punpopular[i], \"of being unpopular\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[[ 1.  0.  0. ...,  0.  0.  0.]\n",
        " [ 0.  1.  0. ...,  0.  0.  0.]\n",
        " [ 0.  0.  1. ...,  0.  0.  0.]\n",
        " ..., \n",
        " [ 0.  0.  0. ...,  1.  0.  0.]\n",
        " [ 0.  0.  0. ...,  0.  1.  0.]\n",
        " [ 0.  0.  0. ...,  0.  0.  1.]]\n",
        "2\n",
        "(0.47934482125623162, 0.47934482125623162, 0.38033351431680179, 0.18710041742456696, 0.53506130961558718, 0.31522256193032844, 0.47934482125623162, 0.58000583518676307, 0.26914624320784236, 0.47934482125623162, 0.47934482125623207, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.18710041742456696, 0.47934482125623162, 0.47934482125623162, 0.64805015621602136, 0.31522256193032844, 0.47934482125623162, 0.37332333376356541, 0.23482211781172463, 0.73418189005384737, 0.31522256193032844, 0.47934482125623162, 0.16984232321856516, 0.31522256193032788, 0.26914624320784236, 0.11623492556823707, 0.47934482125623162, 0.31522256193032844, 0.17523004801724301, 0.35583371148528048, 0.2829316890657948, 0.47934482125623207, 0.11623492556823717, 0.47934482125623162, 0.73418189005384737, 0.47934482125623162, 0.73418189005384737, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.29502024142046768, 0.64805015621602136, 0.31522256193032788, 0.47934482125623207, 0.23482211781172463, 0.47934482125623162, 0.18710041742456696, 0.35583371148528048, 0.39672255925582162, 0.31522256193032844, 0.47934482125623207, 0.40845649197653361, 0.38033351431680146, 0.47934482125623207, 0.38033351431680146, 0.31522256193032844, 0.30991695841046157, 0.31522256193032816, 0.23482211781172463, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.64805015621602136, 0.13303028560533389, 0.31522256193032788, 0.47934482125623162, 0.21308263520659412, 0.31522256193032844, 0.31522256193032788, 0.31522256193032816, 0.31522256193032788, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.15549912414706801, 0.47934482125623207, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.31522256193032844, 0.36322241576320236, 0.47934482125623207, 0.10320506398857977, 0.47934482125623162, 0.23482211781172463, 0.32931685912434827, 0.38033351431680179, 0.23482211781172441, 0.38033351431680179, 0.47934482125623162, 0.73418189005384737, 0.47934482125623162, 0.31522256193032844, 0.18710041742456696, 0.73418189005384737, 0.23482211781172463, 0.18710041742456696, 0.55107480963403066, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.15549912414706801, 0.78644470105687458, 0.31522256193032844, 0.18710041742456679, 0.31522256193032816, 0.34473052242680963, 0.31522256193032844, 0.18710041742456696, 0.47934482125623162, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.38033351431680146, 0.38033351431680146, 0.38033351431680146, 0.31522256193032844, 0.27586685409756367, 0.58000583518676307, 0.78644470105687458, 0.39189760045104849, 0.26914624320784236, 0.31522256193032844, 0.40845649197653322, 0.23482211781172463, 0.16477602391477381, 0.47934482125623162, 0.23482211781172463, 0.64805015621602136, 0.23482211781172463, 0.21642197262585144, 0.34473052242680963, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.58000583518676307, 0.38647415746979358, 0.55107480963403066, 0.47934482125623162, 0.47934482125623207, 0.23482211781172463, 0.31522256193032816, 0.31522256193032844, 0.15549912414706801, 0.31522256193032844, 0.47934482125623162, 0.58000583518676307, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.40845649197653361, 0.31522256193032844, 0.47934482125623162, 0.26914624320784236, 0.47934482125623207, 0.47934482125623207, 0.31522256193032788, 0.64805015621602136, 0.31522256193032844, 0.73418189005384737, 0.23482211781172463, 0.58000583518676307, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.38033351431680146, 0.23482211781172463, 0.47934482125623162, 0.40845649197653361, 0.23482211781172463, 0.73418189005384737, 0.47934482125623162, 0.40845649197653361, 0.34344452975487894, 0.18710041742456696, 0.35583371148528048, 0.38033351431680146, 0.17805415800470203, 0.23482211781172463, 0.64805015621602136, 0.64805015621602136, 0.40845649197653361, 0.23482211781172463, 0.26709145590306499, 0.23482211781172463, 0.23482211781172463, 0.58000583518676307, 0.52489285149205234, 0.73418189005384737, 0.47934482125623162, 0.64805015621602136, 0.23482211781172463, 0.15549912414706801, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.23482211781172441, 0.31522256193032844, 0.18710041742456696, 0.38033351431680146, 0.47934482125623162, 0.58000583518676307, 0.38033351431680146, 0.47934482125623162, 0.47934482125623162, 0.39672255925582162, 0.47934482125623207, 0.64805015621602136, 0.38033351431680146, 0.31522256193032844, 0.18710041742456696, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.58000583518676307, 0.23482211781172441, 0.23482211781172463, 0.18710041742456679, 0.23482211781172463, 0.38033351431680146, 0.18710041742456679, 0.31522256193032844, 0.47934482125623162, 0.38033351431680146, 0.31522256193032844, 0.47934482125623162, 0.64805015621602136, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.31522256193032788, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.2829316890657948, 0.18710041742456696, 0.64805015621602136, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.64805015621602136, 0.23482211781172441, 0.31522256193032844, 0.55107480963403066, 0.47934482125623207, 0.47934482125623162, 0.40845649197653361, 0.26914624320784236, 0.47934482125623162, 0.47934482125623207, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.38033351431680146, 0.64805015621602136, 0.47934482125623162, 0.38033351431680146, 0.23482211781172463, 0.47934482125623207, 0.64805015621602136, 0.31522256193032844, 0.31522256193032788, 0.31522256193032844, 0.64805015621602136, 0.47934482125623207, 0.23482211781172463, 0.38033351431680146, 0.23482211781172463, 0.31522256193032844, 0.33614865412896489, 0.23482211781172463, 0.47934482125623162, 0.47934482125623207, 0.47934482125623207, 0.18710041742456696, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.64805015621602136, 0.31522256193032788, 0.47934482125623162, 0.25664173798415002, 0.23482211781172463, 0.23482211781172463, 0.73418189005384737, 0.47934482125623162, 0.23482211781172463, 0.23482211781172463, 0.47934482125623162, 0.58000583518676307, 0.26914624320784236, 0.58000583518676307, 0.31522256193032844, 0.18710041742456696, 0.38033351431680212, 0.47934482125623162, 0.13303028560533389, 0.47934482125623162, 0.23482211781172463, 0.15549912414706801, 0.18710041742456696, 0.47934482125623162, 0.23482211781172463, 0.73418189005384737, 0.31522256193032844, 0.47934482125623162, 0.26364101390898881, 0.31522256193032788, 0.23482211781172463, 0.47934482125623162, 0.38033351431680146, 0.23482211781172463, 0.58000583518676307, 0.26914624320784236, 0.38033351431680146, 0.23482211781172463, 0.31522256193032788, 0.47934482125623162, 0.47934482125623162, 0.18710041742456696, 0.47934482125623162, 0.23482211781172463, 0.64805015621602136, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.31522256193032844, 0.23482211781172463, 0.64805015621602136, 0.38033351431680146, 0.31522256193032844, 0.20174298802771426, 0.1554991241470676, 0.47934482125623162, 0.15549912414706801, 0.64805015621602136, 0.23482211781172463, 0.20826247755877331, 0.31522256193032844, 0.31522256193032844, 0.64805015621602136, 0.47934482125623162, 0.31522256193032844, 0.15549912414706801, 0.47934482125623162, 0.64805015621602136, 0.47934482125623162, 0.38033351431680146, 0.26615363727421543, 0.47934482125623162, 0.18710041742456696, 0.23482211781172463, 0.47934482125623162, 0.26364101390898881, 0.38033351431680212, 0.31522256193032844, 0.38033351431680212, 0.23482211781172463, 0.31522256193032844, 0.34473052242680929, 0.38033351431680179, 0.64805015621602136, 0.64805015621602136, 0.31522256193032844, 0.34473052242680963, 0.18710041742456696, 0.18710041742456696, 0.31522256193032844, 0.47934482125623207, 0.47934482125623162, 0.47934482125623207, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.58000583518676307, 0.31522256193032844, 0.35583371148528048, 0.31522256193032788, 0.47934482125623162, 0.11623492556823717, 0.18710041742456696, 0.31522256193032788, 0.64805015621602136, 0.47934482125623162, 0.38033351431680146, 0.47934482125623207, 0.47934482125623162, 0.40845649197653361, 0.22342480329102071, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.23482211781172441, 0.31522256193032844, 0.73418189005384737, 0.18710041742456679, 0.38033351431680146, 0.58000583518676307, 0.38033351431680146, 0.47934482125623162, 0.58000583518676307, 0.11623492556823717, 0.31522256193032788, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.23482211781172463, 0.38033351431680146, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.1554991241470676, 0.23482211781172463, 0.35583371148528048, 0.38033351431680212, 0.60543369718673612, 0.23482211781172463, 0.23482211781172463, 0.33030142622646952, 0.40104291900232725, 0.47934482125623162, 0.18710041742456696, 0.47934482125623162, 0.31522256193032844, 0.18710041742456696, 0.47934482125623162, 0.47934482125623162, 0.47934482125623207, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.38033351431680146, 0.47934482125623162, 0.64805015621602136, 0.34473052242680963, 0.47934482125623162, 0.23482211781172441, 0.23482211781172441, 0.40845649197653361, 0.31522256193032844, 0.23482211781172441, 0.47934482125623162, 0.47934482125623162, 0.38033351431680146, 0.31522256193032844, 0.38033351431680146, 0.47934482125623162, 0.41458765791519331, 0.31522256193032788, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.23482211781172441, 0.58000583518676307, 0.23482211781172463, 0.26914624320784236, 0.23482211781172463, 0.23482211781172463, 0.18710041742456696, 0.23482211781172463, 0.35130769994132355, 0.31522256193032844, 0.23482211781172463, 0.23482211781172441, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.18710041742456696, 0.47934482125623162, 0.26914624320784236, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.40845649197653361, 0.47934482125623162, 0.47934482125623162, 0.17523004801724301, 0.31522256193032844, 0.31522256193032844, 0.26150146993936807, 0.38033351431680146, 0.47934482125623162, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.39672255925582162, 0.23482211781172463, 0.23482211781172463, 0.27162774518304256, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.18710041742456696, 0.33839509451828553, 0.23482211781172463, 0.73418189005384737, 0.23482211781172463, 0.31522256193032844, 0.35583371148528048, 0.39672255925582162, 0.24370189395959482, 0.2829316890657948, 0.31522256193032844, 0.47934482125623162, 0.35583371148528048, 0.47934482125623207, 0.47934482125623207, 0.47934482125623162, 0.18710041742456696, 0.47934482125623207, 0.31522256193032788, 0.64805015621602136, 0.47934482125623207, 0.47934482125623162, 0.47934482125623162, 0.23482211781172441, 0.43413762111230719, 0.18710041742456696, 0.35583371148528048, 0.31522256193032844, 0.31522256193032844, 0.23482211781172463, 0.18710041742456696, 0.64805015621602136, 0.47934482125623162, 0.24370189395959482, 0.47934482125623207, 0.31522256193032844, 0.23482211781172463, 0.64805015621602136, 0.64805015621602136, 0.58000583518676307, 0.64805015621602136, 0.23482211781172463, 0.58000583518676307, 0.58000583518676307, 0.18710041742456679, 0.23482211781172441, 0.73418189005384737, 0.27488629116757035, 0.31522256193032844, 0.73418189005384737, 0.43413762111230719, 0.38033351431680146, 0.38033351431680146, 0.34940303558166863, 0.20826247755877331, 0.23482211781172463, 0.25664173798415002, 0.47934482125623207, 0.26914624320784236, 0.40845649197653361, 0.52489285149205234, 0.53506130961558718, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.38033351431680146, 0.18710041742456696, 0.64805015621602136, 0.23482211781172463, 0.73418189005384737, 0.31522256193032844, 0.51786312371982479, 0.38033351431680212, 0.31522256193032844, 0.47934482125623162, 0.1358341935089403, 0.47934482125623162, 0.58000583518676307, 0.23482211781172463, 0.23482211781172463, 0.26914624320784236, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.18710041742456696, 0.18710041742456696, 0.47934482125623162, 0.18710041742456696, 0.64805015621602136, 0.26709145590306499, 0.64805015621602136, 0.13303028560533389, 0.23482211781172463, 0.31522256193032844, 0.30278225453703822, 0.47934482125623207, 0.20826247755877331, 0.20826247755877367, 0.58000583518676307, 0.47934482125623162, 0.18710041742456696, 0.35583371148528048, 0.38033351431680179, 0.15549912414706801, 0.47934482125623162, 0.31522256193032844, 0.15549912414706801, 0.26914624320784258, 0.47934482125623162, 0.15549912414706774, 0.20826247755877331, 0.38033351431680146, 0.31522256193032844, 0.42413747769139598, 0.47934482125623162, 0.78644470105687458, 0.64805015621602136, 0.34473052242680963, 0.47934482125623162, 0.73418189005384737, 0.31522256193032788, 0.16984232321856516, 0.73418189005384737, 0.40845649197653361, 0.47934482125623162, 0.64805015621602136, 0.39189760045104849, 0.31522256193032844, 0.64805015621602136, 0.31522256193032844, 0.31522256193032788, 0.38033351431680146, 0.47934482125623162, 0.58000583518676307, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.64805015621602136, 0.18710041742456696, 0.31522256193032844, 0.40845649197653361, 0.18710041742456696, 0.47934482125623162, 0.23482211781172441, 0.2829316890657948, 0.19168333817724201, 0.47934482125623162, 0.42413747769139598, 0.38033351431680146, 0.31522256193032788, 0.23482211781172463, 0.31522256193032844, 0.31522256193032788, 0.47934482125623207, 0.31522256193032844, 0.11623492556823717, 0.15549912414706801, 0.82153325712007286, 0.23482211781172463, 0.22137817472454702, 0.47934482125623162, 0.31522256193032788, 0.13303028560533389, 0.73418189005384737, 0.17979274505432935, 0.23482211781172463, 0.31522256193032844, 0.64805015621602136, 0.31522256193032844, 0.47934482125623162, 0.18710041742456696, 0.73418189005384737, 0.47934482125623162, 0.34824116560613577, 0.47934482125623162, 0.18710041742456696, 0.31522256193032844, 0.47934482125623162, 0.38033351431680146, 0.73418189005384737, 0.31522256193032844, 0.47934482125623207, 0.73418189005384737, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.23482211781172441, 0.23482211781172463, 0.23482211781172463, 0.64805015621602136, 0.23482211781172463, 0.42413747769139598, 0.64805015621602136, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.13303028560533389, 0.73418189005384737, 0.47934482125623162, 0.20826247755877331, 0.64805015621602136, 0.50567294501341631, 0.58000583518676307, 0.36524484821626396, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.23482211781172441, 0.23482211781172463, 0.23482211781172463, 0.47934482125623162, 0.330239718339088, 0.20826247755877331, 0.25125408980562269, 0.82153325712007286, 0.47934482125623162, 0.29820552718657517, 0.084304174740585075, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.15549912414706801, 0.41727201160309441, 0.18710041742456696, 0.47934482125623162, 0.17683275508227783, 0.47934482125623207, 0.38033351431680146, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.13303028560533378, 0.25664173798415002, 0.31522256193032844, 0.26914624320784236, 0.47934482125623162, 0.31522256193032844, 0.64805015621602136, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.2829316890657948, 0.31522256193032844, 0.73418189005384737, 0.47934482125623162, 0.18710041742456696, 0.78644470105687458, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.47934482125623207, 0.15549912414706801, 0.64805015621602136, 0.58000583518676307, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.31522256193032788, 0.23482211781172463, 0.40845649197653361, 0.40845649197653361, 0.38033351431680212, 0.31522256193032844, 0.38033351431680146, 0.13303028560533389, 0.47934482125623162, 0.73418189005384737, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.35583371148528048, 0.47934482125623207, 0.23482211781172463, 0.23482211781172463, 0.31522256193032844, 0.73418189005384737, 0.18710041742456679, 0.23482211781172463, 0.36943233171251405, 0.31522256193032844, 0.73418189005384737, 0.2452475628537388, 0.64805015621602136, 0.47934482125623207, 0.23482211781172463, 0.73418189005384737, 0.47934482125623207, 0.47934482125623162, 0.47934482125623162, 0.38033351431680146, 0.36524484821626396, 0.58000583518676307, 0.40845649197653361, 0.18710041742456696, 0.13303028560533389, 0.26914624320784236, 0.38033351431680146, 0.31522256193032844, 0.64805015621602136, 0.47934482125623162, 0.10320506398857977, 0.18710041742456696, 0.18710041742456696, 0.35583371148528048, 0.31522256193032788, 0.26914624320784236, 0.23482211781172463, 0.16984232321856516, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.4556225948976198, 0.31522256193032788, 0.22835067752200081, 0.36524484821626396, 0.20826247755877331, 0.47934482125623162, 0.23482211781172463, 0.13303028560533378, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.58000583518676307, 0.31522256193032844, 0.47934482125623162, 0.38033351431680146, 0.47934482125623162, 0.38033351431680146, 0.13303028560533378, 0.23482211781172463, 0.73418189005384737, 0.18710041742456696, 0.30425686526144202, 0.47934482125623162, 0.64805015621602136, 0.31522256193032844, 0.40845649197653361, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.35583371148528048, 0.55107480963403066, 0.23482211781172463, 0.31522256193032844, 0.64805015621602136, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.25664173798415002, 0.31522256193032788, 0.47934482125623162, 0.64805015621602136, 0.23482211781172441, 0.38033351431680146, 0.23482211781172463, 0.18710041742456696, 0.64805015621602202, 0.64805015621602136, 0.47934482125623162, 0.47934482125623207, 0.31522256193032844, 0.47934482125623162, 0.2829316890657948, 0.31522256193032844, 0.31522256193032844, 0.64805015621602136, 0.18710041742456696, 0.31522256193032844, 0.64805015621602136, 0.11623492556823717, 0.38761200427120651, 0.31522256193032844, 0.38033351431680146, 0.2829316890657948, 0.47934482125623162, 0.31522256193032788, 0.36165790728141728, 0.15549912414706801, 0.28053687828641866, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.18710041742456696, 0.25965765459096024, 0.55107480963403066, 0.31522256193032816, 0.47934482125623162, 0.47934482125623162, 0.73418189005384737, 0.58000583518676307, 0.64805015621602202, 0.23482211781172463, 0.29706005956078718, 0.18710041742456696, 0.38033351431680146, 0.47934482125623207, 0.23482211781172463, 0.47934482125623162, 0.64805015621602136, 0.31522256193032844, 0.58000583518676307, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.38033351431680146, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.58000583518676307, 0.22756674126434379, 0.35583371148528048, 0.23482211781172463, 0.2829316890657948, 0.23482211781172463, 0.31522256193032844, 0.26914624320784258, 0.47934482125623162, 0.20826247755877367, 0.64805015621602136, 0.18710041742456696, 0.47934482125623162, 0.64805015621602136, 0.23482211781172463, 0.73418189005384737, 0.46088614744999284, 0.31522256193032844, 0.18710041742456696, 0.64805015621602136, 0.2116828293724615, 0.44107054409392599, 0.64805015621602136, 0.73418189005384737, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.31522256193032788, 0.47934482125623162, 0.27322144565670764, 0.34473052242680963, 0.73418189005384737, 0.23482211781172463, 0.18710041742456696, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.43413762111230719, 0.47934482125623162, 0.31522256193032844, 0.33839509451828553, 0.31522256193032844, 0.31522256193032844, 0.3456550006787083, 0.39672255925582162, 0.2829316890657948, 0.23482211781172463, 0.18710041742456696, 0.38033351431680146, 0.26914624320784258, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.18710041742456696, 0.47934482125623162, 0.31522256193032844, 0.32312501624462142, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.31522256193032844, 0.25965765459096024, 0.20826247755877331, 0.31522256193032844, 0.25664173798415002, 0.32044720517306602, 0.64805015621602136, 0.38033351431680146, 0.35583371148528048, 0.15549912414706801, 0.58000583518676307, 0.47934482125623162, 0.31522256193032844, 0.23482211781172441, 0.47934482125623207, 0.26914624320784236, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.23482211781172463, 0.23482211781172441, 0.1472111289892844, 0.64805015621602136, 0.64805015621602136, 0.38033351431680146, 0.31522256193032788, 0.58000583518676307, 0.58000583518676307, 0.31522256193032844, 0.20826247755877331, 0.47934482125623162, 0.78644470105687458, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.29642748300569122, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.38033351431680179, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.58000583518676307, 0.18710041742456696, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.13303028560533389, 0.47934482125623162, 0.64805015621602136, 0.40845649197653361, 0.47934482125623162, 0.31522256193032788, 0.47934482125623162, 0.35583371148528048, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.31522256193032788, 0.23482211781172463, 0.18710041742456696, 0.58000583518676307, 0.47934482125623162, 0.38033351431680146, 0.41727201160309441, 0.47934482125623162, 0.40845649197653361, 0.31522256193032844, 0.64805015621602136, 0.31522256193032788, 0.23482211781172463, 0.47934482125623162, 0.47934482125623207, 0.38033351431680146, 0.47934482125623162, 0.38033351431680146, 0.58000583518676307, 0.31522256193032844, 0.55107480963403066, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.73418189005384737, 0.18710041742456696, 0.40845649197653361, 0.38033351431680146, 0.23482211781172463, 0.23482211781172463, 0.64805015621602136, 0.38033351431680146, 0.23482211781172463, 0.26914624320784236, 0.47934482125623162, 0.31522256193032844, 0.64805015621602136, 0.13303028560533389, 0.31522256193032844, 0.2829316890657948, 0.23482211781172441, 0.39672255925582162, 0.47934482125623162, 0.38033351431680146, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.26914624320784236, 0.23482211781172463, 0.18710041742456696, 0.23482211781172463, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.23482211781172463, 0.13303028560533389, 0.47934482125623162, 0.31522256193032788, 0.31522256193032844, 0.31522256193032788, 0.73418189005384737, 0.47934482125623162, 0.23482211781172463, 0.23482211781172463, 0.38033351431680146, 0.26914624320784236, 0.38033351431680146, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.31522256193032844, 0.64805015621602136, 0.47934482125623207, 0.31522256193032844, 0.47934482125623162, 0.64805015621602136, 0.23482211781172463, 0.34473052242680963, 0.47934482125623162, 0.58000583518676307, 0.47934482125623162, 0.29502024142046768, 0.23482211781172441, 0.64805015621602136, 0.34473052242680929, 0.42413747769139598, 0.31522256193032844, 0.31522256193032844, 0.64805015621602136, 0.38033351431680146, 0.35583371148528048, 0.64805015621602136, 0.23482211781172463, 0.31522256193032788, 0.58000583518676307, 0.28162038352851276, 0.64805015621602136, 0.38033351431680146, 0.31522256193032844, 0.38033351431680146, 0.18710041742456696, 0.47934482125623162, 0.47934482125623162, 0.78644470105687458, 0.38033351431680146, 0.18710041742456696, 0.31522256193032844, 0.15549912414706801, 0.23482211781172463, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.73418189005384737, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.31522256193032844, 0.38033351431680146, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.58000583518676307, 0.31522256193032844, 0.31522256193032844, 0.23482211781172463, 0.31522256193032844, 0.38033351431680146, 0.47934482125623162, 0.47934482125623162, 0.64805015621602136, 0.18710041742456696, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.23482211781172463, 0.38033351431680146, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.29036788949690467, 0.23482211781172463, 0.64805015621602136, 0.22074627081167192, 0.47934482125623162, 0.31522256193032844, 0.64805015621602136, 0.31522256193032788, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.13303028560533389, 0.38033351431680146, 0.19711509086740661, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.73418189005384737, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.31522256193032844, 0.38033351431680212, 0.64805015621602136, 0.47934482125623162, 0.23482211781172482, 0.38033351431680146, 0.73418189005384737, 0.64805015621602136, 0.47934482125623162, 0.23482211781172463, 0.58000583518676307, 0.38033351431680146, 0.47934482125623162, 0.47934482125623162, 0.20826247755877367, 0.38033351431680146, 0.69712044237448512, 0.31522256193032788, 0.26364101390898881, 0.47934482125623162, 0.23482211781172463, 0.64805015621602136, 0.24893151715282938, 0.73418189005384737, 0.31522256193032844, 0.58000583518676307, 0.64805015621602136, 0.23482211781172463, 0.23482211781172463, 0.38033351431680146, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.29036788949690467, 0.31522256193032844, 0.47934482125623162, 0.55107480963403066, 0.31522256193032844, 0.38033351431680146, 0.31522256193032844, 0.33429927891039479, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.51271316695434399, 0.47934482125623162, 0.39672255925582162, 0.47934482125623207, 0.58000583518676307, 0.64805015621602136, 0.23482211781172463, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.38033351431680146, 0.73418189005384737, 0.31522256193032844, 0.55107480963403066, 0.35583371148528048, 0.31522256193032788, 0.35583371148528048, 0.43413762111230719, 0.47934482125623162, 0.78644470105687458, 0.47934482125623162, 0.40845649197653361, 0.47934482125623162, 0.47934482125623162, 0.47934482125623207, 0.47934482125623162, 0.64805015621602136, 0.18710041742456696, 0.47934482125623162, 0.18710041742456696, 0.28884954201455182, 0.38033351431680146, 0.47934482125623162, 0.73418189005384737, 0.31522256193032816, 0.15549912414706801, 0.47934482125623162, 0.27854193965496554, 0.47934482125623207, 0.24208639850646005, 0.47934482125623162, 0.38033351431680146, 0.25664173798415002, 0.31522256193032844, 0.23482211781172463, 0.73418189005384737, 0.47934482125623162, 0.23482211781172463, 0.64805015621602136, 0.31522256193032844, 0.35583371148528048, 0.47934482125623162, 0.47934482125623162, 0.31522256193032844, 0.64805015621602136, 0.47934482125623162, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.40845649197653361, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.64805015621602136, 0.47934482125623162, 0.47934482125623162, 0.36524484821626396, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.31522256193032844, 0.47934482125623162, 0.73418189005384737, 0.58000583518676307, 0.47934482125623162, 0.47934482125623162, 0.2829316890657948, 0.23482211781172463, 0.23482211781172463, 0.2829316890657948, 0.58000583518676307, 0.47934482125623162, 0.20069588412720119, 0.33839509451828581, 0.47934482125623162, 0.47934482125623162, 0.40845649197653361, 0.26914624320784236, 0.23482211781172463, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.38033351431680146, 0.47934482125623207, 0.31522256193032844, 0.38033351431680146, 0.23482211781172463, 0.47934482125623207, 0.26914624320784236, 0.31522256193032844, 0.23482211781172441, 0.47934482125623162, 0.18710041742456696, 0.31522256193032844, 0.55107480963403066, 0.47934482125623162, 0.18710041742456696, 0.31522256193032844, 0.31522256193032844, 0.47934482125623207, 0.18710041742456696, 0.31522256193032844, 0.73418189005384737, 0.31522256193032844, 0.23482211781172441, 0.31522256193032844, 0.23482211781172463, 0.23482211781172463, 0.31522256193032844, 0.64805015621602136, 0.47934482125623162, 0.18710041742456696, 0.26150146993936807, 0.31522256193032844, 0.23482211781172463, 0.38033351431680146, 0.69712044237448512, 0.64805015621602136, 0.31522256193032844, 0.31522256193032844, 0.31522256193032844, 0.38033351431680146, 0.31522256193032844, 0.47934482125623207, 0.58000583518676307, 0.31522256193032844, 0.31522256193032844, 0.38033351431680146, 0.58000583518676307, 0.1472111289892844, 0.47934482125623162, 0.18710041742456696, 0.23482211781172463, 0.31522256193032844, 0.15549912414706801, 0.47934482125623162, 0.73418189005384737, 0.31522256193032788, 0.26150146993936807, 0.47934482125623162, 0.3475596557691869, 0.35583371148528048, 0.30705175440180232, 0.64805015621602136, 0.25081531191074591, 0.31522256193032816, 0.31522256193032844, 0.31522256193032788, 0.31522256193032816, 0.73418189005384737, 0.40845649197653361, 0.32534946995360808, 0.31522256193032788, 0.16601404994046395, 0.23482211781172463, 0.47934482125623162, 0.31177902576090227, 0.245247562853739, 0.32258587967162772, 0.23482211781172463, 0.38033351431680146, 0.26914624320784208, 0.1554991241470676, 0.23482211781172463, 0.17523004801724301, 0.64805015621602136, 0.15549912414706801, 0.20826247755877331, 0.23482211781172463, 0.52489285149205234, 0.47934482125623162, 0.47934482125623162, 0.64805015621602136, 0.31522256193032844, 0.43671177490224955, 0.23482211781172463, 0.53506130961558718, 0.38033351431680146, 0.23482211781172463, 0.31522256193032844, 0.28570704143259634, 0.17805415800470203, 0.15549912414706801, 0.26914624320784236, 0.47934482125623162, 0.38033351431680146, 0.25081531191074613, 0.64805015621602136, 0.40845649197653361, 0.47934482125623162, 0.31522256193032844, 0.26914624320784236, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.60543369718673612, 0.35583371148528048, 0.13303028560533389, 0.47934482125623162, 0.73418189005384737, 0.47934482125623162, 0.41727201160309441, 0.31522256193032844, 0.15549912414706801, 0.23482211781172463, 0.23482211781172441, 0.15549912414706801, 0.34473052242680963, 0.47934482125623162, 0.23482211781172463, 0.38033351431680212, 0.47934482125623162, 0.25664173798415002, 0.38033351431680146, 0.31522256193032844, 0.47934482125623162, 0.31522256193032844, 0.18710041742456696, 0.73418189005384737, 0.47934482125623162, 0.23482211781172463, 0.47934482125623162, 0.26914624320784236, 0.47934482125623162, 0.43413762111230719, 0.23482211781172463, 0.23482211781172463, 0.26914624320784236, 0.31522256193032844, 0.23482211781172463, 0.40845649197653361, 0.64805015621602136, 0.23482211781172463, 0.58000583518676307, 0.47934482125623162, 0.23482211781172441, 0.25081531191074591, 0.68235813312428129, 0.13303028560533389, 0.23482211781172463, 0.29502024142046768, 0.26914624320784236, 0.42413747769139598, 0.23482211781172463, 0.47934482125623162, 0.47934482125623207, 0.23482211781172463, 0.47934482125623162, 0.31522256193032844, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.10320506398857977, 0.64805015621602136, 0.2829316890657948, 0.47934482125623162, 0.47934482125623162, 0.47934482125623162, 0.58000583518676307, 0.78644470105687458, 0.23482211781172463, 0.23482211781172463, 0.47934482125623162, 0.23482211781172463, 0.31522256193032844, 0.23482211781172463, 0.47934482125623162, 0.45768169882795717, 0.18710041742456696, 0.23482211781172463, 0.31522256193032844, 0.38033351431680146, 0.21790672849074119, 0.34473052242680963, 0.73418189005384737, 0.31522256193032844, 0.38033351431680146, 0.84671843739649588, 0.23482211781172463, 0.36943233171251405, 0.18710041742456696, 0.33429927891039452, 0.31522256193032844, 0.31522256193032844, 0.47934482125623162, 0.58000583518676307, 0.73418189005384737, 0.7631622243479137, 0.18710041742456696, 0.69712044237448512, 0.2829316890657948, 0.31522256193032844, 0.23482211781172441, 0.38033351431680212, 0.22950063864093276, 0.47934482125623162, 0.47934482125623162, 0.31573981432542469, 0.47934482125623162, 0.31307866983588351, 0.2430890679539526, 0.36165790728141728, 0.22524689865173497, 0.10933315258722899, 0.47934482125623162, 0.23482211781172463, 0.25835648285985768, 0.58000583518676307, 0.64805015621602136, 0.18710041742456696, 0.23482211781172463, 0.43609611623736871, 0.47934482125623162, 0.23482211781172463, 0.26914624320784236, 0.47934482125623162, 0.31522256193032844, 0.23482211781172463, 0.25664173798415002, 0.28517886186570879, 0.42413747769139598, 0.31522256193032844, 0.23482211781172463, 0.11623492556823717, 0.15549912414706801, 0.31522256193032844, 0.31522256193032844, 0.15549912414706801, 0.23482211781172463, 0.35583371148528048, 0.73418189005384737, 0.20826247755877331, 0.31522256193032788, 0.18710041742456696, 0.31522256193032788, 0.35583371148528048, 0.47934482125623162, 0.31522256193032844, 0.31522256193032844, 0.47934482125623207, 0.29293425435065179, 0.47934482125623207, 0.47934482125623162, 0.322585879671628, 0.26914624320784236, 0.73418189005384737, 0.64805015621602136, 0.47934482125623207, 0.15549912414706801, 0.36524484821626396, 0.14338983963135854, 0.31522256193032844, 0.23482211781172463, 0.38033351431680212, 0.28074351215204907, 0.31522256193032844, 0.23482211781172463, 0.2829316890657948, 0.26914624320784236, 0.30520222505124406, 0.20826247755877331, 0.23482211781172463, 0.092802016664080375, 0.47934482125623162, 0.2441635450262632, 0.31522256193032844)\n",
        "Top 10 \"Popular\" Words:\n",
        "\n",
        "The word \" watching \" has probability 0.846718437396 of being popular\n",
        "The word \" item \" has probability 0.82153325712 of being popular\n",
        "The word \" ideas \" has probability 0.82153325712 of being popular\n",
        "The word \" hidden \" has probability 0.786444701057 of being popular\n",
        "The word \" visit \" has probability 0.786444701057 of being popular\n",
        "The word \" became \" has probability 0.786444701057 of being popular\n",
        "The word \" site \" has probability 0.786444701057 of being popular\n",
        "The word \" pc \" has probability 0.786444701057 of being popular\n",
        "The word \" lame \" has probability 0.786444701057 of being popular\n",
        "The word \" awesome \" has probability 0.786444701057 of being popular\n",
        "\n",
        "Top 10 \"Unpopular\" Words:\n",
        "\n",
        "The word \" ive \" has probability 0.915695825259 of being unpopular\n",
        "The word \" yourself \" has probability 0.907197983336 of being unpopular\n",
        "The word \" around \" has probability 0.896794936011 of being unpopular\n",
        "The word \" look \" has probability 0.896794936011 of being unpopular\n",
        "The word \" very \" has probability 0.896794936011 of being unpopular\n",
        "The word \" while \" has probability 0.890666847413 of being unpopular\n",
        "The word \" advice \" has probability 0.883765074432 of being unpopular\n",
        "The word \" mom \" has probability 0.883765074432 of being unpopular\n",
        "The word \" end \" has probability 0.883765074432 of being unpopular\n",
        "The word \" id \" has probability 0.883765074432 of being unpopular\n"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concepts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see here that perhaps just using the titles might not give us enough information to accurately ascertain what the posts are about."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "subreddit = 'explainlikeimfive'\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")\n",
      "\n",
      "topids = list(df['id'])\n",
      "\n",
      "endat = 150\n",
      "commentlen = []\n",
      "#for topid in topids:\n",
      "for i in range(endat):\n",
      "    try:\n",
      "        commentlen.append(len(list(get_comments(subreddit, topids[i], 'top', user_agent)['comment'])[0]))\n",
      "    except:\n",
      "        print topids[i]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15u5d0\n",
        "s8l4e"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids.index('15u5d0')\n",
      "topids.index('s8l4e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "122"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(commentlen)\n",
      "\n",
      "topscores = list(df['score'])[:50] + list(df['score'])[51:122] + list(df['score'])[123:endat]\n",
      "topmetrics = list(df['mymetric'])[:50] + list(df['mymetric'])[51:122] + list(df['mymetric'])[123:endat]\n",
      "topups = list(df['upvotes'])[:50] + list(df['upvotes'])[51:122] + list(df['upvotes'])[123:endat]\n",
      "topcont = list(df['up/down'])[:50] + list(df['up/down'])[51:122] + list(df['up/down'])[123:endat]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "148\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There appears to be some minor correlation between the length of the top comment and popularity of the post, but it appears like this effect is on the decline over time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.stats import pearsonr\n",
      "\n",
      "print pearsonr(commentlen, topscores)\n",
      "print pearsonr(commentlen, topmetrics)\n",
      "print pearsonr(commentlen, topups)\n",
      "print pearsonr(commentlen, topcont)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.44383859210747884, 1.6035273359197743e-08)\n",
        "(0.44319470358692481, 1.6912587281428895e-08)\n",
        "(0.25094344719532163, 0.0020954876387472149)\n",
        "(0.041284452559721747, 0.61834237353945753)\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#following code will create a counter dictionary will all keywords in titles for a single subreddit\n",
      "\n",
      "def get_keywords(subreddit):\n",
      "    returndict = Counter()\n",
      "    for a in subreddit.title:\n",
      "        keywords = p.run_method(a, 'keywords')\n",
      "        if keywords:\n",
      "            for b in keywords:\n",
      "                returndict[b[1]] = returndict[b[1]] + 1\n",
      "    return returndict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_keywords = get_keywords(pd.read_csv(csvfiles[13], encoding='utf-8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-38-d70f96484ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-35-20ccc0388b59>\u001b[0m in \u001b[0;36mget_keywords\u001b[1;34m(subreddit)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreturndict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keywords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/j/Dropbox/College/Data Science/Project/intelligent-dolphins/myalchemy.py\u001b[0m in \u001b[0;36mrun_method\u001b[1;34m(self, comment, whichtoget, otherparams)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0malchemybase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__look_at\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhichtoget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlencode\u001b[1;34m(query, doseq)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}