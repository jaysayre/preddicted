{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import pandas as pd\n",
      "import matplotlib.pyplot as plt\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "from collections import Counter\n",
      "\n",
      "def remove_border(axes=None, top=False, right=False, left=True, bottom=True):\n",
      "    \"\"\"\n",
      "    Minimize chartjunk by stripping out unnecessary plot borders and axis ticks\n",
      "    \n",
      "    The top/right/left/bottom keywords toggle whether the corresponding plot border is drawn\n",
      "    \"\"\"\n",
      "    ax = axes or plt.gca()\n",
      "    ax.spines['top'].set_visible(top)\n",
      "    ax.spines['right'].set_visible(right)\n",
      "    ax.spines['left'].set_visible(left)\n",
      "    ax.spines['bottom'].set_visible(bottom)\n",
      "    \n",
      "    #turn off all ticks\n",
      "    ax.yaxis.set_ticks_position('none')\n",
      "    ax.xaxis.set_ticks_position('none')\n",
      "    \n",
      "    #now re-enable visibles\n",
      "    if top:\n",
      "        ax.xaxis.tick_top()\n",
      "    if bottom:\n",
      "        ax.xaxis.tick_bottom()\n",
      "    if left:\n",
      "        ax.yaxis.tick_left()\n",
      "    if right:\n",
      "        ax.yaxis.tick_right()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 108
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\" #Fill in your own\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "bigcsv = csvfiles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(bigcsv, encoding='utf-8') # Top all is our training data set\n",
      "print len(df)\n",
      "df['up/down'] = df['upvotes'].astype(float)/df['downvotes'].astype(float) # Reddit fuzzes this so... "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "26948\n"
       ]
      }
     ],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topcomments=float(max(df['comments']))\n",
      "topsscore=float(max(df['score']))\n",
      "leastcontro = max(df['up/down'])\n",
      "# This needs to be improved. Sticking with it simply for testing purposes\n",
      "df['mymetric'] = (((df['comments'].astype(float)/topcomments)*0.10)+((df['score'].astype(float)/topsscore)*0.85)+((df['up/down']/leastcontro)*0.05))**(0.30)\n",
      "df['nrmscore'] = (df['score'].astype(float)/topsscore)**(0.30)\n",
      "bigdf = df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = df[df['subreddit'] == 'AskReddit']\n",
      "df2 = df[df['type'] == 'top_week']\n",
      "df = df[df['type'] == 'top_all']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So a few things about the way Reddit generates its metrics. First, I highly encourage that you read [this article on how Reddit ranks posts](http://amix.dk/blog/post/19588). Second, Reddit \"fuzzes\" the upvotes and downvotes so spambots can't manipulate the forum easily, so while the score is accurate, the number of upvotes and downvotes is not. For reference, $score = upvotes - downvotes$. Therefore, it must be that reddit adds/subtracts some unknown constant $k$ to the number of upvotes and downvotes.\n",
      "\n",
      "Currently, I've simply computed up/down as a measure of whether or not a post is controversial, but mathematically we may want to talk about methods to try to normalize this figure (if such a method exists).\n",
      "\n",
      "Third, I've found the actual paper that the Stanford researchers produced, and [it's worth a read over](http://i.stanford.edu/~julian/pdfs/icwsm13.pdf)."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#It's important in cross validation that the sets are disjoint, so we are removing duplicates\n",
      "dfids = list(df['id'])\n",
      "df2ids = list(df2['id'])\n",
      "\n",
      "dupids = []\n",
      "for redditid in dfids:\n",
      "    if redditid in df2ids:\n",
      "        dupids.append(redditid)\n",
      "\n",
      "#This part is slightly overengineered, but the motivation behind it is that we didn't want to simply strip out the \n",
      "#posts from other data set at will. Instead, we are splitting the duplicates in half and assigning them to one of the data sets\n",
      "#to avoid some sort of possible bias.\n",
      "if len(dupids)%2 != 0:\n",
      "    a = len(dupids)/2\n",
      "    a = a+1\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "else: \n",
      "    a = len(dupids)/2\n",
      "    dup1 = dupids[0:a]\n",
      "    dup2 = dupids[a:]\n",
      "    \n",
      "if np.random.randint(2) == 0:\n",
      "    df=df[df['id'].apply(lambda x: x in dup1) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup2) == False]\n",
      "else: \n",
      "    df=df[df['id'].apply(lambda x: x in dup2) == False]\n",
      "    df2=df2[df2['id'].apply(lambda x: x in dup1) == False]\n",
      "\n",
      "#df['mymetric'] = df['score'] + (df['comments'].astype(float)/2)*0.30 - ((df['up/down']/20)*(0.30))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df.head(10)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>author</th>\n",
        "      <th>comments</th>\n",
        "      <th>downvotes</th>\n",
        "      <th>id</th>\n",
        "      <th>karma</th>\n",
        "      <th>link_karma</th>\n",
        "      <th>score</th>\n",
        "      <th>selftext</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>title</th>\n",
        "      <th>type</th>\n",
        "      <th>upvotes</th>\n",
        "      <th>up/down</th>\n",
        "      <th>mymetric</th>\n",
        "      <th>nrmscore</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0  </th>\n",
        "      <td>             lieface</td>\n",
        "      <td>  8435</td>\n",
        "      <td>  5998</td>\n",
        "      <td> zzz5p</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 1989</td>\n",
        "      <td> For me its that Im happyI laugh and joke and s...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>               Whats a huge lie you tell everyday </td>\n",
        "      <td> top_all</td>\n",
        "      <td>  7987</td>\n",
        "      <td> 1.331611</td>\n",
        "      <td> 0.565861</td>\n",
        "      <td> 0.570535</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>10 </th>\n",
        "      <td>           Yossarian</td>\n",
        "      <td>  3615</td>\n",
        "      <td>  6196</td>\n",
        "      <td> zyyjj</td>\n",
        "      <td>     0</td>\n",
        "      <td>     0</td>\n",
        "      <td> 2298</td>\n",
        "      <td> EDIT Thank you Now that Im up here Id like to ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> What is the most genuinely useful subreddit yo...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  8494</td>\n",
        "      <td> 1.370884</td>\n",
        "      <td> 0.576411</td>\n",
        "      <td> 0.595795</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>53 </th>\n",
        "      <td>          kiltedfrog</td>\n",
        "      <td>  9066</td>\n",
        "      <td> 12638</td>\n",
        "      <td> zrotp</td>\n",
        "      <td>  2363</td>\n",
        "      <td>    58</td>\n",
        "      <td> 2115</td>\n",
        "      <td> Two drums and a cymbal fall off a cliffDuh dum...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>                Whats the best clean joke you know</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 14753</td>\n",
        "      <td> 1.167352</td>\n",
        "      <td> 0.576621</td>\n",
        "      <td> 0.581146</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>65 </th>\n",
        "      <td>    BitBulletBarrage</td>\n",
        "      <td>  2581</td>\n",
        "      <td> 22187</td>\n",
        "      <td> zpf3j</td>\n",
        "      <td>    55</td>\n",
        "      <td>     1</td>\n",
        "      <td> 1969</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Reddit why is there not a button on the side o...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 24156</td>\n",
        "      <td> 1.088746</td>\n",
        "      <td> 0.548896</td>\n",
        "      <td> 0.568808</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>91 </th>\n",
        "      <td> browneisthenewblack</td>\n",
        "      <td>  2740</td>\n",
        "      <td> 14983</td>\n",
        "      <td> zkln1</td>\n",
        "      <td>    63</td>\n",
        "      <td>    95</td>\n",
        "      <td> 2397</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Why is Ziploc yet to partner with cereal compa...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 17380</td>\n",
        "      <td> 1.159981</td>\n",
        "      <td> 0.581301</td>\n",
        "      <td> 0.603382</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>121</th>\n",
        "      <td>             mizuhri</td>\n",
        "      <td> 16759</td>\n",
        "      <td> 10950</td>\n",
        "      <td> zdzdj</td>\n",
        "      <td> 16707</td>\n",
        "      <td>  4128</td>\n",
        "      <td> 2021</td>\n",
        "      <td>                                               NaN</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td>    The year is  what doesnt exist anymore and why</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 12971</td>\n",
        "      <td> 1.184566</td>\n",
        "      <td> 0.588356</td>\n",
        "      <td> 0.573273</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>125</th>\n",
        "      <td>   steelpantherrocks</td>\n",
        "      <td>  4197</td>\n",
        "      <td>  8483</td>\n",
        "      <td> zcp8d</td>\n",
        "      <td>   627</td>\n",
        "      <td> 13919</td>\n",
        "      <td> 2374</td>\n",
        "      <td>  hours after original post with a little more ...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Not only did Geico not save me  or more in  mi...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 10857</td>\n",
        "      <td> 1.279854</td>\n",
        "      <td> 0.583164</td>\n",
        "      <td> 0.601639</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>150</th>\n",
        "      <td>         alexiagrace</td>\n",
        "      <td>  2582</td>\n",
        "      <td>  9230</td>\n",
        "      <td> z80hb</td>\n",
        "      <td>  2336</td>\n",
        "      <td> 13129</td>\n",
        "      <td> 2641</td>\n",
        "      <td> Edit Whoa I went to sleep and when I woke up t...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Someone recently told me The way you talk to y...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 11871</td>\n",
        "      <td> 1.286132</td>\n",
        "      <td> 0.597478</td>\n",
        "      <td> 0.621187</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>174</th>\n",
        "      <td>              ananci</td>\n",
        "      <td>  2171</td>\n",
        "      <td> 14329</td>\n",
        "      <td> z2rhj</td>\n",
        "      <td>  3134</td>\n",
        "      <td>   191</td>\n",
        "      <td> 2160</td>\n",
        "      <td> Just a bit under a year ago the a good portion...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Just a few months ago Reddit was boycotting Go...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 16489</td>\n",
        "      <td> 1.150743</td>\n",
        "      <td> 0.562659</td>\n",
        "      <td> 0.584828</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>181</th>\n",
        "      <td>            damarust</td>\n",
        "      <td>   541</td>\n",
        "      <td> 12285</td>\n",
        "      <td> z177g</td>\n",
        "      <td>  1968</td>\n",
        "      <td>  3566</td>\n",
        "      <td> 3248</td>\n",
        "      <td> I just thought about it You could choose the d...</td>\n",
        "      <td> AskReddit</td>\n",
        "      <td> Would Reddit want a flashback feature added to...</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 15533</td>\n",
        "      <td> 1.264387</td>\n",
        "      <td> 0.630580</td>\n",
        "      <td> 0.660962</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 7,
       "text": [
        "                  author  comments  downvotes     id  karma  link_karma  \\\n",
        "0                lieface      8435       5998  zzz5p      0           0   \n",
        "10             Yossarian      3615       6196  zyyjj      0           0   \n",
        "53            kiltedfrog      9066      12638  zrotp   2363          58   \n",
        "65      BitBulletBarrage      2581      22187  zpf3j     55           1   \n",
        "91   browneisthenewblack      2740      14983  zkln1     63          95   \n",
        "121              mizuhri     16759      10950  zdzdj  16707        4128   \n",
        "125    steelpantherrocks      4197       8483  zcp8d    627       13919   \n",
        "150          alexiagrace      2582       9230  z80hb   2336       13129   \n",
        "174               ananci      2171      14329  z2rhj   3134         191   \n",
        "181             damarust       541      12285  z177g   1968        3566   \n",
        "\n",
        "     score                                           selftext  subreddit  \\\n",
        "0     1989  For me its that Im happyI laugh and joke and s...  AskReddit   \n",
        "10    2298  EDIT Thank you Now that Im up here Id like to ...  AskReddit   \n",
        "53    2115  Two drums and a cymbal fall off a cliffDuh dum...  AskReddit   \n",
        "65    1969                                                NaN  AskReddit   \n",
        "91    2397                                                NaN  AskReddit   \n",
        "121   2021                                                NaN  AskReddit   \n",
        "125   2374   hours after original post with a little more ...  AskReddit   \n",
        "150   2641  Edit Whoa I went to sleep and when I woke up t...  AskReddit   \n",
        "174   2160  Just a bit under a year ago the a good portion...  AskReddit   \n",
        "181   3248  I just thought about it You could choose the d...  AskReddit   \n",
        "\n",
        "                                                 title     type  upvotes  \\\n",
        "0                  Whats a huge lie you tell everyday   top_all     7987   \n",
        "10   What is the most genuinely useful subreddit yo...  top_all     8494   \n",
        "53                  Whats the best clean joke you know  top_all    14753   \n",
        "65   Reddit why is there not a button on the side o...  top_all    24156   \n",
        "91   Why is Ziploc yet to partner with cereal compa...  top_all    17380   \n",
        "121     The year is  what doesnt exist anymore and why  top_all    12971   \n",
        "125  Not only did Geico not save me  or more in  mi...  top_all    10857   \n",
        "150  Someone recently told me The way you talk to y...  top_all    11871   \n",
        "174  Just a few months ago Reddit was boycotting Go...  top_all    16489   \n",
        "181  Would Reddit want a flashback feature added to...  top_all    15533   \n",
        "\n",
        "      up/down  mymetric  nrmscore  \n",
        "0    1.331611  0.565861  0.570535  \n",
        "10   1.370884  0.576411  0.595795  \n",
        "53   1.167352  0.576621  0.581146  \n",
        "65   1.088746  0.548896  0.568808  \n",
        "91   1.159981  0.581301  0.603382  \n",
        "121  1.184566  0.588356  0.573273  \n",
        "125  1.279854  0.583164  0.601639  \n",
        "150  1.286132  0.597478  0.621187  \n",
        "174  1.150743  0.562659  0.584828  \n",
        "181  1.264387  0.630580  0.660962  "
       ]
      }
     ],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from sklearn.feature_extraction.text import CountVectorizer\n",
      "from sklearn.cross_validation import train_test_split\n",
      "from sklearn.naive_bayes import MultinomialNB"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "title = list(df['title']) + list(df2['title'])\n",
      "vectorizer.fit(title)\n",
      "\n",
      "def category(x, df, num=2):\n",
      "    size = len(df)\n",
      "    blocksize = size/num\n",
      "    for i in range(num):\n",
      "        blockmax = max(sorted(df['score'])[blocksize*i:blocksize*(i+1)])        \n",
      "        if x < blockmax:\n",
      "            return i+1\n",
      "    return num\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "#X = vectorizer.transform(title)\n",
      "#Y = np.array(scores)\n",
      "x_train = vectorizer.transform(df['title'])\n",
      "x_test = vectorizer.transform(df2['title'])\n",
      "score = [category(i, df2) for i in df['score']]\n",
      "score2 = [category(i, df2) for i in df2['score']]\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training accuracy is 1.0\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test accuracy is 0.517694641052\n"
       ]
      }
     ],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#words = vectorizer2.get_feature_names()\n",
      "#words\n",
      "#diag_words = np.eye(len(words))\n",
      "#probword = pd.DataFrame(clf.predict_proba(diag_words))\n",
      "#probword.rename(columns={0: 'rotten', 1: 'fresh'}, inplace=True)\n",
      "#probword['words'] = words\n",
      "#print \"Top 10 Rotten words are\"\n",
      "#probword.sort([3], ascending=False)\n",
      "#probword['words'].head(20)\n",
      "\n",
      "#print \"\\n Top 10 Fresh words are\"\n",
      "#print probword.sort(['fresh'], ascending=False)[0:10]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "So while I have quite figured out how to get this to work properly, it's pretty obvious from doing a quick Naive Bayes fit that this method isn't going that work, regardless of what parameters we pick -- I don't really know though so you should check me on this. It could work-- idk. Alternately, could just be this subreddit.\n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "#apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 37
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from myalchemy import MyAlchemy\n",
      "p = MyAlchemy(apikey)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "dftitles = list(df['title'])\n",
      "df2titles = list(df2['title'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 16
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print dftitles[5]\n",
      "print p.run_method(dftitles[5], 'concepts')\n",
      "print p.run_method(dftitles[5], 'keywords')\n",
      "print p.run_method(dftitles[5], 'category')\n",
      "#print p.run_method(dftitles[5], 'sentiment')\n",
      "print p.run_method(dftitles[5], 'entities')\n",
      "print len(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "The year is  what doesnt exist anymore and why\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "(u'unknown', u'english', u'0.400001', u'OK')"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "[]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "981\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Below is the testing I did for building a model off of concepts. I tried everything from concepts to categories to entities, comparing weekly and all time, even just comparing All Time and it yielded little to no results. Even with concepts, which are far more broad than keywords, there is just way too much difference between posts and no specific concept makes anything more or less popular, so there are no trends of any statistical significance. Additionally, Alchemy only allows for 1000 calls per day which is a massive limitation (I pretty much used up all of our 3,000 limit across 3 accounts just for testing) so running it on any even decently large data set would be unweildy - it is both slow and yields too few results (for free) for us to really do anything. So here again, it looks like we'll have to abandon ship with trying to bag-of-words this model.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "categories = []\n",
      "concepts3, concepts24 = [], []\n",
      "for i in range(30):\n",
      "    conceptlist = p.run_method(dftitles[i], 'concepts')\n",
      "    for c in conceptlist:\n",
      "        concepts3.append(c[1])\n",
      "        \n",
      "for i in range(30):\n",
      "    conceptlist = p.run_method(df2titles[i], 'concepts')\n",
      "    for c in conceptlist:\n",
      "        concepts24.append(c[1])\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "KeyError",
       "evalue": "'concepts'",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-20-03b96e6d8c74>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mconcepts\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconcepts2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftitles\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mconceptlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdftitles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'concepts'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mconceptlist\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m         \u001b[0mconcepts\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sergsb\\Documents\\School\\Year 2\\Tareas\\CS109\\intelligent-dolphins\\myalchemy.pyc\u001b[0m in \u001b[0;36mrun_method\u001b[1;34m(self, comment, whichtoget, otherparams)\u001b[0m\n\u001b[0;32m     82\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_category\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_concepts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m4\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtext_sentiment\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32mC:\\Users\\sergsb\\Documents\\School\\Year 2\\Tareas\\CS109\\intelligent-dolphins\\myalchemy.pyc\u001b[0m in \u001b[0;36mtext_concepts\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     20\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mtext_concepts\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0mreturnlist\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 22\u001b[1;33m         \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'concepts'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     23\u001b[0m             \u001b[1;31m#This is a website with information for the text - not in our model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     24\u001b[0m             \u001b[1;31m#print item['dbpedia']\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mKeyError\u001b[0m: 'concepts'"
       ]
      }
     ],
     "prompt_number": 20
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print concepts\n",
      "print \"--------\"\n",
      "print concepts2"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'2006 albums', u'Corporation', u'Containers', u'E-mail', u'E-mail address', u'2007 singles', u'Go Daddy', u'Digg', u'Plane', u'Sex offender', u'Media technology', u'Internet', u'History of the Internet', u'World Wide Web', u'Freedom of information', u'Breast cancer', u'Cancer', u'Metastasis', u'National Breast Cancer Awareness Month', u'Pleural effusion', u'Conspiracy theory', u'Manchester', u'The Cult', u'Marilyn Manson', u'Phantasmagoria: The Visions of Lewis Carroll', u'Marilyn Manson', u'Charles Manson', u'Alex Shelley', u'Want', u'Need', u'WANT', u'I Survived...', u'Lebanon, Tennessee', u'Economics terminology', u'Robin Beck', u'Surface', u'Whistleblower', u'Caribbean Sea', u'Caribbean', u'Barbados', u'Hell', u'What Happened', u'Credit history', u'Ayumi Hamasaki', u'2007 albums', u'Education', u'Digg', u'Help me', u'Help', u'Internet', u'Internet', u'History of the Internet', u'World Wide Web', u'Freedom of information', u'Westboro Baptist Church', u'Picketing', u'Major League Soccer', u'The Red Chord', u'Coming out', u'English-language films', u'Suicide', u'American alternative rock groups', u'2008 MTV Video Music Awards', u'Han Chinese', u'Qing Dynasty', u'Zhonghua minzu', u\"People's Republic of China\", u'Liaoning', u'Xinjiang', u'Chinese nationalism', u'Thought', u'Wedding reception', u'Wedding', u'American record labels', u'Black-and-white films', u'Italian films', u'English-language films', u'Liv Tyler', u'1990 singles', u'Knowledge', u'David Kellogg Lewis', u'Cognition', u'Philosophy', u'English-language films', u'2006 albums', u'List', u'Lists', u'John Carpenter', u'English-language films', u'Bertrand Russell', u'Artificial intelligence', u'Philosophy', u'Truth', u'Reality', u'Reason', u'Mika Nakashima', u'Entropy', u'Mother goddess', u'Greatest hits albums', u'Saw', u'Hearing impairment', u'Audiogram', u'Cochlea', u'Hearing', u'Tinnitus', u'The Favor', u'Deaf culture', u'English-language films', u'Critical thinking', u'Black-and-white films', u'Italian films', u'Power pop', u'Life', u'Time', u'Blue Bird', u'Henry Luce', u'Ayumi Hamasaki', u'Superboy', u'Want', u'Need', u'WANT', u'Lebanon, Tennessee', u'Economics terminology', u'Easter', u'Easter egg', u'Sibling', u'Birthday', u'Parent', u'Party', u'ARIA Charts', u'Iggy Pop', u'Family', u'Dr. Luke', u'Woman', u'Marriage', u'Alimony', u'Monogamy', u'Annulment', u'Divorce', u'Family law', u'Search engine optimization', u'Knowledge', u\"April Fools' Day\", u'Practical joke', u'Fools Guild', u'Audi', u'2006 albums', u'Guy', u'The Point', u'Bill Gates', u'Work', u'World Wide Web', u'Energy', u'Science', u'Experiment', u'Sample', u'Sample size', u'Sampling', u'The Front Page', u'Wii Play', u'Performing arts', u'Punk rock', u'Fish', u'Internet censorship', u'Internet', u'2006 in music', u'2006 albums', u'Government', u'Sovereign state', u'YouTube', u'Prince', u'Barack Obama', u'Every time you masturbate... God kills a kitten', u'Time', u'The Matrix', u'The Matrix Reloaded', u'The Matrix Revolutions', u'List', u'Lists', u'Laughter', u'Nicktoons', u'Fearless', u'2008 albums', u'Bill Clinton', u'President of the United States', u'George W. Bush', u'George H. W. Bush', u'White House', u'Democratic Party', u'United States Congress', u'Ross Perot', u'Roscoe Orman', u'Sesame Street', u'Sesame Workshop', u'Cond\\xe9 Nast Publications', u'Idea', u'Cond\\xe9 Montrose Nast', u'Credit card', u'TED', u'Renting', u'The Animals', u'United States Marine Corps', u'Battalion', u'Augustus', u'Royal Marines', u'Praetorian Guard', u'Roman Empire', u'Brigade', u'Marine', u'Television', u'2007 albums', u'Understanding', u'United States public debt', u'Food', u'Vehicle', u'Ageing', u'Prostitution', u'Police', u'Janet Jackson', u'Bin Laden family', u'Fast food', u'Maize', u'Fast food restaurant', u'Taxation in the United States', u'Tax', u'Tax law', u'Internal Revenue Service', u'Tax refund', u'Cond\\xe9 Nast Publications', u'2008 singles', u'Association football goalkeepers', u'Help me', u'Cond\\xe9 Nast Publications', u'Thought', u'Elections', u'Elections in the United States', u'English-language films', u'Television', u'Hypertension', u'Westboro Baptist Church', u'American magazines', u'Reality', u'Aerosmith', u'World Wide Web', u'Internet', u'Website', u'Hypertext Transfer Protocol', u'Web page', u'Web design', u'Google Chrome', u'Google', u'Google Chrome OS', u'Web browser', u'Gmail', u'Celebrity', u'Paddy Chayefsky', u'George C. Scott', u'Diana Rigg', u'No One Knows', u'Academic degree', u'Doctorate', u'Higher education', u'University', u'Madrasah', u'Diploma', u'Faculty', u'2006 albums', u'2006 albums', u'Robin Beck', u'Surface', u'Fuck', u'Profanity', u'Alien abduction', u'Entertainment', u'The Luckiest', u'Death', u'1970s music groups', u'Puberty', u'Form of the Good', u'Bad', u'History of the world', u'Ibn Khaldun', u'Modulor', u'Easter', u'Easter egg', u'English-language films', u'2006 albums', u'One Piece', u'Scariest Places on Earth', u'Debut albums', u'2001 albums', u'Categorization', u'Ontology', u'The Dark Side of the Moon', u'Jeff Lynne', u'The Funniest Joke in the World', u'Joke', u'Jokes', u'ILOVEYOU', u'Henri Bergson', u'World War II', u'Germany', u'Shaun the Sheep', u'United States', u'Cook', u'2005 albums', u'What Happened', u'Exit', u'Fork', u'England', u'2009 albums', u'Commander-in-chief', u'Military', u'Military of the United States', u'Economics', u'Marriage', u'Education', u'English-language films', u'Poker', u'John Lennon', u'Morality', u'Science', u'Heshen', u'Unexplained Mysteries', u'Native Americans in the United States', u'Animal Collective', u'1995 singles', u'Horror and terror', u'Horror film', u'Speculative fiction', u'Live television', u'African American rappers', u'Publications established in 2005', u'2006 albums', u'The Road', u'Boy', u'Girl', u'Vector space', u'Heather Small', u'Nutrition', u'Human', u'Time', u'Monte Moir', u'Morris Day', u'Human', u'Race', u'Competition', u'Kumi Koda', u'Blue Bird', u'Ayumi Hamasaki', u'Superboy', u'High school', u'Education', u'Teacher', u'High School Musical', u'University', u'Education', u'Logic', u'Absolute', u'2006 albums', u'National Film Registry', u'English-language films', u'Guy Chambers', u'2006 albums', u'2009 singles', u'Black-and-white films', u'Pornography', u'Erotica', u'Doing It', u'2006 albums', u'Fascination Records', u'2006 albums', u'United States public debt', u'Philosophy of science', u'Illegal drug trade', u'Drug', u'Reality', u'Reality', u'Future', u'Song', u'Question', u'Interrogative word', u'Human', u'Wife', u'Family', u'Space exploration', u'Outer space', u'Wernher von Braun', u'NASA', u'TED', u'Biology', u'Humans', u'Meaning of life', u'Haunted house', u'Rudeness', u'Deviance', u'2001 albums', u'English-language films', u'United States', u'Federal government of the United States', u'Word', u'Phrase', u'Sentence', u'English-language films', u'2006 albums', u'Human body', u'Human anatomy', u'2006 albums', u'Reality', u'Real life', u'Social constructionism', u'Meatspace', u'Coma', u'Public library', u'Society', u'Taste', u'Conspiracy theory', u'Conspiracy?', u'Paddy Chayefsky', u'George C. Scott', u'Diana Rigg', u'Internet', u'History of the Internet', u'World Wide Web', u'Freedom of information', u'Reality', u'Real life', u'Social constructionism', u'Meatspace', u'English-language films', u'World', u'Earth', u'Internet pornography', u'Internet', u'Gerontology', u'Ageing', u'Old age', u'Restaurant', u'Fast food', u'Menu', u'YouTube', u'Language', u'Debut albums', u'2001 albums', u'Television', u'Television program', u'Reality television', u'Television network', u'Physician', u'Cognition', u'Concepts in metaphysics', u'Cognition', u'Parent', u'Western culture', u'Russia', u'Biology', u'Telephony', u'2006 albums', u'Mobile phone', u'Voice over Internet Protocol', u'The Point', u'Instant messaging', u'Victimless crime', u'Crime', u'Academic department', u'Image search', u'Google Images', u'Google services', u'Searching', u'Boy', u'Conspiracy theory', u'Conspiracy?', u'Art', u'Fine art', u'Education', u'Teacher', u'Entertainment', u'Immigration to the United States', u'Hispanic and Latino Americans', u'Demographics of the United States', u'United States Constitution', u'Birthright citizenship in the United States of America', u'Race and ethnicity in the United States Census', u'Table of United States Metropolitan Statistical Areas', u'Concepts in metaphysics', u'Human body', u'Psychology', u'Ontology', u'Human anatomy', u'Plot', u'2002 albums', u'Defunct video game companies', u'Brian Wilson', u'Rick Rubin', u'Atmosphere', u'Planet', u'Evolution', u'Human', u'Pluto', u'Billboard Hot Country Songs number-one singles', u'Consciousness', u'Brad Pitt', u'Color', u'Dye', u'Death', u'Pulp magazine', u'Pulp magazines', u'Bandwagon effect', u'Fads and trends', u'Celebrity', u'Future', u'Google search', u'Greatest hits albums', u'1998 albums', u'World', u'Asia', u'Europe', u'Accept', u'North Korea', u'Pacific Ocean', u'Korea', u'South Korea', u'2000 albums', u'John Carpenter', u'What Happened', u'Ghostzapper', u'Family', u'Dissociative identity disorder', u'Source code', u'Computer software', u'Personal computer', u'Melody Maker', u'2007 albums', u'1995 albums', u'Medicine', u'Sacrament', u'Love', u'World', u'Earth', u'Logic', u'Healthcare reform', u'Medicine', u'United States', u'Public health', u'Health economics', u'Data modeling', u'Grunge', u'Corporation', u'Ireland', u'A Lifetime', u'Kissing to Be Clever', u'Do You Really Want to Hurt Me', u'2006 albums', u'Sociology', u'Debut albums', u'High school', u'High School Musical', u'Question', u'2008 albums', u'2006 albums', u'Robert Nozick', u'Silver Surfer', u'Beyonder', u'Cognition', u'Milan Kundera', u'Neolithic', u'Prehistory', u'Mother', u'Father', u'Queen', u'Brian May', u'Meat', u'Nutrition', u'In vitro meat', u'2007 albums', u'1995 albums', u'Immigration law', u'Motion Picture Association of America film rating system', u'Writing', u'Creative writing', u'Life', u'Time', u'Time', u'Monte Moir', u'Morris Day', u'Unsolved Mysteries', u'Garry Bushell', u'English-language films', u'University', u'Professor', u'English-language films', u'Flag of the United States', u'Personal computer', u'Yolanda Adams', u'Believe', u'Robin Beck', u'Surface', u'2005 albums', u'Ilmar Raag', u'Time', u'Monte Moir', u'Time', u'Morris Day', u'Horror fiction', u'Urban legend', u'Folklore', u'Secrecy', u'Taxi Driver', u'English-language films', u'Lingerie', u\"Victoria's Secret\", u'Film criticism', u'2006 albums', u'Vocal duets', u'Mammal', u'Evolution', u'Debut albums', u'English-language films', u'Psychology', u'Psychology', u'Thought', u'Fuck', u'Sociology', u'Google Earth', u'A Story', u'Warner Bros. Records albums', u'1983 albums', u'Fascination Records', u'Family', u'Brain', u'Human body', u'Human anatomy', u'Horror film', u'Significant Other', u'Television', u'Television program', u'Reality television', u'Television network', u\"Somebody Else's Problem\", u'2007 singles', u'Greatest hits albums', u'English language', u'Phonology', u'Vowel', u'General American', u'United States', u'Country music', u'American', u'Old-time music', u'Indian reservation', u'Rock and roll', u'Humid subtropical climate', u'Bird', u'Digg', u'Government', u'Sovereign state', u'Funeral', u'Paul Lynde', u'Thought', u'Comedy', u'Thought', u'Human', u'Religion', u'Meaning of life', u'World', u'Earth', u'Property', u'Topological space', u'A Little Bit', u'A Little Bit Longer', u'Bad', u'Tynwald', u'2006 albums', u'Love', u'The Lettermen', u'Word', u'Grammatical conjugation', u'Mother', u'Greek loanwords', u'Opera', u'Wolfgang Amadeus Mozart', u'Musical notation', u'Counterpoint', u'Musical improvisation', u'Baroque music', u'College or university school of music', u'Classical music', u'Cognition', u'Prison', u'Sentence', u'The Cleverest', u'Absolute', u'Coming of age', u'Robin Beck', u'Surface', u'1987 singles', u'The Best Party Ever', u'Question', u'Government', u'Sovereign state', u'History', u'1979 albums', u'Surveillance', u'Soul', u'Philosophy of mind', u'Ego', u'Comedy', u'Johnny Depp', u'Yolanda Adams', u'Believe', u'2005 albums', u'American magazines', u'Personal life', u'World', u'Earth', u'Seduction community', u'Seduction', u'The Front Page', u'Higher education', u'University', u\"Associate's degree\", u'Madrasah', u'Academic degree', u'Photography', u'Image', u'Reality', u'Multiplication', u'Sex doll', u'Blue Bird', u'Ayumi Hamasaki', u'Skill', u'English-language films', u'Racial segregation', u'Same-sex marriage', u'Racism', u'Sociology', u'Instant messaging', u'Scooter', u'Earth', u'Supercontinent', u'Africa', u'World', u'Pangaea', u'American films', u'Truth', u'Knowledge', u'Georg Wilhelm Friedrich Hegel', u'Science', u'Epistemology', u'Page', u'Mother', u'1995 albums', u'The Nature Conservancy', u'Reinhard Heydrich', u'Vocal music', u'Rock music', u'Love', u\"Can't Help Falling in Love\", u'Right-wing politics', u'Sex offender', u'Microsoft', u'Bing', u'Employment', u'Job interview', u'Recruitment', u'Family law', u'Character', u'Fiction', u'Novel', u'Characterisation', u'Human', u'Jupiter', u'Europa', u'Extraterrestrial life', u'Planet', u'Extrasolar planet', u'Mars', u'Science', u'Television program', u'Scariest Places on Earth', u'Wage', u'Island Records albums', u'Video game culture', u'Telepresence', u'A Story', u'Audi', u'1995 singles', u'Time', u'Religion', u'Germany', u'Cold War', u'Soviet Union', u'West Germany', u'East Berlin', u'East Germany', u'Willy Brandt', u'Communism', u'English-language films', u'Bad', u'English-language films', u'Unsolved Mysteries', u'Million', u'Melody Maker', u'2001 albums', u'The Animals', u\"Don't Let Me Be Misunderstood\", u'2009 albums', u'Scientific method', u'Research', u'Science', u'History of the Internet', u'Website', u'Internet', u'World Wide Web', u'Freedom of information', u'Believe', u'Time travel', u'General relativity', u'Arashi', u'Spacetime', u'Horror film', u'Film', u'Every time you masturbate... God kills a kitten', u'Ayumi Hamasaki', u'Days/Green', u'Place name disambiguation pages', u'Reader', u'Woman', u'Political spectrum', u'High school', u'College', u'Greatest hits albums', u'University', u'Student', u'Korea', u'World War II', u'United States', u'South Korea', u'United States', u'U.S. state', u'President of the United States', u'United States Marine Corps', u'United States Army', u'Joint Chiefs of Staff', u'United States Senate', u'Political philosophy', u'Nuclear Non-Proliferation Treaty', u'Nuclear weapon', u'North Korea and weapons of mass destruction', u'World War II', u'Nuclear disarmament', u'Cold War', u'Missile', u'North Korea', u'High school', u'High School Musical', u'2006 albums', u'Arista Records albums', u'Human body', u'Human anatomy', u'Profanity', u'Trigraph', u'Gh', u'History of education', u'BASIC', u'Truth or Dare?', u'Ocean', u'Draw-A-Person Test', u'Automobile', u'Customer service', u'Soviet Union', u'Vladimir Lenin', u'Gulag', u'Greatest hits albums', u'Historian', u'The Notorious B.I.G.', u'TED', u'2006 albums', u'2007 singles', u'Country ballads', u'Phantom Stranger', u'2006 albums', u'Unsolved Mysteries', u'Europe', u'World', u'Africa', u'Federal government of the United States', u'Earth', u'Butterfly', u'Future', u'The Road', u'Fast food', u'Menu', u'Restaurant', u'Take-out', u'Online game', u'Confession', u'Marketing', u'Nutrition', u'Aerosmith', u'2006 albums', u'Gender role', u'Childbirth', u'Paranormal', u'Occult', u'Esotericism', u'United States', u'President of the United States', u'Fuck', u'Profanity', u'Shit', u'George Carlin', u'Seven dirty words', u'U.S. state', u'Economic history', u'Austrian School', u'Sociology', u'Norm', u'Norm', u'Heteronormativity', u'2006 albums', u'Internet privacy', u'Internet', u'History of the Internet', u'World Wide Web', u'Freedom of information', u'Cognition', u'Amateur radio', u'Typeface', u'Blue Bird', u'Ayumi Hamasaki', u'2006 albums', u'Law', u'Internet', u'History of the Internet', u'World Wide Web', u'Freedom of information', u'Man']\n",
        "--------\n",
        "[]\n"
       ]
      }
     ],
     "prompt_number": 21
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "vectorizer.fit(concepts)\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "X = vectorizer.transform(concepts)\n",
      "Y = np.array(df['score'][0:938])\n",
      "'''\n",
      "x_train = vectorizer.transform(concepts)\n",
      "x_test = vectorizer.transform(concepts2)\n",
      "score = df['score']\n",
      "score2 = df2['score']\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 34,
       "text": [
        "\"\\nx_train = vectorizer.transform(concepts)\\nx_test = vectorizer.transform(concepts2)\\nscore = df['score']\\nscore2 = df2['score']\\ny_train = np.array(score)\\ny_test = np.array(score2)\\n\""
       ]
      }
     ],
     "prompt_number": 34
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(Y)\n",
      "x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)\n",
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "938\n",
        "Training accuracy is "
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "0.528784648188\n",
        "Test accuracy is 0.0\n"
       ]
      }
     ],
     "prompt_number": 36
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "**Below is what I believe to be our best chance of predicting popularity (based on above average ratings) for a weekly post.**"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "vectorizer = CountVectorizer(min_df=0.001)\n",
      "title = list(dftitles) + list(df2titles)\n",
      "vectorizer.fit(title)\n",
      "\n",
      "def category(x, df, num=2):\n",
      "    size = len(df)\n",
      "    blocksize = size/num\n",
      "    for i in range(num):\n",
      "        blockmax = max(sorted(df['score'])[blocksize*i:blocksize*(i+1)])        \n",
      "        if x < blockmax:\n",
      "            return i+1\n",
      "    return num\n",
      "\n",
      "#scores = [category(i) for i in df2['score']]\n",
      "#print scores\n",
      "#X = vectorizer.transform(title)\n",
      "#Y = np.array(scores)\n",
      "x_train = vectorizer.transform(dftitles)\n",
      "x_test = vectorizer.transform(df2titles)\n",
      "score = [1 if i > np.mean(df['score']) else 0 for i in df['score']]\n",
      "score2 = [1 if i > np.mean(df2['score']) else 0 for i in df2['score']]\n",
      "y_train = np.array(score)\n",
      "y_test = np.array(score2)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 87
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "vectorizer2 = CountVectorizer(min_df=0.001)\n",
      "title2 = df2['title']\n",
      "vectorizer2.fit(title2)\n",
      "X2 = vectorizer2.transform(title2)\n",
      "Y2 = np.array(df2['score'])\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 88,
       "text": [
        "\"\\nvectorizer2 = CountVectorizer(min_df=0.001)\\ntitle2 = df2['title']\\nvectorizer2.fit(title2)\\nX2 = vectorizer2.transform(title2)\\nY2 = np.array(df2['score'])\\n\""
       ]
      }
     ],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#x_train, x_test, y_train, y_test = train_test_split(X, Y, train_size=0.5) #I added the train size parameter.\n",
      "\n",
      "clf = MultinomialNB(alpha=1)\n",
      "clf.fit(x_train, y_train)\n",
      "print \"Training accuracy is\", clf.score(x_train, y_train)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Training accuracy is 0.851172273191\n"
       ]
      }
     ],
     "prompt_number": 89
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print \"Test accuracy is\", clf.score(x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Test accuracy is 0.756319514661\n"
       ]
      }
     ],
     "prompt_number": 90
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def calibration_plot(clf, X, Y):\n",
      "    #Number of bins and array of bin edges from 0 to .95\n",
      "    N = 20\n",
      "    bins = [float(i)/N for i in range(N)]\n",
      "    unpopular, popular = zip(*clf.predict_proba(X))\n",
      "    \n",
      "    #Use nmupy digitize to separate data into bins and make 2D array for bins\n",
      "    digitized = np.digitize(popular, bins)\n",
      "    results = [[] for x in xrange(N)] \n",
      "    for i in range(len(digitized)):\n",
      "        results[digitized[i]-1].append(Y[i])\n",
      "    \n",
      "    #Calculate the y error values\n",
      "    yr = []\n",
      "    for i in results:\n",
      "        total = len(i)\n",
      "        frac = np.mean(i)\n",
      "        err = ((frac*(1-frac))/total)**.5\n",
      "        yr.append(err)\n",
      "    \n",
      "    #Calculate the \"Fresh\" fractions and plot them\n",
      "    means = [np.mean(x) for x in results]\n",
      "    fig1=plt.figure()\n",
      "    #Line y=x\n",
      "    x = np.array(range(0,2))\n",
      "    #Format the plot\n",
      "    plt.plot(x,x, c='r', label='Expectation')\n",
      "    plt.errorbar(bins,means,yerr=yr, c='black', label='Model with error', ecolor='b')\n",
      "    plt.axes().set_title('Popularity Fraction vs P(Popular)')\n",
      "    plt.xlabel('Popularity Probability') \n",
      "    plt.ylabel('Fraction Popular')\n",
      "    plt.legend(loc = 0).draw_frame(False)\n",
      "    remove_border()\n",
      "    plt.show()\n",
      "    \n",
      "    #Use the fresh array to plot histogram\n",
      "    fig2=plt.figure()\n",
      "    n, b, p = plt.hist(popular, N, label='examples', edgecolor='black')\n",
      "    #Annotate the bins with appropriate values\n",
      "    for i in range(len(bins)):\n",
      "        plt.annotate(int(n[i]), (.005+float(i)/N,n[i]+20))\n",
      "    #Format the plot\n",
      "    plt.axes().set_title('Number of Examples per Bin')\n",
      "    plt.xlabel('Bins') \n",
      "    plt.ylabel('Examples')\n",
      "    plt.legend(loc = 0).draw_frame(False)\n",
      "    remove_border()\n",
      "    plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 111
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "calibration_plot(clf, x_test, y_test)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 114
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Get feature names and diagonalize them\n",
      "mywords = vectorizer.get_feature_names()\n",
      "diag = np.eye(len(mywords))\n",
      "\n",
      "#Calculate the rotten and fresh word probabilities \n",
      "#and create a new, sorted DataFrame for them\n",
      "unpopular, popular = zip(*clf.predict_proba(diag))\n",
      "data = pd.DataFrame({'words': mywords, 'punpopular': unpopular, 'ppopular': popular})\n",
      "sort = data.sort('ppopular', ascending=False).copy()\n",
      "\n",
      "print 'Top 10 \"Popular\" Words:'\n",
      "print\n",
      "for i in sort[:10].index:\n",
      "    print \"The word \\\"\",sort.words[i],\"\\\" has probability\", sort.ppopular[i], \"of being popular\"\n",
      "print\n",
      "print 'Top 10 \"Unpopular\" Words:'\n",
      "print\n",
      "for i in sort[:-11:-1].index:\n",
      "    print \"The word \\\"\",sort.words[i],\"\\\" has probability\", sort.punpopular[i], \"of being unpopular\""
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Top 10 \"Popular\" Words:\n",
        "\n",
        "The word \" watching \" has probability 0.847014874857 of being popular\n",
        "The word \" longer \" has probability 0.821868154368 of being popular\n",
        "The word \" ideas \" has probability 0.821868154368 of being popular\n",
        "The word \" item \" has probability 0.821868154368 of being popular\n",
        "The word \" update \" has probability 0.786828357489 of being popular\n",
        "The word \" lame \" has probability 0.786828357489 of being popular\n",
        "The word \" hidden \" has probability 0.786828357489 of being popular\n",
        "The word \" became \" has probability 0.786828357489 of being popular\n",
        "The word \" regret \" has probability 0.786828357489 of being popular\n",
        "The word \" visit \" has probability 0.786828357489 of being popular\n",
        "\n",
        "Top 10 \"Unpopular\" Words:\n",
        "\n",
        "The word \" yourself \" has probability 0.907005358568 of being unpopular\n",
        "The word \" each \" has probability 0.896583179655 of being unpopular\n",
        "The word \" very \" has probability 0.896583179655 of being unpopular\n",
        "The word \" ive \" has probability 0.896583179655 of being unpopular\n",
        "The word \" look \" has probability 0.896583179655 of being unpopular\n",
        "The word \" while \" has probability 0.890444053474 of being unpopular\n",
        "The word \" advice \" has probability 0.883530055509 of being unpopular\n",
        "The word \" around \" has probability 0.883530055509 of being unpopular\n",
        "The word \" mom \" has probability 0.883530055509 of being unpopular\n",
        "The word \" end \" has probability 0.883530055509 of being unpopular\n"
       ]
      }
     ],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "concepts"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 116,
       "text": [
        "[u'2006 albums',\n",
        " u'Corporation',\n",
        " u'Containers',\n",
        " u'E-mail',\n",
        " u'E-mail address',\n",
        " u'2007 singles',\n",
        " u'Go Daddy',\n",
        " u'Digg',\n",
        " u'Plane',\n",
        " u'Sex offender',\n",
        " u'Media technology',\n",
        " u'Internet',\n",
        " u'History of the Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Breast cancer',\n",
        " u'Cancer',\n",
        " u'Metastasis',\n",
        " u'National Breast Cancer Awareness Month',\n",
        " u'Pleural effusion',\n",
        " u'Conspiracy theory',\n",
        " u'Manchester',\n",
        " u'The Cult',\n",
        " u'Marilyn Manson',\n",
        " u'Phantasmagoria: The Visions of Lewis Carroll',\n",
        " u'Marilyn Manson',\n",
        " u'Charles Manson',\n",
        " u'Alex Shelley',\n",
        " u'Want',\n",
        " u'Need',\n",
        " u'WANT',\n",
        " u'I Survived...',\n",
        " u'Lebanon, Tennessee',\n",
        " u'Economics terminology',\n",
        " u'Robin Beck',\n",
        " u'Surface',\n",
        " u'Whistleblower',\n",
        " u'Caribbean Sea',\n",
        " u'Caribbean',\n",
        " u'Barbados',\n",
        " u'Hell',\n",
        " u'What Happened',\n",
        " u'Credit history',\n",
        " u'Ayumi Hamasaki',\n",
        " u'2007 albums',\n",
        " u'Education',\n",
        " u'Digg',\n",
        " u'Help me',\n",
        " u'Help',\n",
        " u'Internet',\n",
        " u'Internet',\n",
        " u'History of the Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Westboro Baptist Church',\n",
        " u'Picketing',\n",
        " u'Major League Soccer',\n",
        " u'The Red Chord',\n",
        " u'Coming out',\n",
        " u'English-language films',\n",
        " u'Suicide',\n",
        " u'American alternative rock groups',\n",
        " u'2008 MTV Video Music Awards',\n",
        " u'Han Chinese',\n",
        " u'Qing Dynasty',\n",
        " u'Zhonghua minzu',\n",
        " u\"People's Republic of China\",\n",
        " u'Liaoning',\n",
        " u'Xinjiang',\n",
        " u'Chinese nationalism',\n",
        " u'Thought',\n",
        " u'Wedding reception',\n",
        " u'Wedding',\n",
        " u'American record labels',\n",
        " u'Black-and-white films',\n",
        " u'Italian films',\n",
        " u'English-language films',\n",
        " u'Liv Tyler',\n",
        " u'1990 singles',\n",
        " u'Knowledge',\n",
        " u'David Kellogg Lewis',\n",
        " u'Cognition',\n",
        " u'Philosophy',\n",
        " u'English-language films',\n",
        " u'2006 albums',\n",
        " u'List',\n",
        " u'Lists',\n",
        " u'John Carpenter',\n",
        " u'English-language films',\n",
        " u'Bertrand Russell',\n",
        " u'Artificial intelligence',\n",
        " u'Philosophy',\n",
        " u'Truth',\n",
        " u'Reality',\n",
        " u'Reason',\n",
        " u'Mika Nakashima',\n",
        " u'Entropy',\n",
        " u'Mother goddess',\n",
        " u'Greatest hits albums',\n",
        " u'Saw',\n",
        " u'Hearing impairment',\n",
        " u'Audiogram',\n",
        " u'Cochlea',\n",
        " u'Hearing',\n",
        " u'Tinnitus',\n",
        " u'The Favor',\n",
        " u'Deaf culture',\n",
        " u'English-language films',\n",
        " u'Critical thinking',\n",
        " u'Black-and-white films',\n",
        " u'Italian films',\n",
        " u'Power pop',\n",
        " u'Life',\n",
        " u'Time',\n",
        " u'Blue Bird',\n",
        " u'Henry Luce',\n",
        " u'Ayumi Hamasaki',\n",
        " u'Superboy',\n",
        " u'Want',\n",
        " u'Need',\n",
        " u'WANT',\n",
        " u'Lebanon, Tennessee',\n",
        " u'Economics terminology',\n",
        " u'Easter',\n",
        " u'Easter egg',\n",
        " u'Sibling',\n",
        " u'Birthday',\n",
        " u'Parent',\n",
        " u'Party',\n",
        " u'ARIA Charts',\n",
        " u'Iggy Pop',\n",
        " u'Family',\n",
        " u'Dr. Luke',\n",
        " u'Woman',\n",
        " u'Marriage',\n",
        " u'Alimony',\n",
        " u'Monogamy',\n",
        " u'Annulment',\n",
        " u'Divorce',\n",
        " u'Family law',\n",
        " u'Search engine optimization',\n",
        " u'Knowledge',\n",
        " u\"April Fools' Day\",\n",
        " u'Practical joke',\n",
        " u'Fools Guild',\n",
        " u'Audi',\n",
        " u'2006 albums',\n",
        " u'Guy',\n",
        " u'The Point',\n",
        " u'Bill Gates',\n",
        " u'Work',\n",
        " u'World Wide Web',\n",
        " u'Energy',\n",
        " u'Science',\n",
        " u'Experiment',\n",
        " u'Sample',\n",
        " u'Sample size',\n",
        " u'Sampling',\n",
        " u'The Front Page',\n",
        " u'Wii Play',\n",
        " u'Performing arts',\n",
        " u'Punk rock',\n",
        " u'Fish',\n",
        " u'Internet censorship',\n",
        " u'Internet',\n",
        " u'2006 in music',\n",
        " u'2006 albums',\n",
        " u'Government',\n",
        " u'Sovereign state',\n",
        " u'YouTube',\n",
        " u'Prince',\n",
        " u'Barack Obama',\n",
        " u'Every time you masturbate... God kills a kitten',\n",
        " u'Time',\n",
        " u'The Matrix',\n",
        " u'The Matrix Reloaded',\n",
        " u'The Matrix Revolutions',\n",
        " u'List',\n",
        " u'Lists',\n",
        " u'Laughter',\n",
        " u'Nicktoons',\n",
        " u'Fearless',\n",
        " u'2008 albums',\n",
        " u'Bill Clinton',\n",
        " u'President of the United States',\n",
        " u'George W. Bush',\n",
        " u'George H. W. Bush',\n",
        " u'White House',\n",
        " u'Democratic Party',\n",
        " u'United States Congress',\n",
        " u'Ross Perot',\n",
        " u'Roscoe Orman',\n",
        " u'Sesame Street',\n",
        " u'Sesame Workshop',\n",
        " u'Cond\\xe9 Nast Publications',\n",
        " u'Idea',\n",
        " u'Cond\\xe9 Montrose Nast',\n",
        " u'Credit card',\n",
        " u'TED',\n",
        " u'Renting',\n",
        " u'The Animals',\n",
        " u'United States Marine Corps',\n",
        " u'Battalion',\n",
        " u'Augustus',\n",
        " u'Royal Marines',\n",
        " u'Praetorian Guard',\n",
        " u'Roman Empire',\n",
        " u'Brigade',\n",
        " u'Marine',\n",
        " u'Television',\n",
        " u'2007 albums',\n",
        " u'Understanding',\n",
        " u'United States public debt',\n",
        " u'Food',\n",
        " u'Vehicle',\n",
        " u'Ageing',\n",
        " u'Prostitution',\n",
        " u'Police',\n",
        " u'Janet Jackson',\n",
        " u'Bin Laden family',\n",
        " u'Fast food',\n",
        " u'Maize',\n",
        " u'Fast food restaurant',\n",
        " u'Taxation in the United States',\n",
        " u'Tax',\n",
        " u'Tax law',\n",
        " u'Internal Revenue Service',\n",
        " u'Tax refund',\n",
        " u'Cond\\xe9 Nast Publications',\n",
        " u'2008 singles',\n",
        " u'Association football goalkeepers',\n",
        " u'Help me',\n",
        " u'Cond\\xe9 Nast Publications',\n",
        " u'Thought',\n",
        " u'Elections',\n",
        " u'Elections in the United States',\n",
        " u'English-language films',\n",
        " u'Television',\n",
        " u'Hypertension',\n",
        " u'Westboro Baptist Church',\n",
        " u'American magazines',\n",
        " u'Reality',\n",
        " u'Aerosmith',\n",
        " u'World Wide Web',\n",
        " u'Internet',\n",
        " u'Website',\n",
        " u'Hypertext Transfer Protocol',\n",
        " u'Web page',\n",
        " u'Web design',\n",
        " u'Google Chrome',\n",
        " u'Google',\n",
        " u'Google Chrome OS',\n",
        " u'Web browser',\n",
        " u'Gmail',\n",
        " u'Celebrity',\n",
        " u'Paddy Chayefsky',\n",
        " u'George C. Scott',\n",
        " u'Diana Rigg',\n",
        " u'No One Knows',\n",
        " u'Academic degree',\n",
        " u'Doctorate',\n",
        " u'Higher education',\n",
        " u'University',\n",
        " u'Madrasah',\n",
        " u'Diploma',\n",
        " u'Faculty',\n",
        " u'2006 albums',\n",
        " u'2006 albums',\n",
        " u'Robin Beck',\n",
        " u'Surface',\n",
        " u'Fuck',\n",
        " u'Profanity',\n",
        " u'Alien abduction',\n",
        " u'Entertainment',\n",
        " u'The Luckiest',\n",
        " u'Death',\n",
        " u'1970s music groups',\n",
        " u'Puberty',\n",
        " u'Form of the Good',\n",
        " u'Bad',\n",
        " u'History of the world',\n",
        " u'Ibn Khaldun',\n",
        " u'Modulor',\n",
        " u'Easter',\n",
        " u'Easter egg',\n",
        " u'English-language films',\n",
        " u'2006 albums',\n",
        " u'One Piece',\n",
        " u'Scariest Places on Earth',\n",
        " u'Debut albums',\n",
        " u'2001 albums',\n",
        " u'Categorization',\n",
        " u'Ontology',\n",
        " u'The Dark Side of the Moon',\n",
        " u'Jeff Lynne',\n",
        " u'The Funniest Joke in the World',\n",
        " u'Joke',\n",
        " u'Jokes',\n",
        " u'ILOVEYOU',\n",
        " u'Henri Bergson',\n",
        " u'World War II',\n",
        " u'Germany',\n",
        " u'Shaun the Sheep',\n",
        " u'United States',\n",
        " u'Cook',\n",
        " u'2005 albums',\n",
        " u'What Happened',\n",
        " u'Exit',\n",
        " u'Fork',\n",
        " u'England',\n",
        " u'2009 albums',\n",
        " u'Commander-in-chief',\n",
        " u'Military',\n",
        " u'Military of the United States',\n",
        " u'Economics',\n",
        " u'Marriage',\n",
        " u'Education',\n",
        " u'English-language films',\n",
        " u'Poker',\n",
        " u'John Lennon',\n",
        " u'Morality',\n",
        " u'Science',\n",
        " u'Heshen',\n",
        " u'Unexplained Mysteries',\n",
        " u'Native Americans in the United States',\n",
        " u'Animal Collective',\n",
        " u'1995 singles',\n",
        " u'Horror and terror',\n",
        " u'Horror film',\n",
        " u'Speculative fiction',\n",
        " u'Live television',\n",
        " u'African American rappers',\n",
        " u'Publications established in 2005',\n",
        " u'2006 albums',\n",
        " u'The Road',\n",
        " u'Boy',\n",
        " u'Girl',\n",
        " u'Vector space',\n",
        " u'Heather Small',\n",
        " u'Nutrition',\n",
        " u'Human',\n",
        " u'Time',\n",
        " u'Monte Moir',\n",
        " u'Morris Day',\n",
        " u'Human',\n",
        " u'Race',\n",
        " u'Competition',\n",
        " u'Kumi Koda',\n",
        " u'Blue Bird',\n",
        " u'Ayumi Hamasaki',\n",
        " u'Superboy',\n",
        " u'High school',\n",
        " u'Education',\n",
        " u'Teacher',\n",
        " u'High School Musical',\n",
        " u'University',\n",
        " u'Education',\n",
        " u'Logic',\n",
        " u'Absolute',\n",
        " u'2006 albums',\n",
        " u'National Film Registry',\n",
        " u'English-language films',\n",
        " u'Guy Chambers',\n",
        " u'2006 albums',\n",
        " u'2009 singles',\n",
        " u'Black-and-white films',\n",
        " u'Pornography',\n",
        " u'Erotica',\n",
        " u'Doing It',\n",
        " u'2006 albums',\n",
        " u'Fascination Records',\n",
        " u'2006 albums',\n",
        " u'United States public debt',\n",
        " u'Philosophy of science',\n",
        " u'Illegal drug trade',\n",
        " u'Drug',\n",
        " u'Reality',\n",
        " u'Reality',\n",
        " u'Future',\n",
        " u'Song',\n",
        " u'Question',\n",
        " u'Interrogative word',\n",
        " u'Human',\n",
        " u'Wife',\n",
        " u'Family',\n",
        " u'Space exploration',\n",
        " u'Outer space',\n",
        " u'Wernher von Braun',\n",
        " u'NASA',\n",
        " u'TED',\n",
        " u'Biology',\n",
        " u'Humans',\n",
        " u'Meaning of life',\n",
        " u'Haunted house',\n",
        " u'Rudeness',\n",
        " u'Deviance',\n",
        " u'2001 albums',\n",
        " u'English-language films',\n",
        " u'United States',\n",
        " u'Federal government of the United States',\n",
        " u'Word',\n",
        " u'Phrase',\n",
        " u'Sentence',\n",
        " u'English-language films',\n",
        " u'2006 albums',\n",
        " u'Human body',\n",
        " u'Human anatomy',\n",
        " u'2006 albums',\n",
        " u'Reality',\n",
        " u'Real life',\n",
        " u'Social constructionism',\n",
        " u'Meatspace',\n",
        " u'Coma',\n",
        " u'Public library',\n",
        " u'Society',\n",
        " u'Taste',\n",
        " u'Conspiracy theory',\n",
        " u'Conspiracy?',\n",
        " u'Paddy Chayefsky',\n",
        " u'George C. Scott',\n",
        " u'Diana Rigg',\n",
        " u'Internet',\n",
        " u'History of the Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Reality',\n",
        " u'Real life',\n",
        " u'Social constructionism',\n",
        " u'Meatspace',\n",
        " u'English-language films',\n",
        " u'World',\n",
        " u'Earth',\n",
        " u'Internet pornography',\n",
        " u'Internet',\n",
        " u'Gerontology',\n",
        " u'Ageing',\n",
        " u'Old age',\n",
        " u'Restaurant',\n",
        " u'Fast food',\n",
        " u'Menu',\n",
        " u'YouTube',\n",
        " u'Language',\n",
        " u'Debut albums',\n",
        " u'2001 albums',\n",
        " u'Television',\n",
        " u'Television program',\n",
        " u'Reality television',\n",
        " u'Television network',\n",
        " u'Physician',\n",
        " u'Cognition',\n",
        " u'Concepts in metaphysics',\n",
        " u'Cognition',\n",
        " u'Parent',\n",
        " u'Western culture',\n",
        " u'Russia',\n",
        " u'Biology',\n",
        " u'Telephony',\n",
        " u'2006 albums',\n",
        " u'Mobile phone',\n",
        " u'Voice over Internet Protocol',\n",
        " u'The Point',\n",
        " u'Instant messaging',\n",
        " u'Victimless crime',\n",
        " u'Crime',\n",
        " u'Academic department',\n",
        " u'Image search',\n",
        " u'Google Images',\n",
        " u'Google services',\n",
        " u'Searching',\n",
        " u'Boy',\n",
        " u'Conspiracy theory',\n",
        " u'Conspiracy?',\n",
        " u'Art',\n",
        " u'Fine art',\n",
        " u'Education',\n",
        " u'Teacher',\n",
        " u'Entertainment',\n",
        " u'Immigration to the United States',\n",
        " u'Hispanic and Latino Americans',\n",
        " u'Demographics of the United States',\n",
        " u'United States Constitution',\n",
        " u'Birthright citizenship in the United States of America',\n",
        " u'Race and ethnicity in the United States Census',\n",
        " u'Table of United States Metropolitan Statistical Areas',\n",
        " u'Concepts in metaphysics',\n",
        " u'Human body',\n",
        " u'Psychology',\n",
        " u'Ontology',\n",
        " u'Human anatomy',\n",
        " u'Plot',\n",
        " u'2002 albums',\n",
        " u'Defunct video game companies',\n",
        " u'Brian Wilson',\n",
        " u'Rick Rubin',\n",
        " u'Atmosphere',\n",
        " u'Planet',\n",
        " u'Evolution',\n",
        " u'Human',\n",
        " u'Pluto',\n",
        " u'Billboard Hot Country Songs number-one singles',\n",
        " u'Consciousness',\n",
        " u'Brad Pitt',\n",
        " u'Color',\n",
        " u'Dye',\n",
        " u'Death',\n",
        " u'Pulp magazine',\n",
        " u'Pulp magazines',\n",
        " u'Bandwagon effect',\n",
        " u'Fads and trends',\n",
        " u'Celebrity',\n",
        " u'Future',\n",
        " u'Google search',\n",
        " u'Greatest hits albums',\n",
        " u'1998 albums',\n",
        " u'World',\n",
        " u'Asia',\n",
        " u'Europe',\n",
        " u'Accept',\n",
        " u'North Korea',\n",
        " u'Pacific Ocean',\n",
        " u'Korea',\n",
        " u'South Korea',\n",
        " u'2000 albums',\n",
        " u'John Carpenter',\n",
        " u'What Happened',\n",
        " u'Ghostzapper',\n",
        " u'Family',\n",
        " u'Dissociative identity disorder',\n",
        " u'Source code',\n",
        " u'Computer software',\n",
        " u'Personal computer',\n",
        " u'Melody Maker',\n",
        " u'2007 albums',\n",
        " u'1995 albums',\n",
        " u'Medicine',\n",
        " u'Sacrament',\n",
        " u'Love',\n",
        " u'World',\n",
        " u'Earth',\n",
        " u'Logic',\n",
        " u'Healthcare reform',\n",
        " u'Medicine',\n",
        " u'United States',\n",
        " u'Public health',\n",
        " u'Health economics',\n",
        " u'Data modeling',\n",
        " u'Grunge',\n",
        " u'Corporation',\n",
        " u'Ireland',\n",
        " u'A Lifetime',\n",
        " u'Kissing to Be Clever',\n",
        " u'Do You Really Want to Hurt Me',\n",
        " u'2006 albums',\n",
        " u'Sociology',\n",
        " u'Debut albums',\n",
        " u'High school',\n",
        " u'High School Musical',\n",
        " u'Question',\n",
        " u'2008 albums',\n",
        " u'2006 albums',\n",
        " u'Robert Nozick',\n",
        " u'Silver Surfer',\n",
        " u'Beyonder',\n",
        " u'Cognition',\n",
        " u'Milan Kundera',\n",
        " u'Neolithic',\n",
        " u'Prehistory',\n",
        " u'Mother',\n",
        " u'Father',\n",
        " u'Queen',\n",
        " u'Brian May',\n",
        " u'Meat',\n",
        " u'Nutrition',\n",
        " u'In vitro meat',\n",
        " u'2007 albums',\n",
        " u'1995 albums',\n",
        " u'Immigration law',\n",
        " u'Motion Picture Association of America film rating system',\n",
        " u'Writing',\n",
        " u'Creative writing',\n",
        " u'Life',\n",
        " u'Time',\n",
        " u'Time',\n",
        " u'Monte Moir',\n",
        " u'Morris Day',\n",
        " u'Unsolved Mysteries',\n",
        " u'Garry Bushell',\n",
        " u'English-language films',\n",
        " u'University',\n",
        " u'Professor',\n",
        " u'English-language films',\n",
        " u'Flag of the United States',\n",
        " u'Personal computer',\n",
        " u'Yolanda Adams',\n",
        " u'Believe',\n",
        " u'Robin Beck',\n",
        " u'Surface',\n",
        " u'2005 albums',\n",
        " u'Ilmar Raag',\n",
        " u'Time',\n",
        " u'Monte Moir',\n",
        " u'Time',\n",
        " u'Morris Day',\n",
        " u'Horror fiction',\n",
        " u'Urban legend',\n",
        " u'Folklore',\n",
        " u'Secrecy',\n",
        " u'Taxi Driver',\n",
        " u'English-language films',\n",
        " u'Lingerie',\n",
        " u\"Victoria's Secret\",\n",
        " u'Film criticism',\n",
        " u'2006 albums',\n",
        " u'Vocal duets',\n",
        " u'Mammal',\n",
        " u'Evolution',\n",
        " u'Debut albums',\n",
        " u'English-language films',\n",
        " u'Psychology',\n",
        " u'Psychology',\n",
        " u'Thought',\n",
        " u'Fuck',\n",
        " u'Sociology',\n",
        " u'Google Earth',\n",
        " u'A Story',\n",
        " u'Warner Bros. Records albums',\n",
        " u'1983 albums',\n",
        " u'Fascination Records',\n",
        " u'Family',\n",
        " u'Brain',\n",
        " u'Human body',\n",
        " u'Human anatomy',\n",
        " u'Horror film',\n",
        " u'Significant Other',\n",
        " u'Television',\n",
        " u'Television program',\n",
        " u'Reality television',\n",
        " u'Television network',\n",
        " u\"Somebody Else's Problem\",\n",
        " u'2007 singles',\n",
        " u'Greatest hits albums',\n",
        " u'English language',\n",
        " u'Phonology',\n",
        " u'Vowel',\n",
        " u'General American',\n",
        " u'United States',\n",
        " u'Country music',\n",
        " u'American',\n",
        " u'Old-time music',\n",
        " u'Indian reservation',\n",
        " u'Rock and roll',\n",
        " u'Humid subtropical climate',\n",
        " u'Bird',\n",
        " u'Digg',\n",
        " u'Government',\n",
        " u'Sovereign state',\n",
        " u'Funeral',\n",
        " u'Paul Lynde',\n",
        " u'Thought',\n",
        " u'Comedy',\n",
        " u'Thought',\n",
        " u'Human',\n",
        " u'Religion',\n",
        " u'Meaning of life',\n",
        " u'World',\n",
        " u'Earth',\n",
        " u'Property',\n",
        " u'Topological space',\n",
        " u'A Little Bit',\n",
        " u'A Little Bit Longer',\n",
        " u'Bad',\n",
        " u'Tynwald',\n",
        " u'2006 albums',\n",
        " u'Love',\n",
        " u'The Lettermen',\n",
        " u'Word',\n",
        " u'Grammatical conjugation',\n",
        " u'Mother',\n",
        " u'Greek loanwords',\n",
        " u'Opera',\n",
        " u'Wolfgang Amadeus Mozart',\n",
        " u'Musical notation',\n",
        " u'Counterpoint',\n",
        " u'Musical improvisation',\n",
        " u'Baroque music',\n",
        " u'College or university school of music',\n",
        " u'Classical music',\n",
        " u'Cognition',\n",
        " u'Prison',\n",
        " u'Sentence',\n",
        " u'The Cleverest',\n",
        " u'Absolute',\n",
        " u'Coming of age',\n",
        " u'Robin Beck',\n",
        " u'Surface',\n",
        " u'1987 singles',\n",
        " u'The Best Party Ever',\n",
        " u'Question',\n",
        " u'Government',\n",
        " u'Sovereign state',\n",
        " u'History',\n",
        " u'1979 albums',\n",
        " u'Surveillance',\n",
        " u'Soul',\n",
        " u'Philosophy of mind',\n",
        " u'Ego',\n",
        " u'Comedy',\n",
        " u'Johnny Depp',\n",
        " u'Yolanda Adams',\n",
        " u'Believe',\n",
        " u'2005 albums',\n",
        " u'American magazines',\n",
        " u'Personal life',\n",
        " u'World',\n",
        " u'Earth',\n",
        " u'Seduction community',\n",
        " u'Seduction',\n",
        " u'The Front Page',\n",
        " u'Higher education',\n",
        " u'University',\n",
        " u\"Associate's degree\",\n",
        " u'Madrasah',\n",
        " u'Academic degree',\n",
        " u'Photography',\n",
        " u'Image',\n",
        " u'Reality',\n",
        " u'Multiplication',\n",
        " u'Sex doll',\n",
        " u'Blue Bird',\n",
        " u'Ayumi Hamasaki',\n",
        " u'Skill',\n",
        " u'English-language films',\n",
        " u'Racial segregation',\n",
        " u'Same-sex marriage',\n",
        " u'Racism',\n",
        " u'Sociology',\n",
        " u'Instant messaging',\n",
        " u'Scooter',\n",
        " u'Earth',\n",
        " u'Supercontinent',\n",
        " u'Africa',\n",
        " u'World',\n",
        " u'Pangaea',\n",
        " u'American films',\n",
        " u'Truth',\n",
        " u'Knowledge',\n",
        " u'Georg Wilhelm Friedrich Hegel',\n",
        " u'Science',\n",
        " u'Epistemology',\n",
        " u'Page',\n",
        " u'Mother',\n",
        " u'1995 albums',\n",
        " u'The Nature Conservancy',\n",
        " u'Reinhard Heydrich',\n",
        " u'Vocal music',\n",
        " u'Rock music',\n",
        " u'Love',\n",
        " u\"Can't Help Falling in Love\",\n",
        " u'Right-wing politics',\n",
        " u'Sex offender',\n",
        " u'Microsoft',\n",
        " u'Bing',\n",
        " u'Employment',\n",
        " u'Job interview',\n",
        " u'Recruitment',\n",
        " u'Family law',\n",
        " u'Character',\n",
        " u'Fiction',\n",
        " u'Novel',\n",
        " u'Characterisation',\n",
        " u'Human',\n",
        " u'Jupiter',\n",
        " u'Europa',\n",
        " u'Extraterrestrial life',\n",
        " u'Planet',\n",
        " u'Extrasolar planet',\n",
        " u'Mars',\n",
        " u'Science',\n",
        " u'Television program',\n",
        " u'Scariest Places on Earth',\n",
        " u'Wage',\n",
        " u'Island Records albums',\n",
        " u'Video game culture',\n",
        " u'Telepresence',\n",
        " u'A Story',\n",
        " u'Audi',\n",
        " u'1995 singles',\n",
        " u'Time',\n",
        " u'Religion',\n",
        " u'Germany',\n",
        " u'Cold War',\n",
        " u'Soviet Union',\n",
        " u'West Germany',\n",
        " u'East Berlin',\n",
        " u'East Germany',\n",
        " u'Willy Brandt',\n",
        " u'Communism',\n",
        " u'English-language films',\n",
        " u'Bad',\n",
        " u'English-language films',\n",
        " u'Unsolved Mysteries',\n",
        " u'Million',\n",
        " u'Melody Maker',\n",
        " u'2001 albums',\n",
        " u'The Animals',\n",
        " u\"Don't Let Me Be Misunderstood\",\n",
        " u'2009 albums',\n",
        " u'Scientific method',\n",
        " u'Research',\n",
        " u'Science',\n",
        " u'History of the Internet',\n",
        " u'Website',\n",
        " u'Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Believe',\n",
        " u'Time travel',\n",
        " u'General relativity',\n",
        " u'Arashi',\n",
        " u'Spacetime',\n",
        " u'Horror film',\n",
        " u'Film',\n",
        " u'Every time you masturbate... God kills a kitten',\n",
        " u'Ayumi Hamasaki',\n",
        " u'Days/Green',\n",
        " u'Place name disambiguation pages',\n",
        " u'Reader',\n",
        " u'Woman',\n",
        " u'Political spectrum',\n",
        " u'High school',\n",
        " u'College',\n",
        " u'Greatest hits albums',\n",
        " u'University',\n",
        " u'Student',\n",
        " u'Korea',\n",
        " u'World War II',\n",
        " u'United States',\n",
        " u'South Korea',\n",
        " u'United States',\n",
        " u'U.S. state',\n",
        " u'President of the United States',\n",
        " u'United States Marine Corps',\n",
        " u'United States Army',\n",
        " u'Joint Chiefs of Staff',\n",
        " u'United States Senate',\n",
        " u'Political philosophy',\n",
        " u'Nuclear Non-Proliferation Treaty',\n",
        " u'Nuclear weapon',\n",
        " u'North Korea and weapons of mass destruction',\n",
        " u'World War II',\n",
        " u'Nuclear disarmament',\n",
        " u'Cold War',\n",
        " u'Missile',\n",
        " u'North Korea',\n",
        " u'High school',\n",
        " u'High School Musical',\n",
        " u'2006 albums',\n",
        " u'Arista Records albums',\n",
        " u'Human body',\n",
        " u'Human anatomy',\n",
        " u'Profanity',\n",
        " u'Trigraph',\n",
        " u'Gh',\n",
        " u'History of education',\n",
        " u'BASIC',\n",
        " u'Truth or Dare?',\n",
        " u'Ocean',\n",
        " u'Draw-A-Person Test',\n",
        " u'Automobile',\n",
        " u'Customer service',\n",
        " u'Soviet Union',\n",
        " u'Vladimir Lenin',\n",
        " u'Gulag',\n",
        " u'Greatest hits albums',\n",
        " u'Historian',\n",
        " u'The Notorious B.I.G.',\n",
        " u'TED',\n",
        " u'2006 albums',\n",
        " u'2007 singles',\n",
        " u'Country ballads',\n",
        " u'Phantom Stranger',\n",
        " u'2006 albums',\n",
        " u'Unsolved Mysteries',\n",
        " u'Europe',\n",
        " u'World',\n",
        " u'Africa',\n",
        " u'Federal government of the United States',\n",
        " u'Earth',\n",
        " u'Butterfly',\n",
        " u'Future',\n",
        " u'The Road',\n",
        " u'Fast food',\n",
        " u'Menu',\n",
        " u'Restaurant',\n",
        " u'Take-out',\n",
        " u'Online game',\n",
        " u'Confession',\n",
        " u'Marketing',\n",
        " u'Nutrition',\n",
        " u'Aerosmith',\n",
        " u'2006 albums',\n",
        " u'Gender role',\n",
        " u'Childbirth',\n",
        " u'Paranormal',\n",
        " u'Occult',\n",
        " u'Esotericism',\n",
        " u'United States',\n",
        " u'President of the United States',\n",
        " u'Fuck',\n",
        " u'Profanity',\n",
        " u'Shit',\n",
        " u'George Carlin',\n",
        " u'Seven dirty words',\n",
        " u'U.S. state',\n",
        " u'Economic history',\n",
        " u'Austrian School',\n",
        " u'Sociology',\n",
        " u'Norm',\n",
        " u'Norm',\n",
        " u'Heteronormativity',\n",
        " u'2006 albums',\n",
        " u'Internet privacy',\n",
        " u'Internet',\n",
        " u'History of the Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Cognition',\n",
        " u'Amateur radio',\n",
        " u'Typeface',\n",
        " u'Blue Bird',\n",
        " u'Ayumi Hamasaki',\n",
        " u'2006 albums',\n",
        " u'Law',\n",
        " u'Internet',\n",
        " u'History of the Internet',\n",
        " u'World Wide Web',\n",
        " u'Freedom of information',\n",
        " u'Man']"
       ]
      }
     ],
     "prompt_number": 116
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We can see here that perhaps just using the titles might not give us enough information to accurately ascertain what the posts are about."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 28
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 64
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "subreddit = 'explainlikeimfive'\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")\n",
      "\n",
      "topids = list(df['id'])\n",
      "\n",
      "endat = 150\n",
      "commentlen = []\n",
      "#for topid in topids:\n",
      "for i in range(endat):\n",
      "    try:\n",
      "        commentlen.append(len(list(get_comments(subreddit, topids[i], 'top', user_agent)['comment'])[0]))\n",
      "    except:\n",
      "        print topids[i]\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "15u5d0\n",
        "s8l4e"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 98
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids.index('15u5d0')\n",
      "topids.index('s8l4e')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 99,
       "text": [
        "122"
       ]
      }
     ],
     "prompt_number": 99
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "print len(commentlen)\n",
      "\n",
      "topscores = list(df['score'])[:50] + list(df['score'])[51:122] + list(df['score'])[123:endat]\n",
      "topmetrics = list(df['mymetric'])[:50] + list(df['mymetric'])[51:122] + list(df['mymetric'])[123:endat]\n",
      "topups = list(df['upvotes'])[:50] + list(df['upvotes'])[51:122] + list(df['upvotes'])[123:endat]\n",
      "topcont = list(df['up/down'])[:50] + list(df['up/down'])[51:122] + list(df['up/down'])[123:endat]\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "148\n"
       ]
      }
     ],
     "prompt_number": 102
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "There appears to be some minor correlation between the length of the top comment and popularity of the post, but it appears like this effect is on the decline over time."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "from scipy.stats.stats import pearsonr\n",
      "\n",
      "print pearsonr(commentlen, topscores)\n",
      "print pearsonr(commentlen, topmetrics)\n",
      "print pearsonr(commentlen, topups)\n",
      "print pearsonr(commentlen, topcont)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "(0.44383859210747884, 1.6035273359197743e-08)\n",
        "(0.44319470358692481, 1.6912587281428895e-08)\n",
        "(0.25094344719532163, 0.0020954876387472149)\n",
        "(0.041284452559721747, 0.61834237353945753)\n"
       ]
      }
     ],
     "prompt_number": 104
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#following code will create a counter dictionary will all keywords in titles for a single subreddit\n",
      "\n",
      "def get_keywords(subreddit):\n",
      "    returndict = Counter()\n",
      "    for a in subreddit.title:\n",
      "        keywords = p.run_method(a, 'keywords')\n",
      "        if keywords:\n",
      "            for b in keywords:\n",
      "                returndict[b[1]] = returndict[b[1]] + 1\n",
      "    return returndict"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "all_keywords = get_keywords(pd.read_csv(csvfiles[13], encoding='utf-8'))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "ename": "UnicodeEncodeError",
       "evalue": "'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)",
       "output_type": "pyerr",
       "traceback": [
        "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m\n\u001b[1;31mUnicodeEncodeError\u001b[0m                        Traceback (most recent call last)",
        "\u001b[1;32m<ipython-input-38-d70f96484ad9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mall_keywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mget_keywords\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcsvfiles\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m13\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'utf-8'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
        "\u001b[1;32m<ipython-input-35-20ccc0388b59>\u001b[0m in \u001b[0;36mget_keywords\u001b[1;34m(subreddit)\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mreturndict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCounter\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0ma\u001b[0m \u001b[1;32min\u001b[0m \u001b[0msubreddit\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m         \u001b[0mkeywords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun_method\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'keywords'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m             \u001b[1;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mkeywords\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/home/j/Dropbox/College/Data Science/Project/intelligent-dolphins/myalchemy.py\u001b[0m in \u001b[0;36mrun_method\u001b[1;34m(self, comment, whichtoget, otherparams)\u001b[0m\n\u001b[0;32m     63\u001b[0m         \u001b[1;32mglobal\u001b[0m \u001b[0malchemybase\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcomment\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 65\u001b[1;33m         \u001b[0mform\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0murlencode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     66\u001b[0m         \u001b[0mmethod\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__look_at\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwhichtoget\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     67\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmethod\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;32m/opt/anaconda/lib/python2.7/urllib.pyc\u001b[0m in \u001b[0;36murlencode\u001b[1;34m(query, doseq)\u001b[0m\n\u001b[0;32m   1324\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mv\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mquery\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1325\u001b[0m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1326\u001b[1;33m             \u001b[0mv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mquote_plus\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1327\u001b[0m             \u001b[0ml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mk\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m'='\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mv\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1328\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
        "\u001b[1;31mUnicodeEncodeError\u001b[0m: 'ascii' codec can't encode character u'\\xfe' in position 61: ordinal not in range(128)"
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}