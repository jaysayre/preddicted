{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    \n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 44
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search_keyword(term, maxscores, user_agent, sort_call='relevance', t='all', subreddit=None, api_call_limit=100):\n",
      "    scores = {}\n",
      "    header = {'User-agent': user_agent}\n",
      "    if subreddit == None:\n",
      "        reddit_base = 'http://www.reddit.com/r/search/search.json?' # If we want to search all of reddit\n",
      "        post_params = {'q':term, 'sort': sort_call, 't':t, 'limit':api_call_limit}\n",
      "    else:\n",
      "        reddit_base = 'http://www.reddit.com/r/%s/search.json?' % subreddit\n",
      "        post_params = {'q':term, 'sort': sort_call, 't':t, 'limit':api_call_limit, 'restrict_sr':'on'}\n",
      "    \n",
      "    #Makes sure maxscores isn't a float!\n",
      "    maxscores = int(maxscores) \n",
      "    api_call_limit = int(api_call_limit)\n",
      "    \n",
      "    #Since reddit only provides <= 100 calls at a time, looks at n requested and splits it up into different requests\n",
      "    if maxscores%api_call_limit != 0:\n",
      "        remainder = maxscores%api_call_limit\n",
      "        num = (maxscores/api_call_limit) +1\n",
      "    else:\n",
      "        num = maxscores/api_call_limit\n",
      "        remainder = api_call_limit\n",
      "        \n",
      "    #Makes an api call for all n entries based on the api call limit\n",
      "    for i in range(num):\n",
      "        if i == 0:\n",
      "            jsondata = json_extract(reddit_base, header, post_params)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            for item in jsondata['data']['children']:\n",
      "                scores.update({item['data']['id'] : item['data']['score']})\n",
      "        elif i == num - 1:\n",
      "            post_params.update({'limit': remainder, 'after': tostartfrom}) #Indicates the post after we wish to call from\n",
      "            jsondata = json_extract(reddit_base, header, post_params)\n",
      "            for item in jsondata['data']['children']:\n",
      "                scores.update({item['data']['id'] : item['data']['score']})\n",
      "        else: \n",
      "            post_params.update({'after': tostartfrom}) \n",
      "            jsondata = json_extract(reddit_base, header, post_params)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            for item in jsondata['data']['children']:\n",
      "                scores.update({item['data']['id'] : item['data']['score']})\n",
      "                \n",
      "    return scores.values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 65
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a =search_keyword('ww1', 810, user_agent, subreddit='askhistorians')\n",
      "print len(a)\n",
      "print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "340\n",
        "[17, 23, 1, 4, 2, 20, 2, 5, 1, 2, 8, 8, 0, 6, 41, 20, 13, 1, 2, 16, 16, 36, 4, 5, 2, 3, 17, 2, 35, 0, 12, 217, 79, 6, 0, 0, 2, 16, 0, 4, 11, 45, 3, 8, 1, 2, 1, 613, 0, 27, 0, 6, 2, 5, 2, 3, 8, 3, 21, 0, 0, 8, 3, 17, 10, 9, 6, 1, 5, 90, 0, 25, 0, 3, 1, 2, 9, 0, 3, 0, 2, 6, 18, 1, 0, 1, 2, 1, 2, 355, 2, 65, 0, 1, 16, 0, 0, 53, 13, 125, 8, 3, 1, 10, 6, 2, 6, 5, 1, 4, 12, 1, 27, 34, 9, 2, 0, 1, 2, 46, 15, 2, 4, 23, 2, 3, 7, 0, 1, 22, 16, 3, 1, 0, 9, 3, 348, 63, 0, 0, 9, 5, 2, 89, 7, 7, 7, 2, 37, 9, 16, 3, 3, 884, 12, 1, 10, 6, 10, 7, 4, 12, 5, 33, 1, 1, 5, 15, 3, 4, 9, 5, 5, 7, 24, 12, 0, 1, 2, 15, 31, 20, 6, 2, 27, 56, 4, 0, 134, 159, 2, 13, 7, 11, 2, 41, 9, 4, 0, 5, 4, 6, 14, 2, 3, 1, 1, 12, 2, 20, 1, 3, 3, 7, 2, 4, 13, 2, 130, 7, 2, 1, 43, 1, 14, 17, 1, 4, 0, 13, 36, 18, 2, 2, 11, 2, 1, 0, 2, 9, 11, 13, 7, 2, 17, 1, 0, 6, 30, 5, 5, 1, 8, 2, 13, 25, 4, 5, 3, 0, 10, 2, 5, 1, 17, 41, 7, 8, 2, 15, 15, 16, 1, 15, 8, 112, 29, 15, 9, 2, 4, 1, 5, 9, 6, 421, 6, 1, 17, 3, 16, 15, 2, 101, 3, 5, 12, 7, 8, 308, 1, 1, 8, 14, 101, 2, 43, 11, 6, 3, 3, 23, 36, 14, 8, 2, 17, 5, 9, 2, 6, 1, 4, 5, 3, 4, 69, 0, 8, 0, 11, 1, 3, 70, 4, 13, 2, 3, 5, 15]\n"
       ]
      }
     ],
     "prompt_number": 73
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}