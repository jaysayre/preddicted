{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import nltk\n",
      "from nltk.util import ngrams\n",
      "from nltk.collocations import *\n",
      "from nltk.corpus import stopwords\n",
      "\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    \n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n",
      "\n",
      "def return_grams(sentence, n=[1, 3],  minlength=3):\n",
      "    gramslist = []\n",
      "    mysentencetokens_sw= nltk.word_tokenize(sentence)\n",
      "    mysentencetokens = [token for token in mysentencetokens_sw if (not token in stopwords.words('english')) and len(token) >= minlength]\n",
      "    for j in range(n[0], n[1]+1):\n",
      "        somegrams = ngrams(mysentencetokens, j)\n",
      "        for grams in somegrams:\n",
      "            gramslist.append(' '.join(grams))\n",
      "    return gramslist     "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def search_keyword(title, maxscores, user_agent, n=[3,3], postid='bhjfb', sort_call='relevance', t='all', subreddit=None, api_call_limit=100):\n",
      "    scores = {}\n",
      "    header = {'User-agent': user_agent}\n",
      "    for term in return_grams(title, [n[0],n[1]]): \n",
      "        post_params = {'q':term, 'sort': sort_call, 't':t, 'limit':api_call_limit}\n",
      "        if subreddit == None:\n",
      "            reddit_base = 'http://www.reddit.com/r/search/search.json?' # If we want to search all of reddit\n",
      "        else:\n",
      "            reddit_base = 'http://www.reddit.com/r/%s/search.json?' % subreddit\n",
      "            post_params.update ({'restrict_sr':'on'})\n",
      "        \n",
      "        #Makes sure maxscores isn't a float!\n",
      "        maxscores = int(maxscores) \n",
      "        api_call_limit = int(api_call_limit)\n",
      "    \n",
      "        #Since reddit only provides <= 100 calls at a time, looks at n requested and splits it up into different requests\n",
      "        if maxscores%api_call_limit != 0:\n",
      "            remainder = maxscores%api_call_limit\n",
      "            num = (maxscores/api_call_limit) +1\n",
      "        else:\n",
      "            num = maxscores/api_call_limit\n",
      "            remainder = api_call_limit\n",
      "            \n",
      "        #Makes an api call for all n entries based on the api call limit\n",
      "        for i in range(num):\n",
      "            if i == 0:\n",
      "                jsondata = json_extract(reddit_base, header, post_params)\n",
      "                tostartfrom = jsondata['data']['after']\n",
      "                for item in jsondata['data']['children']:\n",
      "                    if item['data']['score'] != 0:\n",
      "                        scores.update({item['data']['id'] : item['data']['score']})\n",
      "            elif i == num - 1:\n",
      "                post_params.update({'limit': remainder, 'after': tostartfrom}) #Indicates the post after we wish to call from\n",
      "                jsondata = json_extract(reddit_base, header, post_params)\n",
      "                for item in jsondata['data']['children']:\n",
      "                    if item['data']['score'] != 0:\n",
      "                        scores.update({item['data']['id'] : item['data']['score']})\n",
      "            else: \n",
      "                post_params.update({'after': tostartfrom}) \n",
      "                jsondata = json_extract(reddit_base, header, post_params)\n",
      "                tostartfrom = jsondata['data']['after']\n",
      "                for item in jsondata['data']['children']:\n",
      "                    if item['data']['score'] != 0:\n",
      "                        scores.update({item['data']['id'] : item['data']['score']})\n",
      "    try:\n",
      "        scores.pop(postid)\n",
      "    except:\n",
      "        pass\n",
      "        \n",
      "    return scores.values()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 88
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "df = pd.read_csv(\"Data/full.csv\", encoding=\"utf-8\")\n",
      "df = df.drop_duplicates('id')\n",
      "len(df)\n",
      "\n",
      "df['titlescore'] = ['null']*len(df)\n",
      "df['titlestd'] = ['null']*len(df)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 109
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for i in df.index:\n",
      "    b = search_keyword(df['title'][i], 300, user_agent, postid=df['id'][i], subreddit=df['subreddit'][i])\n",
      "    if len(b) > 0:\n",
      "        df['titlescore'][i] = np.mean(b)\n",
      "        df['titlestd'][i] = np.std(b)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "195.664974619\n",
        "524.484769602\n"
       ]
      }
     ],
     "prompt_number": 95
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "cols = df.columns\n",
      "df  = df.drop(cols[0:3], 1)\n",
      "df = df.drop(cols[4:14], 1)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 115
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#df.to_csv(\"Data/titlescore.csv\", index=False, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "a =search_keyword('ww1', 810, user_agent, subreddit='askhistorians')\n",
      "print len(a)\n",
      "print a"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "310\n",
        "[17, 22, 2, 4, 2, 23, 2, 3, 1, 2, 9, 7, 3, 43, 17, 15, 1, 4, 18, 18, 36, 5, 3, 2, 3, 18, 2, 33, 12, 215, 82, 5, 2, 2, 15, 5, 9, 48, 3, 7, 1, 2, 1, 615, 2, 27, 4, 6, 2, 5, 2, 3, 7, 3, 20, 2, 1, 6, 3, 16, 6, 9, 6, 1, 3, 88, 24, 3, 1, 2, 6, 3, 2, 4, 19, 1, 1, 2, 1, 2, 352, 2, 67, 1, 13, 51, 12, 131, 7, 3, 1, 9, 6, 2, 7, 5, 2, 5, 12, 1, 28, 34, 9, 2, 1, 2, 47, 15, 2, 4, 23, 5, 2, 8, 1, 22, 17, 3, 1, 11, 3, 348, 56, 9, 6, 2, 87, 8, 11, 6, 3, 32, 10, 18, 4, 3, 888, 9, 1, 10, 6, 8, 8, 5, 12, 3, 30, 1, 1, 5, 15, 3, 6, 9, 3, 6, 9, 25, 11, 1, 4, 14, 31, 23, 6, 2, 24, 57, 4, 1, 137, 163, 2, 14, 5, 11, 39, 9, 7, 2, 3, 5, 14, 1, 4, 1, 1, 15, 2, 22, 1, 3, 3, 6, 5, 16, 2, 128, 7, 2, 1, 44, 1, 12, 19, 1, 4, 13, 33, 19, 2, 11, 2, 1, 2, 10, 10, 14, 6, 3, 16, 1, 4, 27, 8, 3, 1, 8, 4, 13, 25, 3, 3, 3, 9, 4, 1, 17, 40, 7, 6, 2, 17, 16, 15, 1, 11, 8, 112, 27, 18, 8, 5, 5, 1, 5, 6, 6, 427, 6, 1, 17, 3, 16, 17, 2, 97, 3, 4, 14, 6, 7, 309, 1, 1, 6, 15, 106, 2, 35, 14, 7, 5, 3, 24, 31, 12, 10, 18, 5, 9, 2, 3, 1, 4, 6, 3, 5, 73, 7, 9, 1, 3, 69, 6, 11, 3, 3, 5, 17]\n"
       ]
      }
     ],
     "prompt_number": 68
    }
   ],
   "metadata": {}
  }
 ]
}