{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import urllib\n",
      "import urllib2\n",
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import numpy as np\n",
      "\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, subtype, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    try:\n",
      "        return pd.DataFrame({'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, \n",
      "                    'distinguished': distin, 'subreddit': subreddit, 'post': postid, 'type': subtype})\n",
      "    except:\n",
      "        #print len(comments), len(ids), len(ups), len(downs), len(authors)\n",
      "        ids.pop(0) #Might need to be more formulaic\n",
      "        return pd.DataFrame({'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, \n",
      "                    'distinguished': distin, 'subreddit': subreddit, 'post': postid, 'type': subtype})\n",
      "    \n",
      "\n",
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\" #Fill in your own\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "bigcsv = csvfiles[0]\n",
      "bigdf = pd.read_csv(bigcsv, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "subs = list(bigdf['subreddit'].unique())[:-1] #Gets rid of NaN\n",
      "types = list(bigdf['type'].unique())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sub in subs:\n",
      "    print sub\n",
      "    for typ in types:\n",
      "        print typ\n",
      "        df = bigdf[bigdf['subreddit'] == sub]\n",
      "        df = df[df['type'] == typ]\n",
      "        dfidlist = list(df.index)\n",
      "        comments = pd.DataFrame()\n",
      "        for i in dfidlist:\n",
      "            try:\n",
      "                comments = comments.append(get_comments(df['subreddit'][i], df['id'][i], 'top', df['type'][i], user_agent))\n",
      "            except:\n",
      "                print i\n",
      "                with open('Data/Comments/missingcomments.txt', 'a') as the_file:\n",
      "                    the_file.write(df['id'][i])\n",
      "                    the_file.write(' ' + sub)\n",
      "                    the_file.write(' ' + typ + '\\n')\n",
      "        filename = 'Data/Comments/' + sub + typ + '.csv'\n",
      "        comments.to_csv(filename, index=False, encoding='utf-8')\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "AskReddit\n",
        "top_all\n",
        "1684"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}