{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 33
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\"\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 71
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 77
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function to determine if float\n",
      "def isfloat(x):\n",
      "    try:\n",
      "        float(x)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "#Function to clean strings\n",
      "def filterstr(x):\n",
      "    try:\n",
      "        y= ''.join(e for e in x if (e == ' ' or e.isalnum()) and not e.isdigit())\n",
      "        return y\n",
      "    except:\n",
      "        return ''\n",
      "    \n",
      "def check_null(x):\n",
      "    try:\n",
      "        return np.isnan(x)\n",
      "    except:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to clean downloaded information\n",
      "def subset_subreddit(df):\n",
      "    df = df[df['distinguished'].apply(lambda x: check_null(x))] # Makes sure poster isn't a moderator\n",
      "    df = df[df['media'].apply(lambda x: check_null(x))] # Also makes sure post is text based\n",
      "    df = df.drop('distinguished',1)\n",
      "    df = df.drop('media', 1)\n",
      "    # Occasionally, text strings will be in areas where only ints should be. This takes care of that\n",
      "    df=df[df['comments'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['downvotes'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['score'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['upvotes'].apply(lambda x: isfloat(x))]\n",
      "    for i in df.index:\n",
      "        df['title'][i] = filterstr(df['title'][i])\n",
      "        df['author'][i] = filterstr(df['author'][i])\n",
      "        df['selftext'][i] = filterstr(df['selftext'][i])\n",
      "    df = df.drop_duplicates() #Remove duplicate entries, should they exist\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f in csvfiles:\n",
      "    df = pd.read_csv(f, encoding='utf-8') # Top all is our training data set\n",
      "    df = subset_subreddit(df)\n",
      "    df.to_csv(f, index=False, encoding='utf-8') #Run this only when done..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    authorkarma = []\n",
      "    linkkarma = []\n",
      "    count = 0\n",
      "    for author in df['author']:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % author\n",
      "            headers = {'User-agent': user_agent}\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            authorkarma.append(jsondata['data']['comment_karma'])\n",
      "            try:\n",
      "                linkkarma.append(jsondata['data']['link_karma'])\n",
      "            except:\n",
      "                linkkarma.append(0)\n",
      "            count += 1\n",
      "        except:\n",
      "            authorkarma.append(0)\n",
      "            linkkarma.append(0)\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count\n",
      "                \n",
      "    df['karma'] = authorkarma\n",
      "    df['link_karma'] = linkkarma\n",
      "    return df\n",
      "\n",
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 79
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add in karma... Be careful as this is really quite slow\n",
      "#print csvfiles[0]\n",
      "#df = pd.read_csv(csvfiles[0], encoding='utf-8') # Top all is our training data set\n",
      "#df = add_karma(df, user_agent, 100)\n",
      "#df.to_csv(csvfiles[0], index=False, encoding='utf-8') #Run this only when done...\n",
      "\n",
      "for f in csvfiles:\n",
      "    try:\n",
      "        df = pd.read_csv(f, encoding='utf-8') # Top all is our training data set\n",
      "        df = add_karma(df, user_agent, 100)\n",
      "        df.to_csv(f, index=False, encoding='utf-8') #Run this only when done...\n",
      "    except:\n",
      "        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Data/AskRedditnew.csv\n",
        "Retrieved karma for 100 users"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Retrieved karma for 200 users"
       ]
      }
     ],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}