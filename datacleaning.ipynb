{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\"\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "\n",
      "csvfiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['Data/big.csv']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function to determine if float\n",
      "def isfloat(x):\n",
      "    try:\n",
      "        float(x)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "#Function to clean strings\n",
      "def filterstr(x):\n",
      "    try:\n",
      "        y= ''.join(e for e in x if (e == ' ' or e.isalnum()) and not e.isdigit())\n",
      "        return y\n",
      "    except:\n",
      "        return ''\n",
      "    \n",
      "def check_null(x):\n",
      "    try:\n",
      "        return np.isnan(x)\n",
      "    except:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to clean downloaded information\n",
      "def subset_subreddit(df):\n",
      "    df = df[df['distinguished'].apply(lambda x: check_null(x))] # Makes sure poster isn't a moderator\n",
      "    df = df[df['media'].apply(lambda x: check_null(x))] # Also makes sure post is text based\n",
      "    df = df.drop('distinguished',1)\n",
      "    df = df.drop('media', 1)\n",
      "    # Occasionally, text strings will be in areas where only ints should be. This takes care of that\n",
      "    df=df[df['comments'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['downvotes'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['score'].apply(lambda x: isfloat(x))]\n",
      "    df=df[df['upvotes'].apply(lambda x: isfloat(x))]\n",
      "    for i in df.index:\n",
      "        df['title'][i] = filterstr(df['title'][i])\n",
      "        df['author'][i] = filterstr(df['author'][i])\n",
      "        df['selftext'][i] = filterstr(df['selftext'][i])\n",
      "    df = df.drop_duplicates() #Remove duplicate entries, should they exist\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 69
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f in csvfiles:\n",
      "    df = pd.read_csv(f, encoding='utf-8') # Top all is our training data set\n",
      "    df = subset_subreddit(df)\n",
      "    #df.to_csv(f, index=False, encoding='utf-8') #Run this only when done..."
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 70
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20, start=0, end=len(df)):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    \n",
      "    count = 0\n",
      "    dfidlist = list(df.index)\n",
      "    dfidlist = dfidlist[start : end]\n",
      "    for i in dfidlist:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % df['author'][i]\n",
      "            headers = {'User-agent': user_agent}\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            df['karma'][i] = jsondata['data']['comment_karma']\n",
      "            try:\n",
      "                df['link_karma'][i] = jsondata['data']['link_karma']\n",
      "            except:\n",
      "                df['link_karma'][i] = 0\n",
      "            count += 1\n",
      "        except:\n",
      "            df['karma'][i] = 0\n",
      "            df['link_karma'][i] = 0\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count          \n",
      "    return df\n",
      "\n",
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add in karma... Be careful as this is really quite slow\n",
      "df = pd.read_csv(csvfiles[0], encoding='utf-8')\n",
      "df = add_karma(df, user_agent, display=5, start=0, end=200)\n",
      "df.to_csv(csvfiles[0], index=False, encoding='utf-8')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<div style=\"max-height:1000px;max-width:1500px;overflow:auto;\">\n",
        "<table border=\"1\" class=\"dataframe\">\n",
        "  <thead>\n",
        "    <tr style=\"text-align: right;\">\n",
        "      <th></th>\n",
        "      <th>author</th>\n",
        "      <th>comments</th>\n",
        "      <th>downvotes</th>\n",
        "      <th>id</th>\n",
        "      <th>karma</th>\n",
        "      <th>link_karma</th>\n",
        "      <th>score</th>\n",
        "      <th>selftext</th>\n",
        "      <th>subreddit</th>\n",
        "      <th>title</th>\n",
        "      <th>type</th>\n",
        "      <th>upvotes</th>\n",
        "    </tr>\n",
        "  </thead>\n",
        "  <tbody>\n",
        "    <tr>\n",
        "      <th>0</th>\n",
        "      <td>        lieface</td>\n",
        "      <td> 8435</td>\n",
        "      <td> 5998</td>\n",
        "      <td> zzz5p</td>\n",
        "      <td>    0</td>\n",
        "      <td>    0</td>\n",
        "      <td> 1989</td>\n",
        "      <td> For me its that Im happyI laugh and joke and s...</td>\n",
        "      <td>            AskReddit</td>\n",
        "      <td>               Whats a huge lie you tell everyday </td>\n",
        "      <td> top_all</td>\n",
        "      <td> 7987</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>1</th>\n",
        "      <td>    DrissPinker</td>\n",
        "      <td>  107</td>\n",
        "      <td>   98</td>\n",
        "      <td> zzw6x</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  489</td>\n",
        "      <td> Im your typical neckbearded loner redditor Thi...</td>\n",
        "      <td>                 tifu</td>\n",
        "      <td>                      TIFU by drinking my own piss</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  587</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>2</th>\n",
        "      <td> AaawwwYeeeaaah</td>\n",
        "      <td>   63</td>\n",
        "      <td> 1026</td>\n",
        "      <td> zzqsd</td>\n",
        "      <td>  NaN</td>\n",
        "      <td>  NaN</td>\n",
        "      <td> 2575</td>\n",
        "      <td> Four friends who hadnt seen each other in  yea...</td>\n",
        "      <td>                jokes</td>\n",
        "      <td>                           Four Friends At A Party</td>\n",
        "      <td> top_all</td>\n",
        "      <td> 3601</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>3</th>\n",
        "      <td>           imtk</td>\n",
        "      <td>  204</td>\n",
        "      <td>  209</td>\n",
        "      <td> zzmlr</td>\n",
        "      <td> 7336</td>\n",
        "      <td> 1121</td>\n",
        "      <td>  762</td>\n",
        "      <td> I work in teir  tech support for a major ISP M...</td>\n",
        "      <td> TalesFromTechsupport</td>\n",
        "      <td>                                      I work in IT</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  971</td>\n",
        "    </tr>\n",
        "    <tr>\n",
        "      <th>4</th>\n",
        "      <td>          nstan</td>\n",
        "      <td>   65</td>\n",
        "      <td>   84</td>\n",
        "      <td> zzmiq</td>\n",
        "      <td>  117</td>\n",
        "      <td>  745</td>\n",
        "      <td>  328</td>\n",
        "      <td> I recently got back from a transatlantic voyag...</td>\n",
        "      <td>        askhistorians</td>\n",
        "      <td> What did early European explorers think of bio...</td>\n",
        "      <td> top_all</td>\n",
        "      <td>  412</td>\n",
        "    </tr>\n",
        "  </tbody>\n",
        "</table>\n",
        "</div>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "           author  comments  downvotes     id  karma  link_karma  score  \\\n",
        "0         lieface      8435       5998  zzz5p      0           0   1989   \n",
        "1     DrissPinker       107         98  zzw6x    NaN         NaN    489   \n",
        "2  AaawwwYeeeaaah        63       1026  zzqsd    NaN         NaN   2575   \n",
        "3            imtk       204        209  zzmlr   7336        1121    762   \n",
        "4           nstan        65         84  zzmiq    117         745    328   \n",
        "\n",
        "                                            selftext             subreddit  \\\n",
        "0  For me its that Im happyI laugh and joke and s...             AskReddit   \n",
        "1  Im your typical neckbearded loner redditor Thi...                  tifu   \n",
        "2  Four friends who hadnt seen each other in  yea...                 jokes   \n",
        "3  I work in teir  tech support for a major ISP M...  TalesFromTechsupport   \n",
        "4  I recently got back from a transatlantic voyag...         askhistorians   \n",
        "\n",
        "                                               title     type  upvotes  \n",
        "0                Whats a huge lie you tell everyday   top_all     7987  \n",
        "1                       TIFU by drinking my own piss  top_all      587  \n",
        "2                            Four Friends At A Party  top_all     3601  \n",
        "3                                       I work in IT  top_all      971  \n",
        "4  What did early European explorers think of bio...  top_all      412  "
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}