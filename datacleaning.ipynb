{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/\" #Fill in your own\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Function to determine if float\n",
      "def isfloat(x):\n",
      "    try:\n",
      "        float(x)\n",
      "        return True\n",
      "    except:\n",
      "        return False\n",
      "\n",
      "#Function to clean strings\n",
      "def filterstr(x):\n",
      "    try:\n",
      "        y= ''.join(e for e in x if (e == ' ' or e.isalnum()) and not e.isdigit())\n",
      "        return y\n",
      "    except:\n",
      "        return ''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for f in csvfiles:\n",
      "    df = pd.read_csv(f, encoding='utf-8') # Top all is our training data set\n",
      "    print \"First length is\", len(df)\n",
      "    \n",
      "    #Filter out non-float comments\n",
      "    df=df[df['comments'].apply(lambda x: isfloat(x))]\n",
      "\n",
      "    #Filter out non-float downvotes\n",
      "    df=df[df['downvotes'].apply(lambda x: isfloat(x))]\n",
      "\n",
      "    #Filter out non-float score\n",
      "    df=df[df['score'].apply(lambda x: isfloat(x))]        \n",
      "\n",
      "    #Filter out non-float score\n",
      "    df=df[df['upvotes'].apply(lambda x: isfloat(x))] \n",
      "\n",
      "    #Filter numbers out of title strings\n",
      "    for i in range(len(df)):\n",
      "        df['title'][i] = filterstr(df['title'][i])\n",
      "\n",
      "    #Filter numbers out of author strings\n",
      "    for i in range(len(df['author'])):\n",
      "        df['author'][i] = filterstr(df['author'][i])\n",
      "        \n",
      "    #Filter numbers out of selftext strings\n",
      "    for i in range(len(df['selftext'])):\n",
      "        df['selftext'][i] = filterstr(df['selftext'][i])\n",
      "    \n",
      "    print \"Second\", len(df)\n",
      "    \n",
      "#df.to_csv(f, index=False, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "993\n",
        "993"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "995\n",
        "995"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "999\n",
        "999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "999\n",
        "999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "221\n",
        "221"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "994\n",
        "994"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "970\n",
        "970"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "931\n",
        "931"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "998\n",
        "998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "978\n",
        "978"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "628\n",
        "628"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "997\n",
        "997"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "976\n",
        "976"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1000\n",
        "1000"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "989\n",
        "989"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "999\n",
        "999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "452\n",
        "452"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "987"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "987"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "344\n",
        "344"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "1004\n",
        "1004"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "999\n",
        "999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "210\n",
        "210"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "996\n",
        "996"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "999\n",
        "999"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "290\n",
        "290"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "995\n",
        "995"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "998\n",
        "998"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "178\n",
        "178"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    authorkarma = []\n",
      "    linkkarma = []\n",
      "    count = 0\n",
      "    for author in df['author']:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % author\n",
      "            headers = {'User-agent': user_agent}\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            authorkarma.append(jsondata['data']['comment_karma'])\n",
      "            try:\n",
      "                linkkarma.append(jsondata['data']['link_karma'])\n",
      "            except:\n",
      "                linkkarma.append(0)\n",
      "            count += 1\n",
      "        except:\n",
      "            authorkarma.append(0)\n",
      "            linkkarma.append(0)\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count\n",
      "                \n",
      "    df['karma'] = authorkarma\n",
      "    df['link_karma'] = linkkarma\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to clean downloaded information\n",
      "def subset_subreddit(df):\n",
      "    df = df[df['distinguished'].apply(lambda x: x == None)] # Makes sure poster isn't a moderator\n",
      "    df = df[df['media'].apply(lambda x: x == None)] # Also makes sure post is text based\n",
      "    df = df.drop('distinguished',1)\n",
      "    df = df.drop('media', 1)\n",
      "    # Occasionally, text strings will be in areas where only ints should be. This takes care of that\n",
      "    df=df[df['comments'].apply(lambda x: isfloat(x))] \n",
      "    df=df[df['score'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df=df[df['upvotes'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df=df[df['downvotes'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df = df.drop_duplicates() #Remove duplicate entries, should they exist\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add karma seperately since it's slower\n",
      "#top = add_karma(top, user_agent, 20)\n",
      "#hot = add_karma(hot, user_agent, 20)\n",
      "#new = add_karma(new, user_agent, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Subset downloaded subreddits\n",
      "#top_all = subset_subreddit(top_all)\n",
      "#top_week = subset_subreddit(top_week)\n",
      "#top_day = subset_subreddit(top_day)\n",
      "#hot = subset_subreddit(hot)\n",
      "#new = subset_subreddit(new)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}