{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "#from myalchemy import MyAlchemy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def subreddit_json_parser(data, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt):\n",
      "        for post in data['data']['children']:\n",
      "            if post['kind'] == 't3':\n",
      "                for key, value in post['data'].items():\n",
      "                    if key == \"title\":\n",
      "                        title.append(value)\n",
      "                    elif key == \"id\":\n",
      "                        ids.append(str(value))\n",
      "                    elif key == \"ups\":\n",
      "                        upvts.append(value)\n",
      "                    elif key == \"downs\":\n",
      "                        downvts.append(value)\n",
      "                    elif key == \"author\":\n",
      "                        authors.append(value)\n",
      "                    elif key == \"num_comments\":\n",
      "                        comments.append(value)\n",
      "                    elif key == \"score\":\n",
      "                        score.append(value)\n",
      "                    elif key == \"media\":\n",
      "                        media.append(value)\n",
      "                    elif key == \"distinguished\":\n",
      "                        distin.append(value)\n",
      "                    elif key == \"selftext\":\n",
      "                        selftxt.append(value)\n",
      "                    else:\n",
      "                        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_subreddit_df(subreddit, sort_call, user_agent, n, t=None, api_call_limit=100, status=False):\n",
      "    '''\n",
      "    Builds a subreddit dataframe.\n",
      "    \n",
      "    Parameters --\n",
      "    subreddit: specifies which reddit subreddit to pull information from\n",
      "    sort_call: specifies whether you want to sort by top, hot, rising etc.\n",
      "    user_agent: so reddit won't reject our https requests\n",
      "    n: int describing how many top results one would like\n",
      "    t: specifies whether scope should be week, hour, day, year, etc.\n",
      "    api_call_limit: Reddit doesn't serve more than 100 results at a time\n",
      "    status: whether or not you want the status of the download printed in the console\n",
      "    \n",
      "    Returns --\n",
      "    \n",
      "    A pandas dataframe containing various bits of information about the top posts.\n",
      "    \n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/%s.json?' % (subreddit, sort_call) #Base api call\n",
      "    headers = {'User-agent': user_agent}\n",
      "    \n",
      "    #Empty lists for information we'll extract\n",
      "    ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt = [], [], [], [], [], [], [], [], [], []\n",
      "    \n",
      "    #Makes sure n and api_call_limit aren't floats!\n",
      "    n = int(n) \n",
      "    api_call_limit = int(api_call_limit)\n",
      "    \n",
      "    #Since reddit only provides <= 100 calls at a time, looks at n requested and splits it up into different requests\n",
      "    if n%api_call_limit != 0:\n",
      "        remainder = n%api_call_limit\n",
      "        num = (n/api_call_limit) +1\n",
      "    else:\n",
      "        num = n/api_call_limit\n",
      "        remainder = api_call_limit\n",
      "\n",
      "    #Makes an api call for all n entries based on the api call limit\n",
      "    for i in range(num):\n",
      "        if i == 0:\n",
      "            post_params = {'limit': api_call_limit} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        elif i == num - 1:\n",
      "            post_params = {'limit': remainder, 'after': tostartfrom} #Indicates the post after we wish to call from\n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        else: \n",
      "            post_params = {'limit': api_call_limit, 'after': tostartfrom} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "    \n",
      "    tempdict = {'id': ids, 'title': title, 'upvotes': upvts, 'media': media, 'distinguished' : distin,\\\n",
      "                'selftext': selftxt, 'downvotes': downvts, 'comments': comments, 'score': score, 'author': authors}\n",
      "\n",
      "    return pd.DataFrame(tempdict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    authorkarma = []\n",
      "    linkkarma = []\n",
      "    count = 0\n",
      "    for author in df['author']:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % author\n",
      "            headers = {'User-agent': user_agent}\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            authorkarma.append(jsondata['data']['comment_karma'])\n",
      "            try:\n",
      "                linkkarma.append(jsondata['data']['link_karma'])\n",
      "            except:\n",
      "                linkkarma.append(0)\n",
      "            count += 1\n",
      "        except:\n",
      "            authorkarma.append(0)\n",
      "            linkkarma.append(0)\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count\n",
      "                \n",
      "    df['karma'] = authorkarma\n",
      "    df['link_karma'] = linkkarma\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Subset out the moderators and media posts and drop those columns\n",
      "def subset_subreddit(df):\n",
      "    df = df[df['distinguished'].apply(lambda x: x == None)]\n",
      "    df = df[df['media'].apply(lambda x: x == None)]\n",
      "    df = df.drop('distinguished',1)\n",
      "    df = df.drop('media', 1)\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Name of the given subreddit\n",
      "#subreddit = 'explainlikeimfive'\n",
      "subreddit = 'AskReddit'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Reddit only serves 1000 posts on a section of a subreddit..\n",
      "top = get_subreddit_df(subreddit, 'top', user_agent, 999, 'all', status=False)\n",
      "hot = get_subreddit_df(subreddit, 'hot', user_agent, 999, status=False)\n",
      "new = get_subreddit_df(subreddit, 'new', user_agent, 983, status=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "top = subset_subreddit(top)\n",
      "hot = subset_subreddit(hot)\n",
      "new = subset_subreddit(new)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add karma seperately since it's slower\n",
      "#top = add_karma(top, user_agent, 20)\n",
      "#hot = add_karma(hot, user_agent, 20)\n",
      "#new = add_karma(new, user_agent, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 11
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Write scraped subreddit information to a csv file\n",
      "dltop = subreddit + 'top.csv'\n",
      "dlhot = subreddit + 'hot.csv'\n",
      "dlnew = subreddit + 'new.csv'\n",
      "\n",
      "top.to_csv(dltop, encoding='utf-16', index=False)\n",
      "hot.to_csv(dlhot, encoding='utf-16', index=False)\n",
      "new.to_csv(dlnew, encoding='utf-16', index=False)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For more information on api call implemented in function below, please see the [reddit api page](http://www.reddit.com/dev/api#GET_comments_{article}). Currently, subcomments/replies are NOT included. To include them, check for key == replies.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "maybe do switch-case here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wrote my own Alchemy class. Initialize the class and enter concepts, keywords, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "#apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(top['id'])\n",
      "toptitles = list(top['title'])\n",
      "\n",
      "comments = get_comments(subreddit, topids[0], 'top', user_agent)\n",
      "\n",
      "comment_texts = list(comments['comment'])\n",
      "comment_texts =  comment_texts[1]\n",
      "\n",
      "toptitles = toptitles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "A class written by Jay Sayre for calling the Alchemy API\n",
      "'''\n",
      "'''\n",
      "import urllib\n",
      "import urllib2\n",
      "import json\n",
      "'''\n",
      "\n",
      "class MyAlchemy:\n",
      "    '''\n",
      "    Class for calling the alchemy API\n",
      "    '''\n",
      "    def __init__(self, apikey):\n",
      "        self.params = {'apikey' : apikey, 'outputMode' : 'json', 'showSourceText' : 0}\n",
      "    \n",
      "    def get_json(self, url):\n",
      "\tjsonrsp = urllib2.urlopen(url)\n",
      "\tdata = json.load(jsonrsp)\n",
      "\treturn data\n",
      "\n",
      "    def text_concepts(self):\n",
      "        returnlist = []\n",
      "        for item in self.data['concepts']: \n",
      "            #This is a website with information for the text - not in our model\n",
      "            #print item['dbpedia']\n",
      "            returnlist.append((item['relevance'], item['text']))\n",
      "        return returnlist\n",
      "            \n",
      "    def text_category(self):\n",
      "        return self.data['category'], self.data['language'], self.data['score'], self.data['status']\n",
      "        \n",
      "    def text_keywords(self):\n",
      "        returnlist = []\n",
      "        for item in self.data['keywords']: \n",
      "            returnlist.append((item['relevance'], item['text']))\n",
      "        return returnlist\n",
      "            \n",
      "    def text_sentiment(self):\n",
      "        return self.data['docSentiment']['mixed'], self.data['docSentiment']['score'], self.data['docSentiment']['type']\n",
      "    \n",
      "    def text_entities(self):\n",
      "        returnlist = []\n",
      "        for item in self.data['entities']:\n",
      "            returnlist.append((item['count'], item['relevance'], item['text'],  item['type']))\n",
      "        return returnlist\n",
      "            \n",
      "    def __look_at(self, choice):\n",
      "        if 'keywords' in choice:\n",
      "            return \"TextGetRankedKeywords\", 1\n",
      "        elif 'category' in choice:\n",
      "            return \"TextGetCategory\", 2\n",
      "        elif 'concepts' in choice:\n",
      "            return \"TextGetRankedConcepts\", 3\n",
      "        elif 'sentiment' in choice:\n",
      "            return \"TextGetTextSentiment\", 4\n",
      "        elif 'entities' in choice:\n",
      "            return \"TextGetRankedNamedEntities\", 5\n",
      "        else:\n",
      "            return None\n",
      "        \n",
      "    \n",
      "    def run_method(self, comment, whichtoget, otherparams=None):\n",
      "\tglobal alchemybase        \n",
      "\tself.params['text'] = comment\n",
      "        form = urllib.urlencode(self.params)\n",
      "        method, num = self.__look_at(whichtoget)\n",
      "        if method != None:\n",
      "            keywordsurl = \"http://access.alchemyapi.com/calls/text/\" + method + \"?\" + form\n",
      "        else:\n",
      "            print \"Couldn't recognize method\"\n",
      "            pass\n",
      "        #If there happens to be other parameters\n",
      "        if otherparams != None:\n",
      "            otherform = urllib.urlencode(postparams)\n",
      "            keywordsurl = alchemybase + whichtoget + \"?\" + form + otherform \n",
      "        \n",
      "        self.data = self.get_json(keywordsurl)\n",
      "        \n",
      "        if num == 1:\n",
      "            return self.text_keywords()\n",
      "        elif num == 2:\n",
      "            return self.text_category()\n",
      "        elif num == 3:\n",
      "            return self.text_concepts()\n",
      "        elif num == 4:\n",
      "            return self.text_sentiment()\n",
      "        else:\n",
      "            return self.text_entities()\n",
      "            \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 24
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "p= MyAlchemy(apikey)\n",
      "print p.run_method(toptitles, 'keywords')\n",
      "\n",
      "print  \"\\n And now for the concepts on the top comment \\n\"\n",
      "print p.run_method(comment_texts, 'concepts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'0.947909', u'secret'), (u'0.767525', u'time'), (u'0.678472', u'life')]\n",
        "\n",
        " And now for the concepts on the top comment \n",
        "\n",
        "[(u'0.938227', u'Bathroom'), (u'0.920839', u'Bathtub'), (u'0.871524', u'Shower'), (u'0.750148', u'Bathing'), (u'0.627417', u'The Wall'), (u'0.615729', u'Feces'), (u'0.614025', u'Shit'), (u'0.598437', u'Bathtub hoax')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}