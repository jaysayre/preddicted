{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime\n",
      "from myalchemy import MyAlchemy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def subreddit_json_parser(data, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt):\n",
      "        for post in data['data']['children']:\n",
      "            if post['kind'] == 't3':\n",
      "                for key, value in post['data'].items():\n",
      "                    if key == \"title\":\n",
      "                        title.append(value)\n",
      "                    elif key == \"id\":\n",
      "                        ids.append(str(value))\n",
      "                    elif key == \"ups\":\n",
      "                        upvts.append(value)\n",
      "                    elif key == \"downs\":\n",
      "                        downvts.append(value)\n",
      "                    elif key == \"author\":\n",
      "                        authors.append(value)\n",
      "                    elif key == \"num_comments\":\n",
      "                        comments.append(value)\n",
      "                    elif key == \"score\":\n",
      "                        score.append(value)\n",
      "                    elif key == \"media\":\n",
      "                        media.append(value)\n",
      "                    elif key == \"distinguished\":\n",
      "                        distin.append(value)\n",
      "                    elif key == \"selftext\":\n",
      "                        selftxt.append(value)\n",
      "                    else:\n",
      "                        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_subreddit_df(subreddit, sort_call, user_agent, n, t=None, api_call_limit=100, status=False):\n",
      "    '''\n",
      "    Builds a subreddit dataframe.\n",
      "    \n",
      "    Parameters --\n",
      "    subreddit: specifies which reddit subreddit to pull information from\n",
      "    sort_call: specifies whether you want to sort by top, hot, rising etc.\n",
      "    user_agent: so reddit won't reject our https requests\n",
      "    n: int describing how many top results one would like\n",
      "    t: specifies whether scope should be week, hour, day, year, etc.\n",
      "    api_call_limit: Reddit doesn't serve more than 100 results at a time\n",
      "    status: whether or not you want the status of the download printed in the console\n",
      "    \n",
      "    Returns --\n",
      "    \n",
      "    A pandas dataframe containing various bits of information about the top posts.\n",
      "    \n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/%s.json?' % (subreddit, sort_call) #Base api call\n",
      "    headers = {'User-agent': user_agent}\n",
      "    \n",
      "    #Empty lists for information we'll extract\n",
      "    ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt = [], [], [], [], [], [], [], [], [], []\n",
      "    \n",
      "    #Makes sure n and api_call_limit aren't floats!\n",
      "    n = int(n) \n",
      "    api_call_limit = int(api_call_limit)\n",
      "    \n",
      "    #Since reddit only provides <= 100 calls at a time, looks at n requested and splits it up into different requests\n",
      "    if n%api_call_limit != 0:\n",
      "        remainder = n%api_call_limit\n",
      "        num = (n/api_call_limit) +1\n",
      "    else:\n",
      "        num = n/api_call_limit\n",
      "        remainder = api_call_limit\n",
      "\n",
      "    #Makes an api call for all n entries based on the api call limit\n",
      "    for i in range(num):\n",
      "        if i == 0:\n",
      "            post_params = {'limit': api_call_limit} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        elif i == num - 1:\n",
      "            post_params = {'limit': remainder, 'after': tostartfrom} #Indicates the post after we wish to call from\n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        else: \n",
      "            post_params = {'limit': api_call_limit, 'after': tostartfrom} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "    \n",
      "    tempdict = {'id': ids, 'title': title, 'upvotes': upvts, 'media': media, 'distinguished' : distin,\\\n",
      "                'selftext': selftxt, 'downvotes': downvts, 'comments': comments, 'score': score, 'author': authors}\n",
      "\n",
      "    return pd.DataFrame(tempdict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Names of a few popular text-based subreddits\n",
      "subreddits = ['explainlikeimfive', 'AskReddit', 'TalesFromTechsupport', \n",
      "              'talesFromRetail', 'pettyrevenge', 'askhistorians', \n",
      "              'askscience', 'tifu', 'nosleep', 'jokes']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Append date stamp to txt file to indicate when download happened\n",
      "timeformat = '%m-%d-%y-%H%M'\n",
      "printdate = datetime.datetime.now().strftime(timeformat)\n",
      "    \n",
      "with open('Data/dltimestamp.txt', 'a') as the_file:\n",
      "    the_file.write(printdate)\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    print subreddit\n",
      "    \n",
      "    # Reddit only serves 1000 posts on a section of a subreddit..\n",
      "    top_all = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'all', status=False)\n",
      "    top_week = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'week', status=False)\n",
      "    top_day = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'day', status=False)\n",
      "    #hot = get_subreddit_df(subreddit, 'hot', user_agent, 1000, status=False)\n",
      "    new = get_subreddit_df(subreddit, 'new', user_agent, 1000, status=False)\n",
      "    \n",
      "    top_all['subreddit'] = subreddit\n",
      "    top_week['subreddit'] = subreddit\n",
      "    top_day['subreddit'] = subreddit\n",
      "    #hot['subreddit'] = subreddit\n",
      "    new['subreddit'] = subreddit\n",
      "    \n",
      "    #Write scraped subreddit information to a csv file\n",
      "    dltopall = 'Data/' + subreddit + 'top_all.csv'\n",
      "    dltopweek = 'Data/' + subreddit + 'top_week.csv'\n",
      "    dltopday = 'Data/' + subreddit + 'top_day.csv'\n",
      "    #dlhot = 'Data/' + subreddit + 'hot.csv'\n",
      "    dlnew = 'Data/' + subreddit + 'new.csv'\n",
      "    \n",
      "    top_all.to_csv(dltopall, index=False, encoding='utf-8')\n",
      "    top_week.to_csv(dltopweek, index=False, encoding='utf-8')\n",
      "    top_day.to_csv(dltopday, index=False, encoding='utf-8')\n",
      "    #hot.to_csv(dlhot, index=False, encoding='utf-8')\n",
      "    new.to_csv(dlnew, index=False, encoding='utf-8')\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "explainlikeimfive\n",
        "AskReddit"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "TalesFromTechsupport"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "talesFromRetail"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "pettyrevenge"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "askhistorians"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "askscience"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "tifu"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "nosleep"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "jokes"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For more information on api call implemented in function below, please see the [reddit api page](http://www.reddit.com/dev/api#GET_comments_{article}). Currently, subcomments/replies are NOT included. To include them, check for key == replies.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "maybe do switch-case here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wrote my own Alchemy class. Initialize the class and enter concepts, keywords, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "#apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(top['id'])\n",
      "toptitles = list(top['title'])\n",
      "\n",
      "comments = get_comments(subreddit, topids[0], 'top', user_agent)\n",
      "\n",
      "comment_texts = list(comments['comment'])\n",
      "comment_texts =  comment_texts[1]\n",
      "\n",
      "toptitles = toptitles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "p= MyAlchemy(apikey)\n",
      "print p.run_method(toptitles, 'keywords')\n",
      "\n",
      "print  \"\\n And now for the concepts on the top comment \\n\"\n",
      "print p.run_method(comment_texts, 'concepts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'0.947909', u'secret'), (u'0.767525', u'time'), (u'0.678472', u'life')]\n",
        "\n",
        " And now for the concepts on the top comment \n",
        "\n",
        "[(u'0.938227', u'Bathroom'), (u'0.920839', u'Bathtub'), (u'0.871524', u'Shower'), (u'0.750148', u'Bathing'), (u'0.627417', u'The Wall'), (u'0.615729', u'Feces'), (u'0.614025', u'Shit'), (u'0.598437', u'Bathtub hoax')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}