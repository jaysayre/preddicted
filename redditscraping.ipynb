{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime\n",
      "from myalchemy import MyAlchemy"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def subreddit_json_parser(data, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt):\n",
      "        for post in data['data']['children']:\n",
      "            if post['kind'] == 't3':\n",
      "                for key, value in post['data'].items():\n",
      "                    if key == \"title\":\n",
      "                        title.append(value)\n",
      "                    elif key == \"id\":\n",
      "                        ids.append(str(value))\n",
      "                    elif key == \"ups\":\n",
      "                        upvts.append(value)\n",
      "                    elif key == \"downs\":\n",
      "                        downvts.append(value)\n",
      "                    elif key == \"author\":\n",
      "                        authors.append(value)\n",
      "                    elif key == \"num_comments\":\n",
      "                        comments.append(value)\n",
      "                    elif key == \"score\":\n",
      "                        score.append(value)\n",
      "                    elif key == \"media\":\n",
      "                        media.append(value)\n",
      "                    elif key == \"distinguished\":\n",
      "                        distin.append(value)\n",
      "                    elif key == \"selftext\":\n",
      "                        selftxt.append(value)\n",
      "                    else:\n",
      "                        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 4
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_subreddit_df(subreddit, sort_call, user_agent, n, t=None, api_call_limit=100, status=False):\n",
      "    '''\n",
      "    Builds a subreddit dataframe.\n",
      "    \n",
      "    Parameters --\n",
      "    subreddit: specifies which reddit subreddit to pull information from\n",
      "    sort_call: specifies whether you want to sort by top, hot, rising etc.\n",
      "    user_agent: so reddit won't reject our https requests\n",
      "    n: int describing how many top results one would like\n",
      "    t: specifies whether scope should be week, hour, day, year, etc.\n",
      "    api_call_limit: Reddit doesn't serve more than 100 results at a time\n",
      "    status: whether or not you want the status of the download printed in the console\n",
      "    \n",
      "    Returns --\n",
      "    \n",
      "    A pandas dataframe containing various bits of information about the top posts.\n",
      "    \n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/%s.json?' % (subreddit, sort_call) #Base api call\n",
      "    headers = {'User-agent': user_agent}\n",
      "    \n",
      "    #Empty lists for information we'll extract\n",
      "    ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt = [], [], [], [], [], [], [], [], [], []\n",
      "    \n",
      "    #Makes sure n and api_call_limit aren't floats!\n",
      "    n = int(n) \n",
      "    api_call_limit = int(api_call_limit)\n",
      "    \n",
      "    #Since reddit only provides <= 100 calls at a time, looks at n requested and splits it up into different requests\n",
      "    if n%api_call_limit != 0:\n",
      "        remainder = n%api_call_limit\n",
      "        num = (n/api_call_limit) +1\n",
      "    else:\n",
      "        num = n/api_call_limit\n",
      "        remainder = api_call_limit\n",
      "\n",
      "    #Makes an api call for all n entries based on the api call limit\n",
      "    for i in range(num):\n",
      "        if i == 0:\n",
      "            post_params = {'limit': api_call_limit} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        elif i == num - 1:\n",
      "            post_params = {'limit': remainder, 'after': tostartfrom} #Indicates the post after we wish to call from\n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "        else: \n",
      "            post_params = {'limit': api_call_limit, 'after': tostartfrom} \n",
      "            jsondata = json_extract(reddit_base, headers, post_params, t)\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            time.sleep(2)\n",
      "            if status == True:\n",
      "                print \"Downloaded %s posts...\" % len(set(ids))\n",
      "    \n",
      "    tempdict = {'id': ids, 'title': title, 'upvotes': upvts, 'media': media, 'distinguished' : distin,\\\n",
      "                'selftext': selftxt, 'downvotes': downvts, 'comments': comments, 'score': score, 'author': authors}\n",
      "\n",
      "    return pd.DataFrame(tempdict)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 5
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    authorkarma = []\n",
      "    linkkarma = []\n",
      "    count = 0\n",
      "    for author in df['author']:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % author\n",
      "            headers = {'User-agent': user_agent}\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            authorkarma.append(jsondata['data']['comment_karma'])\n",
      "            try:\n",
      "                linkkarma.append(jsondata['data']['link_karma'])\n",
      "            except:\n",
      "                linkkarma.append(0)\n",
      "            count += 1\n",
      "        except:\n",
      "            authorkarma.append(0)\n",
      "            linkkarma.append(0)\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count\n",
      "                \n",
      "    df['karma'] = authorkarma\n",
      "    df['link_karma'] = linkkarma\n",
      "    return df\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to detect float\n",
      "def isfloat(x):\n",
      "    try:\n",
      "        float(x)\n",
      "        return True\n",
      "    except:\n",
      "        return False"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Function to clean downloaded information\n",
      "def subset_subreddit(df):\n",
      "    df = df[df['distinguished'].apply(lambda x: x == None)] # Makes sure poster isn't a moderator\n",
      "    df = df[df['media'].apply(lambda x: x == None)] # Also makes sure post is text based\n",
      "    df = df.drop('distinguished',1)\n",
      "    df = df.drop('media', 1)\n",
      "    # Occasionally, text strings will be in areas where only ints should be. This takes care of that\n",
      "    df=df[df['comments'].apply(lambda x: isfloat(x))] \n",
      "    df=df[df['score'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df=df[df['upvotes'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df=df[df['downvotes'].apply(lambda x: isinstance(x, basestring) == False)]\n",
      "    df = df.drop_duplicates() #Remove duplicate entries, should they exist\n",
      "    return df"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Names of a few popular text-based subreddits\n",
      "subreddits = ['explainlikeimfive', 'AskReddit', 'TalesFromTechsupport', \n",
      "              'talesFromRetail', 'pettyrevenge', 'askhistorians', \n",
      "              'askscience', 'tifu', 'nosleep', 'jokes']"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Append date stamp to txt file to indicate when download happened\n",
      "timeformat = '%m-%d-%y-%H%M'\n",
      "printdate = datetime.datetime.now().strftime(timeformat)\n",
      "    \n",
      "with open('Data/dltimestamp.txt', 'a') as the_file:\n",
      "    the_file.write(printdate)\n",
      "\n",
      "for subreddit in subreddits:\n",
      "    print subreddit\n",
      "    \n",
      "    # Reddit only serves 1000 posts on a section of a subreddit..\n",
      "    top_all = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'all', status=False)\n",
      "    top_week = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'week', status=False)\n",
      "    top_day = get_subreddit_df(subreddit, 'top', user_agent, 1000, 'day', status=False)\n",
      "    #hot = get_subreddit_df(subreddit, 'hot', user_agent, 1000, status=False)\n",
      "    new = get_subreddit_df(subreddit, 'new', user_agent, 1000, status=False)\n",
      "    \n",
      "    #Subset downloaded subreddits\n",
      "    #top_all = subset_subreddit(top_all)\n",
      "    #top_week = subset_subreddit(top_week)\n",
      "    #hot = subset_subreddit(hot)\n",
      "    #new = subset_subreddit(new)\n",
      "    \n",
      "    top_all['subreddit'] = subreddit\n",
      "    top_week['subreddit'] = subreddit\n",
      "    top_day['subreddit'] = subreddit\n",
      "    #hot['subreddit'] = subreddit\n",
      "    new['subreddit'] = subreddit\n",
      "    \n",
      "    #Write scraped subreddit information to a csv file\n",
      "    dltopall = 'Data/' + subreddit + 'top_all.csv'\n",
      "    dltopweek = 'Data/' + subreddit + 'top_week.csv'\n",
      "    dltopday = 'Data/' + subreddit + 'top_day.csv'\n",
      "    #dlhot = 'Data/' + subreddit + 'hot.csv'\n",
      "    dlnew = 'Data/' + subreddit + 'new.csv'\n",
      "    \n",
      "    top_all.to_csv(dltopall, index=False, encoding='utf-8')\n",
      "    top_week.to_csv(dltopweek, index=False, encoding='utf-8')\n",
      "    top_day.to_csv(dltopday, index=False, encoding='utf-8')\n",
      "    #hot.to_csv(dlhot, index=False, encoding='utf-8')\n",
      "    new.to_csv(dlnew, index=False, encoding='utf-8')\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": "*"
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add karma seperately since it's slower\n",
      "#top = add_karma(top, user_agent, 20)\n",
      "#hot = add_karma(hot, user_agent, 20)\n",
      "#new = add_karma(new, user_agent, 20)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "For more information on api call implemented in function below, please see the [reddit api page](http://www.reddit.com/dev/api#GET_comments_{article}). Currently, subcomments/replies are NOT included. To include them, check for key == replies.\n"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "maybe do switch-case here"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    ids.pop(0)\n",
      "    datadict = {'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, 'distinguished': distin}\n",
      "    return pd.DataFrame(datadict)\n",
      "    \n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Wrote my own Alchemy class. Initialize the class and enter concepts, keywords, etc.."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "#apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 14
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(top['id'])\n",
      "toptitles = list(top['title'])\n",
      "\n",
      "comments = get_comments(subreddit, topids[0], 'top', user_agent)\n",
      "\n",
      "comment_texts = list(comments['comment'])\n",
      "comment_texts =  comment_texts[1]\n",
      "\n",
      "toptitles = toptitles[0]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 15
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Concepts, keywords, category, sentiment, entities\n",
      "\n",
      "p= MyAlchemy(apikey)\n",
      "print p.run_method(toptitles, 'keywords')\n",
      "\n",
      "print  \"\\n And now for the concepts on the top comment \\n\"\n",
      "print p.run_method(comment_texts, 'concepts')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[(u'0.947909', u'secret'), (u'0.767525', u'time'), (u'0.678472', u'life')]\n",
        "\n",
        " And now for the concepts on the top comment \n",
        "\n",
        "[(u'0.938227', u'Bathroom'), (u'0.920839', u'Bathtub'), (u'0.871524', u'Shower'), (u'0.750148', u'Bathing'), (u'0.627417', u'The Wall'), (u'0.615729', u'Feces'), (u'0.614025', u'Shit'), (u'0.598437', u'Bathtub hoax')]"
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}