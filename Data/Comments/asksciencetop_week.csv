author,comment,distinguished,downs,id,post,subreddit,type,ups
Farnswirth,"Heat engines require a heat differential to function, by definition.  

However, what /u/JimmyGroove said was correct as well, and is merely another way of stating the first law, which is:  The change in total internal energy in a system (in this case an engine) is equal to the heat in, minus the heat out, minus the work done by the engine.  This is one of the most fundamental principles of thermodynamics, [the first law.](http://en.wikipedia.org/wiki/First_law_of_thermodynamics)

For instance, you could have a very energetic system with no heat differential (eg. a cold car with a full tank of gasoline).  In this case, the heat inside the system (the car) would be at thermal equilibrium with the outside air because you haven't started the engine yet.  In this case, there would be no heat differential in the beginning.  The net reaction for the car as a system would be a conversion of internal energy (the chemical energy of the gasoline) into work.  *It's all a question of how you define your system.*  

So to answer your question - a heat engine requires a heat differential, by definition, because that's how a heat engine works.  But depending on how you define your ""engine"" (thermodynamic system) - you don't necessarily need a heat differential- for work to be produced.  But the energy must come from somewhere, and it must have somewhere to go.

Edit: Another good example of a system that does not require a heat differential to do work is a fuel cell, which directly converts chemical energy into electrical energy with no heat differential.  Obviously this isn't a heat engine, but you could definite it as an ""engine"", and it is used as such in some vehicles. ",null,1,cdkwnan,1r90aa,askscience,top_week,8
JimmyGroove,"You only get a net flow of heat from areas of high temperature to low, so a heat engine could only work off ambient heat if the ambient temperature was different from that of the engine (or can be manipulated into being different, the most commonly used way being to change the pressure of a gas).",null,0,cdkt7bu,1r90aa,askscience,top_week,3
theoreoman,"Law of thermodynamics says that heat can only flow from hot to cold, so for work to happen the heat needs to flow from hot to cold, the larger the differential the more work that can be done.  Quantum mechanics might have a different answer that I'm not aware of",null,0,cdkwkpo,1r90aa,askscience,top_week,1
Jonex,"Not really answering your question - as it's already answered - but an interesting addition:
You can extract ambient heat energy without having a heat differential - if you add more energy than you extract. This is used in earth heating systems where energy is taken from the ground at a few degres to add to the indoor heating at around 20 degrees.

To to this you need an heat pump, powered by for instance elecricity. There are physical limitations of efficiancy. But it's a popular way to increase the efficiancy of electricaly powered heating.

I'm a bit tired so not an awesome explanation, someone who has done their thermodynamics more recently can hopefully expand and clear things up a bit.",null,0,cdl4g2a,1r90aa,askscience,top_week,1
Updatebjarni,"Consider the fact that not only are the electrons in the extension cord running back and forth since the cord carries AC, but it's also running down one conductor while it is running up the other inside the cord. So regardless of exactly how electrons behave with respect to gravity in a conductor, that would seem to answer the question.

Same with the battery; there is one conductor carrying electrons from the negative terminal of the battery to ground, and one carrying them from ground to the positive terminal.
",null,1,cdl15ac,1r933v,askscience,top_week,12
iorgfeflkd,The voltage required to move an electron through a gravitational field is very small but nonzero. It is about 5x10^-11 Volts/meter.,null,2,cdl1mfs,1r933v,askscience,top_week,10
sporclesam,"If you mean [Sulphate-reducing bacteria](http://en.wikipedia.org/wiki/Sulfate-reducing_bacteria) (which use sulphate and not sulphur as terminal acceptor) then, yes, but not like us but more like plants/algae. These anaerobes use dissimilatory sulfate reduction to obtain sulphide as waste (somewhat similar to plants releasing oxygen from water)

Check out links on [chemosynthesis] (http://en.wikipedia.org/wiki/Chemosynthesis) *vis-a-vis* photosynthesis. ",null,0,cdl3l9h,1r99pm,askscience,top_week,2
Platypuskeeper,"Not really, they reduce SOx to H2S, while we reduce O2 to H2O. H2S doesn't perform any functions beyond that, while we use water for quite a lot of other things. Sulfur-reducing bacteria still use water for all those other things.

It's also a very different enzyme and mechanism. 
",null,1,cdl64y4,1r99pm,askscience,top_week,2
dudley_love,"Great question, putting the finger on the limit of the SF theory. 

One variation of the SF is [Yanagida](http://www.ncbi.nlm.nih.gov/pubmed/2082730)'s, where the actin-myosin crossbridge is not a solid-ish state, but a ""loose"", transient one. 

So instead of a ladder with strong actin ""hands"", you have a ratchet with potential for slippage.",null,0,cdmjgkh,1r9a7m,askscience,top_week,4
KarlOskar12,"[Eccentric contraction](http://medical-dictionary.thefreedictionary.com/eccentric+contraction) happens when you apply force to the muscle through its range of motion while the muscles contractile force is less than the applied for causing the muscle to stretch. The farther you stretch  the [muscle](http://puu.sh/5qXWU.jpg), the less myosin heads will be able to bind to the thick filaments and the amount of force the muscle can produce rapidly decreases as the stretch progresses.

This [article](http://muscle.ucsd.edu/musintro/contractions.shtml) explains it pretty well.",null,2,cdl90c6,1r9a7m,askscience,top_week,2
Daegs,"no one says it ""must"" be. 

There are 4 forces: strong, weak, electromagnetic and gravity.

The first 3 are all very similar, and in fact the weak / electromagnetic have been joined into a single force, the electro-weak. It *seems* that the strong also fits in very well mathematically, and it is expected in the coming years / decades that we could actually integrate all 3 (strong, weak, electric) into a single force. This is called ""Grand Unification"":

http://en.wikipedia.org/wiki/Grand_Unified_Theory

So the problem is asking why are 3 out of 4 of the forces so similar to the point where they can actually be shown to be the same underlying force, while gravity is such an oddball and so much weaker. 

One *possible* explanation is that all 4 of the forces are equal in strength, and could be unified into a single force, while gravity is special because it ""leaks"" its force to an unseen dimension. This would allow it, in theory, to be unified with the other 3 forces.

There are other explanations, including that gravity is an emergent property of our universe (such as holographic universe theory) rather than a fundamental force.

Or, it could simply be that gravity is fundamentally different from the other 3 forces, which would just be troubling for physicist.

Without an explanation such as extra dimensions or more exotic behavior we haven't seen, there isn't going to be a way to unify gravity with the other 3 forces.",null,4,cdkxnpf,1r9c24,askscience,top_week,17
iorgfeflkd,"I'm not sure I understand your question. Did your teacher say that gravity needs to be as strong as the others, despite not being?

It is thought that at extremely short distances, there is so much energy from interactions of other forces (consider two electrons very close together for example) that the energy itself starts to gravitate. At these scales (typically, Planck scales), gravity would overwhelm other interactions. However, we don't have any way of testing this.",null,0,cdkwv4t,1r9c24,askscience,top_week,2
theoreoman,"Gravity is a very weak force and  it takes an entire planet to counteract an electrical charge on a small object.  This thing is no one knows why exactly why yet, there are theories but one of the holy grails of physics it to figure out how gravity relates mathematically to the other forces ",null,1,cdkwzlp,1r9c24,askscience,top_week,2
OverlordQuasar,"Essentially, it seems out of place. We have 2 forces that we have demonstrated via experimentation to be part of one underlying force (electromagnetism and the weak force becoming the electroweak force), another that has been mathematically shown to be part of that, which is awaiting experimental confirmation which requires a bit higher energy particle accelerators (strong force). All these are part of one thing, but gravity isn't. It is so much weaker, and refuses all efforts to unify it with the others mathematically without resorting to things on the level of higher dimensions.

TL;DR It's out of place to have 3 forces that are easy to unify and one that is much weaker and appears to be completely seperate.",null,0,cdl2y71,1r9c24,askscience,top_week,1
nairebis,"That's all in the BIOS (Basic Input/Output System), which is a program stored in a special chip on the motherboard. That provides a standardized interface between the hardware and the operating system, which is why you can load Windows on any Brand-X motherboard. OS/X has the same concept, though of course Apple only officially supports certain motherboards.

How much power it uses would be dependent on the motherboard design, but if it uses electronic starting, then it uses some small amount of power to monitor the button. I believe all modern motherboards do it this way. In the relatively distant past, they used to use a mechanical on-off switch, but there were a lot of advantages to electronic power on/off, so that became standard.

Whether you can use a different key, etc, is whether that feature is programmed into the BIOS. Here is a page that describes typical BIOS settings:

http://www.tomshardware.com/reviews/bios-beginners,1126-8.html

BIOS settings are usually accessed by pressing a special key when you start up the machine, but before it starts loading the operating system. I've typically seen the DEL key, the F2 key and the F8 key. I don't know how Apple does it. If it works like how Apple does other things, then they picked some arbitrary key that no one else uses. :)

Edit: As haikuginger pointed out, the more modern version of BIOS is EFI, which allows changing some settings from within the Operation System (particularly in the case of Macs). But the principle is the same, the controlling program here comes from the motherboard firmware.",null,0,cdlach2,1r9c6s,askscience,top_week,4
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,top_week,17
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,top_week,18
RelativisticMechanic,"Note that when dealing with roots and complex numbers, you actually get multiple results. For example, -1^(1/2) actually has two values:

1. i
2. -i

Similarly, -1^(1/3) has three values:

1. -1
2. cos(π/3) + i\*sin(π/3)
3. cos(π/3) - i\*sin(π/3)

Finally, note that 1/2.5 = 2/5. Thus, (-1)^(1/2.5) = (-1)^(2/5) = 1^(1/5). We therefore need to find the fifth roots of 1. There are five of them, and they are

1. 1
2. cos(2π/5) + i\*sin(2π/5)
3. cos(2π/5) - i\*sin(2π/5)
4. cos(4π/5) + i\*sin(4π/5)
5. cos(4π/5) - i\*sin(4π/5)

Any of these numbers satisfies x^(5/2) = -1 (and x^(5/2) = 1), since they all satisfy x^5 = 1, and -1 satisfies -1^2 = 1.",null,44,cdkxquq,1r9elk,askscience,top_week,334
Tsien,"To elaborate on what RelativisticMechanic wrote, the roots come from Euler's formula: e^(ix) = cosx + isinx. If you're familiar with Taylor series, this formula can be derived from taking Taylor expansions around x = 0 for e^(ix), cosx, and sinx:  

e^(ix) = 1 + ix - x^2 /2! - ix^3 /3! + x^4 /4! + ix^5 /5! - ...  
cosx   = 1 - x^2 /2! + x^4 /4! - x^6 /6! + ...  
isinx   = ix - ix^3 /3! + ix^5 /5! - ix^7 /7! + ...  

So, using this formula, we know that e^i\*2kpi = 1 where k is an integer. So 1^(1/5) becomes

e^i\*2kpi/5 = cos(2kpi/5) + isin(2kpi/5)

Using k = 0, 1, -1, 2, -2 (in that order) gives RelativisticMechanic's answers. Notice that since cos and sin are 2-pi periodic, if you try to use other values of k, you'll end up getting one of the 5 answers already listed.",null,3,cdkyeo1,1r9elk,askscience,top_week,15
regnirps,"This should help you out: [Graph of (-1)^x in Wolfram Alpha.](http://www.wolframalpha.com/input/?i=%28-1%29%5Ex)  This graph show the real vs. imaginary parts of the powers of (-1).  Notice that (-1)^(1/2.5) is in the part of the graph with nonzero real *and* nonzero imaginary parts!

In general, I think your question has to do with the properties of the function f(x) = a^x, but I'm not exactly sure what explanation you want for ""why"" beyond the graph linked above.",null,5,cdkx2fk,1r9elk,askscience,top_week,17
SidusObscurus,"For this question, you have to compare an inverse function, as defined by convention, with the solutions to an equation involving a function that is not one-to-one (invertible).

In order to define a function, you need exactly one output for each input. For possible multivalued functions, we select one solution completely by convention (sqrt function, inverse trig functions, and many others).

Solving x^2 = -1 has two answers, +i and -i, both imaginary.
Solving x^3 = -1 has three answers, e^(i*pi/3), -1 = e^(i*pi/3 +2*pi/3), and e^(i*pi/3 +4*pi/3).

To define the inverse functions of these equations, we can only pick one of these answers, so we pick one answer entirely based on mathematical convention and general agreement. Typically we pick the real number solutions first if we can (ex. cube root), positive part solutions after this if we can (ex. sqrt), and if it is still multivalued pick the solution set that intersects 0 if we can (ex. inverse trig functions). This defines the inverse functions x^(1/2) and x^(1/3). If we cannot do any of those, it's less clear what we pick, and mathematicians will usually explicitly state the solution branch they are using for their math, so no one is confused.

What about x^(2.5)? Once you have defined all the integer rational roots and powers, the standard convention is to interpret x^(5/2) as either of the two equal expressions,
sqrt(x^5) = sqrt(x)^5.
There is no ambiguity in these expressions. We could also solve
x^(2/5) = -1, which would have multiple solutions. Our solutions would be
e^(i*pi/2.5 + 2*pi*k/2.5) for each integer k.
See [Roots of Unity](http://en.wikipedia.org/wiki/Root_of_unity) and [Euler's Identity](http://en.wikipedia.org/wiki/Euler%27s_Identity) for more information. In this case, we would only have 5 total solutions, due to the 5th root.

For irrational numbers, everything gets a little more complicated, as irrational powers of negative numbers aren't very well defined. But that's another issue entirely.

*Edit: Corrected a really silly mistake of mine.*",null,0,cdl8h2z,1r9elk,askscience,top_week,3
Borlaug,"When the exponent is a fraction, they want you to find the root of the coefficient using the denominator as the radical. 

For the first one, no known number when multipled by itself results to -1. In other words, the square root of -1 is imaginary. This imaginary is represented by the letter i. 

The second one is asking for the cube root of -1, which is -1. i. e. -1*-1 *-1=-1",null,3,cdl6se0,1r9elk,askscience,top_week,5
RelativisticMechanic,"Note that when dealing with roots and complex numbers, you actually get multiple results. For example, -1^(1/2) actually has two values:

1. i
2. -i

Similarly, -1^(1/3) has three values:

1. -1
2. cos(π/3) + i\*sin(π/3)
3. cos(π/3) - i\*sin(π/3)

Finally, note that 1/2.5 = 2/5. Thus, (-1)^(1/2.5) = (-1)^(2/5) = 1^(1/5). We therefore need to find the fifth roots of 1. There are five of them, and they are

1. 1
2. cos(2π/5) + i\*sin(2π/5)
3. cos(2π/5) - i\*sin(2π/5)
4. cos(4π/5) + i\*sin(4π/5)
5. cos(4π/5) - i\*sin(4π/5)

Any of these numbers satisfies x^(5/2) = -1 (and x^(5/2) = 1), since they all satisfy x^5 = 1, and -1 satisfies -1^2 = 1.",null,44,cdkxquq,1r9elk,askscience,top_week,334
Tsien,"To elaborate on what RelativisticMechanic wrote, the roots come from Euler's formula: e^(ix) = cosx + isinx. If you're familiar with Taylor series, this formula can be derived from taking Taylor expansions around x = 0 for e^(ix), cosx, and sinx:  

e^(ix) = 1 + ix - x^2 /2! - ix^3 /3! + x^4 /4! + ix^5 /5! - ...  
cosx   = 1 - x^2 /2! + x^4 /4! - x^6 /6! + ...  
isinx   = ix - ix^3 /3! + ix^5 /5! - ix^7 /7! + ...  

So, using this formula, we know that e^i\*2kpi = 1 where k is an integer. So 1^(1/5) becomes

e^i\*2kpi/5 = cos(2kpi/5) + isin(2kpi/5)

Using k = 0, 1, -1, 2, -2 (in that order) gives RelativisticMechanic's answers. Notice that since cos and sin are 2-pi periodic, if you try to use other values of k, you'll end up getting one of the 5 answers already listed.",null,3,cdkyeo1,1r9elk,askscience,top_week,15
regnirps,"This should help you out: [Graph of (-1)^x in Wolfram Alpha.](http://www.wolframalpha.com/input/?i=%28-1%29%5Ex)  This graph show the real vs. imaginary parts of the powers of (-1).  Notice that (-1)^(1/2.5) is in the part of the graph with nonzero real *and* nonzero imaginary parts!

In general, I think your question has to do with the properties of the function f(x) = a^x, but I'm not exactly sure what explanation you want for ""why"" beyond the graph linked above.",null,5,cdkx2fk,1r9elk,askscience,top_week,17
SidusObscurus,"For this question, you have to compare an inverse function, as defined by convention, with the solutions to an equation involving a function that is not one-to-one (invertible).

In order to define a function, you need exactly one output for each input. For possible multivalued functions, we select one solution completely by convention (sqrt function, inverse trig functions, and many others).

Solving x^2 = -1 has two answers, +i and -i, both imaginary.
Solving x^3 = -1 has three answers, e^(i*pi/3), -1 = e^(i*pi/3 +2*pi/3), and e^(i*pi/3 +4*pi/3).

To define the inverse functions of these equations, we can only pick one of these answers, so we pick one answer entirely based on mathematical convention and general agreement. Typically we pick the real number solutions first if we can (ex. cube root), positive part solutions after this if we can (ex. sqrt), and if it is still multivalued pick the solution set that intersects 0 if we can (ex. inverse trig functions). This defines the inverse functions x^(1/2) and x^(1/3). If we cannot do any of those, it's less clear what we pick, and mathematicians will usually explicitly state the solution branch they are using for their math, so no one is confused.

What about x^(2.5)? Once you have defined all the integer rational roots and powers, the standard convention is to interpret x^(5/2) as either of the two equal expressions,
sqrt(x^5) = sqrt(x)^5.
There is no ambiguity in these expressions. We could also solve
x^(2/5) = -1, which would have multiple solutions. Our solutions would be
e^(i*pi/2.5 + 2*pi*k/2.5) for each integer k.
See [Roots of Unity](http://en.wikipedia.org/wiki/Root_of_unity) and [Euler's Identity](http://en.wikipedia.org/wiki/Euler%27s_Identity) for more information. In this case, we would only have 5 total solutions, due to the 5th root.

For irrational numbers, everything gets a little more complicated, as irrational powers of negative numbers aren't very well defined. But that's another issue entirely.

*Edit: Corrected a really silly mistake of mine.*",null,0,cdl8h2z,1r9elk,askscience,top_week,3
Borlaug,"When the exponent is a fraction, they want you to find the root of the coefficient using the denominator as the radical. 

For the first one, no known number when multipled by itself results to -1. In other words, the square root of -1 is imaginary. This imaginary is represented by the letter i. 

The second one is asking for the cube root of -1, which is -1. i. e. -1*-1 *-1=-1",null,3,cdl6se0,1r9elk,askscience,top_week,5
ekohfa,"Solar photovoltaic panels create current due to the [photoelectric effect](http://en.wikipedia.org/wiki/Photoelectric_effect).  Incoming photons cause electrons to jump across the energy ""band gap"" in a semiconductor, typically made of silicon.  The silicon consists of a positively doped layer and a negatively doped layer, just like a diode.  Metal leads are applied to each side to allow electrical current to flow to an external circuit.

Edit: To be clear, the metal leads are not oxidized; they are present only to carry the current created in the semiconductor.",null,2,cdkynzl,1r9es2,askscience,top_week,4
hal2k1,"&gt; Where are these electrons sourced from?

This is a misconception. Electrical current is charge flowing in a [circuit](http://en.wikipedia.org/wiki/Electrical_circuit).

&gt; An electrical circuit is a network consisting of a closed loop, giving a return path for the current.

A circuit is a [**loop** of conductor](http://en.wikipedia.org/wiki/File:Ohm%27s_Law_with_Voltage_source_TeX.svg). 

The carriers of charge are normally electrons (as your question suggests). The electrons are already part of the conductor. A current is merely a flow, or movement, of charge (electrons) around the loop of conductor (the circuit).

The solar panel merely converts photons into a ""push"" for the charge carriers (electrons), like a pump does for a fluid. The solar panel then ""pushes"" the electrons (which are already in the conductors) around the circuit. In electrical circuits, we do not call this pushing force ""pressure"", but rather we use the term ""voltage"".",null,0,cdl4dv3,1r9es2,askscience,top_week,1
jayd42,"My understanding of current is that the 'flow' is actually photons moving between electrons as the electrons change energy levels, gaining photons to raise in energy and losing photons to lower in energy, and not the physical movement of electrons through a material.

From this understanding, solar cells become very easy to understand from a non technical point of view. The cell gets hit with photons raising the energy level of the electrons in the cell and instead of reflecting the photons back as light the photons are redirected into an electric circuit as current.

I'm sure that's not 'exactly' right but close enough is good for me.
",null,0,cdl8979,1r9es2,askscience,top_week,1
iam_sancho2,"A solar cell is made of layers of different materials. The top layer will be some kind of anti-reflection surface. The next region is a thin layer of n-type material followed by a larger region of p-type material. The juxtaposition of these two differing materials creates an internal electric field. 

When a photon with sufficient energy enters the inner material, it generates electron-hole pairs, which are immediately swept to apart from each other by the internal electric field. With the electrons going one way and the holes going another, a photocurrent is generated within the solar cell. 


There is no net loss of electrons in the material. ",null,0,cdlif43,1r9es2,askscience,top_week,1
clever_cuttlefish,"I think you're mistaken about the current.

Electrical current is a flow of electrons, but no electrons are created or lost in the panel. The energy carried from the photons creates a voltage, which causes the electrons to move around, but none are created.

Think of it like this: All the electrons are happily bound to their atoms, but the photon can bump into one and knock it out of place. However, all the atoms and electrons this has happened to would much rather that another electron comes to fill it's place. So electrons move across the panel to fill in the gaps. This flow of electrons is the current, and is what we get the energy from.",null,3,cdl029v,1r9es2,askscience,top_week,3
Tacomelt,"I can give you a short run down.

Light emits photons, the photons are captured and piled up on the plate causing a voltage.  A current is then formed from the resulting voltage producing electricity.  The electricity is stored into a capacitor or battery system.

The electricity is a direct current, most systems go through an alternator changing the dc to ac.  It is now capable to power your home.

",null,4,cdkyc5n,1r9es2,askscience,top_week,2
iorgfeflkd,"Protons and neutrons are made of quarks, held together by gluons. Electrons and quarks, to the best of our knowledge, are fundamental, and so are neutrinos. With better experiments this understanding can change, but right now as far as we know, certain particles are fundamental.",null,2,cdkxbap,1r9g54,askscience,top_week,14
frogdude2004,"A balloon is inflated when the pressure inside the balloon is greater than the outside: the air inside the balloon's walls stretches it while it pushes back. This continues until the combined force of the balloon and the air outside the balloon equals the force from the air inside the balloon. If the pressure inside the balloon equals the pressure of the outside of the balloon to start, then the balloon exerts no force: it is 'deflated'. If you were to put a balloon that was filled to a certain pressure into a box with the same pressure and then let the balloon open, the sum of the balloon and outside air force would be greater than the force inside, and it would then contract until deflated. I hope this answers your question. If it wasn't clear, please let me know.",null,0,cdl8ze2,1r9hf4,askscience,top_week,3
Naf623,"No, it cannot stay inflated without something blocking the neck. The rubber skin if the balloon will always be stretched and exerting a force to expel air. 
There is one exception; if the neck on the ballon is open to a source of gas, but the rest of the balloon is placed at a sufficiently lower pressure, then the balloon could be inflated by suction. ",null,0,cdlaggw,1r9hf4,askscience,top_week,2
the_dan_man,"http://en.wikipedia.org/wiki/Tunica_media

Muscle cells are arranged in a circular fashion around the vessel's interior. When these cells contract, the lumen (inside space) of the vessel grows smaller. It's a radial-ish contraction.",null,1,cdl4a5l,1r9ilc,askscience,top_week,2
the_dan_man,... the same way any cultivated crop survives predation. Human intervention.,null,1,cdl45l4,1r9omj,askscience,top_week,2
glittercheese,"People taking statins (cholesterol lowering medications) are often told to avoid grapefruit and grapefruit juice because consuming grapefruit can cause higher-than-expected levels of the drugs in the blood. Statins are metabolized in the liver by the same type if enzymes that metabolize grapefruit. If these enzymes are busy processing grapefruit, they are unable to metabolize the drugs, leading to more of the drug circulating in the body. This can cause an increase in adverse side effects of the drugs. 

I think the reason other fruits don't cause the same effects is that they are not metabolized by the same liver enzymes. 

This isn't overly scientific but I am a nurse and I wrote a short pamphlet on this subject when I was in nursing school. ",null,2,cdlao57,1r9qsz,askscience,top_week,8
justin3003,"Glittercheese gives a good basic overview of how the process works. However, to add, a substance called bergamottin and the related 6,7-dihydroxybergamottin are thought to be the culprits present within grapefruit specifically. 

These chemicals are potent inhibitors of CYP3A4, which is part of the cytochrome P450 family of enzymes. These enzymes are responsible for liver detoxification of substances in the blood (generally by making them more soluble for excretion in urine or bile). The exact mechanism isn't important, but what is important is that these guys are potent inhibitors of CYP3A4 and thus prevent CYP3A4 from detoxifying a whole host of important things, including a large number of pharmaceutical drugs. Given that many drugs have a fairly narrow therapeutic index (blood concentration range at which they are safe and have an effect), inhibiting their liver metabolism can cause blood levels to rise to toxic or even deadly levels quickly. Also, while they aren't the strongest inhibitors of CYP3A4, they are the most readily available, common, and seemingly innocuous and thus potentially dangerous.  

There is a good list of things CYP3A4 metabolizes at the bottom of this wikipedia article (under substrates): http://en.wikipedia.org/wiki/CYP3A4",null,0,cdli08l,1r9qsz,askscience,top_week,2
iorgfeflkd,"You'd have to sum up the contribution from each height.

If you assume a constant density and cross section ( which isn't exactly true for a person, but you could make it more complicated if you want) would be the integral from r=6380 km to r=6580 km (or whatever) of GMdA/r^2 by dr where G is the gravitational constant, M is the mass of Earth, d is the density, A is the cross sectional area, and r is the radial position.

For heights much less than the diameter of the Earth, just using mg isn't too too wrong.",null,0,cdl94jj,1r9r7m,askscience,top_week,4
goingforth,"If we're going off of your theoretical proposition, the solution would be rather simple. If only half of the person's mass is influenced by gravity, then they would consequently have half their original weight. However, that concrete of a border doesn't exist. The above comments describe the mathematics of the issue, but the conclusion will be that, assuming the person's head is around low earth orbit, their weight will only change by a very small amount, as acceleration due to gravity doesn't change very appreciably from the surface to orbit (about 9.7 rather than 9.8) This effect will, of course, be amplified if the person's head is, say, on the moon, and will thus require definitive calculations in order to come to an accurate conclusion. ",null,0,cdlq4ba,1r9r7m,askscience,top_week,1
NotFreeAdvice,"hmmm...well, it depends on the reaction that you are interested in.  Multiple things can happen.

1) You can just denature proteins at high/low pH.  It is *very* common to ""cook"" fish in acids (like lemon juice) or bases (such as lye).  These foods would be called ceviche or lutefisk, respectively.  
2) You can hydrolyze bonds that are holding in the flesh together, in which the acid or base acts as a catalyst for this reaction.  

There are probably more reactions of interest, but as a chemist, these are what immediately comes to mind.  ",null,0,cdl81pd,1r9zbg,askscience,top_week,3
Jyesss,"I think the question that you are getting at is ""what happens at the sub-cellular, or protein level, when a strong acid breaks down flesh?"" Proteins are made up of amino acids, and the way that these amino acids fold in 3-dimensional shape largely determines their function and appearance. Their shape depends on the stabilizing interactions between positive and negative charges on the amino acids, on hydrophobic interactions, and on hydrogen bonding. A strong acid increases the concentration of hydrogen ions and thus affects the protonation/deprotonation status of the side chains on the amino acids. The correct folding of the protein will be interrupted if a group that is normally deprotonated at physiological pH is changed to being protonated, thus resulting in the change in appearance. This same thing also happens when the temperature is increased, increasing the vibrational energy in the bonds which can eventually overcome the stabilizing forces at play. This is why an egg looks different when cooked. ",null,0,cdlf7c8,1r9zbg,askscience,top_week,1
inmate992,"Self renewal is the ability of cells to continually replicate - preserving genetic information in all subsequent daughter cells. Stem cell reservoirs are usually tightly regulated in the human body to prevent them becoming cancerous.

As for the difference between Autologous and Embryonic, embryonic stem cells are much more pluripotent ie given the right external environment they can assume any cell type (eg cardiac, neurons, immune cells etc).
Autologous stem cells on the other hand are often precursor cells, for example OPC (oligodendrocyte precursor cells) cells are cells that are able to differentiate into immune cells, but their cellular fate is already decided, so I suppose they lose their pluripotency to an extent as their fate is already decided.

I hope this makes sense!",null,0,cdlgfhw,1ra41i,askscience,top_week,2
Osymandius,Could you clarify your question slightly? Partially differentiated haematopoietic stem cells in my hands happily divide lineally for 50 rounds of divisions in culture with no change in genotype or phenotype. eSCs are similarly capable. ,null,0,cdlayhh,1ra41i,askscience,top_week,1
bohr_exciton,"That's actually a tricky question. The most direct explanation is that ice is slippery because it behaves as being wet, i.e. that there is liquid water between the bulk of the ice and an object gliding on the surface. However, what causes ice to be wet with respect to objects moving on it is still under debate. There was a Physics Today [article](http://scitation.aip.org/content/aip/magazine/physicstoday/article/58/12/10.1063/1.2169444) that came out a few years ago that neatly describes possible explanations. To briefly summarize them, they are as follows:

1) Pressure melting. Due to the fact that ice (at least the common form) is less dense than water, applying pressure reduces the melting point of the ice. To apply this example to say a skater, the idea is that a skate bay locally increase the pressure to a sufficient extent that the ice can melt, making it slippery locally and the water then refreezes after the skate passes. This is the most common explanation that has been invoked historically, but the problem is that it's not clear whether this additional pressure can be enough to melt the ice.

2) Frictional heating. When objects move across a surface there is (virtually) always some friction which results in local heating. Again, this heat may be enough to melt the ice.  This explanation, however, can't really explain why ice is slippery even for stationary objects immediately before moving. 

3) Ice may be intrinsically wet. In describing solids, we often tend to ignore surfaces to a first approximation, because this simplifies the description, but surfaces can behave very differently from the bulk. In the case of ice, it's been speculated that the different local environment of the first few layers of water molecules may result in this molecules being less strongly bound to the bulk than is the case for molecules within the body of this crystal. Because of this, these surface layers may behave as being liquid, which would then cause the ice to be slippery under all circumstances. ",null,1,cdl7ngd,1ra493,askscience,top_week,8
stillealles,"Because either 
a) a thin amount of water is on top (in the case of ice skating and such)
Or 
B) when you apply pressure to ice, it causes it to melt because of properties of water (this also applies to ice skating and such, but is easier to think about if you have a piece of ice on the floor and step on it and slip) 
Veritasium has a good video on it ",null,0,cdl8vq4,1ra493,askscience,top_week,1
jericho,"This is one of those great questions that results in arguments around the water cooler in the Physics Dept. Short answer; we don't know. [Here is a nytimes article that cover some bases.](https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;ved=0CFwQFjAH&amp;url=http%3A%2F%2Fwww.nytimes.com%2F2006%2F02%2F21%2Fscience%2F21ice.html&amp;ei=nPKQUt78OoXMqgHX2oGwCw&amp;usg=AFQjCNH9Hpt9VbWa5zPA0g36hCt64d71cQ&amp;bvm=bv.56988011,d.aWM)
",null,0,cdlac1j,1ra493,askscience,top_week,1
Ejb90,"The auroras depend on two main things:
1) Solar activity
2) latitude

Solar activity produces the solar wind - a stream of charged particles from the sun. These interact with the Earth's magnetic field, being drawn up along the field lines to where they intersect the Earth, at the poles. The interaction of these particles produces the lights. There are more particles when the activity is greatest. At the moment we are nearing a sunspot maximum, so the activity should be greater.
The Forster North/south the more field lines, so the more prominent the lights are.

The lights fluctuate daily, but there is a good chance of seeing them. Going in winter doesn't make too much of a difference, though it is darker for longer so they will be clearer for longer.",null,0,cdli1qs,1ra4dm,askscience,top_week,3
now_you_listen_here,"I'm going to start by explaining it very simply, and then if you want more details I can expand on it!



As far as the part about being non-reactive to CMV (cytomegalovirus), that simply means that you do not have antibodies to the virus.  This means that you have not been exposed to CMV before.  It doesn't mean that your blood ""type"" is non-reactive; we don't speak in terms of blood *types* being reactive or non-reactive to a virus.


As far as your O(-) status, I'm afraid she gave you some misinformation.  That does not mean that your blood is ""good"" for newborns.  It doesn't mean it is ""bad,"" either.  The (+) or (-), as you probably know, is referring to the presence or absence of Rh factor, which is a protein on the outside of red blood cells (you are either Rh+ or Rh-).  You DON'T have it.  Therefore, if your blood was exposed to blood that DID have it, your immune system could form antibodies against it, because it is recognized as something that is ""foreign"" to your body.


The importance of this would be if you would become impregnated by a male who is Rh(+).  In that case, the baby might inherit the gene from the father and also be Rh(+).  Therefore, if your blood (which is Rh-) was exposed to the baby's blood, whether during birth or some event during the pregnancy where bleeding occurred, your immune system could recognize the Rh factor on the baby's red blood cells and form antibodies, which would allow for an immune response at sometime in the future.  Your first baby would be just fine (the immune response takes too long that first time for it to put that baby in danger), but any babies you have after that are also Rh+ would be at risk of your immune system attacking their red blood cells, something we call [hemolytic disease of the newborn](http://en.wikipedia.org/wiki/Erythroblastosis_fetalis). 


There is good news, though!  We have developed something called Rhogam, which can prevent all this!  It's an injection that basically suppresses the mother's immune response to the Rh factor.  We can give it to pregnant women during their pregnancy and prevent the bad stuff that might have happened otherwise.  Science is great, isn't it?
",null,0,cdl8582,1ra6bd,askscience,top_week,6
abbe-normal1,"I'm going to expand on /u/now_you_listen_here since there are a few points I believe they didn't explain as well as could be.  CMV non-reactive as previously stated means you haven't been exposed to CMV.  The reason this is important is because CMV while causing a relatively minor infection in healthy adults is considerably more worrisome for immunocompromised individuals and babies.  See more information [here](http://www.mayoclinic.com/health/cmv/DS00938/DSECTION=symptoms).  The reason your blood is good for these individuals is that you can transmit the virus to them through a blood transfusion.  See [here](http://www.mayomedicallaboratories.com/test-catalog/Clinical+and+Interpretive/62067) again for more information.

As for O-, again as stated previously you lack the Rh factor that can cause an immune response in a recipient of a blood transfusion.  Your blood isn't just 'good for babies' (that's the CMV- part) you are actually known as the universal donor.  Your blood can be given to anyone regardless of blood type because you not only lack the Rh factor, but you also won't react with A or B blood because you being O lack those antigens as well.  Therefore your blood can be given to anyone A, B, AB or O without a life threatening reaction from their body.  In an emergency O- is given to a patient when there isn't time to check their blood type or until cross matched blood is available.  Also, others with your blood type can only receive O- so blood is harder to get because it is rarer.  

TLDR:  GIVE BLOOD!  O- blood is always in need and you would do a great service by regularly donating your blood to help others! ",null,0,cdl8f7o,1ra6bd,askscience,top_week,3
user31415926535,"""CMV non-reactive"" means that your body has never developed antibodies to cytomegalovirus (CMV) - that is, negative for both CMV-[IgM](http://en.wikipedia.org/wiki/Immunoglobulin_M) and CMV-[IgG](http://en.wikipedia.org/wiki/Immunoglobulin_G). A positive CMV-IgM test would mean you have a current CMV infection. A positive CMV-IgG test would mean that you had been infected some time in the past. What's important is that if you have been infected some time in the past, you still have some level of the virus in your body; a negative CMV-IgM test doesn't mean that you are entirely free of the virus. 

[CMV is an extremely common virus](http://www.cdc.gov/CMV/index.html) in humans; worldwide, 40% of adults have antibodies to CMV. [It's not particularly dangerous to healthy adults](http://www.mayoclinic.com/health/cmv/DS00938) (though it is implicated in some cancers and rare syndromes). If you notice an infection at all, it's usually similar to [mono](http://en.wikipedia.org/wiki/Infectious_mononucleosis). But to humans with undeveloped immune systems - newborn infants or immunocompromised people - it can be deadly. [In infants, CMV can cause blindness, deafness, neurological deficits, even death.](http://en.wikipedia.org/wiki/Congenital_cytomegalovirus_infection) 

So we need to be sure that blood used for infants has no trace of CMV in it. Hence, we look to people like you as a source of CMV-negative blood. 

[*Source: I'm not a medical professional, rather I'm an immunocompromised adult who has to know these things to survive.*]",null,0,cdl8jtj,1ra6bd,askscience,top_week,1
Izawwlgood,"To your general question, yes! A number of groups are working on culturing induced pluripotent stem cells into whole organs, by either decellurizing pig equivalents and seeding the collagen matrix with the iPSCs, or causing the tissues to grow onto some other matrix. It's pretty cool and exciting work!

But as to why some transplants need to be grafted to a person; the host body will facilitate the vascularization of the tissue, and 'feed' it, which will allow it to grow and remain healthy. At a later date, when the graft is ready, surgeons will move it. 

Ever seen someone with a crush wound have their hand stitched to their chest or side? It's to promote blood flow into the damaged tissue. Similar idea.",null,0,cdl8bvk,1ra6bi,askscience,top_week,6
KarlOskar12,"The reason they grew the nose/facial skin on the person is because a complex medium is required to grow human tissue. It *can* be done in a lab, but it is much easier to attach it to the person and use their blood supply to get all the required nutrients for growth.

As to your main question: we will absolutely be able to have organ farms. Take this [mouse](http://en.wikipedia.org/wiki/Vacanti_mouse) for example. A live host is - for now - the easiest way to do it but that most definitely does not have to be the case.",null,0,cdl8szw,1ra6bi,askscience,top_week,2
iorgfeflkd,"Matter doesn't contain gravitons, and if gravitons are used to describe gravity they are neutral particles that are their own anti-particles.

Anti-matter is expected to behave normally in gravitational fields, although it's hard to get enough of it in one place to test this. There is an experiment at CERN working on it, [here is their 1990s looking website](http://aegis.web.cern.ch/aegis/).",null,1,cdl9d7n,1ra79d,askscience,top_week,14
stevenstevenstevenst,"As explained, antimatter will behave classically with respect to gravity.  If, however, you want something with the repulsive properties described, you need negative energy (or, correspondingly negative mass).  While this is a rather abstract idea, negative energy has in fact been observed-notably in the Casimir effect.  

In another example, lasers are reported to produce energy in an oscillation of positive and negative energy.  Assuming this is correct, it is easy to imagine a series of rotating mirror which could then separate the negative and positive energy.  This is the beginning of a discussion on how to expand a singularity in a rotation wormhole blahblahblah time travel etc.  For this process it would be necessary to isolate negative energy.

http://en.wikipedia.org/wiki/Casimir_effect",null,0,cdmfqwz,1ra79d,askscience,top_week,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,top_week,20
null,null,null,7,cdl88u6,1rae6m,askscience,top_week,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,top_week,1
null,null,null,3,cdlc4wt,1rae6m,askscience,top_week,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,top_week,20
null,null,null,7,cdl88u6,1rae6m,askscience,top_week,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,top_week,1
null,null,null,3,cdlc4wt,1rae6m,askscience,top_week,1
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,843,cdlaovd,1raftj,askscience,top_week,2816
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,22,cdl8mfx,1raftj,askscience,top_week,117
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,14,cdlb9od,1raftj,askscience,top_week,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,top_week,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,2,cdl9sks,1raftj,askscience,top_week,19
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,5,cdldi1w,1raftj,askscience,top_week,13
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,0,cdl8ok4,1raftj,askscience,top_week,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,4,cdl8sip,1raftj,askscience,top_week,6
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,top_week,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,top_week,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,top_week,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,top_week,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,top_week,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,top_week,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say ⌂V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (⌂V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming ⌂V_l &lt;&lt; ⌂V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as ⌂V -&gt; ⌂V_l) [⌂m/⌂V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,top_week,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,7,cdl9v5f,1raftj,askscience,top_week,7
null,null,null,0,cdl8egp,1raftj,askscience,top_week,1
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,843,cdlaovd,1raftj,askscience,top_week,2816
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,22,cdl8mfx,1raftj,askscience,top_week,117
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,14,cdlb9od,1raftj,askscience,top_week,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,top_week,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,2,cdl9sks,1raftj,askscience,top_week,19
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,5,cdldi1w,1raftj,askscience,top_week,13
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,0,cdl8ok4,1raftj,askscience,top_week,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,4,cdl8sip,1raftj,askscience,top_week,6
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,top_week,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,top_week,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,top_week,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,top_week,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,top_week,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,top_week,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say ⌂V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (⌂V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming ⌂V_l &lt;&lt; ⌂V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as ⌂V -&gt; ⌂V_l) [⌂m/⌂V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,top_week,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,7,cdl9v5f,1raftj,askscience,top_week,7
null,null,null,0,cdl8egp,1raftj,askscience,top_week,1
rupert1920,"Disclaimer: I'm not an expert in protein folding, and would love to be educated more on the matter.

Check out [Levinthal's paradox](http://en.wikipedia.org/wiki/Levinthal's_paradox), which states that the sheer degrees of freedom a protein has makes it highly unlikely to spontaneously fold into the energetically stable conformation. Which means that there must be other effects - other than thermal sampling - that ""guide"" the protein into the proper conformation. This could be chaperones, or stable intermediates.",null,1,cdl9son,1raob3,askscience,top_week,5
Osymandius,"As /u/rupert1920 has said, Levinthal's paradox states that it would take longer than the age of the universe for a polypeptide of 100 residues to fold into the correct configuration by ""trying"" all phi/psi angles. Anfinsen et al (Anfinsen, CB et al (1961) ProcNatAcadSci 47, 1309-1314) showed that primary structure directly determines tertiary fold, therefore trying all the possible angles is not required - as was evident by proteins folding on a biological time scale and life existing to begin with!

Your question, therefore, is a very good one: if primary structure does determine tertiary structure, why bother with chaperones?

Ken Dill answers this nicely in an excellent review [here](http://www.nature.com/nsmb/journal/v4/n1/abs/nsb0197-10.html). He encourages you not to think of protein folding pathways, rather protein folding tunnels where there exist multiple routes to the most stable configuration (i.e. the lowest energy). Through these multiple routes can exist ""energy traps"" - local energetic minima which require energy input to overcome such that the polypeptide can reach the final fold. This is where chaperones come in. You can sort of think of them as proteins which recognise improper folds - say extensive hydrophobic stretches facing the surface of a protein - pull them apart and say ""try folding again"". This is why we don't get trapped in an infinite loop: who folds the chaperones? The chaperones aren't specific to any one protein, rather they recognise common folding mistakes.

Edit: His review really is rather good - if it's trapped behind a pay wall, reply and I'll get it for you - he explains it much better than I do.",null,1,cdlatkk,1raob3,askscience,top_week,5
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,top_week,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,top_week,5
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,top_week,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,top_week,5
Chandley54,"Yes, in veterinary medicine we can categorise hyperthyroidism based on where the tumour is within the hypothalamic-pituitary-thyroid axis. It slightly alters our treatment options for it, but as far as I know, hypothalamic/pituitary surgery is almost never carried out in animals clinically, so many general practice vets will simply treat the general hyperthyroidism. I imagine in human medicine where surgical removal of the tumour is a possibility they are a lot more rigorous with determining where the cause is in all cases!",null,3,cdlcihm,1raq8w,askscience,top_week,6
mklevitt,"theoretically, yes, but i don't know that one has ever been proven or written up in the human scientific literature. secretory (hormone-producing) tumors of the hypothalamus are rare in general, and to find one that specifically comes from clonal expansion of TRH neurons, and then actually secretes? i couldn't find an example among the human literature. a much more common (albeit still very rare) cause of hyperthyroidism is TSH-secreting pituitary tumors, like you said. most hypothalamic tumors cause dysfunction by damaging/suppressing 'normal' functioning hypothalamic nuclei. thus common hypothalamic tumors like craniopharyngiomas can cause hypopituitarism (from suppression of nuclei that stimulate the pituitary) but not hyperpituitarism.",null,0,cdmppa5,1raq8w,askscience,top_week,1
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,top_week,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,top_week,2
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,top_week,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,top_week,2
iorgfeflkd,"The way it would manifest itself is through a change in density, but because water isn't very compressible, its density only changes by like 3% even at the bottom of the ocean, so the transmission of light from a source at that depth isn't too much different.",null,2,cdlclzj,1rarw1,askscience,top_week,5
PepperJack_delicacy,"Smoking cigarettes essentially speeds up the aging process of the skin, which leads to wrinkles. The main reason this happens is because **nicotine** is a **vasoconstrictor**, meaning that it narrows the blood vessels that supply the skin. When you impair blood flow, it has a harder time getting oxygen and absorbing nutrients such as **Vitamin A**, which normally keeps the skin hydrated and protects it from oxidative damage. Furthermore, it will have a harder time repairing wounds and synthesizing a protein called **collagen**, which keeps the structure of the skin intact. 

In summary: 

*Smoking cigarettes ==&gt; decreased blood flow to skin ==&gt; skin gets less oxygen and nutrients ==&gt; skin has a harder time protecting itself from damage and repairing wounds.* 

Sources:

http://dermnetnz.org/reactions/smoking.html

http://www.mayoclinic.com/health/smoking/AN00644",null,1,cdll8tr,1rav3s,askscience,top_week,7
ColdWaterEnthusiast,"Good question. It is almost pointless to try and find out whether the Sahara is growing or shrinking, because of the sheer size of the desert (as well as demarcating what exactly constitutes 'desert'). In the late 90s to mid 2000s, the thought was that the Sahara desert was expanding southwards by a certain extent each year. This was of course somewhat exaggerated. On the other hand, so is the perception that the deserts are 'in retreat' as these articles seem to imply.

The thing is, between the late 70s to late 80s, there was a significantly dry period in the Sahel region (the transition zone between the Sahara and the savanna) which exacerbated the effects of desertification, leading to the perception in the 1990s-2000s that the desert was indeed expanding. However, over the last 15-20 years in terms of precipitation, the region has been in a comparatively very wet period. Relative to the significant drought the area previously experienced, it may seem that the deserts are in 'retreat' but arguably that is essentially what is expected in terms of how vegetation has responded (it gets a bit more complicated because some of the previous mesic vegetation has been replaced by xeric vegetation in certain areas so while it is greener, it is not quite the same)

I hope this help. I could go into more detail but this should give you an idea of how complicated it is to understand.

Source - My dissertation research has broadly to do with understanding how vegetation responds to moisture events",null,1,cdlfsdd,1ravcx,askscience,top_week,6
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,top_week,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Piškur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Piškur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,top_week,8
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,top_week,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Piškur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Piškur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,top_week,8
Sunscorch,"Ok.

For a second, forget that the Earth and Moon are orbiting, and picture the Earth falling straight down towards a stationary Moon. At the very start of our thought-experiment, the Earth is also stationary and its oceans are equally spread out across the entire surface.

The Earth then begins to accelerate towards the Moon, as is begins to fall. The ocean nearest the Moon experiences the greatest acceleration, because it is closer and therefore is exposed to the greatest force. Likewise, the ocean furthest from the Moon experiences the lowest acceleration for opposite reasons. The Earth itself, of course, experiences an average acceleration.

So! The Moon-side ocean begins to move away from the surface of the Earth, as it is accelerating faster than everything else, creating the bulge that is easiest to understand. The ocean on the other side, however, gets ""left behind"" because it is accelerating slower than the Earth. Essentially, the Earth is moving away from it, which creates the opposing bulge on the far side from the Moon.

That is how it works in our thought-experiment, but exactly the same thing happens in orbit because the Earth and Moon are essentially falling towards each other, and are constantly accelerating because of the constant change in direction. That is why there is a tide on each side of the Earth.",null,2,cdlgab2,1ravmk,askscience,top_week,12
thumbs55,"Excellent question:

[Sixty symbols did a video on it.](http://www.youtube.com/watch?v=YO3eDYzFp8Y)

[In this image](http://hendrix2.uoregon.edu/~imamura/123cs/lecture-2/tides.jpg)

Thinking in vectors the first image is of 5 vector forces acting toward the moon. Note that the farther they are the weaker they are and the top and bottom are not parrallel to the other three.

But we dont live in space we live on the earth so if the earth moves we move with it and dont notice so for that reason in the second image the middle vector is subtracted.

Giving a zero vector in the middle since anything minus itself is zero.

The top and bottom vector are pointing in since they were already pointing a little bit in.

The vector nearest to the moon is a lot shorter but still pointing toward the moon.

And most interestingly the vector farthest from the moon is actually pointing away from the moon.",null,2,cdlgink,1ravmk,askscience,top_week,7
SingleMonad,"What you're asking has been done.  It called a pulsed laser.  You are imagining making little pulses of red light no more than a few femtoseconds long.  The output is not red.  It is *white*.  The light has a broad spectrum, centered about red, the width is inversely proportional to the pulse duration.

Wiki ulatrafast, supercontinuum, frequency comb, laser.  If you disperse the output, you will see the individual colors in the laser.

http://grad.physics.sunysb.edu/~meardley/fiber/weiss5.jpeg",null,0,cdloqib,1rawb8,askscience,top_week,10
null,null,null,15,cdlfycp,1rawb8,askscience,top_week,8
onyablock,"People can become immunocompromised through various ways including pregnancy, viral infection, steroid treatment etc. etc.

The reason it is important when considering vaccinations is that being immunocompromised can increase your risk of obtaining the infection to which you're being immunized against (if the vaccine is 'live' virus) or being vaccinated can be pointless as no immunity will actually be gained from the vaccine.

The effects of an immune deficiency on vaccines varies greatly depending on the vaccine and its application regiment. For example it could be recommended that you receive multiple booster shoots or don't receive the vaccine at all if you are immunocompromised. 

In the case of the flu shot, obtaining the killed-virus vaccine won't allow you to actually contract the flu, however the chances of you generating good and long lasting immunity to the flu is reduced if you are immunocompromised. Obtaining the 'live' attenuated vaccine would not be recommended for immunocompromised patients as there is potential for infection, hence why the box is there.

I hope that makes sense.

Source: 4th year immunology student.",null,2,cdlcf5d,1rawff,askscience,top_week,17
tthershey,"Live vaccines are generally contraindicated in immunosuppressed patients because these patients will not be able to mount a sufficient immune response to the vaccine.  Consequently, the live vaccine could induce an infection.

Inactivated or component vaccines won't put immunosuppressed patients at risk, but the patients might not gain much protection from the vaccine.  This is because immunosuppressed patient's won't be able to generate antibodies to help fight off future infections.

Immunosuppressed patients most certainly need to get vaccinated because they are at greater risk for getting serious complications from infections.  It's important for health care providers to know a patient's immune state in order to deliver the right kind of vaccine.",null,0,cdlk7wj,1rawff,askscience,top_week,3
Chandley54,"There're three reasons why you need to be immunocompetent when vaccinated. 

1. It is rare nowadays for a vaccination to be live &amp; pose a direct threat, some of them may revert to being harmful, for example attenuated vaccines commonly have a harmful gene removed before they are given to the host so they can be dealt with by the immune system without any significant risk. In some cases the process may not be perfect and so some live unattenuated virus may get into the vaccines, so although in theory the virus should not be able to establish itself and replicate, it sometimes can and is therefore much more hazardous to immunocompromised patients. The same is true for killed virus vaccines - in theory all of the virus should be dead, but whatever process they use to kill the virus (e.g. exposure to UV light) may be ineffective, so again, live virus may end up being present in the vaccine and lead to an active viral infection.
2. If a patient is immunocompromised, there is a possibility that the body will not be able to respond effectively and generate the necessary memory lymphocytes for the vaccine to be effective. This would mean that should the person then encounter a live version of the virus, the vaccine would not have provided them with any protection at all as the memory cells were not generated at the initial vaccination. so are not available to respond.
3. Although the vaccine you received was killed, the form you filled out was probably written by some legal team at some point in history when they were using a different type of vaccine, so they're basically just covering their own backs, and if may be expensive for them to change the documentation.

I would imagine it is likely a combination of the above 3 reasons!
Hope this was helpful.

Source: Veterinary Surgeon/Anatomical Pathologist",null,2,cdlcbxz,1rawff,askscience,top_week,3
Urgullibl,"The point of a vaccine is to stimulate your immune system into producing antibodies against whatever it is you're being vaccinated against. If your immune system is suppressed, there is no point in vaccinating, as the reaction would not result in protection from infection.

In case of the flu vaccine, we're talking about a dead vaccine, i.e. there are no attenuated whole viruses in it, hence there is no risk of getting the flu from it. In case of vaccines containing whole attenuated viruses, an immunosuppressed patient might get sick from such a vaccine.",null,1,cdlk4zy,1rawff,askscience,top_week,2
killer_alien,basically there are two types of vaccines: antibodies and dead virus or w/e cells. Anti bodies is a short term vaccine which basically grands you immunity whereas dead cell ones stimulates your body to create antibodies which is a long lasting treatment. (This is the most basic i can put it w/o making to wrong and confusing),null,0,cdmicxd,1rawff,askscience,top_week,1
invariance,"No. It is simply inconclusive, because there exist series which diverge and series which converge for which the ratio test gives 1. For the series a_n = 1/n, the sum diverges. For the series a_n = 1/n^2, the sum converges.

The ratios are not 1 in magnitude, except in the limit. Note also that both ratios converge to 1 from below. A refined version of the ratio test will tell you that if |a_{n+1}/a_n| &gt;= 1 for sufficiently large n (so the ratios converge to 1 from above), the series will diverge. The proof for this follows from what you have already said. So in fact, the only inconclusive case is if the limit of the ratios converges to 1 from below.

So the short answer is that if the limit of the ratios converges to 1 from below, the test is inconclusive because there are examples which converge and examples which diverge. ",null,2,cdlud5b,1rawis,askscience,top_week,7
iCookBaconShirtless,"The issue is not which direction that the limit is approached, as you conjecture.  While approaching 1 from above assures divergence, approaching from below does not assure convergence.  A simple example is the harmonic series.

They key to understanding the ratio test is to more precisely understand this statement that you made:  

&gt; I understand what happens when the limit is smaller than 1 (every element of the series is smaller than the previous by a factor L, hence the series tend to stabilize and converge).

The fact that every element of the sequence is smaller than the previous by a factor of L (at least in an asymptotic sense) implies that the sequence converges to zero **exponentially fast**.  Basically, it looks like a geometric series at large n.  This exponential convergence of terms is enough to ensure that the series converges, but it's strictly stronger than what is needed.  Plenty of series have terms that converge more slowly than exponentially, but still converge as a series.  Any p-series with p&gt;1 for example (e.g., 1/n^2 ).

tl;dr Ratio test determines exponential convergence of terms, which is more than is needed for convergence of series.",null,0,cdlv91r,1rawis,askscience,top_week,4
wgunther,"In order to understand the ratio test you have to understand the proof. The proof is if the ratio a_{n+1}/a_n goes to L&lt;1 then eventually the of consecutive terms is less than 1-epsilon for some small but positive epsilon, and therefore, the series is smaller than the geometric series a(1-epsilon)^n for some suitable a. Thus it converges by direct comparison. 

If L&gt;1 then you can do the same thing but with 1+epsilon. 

The problem is this proof doesn't work if L=1. The ratio test will only work when the sequence of the summand of the series converges *faster* than something geometric whose series converges or *slower* than something geometric whose series diverges. In the case when L=1, one can not compare the series to a geometric series that converges or diverges. ",null,0,cdmmc8x,1rawis,askscience,top_week,2
snusmumrikan,"[This paper](http://www.academia.edu/372962/Giants_on_the_landscape_modelling_the_abundance_of_megaherbivorous_dinosaurs_of_the_Morrison_Formation_Late_Jurassic_western_USA_) discusses it for herbivorous large dinosaurs and says a few tens of each per sq km. 

[This one](http://earth.geology.yale.edu/~ajs/1993/11.1993.06Farlow.pdf) discusses the limiting factors in population density of large carnivores and the balance between food availability and having enough of each species to ensure a mating fequency high enough to avoid extinction.

It seems your question can't be answered reliably as so much depends upon the preservation of dinosaur remains for that. Looking at the variables and comparing it to modern-day predators might be the best option?",null,3,cdlea2h,1raxgi,askscience,top_week,9
Ruiner,Only by the amount of mass it consumed. ,null,0,cdlt15h,1rb1bz,askscience,top_week,4
Infinite_Ambiguity,"Steven Hawking has shown that black holes also radiate energy because of quantum effects near the event horizon.  Consequently, black holes might increase in mass/energy by the amount of mass/energy consumed, but they are also radiating mass and energy (equivalent by e=mc-squared) and thereby also simultaneously evaporating to some degree. .  ",null,0,cdltx5v,1rb1bz,askscience,top_week,2
Ejb90,"I think you're misunderstanding the structure of an atom. The ""shells"" the electrons occupy aren't determined by their distance from the nucleus, but by the relative energies of the electrons in each. Why this happens is explained by quantum mechanics.
In the classical case, the ""radius"" of the first shell would simply expand with the nucleus, though that would never be a real problem - the nucleus is 10^-15m across and the atom 10^-10 - that's 10,000 times bigger, so any atom that big would be inherently unstable.
However in reality the electrons aren't hard point of mass whiz zing around the nucleus, they're a ""cloud"" of delocalised charge with certain characteristics, also described by quantum mechanics. If you want to know more there are loads of online resources about this.",null,0,cdlgd0k,1rb1wp,askscience,top_week,7
Platypuskeeper,"Why would it 'force electrons out of the lowest shell'?
",null,1,cdlg0vx,1rb1wp,askscience,top_week,3
thumbs55,"Basically no.

If you have a very small object (less than the DeBrogle wavelength of the electron ~ 10nm) then the electrons in the outer shell of this object may have descrete energy levels and be treated as a giant atom.

This is refferd to as a [Quantum dot.](http://en.wikipedia.org/wiki/Quantum_dot)

So these quantum dots are much larger than the lowest shell of a Hydrogen atom but still dsiplay quantum properties.

Sure the nucleus is like 15 orders of magnitude smaller than the electron shells but if we had a nutron star and some how made it positevly charged, and placed an electron to see if it would orbit it:

Then we would find that the electron and the proton in the star join together to create a neutron due to all of that pesky gravity.

If the system is changed such that the lowest shell no longer exists then the next lowest shell by definition becomes the lowest. This is just simantics and is a bad argument since the lowest energy is an s shell and the next lowest is a p shell and behave measurable differently. And if you did get rid of the first s shell then the new lowest shell would be an s shell at a higher energy.",null,0,cdlgc3l,1rb1wp,askscience,top_week,1
miczajkj,"In fact this is something, that can happen but not for electrons. 

You may know, that there are three generations of quarks and leptons, while our universe consists mostly from the first generation (up- and down-quarks + electrons and electron-neutrinos). But there are certain natural processes, that produce particles from higher generation, for example the cosmic radiation in the earth's athmosphere. 
The electron equivalent in the second generation is the muon - and if you construct muon-atoms your question gets important!

Like the others already mentioned, 'orbiting' electrons (or muons) don't really orbit the nukleus: their position gets described by a probability density; those densities are the absolute squares of the particles wave function. 
If you calculate those wave functions you find, that the probability density of the ground state has it's maximum at the classical Bohr radius, that can be calculated by using a semi-classical force approach (for muonic hydrogenium): 

Let the Coulomb-Force (ℏαc/r^2) equal the centripetal force (p²/mr). Also keep in mind the the uncertainty principle: pr ~ ℏ. It follows:

ℏαc/r^2 = ℏ^2 /mr^3 
r = ℏ/αmc

Now, because the mass of the muon is 200 times bigger than the mass of the electron, therefore it's Bohr Radius is 200 times smaller and if you also consider the finite circumference of the proton it is possible, that the muon may have finite probability to be found inside of the nukleus, especially if you talk about heavier nuklei. 

The main consequence is a displacement of the energy niveaus. This fact was used, to find the much used formula

r = 1.2 fm * A^(1/3) 

for the radius of the nukleus depending on the Mass number. 

Another consequence is the increased probability of decays like the K-capture, where a muon from the K-shell reacts with a proton:

µ^- + p -&gt; n + ν_µ",null,0,cdlsooe,1rb1wp,askscience,top_week,1
ShwinMan,"Short answer: no

LADEE is a small spacecraft, it's only 2.37m high and if it were visible then so should all the other spacecraft there now as well, including the Apollo descent stages, lunar rovers, debris etc. Even Hubble wouldn't be able to make it out.",null,0,cdmxh0z,1rb6hn,askscience,top_week,1
sharp12180,"The force of gravity due to an object with mass is never zero. It can be very small if the object has very low mass or you are far away from the object. For a galaxy, which is very massive, you can get far enough away where the force of gravity acting on you is negligible but it will never be zero. In fact, the gravitational force is proportional to the inverse-square of the distance between two objects, meaning if you double your distance from an object the force of gravity decreases by a factor of four. Still, this force will never be zero.  ",null,0,cdm0ted,1rb6tc,askscience,top_week,3
Ejb90,"In a way, no - the gravitational field produced by mass is infinite, so the field has an effect throughout the universe.
In another way, yes - there exist points called Lagrangian points where the gravitational fields of objects cancel to produce zero net acceleration. These are quite common, there are several around earth, being utilised for their stability. Though here the rotational effects of the entire galaxy have been ignored and considers the Sun-Earth system as relatively stationary.",null,0,cdm0vay,1rb6tc,askscience,top_week,2
PepperJack_delicacy,"The ""pins and needles"" feeling is referred to as **paresthesia**, which occurs when a nerve and the arteries supplying the nerve are compressed. This prevents the nerve from carrying electrical impulses that transmit the sense of touch, which you feel as ""pins and needles"". 

It's harmless when it occurs transiently (like after you fall asleep on your hand) because once the pressure is removed, blood supply to the nerve will be restored. However, there are certain chronic cases that are indicative of neurological disease or more traumatic nerve injury, which are more serious.

In the case of a blood clot, though, you are created a plug in an artery that prevents blood flow to a tissue. There is nothing you personally can do (such as switch body positions) that will remove the clot. Furthermore, in the case of a heart attack or stroke, you are preventing blood supply to the heart or brain--two of the most important organs in the body, which absolutely need blood for you to survive. 

So overall, it's true that ""pins and needles"", strokes, and heart attacks are caused by circulation problems. However, in the case of ""pins and needles"", the blockage is readily reversible and the organ that is losing blood supply is no where near as important as the brain or heart. 

Sources:

http://www.ninds.nih.gov/disorders/paresthesia/paresthesia.htm

http://www.urmc.rochester.edu/encyclopedia/content.aspx?ContentTypeID=1&amp;ContentID=58",null,0,cdm065z,1rb75x,askscience,top_week,2
brawnkowsky,"'pins and needles', which is called Paresthesia, is caused by pressure applied to a nerve.  This inhibits its ability to conduct a signal and eventually leads to a limb 'falling asleep' (a dead leg).  a lack of blood circulation does not cause this.

lack of circulation (specifically tissue perfusion) results in a failure to deliver oxygen to systematic cells and to remove metabolic waste.  This lack of oxygen causes cells to create energy through alternative pathways that create acidic products (lactic acid is common), causing acidosis.  Eventually, the cells will die because they fail to maintain the pH needed to function; this is called Ischemia.  Ischemia in organs can lead to organ failure, which will kill a person.  ",null,0,cdm0add,1rb75x,askscience,top_week,1
adoarns,"Pinch a nerve long enough, and it becomes permanently damaged. The nerve fibers will wither back, and you will lose sensation until the nerve grows back (about 1 mm/day). Even then you may expect that not all the withered fibers will find their way back to their proper locations.

Heart muscle and brain tissue are much more metabolically active, and take much less time without proper blood flow to be permanently damaged.

Unlike peripheral nerves, brain tissue and heart muscle does not grow back. You lose it, and it's gone for good.",null,0,cdmc3ds,1rb75x,askscience,top_week,1
_Momotsuki,"If you squeeze just one vessel of your arm, there are many collateral vessels to take up the slack and perfuse the rest of the arm. This principle applies to your heart because there are only a few main vessels that supply the heart (with great variation between individuals, and very simplistically, there's the left and right coronary arteries with the left splitting very early to become the left anterior descending and circumflex artery). Any blockage in one of those 3 vessels will cause ischaemia and lead to death of the tissue areas that are meant to be supplied. Indeed, there are collaterals present in the coronary circulation. However, these are usually functionally non-patent and can't really help with distributing blood because they have such a small lumen. This is especially the case when there is ischaemia due to a sudden thrombo-embolic event. With that said, in *some* cases where there is a slow build up of atherosclerotic plaque within arteries, you can get slow opening of these collateral vessels to help perfuse the heart.",null,0,cdn9eo8,1rb75x,askscience,top_week,1
StarshipEngineer,"There is no such thing as terminal velocity in an airless environment. It doesn't matter what the terminal velocity of an object in air is, if there is no air for the object to interact with through friction, the object will keep accelerating as it falls until it hits a solid surface.",null,1,cdlfu8p,1rb77o,askscience,top_week,12
Smoothened,"Microorganisms are the main cause of spoilage, but there are other ways in which food can go bad. This varies both with the content of the food and the environment it is exposed to. For example, food that is partially composed of water can dry out. Protein and other molecules in the food would undergo degradation, which will result in changes in both texture and taste. Another thing that can happen is the separation of ingredients in different phases. These changes would be less obvious in foods that are homogeneous and mainly composed of simple molecules such as sugars. All in all, the changes would generally occur more slowly than in the presence of microorganisms. Most likely the resulting food wouldn't make you as sick as if you ate food spoiled by bacteria or fungi. ",null,0,cdmc7d1,1rb7jz,askscience,top_week,2
stevenstevenstevenst,"I do not know a lot about what the atmosphere is actually like within the ISS, but I can tell you that in a pure oxygen environment, as was used on some NASA flights, those on board have reported that it is difficult to hear one another.  This is perhaps related to the pressures in these vessels more than atmosphere itself, as a denser atmosphere transmit sound waves more readily.  

Kind of just a curiosity, so I apologize if your question was not answered in its entirety.  The atmosphere being thicker or thinner is related more to pressurization than the content- although content does play a role.",null,0,cdmgh7o,1rb86r,askscience,top_week,2
Proxymace,"At high concentrations oxygen is toxic to organisms, at 100% humans get a ""high"" from breathing it. This tends to make people calmer and is why planes deploy oxygen masks and not just air masks. For the iss I imaging its due to weight limits on supply ships. Carrying a load of nitrogen that you don't need isn't very good.",null,1,cdmjb36,1rb86r,askscience,top_week,3
chrisbaird,"""I saw somewhere that in the ISS and other stations that they have a 100% oxygen environment"" You saw wrong. The composition of air on the ISS is definitely not 100% oxygen, and is in fact intentionally regulated to match the composition of air on earth's surface, with about 21% oxygen:

http://www.nasa.gov/missions/highlights/webcasts/shuttle/sts112/iss-qa_prt.htm

There are a couple of reasons for this:

- Pure oxygen is highly flammable. NASA unfortunately learned this the hard way with the Apollo 1 accident
- High oxygen concentrations are unhealthy to humans as our bodies have evolved to work most efficiently with the oxygen levels common at earth's surface.",null,0,cdmutcf,1rb86r,askscience,top_week,2
owaisofspades,"a-helix and b-sheets are due to hydrogen bonding. Random coil is due to hydrophobic reactions if I remember correctly. There's also di-sulfide bridges, which only cysteine can form. 

Whether a protein will form an a-helix or a b-sheet depends on the sequence (the amount of residues between the two interacting residues determines which one will form)",null,0,cdmbq05,1rb8il,askscience,top_week,2
reddishpanda,"Short answer: the types of interactions needed to produce a secondary structure can be made by a variety of amino acids. 

Helices and sheets are made by the backbone interactions between the amino acids of a proteins, so almost anything goes (except for proline, which would be too geometrically limited to form a helix or sheet and is limited to loops and random coils). You can use many combinations of different amino acids to make one structure or another, but interactions among the side chains of amino acids (where you will find hydrogen bonding and hydrophobic interactions as well as salt bridges between say glutamate and lysine). 


It might be information overload, but try playing around with a protein database like [RCSB](http://www.rcsb.org). If you just come up with an amino acid sequence of your own creation, you can use [Phyre2](http://www.sbg.bio.ic.ac.uk/phyre2/html/page.cgi?id=index) to get a prediction of what your protein might look like and the secondary structures that might form it. ",null,0,cdmedut,1rb8il,askscience,top_week,2
Polyknikes,"I don't think anyone will have an exact answer to your question because it would depend on which cell you are talking about, how well stocked they were beginning the fasting period, and how much energy they are being asked to expend over a given amount of time.  As an alternate answer I will discuss what happens during starvation which hopefully will answer your question.

In the normal course of starvation we first burn carbohydrates which basically refers to glucose.  Glucose is stored in many cells, but particularly in the liver hepatocytes, in the form of glycogen.  The breakdown of glycogen is referred to as glycogenolysis which releases glucose into glycolysis for energy production.

After all the glycogen is used up the body begins catabolizing (burning) proteins.  Protein catabolism involves the breakdown of bodily proteins into amino acids for use in synthesizing more glucose in a process called gluconeogenesis (which takes place in the liver).

Sometime during protein catabolism your body will begin the process of lipolysis which is the breakdown of triglycerides into fatty acids which can be oxidized into multiple units of Acetyl-CoA and fed into the TCA cycle for energy production, bypassing glycolysis.  High rates of fatty acid oxidation will lead to ketogenesis, or the creation of ketone bodies.  Ketone bodies are another form of high-energy molecule like glucose which can be metabolized by many tissues and are especially important for the brain during the starvation state as it has high energy demands and cannot directly metabolize fatty acids.

TLDR: During starvation your cells first use glycogen (stored glucose), then catabolize proteins into glucose, then burn fats in the form of ketone bodies.

Hope this answer helps.
",null,0,cdlx6pc,1rb8zj,askscience,top_week,2
AbouBenAdhem,"When the temperature of a gas changes, its density changes. When its density changes, its index of refraction changes. When light passes from one substance to another substance with a different index of refraction, it travels at a different speed; and when it meets the interface between two such substances at an angle, it bends.",null,2,cdlghd7,1rbb96,askscience,top_week,21
iorgfeflkd,"Freefall doesn't get rid of tidal gravitational fields. The difference in Earth's gravitational field between the front and back of the ship could be detected with precise instruments, and would be absent in intergalactic space.",null,1,cdm16ju,1rbd4j,askscience,top_week,4
owaisofspades,"It's a bit complicated, but I can give you the general idea.

During glucose metabolism you go through glycolyis, which gives you pyruvate. Pyruvate then gets converted to acetyl CoA. Now here's where it gets tricky. If you need energy, your cells are going to send the acetyl CoA through the Citric acid cycle to make loads of ATP. If you don't need energy, the acetyl CoA gets shunted to fatty acid synthesis.

YOu have some enzymes involved that activate the acetyl CoA into malonyl CoA, and this allows you to add an acetyl CoA to it, forming a chain. After a certain number of extensions you get palmitate, which is a freefatty acid, which can then be modified to form other fats or phospholipids, or which can be esterified with a glycerol molecule to form mono-, di-, triglycerides.

This takes place in the liver. Your liver then packages the TAGs into lipid containers (chylomicrons) and then put them into the blood stream. Then lipoprotein lipase on the surface of adipocytes grabs the chylomicron and pulls the TAGs out of them.

Now for the second part of your question. fatty acids don't get converted back into glucose as far as I know, but into acetyl CoA through a process known as beta oxidation (for short and medium chains, I don't remember the long-chain degradation process ATM). The acetyl CoA is for the liver to use (it's not water soluble so can't get transported in blood). For other tissues, the liver converts the acetyl CoA from Beta oxidation into ketone boies, which are water soluble and can be transported to other organs such as the brain through the bloodstream.

Hope this helps, feel free to ask for clarification. As a med student with my biochem final coming up soon i'm trying to keep my knowledge from disappearing haha",null,0,cdmbnot,1rbdz6,askscience,top_week,2
sharp12180,"Electrons are bound to an atom and his bond has a certain amount of energy. If the is an incident photon with energy greater than or equal to this bond energy, it can cause the electron to become unbound. When enough electrons become unbound, you have a current. In a solar panel, some of the photons from the sun have the right energy to dislodge electrons in the panel which creates usable electricity. ",null,0,cdm0x32,1rberh,askscience,top_week,2
rupert1920,"You should also take note that it is the [photovoltaic effect](http://en.wikipedia.org/wiki/Photovoltaic_effect), not the photoelectric effect, at play here (hence a ""photovoltaic cell"").",null,0,cdm8iuh,1rberh,askscience,top_week,1
meerkatmreow,"Black both absorbs and radiates better.  The net result can be a lower equilibrium temperature.  For example, the SR-71 was originally not painted black.  However, it turned out that the black paint lowered the temperature enough to allow the structure to be lighter even with the extra weight of paint, resulting in a net savings.",null,0,cdm5fnf,1rbfb3,askscience,top_week,2
miltoniousbastard,If you have ever touched a tinted window you will notice that it is noticeably hotter than non tinted windows on the same vehicle. The tinted window is absorbing most of the light (energy) which keeps it from being transferred to your cars interior. The heat on the window is radiated to the surrounding outside air vs. the heat radiating off your seats/dash/whatever else is in your cars interior.,null,0,cdm0fzy,1rbfb3,askscience,top_week,1
Greyswandir,"I think perhaps this is a good lesson that aesthetic concerns sometimes (often?) trump engineering ones.  I think the real answer to your question is that many people don't want other people peering into their cars, and as you pointed out, other coatings are more expensive.  I imagine that concerns about heat probably weren't too much a part of the design process.

And, as plenty of other people have pointed out, having the light absorbed by your windows which are in contact with the outside and have a high surface area, keeps the heat from instead being trapped inside your car.",null,0,cdmvmx1,1rbfb3,askscience,top_week,1
bohr_exciton,"Air is not completely transparent, however we perceive it as such for two reasons:

1) The various molecular and atomic species that make up the atmosphere can only absorb light at specific frequencies. If you look at the absorption spectra of the species making up thee majority of the atmosphere, such as nitrogen (N2), you will see that for most of them there will only be negligible absorption in the visible part of the spectrum. 

2) The density of air is so low that we can't perceive the minimal absorption that does take place. The best example of this is water. Water actually has a blue color, which you can see by say looking at the sea. However water only absorbs weakly in the visible, such that to observe this color light needs to pass through meters of water before we can observe its absorption. If you just look at a glass of water, it will just look transparent. Now the density of water vapor in the atmosphere is much much lower than in the glass, and therefore you can't possible see this effect in air with your bare eyes.

Finally, as for why you can see the atmosphere from space, or why the sky looks blue, the answer is that while the atmospheric gases do not significantly absorb visible light, they can scatter the light. Moreover, the major scattering mechanism is so-called Rayleigh scattering, which occurs more strongly for higher energies. Because of this blue light is scattered more than red light, which makes the atmosphere look blue. ",null,1,cdlvjoo,1rbhiu,askscience,top_week,4
chrisbaird,"Air is not perfectly invisible. Look at the sky during the day. That blue color is air. Look at a distant mountain on a clear day and compare it to a very close hill. The distant mountain will seem to be covered with a whitish haze. That haze is air. Clean air is composed of very small molecules that are very far apart. For this reason, you need a lot of air in order to see it with your naked eye.",null,0,cdmul9m,1rbhiu,askscience,top_week,2
Qvanta,"Materia excerts light ""color"" only if it has absorbed and emited the light struck by it. Here is the catch, each materia has a specific amount of energy in form of light it accepts. If the light shining on a materia is below or above the requierd amount of energy it needs to possess, light will just pass by.

The atmospheric light comes from the scattering of the blue spectrum when the light passes through our atmosphere. So you actually dont see any color only the smudging of the suns blue light.  ",null,2,cdluxwu,1rbhiu,askscience,top_week,1
DeadVirus0,"Earth's solstices and equinoxes are based exclusively on its 23.5 degree axial tilt.

For example, our next perihelion will be on January 4th, 2014 while the upcoming winter solstice will be on December 21st. This means that the northern hemisphere's longest night is 2 weeks away from the Earth's closest orbital position to the Sun. [Source: US Naval Observatory](http://aa.usno.navy.mil/data/docs/EarthSeasons.php)

Additionally, 4 Vesta is, relative to other celestial bodies, small and amorphous.

So, I guess what I'm trying to say is that your assumption that solstice/equinox are related to perihelion/aphelion is flawed.",null,0,cdm19tt,1rbj3h,askscience,top_week,1
rupert1920,"Ash is material is leftover material that cannot undergo further combustion - in the case of complete combustion. This is the white ash you often see when something is completely burnt. An example of this would be [wood ash](http://en.wikipedia.org/wiki/Wood_ash), which consists of the trace mineral compounds in wood that don't combust, such as calcium and potassium (which comes from the word [potash](http://en.wikipedia.org/wiki/Potash), with a similar etymology in how it was made historically - from ash).

Since different compounds will have different amounts of these non-combustible elements, they will naturally have different ashes.

This concept also has direct impact in [gravimetric analysis](http://en.wikipedia.org/wiki/Gravimetric_analysis) - which relies on very careful measurements of weight of a compound before and after some process. You can attempt to weigh out your analyte, then completely burn it at high temperatures, and weigh out the ash that is left over in order to find the chemical composition of the analyte (for example, finding the stoichiometric ratio can give you oxidation states). You'll also find filter papers used for this purpose to be ""ashless"" - it produces purely gaseous products under combustion so it won't skew the results of such measurements.",null,1,cdlzjby,1rbk7h,askscience,top_week,3
Platypuskeeper,"Alcohol (ethanol) and water are miscible fluids because the -OH group of the ethanol molecules forms [hydrogen bonds](http://en.wikipedia.org/wiki/Hydrogen_bond) with water, just as water does with itself. 

Hydrocarbons, as in oils, only bond negligibly with water molecules, which means it costs energy to separate the water molecules from each other and stick a hydrocarbon molecule between them. The lowest energy situation is if you lump all the hydrocarbon molecules together and minimize the contact area with the water, that is, form an oil phase.
",null,3,cdm0b7z,1rbl1i,askscience,top_week,5
LoyalSol,Density has nothing to do with it unless the two materials don't mix.  In the case of oil and water there is a mismatch in how the molecules interact with each other so they prefer to stay separated.   ,null,1,cdm0l07,1rbl1i,askscience,top_week,2
TehMulbnief,"Couple of reasons. The most direct reason is that alcohol and water are miscible. They mix together very nicely, so much so that you can't tell them apart once they are mixed. The resulting solution is sort of like a metallic alloy. When you look at a stick of bronze, you don't see copper and iron, you just see nice, homogenous bronze.

The other less obvious (and maybe a bit niggly) reason is the Second Law of Thermodynamics. This law introduces the idea of ""entropy"" or randomness of a system. When you add water to alcohol, the tendency is going to be for the two to mix, until the alcohol molecules are perfectly and uniformly distributed amongst the water molecules and vice versa. Once you're at this point, the system is going to stay that way because that's the most stable way for the system to be.",null,0,cdmx3br,1rbl1i,askscience,top_week,1
Farnswirth,"You can absolutely see one atom thick graphene sheets, this is one of the things that makes it such a remarkable material.  Just look at [This picture](http://3.bp.blogspot.com/-2UU-zkkrxm0/UPXIX6at0RI/AAAAAAAACNc/LsODcsw_1Oo/s1600/High-Speed+Graphene+Circuits.jpg), [the bottom of image a in this picture](http://www.nature.com/srep/2012/120921/srep00682/images/srep00682-f2.jpg), and [image c at the bottom left in this one which shows two single-atom graphene sheets layered on top of eachother on a PET film](http://www.nature.com/nnano/journal/v5/n8/images/nnano.2010.132-f2.jpg).  

We can see graphene when it is only one atom thick because it is exceptionally good at absorbing light.  Graphite and graphene are extraordinarily good at absorbing light mainly because the individual sheets of graphene have an extremely low [band gap](http://en.wikipedia.org/wiki/Band_gap) (pretty much 0eV).  This is also one of the reasons it is such a good electrical conductor as well.  Notice diamond has a very high band gap, which makes it transparant to visible light and a poor electrical conductor.  Finally, take a look at the [physicochemical properties of Boron Nitride](http://en.wikipedia.org/wiki/Boron_nitride#Physical).  While its structure is remarkably similar to graphene, it has a high band gap, which makes it appear white or transparent.  ",null,2,cdlranh,1rbqyc,askscience,top_week,7
organiker,"If your application calls for multiple layers, then you grow it as multiple layers.

If you're making electronic devices, you lay it flat on a substrate like glass, silicon or plastic, then add electrodes.

If you're making a coating, you layer it on whatever you want to coat. Probably with a protective layer on top.

If you're making batteries or capacitors, you mix it with your electrode materials.


",null,0,cdlo8z4,1rbqyc,askscience,top_week,1
EdwardDeathBlack,"You incorporate it into another matrix. You layer it in a sandwich of other materials...all that to exploit either its electrical or mechanical properties. 

It is not a first...here is a [TEM](http://large.stanford.edu/courses/2007/ap272/kimej1/images/f2.jpg)  of a gate oxide (a semiconductor term about one of the common layer in modern electronics) that is only a few atoms thick. Or here is  a [STEM](http://origin-ars.els-cdn.com/content/image/1-s2.0-S0038109805008914-gr1.jpg) of a quantum well of the type often use in vcsel and the like. Also atoms thick...

In a world where ""everything"" is made of atoms, all we have is made of ""stacks"" of single atom. The art is to stack them in the right order to make new properties arise that are useful to us. ",null,2,cdlnz09,1rbqyc,askscience,top_week,2
Jyesss,"Antibiotics target certain essential enzymes and proteins that bacteria have but humans do not, thus giving their specificity. These proteins are coded from genes, so in a round about way, yes, antibiotics are based off of genetic targeting in that they disrupt the proteins that those genes create. Bacteria generally do not become antibiotic resistant by changing the protein the antibiotic targets because this would probably kill the organism in the process. Instead, they evolve new genes that code for enzymes that will break down the antibiotic before it can harm the bacteria. ",null,0,cdme16k,1rbr4k,askscience,top_week,1
leftnuttriedtokillme,"There are a couple of hurdles.  For one thing, you can't target ""just bacterial DNA/RNA"", since the actual structure isn't any different from human DNA/RNA.  You can go after the proteins that form the nucleic acids, because those are slightly different.  And there are some antibiotics that do exactly that.  But it's rather difficult to target bacterial nucleic acids themselves specifically.

There has been some research into using artificial nucleic acids to target specific segments of a genome and basically turn it off, but I don't think they've been able to get it into a practical form that could be used in medicine quite yet.

There has, however, been success in using DNA to determine what antibiotics a particular organism is susceptible to.  Currently one of the normal steps in treating a bacterial infection is to culture and ID the organisms involved, and also to perform a susceptibility test on them, which determines the effectiveness of a number of common antibiotics.  

Traditionally this was done by exposing them to the antibiotic at certain concentrations and seeing whether or not it grows.  The new process allows us to look for the genes responsible for certain types of antibiotic resistance.

The best example of how this is useful would be for MRSA.  If a doctor suspects a MRSA infection, he would traditionally do a culture, which would take 1-3 days to tell him anything.  The new genetic method could tell him if it was *Staph. aureus* in a few hours, and whether or not it was the resistant form at the same time.",null,0,cdmhr6b,1rbr4k,askscience,top_week,1
abstrusey,"""Normal"" is usually defined by sampling lots of apparently healthy individuals and then using statistics to calculate an expected range into which the large majority will fall. This range is often referred to as a ""reference interval."" For aspects of physiology (e.g. heart rate, respiratory rate, temperature, blood pressure, etc.), these ranges are typically set as a standard that you read in a book and/or memorize. For test values (e.g. sodium/potassium/pH value of the blood, red blood cell count, etc.), they are often established individually by the testing facility, and they typically print that range next to the value of the sample they analyzed. 

A reference interval can be established by collecting test results from at least 50 healthy adult animals/humans. In this example, therefore, the range would only represents adults. At least 50 juveniles (and you'd have to define the age range) would have to be collected to have a new ""normal"" range determined. Two statistical analysis techniques are used, based on the distribution of the data. For normal distribution (also called gaussian distribution), there is a ""bell curve"". The data has ""variance,"" which is a representation of how widely scattered most of the animals are when compared to the average of all of them (e.g. they may all be tightly clustered near the average (low variance), or they could be very high and very low away from the average (high variance)). This variance is represented by a number called the ""standard deviation."" In gaussian and non-gaussian distributions, you can use the average ± 2 standard deviations to select for the ""middle"" 95% of the data. The lowest and highest numbers that you collected are now your range, and putting a dash between them makes it a reference interval. In vet med, we have reference intervals for most parameters for dogs, cats, horses, cattle, goats and alpacas, and many exotic species as well. 

It is very important to remember, though, that if the range only represents 95% of the samples -- 100% of which were apparently healthy -- then you should expect about 5 of 100 individuals to have values outside of the range and *still* be okay.",null,0,cdlm8h3,1rbtd9,askscience,top_week,10
gettingoldernotwiser,"Another way of defining ""abnormal"" is increased risk of death/disease.  People with high blood pressures can have an increased risk of cardiovascular disease, stroke or death compared to those with normal blood pressure.

Similarly, abnormal lab values (glucose, cholesterol) confer a higher risk of disease than normal ones. ",null,0,cdlrvxj,1rbtd9,askscience,top_week,1
Asrat,"In hospitals and other medical facilities, we typically take an individuals blood pressure every 4 to 8 hours and start generating a baseline for the individual. Your primary care physician does this as well to identify any changes. We also ask what you normally run if you are competent enough to answer. Using that information, for example, we can tell if you normally run 95/55 and today your pressure is 125/75, something is wrong and we need to identify that.
",null,0,cdlsaif,1rbtd9,askscience,top_week,1
patchgrabber,"Copper is toxic to algae and bacteria in moderate to high concentrations. A copper plate should have an antimicrobial effect, yes; brass doorknobs are known to have antibiotic properties and sterilize faster than, say, an aluminum doorknob.",null,0,cdm5yoq,1rbtxa,askscience,top_week,2
blacksheep998,"I encountered [this study](http://www.ncbi.nlm.nih.gov/pubmed/12042333) a few years back about whales. They found that the energy demands of accelerating their huge bodies to lunge-feeding speeds to fill their massive mouths with seawater and krill are extremely high.

Massive whales are up against the law of diminishing returns, where each unit they increase in size gives them less and less of a return on that investment.

More info: http://www.americanscientist.org/issues/issue.aspx?id=8779&amp;y=0&amp;no=&amp;content=true&amp;page=2&amp;css=print",null,1,cdlt3h1,1rbu2s,askscience,top_week,6
Izawwlgood,"Some of it has to do with what other organisms are doing. For example, one of the precipitous drops in insect size was due to the development of birds, which are superior fliers. Bison twice as large may be too cumbersome to evade a pack of wolves, for example.",null,1,cdlxbjl,1rbu2s,askscience,top_week,3
promega,"The largest discovered organism on earth is actually a fungus.  

""The discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, believed by most to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. Based on its current growth rate, the fungus is estimated to be 2,400 years old but could be as ancient as 8,650 years, which would earn it a place among the oldest living organisms as well.""

Source: http://www.scientificamerican.com/article.cfm?id=strange-but-true-largest-organism-is-fungus

In theory such an organism could continue to grow until it exhausted one of its resources.",null,0,cdmqwxw,1rbu2s,askscience,top_week,1
null,null,null,3,cdls9rb,1rbu2s,askscience,top_week,1
deruch,"You need to be more careful with your terms.  How are you defining size?  By mass, volume, area, etc.?  Do you really mean ""animal"" instead of ""organism""?  In terms of organisms, I can make the claim that the largest one is an [aspen forest](https://en.wikipedia.org/wiki/Pando_%28tree%29) in Utah, or maybe a [fungus colony](https://en.wikipedia.org/wiki/Largest_organisms#Fungi) in Oregon.",null,17,cdlse7h,1rbu2s,askscience,top_week,11
blacksheep998,"I encountered [this study](http://www.ncbi.nlm.nih.gov/pubmed/12042333) a few years back about whales. They found that the energy demands of accelerating their huge bodies to lunge-feeding speeds to fill their massive mouths with seawater and krill are extremely high.

Massive whales are up against the law of diminishing returns, where each unit they increase in size gives them less and less of a return on that investment.

More info: http://www.americanscientist.org/issues/issue.aspx?id=8779&amp;y=0&amp;no=&amp;content=true&amp;page=2&amp;css=print",null,1,cdlt3h1,1rbu2s,askscience,top_week,6
Izawwlgood,"Some of it has to do with what other organisms are doing. For example, one of the precipitous drops in insect size was due to the development of birds, which are superior fliers. Bison twice as large may be too cumbersome to evade a pack of wolves, for example.",null,1,cdlxbjl,1rbu2s,askscience,top_week,3
promega,"The largest discovered organism on earth is actually a fungus.  

""The discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, believed by most to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. Based on its current growth rate, the fungus is estimated to be 2,400 years old but could be as ancient as 8,650 years, which would earn it a place among the oldest living organisms as well.""

Source: http://www.scientificamerican.com/article.cfm?id=strange-but-true-largest-organism-is-fungus

In theory such an organism could continue to grow until it exhausted one of its resources.",null,0,cdmqwxw,1rbu2s,askscience,top_week,1
null,null,null,3,cdls9rb,1rbu2s,askscience,top_week,1
deruch,"You need to be more careful with your terms.  How are you defining size?  By mass, volume, area, etc.?  Do you really mean ""animal"" instead of ""organism""?  In terms of organisms, I can make the claim that the largest one is an [aspen forest](https://en.wikipedia.org/wiki/Pando_%28tree%29) in Utah, or maybe a [fungus colony](https://en.wikipedia.org/wiki/Largest_organisms#Fungi) in Oregon.",null,17,cdlse7h,1rbu2s,askscience,top_week,11
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,top_week,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,top_week,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,top_week,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,top_week,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,top_week,2
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,top_week,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,top_week,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,top_week,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,top_week,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,top_week,2
iorgfeflkd,"The sun's power is distributed evenly over an area. When you are focusing it with a magnifying glass, you are essentially taking all the light that reaches an area the size of the lens and combining it to a region the size of the focus.",null,2,cdlpmaf,1rbw5f,askscience,top_week,3
Mazetron,The sun is a powerful source of light on many wavelengths.  The mafnifying glass just focuses light.  You could burn something with an electric light and a magnifying glass of the light was poweful enough.  I have done it with a laser pointer.,null,0,cdlqqmx,1rbw5f,askscience,top_week,1
chrisbaird,"Light carries energy. Energy causes damage to materials when it is absorbed in a given area faster than it can be dissipated from that area. The ability of energy (and therefore light) to damage materials therefore is a result of a high energy delivered per unit area per unit time, which we call power density. A lens does not create energy, it just focuses the energy so that there is a high power density in one small region and lower power densities in surrounding regions. If the power density is high enough in the focal region, the material heats up faster than it can cool to its surroundings, and its temperature steadily rises. With high enough temperature, materials will melt, burn, ignite, etc.

Light can be focused because it is a wave and waves can be bent (refracted) at the interface between two optically dissimilar materials (such as glass and air). ",null,0,cdmufp9,1rbw5f,askscience,top_week,1
neverlupus16,"It's the infrared light of the sun. The fusion reactions of the sun release electromagnetic radiation across the spectrum. You have infrared photons, which explains why you feel heat. You have visible light, which explains how we can see things using sunlight. And we lastly have ultraviolet light, which is how our skin burns and tans.

The wave nature of light allows it to be refracted by traveling through a medium such as glass. By focusing the wave (and all of it's energy) to a single point, you change the energy from being diffuse and spread out to being concentrated in one small region. It is now easy for the infrared energy to be transferred to another object. If the energy is transferred faster than it can be dissipated, the object will increase in temperature and possibly reach ignition.",null,8,cdlon4k,1rbw5f,askscience,top_week,1
therationalpi,"Common misconception about sonic booms: You don't just create a boom at the moment when you ""break the sound barrier."" In truth, for the entire duration that an object is traveling faster than the speed of sound, it generates a shock front. This is more apparent when you look at an image of the effect. Here's some [Schlieren photography](http://library.thinkquest.org/12228/Page4.html) of a supersonic jet. The dark colored bow shocks that start in front of the plane are the ""sonic booms"" that it's creating.

So, in answer to your question, nothing particularly special happens when you reach 2 or 3 times the speed of sound. Indeed, you will still be creating sonic booms at those speeds, but you would likewise be continuously generating booms if you were traveling at 1.2x the speed of sound.

Hope that helps!",null,5,cdlnxzg,1rbw8z,askscience,top_week,24
rocketsocks,"You don't create a sonic boom only when you pass the speed of sound.

When you are traveling at or above the speed of sound you trail a cone shaped shock front which travels at the speed of sound. You usually only here the sonic boom once as an observer because the shock front only passes over you once.",null,2,cdlo7lt,1rbw8z,askscience,top_week,5
therationalpi,"Common misconception about sonic booms: You don't just create a boom at the moment when you ""break the sound barrier."" In truth, for the entire duration that an object is traveling faster than the speed of sound, it generates a shock front. This is more apparent when you look at an image of the effect. Here's some [Schlieren photography](http://library.thinkquest.org/12228/Page4.html) of a supersonic jet. The dark colored bow shocks that start in front of the plane are the ""sonic booms"" that it's creating.

So, in answer to your question, nothing particularly special happens when you reach 2 or 3 times the speed of sound. Indeed, you will still be creating sonic booms at those speeds, but you would likewise be continuously generating booms if you were traveling at 1.2x the speed of sound.

Hope that helps!",null,5,cdlnxzg,1rbw8z,askscience,top_week,24
rocketsocks,"You don't create a sonic boom only when you pass the speed of sound.

When you are traveling at or above the speed of sound you trail a cone shaped shock front which travels at the speed of sound. You usually only here the sonic boom once as an observer because the shock front only passes over you once.",null,2,cdlo7lt,1rbw8z,askscience,top_week,5
neverlupus16,"The question here is phrased incorrectly: when it's a cold morning and you breathe out, you see the water in your breath. But it's not water VAPOR. Water vapor is gaseous water. When it is cooled sufficiently, it will condense into liquid water. 

That condensation is what is actually happening when you see your breath. You see VERY tiny droplets of liquid water suspended in the air. They are so small that the current of air from your lungs will suspend and move them in front of you AS IF they were a gas.",null,1,cdlokzd,1rbyel,askscience,top_week,18
MarkWW,"Supersaturation is the point at which you can no longer stir sugar into your tea, because the water - at that temperature - can no longer dissolve solids into it. Cool it further and more of the sugar comes out as a solid.

The same happens with cold air &amp; foggy breath. Think of other forms of condensation - water appearing on the outside of a cold glass, or on a window. This is water vapor (gas) from the air turning into a liquid, because the surface is so cold that it turns the gas into a liquid.

The same is true for the breath you can see on cold day. The air outside your body is so cold, that the warm, moisture laden air inside your body instantly turns into liquid - albeit, in very tiny droplets.

Something similar happens when you create rock candy &amp; I encourage you to make rock candy with your brother... Because, yum.

Basically warm gas (or liquid) can support lots of gaseous liquid (or solid) in it because it's so chaotic and full of energy that it can keep the liquid (or solid) a gas (or liquid).

As it cools down, it loses energy and more of the matter that's right at the edge of being a gas/liquid reverts to the state you normally associate it with at that temperature. In other words, water isn't always a liquid between 0C and 100C - it's in a constant state of flux, with more water being gaseous the warmer it gets.

For more mind blowing facts - research Swamp Coolers, which cool the air by adding water to it.",null,0,cdlr7a9,1rbyel,askscience,top_week,4
chrisbaird,"Water vapor always comes out of your mouth in gas form (water vapor) when you breath. Water comes out of your mouth in liquid form when the air is cold enough to condense the gaseous water that you always breath out into small drops of liquid water, which we see as a white cloud of steam. ",null,0,cdmu6un,1rbyel,askscience,top_week,1
goingforth,"It is likely a combination of both, but your former suggestion likely has the largest effect. Moving clouds tend to maintain a consistent shape over relatively short periods of time, and the distortions that do occur are often just that, and don't involve the addition or subtraction of parts of the clouds (again, this is on a short time frame) Likewise, clouds tend to move faster with higher wind speeds, suggesting a correlation. Your second suggestion is responsible for clouds forming, changing shape, and disappearing, but not as much for the movement of clouds.",null,0,cdlptxo,1rbymp,askscience,top_week,2
EdwardDeathBlack,"I am not sure you are asking a ""clean"" question. 

First,  let us assume room temperature (293K) and atmospheric pressure (101300Pa or so) for most/all of our discussion. Under different condition, water cohesion can change drastically. 

In the absence of gravity, water will easily for an orb the size of a baseball. SO water cohesion

In the presence of gravity, water reacts like a viscous material. It means the rate of deformation of the water is proportional to the stress applied. Note that a viscous material will always deform in the presence of an arbitrarily small stress.  So if I consider this, you are asking how to make water into an elastic material, instead of a viscous material. Freezing seems the obvious answer. 

There are also polymeric materials that you can add to the water to bind it in place and make it an elastic material, [Jell-O](http://en.wikipedia.org/wiki/Jell-O) being particularly well known. 

No sure I answered your question, but as I said, I am not sure I completely grok'ed your intent...",null,0,cdlo75d,1rbyp8,askscience,top_week,1
SmellyRaghead,"Yes, you can alter the surface tension by using surfactants. As for making a giant ball of it, probably not, unless you were in zero gravity.",null,0,cdlr0fy,1rbyp8,askscience,top_week,1
dreemqueen,"If you dissolve ionic compounds like NaCl into water, the water becomes more polar and cohesive.  If you dissolve sucrose which is an organic covalently bonded molecule in water,  the water becomes less polar and less cohesive.  You can see the difference if you measure the wetting or contact angle.   Salt increases the contact angle, sugar decreases it.  This is the best way to measure relative surface tension.",null,0,cdmqcd4,1rbyp8,askscience,top_week,1
SingleMonad,"You don't have everything pinned down in your question.  Namely you need to know how big an ice cube.  Given a 500 ml glass of water, it may well be impossible with what would be considered a conventional ice cube.  

Assuming that no heat is lost to the environment, setting the heat lost from the water equal to the heat gained by the ice, using values in Wikipedia for specific heat and heat of fusion for water, and assuming the final temperature is 0 c, I get that the original ice cube warms by the following amount:

**Delta T = 200 M/m** (degrees c), where M is water mass, and m is ice mass.  Since the final temperature (0 c) is 271 degrees above absolute zero, it had better be a pretty big cube.

Disclaimer:  my arithmetic sucks.  Don't bet anything important on the basis of my answer.

Edit:  also assumed the water was initially at room temp (21 c).",null,1,cdlodsp,1rbyvm,askscience,top_week,9
Farnswirth,"Due to the conflict between /u/just_helping and /u/InexplicableContent results I did my own calculations, which came out to:

167000+2100(Initial water temperature, C) = 576.03(mass of ice) - 2.11(mass of ice)*(temperature of ice, K)

with all masses in grams.

When the initial water temperature is 25C and the temperature of the ice is absolute zero, the required mass of ice needed to freeze the whole cup is: **381g, which agrees with** /u/just_helping 

With an initial water temperature of 21C, and ice temperature of absolute zero, the mass of the ice is: **336g**

For a more realistic scenario, you could assume the initial water temperature is at 1C, and the ice has been cooled with liquid helium (4K).  This gives an ice mass of: **298g**  For liquid nitrogen (77K) the ice mass is: **409g**",null,2,cdlr4kj,1rbyvm,askscience,top_week,5
null,null,null,0,cdlpv6f,1rbyvm,askscience,top_week,2
tysongrey,What temperature is the room?,null,0,cdlnj0m,1rbyvm,askscience,top_week,1
Gradri,"That depends on the initial temperature of the water (probably about 21 °C), and the mass of the ice cube.",null,0,cdlnxxe,1rbyvm,askscience,top_week,1
EdwardDeathBlack,"You are asking two questions. The first one is what do thermal fluctuations look like in a  DNA molecule. The second is can DNA melt. 

First, the double helix of DNA is indeed held together by Hydrogen bonds. Above a certain temperature, the energy is high enough to overcome those bonds. The double helix melts and the two molecules separate. This has been extensively [studied](http://en.wikipedia.org/wiki/DNA_melting) and is an essential tool of biotechnology (and especially of [PCR](http://en.wikipedia.org/wiki/Polymerase_chain_reaction) ). If you have any background in the thermodynamics of how, say, water, freezes and melts, you will find it very similar to look at DNA melting. 

Now, to address the first point, what does a hot (but not melted) DNA molecule look like. First, a DNA molecule will [""ball up""](http://en.wikipedia.org/wiki/Random_coil), it doesn't stay a nice stretched thing. That little ball jiggles and wiggles along with the solvent, exhibiting [Brownian motion](http://en.wikipedia.org/wiki/Brownian_motion) . It will also have [thermal phonons](http://en.wikipedia.org/wiki/Phonon), of the [1-dimensional kind](http://en.wikipedia.org/wiki/Phonon#One_dimensional_lattice). 

If the temperature is high enough, but not enough to melt completely the DNA, there will also be [""bubbles""](http://www.bu.edu/meller/research_bubbles.html) forming alongside the DNA of partially melted areas.. These will have a limited lifetime, will occur predominantly in areas of weak Hydrogren bonding (AT rich areas), and have a lot of roles to play in living organism. 

Anyway, all that is already a lot. Maybe I'll let you read what I linked to and ask more questions rather than drone on...",null,0,cdloddj,1rbz3z,askscience,top_week,2
owaisofspades,"yup, you have about 10 layers and 5 different types of nerve cells  on your retina, and the photoreceptors are one of them. They have pigments on them (rhodopsin for rods and iodopsin (?) for cones) which respond to light, and transmit the signal through the rest of the layers into your optic nerve",null,1,cdlos9c,1rc0vh,askscience,top_week,4
dakami,"Genes (OPN1SW, OPN1MW, and OPN1LW) express one of three proteins, called opsins.  When an opsin is hit by a photon, it has some chance of isomerizing.  This process causes an electron to be released, and the chance is related to two things:

1) The wavelength of the photon
2) Which opsin is hit

Your ""red"" opsins are more likely to be isomerize in response to ""red"" light.  (This is an extreme oversimplification.)

As you can imagine, nerves are quite good at responding to electrical signals, and creating complex computational cascades.  So vision stops being about light really, really early.

Once isomerized, the spent opsin is recycled.  Your retina is actually among the most (if not the most) metabolically active portions of the body.  Really, your eyes work a lot more like living film than CCD/CMOS silicon.",null,0,cdlqugn,1rc0vh,askscience,top_week,2
brawnkowsky,"not all of it is rinsed, some of it passes into the epithelial cells.  once it passes through the epithelium, it is able to inhibit cyclooxygenase, an enzyme necessary for the creation of prostalgandins.  prostaglandins are involved in pain and vasodilation, two major components of inflammation, which is what a 'breakout' is

source: wikipedia, student of medicine",null,1,cdm050e,1rc303,askscience,top_week,2
Quant_Liz_Lemon,"You need to be awake during brain surgery in order to ensure that nothing important is damaged during the procedure. This is especially important if surgery is being conducted near functional areas of the brain. Otherwise, you might risk permanent brain damage. Depending on what area of the brain you're near, a surgeon might ask you to make specific movements, count, say specific phrases, etc, while performing the surgical procedure. 

[source: Mayo Clinic](http://www.mayoclinic.org/awake-brain-surgery/about.html)",null,0,cdluba1,1rc3d0,askscience,top_week,7
U235EU,"I work for a medical device company, one of our products is used to treat movement disorders by deep brain stimulation. The patients are conscious during the implant so that the doctor can insure the proper location of the stimulating leads by direct feedback from the patient, and by neurological monitoring. See this video:

http://www.youtube.com/watch?v=lUG8iFxukig",null,0,cdm1wfl,1rc3d0,askscience,top_week,1
Polyknikes,"Some surgeons still perform operations while the patient is awake but a more modern technique is to use various functional brain imaging techniques prior to the surgery while asking the patient to perform certain tasks, seeing which areas of the brain light up near the tumor, and then avoiding those areas during the surgery.  With this technique the patient can be fully sedated during surgery.",null,2,cdlxqlz,1rc3d0,askscience,top_week,1
Ruiner,"I'm having trouble understanding exactly what you mean. But let me try to answer:
(First, keep in mind that if there is a liquid inside, things are a lot more complicated because of convection, which makes thermalization faster than if there was only conduction)

You start heating the bottom of the point. At this point, that surface gains heat through the source and loses heat through conduction, fine. If you leave the source on, it will at some point reach a steady state. This state is not really thermal equilibrium, but it's a state in which you have a constant flux of heat from the bottom to the top, so there is a temperature gradient. You know that you reached the steady state because the temperature of everything inside the pot is no longer increasing, but all the energy coming in is compensated by energy going out. (btw, if you're trying to boil a pot of water, you never get to see the steady state, since the average temperature keeps on increasing).

Once you turn off the source, the steady state will now be approximately thermal equilibrium. It would be thermal equilibrium of the pot was a closed system, but since it interacts, it will still have a temperature gradient - the coldest parts being those that interact the most with the outside. ",null,1,cdlt6gm,1rc3z2,askscience,top_week,2
PENIS_VAGINA,"Interesting question. I'll try to answer this. 


First off sugar does not neutralize the acidity. However you are correct that sourness is based on H+ ion mediated receptors on the tongue (TRP family receptors). 


I don't believe the sugar changes our tongues ability to taste sour because sweet and sour receptors are distinct from each other so there should not be competitive inhibition of sour receptors when a substance that activates sweet receptors is present. In fact, it is possible that the low pH from the sour substance is enhancing ligand binding to sweet receptors. This is what happens when you ""Taste Trip"" with miraculin. 

My thought is that you are experiencing both tastes simultaneously and therefore your brain in an attempt to process both tastes is not pronouncing the sour taste as strongly as it would if sour was the only taste happening. 

I am looking for a source to confirm this and will update if I can find something. 

Edit: May as well add (because its a common misconception) that the classic ""taste map"" that shows different areas of the tongue to have varying densities of different kinds of taste receptors is FALSE.",null,0,cdlxpl2,1rc6y9,askscience,top_week,1
zalaesseo,"Stationary electrons generate electric fields. No current

Moving electrons generate magnetic fields. Constant current

Accelerating electrons generate electromagnetic fields. Changing current.

Oscillating circuits generate changing currents, and thus electromagnetic fields. 

Then we can either amplitude or frequency  modulate signals into the carrier wave.",null,0,cdludhm,1rc957,askscience,top_week,3
drzowie,"zalaesseo gave a nice answer.  Another, perhaps even more simple, is:

Shaking electric charge produces electromagnetic waves, just as shaking objects in air produces sound waves (the physics is different but the fundamental waveness is the same).  We make electromagnetic signals by shaking electric charges.  

Every time you switch on or off a circuit, you create electromagnetic waves - so there's a lot of electromagnetic noise all around us.  To punch through that interference, devices that signal each other (like radios, or phones, or wifi units, or whatever) pick a particular frequency and shake electric charges at almost exactly that frequency.  That's like cutting through the noise of a large room full of drunk people, with a particular clear tone (say, from a flute).  You can discern the flute even though it's not any louder than the people, because all the energy is concentrated into a single tone.

Small variations in the strength or frequency of the electromagnetic signal carry the information people want to transmit.  For an old-style AM radio, the strength of the signal indicates where the speaker cone of the receiver should go.  The station gets ""louder"" and ""softer"" very rapidly to move your receiver's speaker cone around, forming sound waves.  FM radios use small changes in the pitch of the radiofrequency tone to control where the speaker cone should go.  Digital radios like wifi or modern cell phones use variants on those two strategies to communicate streams of bits.  Those bits encode the sounds and internet packets that are being transmitted.
",null,0,cdlw28q,1rc957,askscience,top_week,2
chrisbaird,"Causing electric charges to oscillate (bump, jiggle, shake, collide, change energy levels, transition between states, etc.) *always* creates electromagnetic waves, and not just in fancy circuits. The chair you are sitting on is emitting electromagnetic waves right now (mostly in the infrared) because its electrons are slamming together due to thermal motion. There are many ways to oscillate electric charges, and so there are many ways to create EM waves:

- heat them up so they collide more (incandescence)
- shake them up and down a wire using applied voltages (antenna radiation)
- excite electrons into different states and then have them transition back down (lasers, fluorescence, phosphorescence, gas discharge, chemiluminescse)
- send electrons passed a system of magnets that makes them wiggle or circle quickly (free electron laser, cyclotron radiation)
- smash a charged particle at high speed into a material (Brehmstrahlung)

If you want to send a signal on an EM wave, then you need to precisely control the shape of the EM wave. You must therefore precisely control the movement of electric charges. Electric circuits come in handy for that.
",null,0,cdmu1if,1rc957,askscience,top_week,2
JohnSmith1800,"There's sort of two features going on here, the macro and microscopic, so I'll detail them both.

The light first passes through the pupil and lens, which focus it in particular on a small patch of the retina known as the fovea. This is where the concentration of cone cells (those which detect colour) is highest. You also have a lot of rod cells (just detect light generally, more sensitive in dark settings) here, but they're more common as you move away from the fovea. Collectively rod and cone cells are photoreceptors. These photoreceptors are hooked up to bipolar cells, horizontal cells and ganglion cells, which together do some initial ""processing"" of the image before it passes down the optic nerve to the brain. Interestingly, because the horizontal, bipolar and ganglion cells are actually between the retina and the pupil, the optic nerve has to travel through the retina, which creates a blind spot in each eye, about 15degrees off-centre (which your brain lies to you to fill in).

On the microscopic level, photons travel through the eye to the retina, where some encounter either rod or cone cells. In rod cells there is a stack of ""plates"" which are coated in an enzyme called rhodopsin. When a photon is absorbed by rhodopsin, it changes conformation and can activate another protein known as transducin. Transducin is what is known as a ""G-Protein"", when activated it in turn activates another protein in turn, which then changes cGMP (a small second messenger) into 5'-cGMP. This leads to a closure of Na^+ channels. This hyperpolarises (makes more negative) the cell. Neurons only release neurotransmitter when they depolarise, so this reduces the release of neurotransmitter. I'm not familiar with the exact pathway in cone cells, it is photopsin rather than rhodopsin which absorbs the photon, but otherwise I believe it to be generally similar. This whole process actually takes quite a while, about 200ms from memory between when the photon hits and when your photoreceptor's sodium channels close. This is because of the ""protein cascade"" which occurs, it takes time. However, it does greatly increase your sensitivity to light: A single rhodopsin can activate ~500 transducins, which will in turn do ~500 cGMP's each. This will close ~100 sodium channels, stopping 10^~11 ions, and hyperpolarising the cell by almost a mV. That is to say, your eyes are actually very sensitive to light.

Intriguingly photoreceptors are actually inhibitory neurons, they release a neurotransmitter which hyperpolarises other neurons. As such, when a photon is absorbed their rate of firing decreases, which increases the firing rate of bipolar cells. The rest of the neural pathways are way over my head, but it involves ""receptor fields"" and the other accessory neurons.

Edit: 
Source: I'm a 2nd year physiology student / L. Sherwood ""Human Physiology: From Cells to Systems"" 8th Edn.
Also, I've left out a protein or two in the signalling pathway, but in terms of answering the question I think it's sufficient?",null,0,cdlu2z4,1rc9cl,askscience,top_week,2
rupert1920,"You can read about it in the Wikipedia article for [phototransduction](http://en.wikipedia.org/wiki/Phototransduction).

Basically, it involves the photoisomerization of a molecule of [retinal](http://en.wikipedia.org/wiki/All-trans_retinal) - the incoming photon excites an electron in that molecule, and allows for a rotation of one of the double bonds. This causes a conformational change in the protein that houses it - [rhodopsin](http://en.wikipedia.org/wiki/Rhodopsin) - which then sets of a cascade that leads to the nerve signal.",null,0,cdlu45k,1rc9cl,askscience,top_week,1
do_od,"Mountains on Earth will never be much taller than they are now. That is  because at some point the weight of the mountain causes such enormous pressure that the base of the mountain will start to liquify and deform.  

Planets with lower gravity can have higher mountains. A prime example is [Olympus Mons](http://en.wikipedia.org/wiki/Olympus_mons), the tallest mountain on Mars at 22 km. The surface gravity on Earth is about 2.7 times that of Mars. Because pressure scales linearly with height, we can expect the tallest mountains on Mars to be about 2.7 times taller than the tallest on Earth. 2.7 * 8.9 km = 24 km, is in that ballpark.  

Wether or not Mt Everest is higher than the atmosphere depends on your definition. You could say that it is because humans can't survive there for very long.",null,0,cdm3ptj,1rc9ea,askscience,top_week,3
soylentblueissmurfs,"One of the reaons inbreeding can be harmful is you run a much higher risk of recessive genetic disease since your relatives are more likely to carry the same damaged alleles. However, if you inbreed enough those mutations will be weeded out so the answer is basically: they are SO inbred it's a small problem.",null,0,cdlut33,1rc9t0,askscience,top_week,5
mak484,"I'll handle the follow-up question. Mice are an ideal model organism for many reasons- they have a relatively short gestational period, produce many offspring, and reach sexual maturity very quickly. All of these factors lead to incredibly short generation times with a large exponential increase in population size with each generation. This allows scientists to very easily weed out deleterious recessive alleles, leaving breeding stocks with very uniform and well-understood genotypes.Compare mice to humans- it takes 12-16 years to reach sexual maturity, and females can only give birth (naturally) to 1-2 offspring per year. Factor in a high level of allelic heterozygocity, and you can see where aliens would have a difficult time creating a genetically uniform breeding stock.*I got a little morbid below, sorry if this disturbs anyone*Now, if I were the aliens, assuming I had unlimited resources and appropriate technology, I would find brother-sister pairs of paternal twins, and harvest their sperm and eggs. I would then fertilize all of the eggs simultaneously, and begin genetic testing once the fetuses reached several weeks. The offspring with the highest levels of homozygocity would be selected for the F1 generation of breeding, where I would repeat the process. After maybe 5-10 generations complete homozygocity and lack of deleterious genes could be reached. Since I started this process with numerous genetically diverse brother-sister pairs, I could develop multiple lines of humans that carry whatever combination of genes I want. ",null,0,cdlvtyw,1rc9t0,askscience,top_week,4
Platypuskeeper,"&gt; What exactly is an oil? [the requirements to call something oil]

Typically something with a lot of long-chain hydrocarbons in it, but in the broadest sense (e.g. essential oils, vegetable oils) it could really be any liquid composed of non-polar (oily) compounds. It's not a precise (or 'technical') term as far as chemistry is concerned.

&gt; Are there oils which do not have carbon in it?

[Silicone oil](http://en.wikipedia.org/wiki/Silicone_oil), although the term 'oil' there is more because of its use as a lubricant than its chemical composition.

&gt; Is it true that kerosene is not technically an oil? And why?

I don't see what usable definition of 'oil' would exclude kerosene. It's not _crude oil_, it contains a more limited subset of hydrocarbons. But it's still a hydrocarbon mixture.


",null,1,cdm06t8,1rc9wk,askscience,top_week,4
dapwnsauce,"Most oils do not dissolve in water as they have two characterizing features, a polar(hydrophobic) and non polar end(hydrophobic).  An example is a [Micelle](http://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Micelle_scheme-en.svg/532px-Micelle_scheme-en.svg.png).  Kerosene is an really long hydrocarbon chain in comparison to others which are used as **fuels**(although there are some which are longer).  Kerosene does not have a polar end, it is in other words a potential backbone to an oil.  Considering this, it would be classified more as a **fuel** rather than an oil.

Silicone oil does contain carbons and it maintains its ""oil"" status due to the polarity of the Si-O bond.  Non-polar end being the carbon groups that are attached to it.  The design of silicones have multiple chemical uses which sets them apart from conventional oils.  
  
The fluidity of an oil really depends on the hydrocarbon backbone.  The arrangement of the molecules can determine whether they become more rigid and less viscous or more fluid.  When oils are able to tightly pack together, they tend to form more viscous structures and even solid structures (ie butter/shortening).   When they have rigid backbones (ie kinks in the chain) they tend to form more fluid structures such as olive oil.  

Are there any oils that do not have any carbons in them whatsoever?  Theoretically there should be some that exist, though none come across my mind at the moment.

Hope that helps.",null,0,cdmkda9,1rc9wk,askscience,top_week,2
weinerjuicer,"without drawing energy from the environment, it seems like it should in principle be possible to exchange kinetic energy due to translational motion for gravitational potential energy: if they are going slower horizontally after the dive-down-then-pull-up move it may not violate conservation of energy.",null,2,cdlurjr,1rc9xn,askscience,top_week,6
drzowie,"Without some effect to bring energy to the hang-glider, diving and rising will always cause the hang-glider to go down.  That will always happen faster than if the hang-glider pilot just flew along at his best-sink-rate speed and attitude (which should be close to his best-glide-ratio speed and attitude)

That said, energy is not particularly well conserved in the hang-glider's system.  An experienced hang-glider pilot can make use of many counterintuitive effects, mostly involving wind shear or vertical winds, to scrub energy from the environment.  

Hang-gliders near Torrey Pines in San Diego, CA can fly all day without any thermals at all due to the vertical wind break at the Torrey Pines cliffs.
",null,2,cdlw5tk,1rc9xn,askscience,top_week,4
zlatan08,"According to conservation of energy, the total energy, which is the sum of gravitational potential energy, kinetic energy, chemical potential, magnetic potential etc.., must remain the same. In this case, let's consider just kinetic and gravitational potential energy. At any point in time, as long as no outside forces act on the glider (i.e. thermals), their sum must be constant. When the glider dips down, it trades gravitational potential energy for kinetic; when it rises back up, it trades the kinetic for gravitational potential. If it tries to rise to high, the kinetic will come close to zero and the glider will stall. In real life, if the glider keeps trying to dip down, rise, stall and then dip down again, drag forces on the surfaces on the glider will cause it to lose energy and every time the glider stalls, its height will be lower than the previous time. Drag would be considered the outside force in this case and energy would not be conserved.",null,1,cdlv4jy,1rc9xn,askscience,top_week,2
_Jordan,"I have heard that competition gliders, under the rules of the competition get towed to a certain height, and see how long they can stay in the air.

A trick they use, is to carry a tank of water with them when they get towed to the starting altitude. They immediately dive down to the ground (trading altitude for speed), then dump the water low to the ground, and go up (trading speed for altitude). Using this trick, they can rise up above the starting height, and stay in the air for longer.

If you see a hang glider diving off of something high, and then rising higher than they started, look to see if they dropped anything at the bottom of their dive.",null,1,cdm1ln5,1rc9xn,askscience,top_week,1
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,209,cdlyfi3,1rccq1,askscience,top_week,1149
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,top_week,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,20,cdlusqz,1rccq1,askscience,top_week,40
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,10,cdlwc5k,1rccq1,askscience,top_week,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,top_week,18
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,10,cdlvk3k,1rccq1,askscience,top_week,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,top_week,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,10,cdlushn,1rccq1,askscience,top_week,16
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,10,cdlz6us,1rccq1,askscience,top_week,15
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,top_week,14
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,11,cdm5uag,1rccq1,askscience,top_week,14
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,12,cdmelu9,1rccq1,askscience,top_week,15
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because ಠ_ಠ, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,9,cdlyunp,1rccq1,askscience,top_week,13
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,top_week,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,top_week,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,top_week,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,top_week,12
null,null,null,10,cdmg52h,1rccq1,askscience,top_week,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,top_week,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,9,cdm4kp8,1rccq1,askscience,top_week,11
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,209,cdlyfi3,1rccq1,askscience,top_week,1149
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,top_week,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,20,cdlusqz,1rccq1,askscience,top_week,40
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,10,cdlwc5k,1rccq1,askscience,top_week,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,top_week,18
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,10,cdlvk3k,1rccq1,askscience,top_week,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,top_week,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,10,cdlushn,1rccq1,askscience,top_week,16
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,10,cdlz6us,1rccq1,askscience,top_week,15
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,top_week,14
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,11,cdm5uag,1rccq1,askscience,top_week,14
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,12,cdmelu9,1rccq1,askscience,top_week,15
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because ಠ_ಠ, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,9,cdlyunp,1rccq1,askscience,top_week,13
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,top_week,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,top_week,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,top_week,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,top_week,12
null,null,null,10,cdmg52h,1rccq1,askscience,top_week,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,top_week,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,9,cdm4kp8,1rccq1,askscience,top_week,11
Platypuskeeper,"Ice behaves like a normal solid, the [density increases with lower temperature.](http://en.wikipedia.org/wiki/File:Density_of_ice_and_water_%28en%29.svg). 

Nothing particularly interesting happens at the molecular level, unless you have a phase change. The average distance between molecules decreases because they vibrate less at lower temperatures.
",null,1,cdltrht,1rcdt3,askscience,top_week,2
Wrathchilde,"There are many forms of solid water [ice](http://www1.lsbu.ac.uk/water/ice.html), and which form exists is a function of temperature and pressure.  Some forms, as described in the link above, are more dense than liquid water.

However, since you did not include pressure changes in your question, let's assume a constant 1 atm.  This [phase diagram](http://en.wikipedia.org/wiki/File:Phase_diagram_of_water.svg) shows that below about 70k ""normal"" ice will change to ice-XI, a more structured crystal.  However, as the first link describes, the density of ice-XI is pretty much the same.
",null,0,cdlufvc,1rcdt3,askscience,top_week,1
NotAStructrlBiologst,"I hope this gives you a better picture of what is going on at the molecular level.

Even in the crystal/solid state molecules may not move but their atoms continue to vibrate. Bonds contract/expand and angles wobble ever so slightly. Continuing to cool something will also decrease these motions, increasing the order",null,0,cdm0rnu,1rcdt3,askscience,top_week,1
Ruiner,"First, kudos for doing your own experiment and trying to interpret results.

Now, my suggestion is that you should try striking with two coins instead of one. And afterwards, use a coin that's a bit heavier than the others.

After you're done with it, try reading the physics explanation on this [page](http://en.wikipedia.org/wiki/Newton's_cradle).",null,1,cdlsz1r,1rcgby,askscience,top_week,6
Ruiner,"If you were in a planet without atmosphere, then it would, but since we have air friction, the bullet would land with its terminal velocity.

correction.: if its terminal velocity is bigger than its speed when it is fired. Otherwise, it will land with a smaller velocity, given the energy it lost to friction.",null,3,cdlt0kg,1rch1e,askscience,top_week,19
meerkatsrgay,"so, I have never been full body immersed in -90. However, I have ineracted with huge standup freezers that cool biological samples to -86C (I have seen -89 once on the readout) Generally when interacting with this environment you wear protective gloves. Grabbing a cooled piece of metal can be dangerous if held for more than a few seconds as it will freeze the moisture on your hand. But, through my interaction with these freezers I would assume that standing in an environment like that naked you would be fine for even up to 15mins provided you only contacted anything solid through your feet (and had shoes) AND there was absolutely no wind. In an environment with no air movement your body is able to build up a very tiny layer of warm air close to the skin. This is the same reason why holding dry ice or sticking your hand into liquid nitrogen is fine, the evaporated gas that is instantly created between you and the material acts as an insulator. 

Either way, I wouldn't recommend going outside naked in -90C weather because you prolly don't look as good as you think and no one wants to see you naked.",null,2,cdm6hb4,1rchgw,askscience,top_week,11
shiningPate,"See the information on the South Pole station [300 Club](http://en.wikipedia.org/wiki/300_Club). To get into the club, you have to run naked from the 200 degree sauna, down the access tunnel, outside 20 feet to the south pole marker, touch it, and return back to the sauna. The dash has to be done when the temperature outside is -100 F. ",null,5,cdm1xw7,1rchgw,askscience,top_week,10
null,null,null,2,cdm2oo5,1rchgw,askscience,top_week,4
meerkatsrgay,"so, I have never been full body immersed in -90. However, I have ineracted with huge standup freezers that cool biological samples to -86C (I have seen -89 once on the readout) Generally when interacting with this environment you wear protective gloves. Grabbing a cooled piece of metal can be dangerous if held for more than a few seconds as it will freeze the moisture on your hand. But, through my interaction with these freezers I would assume that standing in an environment like that naked you would be fine for even up to 15mins provided you only contacted anything solid through your feet (and had shoes) AND there was absolutely no wind. In an environment with no air movement your body is able to build up a very tiny layer of warm air close to the skin. This is the same reason why holding dry ice or sticking your hand into liquid nitrogen is fine, the evaporated gas that is instantly created between you and the material acts as an insulator. 

Either way, I wouldn't recommend going outside naked in -90C weather because you prolly don't look as good as you think and no one wants to see you naked.",null,2,cdm6hb4,1rchgw,askscience,top_week,11
shiningPate,"See the information on the South Pole station [300 Club](http://en.wikipedia.org/wiki/300_Club). To get into the club, you have to run naked from the 200 degree sauna, down the access tunnel, outside 20 feet to the south pole marker, touch it, and return back to the sauna. The dash has to be done when the temperature outside is -100 F. ",null,5,cdm1xw7,1rchgw,askscience,top_week,10
null,null,null,2,cdm2oo5,1rchgw,askscience,top_week,4
Polyknikes,"Short answer: Stimulation of the vagus nerve (CN-X) which induces bradycardia.

Long answer:

I had not heard the term apneic pause before but from googling it you are referring to a cessation of breathing for at least 10 seconds, commonly referenced in relation to sleep apnea.  I didn't know the term but I do understand cardiac physiology.

In 10 seconds your oxygen saturation levels do not drop by a measurable level.  Try holding your breath while using an oxygen saturation meter and see if you can even get it to go down by 1% - it's really difficult even if you can hold your breath for several minutes.  So I don't believe it is related to lack of oxygen unless we are talking about a much longer duration apneic pause.

During normal inspiration and expiration the heart rate increases and decreases, respectively.  This is thought to be induced by changes in vagal nerve tone (activity) but the mechanism by which breathing influences vagal tone is not understood.  The vagus nerve acts to reduce heart rate by hyperpolarizing the intrinsic pacemaking cells of the heart.  

Stimulating the vagus nerve would decrease heart rate and this can be accomplished by various means including hypoventilation, hypoxemia, respiratory acidosis, or vigorous inspiratory effort against a closed airway known as the Mueller's maneuver.  In the case of obstructive sleep apnea a person may be attempting to inspire but cannot due to a blockage of their airway (usually seen in obesity) which could stimulate the vagus nerve by the Mueller's maneuver mechanism.  But again, the exact mechanism by which the vagus nerve is stimulated by these pressure changes is not known, it has simply been observed indirectly.

I hope this at least partially answers your question!",null,0,cdlwfmx,1rci0v,askscience,top_week,3
Platypuskeeper,"There is no motion in a classical sense. Electrons have kinetic energy and that, but they do not follow specific trajectories. The probability of knowing where you might find the electron is all you've got. For specific energy states, these probabilities don't change with time. Electrons have no size of their own, their position-probability distribution in space is basically where the electron is.

The wave function/probability distribution, which for a single particle has [solutions like this](http://chemwiki.ucdavis.edu/@api/deki/files/8855/Single_electron_orbitals.jpg), is analogous to a standing wave in three dimensions. The angle-dependent part of the functions are the spherical harmonics.

Photons and electrons are particles, they both have energy but they have more than that as well. They carry both linear and angular momentum, for instance.
",null,3,cdlufy4,1rcj7e,askscience,top_week,5
The_Serious_Account,"&gt; So do electron move like those diagrams of standing waves? Or do they not wave like that, but whiz around like a particle? If I was reduced in size, is the electron a hard ball or is more of a packet of energy like a photon?


I think the most sensible thing to do is to give up the concept of particles. There's no such thing as a particle. Just drop the concept entirely. We keep it in language, but as a mental picture it's dangerous and misleading. 

The electron doesn't move around within the standing wave. The electron *is* the standing wave. The word electron should refer to the standing wave and nothing else. It's an extremely hard concept to accept, but repeat it to yourself. There is no particle. There is no particle. There's only the wave function. ",null,2,cdlucvm,1rcj7e,askscience,top_week,3
biffym,"It wasn't to make them believe it was real so much as to make it feel more real. They knew it was an experiment and that being arrested was part of it, but it adds to the feeling that they aren't there by choice. If they'd walked in of their own free will the jail would have felt less oppressive.",null,0,cdltsfb,1rcjmy,askscience,top_week,3
DougWC,"What is most interesting about the experiments and others like it is not the obvious.  It's the implications for ""real world"" situations of authority and subordination - that they are no more truly legitimate than are contrived ones.  They are all contrived and all should be seen through.",null,0,cdm1916,1rcjmy,askscience,top_week,1
cromonolith,"The only thing stopping this from being intuitive to you is what ""bigger"" means. 

You're used to judging the sizes of things by counting each of them and comparing the numbers. That is, if I gave you two bunches of apples, you might count and see that there are 10 apples in the first bunch and 14 in the second, and conclude that the second bunch is bigger. 

That's fine, but it's not a good way of measuring the size of infinite collections. So here's a better way of comparing the size of two collections: make a pairing between the collections, and the one that has stuff left over is bigger. 

As an example, let's say we had a huge auditorium and a huge crowd of people who want to see a show there. What's the best way to see if we have the same number of seats as people? You can count the seats and the people, but that's dumb. The smart thing to do is to tell everyone to sit down. As long as no one sits down stupidly (two people in the same chair), then you can easily check. If there are empty seats left over there are more seats than people. If there are people left standing there are more people than seats. 

What does this have to do with infinities? Well, this kind of pairing (like pairing people with seats) is a function. If no two people sit in the same seat, the function is called ""injective"" (or ""one-to-one""). If no seats are left unoccupied, the function is called ""surjective"" (or ""onto""). If a function is both injective and surjective (one person to a seat and all the seats are filled) the function is called ""bijective"". 

Bijective functions are what we use to compare the sizes of all sets, including infinite ones. Two sets are *defined* to be the same size if there exists a bijective function between them. So for example, the set of all natural numbers **N** = {1, 2, 3, 4, ....} is the same size as the set of even numbers 2**N** = {2, 4, 6, 8, ...} because, as you can check, multiplication by two is a bijective function from the former to the latter. That is, the function f where f(n) = 2n hits every even number and never sends two numbers to the same even number. 

On the other hand, you might want to compare the set of natural numbers as above to the set **R** of real numbers (the whole number line). This isn't obvious, but there's a [relatively straightforward proof](http://www.mathpages.com/home/kmath371.htm) that no matter how you try, it's impossible to make a bijective function from **N** to **R**, so they can't have the same size. Since **N** is actually a subset of **R**, we say the size of **N** is smaller than the size of **R**. ",null,1,cdlzcjv,1rcjr9,askscience,top_week,18
rlee89,"There are several notions of size when it comes to infinity.  The most common is *cardinality* which in lay terms asks whether one infinity can be fit (or more formally, mapped) into another.

For example, if you have a list of the positive integers, there is a way to place a rational number next to each positive integer in the list in such a way that each rational number is next to some positive integer, and vice versa.  We would formally call this a bijection between the positive integers and the rational numbers, and it would demonstrate that the two are the same cardinality.

[Cantor's diagonal argument](http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument) shows that any attempt to match up the positive integers with the real numbers will necessarily miss at least one real number.  Thus the of real numbers won't fit into the poisitive integers, and thus have a larger cardinality than the positive integers and are a bigger infinity.",null,1,cdlznix,1rcjr9,askscience,top_week,2
protocol_7,"Here's another way of thinking about it: Any natural number can be represented by a finite amount of information. (For example, you can write it down in base 2 as a finite string of 0's and 1's.)

However, a real number has an *infinite* string of digits past the decimal point, so there's no guarantee that any particular real number can be expressed using a finite amount of information. (And, in fact, most real numbers *cannot* be uniquely identified by any finite amount of information.)

So, intuitively, it's the difference between finitely many versus infinitely many ""degrees of freedom"". Since specifying a single real number can involve an infinite number of choices of digit, there are vastly more real numbers than natural numbers.

This is just a vague intuition, though. You can make it precise using [information theory](https://en.wikipedia.org/wiki/Information_theory), but that's rather technical. Instead, here's how to see very quickly that some infinite sets are larger than others — in particular, that given any set (finite or infinite), there's another set that's larger.

By definition, two sets have the same [cardinality](https://en.wikipedia.org/wiki/Cardinality) (basically, size) if they can be put into [one-to-one correspondence](https://en.wikipedia.org/wiki/Bijection) with each other — that is, if you can pair up elements of the sets so that each element of one set is paired up with a unique element of the other set.

**Theorem** ([Cantor](https://en.wikipedia.org/wiki/Cantor%27s_theorem))**.** Let S be any set, and let P(S) be the [set of all subsets of S](https://en.wikipedia.org/wiki/Power_set). Then S and P(S) do not have the same cardinality.

*Proof.* Let f be any function from S to P(S), that is, for each element x in S, we assign a unique element f(x) in P(S). Since each f(x) is a subset of S, we can ask whether x is an element of f(x). In particular, let T be the set of all elements x of S such that x is *not* an element of f(x).

Now we can ask, is there some element y in S such that f(y) = T? Suppose there was: then we can ask whether y is an element of T. If it is, then y is an element of f(y), so by the definition of T, y is not an element of f(y) — a contradiction. If it isn't, then y is not an element of f(y), so by the definition of T, y *is* an element of f(y) — again a contradiction. Therefore, it's impossible for such an element y to exist. So no element of S is paired up with T.

Thus, *any* attempt to pair up elements of S and elements of P(S) fails, which means that S and P(S) don't have the same cardinality. ∎

But we can embed S inside P(S) by sending each element x in S to the [singleton set](https://en.wikipedia.org/wiki/Singleton_%28mathematics%29) {x}, a subset of P(S). So, in fact, S is strictly smaller than P(S).

Notice that there's no mention of whether S is finite or infinite. (If S is finite with n elements, then P(S) has 2^n elements.) So the proof is valid regardless, meaning that it's true for infinite sets, too.

For instance, if we denote the set of natural numbers by **N**, then P(**N**) is strictly bigger than **N**. (Actually, it turns out to be the same size as the set of real numbers.) And the set P(P(**N**)) is bigger than that, and so on.",null,1,cdlzsre,1rcjr9,askscience,top_week,2
null,null,null,3,cdlz5a1,1rcjr9,askscience,top_week,2
shiningPate,"First, this effect only occurs in a vacuum. In air, the gas molecules rapidly absorb the emitted electrons. Basically the reason xrays are emitted is because of the static electricity generated by separating the tape film from the surface below it. As the tape is pulled away, an electrical field is formed at the point of separation. As the roll rotates away, the film separates from the layer below it, reducing the strength of the field between the point that just separated, and the point at which it was previously touching. Meanwhile a new point has just separated from the roll, and higher strength field is formed at that point. The result is an electrical field at a gradient intensity in the wedge between the roll and film. At the point of separation, the field strength is so high electrons are ripped away from the film material. The film material was also electrically charged as part of the manufacturing process to get it to roll up smoothly. Electrons emitted into the gradient electrical field will begin accelerating along the gradient. Accelerating electrons emit xrays",null,0,cdm2gu7,1rcmj7,askscience,top_week,3
dampew,"http://www.nature.com/news/2008/012345/full/news.2008.1185.html

""The leading explanation posits that when a crystal is crushed or split, the process separates opposite charges. When these charges are neutralized, they release a burst of energy in the form of light.""",null,0,cdm0glp,1rcmj7,askscience,top_week,2
KerSan,"Energy is a consequence of a symmetry in the laws of physics. This is a deep point, so let me try to explain.

It is first worth explaining probably the most important concept in physics, momentum. Early on in your education, you are told that it is simply mass times velocity. This turns out not to be true generally, because momentum is really what is called the 'generator' of translations.

Consider a point particle that has a definite position in space. That position is actually an arbitrary label, since it refers to your chosen co-ordinate system rather than some actual property of the universe. You could just as easily switch co-ordinate systems by, say, moving the origin of your co-ordinates somewhere else. This action of shifting co-ordinates is called 'translation'.

Now let us suppose that the particle is moving (relative to you, the observer). No matter which co-ordinates you chose, you will notice that the position of the particle is changing over time. If you want to predict where you will see the particle next, you need to translate the last known co-ordinates of the particle by some amount. That translation is determined by the momentum of the particle. Keep in mind that the momentum is not changed when you translate your co-ordinate system. The momentum is, in a technical sense, dual to position.

What does this have to do with energy? Well, energy happens to be the generator of translations in time. This is because *energy is defined to be that quantity which remains invariant under translations in time*, much as momentum is that quantity which is invariant under translations in position. Your co-ordinates for time are just as arbitrary as your co-ordinates for position, because your choice of starting time is your choice and not some property of the universe.

Energy therefore tells you something about how quickly the properties of an object change over time. The more energy, the faster changes can happen. I could write essays explaining this further, but I don't want to lose you.

Work is the addition or subtraction of energy from a system. That energy can be added in a variety of ways, but it is common in early physics education to think in terms of force rather than energy. Energy is actually more fundamental than force, so I'll explain the force times displacement rule by first explaining force in terms of energy.

Force is the derivative of potential energy with respect to position. If you think of the potential energy of a particle as a function of the position of the particle (for instance, gravitational potential varies linearly in the height of the object), then force experience by the particle at a given position is the slope of the potential function at that position. In other words, force is the negative change in potential energy (i.e. the work) divided by displacement (in an infinitesimal sense). Multiply both sides by displacement and you have the relation work = force * displacement.

Hope that helps.",null,2,cdlzluj,1rcojz,askscience,top_week,7
miczajkj,"What is energy?

This is a very interesting question - but we we don't have an answer to this. We formulate physics in equations and stuff and there is this certain quantity that shows up in some of them and seems to be conserved under certain circumstances. 

The first kinds of energy you encounter, when you start to think about physics are the kinetic energy and the potential energy.
Assume a body at the position x that is exposed to a force F(x). Following Newton, you get the differential equation

m*x°° = F(x) 

The second derivative of the bodies displacement with respect to time times its mass is equal to the force on the particle.  
If it is possible to write the force as the derivative of a potential, namely F(x) = -V'(x) you see, that the following quantity is conserved for every solution of the mentioned equation: 

E = 1/2 mx°² + V(x) 

If you take it's derivative, you see: 
E° = m*x°*x°° + V'(x)*x° = x°*(mx°°-F(x))

So if the differential equation is fulfilled, then E' = 0 follows directly. 

You can see: because of the form of the differential equation that describes the movement, there is a certain quantity, that won't change - we call it a conserved quantity. 

If you dive deeper into the mathematical foundament of physics, you find that symmetries are responsible for the most conserved quantities. The energy for example, is only conserved, if the physical laws describing the process are time independent. 

All in all: energy is just a number. A phrase like 'pure energy' is nonsense. Energy is just a quantity, that appears in our equations. 

Work is defined as the change in energy you need to apply to a system, to move it from one state to another. In most cases this movement is a spatial movement from one place to another - then work is just the difference in potential energies, 

W = V_1 - V_2

or because V'(x) = F(x) 

W = integral F(x) dx

For forces that don't depend on position, you get 

W = F*x


I guess that was a whole lot of different concepts - maybe you've never heard of differentation, integration or newton's 2nd law: but I can't come up with an easier way to describe energy, that is not too simplified or wrong. ",null,0,cdlztxi,1rcojz,askscience,top_week,2
dampew,"Work has units of energy -- they're basically the same thing.  For instance, if you want to know how much work you've done, the answer would be in units of energy (joules, calories, whatever).

Why is it force times distance?  Well, you do more work if you have to push something harder through a larger distance.  I'm not really sure how to answer that question -- it's just a name for something, really...",null,0,cdm0kyk,1rcojz,askscience,top_week,1
FeckSakeLads,"work is the amount of energy you must add using a force of a certain strength to move an object with mass a certain distance in a certain direction (the displacement). the equation is:



work = force (joules) x displacement (metres) x cos(theta) (theta being the angle between the direction of the force relative to the direction of displacement).



See [this](http://www.physicsclassroom.com/calcpad/energy/) for more.",null,3,cdlzbtd,1rcojz,askscience,top_week,1
therationalpi,"Sound waves definitely *are* affected by the wind. Since sound waves travel through a medium, and wind is a bulk flow of the medium, the sound speed in a windy environment (which is normally the same in all directions) suddenly becomes direction dependent. Specifically, the speed of the wave becomes the speed of the wind in that direction plus the speed of sound at rest. Moreover, since wind tends to move faster the higher you move up from the ground, there is usually an effective sound speed gradient as well. In the presence of a sound speed gradient, sound waves tend to refract towards regions of lower sound speed. As a result, sounds sent out against the wind will tend to refract upwards, and sounds made with the wind will tend to refract downwards. Sounds made cross-wind will tend to refract downwind and up. And since your listeners tend to be near the ground (relatively speaking) the net effect is that sounds carry further with the wind than against it.

As for electromagnetic waves (light/radio), I don't believe there is any notable effect, but I would wait for verification from someone with more experience in the field.",null,0,cdlz4kd,1rcooj,askscience,top_week,7
ignorant_,"While we're waiting on someone with better credentials, I'll throw in that for every 4 inches in height over 5ft, a person has a 16% increased risk of cancer. So converse to your statement, it is my understanding that more cells, thus more cell divisions, means greater risk of cancer development. ",null,1,cdlztvc,1rcpmh,askscience,top_week,5
Aniridia,"Obesity does increase the risk of several cancers. The inverse, does being ""skinny"" lower the risk of cancers, is less clear, and I'm not sure if it has been directly studied. There's a [Lancet article](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(11\)60814-3/fulltext#article_upsell) that deals with many of the health risks of obesity, including cancers. (The article is free, but you must log in.) Here is a [PDF PowerPoint type presentation](http://www.mhsimulations.co.uk/Documents/WangC.pdf) of the article from the author. ",null,0,cdmr14g,1rcpmh,askscience,top_week,3
Azurity,"If you're up for a bit of fun and history, here's an ancient (1961) paper that originally investigated various mechanisms of polypeptide assembly: [ASSEMBLY OF THE PEPTIDE CHAINS OF HEMOGLOBIN](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC221568/pdf/pnas00219-0005.pdf)

It's actually a fun article to read if you've got an hour or three and you feel like working out a molecular puzzle using 50-year-old methods and logic. Scientists weren't sure if proteins were synthesized from one end to the other, or started at both ends, or if there was actually a giant cellular ""stamping"" machine that knit every amino acid of a protein together at once! Essentially, they used a series of radiolabeling and quenching experiments and froze moments in time as proteins were being made to mathematically derive a mechanism of N-terminus to C-terminus translation. Cool stuff!",null,0,cdm5qng,1rcqnv,askscience,top_week,6
PENIS_VAGINA,"Well read the section about Crick's contribution to molecular biology here: 


http://en.wikipedia.org/wiki/Francis_Crick#1951.E2.80.931953:_DNA_structure


I'm not sure that there was ONE definitive experiment that determined the mechanism. Perhaps there was, but I can't seem to find it.


If you are wondering how you can prove it now, a simple example would be using GFP to follow a DNA sequence to an mRNA transcript and then to a fluorescing protein. There are other experiments that rely so heavily on mRNA as the transcript of DNA that it basically 100% accepted as the mechanism. ",null,3,cdlytmo,1rcqnv,askscience,top_week,4
quantum_lotus,"It seems like you want the actual experiments that led to our understanding of the triplet nature of the genetic code.  I'll offer you two resources that explain the experimental procedures (and the logic behind them) that led to our current understanding.  Both are at a basic undergraduate level, so I doubt you'll have trouble following.

The first is from the Nobel Prize website called[ ""Crack the Code""](http://www.nobelprize.org/educational/medicine/gene-code/history.html).  I'd read the historic background, but the explaination of the experiment starts with the ""A Clever Experiment"" section a little further down the page.

The second is as [PDF](http://basic.shsmu.edu.cn/jpkc/cellbiota/resource/exper/11.pdf) I found with a basic google search (for ""cracking the tRNA code"") that is unattributed, but hosted on a site from the Shanghai Jaio Tong University.  It appears to be from an undergraduate level textbook and gives a more in-depth look at the same history and experiments.",null,0,cdn4cyp,1rcqnv,askscience,top_week,1
Azurity,"If you're up for a bit of fun and history, here's an ancient (1961) paper that originally investigated various mechanisms of polypeptide assembly: [ASSEMBLY OF THE PEPTIDE CHAINS OF HEMOGLOBIN](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC221568/pdf/pnas00219-0005.pdf)

It's actually a fun article to read if you've got an hour or three and you feel like working out a molecular puzzle using 50-year-old methods and logic. Scientists weren't sure if proteins were synthesized from one end to the other, or started at both ends, or if there was actually a giant cellular ""stamping"" machine that knit every amino acid of a protein together at once! Essentially, they used a series of radiolabeling and quenching experiments and froze moments in time as proteins were being made to mathematically derive a mechanism of N-terminus to C-terminus translation. Cool stuff!",null,0,cdm5qng,1rcqnv,askscience,top_week,6
PENIS_VAGINA,"Well read the section about Crick's contribution to molecular biology here: 


http://en.wikipedia.org/wiki/Francis_Crick#1951.E2.80.931953:_DNA_structure


I'm not sure that there was ONE definitive experiment that determined the mechanism. Perhaps there was, but I can't seem to find it.


If you are wondering how you can prove it now, a simple example would be using GFP to follow a DNA sequence to an mRNA transcript and then to a fluorescing protein. There are other experiments that rely so heavily on mRNA as the transcript of DNA that it basically 100% accepted as the mechanism. ",null,3,cdlytmo,1rcqnv,askscience,top_week,4
quantum_lotus,"It seems like you want the actual experiments that led to our understanding of the triplet nature of the genetic code.  I'll offer you two resources that explain the experimental procedures (and the logic behind them) that led to our current understanding.  Both are at a basic undergraduate level, so I doubt you'll have trouble following.

The first is from the Nobel Prize website called[ ""Crack the Code""](http://www.nobelprize.org/educational/medicine/gene-code/history.html).  I'd read the historic background, but the explaination of the experiment starts with the ""A Clever Experiment"" section a little further down the page.

The second is as [PDF](http://basic.shsmu.edu.cn/jpkc/cellbiota/resource/exper/11.pdf) I found with a basic google search (for ""cracking the tRNA code"") that is unattributed, but hosted on a site from the Shanghai Jaio Tong University.  It appears to be from an undergraduate level textbook and gives a more in-depth look at the same history and experiments.",null,0,cdn4cyp,1rcqnv,askscience,top_week,1
Das_Mime,"The expansion of the universe and the speed of light have different units, therefore you can't compare them.

The Hubble Constant is the fraction by which a given parcel of space will grow in a given amount of time. It has units of inverse time, s^(-1). The speed of light, of course, has units of distance/time, m s^(-1).

The Hubble Constant is usually given in units of kilometers per second per megaparsec, but the two distance units just cancel out and you get the result that the universe expands by about 0.00000000000000002% per second.

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

There will eventually come a time when there are no other visible galaxies in our observable universe (except for nearby ones that are gravitationally bound to us).",null,3,cdm01m4,1rcr2g,askscience,top_week,10
IAmMe1,"We in fact know that far-away parts of the universe are receding from us faster than the speed of light. However, this is not a problem. It's better to think about the expansion of the universe as an change in space itself rather than the motion of the things in that space; think of it as extra distance appearing between far-off objects. In this way, nothing is moving faster than light in the sense of any actual motion; instead, the distance between us and such an object increases faster than light can traverse that distance (i.e. more than 1 light-year of distance is added per year).

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

Yes indeed. It will be a dark and dismal universe that day far, far into the future!",null,1,cdlyqtu,1rcr2g,askscience,top_week,7
Luminarie,"Based on what we know about physics right now, we have no reason to believe it won't happen. Lawrence Krauss puts it quite succintly: ""Nothing can move faster than light in empty space, but space itself can to whatever the hell it wants"".

And yes, that's what would happen. At some point, the space between galaxies will be expanding faster than light, and at that point they will disappear from our region of the universe, as light would need to be faster than the expansion to be able to get to us. Therefore we will be causally disconnected from the rest of the universe.

Beyond the point where it accelerates faster than light, extrapolations based on an unchanging acceleration end in a [Big Rip](http://en.wikipedia.org/wiki/Big_Rip). Basically, the increasing speed of expansion overcomes all physical forces, and the universe would seem to end in a singularity (the scalar factor that defines expansion becomes infinite) at the moment when this happens.",null,0,cdlz409,1rcr2g,askscience,top_week,2
ofeykk,"I am going to attempt to translate, as best as I can, your problem into a mathematical question. I suspect you are making a bunch of assumptions here which I will try to make more formal.

First, with the view of simplifying as much as possible yet retaining the crux of the original problem, you can make the following assumptions (removing each would yield a slightly different problem to solve):

1. Look for curves not in 3-space but in 2-space.
2. Simplify curvature of earth to be flat — seek planar curves.
3. Simplify sun to be on this flat plane as a point.
4. Simplify yourself to be a point on the sought curve.
5. 180 degrees view is equivalent to dividing the plane by the tangent to the curve at your location (point).
6. Choose an orientation for the curve. This helps fix what it means for a point to be in your field of view or equivalently, to be in the ""correct"" half space of the tangent to the point (you) on the curve.
7. Parametrize looping around the curve to be traveling along the unit interval [0, 1] with the end points {0} and {1} identified — essentially a fancy way of saying that the end points of the interval are glued.
8. Assume that the sun is fixed relative to the time taken to loop around the curve once.
9. Finally is the curve sought smooth or not ? (A circle is a smooth curve whereas a triangle isn't one.) I believe it's easier (at least for me) to imagine smooth curves. Also, will exclude wild curves like space filling curves simply because I am not comfortable dealing with those !!

Now, the question is to find a curve that maximizes the view of the sun when you loop around once.

It appears to me that solution would depend on whether you would wish to make a further assumption about whether your plane is unbounded or not. 

If the plane is unbounded, the answer is simple — any straight line not through the point representing the sun would do.

If the domain on which the curve is sought is compact (or technically if its closure is compact) — think finite if you wish, like a square plot or a circular plot of land — then it depends on where the sun is located relative to this domain. Some examples that come off the top of my mind are as follows:

1. Circular region with the sun in the center: Take any diameter and take a tube around the diameter. This tubular region would have a boundary curve composed of two straight lines (chords of the circle) with a small part of the circular's plot's boundary (two of these actually). You can make this tubular region as small as you please and would provide you with a curve for which the sun is visible for as long as you please. (In other words, give me a number for which you wish the sun to be visible, say 99.99%, and I can give you a *width* of this tubular region for which it would be realized. Make it 99.9999999% and I'll give you a different number for the width and so on.)

2. A circle with the sun not in the center: Use the same idea as in example 1. Drop a radius from the center of the circular plot to the boundary that passes through the sun. Measure the distance, say r, from the sun to the boundary of the circle. Draw a smaller circle within the larger circular plot with radius r and centered at the sun. Repeat example 1.

3. A square with the sun at the intersection of the diagonals: Repeat example 1 with the a circle of length equal to the distance from the sun to any one of the vertices of the square.

4. A square with the sun not at the intersection of the diagonals. Easier to say that one should fall back to example 3 but rather simply draw a circle centered at the sun with a radius equal to the shortest distance from the sun to the boundary of the square.

Can go on but would stop here. I have to say that I did this as a fun Sunday morning exercise, and tried to reason mathematically which may or may not have been what you were looking for ! (I enjoyed it though !) :-)",null,0,cdlztop,1rctma,askscience,top_week,4
Bondator,"Human walking speed is roughly 6 km/h so if you circle around north or south pole at 23km radius, you'll do a full circle in 24 hours, keeping the sun in front of you 100% of the loop.

As for your triangle, you didn't go deep enough. Don't do an equilateral triangle, do an isosceles triangle. Mathematically expressing, if we mark the short side with x, and the equal sides with y, and choose the orientation in such a way that x is the part where you don't face the sun, then uptime of sun in face is lim(x-&gt;0) 2y/(2y+x) = 1.",null,0,cdm9rcs,1rctma,askscience,top_week,2
musubk,"I once drove an 18 hour loop in Alaska in the summer with the Sun shining on the left side of my face for all but about an hour of it.  I suppose if you go above the Arctic Circle at the right time of year and just walk toward the Sun at a constant speed, you'll end up where you started 24 hours later making a complete loop.",null,0,cdnb6l9,1rctma,askscience,top_week,1
shiningPate,"The original computer architectures used different circuitry for retrieving bytes  assembling into word sizes matching the register size in the CPU. When you were looking at the memory sequentially, independent from the CPU, you needed to know which way the CPU assembled the data into register values to understand why your calculations where coming out wrong",null,0,cdm27j3,1rcvso,askscience,top_week,2
bellcrank,Pretty sure you could get away with a plane parallel approximation in this scenario.,null,0,cdmksb0,1rcvv4,askscience,top_week,2
adamhstevens,"If you're talking about long wave radiation from the Earth, I think this is a fairly standard textbook problem. I'll try and look it up when I get home... if I remember!",null,1,cdmildz,1rcvv4,askscience,top_week,2
Farnswirth,"It's actually very simple.  Pure silver is softer and more malleable than silver alloys.  Just like how pure gold is much more malleable than gold alloys.

http://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness#Hardness_.28Vickers.29",null,0,cdm0s7d,1rcwaq,askscience,top_week,5
PENIS_VAGINA,"90% of the blood flow leaving the glomerulus through the efferent arterioles perfuses the cortex (10% to the medulla) under normal conditions. The main purpose of this is to keep the medulla interstitial fluid hypertonic so that concentrated urine can be produced. I suppose that vasculature changes (i.e. arteriolar constriction) could reroute some of that 90% of blood flow into the medulla to aid in decreasing the hypertonicity of the medulla. 


I'm not 100% sure though. I do know that in normal conditions it is hypertonic to aid in urine concentration (your original question). ",null,0,cdlxyv5,1rcwz4,askscience,top_week,1
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,top_week,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,top_week,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,top_week,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,top_week,1
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,top_week,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,top_week,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,top_week,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,top_week,1
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,top_week,49
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,top_week,2
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,top_week,49
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,top_week,2
miczajkj,"If you talk about two different charged particles, that only interact as a closed system (so no external magnetic or electric fields) the problem is equivalent to the [Hyrdogen atom](http://en.wikipedia.org/wiki/Hydrogen_atom). 

Therefore there are quantized stable orbits and a radiation of photons is not allowed without a time-dependent perturbation.",null,0,cdlym8i,1rcxw0,askscience,top_week,1
Platypuskeeper,"Classically, if you're accelerating a charged particle (and a particle moving in a circular pattern is being accelerated constantly), then you will give off radiation. Obviously if you had one particle orbiting another, you would need some kind of outside energy to sustain this, or the thing would give off all its energy and spiral into the other particle.

[Synchrotron light](http://www.iop.org/publications/iop/2011/page_47511.html), which is up in the X-ray range, is produced by moving electrons around at relativistic velocities.


",null,1,cdlyxjg,1rcxw0,askscience,top_week,2
KerSan,"This is *precisely* the problem that made physicists develop quantum mechanics. The answer to your question is 'no', because otherwise the particles would lose energy and crash into each other. Unless the particles are going to merge or something, this is a violation of the Heisenberg Uncertainty Principle because then you would know too much about both the position and momentum of each particle.",null,0,cdlz5t5,1rcxw0,askscience,top_week,1
skleats,"Cells receive and respond to survival/apoptotic signals independently, so the senescence or death of one cell does not directly impact those around it. This is key since [controlled apoptosis is a normal part of embryonic development](http://people.ucalgary.ca/~browder/apoptosis.html) in multicellular organisms. However, a multicellular organism relies on coordination of activities between its many cells, so having a large proportion of senescent or apoptotic cells would be likely to impact the ability of those cells to contribute to survival of the organism. [This article](http://www.ncbi.nlm.nih.gov/m/pubmed/15265523/) describes an *in vitro* model which mimicked chronological aging and showed reduced coordination between cells as they aged.",null,0,cdm6ky2,1rd0kh,askscience,top_week,2
redmeansTGA,"First off, let’s look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I don’t know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldn’t be different) receive what’s called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldn’t have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally don’t have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands don’t often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, let’s look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (‘perching’) birds didn’t evolve to the early cenozoic either. Temperate deciduous forests also didn’t exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, it’s clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,top_week,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,top_week,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,top_week,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,top_week,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,top_week,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,top_week,2
redmeansTGA,"First off, let’s look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I don’t know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldn’t be different) receive what’s called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldn’t have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally don’t have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands don’t often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, let’s look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (‘perching’) birds didn’t evolve to the early cenozoic either. Temperate deciduous forests also didn’t exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, it’s clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,top_week,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,top_week,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,top_week,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,top_week,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,top_week,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,top_week,2
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,top_week,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,top_week,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,top_week,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,top_week,18
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,top_week,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,top_week,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,top_week,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,top_week,18
rossk10,"In my realm (structural engineering), wind tunnels are used to simulate and predict expected wind loading to structures during specified gusts.  Smaller, to-scale models of buildings are built, hooked up with load sensors, and then placed in a wind tunnel that simulates a design storm and provides load data at critical points.

As for your specific question regarding smoke trails with cars, understand that my fluid knowledge comes from two fluid dynamics classes in undergrad.  I think that these trails are used to demonstrate how particles travel over the surface of a car, giving useful information about the aerodynamics and drag coefficient of the car.",null,1,cdm0p63,1rd4yo,askscience,top_week,4
null,null,null,0,cdmcdq3,1rd4yo,askscience,top_week,3
meerkatmreow,"The data from the smoke is a type of qualitative flow visualization.  Based on the behavior of the smoke, conclusions about laminar v. turbulent flow can be drawn.


The data from wind tunnel tests can come in many forms depending on what you're trying to do.  Full field quantitative measurements (using something like Particle Image Velocimetry or Pressure/Temperature Sensitive Paint) can be useful for exploring the entire flowfield.  Point measurements using pressure transducers can provide the needed data if you're interested in a certain area.  Data such as overall forces and moments on the model may be what you're after.  Qualitative measurements such as flow visualization uses smoke lines or laser induced fluorescence can help identify areas where additional investigation would be beneficial (ie, separate flow).

What you want to measure and how you measure it are very tightly coupled.  When you do a wind tunnel test you can often choose how you measure things by what you're interested in rather than using a one size fits all approach.",null,1,cdm59n4,1rd4yo,askscience,top_week,2
AbsolutePwnage,"The smoke shows were the air flow is laminar and where it starts becoming turbulent and therefore, where parasitic drag starts appearing. It also looks cool, which is why they show it very often in ads and other media.",null,0,cdojkoc,1rd4yo,askscience,top_week,1
burninatingpeasants,"At least from my experience, smoke trails are not used to gather numerical data.  Instead, they may be used to gather more fundamental, conceptual data: ""is the airflow separating from this section of the wing?"", ""Is this section of airflow turbulent or smooth"", etc.

For 3D models (such as a scale model of a full airplane), most of the data is gathered by the actual device used to hold the airplane.  The mounting device is rigged with sensors that can detect forces and moments, so you can get an exact measurement of how much force the wind is putting on the model (and therefore, the mounting device).

Another method sometimes used is a series of ""pressure taps"", which are small holes drilled in the surface of a model to which pressure lines are connected.  By measuring the pressures recorded at each tap, you can use math to determine how much force is exerted on the entire model (or at least that section of the model).  This is used more when trying to get a detailed view of the airflow over a certain component, rather than the overall force acting on the entire component as a whole.",null,0,cdpw20l,1rd4yo,askscience,top_week,1
Lost_Wandering,With today's digital imaging capabilities it has gone beyond qualitative to actual quantitative analysis possible from smoke trails in wind tunnel. Digital particle image velocimetry allows for tracking smoke particles in space and can be used to do such things as validate or adjust computational flow dynamics models.,null,0,cdpztz6,1rd4yo,askscience,top_week,1
user2097,"3rd year aerospace engineering student here. Wind tunnels are used largely for models to mimic equivalent flow conditions, and the data from the testing includes qualitative and quantitative data.

Sometimes your test is performed to verify dynamic stability, examine stall characteristics of aeroplanes), examine flow condition (separation, turbulence, mixing...), etc. Other tests will produce data based off sensors attached to the model or tunnel such as force on a wing, dynamic response to an input, measure location of separation with hot wires, etc. ",null,1,cdm4stc,1rd4yo,askscience,top_week,1
null,null,null,31,cdm2wxo,1rd53a,askscience,top_week,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,top_week,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,top_week,19
null,null,null,11,cdm3al3,1rd53a,askscience,top_week,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,top_week,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,top_week,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,top_week,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,top_week,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,top_week,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,top_week,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,top_week,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,top_week,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,top_week,1
null,null,null,22,cdm2cds,1rd53a,askscience,top_week,16
null,null,null,31,cdm2wxo,1rd53a,askscience,top_week,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,top_week,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,top_week,19
null,null,null,11,cdm3al3,1rd53a,askscience,top_week,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,top_week,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,top_week,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,top_week,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,top_week,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,top_week,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,top_week,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,top_week,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,top_week,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,top_week,1
null,null,null,22,cdm2cds,1rd53a,askscience,top_week,16
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil™ alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barré Syndrome (paralysis lasting for years, or permanently — sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven “there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.”

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,top_week,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,top_week,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,top_week,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,top_week,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,top_week,3
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil™ alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barré Syndrome (paralysis lasting for years, or permanently — sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven “there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.”

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,top_week,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,top_week,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,top_week,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,top_week,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,top_week,3
xtxylophone,"Aside from the comparatively 'busy' time around the Earth's formation, nothing has changed. They are just infrequent and the evidence they leave lasts a long time.

Check out: http://en.wikipedia.org/wiki/Impact_event

Impacts that can change geography are about on the scale of the length of Human civilisation. Don't take the data for one being 'due' though. ",null,1,cdm0yuw,1rd5c8,askscience,top_week,8
null,null,null,32,cdm23mb,1rd5mf,askscience,top_week,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,22,cdmhzt8,1rd5mf,askscience,top_week,91
null,null,null,9,cdm6gn5,1rd5mf,askscience,top_week,14
null,null,null,30,cdmgdmn,1rd5mf,askscience,top_week,20
null,null,null,32,cdm23mb,1rd5mf,askscience,top_week,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,22,cdmhzt8,1rd5mf,askscience,top_week,91
null,null,null,9,cdm6gn5,1rd5mf,askscience,top_week,14
null,null,null,30,cdmgdmn,1rd5mf,askscience,top_week,20
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,top_week,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,top_week,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,top_week,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,top_week,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,top_week,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,top_week,1
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,top_week,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,top_week,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,top_week,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,top_week,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,top_week,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,top_week,1
iorgfeflkd,"tl;dr: If the laws of physics don't depend on location, momentum is conserved.

Noether's theorem says that for every symmetry in a process, there is a conserved quantity. For things that are translationally symmetric, that conserved quantity is momentum. This means that if you consider a collision on a highway, and then the same collision a couple of miles down the highway (translation), if they behave the same (where on the highway it is didn't matter), then momentum is conserved.

That's fairly complicated, another but less rigorous way of looking at it is that momentum changes when a force is applied, and if no force is applied then the change in momentum is zero, so in the absence of external forces the total change in momentum is zero.",null,1,cdm156w,1rd5ys,askscience,top_week,9
rupert1920,"Plastics are long polymers, and can undergo [polymer crystallization](http://en.wikipedia.org/wiki/Polymer_crystallization) when stressed. It is the formation of these ordered structures that causes scattering in the material - which is why it looks white.

In some plastics this process can be reversed by heating the plastic (for example, boiling it in water for a few minutes).",null,1,cdm8ncc,1rd67w,askscience,top_week,4
bohr_exciton,"The most probable explanation is that by bending the material you are creating defects, i.e. inhomogeneities in structure, density, etc. These defect sites can then act as scattering centers, which in turn reduces the transparency. This is a similar effect to scratching the surface of ice, for example.",null,1,cdm3rsa,1rd67w,askscience,top_week,4
ultimatety,"The answer to this is actually more complicated than you would think.  It all boils down to the fact that the surface layer of the ice underneath the object is partially melted.  However, the reason for how this top layer melts is somewhat of a scientific controversy.  People used to believe that the pressure exerted causes the ice to melt, however, this appears to be false.  
The two current theories are that: 
1) The friction of the moving object causes the top layer of the ice to melt
or 2) The top layer of water molecules are unable to bind correctly to the layers underneath and thus stay in a quasi water-like state.

TL;DR There is a little bit of liquid water on top of that ice, and liquid on top of something smooth makes it slippery.",null,3,cdm1crb,1rd6cm,askscience,top_week,19
ace425,"Adding on to this, why doesn't waters ability to form hydrogen bonds affect the slipperiness of ice? It seems like since water likes to form hydrogen bonds that ice would not be slippery but instead have a lot of traction, but this obviously isn't the case. Can someone expand on this please?",null,1,cdmjimu,1rd6cm,askscience,top_week,1
sharp12180,"When you step on ice, you apply pressure to the ice directly below you. This pressure decreases the freezing point of ice and so there is a thin layer of liquid water formed between your feet and the ice. Its this difference that causes ice to so slippery.
http://www.youtube.com/watch?v=Stx6kLd9dYI",null,20,cdm153f,1rd6cm,askscience,top_week,6
ultimatety,"The answer to this is actually more complicated than you would think.  It all boils down to the fact that the surface layer of the ice underneath the object is partially melted.  However, the reason for how this top layer melts is somewhat of a scientific controversy.  People used to believe that the pressure exerted causes the ice to melt, however, this appears to be false.  
The two current theories are that: 
1) The friction of the moving object causes the top layer of the ice to melt
or 2) The top layer of water molecules are unable to bind correctly to the layers underneath and thus stay in a quasi water-like state.

TL;DR There is a little bit of liquid water on top of that ice, and liquid on top of something smooth makes it slippery.",null,3,cdm1crb,1rd6cm,askscience,top_week,19
ace425,"Adding on to this, why doesn't waters ability to form hydrogen bonds affect the slipperiness of ice? It seems like since water likes to form hydrogen bonds that ice would not be slippery but instead have a lot of traction, but this obviously isn't the case. Can someone expand on this please?",null,1,cdmjimu,1rd6cm,askscience,top_week,1
sharp12180,"When you step on ice, you apply pressure to the ice directly below you. This pressure decreases the freezing point of ice and so there is a thin layer of liquid water formed between your feet and the ice. Its this difference that causes ice to so slippery.
http://www.youtube.com/watch?v=Stx6kLd9dYI",null,20,cdm153f,1rd6cm,askscience,top_week,6
incognegro76,"You can graphically illustrate a line with this equation but it will form an asymptote very rapidly to zero.

y = 2^-x",null,1,cdm6f0c,1rd6ok,askscience,top_week,3
rlee89,"y=-ln(1-t)/ln(2) seem a good place to look.

For a runner running at velocity 1, y is the number of terms you have added to get the runner's distance at time t.  It is rather obvious that no matter how many terms you add, you will never reach the runners distance for any time after t=1.

This is derived from the closed form of the partial sum 1/2 +1/4 + 1/8 ... 1/2^n = 1-2^(-n).",null,0,cdmam7a,1rd6ok,askscience,top_week,2
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,top_week,42
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,8,cdmh86v,1rd6th,askscience,top_week,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,top_week,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,top_week,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,top_week,1
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,top_week,42
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,8,cdmh86v,1rd6th,askscience,top_week,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,top_week,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,top_week,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,top_week,1
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,top_week,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,top_week,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,top_week,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,top_week,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,top_week,6
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,top_week,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,top_week,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,top_week,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,top_week,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,top_week,6
patchgrabber,"Well, kelp are basically algae, so they are quite different from land plants in pretty much every way except photosynthesis. Although their holdfasts resemble, and may be a primitive form of, plant roots, kelp are fundamentally different. 

In most land plants, while very limited photosynthesis may occur in the stalk of the plant, most of its photosynthetic activity is in the leaves. Kelp, in contrast, photosynthesize in every part of the organism (although different parts have different levels of photosynthetic ability depending on age), allowing for more and making light less of a limitation than it is in land plants.

The environment the kelp lives in is also a big factor. Since it is under water, light is attenuated differently than above water. Due to the large amount of particulates, blue light is attenuated rapidly in coastal waters, and blue light is much more valuable than red light that can penetrate deeper at a higher intensity.

While there are products out there that purport to use kelp in them to make plants grow faster, I'm thinking this is only because of the nutrients, not any special property that is linked to kelp growth. I cannot think of any way at present to genetically transfer this quality to land plants; their limitations are different, their environments are different, and they are just fundamentally different organisms. Kelp would be much better used as fertilizer, as you suggest, than as a source of genetic information, although in the future that may change.",null,0,cdmahyq,1rd7fs,askscience,top_week,2
MarineLife42,"As /u/patchgrabber said, Algae are very different from plants.  
Here, the main difference is that (higher) plants grow, i.e. create new tissue only at specific regions on their body. Usually this is at the tip of the plant or leaf, the apex. In grasses (grains) it happens at the nodes too.  
Kelp, on the other hand, creates new tissue along the entire length of its thallus (the big leaf) which is why in grows so fast.  
Another big difference is that the thallus doesn't have much, if any speciation; it is composed of more or less the same kind of cell. Higher plants, in contrast, have an internal structure of xylem, phloem, bark etc. that requires many different specialized cell types.  
Both these differences work together to prevent us from simply transplanting this ability into our crop plants. 
BTW - some bamboo species can also grow very fast, up to 10cm a day or so but there is trickery involved. In fact the plant tissue has been created at the usual speed beforehand, but compressed. During the elongation phase, the plant sucks up much water and fills the cells so it telescopes upwards. ",null,0,cdn4dss,1rd7fs,askscience,top_week,1
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,top_week,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,top_week,16
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,top_week,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,top_week,16
brawnkowsky,"different ethnicities will have different genes that express proteins differently.  For example, [degrees of lactose intolerance vary between regions, from 5% in north europe to 90% in some african and asian countries](http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0100-879X2007001100004&amp;lng=en&amp;nrm=iso&amp;tlng=en).  this is simply because of altered protein expression (lactase in this example), which is a factor in all protein expression in our body.  also, people will have varying levels of gut microorganisms depending on their environment, what they eat, and their own immune strength; this natural flora is important in digestion.

",null,0,cdnfwoy,1rd803,askscience,top_week,2
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,top_week,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,top_week,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,top_week,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,top_week,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,top_week,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,top_week,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,top_week,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,top_week,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,top_week,13
null,null,null,9,cdm64ib,1rd8ip,askscience,top_week,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,top_week,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,top_week,1
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,top_week,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,top_week,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,top_week,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,top_week,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,top_week,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,top_week,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,top_week,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,top_week,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,top_week,13
null,null,null,9,cdm64ib,1rd8ip,askscience,top_week,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,top_week,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,top_week,1
iorgfeflkd,"Protons and neutrons are held together by the strong nuclear force (or a residual form of it, sort of the equivalent of van der Waals forces for nucleons), which in stable nuclei is much stronger than the electrostatic repulsion between protons. If a nucleus has too few neutrons then the repulsion will break it up.",null,1,cdm1vxx,1rd8yh,askscience,top_week,5
iorgfeflkd,"Beta decay involves a neutron turning into a proton and emitting an electron (beta particle) and an antineutrino. Static electricity involves movement of pre-existing electrons. Nuclear reactions generally involve much higher energies than electronic or atomic. For example, beta particles from potassium decay in bananas have as much energy as if they went through a 1.5 million volt potential, and static discharge is typically in the thousands. However, static discharged generally involves a lot more electrons compared to most radioactive sources.",null,1,cdm28la,1rd953,askscience,top_week,7
zalaesseo,"When Benjamin Franklin said that Charge can only be collected and lost, he really meant it. When you discharge electricity, electrons just moves to the metal object.

Until beta decay. Beta decay literally creates a new proton electron pair and an antineutrino^irrelevant. You're not collecting charges, you're MAKING charges appear from nothing.   ",null,0,cdm2ovh,1rd953,askscience,top_week,3
cosmicosmo4,"When you get a static shock, you're typically experiencing millions-billions of electrons being transferred over thousands or tens of thousands of volts, and they're only doing that because the recipient object (be it you or the metal railing) has a positive charge, meaning there are places for those electrons to settle once they get there.

When a beta particle is emitted, it comes with an energy in the range of millions of volts, and there's no predesignated spot for it to settle, meaning it will fly straight through things until it happens to find a spot to settle, often by displacing some other electron. This is what makes it ionizing radiation.",null,1,cdm357l,1rd953,askscience,top_week,4
owaisofspades,"Your thyroid hormone is responsible for regulating the metabolic rate of most of your body. When you have thyroid insufficiency, your metabolic rate drops and your body no longer functions at full effectiveness. The concentration problems are likely a secondary effect of the lethargy and weakness that are caused by hypothyroidism",null,0,cdmb0lv,1rdckt,askscience,top_week,2
s3c7i0n,"As a basic reply, dogs, like cats, have a reflective coating at the back of their eye, which helps them see in low light situations. The color of the coating is based on the color of the eye, which has some evolutionary benefits having to do with common colors in various environments, but the gist of It is that the colors are caused by the iris colors. 

(edit) the blue eye is red due to a lack of pigment in the reflective layer, so you're actually seeing the reflection of the blood vessels in her retina. ",null,1,cdm3dji,1rdd4v,askscience,top_week,3
Ejb90,"This revolves around the maximum power transfer theorem. There are two ways to look at it.
Firstly from a circuit-theory point of view, when power energy is transferred from one component to another, the maximum is transferred if they are the same resistance. Impedance is the more generalised, complex form of resistance.  This means that if they are matched in resistance (impedance) then the most power is transferred, which is most efficient.
The second way to look at isn't is from a waves point of view. At the frequencies you get in a transfer cable the currents can be modelled as electromagnetic pulses. When they reach a boundary some are reflected and some are transmitted, just like light is when it passes between air and water. When the two mediums either side of the boundary have the same ""resistance"" to the wave, more of the wave propagates through, as it's almost as if there is no boundary, so the maximum energy is transferred, as expected.",null,1,cdm4hjj,1rdd6o,askscience,top_week,6
SwedishBoatlover,"You should *really* watch this [video](http://youtu.be/DovunOxlY1k) from Bell labs, where the host use a wave machine to visually show how waves work. You can actually get an intuitive feel for what the impedence matching does, it's very interesting!",null,0,cdm9s27,1rdd6o,askscience,top_week,3
rat_poison,"This is the distilled wisdom of my Microwave Networks experience regarding impedance.

The most important defining feature of the transmission line is its characteristic impedance. It is affected by the shape of the transmission line and materials that make it up. Generally, for a TL extending to the z direction, we can divide the whole length in small parts of length Δz. Τhose parts can be arbitrarily small: so much so that we can ignore any radiating properties within that Δz. We can therefore make a lumped-circuit equivalent of that Δz length of the tramsission line.
Movement along the length of the transmission line will mean some ohmic resistance (R) and some inductance (L). The neighboring of metal surfaces will cause capacitance (C) and the material between them will cause dielectric losses based on a conductance (G).

As the electromagnetic wave travels through the transmission line along direction of propagation J, we can generally define functions I(z) = I+(z) + I-(z) and V(z) = V+(z) + V-(z)

So current and voltage are made up of two constituents: the first (+) representing movement along the direction of propagation and the second (-) representing the part of the current and voltage that are caused by reflection and therefore are moving opposite the direction of propagation.

Characteristic Impedance is the ratio Z_0(z) = V+(z) / I+(z)

for the length of Δz I have described earlier, it is calculated as Z_0 = sqrt((R+jωL)/G+jωC)) (j = sqrt(-1))

In most cases, trasmission lines are uniform in the z direction, or they are made up of a cascade of uniform parts. Either way, for every uniform TL, the characteristic impedance is the same no matter which Δz I choose within it (as long as it's small), so that's why it's such a defining property of a TL.

We can then define another quantity, Γ(z)=V-(z)/V+(z), called the reflection coefficient. This tells us what is the ratio of reflected and forward waves. Its amplitude is 0 if there are no reflections, 1 if the reflections and the forward waves have the same energy therefore leading to standing waves not able to propagate energy, or an-inbetween state for the other values in between.

If z=l, then we have calculated that Γ_Load = (Ζ_L-Z_0)/(Ζ_L+Z_0). For a lossless transmission line, this will have constant amplitude throughout its length.

Now we want to minimize energy lost in reflections. So Γ should be 0.

If you look closely at my last equation, you'll see that this can only be true if Z_L = Z_0.

Regarding the part about the return line.

When dealing with high frequency circuits, a return line is not necessary. BUT NOT because of impedance matching. If the outer shell or the inner wire of a coaxial cable didn't exist, it wouldn't posses the geometrical properties that induce the field to operate in the desired way. There wouldn't be two points with different potentials along the direction of propagation around which the EM energy could oscillate back and forth. In fact the concepts I have just described break down. BUT there are transmission lines that don't have a return wire: these are waveguides. The wave DOES oscillate back and forth, but the points are not as strictly defined as in two-wire TL's or coaxial cables. Instead, we have modes: depending on the ratio of the wavelength and the dimensions of the waveguide, there are (possibly several) nulls and peaks at the transversal plane, that are defined by how many half-waves fit into that dimension. These nulls and peaks are now the places around which the energy fluctuates in order to propagate forward. Waveguides are the reason you should be careful when using current and voltage concepts on microwave circuits. Therefore you should just stop thinking about TLs in terms of a phase line and a return line, but a single structure which guides the waves along a direction and (maybe) causes reflections along the opposite direction.
",null,0,cdmbm4q,1rdd6o,askscience,top_week,2
selfification,"http://www.youtube.com/watch?v=DovunOxlY1k

This is a classic that explains all phenomena.  Standing waves, interference, impedance matching, refraction, reflection...  everything.  All in one video.",null,0,cdmcdgz,1rdd6o,askscience,top_week,2
ece_option_chair,"By the way, an interesting historical fact is that the first use of transmission lines (and impedance matching) was with telegraph cables where intersymbol interference occurred and the solution was to INCREASE inductive loading (normally one thinks of reducing inductive loading if the bandwidth is not high enough).  [LINK](https://en.wikipedia.org/wiki/Heaviside_condition).  Heaviside doesn't get enough credit for all the things he invented/re-invented/simplified.",null,0,cdpbbbq,1rdd6o,askscience,top_week,1
fastparticles,The event would melt most of Earth and put the upper mantle into orbit around Earth. At this point the moon is thought to come from Earth because they are so isotopically similar. The compositions of the moon and Earth do differ especially in terms of volatile elements (the moon for example is relatively depleted in potassium). ,null,0,cdm6y13,1rdhkw,askscience,top_week,2
ProfEntropy,"Postmortem fluid and tissue toxicology is able to quantify both the drugs and alcohol present at the time the samples were taken.

Connecting that back to the amounts present at the time of death can sometimes be difficult. For example, many drugs are known to partition into different parts of the body after death. Knowledge of this, and sampling tissues and fluids from the proper place will help get more accurate measurements.

Many other factors must be considered when looking at ethanol concentration. See [this article](http://www.sciencedirect.com/science/article/pii/S0379073806002891) for a good review of postmortem alcohol concentrations and how they relate to BAC at time of death.",null,2,cdm82ym,1rdosg,askscience,top_week,9
Smoothened,"The machinery behind X-inactivation specifically targets the X chromosome as opposed to any chromosome. For example, the gene XIST is located on the X chromosome and is required for its inactivation. When this gene is expressed, its transcript, a long noncoding RNA (Xist) coats the respective chromosome, becoming involved in its silencing. A chromosome lacking XIST would not undergo inactivation. If you insert the XIST gene in an autosomal chromosome, that chromosome can then be inactivated. 

A more interesting question is how is inactivation targeted to one of the chromosomes in each cell. That question is not entirely answered, but it is believed that an autosomal gene encodes a blocking factor that prevents one X chromosomes from being inactivated. Interestingly, even when there's more than 2 X-chromosomes present, only one remains active in each cell. ",null,0,cdmbqmg,1rdsv9,askscience,top_week,5
Platypuskeeper,"Sea salt is from evaporating seawater, table salt either comes from the sea or from salt mines. 

When you say ""table salt"" you're referring to one single compound: Sodium chloride. The vast bulk of the sea salt, and virtually all of what's 'table salt' is sodium chloride. Sea salt has some other salts in it, how much and what depends on where it's from. Table salt is often [iodized](http://en.wikipedia.org/wiki/Iodised_salt), meaning they've added some iodine salts as a dietary supplement. (Lack of iodine causes developmental disorders and thyroid problems) Depending on the salt it might have small amounts of stuff to avoid it caking together too, which aren't usually added to the stuff marketed as 'sea salt'. (I don't believe there's any _requirements_ on this though)

So sea salt has some other minerals in it, but it's such a small part and you use so little salt, that it probably doesn't make a significant impact on your overall mineral intake. The iodization of salt has had a measurable impact on iodine deficiency-related stuff since it was introduced in the 1920s. For the individual any health effects would depend on whether you get enough iodine from other sources. 

The biggest differences are really taste and texture more than anything, though.
",null,2,cdm93o8,1rdu42,askscience,top_week,6
225274,"Sea salt is the salt produced by evaporating sea water. Table salt is the same thing, just crushed into a fine powder, with fewer impurities of other salts like KCl, and is often iodized, i.e. has added iodine salts. 

Table salt is healthier as iodine is not so commonly available in our diet, but is an essential mineral. 

",null,2,cdm8et4,1rdu42,askscience,top_week,4
chuck10470,"The difference between table and sea salt is the iodine. Fresh from the factory it contains 50 ppm iodine. That's all. 50 ppm. As it ages, the iodine evaporates out, losing half every 40 days.

All salt comes from the sea. Or a lake. Mined salt has precipitated out over thousands of years and built up thick beds that became buried through mountain building. Ironically, most of these mines are today quite some distance from the ocean. The Swiss city of Salzburg has several salt mines,  though it is hundreds of miles from the ocean today. 

And some of this mined salt is quite old. But whether it precipitated out at the bottom of the Tethys Sea 65 million years ago or last week in Sardinia, it's still 99.9% NaCl and 0.1% other minerals. Most of the table salt, industrial salt, road salt, animal feed salt, etc, is mined. Sea salt is precipitated out in huge evaporation ponds. It should be noted that sea salt can be iodized and become table salt, and much of it is. ""Sea salt"" is a marketing name given to un-iodized salt produced by evaporation. It supposedly has better taste, but since it's nearly impossible to determine which minerals give it a specific favor profile, maybe it does, maybe it doesn't. It depends on where it's from. The expensive $10/lb culinary salt is usually sea salt, but with additives like smoke or truffles. The various types of salt available at the grocery store differ mostly in the iodine content and the shape of the crystals. That's it. You can use regular iodized table salt for nearly every application you have,  except for canning. The iodine turns some stuff brown.",null,0,cdmf52z,1rdu42,askscience,top_week,2
Truck43,"The lighter works because the butane is a liquid under pressure, opening the valve lets it spray out and be ignited by the flint. When it's very cold, it contracts, reducing the pressure in the fuel vessel, and it's less volatile, this reduces the amount of fuel that is expelled.  ",null,3,cdm9hep,1rduf4,askscience,top_week,8
adlermann,butane's(what bic lighters use for fuel) vapor pressure drops to near zero at atmospheric pressure about 40F not enough gas is released to fuel a flame.  That is why natural gas and propane are used for heating despite butane's higher energy potential,null,2,cdm9ieg,1rduf4,askscience,top_week,4
Platypuskeeper,"&gt; I'm assuming the lighter fluid has less energy therefore it's lazy.

That's one way of putting. A more formal but roughly equivalent way would be to say that the pressure over the liquid butane in it, is lower when at a lower temperature. The equilibrium is shifted towards more liquid and lower pressure at lower temperatures, higher pressure and less liquid at higher temperatures. 
",null,2,cdmae2f,1rduf4,askscience,top_week,4
cass314,"In both beer and soda, the bubbles are caused by carbon dioxide coming out of solution.  The big difference is what's there to ""catch"" the bubbles and hold them.  In soda, there's not much at all.  In beer, there are proteins.

Soda is mostly water, sugar, salt, and acid.  There's not a lot to give structure, so the bubbles die out quickly, and after a few minutes you can hardly tell there was ever any foam.  Beer, however, has proteins leftover from both the mash (wheat or barley, usually) and the yeasts that did the fermenting, and it's the proteins that give beer such an interesting head. Proteins, especially hydrophobic proteins (they ""like"" oil better than water) and denatured proteins with their inner hydrophobic parts exposed, tend to clump together into structures (many to avoid interacting with water).  These structures can trap air bubbles.  

You can think of it like a less extreme example of whisking sugar water vs. whisking sugar and egg whites.  If you whisk or shake water, you'll get bubbles, but they'll pop very quickly after you stop.  If you whisk egg whites long enough, you'll get meringue.  ",null,0,cdmczfs,1rdy9r,askscience,top_week,3
fishify,"The energy of the initial and final states in beta decay, as in other processes, have the same energy. The W boson that appeas as an intermediate particle in the standard desecription of the process is a so-called virtual particle. In particle physics, our calculational scheme known as perturbation theory tells us that we can calculate what happens using intermediate states known as virtual particles which have the energy, momentum, and other conserved quantities you'd expect; but this also means they have the 'wrong' mass.  We say they are *off mass-shell*.

These virtual particle that appear in calculations are never actually observed. Any W boson you actually detect will have the expected mass of 80.4 GeV/c^(2), or just under 86 proton masses.",null,1,cdmfttr,1re0d7,askscience,top_week,3
fishify,"Depending on your background, this article might be helpful to you:

""The Pumping of a Swing from the Standing Position."" William B. Case, American Journal of Physics, 64, 215 (1996).


",null,1,cdmg0m4,1re182,askscience,top_week,3
Shitler,"As I understand it, motion happens because the swinger shifts their center of gravity, causing gravity to have to recenter the pendulum. However, as is in the nature of pendulums, gravity overdoes it and the swinger ends up on the other side of equilibrium, at which point they shift their center of gravity again. And so on.

Energy is introduced into the pendulum when the swinger shifts their center of gravity by extending or contracting their legs.",null,1,cdmgk9l,1re182,askscience,top_week,3
jofwu,"I'm just going to describe the process...

When swinging forward you lean back, stick your legs out, and pull on the chains. *By leaning your torso back and kicking your legs out you apply torque to your body.* This torque is balanced by pulling on the chains. Imagine trying to perform this action without holding on to or pushing off of something- you can't. Note that the chains bend where you hold them. The line of action of the tension in the chains is *behind* your center of mass. This is where the balance in torque comes from: force (tension in chains) x distance (between force's line of action and your center of mass). On the backswing, everything is the opposite. You pull your torso forward and bend your knees back in, and to balance this out you need a torque in the opposite direction. So you *push* forward on the chains, and the line of action of the chains is *in front* of your center of mass.

*Making these transitions leading up to the peak of your swing is the key.* The movements don't do anything if they aren't timed right. By performing the forward swinging motions, you add some gravitational potential energy at the top of the front of your swing. The back swinging motions add energy at the back end of the swing. *The energy gained is thanks to that little distance you create between you and the chains' line of action- putting you a little bit higher from the ground than if you had just swung freely like a pendulum.* Of course this gravitational potential energy results in more speed/momentum at the bottom. *And I think it's worth mentioning that you don't conjure this extra energy from nowhere. It comes from you body.* The gravitational potential energy you add wouldn't be possible without applying a torque to your body. 

In the end, it's not that much different from swinging on parallel bars. Rather than balancing your torque by pushing/pulling on a chain, you apply a counter torque directly to the bar you hold (with a firm grip).",null,0,cdmmc2l,1re182,askscience,top_week,2
GlowInTheDarkDonkey,"My understanding, as a (uh oh) layman, is that a person on a swing is basically taking advantage of angular momentum in the same way a figure skater tightening their limbs in a spin makes them spin faster.

A shortening of the total length of the swinging body on the upward swing means gravity is being applied to a total body that has less distance to travel (is a shorter swing-arm), and then on the downward swing the thrusting of legs outwards allows gravity to work on a longer swinging body... which again is then shortened on the upward movement.

Some of the angular momentum of the legs themselves also adds to the total forces being shifted around.

When someone is standing on a swing seat you'll notice they put all of their mass to the seat on the down-swing, and then they stand on the upswing.  This, similarly, means gravity is pulling a longer swing-arm (in terms of average mass distribution towards the outermost edge of the arm) on the downward stroke compared to the upward stroke.

I'm curious if someone in a white-coat finds this answer agreeable or not.",null,6,cdmdqxr,1re182,askscience,top_week,5
eliareyouserious,"A presynaptic (fibre) volley can be observed in extracellular field potential recordings. It is caused by activation of (several) presynaptic fibres (usually using a stimulation electrode), which in turn fire and activate their postsynaptic partner. A brief negative potential preceding EPSPs is indicative of presynaptic action potential(s) and is termed the ""presynaptic volley"". Fig.2C on page 92 in this book indicates the volley in a recording: http://books.google.ch/books?id=y_ucmaDffXsC&amp;dq=presynaptic+volley&amp;hl=de&amp;source=gbs_navlinks_s (The link to the book chapter also serves as reference here). ",null,0,cdnryhk,1re1qb,askscience,top_week,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,top_week,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,top_week,45
null,null,null,13,cdmjqhe,1re305,askscience,top_week,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,top_week,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,top_week,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,top_week,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,top_week,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,top_week,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,top_week,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,top_week,45
null,null,null,13,cdmjqhe,1re305,askscience,top_week,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,top_week,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,top_week,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,top_week,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,top_week,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,top_week,1
Platypuskeeper,"The electromagnetic field. It's everywhere.

Somebody is inevitably going to chime in here with virtual particles and whatnot, which are quantum-level descriptions of _how the field works_. But at the end of the day, the 'medium' is the same: Space itself.
",null,1,cdmdzfk,1re5f5,askscience,top_week,11
fishify,"Not every wave needs a medium other than the vacuum in which to travel. Nineteenth century physicists did not recognize this, and thus postulated that the universe was filled with a substance they called *the ether*, which would serve as the medium for light waves.

Einstein in 1905 showed there was no need for an ether. As we understand it today, light travels through space just as an electron does. One way to picture this is to remember that light is made of photons (particle of light), which readily travel through space and which form electromagnetic fields and waves.",null,1,cdmfxew,1re5f5,askscience,top_week,10
killer_alien,"Light is an electromagnetic wave, and therefore does not require a medim to propagate through. On the other hand, waves that need a medium are mechanical waves. Theses include longitidinal, transverse and torsional waves. e.g. sound waves are longitudinal waves",null,0,cdmi1mi,1re5f5,askscience,top_week,2
animationb,"When a field gets enough energy, it ""manifests"" as some fundamental particle. For the electromagnetic wave, energy creates a photon. In sort of the same way matter helps ""create"" (or comes with) a gluon, the fundamental particle for gravity.",null,2,cdmng0x,1re5f5,askscience,top_week,1
KarlOskar12,"That depends what you mean...The major regulators of the cell cycle are [p53 and p27](http://puu.sh/5sB2h.jpg). They both halt the cell cycle, p27 specifically does it by binding to and blocking the action of cyclin and CDK preventing the cell from entering the S phase of the cell cycle (DNA replication phase). Once the cell cycle is halted, the cell is either repaired (let's say for DNA damage). If repair is not possible or too costly, the cell is told to undergo apoptosis (kill itself). This is done by activating [Caspase 3](http://en.wikipedia.org/wiki/Caspase_3) which systematically breaks down the cell by expelling all the water, chopping the DNA up in an orderly manner, degrading the nuclear membrane, degrading the golgi apparatus, blebbing the cytoplasmic membrane, etc.",null,0,cdmen5i,1re5ig,askscience,top_week,2
StringOfLights,"It is not so much that terrestrial mammals were big back then, it's that they're small now. Mammals [increased in size following the Cretaceous-Paleogene extinction and maintained that large body size](http://www.sciencemag.org/content/330/6008/1216.short) for nearly 30 million years years.  Then there was an [extinction at the end of the Pleistocene](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.1991.tb01149.x/abstract). Most vertebrate taxa made it through this extinction, but a lot of large-bodied animals, and especially large-bodied mammals, were hit particularly hard. Some 150 genera of megafauna (defined as animals &gt;44 kg) existed 50,000 years ago; [97 of those were extinct by 10,000 years ago](http://www.sciencemag.org/content/306/5693/70.full):

Given how geologically recent these extinctions are, it's extremely unlikely that anything would have been able to fill the gaps left by the loss of megafaunal mammals, as there appears to be a [maximum rate](http://pnas.org/content/early/2012/01/26/1120774109.abstract) that mammals can increase in size. In that sense it's completely expected that a recent extinction event would leave a gap in body size. 

Also, in all of this discussion it's worth bearing in mind that we're generally talking about terrestrial mammals. There are plenty of large marine mammals still around (for the time being), including the blue whale!

**Edit:** Forgot something! In terms of dealing with cold weather, having a larger body size actually slows heat loss because it lowers the surface area to volume ratio. So while larger mammals had to eat more overall, they [spend less energy per unit of body mass](http://www.planta.cn/forum/files_planta/511_131.pdf) producing heat. This was the original logic behind [Bergmann's Rule](http://en.wikipedia.org/wiki/Bergmann%27s_rule).

",null,3,cdmdkaq,1re5lq,askscience,top_week,9
masiakasaurus,"StringOfLights hit the most important points but I'd like to stress that every animal is a different case and the more you look into a particular species you'll see different or additional reasons for the size they had. While an old source, 1968 Björn Kurtén's *Pleistocene Mammals of Europe* (and I guess its companion New World book, *Pleistocene Mammals of North America* by the same author, though I have not read it) makes a good recap on ice age mammals. Let's see some of the animals you cited.

First of all, **mammoths**. There were different species of mammoths, and they have been cursed with being described as gigantic in popular literature, but they were really not that big. The biggest species was the Eurasian steppe mammoth *Mammuthus trogontherii* (followed by *Mammuthus columbi*, its North American descendant) whose males topped at 4'5 meters (14'5+ feet) tall. The biggest African elephants are about 4 meters tall or a little more. *M. trogontherii* and *M. columbi* didn't live in the tundra, however, but in temperate grasslands to the south. What's more, *M. trogontherii* wasn't an 'ice' animal proper, as it was common during the Cromerian or Gunz-Mindel interglatial 780,000-450,000 years ago, a *warm* period between glatiations. 

*M. trogontherii*'s (Eurasian) descendant, the more famous woolly mammoth *M. primigenius* was adapted to the glatial steppe-tundra and thrived during the cold periods. It was a *smaller* animal than its ancestor, no bigger than modern elephants. Kurtén gives a height at the shoulder for the Late Pleistocene mammoths, who lived through one of the coldest periods of the Pleistocene, as 10 feet or less. There is no doubt, however, that the woolly mammoth *was* the biggest animal of the tundra, but it wasn't the giant mountain it is made out to be in popular imagination, and it is plausible that its *decrease* in size from its predecessor is a result of living in a poorer environment with less food.

**Dire wolves**, despite popular image as well, **were no bigger than grey wolves either**. This confusion stems from the fact that dire wolf jaws and teeth were larger and more adapted to bone-crushing than grey wolves, making them ecologically analogous to Old World hyenas. Indeed, while hyenas inhabited Eurasia and Africa during the Pleistocene (and coexisted with wolves), they never reached America, so the direwolves occupied their niche here, and in fact coexisted with grey wolves who looked after different prey than them. The grey wolf of Late Pleistocene Europe is slightly bigger than modern European wolves, but then again, modern wolves in North America are *also* on average larger than Europe's.

**Giant Sloths** are rather weird as they have no living equivalents and I'm not as familiar with them, but once again the biggest ones ever don't come from colder areas. Wikipedia (yes, I know) has *Eremotherium* (Georgia, Texas, Mexico) and *Megatherium* (subtropical South America) as 6 meters long. The ones that lived in colder areas were smaller: *Mylodon* (Patagonia) and *Megalonyx* (Central America to Alaska and Yukon) peak both at 3 meters, half the lenght of the others.

**Cave Bears** were not particularly related to brown bears (if anything they were closer to black bears) and coexisted briefly with brown bears who originated in Asia (cave bears are exclusive to Europe). Cave bears were mainly vegetarians and didn't live in tundra either, but in temperate forests, and became extinct as the continent became colder about 30,000 years ago (almost like the neanderthals). I don't have size data right now but from what I recall they aren't that big compared to brown or polar bears (someone correct me if I'm wrong please) and once again this seems to be a confusion born of the fact that they are more robust and massive looking. Brown bears in cold periods of the Pleistocene are on average bigger than brown bears in warm periods including modern brown bears, however.

**Cave lions** were, indeed, on average bigger than modern lions, but still the same species. While cave lions lived in the Pleistocene in Eurasia and Alaska modern looking lions (who are not descendants of the former according to DNA) were in Africa and probably the Middle East. The *American* cave lion was bigger than both but its taxonomy is in discussion and could have been a whole different beast altogether (hehe), more related to the jaguar. Like mammoths (and unlike wolves and bears), however, Eurasian cave lions are biggest in the Cromerian and become their smallest in colder periods, especially near the end of the Pleistocene. Some European lions get so small that it has even been suggested that the cave lion became extinct before it is commonly assumed to, and was replaced by modern lions from the Middle East belonging to or closely related to the modern Indian subspecies (one of the smallest living lions by the way). On the issue of cats, leopards (which lived in Europe through the Pleistocene) and cheetahs are also bigger in warm periods and get smaller in colder periods, while lynxes and wildcats do the opposite. I'm not sure why. I have to say, though, that the European wildcat of Late Pleistocene Europe is similar in size to the modern wildcats of southern Spain, which are bigger than other wildcats in the continent, yet are more southernly distributed than them. Confusing, eh?

**Aurochs** were, like cave bears, temperate animals that didn't live in the tundra-steppe, where they were replaced by bison, and are most common in interglatials. Wild aurochs don't seem to have been any smaller than they were in the Pleistocene: Julius Caesar said that aurochs were the size of *elephants*! Mind you, this is the extinct North African Elephant which was smaller than an Asian elephant and closer in size to a very big horse, but it gives an idea of how massive they were compared to its living descendant, domestic cattle. That's a key as to why they became smaller: domestication. Humans selected the smaller, better manageable aurochs and this is why the species decreased in size, with no relation to climate. Had cattle not been domesticated (or aurochs not been hunted to extinction, just like we have wolves and dogs living side by side today), we'd have aurochs bulls of 1'8 meters at the shoulders. The biggest living bovine, the gaur *Bos gaurus* of southeast Asia (not a descendant of the aurochs) [can surpass 2 meters](http://www.ultimateungulate.com/artiodactyla/bos_frontalis.html).

So it seems that in a lot of cases, your hunch is actually right and lack of food and other factors can trump Bergmann's Rule.",null,0,cdp8p80,1re5lq,askscience,top_week,2
stevenstevenstevenst,"One way it is possible to determine age of a material vs. when a tool of that material was crafted is to compare the age of the material (easily determined by any number of techniques, such a radiocarbon dating and other isotopic methods) and to compare the quantity of atmospheric carbon adsorbed to the surface of the tool.  Quantity of adsorbed surface carbon (also known as adventitious carbon) is proportional to the amount of time the surface has been exposed to atmospheric conditions, and thus a comparison of adventitious carbon quantity of a surface known to have been exposed in the manufacture of the instrument and the isotopically-determined age of the material is informative.  

Other techniques are possible, but various analysis of oxidation, carbon adsorption, or other surface chemical phenomena are generally utilized.",null,0,cdmdtae,1re8mg,askscience,top_week,2
jessickofya,"To date when the tool was used we would look at residue on the tool and date that. So for example, if we found a stone tool with blood we can use dating techniques to get a estimation on when the tool was used. Depending on what material you are dating - you would use one of many different techniques.

There are also ways to break down rock into a gas and estimate the age of formation. Archaeology is all about context too. If we found the tool with a hearth or camp we could look at dating other items and estimate the age of when the artifacts were used based on the dates of surrounding artifacts in the same area. We can even use tree rings, dendrochronology,  to estimate the age of the wood used in the site and assume the age would be similar",null,0,cdmdxqu,1re8mg,askscience,top_week,2
Pachacamac,"Someone else just asked a pretty similar question and I saw theirs first, so I answered it first, and left a pretty detailed response. [You should probably just take a look at it.](http://www.reddit.com/r/askscience/comments/1rectr/how_do_scientistsarchaeologists_carbondate_human/cdmfnvd)

Basically, with most types of stone we can't date the stone at all (so we don't know how old it is, expect by talking to geologists who tell us that it comes from a certain formation of a certain age. But we don't typically care about that). We figure out when the tool was made by assuming that it was made, used, and discarded within a relatively short period of time (a century can be ""short"" to us because of the error ranges that all the different dating methods have, but stone tools wear out and break quickly so anything was probably used and tossed away in the same year that it was made). So because it was discarded at a site, we assume that it is as old as the site itself, and we date the tool by association; i.e. it was found with other things that we can date directly (like charcoal on younger sites, or layer of volcanic ash for sites as old as the one in this article), so we assume that it is as old as those things. So the fact that the rock itself might be 400 million years old doesn't matter; we find a tool at a site that we can date to 280,000 years ago and we assume that the tool and the site are the same age, as long as there is no evidence to suggest otherwise.

Now, I said with most stone. Obsidian is different. There's a method called obsidian hydration dating that we can actually use to date obsidian tools, which are what was found at the site in the article. When you make a stone tool you are always chipping away and breaking the surface, so when the tool is brand new it will have a fresh surface. Obsidian weathers at a known rate so you can look at the surface and determine how old it is by how much weathering is on it. This isn't a perfect method and it can't really tell us exactly how old the tool is (because there's so much variation across regions), but it can tell you that one tool is older than the other. Maybe they can get actual calendar years for Ethiopian obsidian too, I don't know (I'm not familiar with the area).",null,0,cdmfyqt,1re8mg,askscience,top_week,2
humanino,"You can access the article here :  
[Thermoelectrically Pumped Light-Emitting Diodes Operating above Unity Efficiency (pdf)](http://dspace.mit.edu/openaccess-disseminate/1721.1/71563)

Please note that they have not broken any thermodynamical law. They have a device which uses electrical power, and converts this power into heat and light. The power emitted in the form of light is larger than than the part of the electrical power directly used to create light. That is because the other part of the electrical power, which created heat, has also been re-converted into more light. That is really neat and clever, and it does have potential applications, but the ""communication"" part might have been misleading. ",null,0,cdnyxu0,1reb7j,askscience,top_week,2
iorgfeflkd,"Yeah, for example a red dwarf orbiting a much brighter star. When the dwarf is transiting, there will be less total light coming from the system.

[Here](http://arxiv.org/pdf/1109.2055.pdf) is a paper where they tried to measure this loss of light from a red dwarf orbiting another star.",null,1,cdmfm8v,1reb7p,askscience,top_week,7
HV250,"You seem to be confusing voltage with current. Voltage is just the potential difference required for current to flow. How much current actually flows is what determines whether you have enough for all components. As the current is consumed, the voltage slowly dwindles over time, till a point where the potential difference is simply not sufficient to let the charges move. That's when you need to charge it.",null,0,cdmh1b3,1rebi1,askscience,top_week,10
kizzap,"There are a number of things that could be happening. 

First, it would be most likely be connected in *parallel* not in series, thus the processor will be getting the 3.7V as well. LEDs take such small current too that a single LED will run for quite some time off that battery.

Secondly, it is quite possible that there is a switching power supply in the controller, which changes a lot of things.

Third, not all LEDs are 3 volts... ",null,0,cdmicso,1rebi1,askscience,top_week,3
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,top_week,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,top_week,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,top_week,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,top_week,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,top_week,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,top_week,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,top_week,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,top_week,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,top_week,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,top_week,1
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,top_week,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,top_week,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,top_week,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,top_week,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,top_week,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,top_week,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,top_week,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,top_week,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,top_week,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,top_week,1
Platypuskeeper,"&gt; Hybridization is a generally good theory, but it doesn't explain properties like magnetism.

Valence-bond theory actually explains the paramagnetism of oxygen, if that's what you're referring to. (and has since the start, it's in Pauling's ""The nature of the chemical bond) It's a common myth though, so anyway...

You have antibonding orbitals because of symmetry. Each 'even' (symmetric) state has a corresponding 'odd' (antisymmetric) state. Now, I don't expect you to get what that means, so I'll demonstrate:

Two hydrogen atoms get close, and their atomic 1s orbitals combine to a _molecular orbital_. (The 1s orbitals are spherical and have a wave function that's like exp(-r), if you neglect constants). We assume for the sake of this example, that they form a linear 'superposition'. The combined wave function is simply the sum of the functions times some constants. 

There are only two possible combinations here: which is 1s_1 + 1s_2 and 1s_1 - 1s_2. This is because the overall phase (sign) of the function doesn't matter. so -1s_1 - 1_s2 is the same thing as 1s_1 + 1s_2. 

In the first one 1s_1 + 1s_2, where they add up, then the electron density is above zero everywhere, since the 1s orbital is exp(-1) and above zero everywhere. So there must be electron density all the way between the two nuclei. It's a _bonding_ molecular orbital.

In the second molecular orbital these two can create, 1s_1 - 1s_2, there is a spot at the exact center between the two nuclei where 1s_1 and 1s_2 are the same (because it's the same 1s orbital and the same distance r from their respective nucleus). So the total wave function there is _zero_. There's a region between the nuclei that lacks electrons! This is an _antibonding_ orbital.

[It's easier to see the thing visualized](http://www.expertsmind.com/CMSImages/2087_bonding1.png)

The antibonding MO has higher energy than the bonding one (fortunately for chemistry). A visual rationalization for this is in there's a higher curvature of the antibonding MO. After all, from one nucleus to the other it has to pass through zero. In quantum mechanics, a higher curvature of the wave function (more tightly located electrons), means higher kinetic energy. So the kinetic energy is higher when you have a 'node' like this (nodes being these areas of zero density, as with where a wave is zero). 

All this holds true whichever orbitals you combine to form your MOs. An antibonding orbital is formed for each bonding one, and the antibonding one has higher energy. 

(Note that the 'formation' here, just as with hybridization, is really just a way describing things. MOs don't suddenly form at a particular distance, it's a seamless transition from AOs to MOs)
",null,0,cdmet34,1recfy,askscience,top_week,7
Pachacamac,"Actually you can't carbon date stone at all. Carbon dating needs organic materials with carbon-14 in them (an unstable isotope of carbon), so we need floral or faunal material. Burned seeds or charcoal are the best, but other organic materials can be dated. There are other dating methods that can date non-organic things and can date much older things that radiocarbon dating (which maxes out at 75,000-100,000 years), and some of these are useful to archaeologists/paleo-anthropologists, but radiocarbon dating is the most common method that archaeologists use. 

I'll mention here that there is one method, obsidian hydration dating, that can actually determine how long it's been since a piece of obsidian (volcanic glass commonly used for stone tools) was broken, which happens when the tool is being made (basically you start with a larger rock and chip away at it to shape it into what you want), but this method has a lot of problems and isn't always reliable. It's about the only way to directly date stone tools that I can think of, though.

So, we can't date the actual. How do we determine how old something like a stone tool is? We rely on one of the key assumptions in archaeology that things found together were probably made and used at roughly the same time (radiocarbon dating has an error range of 25-100 years anyway, so ""same time"" can mean same decade or same century). If you find a stone tool within a fire pit, say, then you assume that someone threw it in there during a fire, and the fire pit will have lots of organic material that we can date, So we date the pit and assume that the tool is as old as the pit. That is the most straightforward example I can think of, but the basic idea, dating by association, is how we get specific calendar dates for most of our sites. Same thing if we get a stone tool and a piece of charcoal at roughly the same depth in a site that we know has not been disturbed, you can date it by association.

Edit: just took a look at the article you linked. They've dated those tools to 280,000 years ago, not 85,000 years ago, so they would definitely not be using radiocarbon dating. I don't know what they used. The article is a bit hyperbolic but just keep in mind that, especially with those really early sites, there is a lot of room for error or unknown things complicating the picture, and a ton of room for interpretation, so the big claims that the article makes might be a bit presumptuous. As always, more research is required.",null,0,cdmfnvd,1rectr,askscience,top_week,2
Solivaga,"There's a wide range of radiometric dating techniques, but as /u/Pachacamac points out, you can't use radiocrbon dating on inorganic materials (such as stone), and radiocarbon dating is only really accurate back to around 50k BP, and completely fails much beyond 75k BP.

The short answer is that we use context and stratigraphy to securely sequence artefacts and features - in turn this allows us to identify material as being conteporaneous.  This enables us to date other material that's from the same phase of occupation or activity as the stone tools.

Dating techniques that stretch further back than C14 include Potassium Argon, Uranium Series, Fission Track, Electron Spin Resonance, abd Obsidian Hydration.  The problem with many of these is that they date natural events (including volcanic rock formation, formation of calcium-carbonate etc.), so often we'll be using these dates natural events to constrain the archaeological materials - i.e. we know that this palaeolithic site was occupied sometime between x and y.

",null,0,cdndjg2,1rectr,askscience,top_week,2
fishify,"Hybrids have both an internal combustion engine and an electric drive system, which enables them to achieve better efficiencies in a few ways. One is that they recapture energy that would otherwise be lost; regenerative breaking allows the energy lost to waste heat in a standard car to instead be used to store energy in the hybrid's batteries. Another is that the internal combustion engine can be smaller, and thus operate more efficiently more of the time, since the electric motor is available for peak demands. In addition, the internal combustion engine can be turned off in situations in which a car is idling.",null,0,cdmfog1,1recy4,askscience,top_week,2
bkkgirl,"Well nothing's stopping you from using it except that few people know how to use it, and very little has been translated to it.

Also, people with different accents would _write_ differently. This is critically important in languages such as Chinese, where the differences would render every dialect mutually unintelligible, and somehwat important in languages like English, becuz eugeniks an da lik wud mak ritin litrl spekn had. Written language preserves etymology, whereas the IPA, which would produce different forms for the same word, does not.

Additionally, what is transcribed in the IPA is not entirely uniform, so representations would be ambiguous even among speakers of the same dialect.

Since people usually read by identifying words as a whole, direct transcription of what was said would be counterproductive and difficult to follow, and since that's what the IPA is for, it would be too.

Disclaimer: I can't speak AAVE, so my transliteration is probably shitty as fuck.",null,0,cdmkd36,1refad,askscience,top_week,9
protestor,"A thing about phonetic alphabets is that often two different sounds are interpreted as being the same phoneme in a given language (they are [allophones](http://en.wikipedia.org/wiki/Allophone)), but on a different language they might be distinguished. On a given language the preferred allophone might depend on region, for example. The fact that two sounds may be interchangeable is called [free variation](http://en.wikipedia.org/wiki/Free_variation):

&gt; When phonemes are in free variation, speakers are sometimes strongly aware of the fact (especially where such variation is only visible across a dialectal or sociolectal divide), and will note, for example, that tomato is pronounced differently in British and American English, or that either has two pronunciations which are fairly randomly distributed.

[Each language has its own set of phonemes](http://en.wikipedia.org/wiki/Phoneme#Numbers_of_phonemes_in_different_languages). Some languages don't use tone to distinguish phonemes (but use them for other things), others use a lot.

This kind of non-uniformity may negate any advantage in uniformizing our writing system.

I also find the latin alphabet pretty convenient to type in a keyboard, but the IPA is less so, because it has too much symbols. (also, IPA is sometimes too specific - how to represent a word that we don't know how to pronounce?)

(ps: I suppose you're suggesting we use IPA to substitute alphabets already in use, instead of using IPA just for phonetic transcription)",null,0,cdmqbkr,1refad,askscience,top_week,2
drzowie,"A superadiabatic gradient is what *drives* convection -- the free energy that gets converted to mechanical flow comes from the positive difference between the gradient and the adiabatic lapse rate.  Convection will happen at *some* level with any nonzero excess in the lapse rate above the adiabatic rate, since the material is a fluid.

In practice, the actual lapse rate doesn't get driven exactly to the adiabatic rate, but it's pretty darned close.  The actual offset is driven by the balance between heat flux and (effective turbulent) viscosity in the fluid.  Since stellar plasmas aren't known for their high viscosity, and the scales are large, the offset turns out to miniscule (negligible by orders of magnitude) in nearly all cases -- so you can treat the adiabatic lapse rate as a strict limit, and be good to go.

Let's apply your example of 10^-6 superadiabaticity to the Sun.   [The convection zone spans about 6 orders of magnitude, or about 14 scale heights, in density](http://solarphysics.livingreviews.org/open?pubNo=lrsp-2009-2&amp;amp;page=articlese1.html).  If the lapse rate differs from adiabatic by 1 part in 10^6, that corresponds to a temperature differential factor of e^(14x2/3x1.000001) compared to e^(14x2/3) across the whole convection zone - so if you assumed the lapse rate was exactly adiabatic, but it was really 1+1x10^-6 times the adiabatic rate (and you knew the photospheric temperature exactly), your calculation of the temperature at the base of the convection zone would be off by a factor or (1+1x10^-5).  Other effects (like convective overshoot and dynamo action) enter at the 10^-3 level, so the superadiabaticity is negligible.",null,0,cdmo4bd,1reh81,askscience,top_week,1
LoyalSol,"There isn't really one universal answer since different materials will react differently with acids/bases, but a large majority of them dissolve because of either oxidation like in the case of metals or through catalyzed reactions (the acid/base speeds up a reaction that normally would occur slowly).

Oxidation is pretty straight forward.  The metals have electrons taken away by the acid and once that happens they form stable ions which can be freely dissolved into solution.  In catalytic reactions the acid/base comes in and binds to a functional group on a molecule (usually organic molecules) and stabilizes the molecule in a way that it can undergo further reactions.  

http://www.organic-chemistry.org/namedreactions/fischer-esterification.shtm

That's an example of the forward reaction, but the reverse reaction is similar.    In large scale a organic molecules such as proteins, each peptide in the chain is linked together by an functional ground (amide group for proteins, O=C-N) and the acid/base will attack these links causes the chain to break apart.  Which is why they are generally detrimental to biological organisms. ",null,0,cdmm2e5,1rehln,askscience,top_week,2
NotFreeAdvice,"Answering your second question, glass is often used for two reasons.  First, the Si-O bonds that are the structure of silica compounds (like glass) are relatively inert.  Thus, they do not like to be broken by other compounds/chemicals.  Second, it is amorphous, which adds both strength to the vessel and well as a reduction in reactivity that can occur at the edges of crystal faces.  Hence, the amorphous nature renders the glass less reactive than it would be if it were crystalline silica.  

There are some things that are not good to store in glass, however.  Potassium hydroxide will etch away the glass, and hydrofluoric acid will do the same.  These are just two examples, but there are a number of chemicals that are not inert, with respect to glass. 

Hope that helps!",null,1,cdmmctc,1rehln,askscience,top_week,2
SilentCastHD,"Well, first of all, you have to differentiate subtractive color from additive color.

In the first case, all the colors give you black, in the second, all the colors give you white.

So to make that clear: If you take a flashlight, and shine it onto a white paper, you see white light. - Duh...

If you take a red marker and mark the page, you strip away ""all the light"" that isn't red and absorb it, so only the red light reflects. The dye subtracts the [wavelengths](http://upload.wikimedia.org/wikipedia/commons/c/c4/Rendered_Spectrum.png) that don't correspond to red.

So you transform white light to red light using the filter ""red marker dye"".
Going forward, with blue and yellow, you strip awaay more and more of the light, until no light is relected anymore, leaving you with black.

The other way around, you [add up colored light](http://upload.wikimedia.org/wikipedia/commons/2/28/RGB_illumination.jpg) to make white light.

So you shine red light onto a white wall, the reflected light is red. If you overlay it with the other colors, you'll get white again.

(This is why green looks black in ""pure red light"", since there is [no refelection of red light on green leafs](http://1.bp.blogspot.com/-hHcuVK0TGHg/TyVDCInAFWI/AAAAAAAAAT0/3tU3h7p1Zbw/s1600/Lights%2B1%2B-%2BOriginal.jpg))

So with that out of the way: What is grey?

Grey is the achromatic color between black and white.

So, since you get the two different colour-systems now, you see that grey in [RGB](http://en.wikipedia.org/wiki/RGB) displays (additive color) has to be different from the grey in a printed picture in [CMYK](http://en.wikipedia.org/wiki/CMYK_color_model)(subtractive color)

So, as you can see [here](http://www.aksiom.net/rgb.html) at the bottom the RGB value for grey is always something where R=G=B, and the stronger the individual light gets, the more you go up to white.

I hope this helps :)",null,0,cdmjc3a,1rekov,askscience,top_week,6
svarogteuse,"
$60 is not worth spending on a telescope. You will end up with a very low end wobble device and be disappointed.  Buy a set of binoculars first. If you decide that you aren't that into astronomy later binoculars have other uses a telescope really doesn't. Next go hang out with the local Astronomical society. Look at what they are using get to know their equipment before you make a purchase more than binoculars.
The smallest scope regularly used in our society 10 years ago was an 6"", well above the $60 price tag and the 2-3"" of the ones you mentioned.

15x70s are huge binoculars. You are going to have problems keeping them steady unless you invest in some sort of [mount for them](http://www.telescope.com/Orion-Paragon-Plus-Binocular-Mount-and-Tripod/p/5379.uts).  With those binoculars you will be able to see the moons of Jupiter, the ears on Saturn, maybe Titan if you are in a good spot, and clusters. They aren't really designed to see galaxies except the brightest ones. The standard binoculars used are 10x50s. Light enough to hold steady, or balance on a chair but powerful enough to see binocular objects (bright clusters, comets, birds). We really don't use binoculars for planetary observing not enough detail. I would recommend a set of 10x50s before the 10x70s. I have never owned nor known anyone to own such large binoculars except for special purposes like comet hunting and defiantly not without a mount.

Neither of those scopes are really worth using for more than a causal, hey that's the moon kind of use. I used a 6"" for many years around 2000 and it was the smallest scope in the group. 8"" is a standard entry level amateur scope. What matters in a telescope is aperture the size of the main lens or mirror. The larger the aperture the more light is concentrated onto your eye, the fainter an object can be seen. You want to spend your money on aperture! Magnification doesn't matter, most observing is done with relatively low magnification but the higher aperture the better.  

Long time amateur astronomer (30+ years), previous president local astronomical society. ",null,0,cdmnyge,1reljs,askscience,top_week,2
_NW_,"Having both works better.  Do the binoculars have a tripod mount?  At that kind of magnification, it's going to be difficult to hold steady.  Also, after a few minutes, your arms are going to start getting really tired.  My first telescope was a 60 mm Tasco.  A good pair of binoculars worked better.  Years later, I bought a 6 inch reflector and finally got see all the things that I couldn't with the Tasco.  I have a pair of 10x50 binoculars that I use alone or with the telescope.  When looking for something in the sky, it helps to find it with binoculars first before using the telescope.  Or, sometimes I just go look with the binoculars just because it's easy.  Also, because it's more than enough to see several galaxies, star clusters, nebulas, and Jupiters moons.  Actually, the Andromeda, LMC, SMC, and a few other galaxies are visable even without binoculars.  Stop at a store and pick up a copy of ""Sky and Telescope"" or ""Astronomy"" Magazine.  Both have a star map that shows what you can see for that month.",null,0,cdmzcq2,1reljs,askscience,top_week,2
botanist2,"I do a bit of bird watching and very amateur stargazing, so I have some experience in this issue.  One of the biggest problems with using binoculars for anything like bird watching or stargazing is that your arms aren't very steady, which isn't that much of an issue at lower magnifications (e.g. looking at birds in the tree above you), but is really bad at higher magnifications (e.g. trying to look at ducks way out in the pond).  I would suggest getting a telescope with a tripod because you'll get a lot more stability and you'll be able to see things more clearly as a result.",null,0,cdmmve6,1reljs,askscience,top_week,1
drzowie,"Jovian interference.  The asteroids are near a couple of major resonances with Jupiter; that gives them enough of a nudge to prevent them from coalescing.  (Source:  while I am not a planetary scientist I work in a lab with a passel of 'em).

A bit more: Small-ratio resonance orbits with major bodies typically have nothing in them, because over time the larger body kicks the smaller ones out of that orbit.  Think of pushing a swing, or operating a cyclotron:  you can transfer a *lot* of energy to an oscillating body just by kicking it gently in some pattern with a harmonic relationship to the oscillation.  Major bodies typically clear out their own orbits over time due to the 1:1 resonance with anything else in that orbit -- anything at, say, 0.999 AU would eventually have a near encounter with Earth, and get ejected.  That effect is why Ceres and Pluto are considered ""dwarf planets"" and not ""planets"" -- the dynamical process is part of our modern definition of a planet.  The 2:1 and 4:1 resonances with Jupiter define reasonable approximate boundaries of the asteroid belt, and there are noticeable gaps near small-integer ratios of Jupiter's period between those values.
",null,0,cdmoip2,1remcu,askscience,top_week,3
The_Evil_Within,"Jupiter exerts enough of a disruptive force on the asteroid belt to keep it thinned out - and due to their relative motion, individual asteroids are at least as likely to smash into *more* debris than they are to coalesce into one bigger mass.

At least, so I was informed when I asked this question here a while ago.  The detailed explanation was kind of over my head, as you might expect.

Given that explanation, I still have trouble understanding how Ceres could form in the first place, yet still not be capable of 'finishing' and collecting the remaining mass of the asteroid belt.",null,1,cdmojfi,1remcu,askscience,top_week,3
GumbyTastic,"Well you have to look at it like this. Why does saturn have rings? Why doesn't all that mass floating around it just make a new moon? Usually the mass doesn't have enough force to coldine and make new objects or there's not enough force keeping the mass together. The asteroid belt (don't quote me) like a big ring like saturn. It's just full of rocks and debris that get caught up in the suns gravitational pull. It looses mass and gains mass when new objects are knocked out and sucked in. Correct me if I'm wrong on anything. I enjoy learning and never see much coverage, hell I never see anything about the asteroid belt!!",null,4,cdmj64t,1remcu,askscience,top_week,3
bobbycorwin123,"bah, I cant find any links to the exact reason. I believe its because of the rotations of mars and Jupiter and the way the gravity of the two bodies prevent stuff from gathering too much...

All I remember is that Jupiter and Saturn have a 2/1 rotation ratio...which helps not at all in this",null,6,cdmjcle,1remcu,askscience,top_week,3
I_Gargled_Jarate,"Gravity isnt strong enough to compress asteroids into larger planets. It takes a high velocity collision for asteroids to fuse together. Gravity does play a part by attracting large bodies which may be potentially travelling at very high velocities, but just sitting next to each other is not enough to form larger objects.",null,4,cdmk4yf,1remcu,askscience,top_week,1
wazoheat,"No. Food and drink go bad due to [spoilage](https://en.wikipedia.org/wiki/Food_spoilage), which is usually due to the growth of bacteria and/or fungus, none of which will grow in plain water.",null,0,cdmi5o5,1remei,askscience,top_week,3
ides_of_june,As wazoheat said water doesn't spoil. It's possible that the container that the water is stored in could undergo thermal degradation making the water unfit for consumption (or at least undesirab. Also if the water is stored open to the environment it can become contaminated though in an indoor environment it's unlikely to become unfit for consumption.,null,0,cdmo21b,1remei,askscience,top_week,1
ramk13,"Depending the temperature ranges you could breakdown the disinfectant residual (usually chlorine or monochloramine at ~1 mg/L) that is normally present in tap water. If the residual is gone, then organisms (e.g. algae) are much likely to grow in your water if spores are present. 

Also if the temperature gets high enough you can have interactions between the water and its container. Metals are more likely to corrode and leach, and probably more relevant, plastics can leach plasticizers into the water. I don't know that there are studies that have quantified whether there are documented effects in animals or humans for tap/drinking water stored in plastic bottles, but many people are concerned about it.

The water itself won't spoil.",null,0,cdmrq6r,1remei,askscience,top_week,1
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,top_week,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,top_week,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,top_week,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,top_week,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,top_week,27
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,top_week,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,top_week,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,top_week,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,top_week,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,top_week,27
paolog,"The ""criss-cross"" distance between two points is called the Manhattan distance between the points, while the straight-line distance is called the Euclidean distance. What you're asking is whether the limit of the Manhattan distance as the grid gets finer is equal to the Euclidean distance. It's easy to show that this is not the case.

Let's take a 1 x 1 square. The Manhattan distance from one corner to the other is 2 (length of bottom edge + length of right edge, for example), while the Euclidean distance is, by Pythogoras' theorem, √2.

Now subdivide the square into a 2 x 2 grid of four squares. To get from one corner to another, we have to zigzag along four edges of the small squares, each of which is 1/2 a unit long. So the total distance is 4 x 1/2, or 2.

It's not hard to show that however we subdivide the square into smaller squares (or even rectangles), the shortest corner-to-corner distance measured along the edges of these squares will always be 2 and will never get anywhere near √2. Hence no matter how many turns we make, the Manhattan distance never equals the Euclidean distance.

So no, nothing changes as the resolution of the grid becomes finer. Furthermore, a diagonal is not imaginary - it is just different from walking along the edges.

EDIT: removed repetition",null,1,cdml7vm,1reu8x,askscience,top_week,21
Professor_Snuggles,"The fundamental point here is this: two curves can be visually similar yet have very different lengths. Imagine a bug taking inch long steps in a long zig-zag across the line. What you're doing is similar to saying that the bug can instead cut closer to the line and decrease the forward distance covered with each zig-zag. This could give a path that stays closer to the original line overall, but has the same length because it wiggles more.

The moral of the story is that curves that stay close to each other do not have to have lengths that stay close to each other. As for real life: yes there is a difference traveled if you take a small step up, then a small step right, etc. compared to directly walking the diagonal. This is easiest to see if you have a robot or something that you can guarantee will travel at a constant speed and a timer. A real life diagonal is not necessarily ""imaginary"", it's just that traveling near it in any way you want is not going to get the same results as traveling *on* it, or other paths that more accurately approximate doing so.",null,1,cdmrs35,1reu8x,askscience,top_week,7
jeff0,"The size of the grid doesn't matter. Say your rectangle is a 1 mile x 1 mile square. The length of the diagonal from A to B is sqrt(2) =~ 1.4 miles. If you instead alternate walking due north with walking due east, you end up walking a total of 1 mile east and 1 mile north = 2 miles total. The size of the grid will only effect the number of times that you turn.

The same idea underlies the [troll math](http://knowyourmeme.com/forums/meme-research/topics/8029-troll-math) meme.",null,0,cdmleqb,1reu8x,askscience,top_week,2
wgunther,"Just to prove the bit more formally instead of showing some examples: if you divide a 1x1 square into an n by n grid then the distance you are traveling in total n*(1/n+1/n); that is, imagining you are in the bottom left corner, you have to go up distance 1/n and right distance 1/n for each subdivision, and there's are n of them. Therefore, the distance you must travel is 2. 

Intuitively it makes sense: all your motion is either purely vertical or purely horizontal. You must move horizontally distance 1 and vertically distance 1. Therefore you must move distance 2. ",null,1,cdmmhao,1reu8x,askscience,top_week,3
ignorant_,"A diagonal line bi-sects a square at 45degrees. The question asked is regarding using horizontal and vertical lines to travel toward the opposite corner. These lines are at 90degrees. Suppose we used intermediate angles. 89 degrees, then 88 degrees, etc., and work our way down toward 45degrees. Wouldn't my distance begin to decrease as the angle approaches 45 degrees, and have a limit of square root of 2?",null,1,cdmw7ub,1reu8x,askscience,top_week,1
yeast_problem,"Quantum Mechanics would bring a limit to this, as the grid size gets smaller the uncertainty principle would mean your momentum could not be zero in the  direction perpendicular to travel. It would become impossible to say that you were actually travelling along the grid lines, at scales around 10^-34 meters.",null,4,cdmvy76,1reu8x,askscience,top_week,1
breadmaniowa,"The real reason you feel the need to breathe is because of the carbon dioxide building up in your blood. Taking in oxygen removes the dissolved carbon dioxide from your body. So basically, the real reason you can't hold your breath for very long is that you need to expel the carbon dioxide from your body. You actually have plenty of oxygen still in your blood when you feel the need to breathe.",null,5,cdmlr1p,1revb2,askscience,top_week,12
fazedx,"There are two drivers in the human body that tells it to breathe. The first one is concentration of carbon dioxide in the blood, and the second one (backup, if you will) is the concentration of oxygen.

Carbon dioxide (CO2) is allowed to pass the blood brain barrier. High concentrations of CO2 diffuse into your cerebral spinal fluid (CSF), dissociates into hydrogen ions and lowers your CSF pH. This is picked up by chemoreceptors and signals your central nervous system to increase ventilation. This is your central, or main control of breathing.

Peripheral control is based on pO2 in arterial blood. If it drops below a certain point, it will send signals to your brain to start breathing.

You can reduce the pain from holding your breathe by hyperventilating before you hold your breathe, thus reducing the buildup of acid and the prolonging the time it takes for your brain to signal to you to breathe.

http://www.winona.edu/biology/adam_ip/misc/assignmentfiles/respiratory/Control_of_Respiration.pdf is a good source/summary",null,0,cdmuuor,1revb2,askscience,top_week,3
bbqbollocks,"Because there are two ways a stm works. Constant current and constant height.

With constant current, the distance between the tip and the sample changes to keep the current flowing through the tip the same. This maps the topography of the surface. 

If the sample is flat enough then you can use the constant height mode. The constant height mode will keep the distance between the tip and sample fixed as it scans across the surface. So if you have 35 xenon atoms writhing range cor quantum tunneling to take place then a current flows where the atoms are. So no nickel atoms can be viewed. This mode looks at the density of states on the surface. ",null,0,cdmlwie,1revqt,askscience,top_week,11
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,27,cdmlm65,1rew42,askscience,top_week,170
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,10,cdmokr1,1rew42,askscience,top_week,21
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,27,cdmlm65,1rew42,askscience,top_week,170
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,10,cdmokr1,1rew42,askscience,top_week,21
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,top_week,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,top_week,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,top_week,13
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,top_week,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,top_week,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,top_week,13
null,null,null,410,cdmn013,1rf2b3,askscience,top_week,1933
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,275,cdmlcrf,1rf2b3,askscience,top_week,1453
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,40,cdmmor1,1rf2b3,askscience,top_week,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,top_week,67
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,10,cdmlptk,1rf2b3,askscience,top_week,53
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,27,cdmrqlw,1rf2b3,askscience,top_week,53
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,11,cdmpnpe,1rf2b3,askscience,top_week,22
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,top_week,11
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,top_week,18
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,18,cdmm72d,1rf2b3,askscience,top_week,22
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,top_week,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,8,cdmqbvb,1rf2b3,askscience,top_week,13
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,top_week,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,1,cdmn10w,1rf2b3,askscience,top_week,6
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,9,cdmpswn,1rf2b3,askscience,top_week,12
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,10,cdmpwee,1rf2b3,askscience,top_week,12
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,top_week,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,1,cdmq96f,1rf2b3,askscience,top_week,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,10,cdmr4on,1rf2b3,askscience,top_week,13
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,top_week,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,top_week,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,9,cdmmecq,1rf2b3,askscience,top_week,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,3,cdmmxgj,1rf2b3,askscience,top_week,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,9,cdmn15l,1rf2b3,askscience,top_week,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,1,cdmn6cx,1rf2b3,askscience,top_week,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,9,cdmngbx,1rf2b3,askscience,top_week,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,9,cdmnh29,1rf2b3,askscience,top_week,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,9,cdmnzr4,1rf2b3,askscience,top_week,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,top_week,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,9,cdmo9dv,1rf2b3,askscience,top_week,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [Rayleigh–Bénard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,top_week,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,1,cdmphjt,1rf2b3,askscience,top_week,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,top_week,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,top_week,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,1,cdmtzvb,1rf2b3,askscience,top_week,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,top_week,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,2,cdmw6gb,1rf2b3,askscience,top_week,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,top_week,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,top_week,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,1,cdn5ejj,1rf2b3,askscience,top_week,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,top_week,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,1,cdna4y1,1rf2b3,askscience,top_week,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,top_week,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,10,cdmnnog,1rf2b3,askscience,top_week,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,top_week,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,top_week,1
null,null,null,410,cdmn013,1rf2b3,askscience,top_week,1933
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,275,cdmlcrf,1rf2b3,askscience,top_week,1453
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,40,cdmmor1,1rf2b3,askscience,top_week,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,top_week,67
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,10,cdmlptk,1rf2b3,askscience,top_week,53
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,27,cdmrqlw,1rf2b3,askscience,top_week,53
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,11,cdmpnpe,1rf2b3,askscience,top_week,22
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,top_week,11
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,top_week,18
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,18,cdmm72d,1rf2b3,askscience,top_week,22
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,top_week,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,8,cdmqbvb,1rf2b3,askscience,top_week,13
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,top_week,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,1,cdmn10w,1rf2b3,askscience,top_week,6
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,9,cdmpswn,1rf2b3,askscience,top_week,12
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,10,cdmpwee,1rf2b3,askscience,top_week,12
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,top_week,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,1,cdmq96f,1rf2b3,askscience,top_week,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,10,cdmr4on,1rf2b3,askscience,top_week,13
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,top_week,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,top_week,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,9,cdmmecq,1rf2b3,askscience,top_week,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,3,cdmmxgj,1rf2b3,askscience,top_week,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,9,cdmn15l,1rf2b3,askscience,top_week,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,1,cdmn6cx,1rf2b3,askscience,top_week,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,9,cdmngbx,1rf2b3,askscience,top_week,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,9,cdmnh29,1rf2b3,askscience,top_week,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,9,cdmnzr4,1rf2b3,askscience,top_week,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,top_week,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,9,cdmo9dv,1rf2b3,askscience,top_week,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [Rayleigh–Bénard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,top_week,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,1,cdmphjt,1rf2b3,askscience,top_week,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,top_week,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,top_week,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,1,cdmtzvb,1rf2b3,askscience,top_week,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,top_week,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,2,cdmw6gb,1rf2b3,askscience,top_week,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,top_week,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,top_week,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,1,cdn5ejj,1rf2b3,askscience,top_week,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,top_week,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,1,cdna4y1,1rf2b3,askscience,top_week,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,top_week,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,10,cdmnnog,1rf2b3,askscience,top_week,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,top_week,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,top_week,1
Osymandius,"Contrary to the answers below ATP **is** produced within the chloroplast. ATP synthase is located in the thylakoid membrane/space and does make use of the proton motive force generated by either cyclic or non cyclic photophosphorylation. But - the ATP produced in the chloroplast just isn't enough to compared to the amount produced in the mitochondria. We move relatively minimal numbers of protons across the membrane during photosynthesis - the really important product of non-cyclic photophosphorylation is the generation of reducing equivalents (NADPH). This can then be used to fuel the Calvin cycle and the production of triose phosphates and sugar derivatives.

Once we have produced TP/sugars, these can be metabolised to produce NADH in the mitochondria. The proton motive force produced by the electron transport chain is considerably greater, and much more ATP can be generated than relying on chloroplasts alone.",null,0,cdmrffo,1rf3cf,askscience,top_week,3
quantum_lotus,"As /u/Osymandius says, both organelles can produce ATP (the most useful form of stored energy for a cell), but that mitochondria are much more efficient at making it.

But there is another consideration.  Evolutionary data and model point to chloroplasts being acquired *after* mitochondria.  So the cells that eventually became the plant lineage already had mitochondria in them before they captured chloroplasts.  ",null,0,cdn40cu,1rf3cf,askscience,top_week,2
botanist2,"No.  The purpose of the chloroplasts is to make the energy needed for respiration, they don't have the ""machinery"" necessary to put the energy in the most usable form like what happens in the mitochondria.  Your question is kind of like asking ""Why can't the gas tank run the car?""",null,2,cdmplym,1rf3cf,askscience,top_week,2
ramk13,"Though diffusion is slower at lower temperatures, lowered vapor pressure is a much bigger influence. Most odors are either small solid particles or vaporized compounds. The equilibrium vapor pressure of a compound is exponentially dependent on temperature, so when it's colder a lot less of the compound gets into the air. Since it doesn't vaporize as much you smell less of it.

Also, most of the stuff you smell is more likely to be transported by convection (movement by temperature induced density gradients) or advection (forced movement) than diffusion.

For an empirical relationship between vapor pressure and temperature, you can use the [Antoine Equation](http://en.wikipedia.org/wiki/Antoine_equation) which is derived from the principles of the [Clausius-Clapeyron relation](http://en.wikipedia.org/wiki/Clausius-Clapeyron_relation).",null,0,cdmrge3,1rf3jg,askscience,top_week,3
stevenstevenstevenst,"At lower temperatures, vibration of particles is decreased due to the decreased energy of the system.  As diffusion of gases relies upon random vibrational motion for the even dispersal of a compound, gaseous compounds (such as any odor) will spread increasingly more slowly with decreasing temperature.",null,0,cdmp1lq,1rf3jg,askscience,top_week,1
Daegs,"This is not a 3D gif. 

A 3D gif would either require:

* two stereoscopic panels which you could view by changing the focus of your eyes so that the panels merge

* A single panel using red / blue shading and 3D glasses

* A single panel and special display to work along with polarized glasses.

This **non-3D** gif simply give perspective by being displayed over the ""break"" and the changing focus which cues our brain that there is 3d information being presented.

In other words, there is nothing special about 2 breaks, 3 breaks, 4 breaks, whatever.... the breaks are just used so that the gun can go ""over"" something that we perceive as flat. ",null,1,cdmtkie,1rf5et,askscience,top_week,6
tigertealc,"Catalysis by definition is a process by which a substoichiometric reagent promotes a reaction by lowering the activation barrier of the reaction. So that would be the common denominator, I suppose. 

Working out the mechanism of a catalytic reaction is not always straightforward. Most often, mechanisms are proposed to follow mechanistic steps that have been determined for related systems, or using intuition. But a number of different control experiments must be run to differentiate between different possibilities. Often these experiments involve the kinetics of the reaction, whether it involves determining the rate law of the reaction or determining a kinetic isotope effect. Isotopically labeled reagents can also assist, by seeing where they end up in the product. Computation can certainly aid in the assignments of mechanisms, but empiricism is the main method. And of course, the exact experiments that one is able to run to elucidate the mechanism is largely dependent upon the specific reaction. 

If you have any specific questions about specific reactions, feel free to ask. ",null,0,cdmotdy,1rf5ro,askscience,top_week,4
Platypuskeeper,"There is no common denominator other than the fact that catalysts catalyze. One reaction might be catalyzed by acid, the presence of H^+ , which participate in the reaction but are released on a later step. Another reaction might be catalyzed by a Lewis base, where the base temporarily donates an electron pair to a reacting atom. Those two scenarios really have nothing in common other than that they fulfill the definition of 'catalyst'. The word describes a role something plays in a reaction, but the reactions can be as different as any chemical reactions. There's no general theory of reactions either.

",null,0,cdmt6k1,1rf5ro,askscience,top_week,4
stuthulhu,"&gt; once you pass the outer layers of our atmosphere you are weightless - why cant we achieve that speed?

Weightlessness is a state achieved when no force other than gravity is acting upon you. When a vehicle is accelerating/decelerating, that force will be acting upon you, and you will not feel weightless. You would feel pushed against the back of the vehicle by the force of the acceleration.

The shuttle must burn fuel to leave our inertial motion, and burn fuel to match that of its destination. Being likely far more massive, both become more expensive actions, and the more fuel required to do either action increases the weight even further. ",null,0,cdmtslm,1rf5vk,askscience,top_week,1
WendyMouse,"The shuttle is bigger.  A LOT bigger.

New Horizons is a very light spacecraft-- about the size of a grand piano, launched from a very powerful rocket.  It was the combination of the two that made it travel so fast, faster than anything else humanity had ever launched.  New Horizons does not have enough propellant to slow itself down to enter into Pluto's orbit.  The fuel to do that would be too heavy.


Escape velocity from Earth is everything.  Humanity hasn't mastered launching a bunch of things in pieces, merging them and having another separate launch in space yet. 

Just because something doesn't have weight, (you are not in zero gravity in space, you are in microgravity), doesn't mean it doesn't have mass or momentum.

",null,0,cdnst3v,1rf5vk,askscience,top_week,1
Daegs,"On earth you can see million miles away yourself, right now!!! Just look at the stars.

Remember the sun is ~93 million miles away, most of the stars you see are orders of magnitude further away. 

We can see the andromeda galaxy with our naked eyes, so that is 14,696,249,500,000,000,000 miles away!

In space, you wouldn't have the atmosphere filtering photons coming from stars, so you'd be able to see even more.

This is why we have the hubble telescope in space, to avoid earth's atmosphere. ",null,0,cdmtpir,1rf86r,askscience,top_week,6
king_of_the_universe,"http://www.uitti.net/stephen/astro/essays/farthest_naked_eye_object.shtml

says:

&gt; Bode's Galaxy (M81), at 12 million (12,000,000) light years has been spotted by several people. This [page at SEDS on M81](http://www.seds.org/messier/m/m081.html) has a description of how to see it.

&gt; The trouble is, at Magnitude 6.9, M81 is dimmer than most consider naked eye. It depends on whose eye it is, and also where the feet are standing. It has to be an exceptionally dark sky site, probably at some altitude, at the right time of year, etc.

WolframAlpha's answer to ""12,000,000 lightyears in miles"" is 7.054×10^19 miles, which is 70,540,000,000,000,000,000 miles. (Take that, Daegs! ;)

The text also says that there could be bright events like super novas that could even be visible with the naked eye from further away for a few days.",null,0,cdncboq,1rf86r,askscience,top_week,2
Platypuskeeper,"The [Golden Rule](http://en.wikipedia.org/wiki/Fermi%27s_golden_rule) says that transition probabilities depend on the overlap between the initial and final states. In a Rydberg atom, you're in a highly excited state where the electron is far from the nucleus, and its overlap with the ground state and lowest-energy states is quite poor. So direct transitions back down to there are improbable. 

",null,0,cdmr8jt,1rf8ta,askscience,top_week,3
amvakar,"The first (and inescapable) factor in the large size of source code compared to the compiled binaries is the lack of information density inherent in any plain-text format -- you've got to keep things human-readable, which means that you're restricted to the alphabet plus enough special characters for basic formatting and organization. Each operation will involve reasonably-descriptive names as opposed to the pointers to their location in memory that the processor will see. Documentation will also be present. In short: you're describing what the computer will do so a person could understand it, while the computer will only need to be told the bare minimum about the operation to complete it. To see this in action, run the source through any compression algorithm -- the size will go down significantly.

The second factor in large software projects is the presence of code that might never actually be used. For an operating system, you'll end up with drivers for devices you don't have or support for CPUs you're not using. For applications, you might have support for different APIs or just functionality you choose not to include in the finished product. For debugging purposes there may be tests and additional information so that problems can be tracked down, and in debugging builds optimization may be turned off. 

In short: source code is far more descriptive than binary for human purposes and includes a lot of things that you may never end up using in the final build.",null,2,cdmstpb,1rf8w9,askscience,top_week,14
LeoPanagiotopoulos,"The limit of the situation you're describing is a ratio of 1 where the planet and moon are indistinguishable because they're the same mass. It's unlikely but possible. You're correct in your suggestion that the distance from the 3rd, larger mass in the system is important. If The distance between our twin planets (or moons? or ploons? or [manets](http://2.bp.blogspot.com/_gJ6d5yFc7fw/TL72k9N-pqI/AAAAAAAAB_I/Gbu1fWmRxPU/s400/g013v_manet_lemon.jpg)?) is comparable to the distance to the larger object in the system, their orbits around each other will be unstable. 

[Consider reading about triple star systems](http://en.wikipedia.org/wiki/Star_system#Triple_star_systems). The situation we're talking about is labeled C on the linked diagram. It's true that interactions between stars that are very close to each other can be a little more complex that cold, non-fusing rocks (planets), but in most cases the dynamics are comparable. 

Almost forgot: the 3rd object is more often smaller and orbiting the two inner objects, which are orbiting each other. Still your situation is possible. ",null,0,cdnk8oc,1rf9zl,askscience,top_week,2
bohr_exciton,"&gt;If we know the wavelength of a polarized photon... then why cant we determine where exactly a given photon will interact with the resist? I'm guessing something here will touch upon wave-particle duality...

Right, specifically it's the wave aspect that sets the limit. Light passing through a specific aperture or lens will not arrive in one infinitely sharp point but in a disk (e.g. the so called Airy disk for circular apertures). For far-field light, the size of this disk will be determined by a number of factors such as diffraction and the aberrations in the imaging system. The best possible case using simple far field optics is to obtain the diffraction limited spot, which is on the order of half the wavelength of the incident light. 

&gt;Part2: If we've got vapor deposition for things like gold ... why can't we vapor deposit a single atom later of the resist ... again being able to do away with the complex mask?

I'm not really sure I understand this question. Under certain circumstances it's possible to deposit metals uniformly for a desired number of monolayers. However, you need a mask if want something other than a uniform layer, e.g. patterning for an integrated circuit. ",null,0,cdmsmcx,1rfapt,askscience,top_week,1
stevenstevenstevenst,"The most serious affect upon the body due to exposure to lower or zero gravity is atrophy of the muscles.  As you will weigh less or nothing at all, you muscles have to work much less and thus will begin to degrade.  This is the reason individuals on the ISS need to work out regularly by running on a treadmill or though other means.

As blood circulation is negatively affected by reduced gravity (due to the way this system has evolved to partially utilize gravity in its function), other health problems may potentially be associated with manned spaceflight, such a neurodegeneration- although this research is ongoing.",null,0,cdmo0v8,1rfbi7,askscience,top_week,2
mzyos,"There is some worry at NASA currently about Optic nerve atrophy. This is where the nerve carrying signals for sight from the eye starts to deteriorate. It seems that about a 3rd of astronauts have this, if they have experienced long bouts of zero G. They are studying on ISS at the moment using a goldmann tonometer which measures eye pressure. They don't really understand what is going on just yet, but it might be due to the lack of gravity causing some of the eye, and it's nerve's blood supply being slowed, or stopped in one way or another. ",null,0,cdojdvm,1rfbi7,askscience,top_week,1
Truck43,"That's really two questions, whether or not the shell will act as a faraday cage, I'll leave to another, but, the microwaves will induce currents in the case that will produce enough heat to start a pretty serious fire, and probably cause catastrophic failure in the battery. ",null,0,cdn3oke,1rfc8w,askscience,top_week,2
auralucario2,"From my limited experience in putting metal in microwaves, I think that the shell itself would begin sparking, due to the movement of electrons caused by the energy of the microwaves. As for the insides, it would probably escape direct harm from the microwaves, but the heat and electricity thrown off of the casing would probably do some serious damage.

Please don't try it though.",null,0,cdnv98v,1rfc8w,askscience,top_week,1
Manhigh,"The only mechanism for heat transfer from the space station is through radiation.  In general, all of the electrical components on a spacecraft and solar incidence (when in sunlight) produce excess heat which needs to be shed.  If you look at a picture of the space station you'll see a series of panels that are perpendicular to the solar panels.  While it is generally desirable that solar arrays always face the sun, it's generally desirable to have the radiators edge-on to the sun, facing deep space.

Coolant passing through the radiators is cooled and then passed back inside to keep removing heat from the station.  If you wanted to heat the station, you could have the radiators face the sun slightly.

In this photo, the radiators are the white  accordion-like structures:  http://milesobrien.files.wordpress.com/2010/08/iss1.jpg",null,0,cdmsvb7,1rfc91,askscience,top_week,6
steeeeve,"There's no 'up' and 'down'. However, your brain is somewhat accustomed to zero-g; it happens whenever we're falling. The fluid isn't really ""floating"" because to the best of my knowledge there's no air in the part of your ear that controls balance. Rather, there's hairs in the ear that will 'flex' under a current that is induced when you accelerate. ",null,0,cdmxa5d,1rfcei,askscience,top_week,1
jadiusatreu,"Great answers from the beekeepers. To add a little more information apart from honeybees, not all bees make a honeycomb. Bumblebees make honey pots in which they store their honey.  These bees make a cylindrical, sometimes round, pot. Just a little more information for you.",null,0,cdmtjth,1rfcsm,askscience,top_week,3
HCOOH,"There are so many wild-types of bees... they don't make nests.
And the ""normal"" honeybees make round shapes, but because of the melting of these round shapes thexy become hexagonal. The whole thing is more.. a succes through error",null,1,cdmp126,1rfcsm,askscience,top_week,2
proule,"Curiosity drives you to ask questions, which, in being answered, can improve your chances of surviving. This ingenuity is perhaps the most evolutionarily successful means of avoiding death due to outside influence. Other evolutionary tactics would include simply being bigger than anything that could otherwise hurt you.

In animals capable of higher learning, curiosity is fundamentally a desire to learn and understand the world you interact with. At the most basic level, curiosity is important to be able to accomplish the key tasks for each living being: Survive and produce offspring.
",null,0,cdnh79b,1rfd57,askscience,top_week,2
spryspring,"Curiosity is a behavior that has probably been selected for in some animals by natural selection, or at least has not been selected against. Suppose that a ""gene for curiosity"" (I'm sure in reality it's not nearly that simple) arose in the ancestors of cats. Proto-cats that had this gene tended to have more offspring than those who didn't (we might guess that they, in being more curious, found more food).

Or maybe it's totally a learned behavior, I don't know. But in any case that's how behaviors can arise.  ",null,2,cdn1stc,1rfd57,askscience,top_week,1
xenneract,"Sure. [You can hire a plant to do it for you.](https://en.wikipedia.org/wiki/Photosynthesis)

If that's not chemical enough for you, there is also active research in making [artificial photosynthetic cells](https://en.wikipedia.org/wiki/Artificial_photosynthesis) that perform the reaction you are describing.",null,0,cdmrnvi,1rfd6j,askscience,top_week,6
sodium_dodecyl,"We *can*, but it's not going to be terribly efficient (or necessarily fast, I don't have any kinects data). An example of a possible pathway: Reduce [Reduce CO2](http://en.wikipedia.org/wiki/Sabatier_reaction) to CH4 + H2O, then use electrolysis to split H2O --&gt; O2",null,0,cdmrezw,1rfd6j,askscience,top_week,1
steeeeve,"Yes, it is possible. However, the reason we make CO2 is because reacting carbon with oxygen to form CO2 bonds releases energy. The same amount of energy has to go into the bond to break it. Since power plants are not 100% efficient (and never can be) the re-separation will always cost more energy then we got from burining the fuel in the first place (assuming the fuel is almost all carbon, like in coal)",null,0,cdmxv3u,1rfd6j,askscience,top_week,1
kyaj21,"Technically, yes. CO2 is just 1 part carbon, 2 parts oxygen, as any school child who has taken introductory chemistry would be able to tell you. Yes, we could extract oxygen from the carbon dioxide, but the carbon would still be there. Reducing carbon emissions is a whole other matter, as in order to reduce carbon emissions, we would have to change the fuel sources or at the very minimum how we process them, and what we would do with the carbon once we extracted the oxygen from the carbon dioxide.",null,5,cdmrfta,1rfd6j,askscience,top_week,1
omgdonerkebab,"It's just a convex mirror.  The mirror is curved toward you, so that the rays of light that get to your eye come from a larger angle.  (Kind of like [this image](http://0.tqn.com/w/experts/Physics-1358/2009/06/Convex-Mirror.jpg), but with the directions of the arrows reversed.)  

This allows you to see a wider angle of stuff behind you, which has its obvious uses when driving.  But it also means that this larger angle is squished into a smaller area on the mirror, so the objects look smaller on the mirror.  Your brain might interpret it as the object being farther away, which would be wrong.  The object is closer than it appears to be.",null,1,cdmpcpr,1rfd8w,askscience,top_week,7
botanist2,"For the sake of reference [here's](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078092) the original article about these stone-tipped spears that you're asking about.  To clarify, they're not talking about aliens using spears, they're talking about different species of *Homo* (e.g., we're *Homo sapiens*, they could be talking about *Homo erectus*).  These spear tips were dated from the substrate in which they were found, they weren't dating the specific material of the spear (which could be much older). 

There's lots of ways to date different materials and the choice depends on what you're trying to test.  Carbon-14 is used predominately for organic materials, the article in question used [argon-argon](http://en.wikipedia.org/wiki/40Ar/39Ar) dating that is good for dating metamorphic and igneous minerals.   

&gt;When it comes to radio active decay, does it magically start over when you shape the object?

Not when it comes to minerals.  Radioisotope dating methods for minerals can only give you an idea of how long its been since the last time they cooled below their closure temperature (the temperature at which its assumed that isotopes aren't flowing in and out).  They tested the age of the substrate where the spear points were found because they wanted to know how long it had been since the points came to rest in that spot (and presumably when they were last used by their owner), not the age of the stones that were used to create the spear points.",null,0,cdmtews,1rfd9c,askscience,top_week,5
descabezado,"For radiometric dating in general, the clock starts once the object stops exchanging atoms with its surroundings.  For rocks, this means when the minerals of interest crystallized; for organic remains, it means when the creature died and stopped taking in air.  So, what they probably mean here is that the spear handle is made of wood that died 500000 years ago.  You are correct that dating the stone spear head would not be useful.

An interesting consequence of this is that you have to be very clear about what you've dated.  If you date pages of a book to be 2500 years old, it means the paper is that old, not the writing on it.  If you date a sedimentary rock to be 200 million years old using U-Pb dating with zircons, it means that the zircons were eroded out of 200 Ma old crystalline rock, but the timing of their erosion and deposition (i.e., the age of the sedimentary rock) could be any time between 200 Ma and yesterday.",null,0,cdmt83q,1rfd9c,askscience,top_week,3
tin_can_conspiracy,"Artifacts such as these are usually only dated by how deep they're buried (similar to how we date the dinosaurs) or by carbon dating artifacts found in the same site as the stone tools (wooden spear handles, bones, and such.)",null,1,cdmpbgx,1rfd9c,askscience,top_week,2
iorgfeflkd,"In [this](http://iopscience.iop.org/0143-0807/16/4/005) paper, they measured how likely it is for toast to land on the buttered side down, and found it was 62% (with thousands of tests), significantly more likely than random chance.",null,3,cdmozzv,1rfdo6,askscience,top_week,8
null,null,null,0,cdmubzf,1rfdo6,askscience,top_week,1
atomfullerene,"Murphy's Law is usually phrased ""Anything that can go wrong, will go wrong, and at the worst possible moment"".

It's meant to be taken tongue-in-cheek, it's not a physical law, but somewhere between a joke and a superstition.  If it was literally true, we'd all be dead. But it does have some level of validity, especially in the engineering context it was invented for.  Complex machines have lots of parts, and often only work right if _all_ the parts work together properly.  The probabilities that each part will fail get multiplied, making it more likely that something will go wrong.  And parts are more likely to fail under stress, which means while the machine is operating--often the worst time.  Eg, it's much worse if the wings fall off your test plane in the air than if it's sitting on the ground.  ",null,0,cdngwpa,1rfdo6,askscience,top_week,1
Jetamors,"We've known about cancers for a very long time. [The oldest known description is Case 45 from this Egyptian papyrus from 1600 BC](http://archive.nlm.nih.gov/proj/ttp/flash/smith/smith.html), though I don't think it theorized about the cause. There's a great article about old Greco-Roman treatments [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820670/). According to the article, the oldest known theory about cancer (written by Galen) attributed tumors to an accumulation of black bile, due to the black veins that appear around many tumors. Galen was working off the [four humors theory](http://en.wikipedia.org/wiki/Four_humors), which was predominant in Western medicine from antiquity to about the 1800s.

Edit: I should correct myself, Galen's theory is the oldest one in the *Greco-Roman tradition*. I don't know much about medicine in other cultures, but I wouldn't be surprised if they (China particularly, but probably others as well) theorized about the origin of tumors at about that time or earlier.",null,0,cdmt0zr,1rfdsx,askscience,top_week,13
Jetamors,"We've known about cancers for a very long time. [The oldest known description is Case 45 from this Egyptian papyrus from 1600 BC](http://archive.nlm.nih.gov/proj/ttp/flash/smith/smith.html), though I don't think it theorized about the cause. There's a great article about old Greco-Roman treatments [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820670/). According to the article, the oldest known theory about cancer (written by Galen) attributed tumors to an accumulation of black bile, due to the black veins that appear around many tumors. Galen was working off the [four humors theory](http://en.wikipedia.org/wiki/Four_humors), which was predominant in Western medicine from antiquity to about the 1800s.

Edit: I should correct myself, Galen's theory is the oldest one in the *Greco-Roman tradition*. I don't know much about medicine in other cultures, but I wouldn't be surprised if they (China particularly, but probably others as well) theorized about the origin of tumors at about that time or earlier.",null,0,cdmt0zr,1rfdsx,askscience,top_week,13
iorgfeflkd,"There's a way of approximating functions called a Taylor series, where you add up diminishing terms with higher and higher powers. For example, the cosine of x can be approximated as 1-x^2 /2 + x^4 /24 - x^6 /720 ...

The tangent is the ratio of the opposite and adjacent sides of a right triangle, and for a 45 degree angle the tangent is 1. 45 degrees in radians is Pi/4. This means that the inverse tangent of 1 is Pi/4.

That series for Pi is based on the Taylor series of the inverse tangent function, substituting x=1 so that it equals Pi/4 (x=1 greatly simplifies the math because 1^anything is 1).

So basically, it's another way of saying that the tangent of 45 degrees is 1.",null,0,cdmpzwi,1rfdy0,askscience,top_week,15
keithb,"You are short-sighted. I can tell becasue your glasses have ""negative"" lenses, which cause a beam of light passing through them to diverge, to spread out. You can see the light which has been diverted in the brighter halo around the shadow of your glasses. If you were long-sighted you would glasses with lenses which are ""positive"", or converging, and there would be a bright spot in the middle of the shadow of the lens rather than a bright rim. 

The soft shadow of the lens appears darker than the carpet around it because the light passing through the lens is spread out over a larger area. The lenses will absorb a little bit of the light passing through them, but mainly they redistribute the light. That's what lenses are for.",null,1,cdmqwmz,1rfepp,askscience,top_week,18
Infinite_Ambiguity,"If galaxies are close enough to start with (as in clustered together, relatively speaking), then there's sufficient gravitational force between them to bring them together and to overcome inflation/expansion.  

To use an extreme example, inflation/expansion doesn't tear the earth apart, or the solar system apart, our own galaxy apart, or any other individual galaxy because the gravitational fields win each such cluster is sufficient to keep everything together.  Same concept between galaxies that are relatively close together.  

Many cosmologies believe that, I. Billions of years, our night sky will be totally dark and telescopes will be insufficient to see anything, except for the galaxies in our own cluster (which, I think, total something like 36 total galaxies).  ",null,0,cdmovvo,1rff0z,askscience,top_week,5
DarkLather,"Galaxies exist in groups. Galaxies within the same group can be gravitationally bound to each other. They can orbit each other and collide. Our Milky Way and the Andromeda galaxy, both members of the ""Local Group"", are currently on a collision course. ",null,0,cdmqwqr,1rff0z,askscience,top_week,2
atomfullerene,"Height is  highly dependent on the amount and quality of food one receives as a child.  Poor farmers are often quite short.  People living in modern countries with plenty of food are taller.  Interestingly, skeletons of hunter gatherers before the dawn of agriculture were also often quite a bit taller than their immediate farmer descendants (though height does recover in the farmers somewhat over time) owing to the better nutrition of the hunter-gatherers as compared to the early farmers.

Farther back in prehistory early protohumans were often shorter than modern people.",null,0,cdmt3vc,1rfff0,askscience,top_week,12
dontgothatway123,"In a specific practical sense when actively measuring the cardiac output (CO) of a person it is important to factor in the persons size.  This makes the CO calculation more relative.  For instance the average CO for a adult male is 5.6L/min (the volume of blood ejected from the heart every minute).  Now we'll introduce two people. One man is 5'2"" (157cm) 105lbs (47kg) with a BSA of 1.44m^2.  The other is 6'6"" (198cm) 285lb (129kg) with a BSA of 2.67m^2.  If we just considered CO (stroke volume x heart rate) would it make intuitive sense that if both of these individuals had a CO of 5.6L/min that would be ok for both?  No, some form of individualization is necessary.  This is obtained by taking the CO and including the BSA into the calculation.  This measurement is called the cardiac index (CI).  Clinically/practically it serves a better purpose and indicator for monitoring hemodynamic states in controlled situations.  Using the examples above the first man would have a CI of 3.89L/min/m^2 and the second man would have a CI of 2.1L/min/m^2.  Considering the normal CI ranges from 2.6-4.2L/min/m^2 the man in the second scenario is about a hairs breadth away from cardiogenic shock.

Hopefully that helps shows the significance of BSA inclusion within a certain situation.  As for whether there are better alternative parameters I am unsure. In research you tend to see body measurement index (BMI), ideal body weight (IBW), lean muscle mass calculations, body fat percentage (BF%), and body surface area (BSA) measurements used a bit.  Each has their own benefits and pitfalls.",null,0,cdn0ahy,1rfg3b,askscience,top_week,2
Trill-Nye,"In this case, it's better to think of color as a result of light absorption and emission, rather than reflection. When light hits a gas, it can be absorbed by various processes. Visible light just happens to be the right energy to excite the electrons bound to atomic nuclei in some molecules, such as those making up chlorine gas. These excited electrons, which have been given energy by a photon, then relax to their original energies, giving off new photons of a particular wavelength (and therefore color).

Electrons are unusual in that, due to quantum effects, they can have only certain discrete energies. This is determined by the structure and composition of the atom, and its interactions with other nearby atoms. Gasses that are not colored do not have electron excitation mechanisms of the correct energy to be excited by visible light, then give off light of a specific color.

If a gas were black, it would have to absorb most incoming photons, then give off accumulated energy as something other than visible light, such as photons of a wavelength that cannot be observed by the human eye. ",null,0,cdmsy9a,1rfggp,askscience,top_week,4
AznInvasian,"In easier terms to understand:

     Light wave goes into gas atom, energizes an electron and pushes it to a higher energy orbital. The electron doesn't like this, and returns to its original orbital, emitting that same amount of energy it absorbed. This makes it glow this specific colour (corresponding to the wavelength of light it absorbed).",null,0,cdnddj3,1rfggp,askscience,top_week,2
CoryCA,"Only in that all life on Earth is related, and that they are both plants. (Though a mango stone reminds me more of a peach stone.)

A pumpkin is a squash variety of the species Cucurbita pepo of the family Cucurbitaceae. Acorn squash are also a C. pepo variety (the species is highly variable), zucchini are a different species in the same genus, and watermelons and cucumbers are part of the same family.

Mangos of genus Manifera of the family Anacardiaceae which also includes cashew, poison ivy, sumac and pistachio.",null,0,cdmrzhg,1rfgwc,askscience,top_week,2
proule,"There are many examples of ""convergent evolution"" in the world. That is, evolution that has caused very distantly related organisms to take on a similar appearance in some fashion. Another example of this is flying insects, birds and bats. Obviously you can see a large difference between insects, birds and bats, but birds and bats may seem like they're more related than not, right?

Birds are more closely related to reptiles than bats, which are mammals. If you look at the parts of their bodies specialized for flight, they *look* similar at a base level, but: A bird's wings are modified forelimbs (arms), and they still have distinct, separated feet. A bat's wings are a leathery extension of skin that stretches between the modified forelimbs, and actually reaches down to the legs. [Here's a picture to illustrate my point](http://upload.wikimedia.org/wikipedia/commons/3/38/Homology.jpg).

In biology, function is very tied to structure. Two structures can evolve to look very similar based on sharing the same end function, however, this does not necessarily imply a close relation.",null,0,cdnhdlw,1rfgwc,askscience,top_week,2
Weed_O_Whirler,"You would barely notice a difference. 

The main reason the magnet in the motor needs to keep being pushed isn't due to friction, but due to [Back EMF](http://en.wikipedia.org/wiki/Counter-electromotive_force) force. When spinning the magnet in the coil, a current is produced in the coil, and a counter-emf voltage opposes the current. These will always be of the same amount of energy- thus even without friction a magnet will very quickly slow down, as you will not be able to extract more energy from the magnet than you used to get it spinning in the first place. 

It is good to think of how these generators are not ""making"" energy, they just ""convert"" it. So, we burn stuff in order to move pistons, the moving piston spins a magnet, and the moving magnet makes electricity. Even without any friction or losses in the burning process, you'll never get more energy out of the generator than you put in. ",null,0,cdmqz7c,1rfhiw,askscience,top_week,7
florinandrei,"Any battery has an internal resistance. Any resistance, when a current passes through it, it heats up. Therefore, a battery will heat up (or at least become a bit warmer) any time you either charge it or discharge it.

The higher temperature of a charging battery is not an indicator of it charging, it merely indicates that some current is passing through it. But same would happen during discharge.",null,1,cdmrmjz,1rfi8e,askscience,top_week,4
Guanglais_disciple,"The chemical reaction is endothermic and then exothermic (li-ion for example) but the joule heating (current ^ 2 * resistance) usually dominates. Since joule heating isn't a function of current direction, you see heating in both cases. For very low current, though, the chemical reaction dominates and it cools slightly. ",null,1,cdmtg6y,1rfi8e,askscience,top_week,4
dudds4,"It would be interesting if that was the case, but no. It'll help to understand why there is heat produced.

Basically the transfer of energy into the battery is not perfectly efficient. some energy is lost. Where does it go though, ( energy can not be destroyed) ? Heat, among other things, is the answer. 

Imagine a flowing stream of water. Some of the water laps up on either side of the stream, and gets absorbed by the land. Not all ( although nearly all) of the water makes it down the stream. Here the water getting absorbed is what we observe as heat


Same goes for discharging energy, it's just another form of energy transfer, and not perfectly efficient

",null,0,cdn4tzd,1rfi8e,askscience,top_week,2
polkasalad,"On discharge the batteries heat up due to the internal resistance.  Internal resistance increases as temperature decreases as well, which is why batteries last longer near room temperature, so if it were to cool the battery would actually lose capacity faster as you used it. Consequently, heating up the battery too much will damage the cell.  

I'm sure someone can offer more info, I'm in a graduate course relating batteries to hybrid-electric cars right now which is where I got my info from. ",null,2,cdmpz1e,1rfi8e,askscience,top_week,2
botanist2,"III&gt;Trees and plants existed millions of years before the first oxygen producing creatures

Photosynthetic organisms (mostly cyanobacteria that form [stromatolites](http://en.wikipedia.org/wiki/Stromatolites)) existed millions of years before the first oxygen producing creatures, but trees and plants as we know them today didn't evolve until much, much later.  

As to the rest of your question, there are a lot of other ways to make CO2 than just living organisms and one of the most likely sources of CO2 was volcanic activity.",null,2,cdmq5zo,1rfia5,askscience,top_week,10
sparky_1966,"I think you meant before the first oxygen consuming creatures. Trees and other plants weren't around for a long time after the start of making oxygen. The first photosynthetic organisms were single celled. When photosynthesis started, the atmosphere was thought to be a reducing atmosphere, so the excess oxygen taken up by iron and made rust, and there was a lot of methane that UV light made into CO2. Carbon was not necessarily limiting, since all the carbonate (limestone) had yet to form, and the oxygen comes from splitting water molecules. The oxygen cycle today is not necessarily the oxygen cycle at the beginning. ",null,1,cdms085,1rfia5,askscience,top_week,2
foamster,"Well, volcanic activity alone 'produces' a *lot* of atmospheric CO2. 

My understanding was that the atmosphere had very little oxygen initially, but plenty of CO2 at around the time that photosynthesis began to take off. Animal life wasn't really able to develop until the atmospheric oxygen concentration was high enough to allow for sufficient metabolic rates -- oxygen produced almost exclusively by algal photosynthesis.",null,1,cdmscd2,1rfia5,askscience,top_week,1
iorgfeflkd,"It's an amorphous solid, which basically means that it behaves like a solid (as most people would interpret them) but the atoms aren't arranged in a crystal lattice. This makes a difference if you try to measure heat transfer through the material, for example, or look at the diffraction of x-rays through it.

[Diagram](http://www.steelguru.com/uploads/reports/sss1-29-08-2008.jpg)",null,0,cdmq2n8,1rfih8,askscience,top_week,11
gfpumpkins,This isn't really an answerable question. The normal bacteria found in humans is incredibly unlikely to be pathogenic to ants. ,null,0,cdmr746,1rfix6,askscience,top_week,3
proule,"Your question seems to be based around the assumption that humans are bigger than ants, thus, our bacteria must somehow be stronger than the bacteria that colonize ant bodies.

There's no fundamental difference between bacteria that colonize ant bodies and those that colonize human bodies. Human bacteria don't need to be ""stronger"" to colonize humans; they're adapted to colonize humans just as bacteria in ants are adapted to colonize ants.",null,0,cdnh2jd,1rfix6,askscience,top_week,2
Osymandius,"You're right - there are lots of ways to kill bacteria. Antibiotics are selective ""weapons"" against bacteria which is why they're so important. Because they're specific to bacterial components, they're safe to give to patients without destroying their own cells.

Let's take another example of a way to kill bacteria: heat. Most bacteria give up at about 50/60^o C, some thermostable bacteria (see T. aquaticus) are good for a bit more - up to 85/90^o C. Yes - all antibiotic resistant bacteria will be killed by a 100oC burst, but then you've got the put the patient through that! 

Take any method that will kill bacteria that isn't antibiotics, and it'll probably do some damage to the host. Irradiation, particulate disruption, salt membrane disruption, electrostatic membrane disruption, intense dehydration etc.",null,1,cdmradq,1rfk3g,askscience,top_week,10
thedveeeee,"There's actually only a fairly limited number of ways to kill bacteria. To list a few, you can kill them through targeting protein synthesis, targeting DNA replication, and using cell wall synthesis/growth inhibitors. Some newer antibiotics are being produced that target ATP synthase, an enzyme that produces ATP for the bacteria.

Unfortunately, the specificity in these antibiotics lies in the fact that we can't administer compounds that are toxic to human cells. Many antibiotics (like methicillin) are mildly toxic to us so they must be modified. That being said, it takes years and millions of dollars to come up with solutions to these problems. 

Edit: To touch on antibiotic resistance, and this is a very simple explanation; when bacteria are exposed to sublethal doses of antibiotic, selective pressure can cause a change in their genome, in which the most advantageous traits are passed on. This leads to strains of bacteria that are resistant to antibiotics, and these bacteria can pass their advantageous genes on to other bacteria. You may have heard of the incredibly famous MRSA group of bacteria; Methicillin Resistant Staph Aureus. This is a strain of Staph aureus (a natural flora of bacteria found on your skin; it's very common) that has evolved to resistant methicillin antibiotics. ",null,13,cdmsn6g,1rfk3g,askscience,top_week,21
justin3003,"The big problem is that there are only so many ways to attack bacteria effectively. Many of our antibiotics center on attacking replication or protein synthesis, two areas of significant difference between humans and bacteria. This makes most modern antibiotics much less toxic to humans than they are to bacteria. Also, some bacteria are totally resistant to many antibiotics simply by their biology (ie. the drug cannot interact with it, etc.), limiting the available options to only a few drugs.

Unfortunately, because we only have these limited points of difference, antibiotic use over time tends to lend itself to the selection of bacteria that are not able to be killed by these mechanisms. As these elements become more resistant, we have more and more limited options to further address this problem. It is further compounded by the fact that antibiotics are not specific to the pathogen you are trying to treat; to eliminate one infectious pathogen you bathe all of the other bacteria in your body with the same drug. Thus you don't just drive resistance of pathogenic bacteria but also harmless bacteria in your body that, under the right circumstances, may become harmful. 

So, to get to your question, that is why we are terribly worried about antibiotic resistance. Bacteria are a constant presence in the environment and evolving faster than we can create effective, tolerable treatments.",null,10,cdmsg9e,1rfk3g,askscience,top_week,12
fazedx,"The most difficult part of drug design and discovery is to kill the thing you want to kill without harming ""healthy"" cells in the body. Most anti-bacterials are beta-lacatam antibiotics. That means they work by interfering with the building of the cell wall of bacteria. To put it simply, it disrupts penicillin binding proteins that are necessary for cross-linking of bacterial cell walls (kind of like the mortar in brick and mortar - without the mortar, the wall would not hold). Without the ability to reconstruct and expand cell walls, bacteria cannot grow or reproduce.

beta-lactam antibiotics work because they have similar structure to the penicillin binding proteins, but do not actually hold cell walls together. The bacteria use these to make their cell walls, but because they don't hold, the cell wall breaks down. It's kind of like giving a bricklayer sand instead of mortar to build a house. 

Some bacteria can produce beta-lactamase, which cleaves the beta-lactam ring and renders it ineffective. ",null,0,cdmv9no,1rfk3g,askscience,top_week,2
foamerc,"The short answer is there are many ways to kill bacteria, but few that discriminate between bacterial and human cells. Bacteria are cells too, and  they share many similarities with human cells, and a few differences here and there. Antibiotics exploit such differences such as bacteria have a cell wall and human cells don't. 

When discussing about killing them after they've infected someone within the body, you're pretty much left with antibiotics, which there are many subtypes working in different manners, but for all intents and purposes are chemicals ingested/injected into a human for the purpose of killing specific bacteria.

In addition you don't want to indiscriminately kill off all bacteria because that's how you select for resistant organisms, kill off normal helpful bacteria, and some nasty ones grow in their place. Look up C. difficile infections - a relatively new cure is to eat processed shit of other people.",null,0,cdo6tkv,1rfk3g,askscience,top_week,1
iorgfeflkd,Its engine was cut off a long time ago. It is on a trajectory that takes it beyond the solar system.,null,2,cdmr7xs,1rfk5l,askscience,top_week,30
Gprime5,"I think you might have misinterpreted something because your description doesn't make sense.

Voyager 1 doesn't have any actual engines, only small thrusters that keep it pointed towards Earth. The craft is in a hyperbolic trajectory meaning it has enough velocity travelling away from the sun that it will never come back.",null,1,cdmrakq,1rfk5l,askscience,top_week,10
PorchPhysics,"http://www.jaymaron.com/asteroid/tour-l.jpg

As the others said, its on a hyperbolic path out of the solar system.  This means its its velocity is greater than or equal to the escape velocity required for the sun.  

As for your idea that ""we're always moving around something"" is not really true at all, but in the case of voyager, it is now and interstellar probe, no longer orbiting our sun or being considered part of our solar system, it now orbits the galactic center.",null,2,cdms1nn,1rfk5l,askscience,top_week,10
Ocaiman,"No, plants cannot survive without oxygen.  They respire on O2 just like any other living thing.  O2 is a byproduct of energy production using photosynthesis and plants eventually give off more O2 than they take in to breath.

To your question, a plant needs oxygen to germinate and grow until it begins photosynthesis.  They do not store O2, thus they cannot live in an atmosphere of CO2 or they would suffocate (the O2 would diffuse into the environment).  They can live in a clear sealed container with access to light, as they continuously reuse the CO2 and O2 that was sealed in with them, but they reach a limit in growth.",null,0,cdmsibb,1rfkeu,askscience,top_week,5
chrisbaird,"Not enough to notice. You can test whether gravity has any noticeable effect easily. Pluck a guitar string whole holding it upside down and see it sounds any different from when plucking it upright. The relevant force for these instruments is the tension in the strings and drum membranes, which is enough stronger than gravity that you can ignore gravitational effects. 

Note you only asked about lack of gravity. I am assuming you mean there is still normal air pressure provided by a pressurized compartment. If there were no air, or lower air pressure, than that would definitely effect sound propagation. ",null,0,cdmt7yy,1rfkjv,askscience,top_week,3
lvachon,"An acoustic guitar has been on the ISS for a while.  According to Cmdr Hadfield, the only thing that required changing was his play style since he no longer had the weight of his arm to help move down the fret board.

Source : http://www.youtube.com/watch?feature=player_detailpage&amp;v=gWTndmDHZQc#t=59",null,0,cdn9no2,1rfkjv,askscience,top_week,1
openLIKEeuchromatin,"The WHY part of the question:
First think of it in terms of fitness (this is always a good idea when navigating through these types of organismal biology questions). The number one goal for life from a biological perspective is to reproduce and pass on your alleles. With that in mind, try to think of why these birds have all grouped together and are ""chatting"" away. Keeping fitness in mind (#1 goal in life is to reproduce) you know that the grouping and social communication behavior of these birds must be important in order pass on their alleles. Since these behaviors (phenotype) are important to the survival of the crow species, then they must have evolved via natural selection.

The HOW part of the question:
Birds calls have evolved for millions of years acted on by natural selection. The chirps, coos and shrieks you hear everyday are a product of that. Many birds have developed a communication system that allows them to recognize individual calls within that population. Much like humans can tell the difference between each others voices. Look at it from the birds perspective. Birds have very sensitive ears and a respiratory system with many airways that allows them to make the complex calls. Try not to fall into the ""anthropomorphism trap"". A large crowd of crowing birds in the eyes of a crow is very different than in the eyes of a human. A ""noisy crowd conversation"" from a humans perspective is loud and hard to decipher what an individual is saying (i.e. sporting events, concerts, etc.). This is not the case for birds. Some birds are able to pick up on some of the slightest changes in frequency to hear exactly who is calling and what the call is about (i.e. food, mate, predators, etc.). Many birds have a critical period during development where they learn specific calls usually unique to that  population. Some bird call are even genetically ""hardwired"" and do not require learning. Not all birds are social though and communication does vary from species. 

Last point:
Calling and crowing is not the only way birds communicate. A wide range of behavioral displays are used in junction with the calls in order to send a complete message to another bird (the receiver). 
",null,0,cdn0pf1,1rfkwr,askscience,top_week,3
Doener_wa,"I can tell you something about the Langmuir isotherm. To get to this equation you have some asumptions to make: first is you have an isotherm system, which means your temperature is constant and second your gas which will be adsorbed formes a mono-layer on your surface (there are equations which involve multi-layer adsorption). Therefor you get the coverage of your surface and your Langmuir-isotherme can describe how much you may adsorb until your surface is fully covered. Also Langmuir isotherms are used to describe how well an adsorber adsorbs a specific gas or a mixture of gases (all will adrob differently). This is very useful because you are now able to characterize reactions which are done using a (heterogene) catalyst or to cunstruct an adsorber like it is used in many chromatographie-applications. 
To your Freundlich isotherm: I think this must be a similar concept just using other asumptions.
I don't want to go in detail now, if you have further questions, just ask on.
Source: I am a graduated chemical engineer and I am currently visiting a lecture about adsorption.",null,0,cdniulx,1rflnd,askscience,top_week,1
creepy_old_grampa,"Police Radar is tied to their speedometer and decremented from the total, Source, I used to convert old cop cars to taxis, and I could always find the speedometer signal wire spliced under the dashboard already when I went to put in a meter.",null,0,cdmttof,1rfltg,askscience,top_week,6
EpicEvslarg,"So a car is travelling at 100 km/h North

A police car is travelling at 100 km/h South

The relative velocity would be 200 km/h

So the police radar would either know what speed the police car is going at, and automatically calculate the velocity of the other car, or the policeman would have to do it in his head by looking at the radar, and his speedometer. 

In this example the radar would either say 100 km/h or 200 km/h, so it would be easy to calculate.

I hope I solved your question.",null,0,cdmsce3,1rfltg,askscience,top_week,3
Dyolf_Knip,"That's just a matter of subtracting out their own velocity.  The real problem is angles.  If you were traveling at 100 mph perpendicular to the beam of the radar gun, it would register your speed at basically zero, because it only measures relative velocity along the path of the beam.  Any deviation from that decreases the measured speed.  So what cops do is position themselves as much as possible such that are directly in the path of incoming traffic.  I.e., right at a bend in the road, on an overpass, etc.

Area radar systems get around that limitation by being smart.  The radio beam can't really tell you how fast something is going, but it can tell you where it is.  An attached computer says ""5 seconds ago it was there, now it's here, x miles away, ergo it's moving this fast"".",null,0,cdnjjys,1rfltg,askscience,top_week,1
steeeeve,"If you had a rigid bottle, a difference in pressure would build up as you travel further into the ocean. This difference in pressure will cause greater net forces on the water at the mouth of the bottle, causing it to enter the bottle more rapidly. The amount that this happens will depend on some complex fluid dynamics, as the air needs to leave the bottle as well as the water entering it. 

If the bottle were pressurized with air so that the pressure was at equilibrium between the inside and outside of the bottle at the bottom of the ocean, then only the difference in buoyancy will cause the bottle to fill, similar to the case of a few feet of water. In this case, the amount of time would be similar for both cases, though perhaps not exactly the same due to changes in the viscosity of the water and air at those pressures.

The collapse of an air-filled bottle would depend on what kind of bottle is being used. For a typical soda bottle, the bottle can be collapsed just by sucking the air out of it (say, using your lungs). This means that a pressure difference of less than one atmosphere will cause the bottle to begin to crumple. The pressure increases with depth at ~1atm/10m of depth, so the bottle would crumple long before reaching the bottom of the ocean.",null,0,cdmxqyd,1rfo1d,askscience,top_week,3
chrisbaird,"You seem to be confusing length (m) with acceleration (m/s^2) which are different things. If gravitational acceleration is very small, that does not imply there is anything in the system with a small length scale. It just means the gravity is very weak. Quite the opposite case is more important actually: quantum effects and gravitational effects should intersect when there is a large amount of gravity in a very small volume (such as in a black hole).",null,0,cdmt1ks,1rfo90,askscience,top_week,1
Surf_Science,"Everything in your cell is doing what is thermodynamically favourable. Proteins involved in transcription bind to a gene because that binding is favourable, they function because that is energetically favourable. The produced proteins bind each other causing actions that occur because those are also energetically favourable. ",null,0,cdms0zn,1rfooz,askscience,top_week,3
sparky_1966,"DNA alone can't determine what a cell does, you can think of it as storing information. That information can be turned into RNA, some of which regulates genes, proteins and a few specific reactions. The proteins handle most of the actual work.

So, in the simples example if a bacteria that can use multiple sugar types for energy is sitting in an environment that has no lactose sugar, it usually doesn't waste energy making enzymes to break it down. If suddenly lactose becomes available, a receptor protein can bind the lactose and either activate transcription of lactose digesting enzyme from the DNA, or more commonly in bacteria, change shape and fall off the DNA, allowing the gene to be expressed. As the enzymes break down all the lactose, eventually the receptor protein wont have any to bind to, and will switch shape again to turn off the gene. There are many other levels of regulation, but that's the simplest example.

As far as viruses, there are a number of different strategies they use to take over a cell. Almost never is it just a piece of naked DNA floating around, since the environment and cells are full of enzymes to destroy those fragments. Probably the easiest system to understand is a DNA virus with a protein coat. The protein coat protects the DNA, but also makes sure it gets delivered. The protein is usually shaped to bind to the bacteria or cell it infects. On binding, the proteins change shape and make a path through the cell membrane for the DNA to get in. There is energy stored in the shape of the protein and the winding of the DNA (taken from the last cell) that allows injection of the DNA without other sources of energy. Once in the cell, the DNA gets replicated and transcribed in to viral proteins and more viruses like any other DNA. That's the simple version, there are any number of different virus types, some use DNA, some RNA, some large viruses carry most of the proteins they need to begin replicating so they can shut down most of the hosts protein production, etc.  ",null,1,cdmsir9,1rfooz,askscience,top_week,1
darksingularity1,"Technically it's not DNA. That determines what a cell does. Think of it as a master blueprint for a house. It contains a great idea, but it's not actually contributing to the building of the house. The workers (proteins) are who/what do everything. The architect might be the direct liaison to the blueprint. He reads it and converts it into instructions for a worker function. Technically new workers are created in the analogy sense, but I'm sure you get what I mean. The proteins are what actually create changes in the cell. In fact, certain proteins even act on DNA to control the expression of other proteins. The DNA does nothing.",null,0,cdndkh6,1rfooz,askscience,top_week,1
aziridine86,"Wikipedia is an OK place to start, but I believe that the most basic answer to the 'why' question is this:

The hydrocarbons that we get from the earth come in a huge variety from gases like methane, ethane, and propane, all the way to thick waxes and tars. 

Because of the prevalence of internal combustion engines used in cars, trucks, planes, etc., we have a much higher demand for gasoline, diesel, and jet fuel than for other hydrocarbons which are heavier or lighter. 

Cracking is one way we can turn less desirable hydrocarbons like high-boiling petroleum into more desirable products such as those used in gasoline. 

If your talking specifically about using kerosene as the feed stock, then the products will contain larger amounts of small (C2-C5) products. For example, [this](http://pubs.acs.org/doi/abs/10.1021/i200024a026?journalCode=iepdaw) paper (full text not free) says that cracking of kerosene yielded significant amounts of ethene, propene, butene, and butadiene. 

These chemicals have many different uses, but a major use of this class of chemical (often called olefins) is to make plastics like polyethylene and polypropylene. ",null,0,cdnc9mz,1rfpcs,askscience,top_week,3
sf_torquatus,"The products of catalytic cracking are smaller hydrocarbons. The catalyst (usually a strong acid zeolite or precious metal) cleaves the C-C bond. You will find a distribution of products corresponding to the temperature, pressure, and catalyst. Kerosene itself is a product of catalytic cracking. One would want to crack it further to produce smaller hydrocarbons. 

Regarding the ""why"" - I'm a bit sketchier on these details, so I'd ask anyone with a better understanding to pitch in. Kerosene is used as jet fuel, and I found a patent that described cracking kerosene to yield gas-phase products, but I don't understand the advantages of such a process versus fuel injection, unless such a process improved the injection in some way.",null,0,cdmu5xz,1rfpcs,askscience,top_week,2
hikaruzero,"It's pretty simple -- photons alone aren't the cause of attraction/repulsion.  It is the *fields themselves* that cause charged objects to attract or repel eachother.  Photons are created when charges accelerate, but if you have a bunch of stationary charges and no actual photons, those charges will still begin to accelerate and attract or repel eachother without emitting or absorbing any photons amongst themselves.

In the context of perturbative theories, this effect can be explained by saying that the vacuum is filled with virtual photons, and that the virtual photons end up exerting a force on the stationary charges.  But virtual photons are not detectable the way real photons are, and also virtual particles do not appear in non-perturbative treatments of electrodynamics, so it is something of a matter of debate whether they even exist at all (indeed in the theory of the strong force, perturbative calculations frequently end up being *wrong*).  Virtual particles can be thought of as simply a mathematical tool for calculating approximate answers -- it's best to just say it is the *fields* that cause charges to accelerate.

Now, real photons themselves are *disturbances* of the fields, and if the fields change, that will cause a change to the acceleration of a charged object, so real (detectable) photons *do* accelerate charged objects, but strictly speaking it is the field that is ""doing the work,"" the presence of photons isn't necessary for attraction and repulsion -- it's not like there have to be a bunch of photons flying around from one particle to the next in order for charged objects to accelerate (though if they are flying around and being absorbed or emitted, they will change how those charged objects are moving via transfers of momentum).

So it's the fields that do the acceleration, whether you want to interpret fields as being made up of virtual particles is something of a matter of philosophy, and not something that experiment can tell us is definitely true or false.

Hope that helps.  Some further (but more technical) reading:  [Wikipedia:  Static forces and virtual-particle exchange](http://en.wikipedia.org/wiki/Static_forces_and_virtual-particle_exchange) and [Wikipedia: Force carrier (particle and field viewpoints)](http://en.wikipedia.org/wiki/Force_carriers#Particle_and_field_viewpoints)",null,0,cdmsq9j,1rfqqd,askscience,top_week,5
siliconlife,"Actually what you suggest does happen, but it's not called subduction because continental crust is too buoyant to descend into the mantle like cold ocean crust.

The Himalayan orogeny actually is so intense that a process called [underplating](http://www.sciencemag.org/content/325/5946/1371/F2.large.jpg) actually takes place. Underplating is the positioning of crust or magma beneath an overriding crust. In the case of the Himalayas, the Indian continental crust is being thrust so strongly that it ends up completely beneath the Eurasian crust. [Link to paper](http://www.sciencemag.org/content/325/5946/1371.abstract)",null,0,cdmvwch,1rfs95,askscience,top_week,6
oloshan,"The Indian plate is indeed being subducted under the Eurasian plate. The Himalayas are the uplift of Eurasian crust, not Indian crust - although their elevation is certainly enhanced by the effects of having the Indian plate shoved beneath the Eurasian at the same time. But not only was the Indian plate subducted, the speed of the collision may have actually driven it deeper than typically subducted plates (probably meaning that it has had less time to melt since being subducted, and so can still be discerned beneath the Eurasian plate).

In addition, a fair amount of lighter continental sediments were essentially ""scraped off"" onto Eurasia by the collision, during the initial phases when the Tethys Sea closed. A similar process happened along the Pacific plate margins as well, and has contributed to the formation of Alaska, Japan, and other ""accreted"" terranes.",null,0,cdo7rsl,1rfs95,askscience,top_week,2
fastparticles,"The Himalayas are being lifted at least in part by this collision, however we do not have a specific mechanism worked out for it. The difficulty with this collision is that this is a continent on continent collision, and both are very buoyant. When you think of a traditional subduction zone you have oceanic crust hitting continental crust, and the oceanic crust is denser and so it sinks. In this case both are continental crust so there is little/no density contrast and India can't just sink.",null,2,cdmvdf1,1rfs95,askscience,top_week,3
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,top_week,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,top_week,5
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,top_week,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,top_week,5
Das_Mime,"&gt;I recall reading something along the lines of observing the orbit of any natural satellite of the object, but a more detailed explanation would be nice. 

If you can see a natural satellite of the object and you can reasonably assume the satellite to have much much lower mass than the planet*, then you can use mechanics to work out the host's mass. In this case I'll also assume a circular orbit, but you can also work out the mass from non-circular orbits.

The force of the planet's gravity on the moon is **F = G m*_p_* m*_m_* / r^(2)** where G is the gravitational constant, r is the orbital radius, and the m's are the masses of planet and moon. In the case of  circular motion, the force on the moon is equal to **F = m*_m_* v^(2) / r** where v is the orbital velocity of the moon. Set these forces equal to each other, and you get:

**G m*_p_* m*_m_* / r^(2) = m*_m_* v^(2) / r**

Canceling out common factors, you get

**m*_p_*  = v^(2) r / G** 

So if you know the distance of the planet and it has a moon (which for Solar System objects can be readily obtained via parallax methods), then you can directly calculate the planet's mass. 

Calculating the mass of a body without natural satellites is a bit more work. Prior to the Space Age, Venus and Mercury's masses were not well constrained, because the best way to measure mass is to measure its gravitational effect on other objects. Venus also exerts a gravitational influence on other planets such as the Earth, and so if you have sufficiently accurate position measurements of both bodies and if you know Earth's mass then you can calculate Venus's mass, but this is still not an ideal method.

Our best measurements of Venus' mass come primarily from spacecraft like the Mariners 2, 5, &amp; 10 (American) and Venera (Soviet) probes sent to Venus. [From analyzing their trajectories](http://adsabs.harvard.edu/full/1968AJS....73R.162A)--some of them were flybys, some were orbiters (e.g. Soviet Venera 15 &amp; 16, American Magellan and ESA *Venus Express*), and some have landed on the surface--you can determine Venus' mass to a high level of accuracy, but in the end this is essentially the same method as the first-- measuring the planet's tug on nearby objects. 

Finally, you can make guesses at the composition of Venus, and since its radius is easily measured with a telescope, you can get an estimate of its mass. This is much less accurate, of course, since it depend entirely on the accuracy of your guess about the composition. 

*true for all Solar System planet/moon pairs except the dwarf planet Pluto and its moon Charon, which is about 12% of Pluto's mass
",null,0,cdmyrqu,1rfuon,askscience,top_week,3
Ejb90,"Even for the simplified case of a planet-star system there are a few ways to find the mass of a planet. I'll describe a common one, [Astrometry](http://en.wikipedia.org/wiki/Astrometry).

From observations we can usually deduce the distance of the star from earth, the ""apparent magnitude"" (how bright it is to us) and the spectral class - what types of elements it's made up of by looking at the light received.
We can also find the period of orbit of the planet around the star by several methods - the transit method is most popular, though the Doppler shift method and others are used dependent on the circumstances).
The next part is the difficult part. The velocity of the star wobbling backwards and forwards must be measured. 
The planet doesn't actually orbit a stationary star - they both orbit their combined centre of gravity, though for the star, which is much more massive, this is relatively close to its centre of gravity. Hence the star itself wobbles backwards and forwards. This speed can be measured from earth via the doppler effect - the light when the star is shifting towards us is shifted slightly up in frequency and then when it is moving away gets shifted down slightly. This can be used to calculate the speed.

From the distance and apparent magnitude we can calculate the ""absolute magnitude"" - how bright it it from a standard distance (30ly IIRC). Using these and some hefty thermodynamics/ fluid mechanics/ stellar structure knowledge (or the simplified [mass-luminosity relation](http://en.wikipedia.org/wiki/Mass%E2%80%93luminosity_relation) or extrapolating roughly from the [Hertzsprung-Russell diagram](http://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram)) to find the mass of the star.
Also from the period of or it we can use Kepler's third law to find the radius of orbit.
Now, the speed of the body can be calculated as the distance it travels and the time it takes is known, and the speed and the mass of the star are known.
Finally these can be used with the conservation of momentum to calculate the mass of the planet.

This technique has several issues. Firstly, this only gives a lower limit, as the orbit may not be head on, so the star may be moving faster than expected. Also, some of the measurements needed aren't possible in some cases. Also, it must be noted that Kepler's laws and the mass determination of the star isn't exact. Finally the issue of having more than one body in the system. Because there are a large amount of bodies in the system, the equations aren't analytically solvable, so there is some error in the determination process.

The mass of moons we can observe is calculated much in the same way, using the planet as the main mass.
I'm not sure what you read about observing moons of planets. This certainly isn't possible with exoplanets - we're only just on the verge of being able to see the very biggest exoplanets as off this year.
",null,0,cdmxscf,1rfuon,askscience,top_week,2
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,5,cdmv2r3,1rfwje,askscience,top_week,43
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,4,cdn6q1y,1rfwje,askscience,top_week,15
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,5,cdmv2r3,1rfwje,askscience,top_week,43
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,4,cdn6q1y,1rfwje,askscience,top_week,15
do_od,"Buoyancy is a force acting on a body as to oppose gravity when that body is immersed in a fluid. This force is proportional to the weight of the volume of fluid displaced. In zero gravity, the fluid has no weight and there is no direction in which buoyancy could act. Buoyancy requires gravity... or more generally a reference frame under acceleration. Example: If you spin a bucket of water on a string in outer space, a ball could be buoyant in the water. That would not be useful for propulsion though. ",null,0,cdmv0go,1rfxwf,askscience,top_week,10
blacksheep998,"Here's a good video answering your question. http://www.youtube.com/watch?v=bgC-ocnTTto

In it an astronaut places an alka-seltzer tablet into a spherical water drop. Without gravity the only major force affecting the bubbles is surface tension, which causes most of the bubbles to combine with each other and eventually form one large bubble in the middle of the water sphere.

There's also this video, http://www.youtube.com/watch?v=QPf5MJluhvo in which an astronaut injects an air bubble into a water sphere, and then injects small water droplets into the bubble.",null,0,cdn17rs,1rfxwf,askscience,top_week,4
AltoidNerd,"It's puffy.  If highly energetic, roughly spherical.  

You can get fireworks to discharge in predetermined shapes by the way you pack the explosives.   By analogy, the shape of a space flame would depend likewise on the shape of the source,  

Spherical enough of course to feel good about 4/3 π r^2 in a physics calculation.",null,0,cdn9gny,1rg4lz,askscience,top_week,2
Nicked777,"The fire will indeed be spherical, this has actually been tried in Space before, it looks pretty cool (I'm on my phone so I won't link it.)

The flame changes colour because the lack of convection causes diffusion to be the dominant transport mechanism. Compared to a terrestrial flame this means the flame burns with more complete combustion, with less soot. (Glowing hot soot is the reason most terrestrial flames are yellow.)

Edit: More information here: http://carambola.usc.edu/research/microgravity.html
",null,0,cdn9k0r,1rg4lz,askscience,top_week,1
therationalpi,"Basically a whistle is a resonator. You either have a Helmholtz resonator (like a beer bottle) or a standing wave resonator (like an organ pipe).

Driving the resonator is the variable airflow through the whistle. In most whistles you will have a hole with a blade shaped edge on it. When the edge is blown on, it creates turbulent airflow in the form of vortexes. These vortexes alternate from side to side in what is called a ""vortex street."" There's a good picture of that [here.](http://www.grc.nasa.gov/WWW/Acoustics/code/adpac/sample/CYLINDER_VORTEX_SHEDDING/) The alternating vortexes create a varying positive and negative acoustic pressure, setting up a wave in the resonator. The resonator, as a result, forces the frequency of the vortexes to align with the whistle's natural frequency. In this way the whistle amplifies the normally irregular vortex variations into a sound loud enough to be heard at a distance.

The reason you must blow at the correct angle is that the flow vortexes will depend greatly on how the moving air stream hits the blade. You must hit the wedge shaped part of the whistle fast enough to create unstable flow, otherwise the wave will not be generated.

Hope that helps!",null,0,cdmz9jk,1rg4zj,askscience,top_week,1
BoxAMu,"The energy of a photon is proportional to frequency, but this energy must match the energy of some transition in the absorbing matter.  The electrons in bonds in glass have transitions in the UV, but not the visible range.

This is the same reason why high energy X-rays are used for imaging: muscle and tissue are mostly transparent to X-rays, while the calcium in bones absorbs them.",null,0,cdmy00a,1rg4zy,askscience,top_week,4
uberhobo,There is no such thing as relative humidity above the boiling point of water.  It will all stay a gas in any proportion with air.,null,0,cdnb8t8,1rg6ug,askscience,top_week,3
whatsup4,it depends if there is something for the water to condense on. Basically think of it like cloud formation. Air high in the atmosphere can sometimes be super saturated and achieve higher than 100% rh because it is hard for the water to form droplets without a surface to form on. Given a large enough decrease in temperature you can see cloud formation.,null,1,cdnaiz3,1rg6ug,askscience,top_week,1
Merrilin,"Anything with a temperature (a.k.a. all matter) is constantly emitting **blackbody radiation**. 

You can think of temperature of an object as being proportional to how much each constituent atom vibrates. The more intense it's vibration, the hotter it is. The short of it is that this vibration causes the release of a photon, which carries with it some energy from the atom, decreasing it's temperature. More on that if I ever get home. 

It so happens that the hotter something is, the higher frequency radiation, on average, it emits. That's why a piece of metal visibly glows when you make it very hot. At room temperature it is emitting light at a range of frequencies, but almost none in the visible light range. As you make the piece of metal hotter, it's blackbody radiation in the visible light range becomes significant enough that a human eye can detect it. 

So, no, matter cannot have temperature without also emitting some frequency of light. And there is no such thing as matter without temperature, so matter is always emitting light. ",null,0,cdmy126,1rg6wj,askscience,top_week,9
thumbs55,"Excellent question.

First of all what is heat and what is temperature? Are they not the same thing?

[Heat](http://en.wikipedia.org/wiki/Heat) is a measure of thermal energy (measurable in joules like all energies), it can be a measure of the ammount of (highly disordere heat typed energy) energy moving from one body to another.

[Temperature](http://en.wikipedia.org/wiki/Temperature) is a measure of the hottness or coldness of a body, two bodies with the same temp will not exchange any net heat and if one body is hotter than the other then the hotter will give energy to the colder in the form of heat.

&gt;everything I can think of that has heat also has light. Stars, lightbulbs, lava, fire, hot metal,

This type of light is called [black body radiation](http://en.wikipedia.org/wiki/Black-body_radiation).

&gt;Metal only emits light after it heats up past a certain temperature.

While it is true that the light becomes visible after a certain temperature is reached, the metal is actually always emitting invisible ""light"" (electromagnetic radiation) at any finite temperature due to said black body radiation.

All of space has [Cosmic microwave background radiation](http://en.wikipedia.org/wiki/Cosmic_microwave_background) which gives ""empty"" space a temperature. (It is not empty of [real] particles if you include the photons giving it said temperature).

The temperature of space (away from stars and such) is around 3 kelvin, so if you have something hotter than that it in space will get colder and if you have something colder than that it will actually warm up.

The heat energy is often stored in the energy of the jiggling of molecules. But for it to move from one place to another it mainly moves in the form of photons (light) but also phonons (sound). If you engineered a particularly exotic system that only exchanged energy in phonons then this system would emit heat with no light.

Nutrenos are also an example of a form of heat (they carry energy from the sun in a manner that is not work) nutrenos are not light. But many forms of nutreno generation would also produce photons.",null,1,cdmyhyr,1rg6wj,askscience,top_week,5
AltoidNerd,How about your hands.,null,1,cdn9ibx,1rg6wj,askscience,top_week,3
Chuk,"Metal only emits light after it heats up past a certain temperature. It can get very hot but still not be glowing. (That is, assuming you are only thinking of visible light.) Living creatures also emit heat without light, as do many other chemical reactions.",null,7,cdmxi6u,1rg6wj,askscience,top_week,2
CosmicWaffle5,"It's called positional alcohol nystagmus. Basically, there are these things in your ears called semicircular canals that are responsible for your sense of balance. The semicircular canals are supported inside of a fluid that is usually the same density as the semicircular canals, but when you drink alcohol it changes the density of the fluid surrounding the membranes and throws your balance system out of walk. 

http://en.m.wikipedia.org/wiki/Positional_alcohol_nystagmus",null,0,cdn7xz4,1rg8rs,askscience,top_week,7
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,top_week,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,top_week,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,top_week,2
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,top_week,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,top_week,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,top_week,2
iorgfeflkd,If the mother and father were half-siblings.,null,3,cdmycqc,1rganp,askscience,top_week,8
ohheytherewhatsup,"No. Crossover events during Meiosis 1 are required to generate tension in the meiotic apparatus.  Without crossover, division will not occur, and crossovers cause mixing of chromosomes from the grandparents.  Each of your chromosomes is a chimera of your two grandparents DNA.",null,1,cdn9ihk,1rganp,askscience,top_week,5
laika84,"Although this would not add up to 50%, the child of a mother with Down's syndrome, (men are essentially infertile and women with DS can have a child but they are less fertile than those without DS,) there would be a 50% chance that the child receives the extra chromosome.

Since this chromosome resulted from a non-disjunction event in one of the grandparents, the child would have more than 25% of his/her genetic material from one grandparent.  Again, not 50%, but still interesting.",null,1,cdn6qe0,1rganp,askscience,top_week,3
null,null,null,0,cdn18w5,1rganp,askscience,top_week,1
iorgfeflkd,"They're not actually instantaneous, they're just treating them that way because it's much simpler to do so in an intro to physics class. Real objects are made of compressible materials, and when they collide the objects deform.",null,0,cdmy3fx,1rgap2,askscience,top_week,4
cylon37,"Let's be clear here. Two events that are simultaneous in one frame of reference may not necessarily be simultaneous in another frame ONLY if the two events are separated by some distance. Conversely, if two simultaneous events happen at the same point in space, they are simultaneous in all frames of reference. A collision as described above is a single point in space-time. The two 'events' that you describe, A transferring momentum to B and B transferring momentum to A happen at the same location and are therefore simultaneous in all frames of reference.",null,0,cdn0i76,1rgap2,askscience,top_week,4
musubk,"Contrary to the other answers, the length of the day decreases at a near constant rate for most of the year. It isn't a sine wave, people! It looks more like a triangle wave with the points lopped off and rounded off. It superficially looks like a sine wave if you view it for lower latitudes because the amplitude is too small to see the shape, but try it for somewhere further north. Fairbanks, AK is a good choice. I just wrote a quick IDL routine to read the daylight hours tables the USNO website gives for a chosen latitude, [here it is for Fairbanks (65 North)](http://i.imgur.com/YMvVGf5.png).

And if you go even further north, like 85 degrees, [you get something silly like this](http://i.imgur.com/bHWfVqK.png).

The point being that the days don't start shortening at a slower rate as you would think for coming over the edge of a sine wave, the rate that days are shortening is actually constant over the majority of the year for a majority of the planet. This is still true at lower latitudes, and if you scale the graphs right you can see that:

[50 degrees latitude](http://i.imgur.com/na9TE3D.png)

[35 degrees latitude](http://i.imgur.com/8BLfwwu.png)

[20 degrees latitude](http://i.imgur.com/PrqgIpq.png)

[5 degrees latitude](http://i.imgur.com/OLRDXDP.png)",null,0,cdnbz0h,1rgcbc,askscience,top_week,6
iamtheonewhotokes,"As we approach Dec. 21 the days will shorten at a slower and slower rate. Similarly as you approach the summer solstice in June days will get longer at a slower rate the closer you get. And as you approach an equinox (in March or Sept.), the rate increases. 

See chart here: http://cycletourist.com/Miscellany/Length_of_day.html (the slope of the curve is the rate)",null,3,cdn12gy,1rgcbc,askscience,top_week,6
iorgfeflkd,"Nothing particularly interesting would happen. Light by itself isn't affected by temperature, and if the light is passing through a vacuum then temperature isn't a meaningful quantity. Depending on the medium that the passes through, its temperature can have effects on how the light absorbs it. For example, in an extremely cold dilute gas it is possible for the atoms to absorb light and stay in that configuration for a brief period of time, so the light is in effect trapped. This is the temperature's effect on the medium, however, not the temperature's effect on the light.",null,0,cdmyy7i,1rgcdh,askscience,top_week,4
stuthulhu,"Another thing to consider, even if the photons *could* be frozen you would not see your display freeze as though stuck in time. You would simply not see your display, since the photons responsible for creating that image are no longer able to *move* to your eye. 

An easy way to simulate what a room would look like if all the photons became frozen in space is to put a box over your head. ",null,0,cdndsia,1rgcdh,askscience,top_week,2
Das_Mime,"Light won't stop moving, even if it's going through a medium which is at absolute zero.

Temperature is about the thermal motion of particles which have mass, like electrons. The colder you get, the less kinetic energy they have. But light has no mass and its energy is proportional to frequency, so it usually doesn't make a great deal of sense to talk about light having a temperature in the same way that a physical material does (although a spectrum of light can certainly have a characteristic black body temperature, lower energy light doesn't travel any slower than high energy light).

Light, on the other hand, is comprised of massless photons. If they're passing through a medium (like water or air), then they will go somewhat slower than the speed of light in a vacuum. This change in speed can be affected (slightly) by the temperature of the medium, which is why you see effects like heat shimmers. Light can't stop moving, although [certain materials can slow it down to extremely slow speeds](http://www.news.harvard.edu/gazette/1999/02.18/light.html).",null,0,cdmz3om,1rgcdh,askscience,top_week,2
Das_Mime,"The idea is that another star's gravity will tug the comet farther out at first, and then when the star passes (or just when the comet continues on its newly more-elliptical orbit), the comet falls back inward.

It should be noted that the Oort Cloud is very poorly understood, not really directly detected, and is basically used as an explanation for long-period comets. Most comets are on highly elliptical orbits, so even if several of them are perturbed by the same star, their orbits will be altered in different ways. Even if multiple comets are sent into the inner solar system in this way, they might arrive years or centuries apart.",null,0,cdn86xc,1rgeiw,askscience,top_week,3
Dyolf_Knip,"East takes you out, out takes you west, **west takes you in**, in takes you east.

The bold one is relevant here.  The star does attract the comet, but does so in a way to slow its velocity relative to the sun.  After the star passes by, the comet assumes an orbit suitable to its new velocity, which means it drops into the inner solar system.",null,0,cdne36w,1rgeiw,askscience,top_week,1
neverdonebefore,"There is a bit more to it than that.  

In FWD cars, the front wheels are doing both the steering and applying the engine torque to the road.  And RWD, the rear wheels are only applying that torque to the road.  Essentially, your fwd cars are 'pulling' while rwd are 'pushing'.  

As you drive down a straight road, you are applying longitudinal force to the road to propel you forward.  As you enter a curve in the road, you add a rotational component to your travel.  The center of mass of the vehicle has to move laterally through the curve, while the vehicle itself has to rotate about that center of mass in order to be pointed straight as you exit the curve.  With a fwd vehicle, the direction of the force applied to the road by the tires changes as you turn your steering wheel, and the back wheels will follow in that path. Fwd vehicles have a tendency to understeer.  An object in motion wants to stay in motion: the inertia of the car in the longitudinal direction makes it want to keep going straight.  The tires want to follow the path on which they are pointed.  If the lateral acceleration into the curve cannot overcome the forward inertia, the car will understeer, or take a path with a larger radius than the curve.  In a rwd vehicle, the the tendency is to oversteer, or turn at a smaller radius than the curve.  This is because the force on the road by the rear wheels is along the path of the vehicles inertia.  The front wheels will want to follow their path around the curve, but the rear wheels will want to keep going straight.  This means it is easier to rotate about the center of mass.  This is how fish tailing and drifting (and spin outs) occur.
",null,1,cdn5t9m,1rgew2,askscience,top_week,8
wwarnout,"In a rear-drive car, when you accelerate, the center of gravity shifts toward the rear.  So, if the only consideration was getting good traction during acceleration, this would be the preferable configuration.

However, since most cars have engines in the front, a front-drive car will have better traction is slippery conditions because more of the weight is over the front wheels.",null,1,cdn0w9f,1rgew2,askscience,top_week,4
AltoidNerd,Take a shopping cart at the grocery store and compare pushing and pulling the cart.  This especially is useful if the back wheels of the cart don't rotate (in analogy to the car).,null,1,cdn9faf,1rgew2,askscience,top_week,2
5secondstozerotime,"I do not think the rocket is directly launching into Geostationary orbit. Rather, it is going into a geostationary transfer orbit (GTO) that will then allow it to go into a geostationary orbit.

I cannot find a reason why this needs a window, however what you are saying about the rocket is wrong. 

[This article talks exstensivly about it](http://www.americaspace.com/?p=45686).",null,0,cdn8uw4,1rgf3r,askscience,top_week,4
Nicked777,"A little known consequence of orbital mechanics is that you must be directly under an orbital path to launch into it. SpaceX do not launch from the equator, so they cannot go straight into GEO, they have to start with a transfer orbit, and then do a plane change somewhere. They can only launch into their transfer orbit when this orbital path is directly overhead, which means waiting for the earth to rotate Cape Canaveral into the right spot, thus the launch window troubles. ",null,0,cdn973l,1rgf3r,askscience,top_week,3
ecopoesis,"Metrics such as temperature describe the behavior of a system that is made up of components.  These types of properties are termed emergent properties because they are derived from the behavior of the system as a whole and are not observable if you were to look only at the components.

So, for your specific question, individual molecules do not have temperature.  They are not ""hot"" or ""cool"" exactly, although they do have energy that is zipping them around their surroundings.  Molecules with more energy will move faster and collide with other matter more frequently and with more force.  It is only when you begin to look at a system of molecules that ideas such as temperature start to be meaningful.  In that sense, a group of molecules with a certain amount of energy will correspond to a certain temperature.  If these molecules are ""hotter"" than other molecules, then they will be moving about much more rapidly and they will be less dense than the latter group of molecules.  Properties such as temperature and density are emergent from the system of molecules interacting with each other and interacting with their surroundings.",null,1,cdn5v46,1rgf8v,askscience,top_week,7
The_Evil_Within,"&gt;an area of hot air becomes less dense, and so it rises above colder areas of air. 

First, you need to look at it the other way around - hot air doesn't rise, cold air sinks.  As it sinks, it forces the hotter air upwards.

Now, think of a mess (and I do mean 'mess' for the imagery, not 'mass') of cold air, with the molecules fairly still and fairly dense.  Then, something heats up a bit of it near the bottom - what's going to happen?

The molecules of hot air will bounce around a lot more than the cold, and sometimes they're going to bounce up.  When they do, the less active cold air is more likely to fall into the gap than to move in another direction, and now there's nowhere for that hot air molecule to go because it will only bounce off the cold air molecule if it bounces downward again. (Transferring some heat in the process, but we can ignore that for the purposes of this explanation)

Multiply this by unimaginable numbers of interactions, and you end up with a column of hot air rising while all the cold air around it rushes in to fill the gap at the bottom.",null,4,cdn6q0h,1rgf8v,askscience,top_week,8
AltoidNerd,"&gt;&gt;But what if there was a single constituent molecule from that wood existing in the water? Would it float? It is neither densely nor sparsely aggregated, existing all by itself.

My reaction to this is no not really - a single molecule would have dynamical behavior that isn't familiar like the bouyant force example you gave.  I have no idea how to describe what that situation *would* be like - but I'm positive it would be invalid to treat it like a whole plank of wood floating.",null,1,cdn9dzb,1rgf8v,askscience,top_week,5
ramk13,"Wanted to add that at the scale of single molecules, static interactions are much more important than buoyant forces. A single molecule of wood will dissolve and behave like another molecule in solution. Even a few molecules of wood together will still be influenced by the hydrogen bonding between water and its external oxygen groups more than the buoyant force on the particle as a whole. All of this applies to your wooden plank example.

To answer your question: And if so, why do they act like an aggregated 'body' with those molecules around them, just because they are at the same temperature?

It's because even in air at atmospheric pressure molecules have a limited mean free path. In air it's [68 nanometers](http://en.wikipedia.org/wiki/Mean_free_path#Mean_free_path_in_kinetic_theory). That is that an oxygen or nitrogen molecule only travels so far before it hits another molecule and they bounce off each other. The molecules collide often enough that they influence each other over that short length. That influence leads to aggregate properties, as each collision redistributes kinetic energy.",null,0,cdnt7nx,1rgf8v,askscience,top_week,1
dirtpirate,"You seem to be misunderstanding the interaction. Comic book guy is asking for a very high number X, and mister Burns is retorting to Smithers ""Give hime Y"", where Y is much smaller than X. Thus a typical haggling scenario. 

The joke isn't that the two numbers are the same, just that instead of comic book guy saying ""I want a billion"", and Burns replying ""I'll give you a million"", they are instead using physical constants. ",null,0,cdnbei2,1rgi0o,askscience,top_week,7
iorgfeflkd,"The Faraday constant is the charge of a mole of electrons or protons, measured in Coulombs. Avogadro's number is 6x10^23 and a Coulomb is 6.2x10^19 fundamental charges, and the ratio is 96485 Coulombs per mole.",null,3,cdn0eae,1rgi0o,askscience,top_week,7
MonadicTraversal,"&gt; Any faster or slower, closer or farther, or difference in direction of travel and the body would de-orbit, spiralling toward the planet or star it's orbiting or flinging off into outer space.

This isn't true. If you smacked a huge meteor into the Earth, you wouldn't knock it into the sun, you'd just change the shape of its orbit a bit. Spiral orbits don't actually exist under inverse-square forces such as gravity; you can show that the only possible orbits are circles, ellipses, parabolas, and hyperbolas. Spiral orbits don't exist except if there's some kind of drag force or whatever dissipating energy from the system; on an interplanetary scale drag doesn't matter. (Note that this is somewhat complicated by the fact that, e.g., Jupiter affects the orbit of the Earth, but in general the perturbations due to planet-planet interactions are small enough to not matter for stability purposes).

&gt; Isn't it difficult for us to keep our own man-made satellites in a stable orbit, requiring periodic adjustments? And yet, the moon is huge and it seems to be in an orbit that will last billions of years with no intervention.

Many man-made satellites are orbiting at an altitude where Earth's atmosphere can still exert some small amount of drag. The moon is so far away from Earth that the drag is essentially negligible. We also want the satellites to be kept in a *predictable* orbit; for a geosynchronous satellite, we want that orbit to be such that it's always above the same spot on the equator. The moon doesn't 'have' to be in any particular orbit, it just orbits wherever it orbits.",null,0,cdn0pyg,1rgi2m,askscience,top_week,9
iorgfeflkd,"For a circular orbit it has to be that precise, but many more initial configurations will lead to stable elliptical orbits, which are stable due to a balance of gravity and angular momentum. We live in a universe where the force of gravity decays with the square of distance, which is related to the fact that we live in three spatial dimensions. It turns out, there are only two types of forces that can produce stable orbits: inverse square, and linear (harmonic, like a spring). So, basically, we live in a universe where stable orbits can exist. Because of that, the fact that we do see stable orbits is not surprising.",null,0,cdn0bqk,1rgi2m,askscience,top_week,5
iorgfeflkd,"It's not changing its constant, it's just changing your units. If you use meter-kilogram-seconds unit then hbar is something like 10^-34 m^2 kg /s but if you use Planck units then hbar is 1, G is 1, and c is 1, and you can measure lengths in terms of (hbar G/c^3 )^(1/2), for example. This makes it easier to do theoretical work because you don't have to keep track of all these constants, but you'll have to do more work to get your results in measurable quantities.",null,0,cdn34jk,1rgmvd,askscience,top_week,14
LoyalSol,"The curve itself, not really.  The function, definitely. There are so many uses it is hard to list them all. ",null,0,cdnph1q,1rgo2l,askscience,top_week,1
Platypuskeeper,"There cannot be such a thing as a 'non-cohesive liquid'. A liquid is by definition a state where the attraction between the molecules is strong enough that the thermal energy is insufficient to let most of them leave the liquid. But unlike a solid, the molecules are still able to move about. 

If you have no intermolecular forces, you have a gas. 
",null,0,cdn6cb2,1rgslj,askscience,top_week,3
iorgfeflkd,"Yes, it's both. Just being still in a gravitational field (like we are now, on the Earth) causes time dilation relative to freefall, and orbiting satellites have to take both into account (this is the famous GPS relativity correction).",null,0,cdn3rku,1rgt18,askscience,top_week,4
mingy,"I think you are right, but it is a minor error that probably got by the editors. I once read the final draft of a textbook written by a renowned expert in optics (long story) and found several errors (mostly units and arithmetic) and I knew maybe 1% of what the author had forgotten. He was grateful nonetheless.

In any event, even the 10 billion bits are wrong. The base pairs are grouped into 3s so you have 64 permutations, however this is not a binary or quaternary system, there are redundant codons and start and stop (http://en.wikipedia.org/wiki/DNA_codon_table) so there are 22 symbols of the 64 permutations.",null,0,cdn4k62,1rgu88,askscience,top_week,4
selfification,"Yeah that was a mistake in a way.  Each nucleotide base pair carries 2 bits of info.  So 5 billion base pairs carries 10 billion bits of info.

But there is the flip side that you have 2 separate sequences.  Each base pair is 2 codons.  Now you can consider that just 1 letter (because one of them precisely specifies the other) but I guess one could consider them 2 separate letters.  I mean...  2 copies of a file have twice the number of bits, even if the *information content* hasn't increased.  So in that interpretation, each base pair contains 4 bits of info...   and that would make Sagan's calculation make sense.",null,0,cdn4nl4,1rgu88,askscience,top_week,2
iorgfeflkd,"Each base-4 base can represent 00, 01, 10, or 11. So there is 4 times as much information as just binary.",null,4,cdn3qce,1rgu88,askscience,top_week,1
Physics_Cat,"In order:

Technically, yes. But the technical definition of temperature isn't what you think it is. More on that in a moment. 

Absolute zero is exactly the same as zero Kelvin. 

Who told you that the temperature of a black hole is absolute zero? That's certainly not correct. In fact, it's not possible for any matter to be at a temperature of exactly zero kelvin, due to the zero-point motion inherent in quantum mechanics. We can get incredibly close in a laboratory (somewhere in the range of hundreds of picoKelvin) but it's not possible to attain exactly zero kelvin. 

As for negative temperature: the colloquial understanding of temperature is something like ""temperature is the average kinetic energy of the constituent particles in a material."" That's a very useful tool for intuitively understanding things like heat capacity, but it's not the ""real"" definition. In thermodynamics, temperature is defined as the partial derivative of internal energy with respect to entropy (not sure how to format that symbolically, so I won't try). There are some kinda-convoluted, not-entirely-realistic examples of physical scenarios with negative temperature. That is, you add a bit of energy to the system, and the entropy goes down. For example, suppose you have N light switches, and each ""quanta"" of energy is represented as turning on one light switch. The entropy of a system is related to the number of configurations (microstates) that lead to the same macroscopic result, so let's say that you have N-1 switches turned on. Then the entropy is, more or less, N (since there are N ways to have N-1 switches turned on in a collection on N switches). Now you add one ""quanta"" of energy and turn on the last light switch. How many microstates are there now? Only one. There's exactly one way to have N out of N light switches turned on. Since we added a unit of energy and saw the entropy decrease, the system could be said to have ""negative temperature"" if you like. There are physical systems that come close to this analogy, but I think the ""light bulb scenario"" is easier to digest.",null,2,cdn51ib,1rgv22,askscience,top_week,9
fishify,"Absolute zero and 0 K are the same temperature.

Negative absolute temperatures are actually *hotter* than any possible positive temperature.  When you look at the mathematics, at any positive temperature, more energetic states are less likely to be populated than less energetic states (though at higher temperatures, the difference between those likelihoods is not as large as at lower temperatures); what you find is that if you had negative absolute temperature is that it would correspond to a situation in which more energetic states were *more* likely to be populated than less energetic ones.  (Lasers are a place where you might see such population inversions.)

Black holes have positive temperature, inversely proportional to their mass.",null,0,cdn54pg,1rgv22,askscience,top_week,4
auralucario2,"First, the statement about black holes is completely false.

Now, according to the law of thermodynamics, it is impossible to reach absolute zero, which is the same as zero kelvin. However, quantum mechanics butts its head in here and offers a workaround (kind of). It would be theoretically possible to achieve a temperature of some negative kelvin by having particles achieve a quantum state in which their entropy actually *decreases* as energy is added to the system. Needless to say, this doesn't exactly happen all the time, but it is possible.",null,0,cdnvd3b,1rgv22,askscience,top_week,1
brickses,"Zero kelvin and absolute zero are the same thing. There is no such thing as negative temperatures except in advanced thermodynamics exams.

Black holes are actually hotter than zero kelvin, like all warm things, they radiate (the same way humans radiate in infrared).",null,3,cdn4wu1,1rgv22,askscience,top_week,2
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,3,cdn5cpe,1rgx9w,askscience,top_week,10
RedBeard17,"Tetanus would be a combination of both, really.  A muscle twitch itself consists of the action potential (AP) (from the neuron, stimulating the synaptic cleft), and then the release of Ca2+ from the lateral sacs.  However, the length of an AP is much, much shorter than the length of a single muscle twitch, which is what allows us to sum the twitches, to increase strength, blah blah blah.

When you get to tetanus, the stimulation of the muscle cell is due to a high frequency of AP's coming from the motor neuron.  Because the muscle twitch takes longer (and you've got a continuous, and very high release of Ca from the sarcoplasmic reticulum), you get this prolonged opening of Ca channels, prolonged release of Ca, and thus you get a tetanic contraction.  

So, to summarize what I've tried to say (I ran on a little bit there):

1. Summation occurs because you get a higher frequency of stimulation of the muscles.  This high frequency stimulation causes a lot of Ach release at the NMJ, and a lot of Ca released in the cell to cause the muscles to contract.

2. Tetanus occurs when you've got such high frequencies, and so much Ach release at the NMJ, that the muscle doesn't have anywhere near enough time to relax, that it's constantly contracting, and therefore you have tetanus.
2a. It's not normally because Ach is TRAPPED in the NMJ, it's usually because it's being very quickly released from the high frequency of AP's coming down the motor neuron.

3. Ach is the only one going to be acting on the postsynaptic membrane.  Ca is only involved with troponin/tropomyosin inside of the cell.

4. Ach is going to bind to non-specific ligand gated channels on the End Plate (the ""top"" portion of the muscle sarcolemma), and while Ca is going to enter in through those channels to depolarize the membrane to create your end plate potential.

Does that make any sense?  Let me know if that helps.",null,0,cdpc92e,1rgx9w,askscience,top_week,2
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,3,cdn5cpe,1rgx9w,askscience,top_week,10
RedBeard17,"Tetanus would be a combination of both, really.  A muscle twitch itself consists of the action potential (AP) (from the neuron, stimulating the synaptic cleft), and then the release of Ca2+ from the lateral sacs.  However, the length of an AP is much, much shorter than the length of a single muscle twitch, which is what allows us to sum the twitches, to increase strength, blah blah blah.

When you get to tetanus, the stimulation of the muscle cell is due to a high frequency of AP's coming from the motor neuron.  Because the muscle twitch takes longer (and you've got a continuous, and very high release of Ca from the sarcoplasmic reticulum), you get this prolonged opening of Ca channels, prolonged release of Ca, and thus you get a tetanic contraction.  

So, to summarize what I've tried to say (I ran on a little bit there):

1. Summation occurs because you get a higher frequency of stimulation of the muscles.  This high frequency stimulation causes a lot of Ach release at the NMJ, and a lot of Ca released in the cell to cause the muscles to contract.

2. Tetanus occurs when you've got such high frequencies, and so much Ach release at the NMJ, that the muscle doesn't have anywhere near enough time to relax, that it's constantly contracting, and therefore you have tetanus.
2a. It's not normally because Ach is TRAPPED in the NMJ, it's usually because it's being very quickly released from the high frequency of AP's coming down the motor neuron.

3. Ach is the only one going to be acting on the postsynaptic membrane.  Ca is only involved with troponin/tropomyosin inside of the cell.

4. Ach is going to bind to non-specific ligand gated channels on the End Plate (the ""top"" portion of the muscle sarcolemma), and while Ca is going to enter in through those channels to depolarize the membrane to create your end plate potential.

Does that make any sense?  Let me know if that helps.",null,0,cdpc92e,1rgx9w,askscience,top_week,2
Dominus_,"When you're wiring your home surround system, no, pretty much not at all. But over long distances like on a concert where some cables run several tenths of meters, sometimes even a hundred meters, the resistance and interference has to be reduced, or else you're going to end up with artifacts and noise. ",null,7,cdnacxc,1rgzbv,askscience,top_week,46
thegreatgazoo,"For just about anything in your house, lamp cord is an excellent choice of speaker wire. Just make sure one side is marked so you keep the polarity correct. 

Anjou Pear speaker wires (and anything similar) are for delusional people who have too much money. 

",null,1,cdnckf0,1rgzbv,askscience,top_week,7
littlegreenalien,"yes.. and no. It's not so much the cable that's the problem, rather the interference it can pick up on the way. The longer the cable the more issues come into play (cable resistance, etc… as mentioned already). But at short cable distances it's mostly interference from power cables, and what not.",null,0,cdnb62y,1rgzbv,askscience,top_week,5
Kriemore,"Computer engineer here: wires are important, but if your question is 'should I spend $90 to get these cables I found at best buy for my home theatre?' Then probably not.

A bad cable will degrade audio quality significantly in addition to causing all manner of other problems with cutting out etc. 

Of course, expensive audio cables were famously compared to a coat hanger with no noticeable difference.


Now, if you're playing a massive theatre... these things start to matter a lot more.",null,0,cdngnf0,1rgzbv,askscience,top_week,4
jgrun,Ultimately there is always a degradation of signal quality when transmitting over a long distance. But since a digital signal is just binary 0s and 1s and you're only sending it 4 or 6 feet to the TV or stereo it doesn't matter. The receiving end will read the signal very clearly because it's hard to mistake a 1 for a 0.,null,7,cdn7wlm,1rgzbv,askscience,top_week,9
thisispointlessshit,"Not really. I just like getting cables that don't feel cheap... If that makes sense. The wire itself tends to wear over time if it has cheap shielding when I'm constantly coiling and uncoiling. Something with decent shielding usually lasts longer for me. For home use it might not be as much of an issue, because it's plugged in and never really moved.

In terms of sound quality? No difference.",null,1,cdng5vb,1rgzbv,askscience,top_week,3
lucaxx85,"You need to distingush three applications: 
1)analog signals in home setups
2)digital signals in home setups
3) live concert signals and similars...

For 1) everything works. Including coat hangers (for the power signals. You need shielding for line ones). The resistence and the impedence of such cables are such that they cannot affect in any way the final signal, which has a very low bandwidth. Only thing to be careful is to have cables large enough for the amp-speaker connection, if you have a very high power system (but I'd guess that you can still forget about this in any practical situation). 

2) Digital signals are more complicated. The bandwidth here is much higher, especially if you're also carrying video. That's why you have maximum lenghts and building them needs lots of care. Still almost any commercial cable is good if you're not trying to do something you shouldn't (e.g.: a 10 meters HDMI connection). In these case of course a 15'000 $ cable made from the finest rhodesian zinc, soldered in a full moon night by an african zoroastrian priest would work as badly as the cheapest one in the store.

3) For concerts and other applications cables can give actual problems. Still not those ""lamented"" by audiophiles. The first thing you look for in a concert cable is the *mechanical* resistance, especially of the connectors. Most of the cables break for a mechanical injury! Those things get torn everywhere. 
Then there is a problem with microphones/guitars signals. They're *extremely* weak. So they're sensitive to interferences. But, like before, those cables that claim to feature platinum in their alloy or even to have a special cristalline structure that favours the signal in a specific direction (how on earth would that work??!?!??!!) won't make *any* difference.   There are other tricks to solve the problem (balanced signals, preamplification before long transmissions etc...)

So you actually need a lot of care and you have a number of problems... But they're so not what the audiophiles claim!",null,0,cdni471,1rgzbv,askscience,top_week,3
Cyanmonkey,"I find the build of the cable more important than impedance rating, etc.

A properly built cable with Neutrik connectors and strain relief lasts much longer than your cheap Guitar Center POS, but as far as signal goes, as long as your not going over 200' it doesn't make a noticable difference.",null,1,cdnfrd9,1rgzbv,askscience,top_week,1
EvilHom3r,"For digital (i.e. HDMI), no it does not matter at all. Digital either works or doesn't, there is no in between.

For analog (RCA, speaker wire, TRS wires), you will always get better quality (even if just slightly) with a better wire. However for the average user they will probably never notice the difference, and more likely than not the quality bottleneck is elsewhere in the system.",null,5,cdncqhj,1rgzbv,askscience,top_week,3
Dyson201,"Not exactly an Audio Engineer, but this isn't a difficult question from a signals standpoint.

Transferring signals through a medium (cable) can pose a variety of challenges that are handled in many different ways.  Without going into extreme detail lets just say that electromagnetic forces could possibly come into play, as well as capacitance to ground producing noise in the circuit, etc. etc.

Long story short, if you're replacing a 3' cable for sound, I highly doubt you'll notice a huge dip in quality between $100 cables and coat hangers.  Both are capable of transferring the signal, and while the expensive cable will transfer the signal with a much greater Signal to Noise ration (SNR), at 3' and with modern noise abatement technology, you would be hard pressed to hear a difference.

Now that being said, I wouldn't wire up your home surround system with soldered together coat hangers, as distance plays a huge factor in the quality of the transmitted signal.  Also, if you buy a cheap ass sound system, expect to hear a big difference in quality between expensive and poor cables, even at 3'.  

Finally, audio quality sound is a very low frequency, and does not travel well over distances with a good SNR.  Quality cables are the only way to increase sound quality over distances (relative term, we're talking meters here not miles).  Technology has come a long way towards discerning the signal from the noise, but any reduction in noise is a huge positive in the quality of the signal.",null,11,cdn6sul,1rgzbv,askscience,top_week,6
generalelectrix,"This is a very vague question.

For digital signals, yes this does matter.  Digital audio signals require significantly higher bandwidth and run at higher frequencies than the audio content they encode, so transmission line effects become important.  If the characteristic impedance of the cable you use to transmit a digital signal is not matched to the source and destination, you can get partial reflections or standing waves on the cable, which can definitely cause errors in the reconstructed signal at the destination.  This becomes more important with longer cables.

For analog signals, the frequency is low enough that the characteristic impedance isn't really important.  So long as the conductors you're using are low-resistance (copper is great), coat hangers should work just as well as fancy cable.  Shielding in cables is important for line-level interconnects to prevent the cables from picking up noise from the environment, though this usually isn't too big of a problem in a home environment.

The only real exception to this is for speaker cables (carrying post-amplifier level signals) for electrostatic speakers, as the load they present to the driving amplifier is largely capacative.  Then the details of the impedance of the cable driving the speaker become a bit more important.

I'm a physics PhD in quantum electronics with a minor hi-fi addiction.

Edit: I give an in-depth and accurate answer and get a ton of downvotes?  SCIENCE!",null,19,cdn89ly,1rgzbv,askscience,top_week,5
Dominus_,"When you're wiring your home surround system, no, pretty much not at all. But over long distances like on a concert where some cables run several tenths of meters, sometimes even a hundred meters, the resistance and interference has to be reduced, or else you're going to end up with artifacts and noise. ",null,7,cdnacxc,1rgzbv,askscience,top_week,46
thegreatgazoo,"For just about anything in your house, lamp cord is an excellent choice of speaker wire. Just make sure one side is marked so you keep the polarity correct. 

Anjou Pear speaker wires (and anything similar) are for delusional people who have too much money. 

",null,1,cdnckf0,1rgzbv,askscience,top_week,7
littlegreenalien,"yes.. and no. It's not so much the cable that's the problem, rather the interference it can pick up on the way. The longer the cable the more issues come into play (cable resistance, etc… as mentioned already). But at short cable distances it's mostly interference from power cables, and what not.",null,0,cdnb62y,1rgzbv,askscience,top_week,5
Kriemore,"Computer engineer here: wires are important, but if your question is 'should I spend $90 to get these cables I found at best buy for my home theatre?' Then probably not.

A bad cable will degrade audio quality significantly in addition to causing all manner of other problems with cutting out etc. 

Of course, expensive audio cables were famously compared to a coat hanger with no noticeable difference.


Now, if you're playing a massive theatre... these things start to matter a lot more.",null,0,cdngnf0,1rgzbv,askscience,top_week,4
jgrun,Ultimately there is always a degradation of signal quality when transmitting over a long distance. But since a digital signal is just binary 0s and 1s and you're only sending it 4 or 6 feet to the TV or stereo it doesn't matter. The receiving end will read the signal very clearly because it's hard to mistake a 1 for a 0.,null,7,cdn7wlm,1rgzbv,askscience,top_week,9
thisispointlessshit,"Not really. I just like getting cables that don't feel cheap... If that makes sense. The wire itself tends to wear over time if it has cheap shielding when I'm constantly coiling and uncoiling. Something with decent shielding usually lasts longer for me. For home use it might not be as much of an issue, because it's plugged in and never really moved.

In terms of sound quality? No difference.",null,1,cdng5vb,1rgzbv,askscience,top_week,3
lucaxx85,"You need to distingush three applications: 
1)analog signals in home setups
2)digital signals in home setups
3) live concert signals and similars...

For 1) everything works. Including coat hangers (for the power signals. You need shielding for line ones). The resistence and the impedence of such cables are such that they cannot affect in any way the final signal, which has a very low bandwidth. Only thing to be careful is to have cables large enough for the amp-speaker connection, if you have a very high power system (but I'd guess that you can still forget about this in any practical situation). 

2) Digital signals are more complicated. The bandwidth here is much higher, especially if you're also carrying video. That's why you have maximum lenghts and building them needs lots of care. Still almost any commercial cable is good if you're not trying to do something you shouldn't (e.g.: a 10 meters HDMI connection). In these case of course a 15'000 $ cable made from the finest rhodesian zinc, soldered in a full moon night by an african zoroastrian priest would work as badly as the cheapest one in the store.

3) For concerts and other applications cables can give actual problems. Still not those ""lamented"" by audiophiles. The first thing you look for in a concert cable is the *mechanical* resistance, especially of the connectors. Most of the cables break for a mechanical injury! Those things get torn everywhere. 
Then there is a problem with microphones/guitars signals. They're *extremely* weak. So they're sensitive to interferences. But, like before, those cables that claim to feature platinum in their alloy or even to have a special cristalline structure that favours the signal in a specific direction (how on earth would that work??!?!??!!) won't make *any* difference.   There are other tricks to solve the problem (balanced signals, preamplification before long transmissions etc...)

So you actually need a lot of care and you have a number of problems... But they're so not what the audiophiles claim!",null,0,cdni471,1rgzbv,askscience,top_week,3
Cyanmonkey,"I find the build of the cable more important than impedance rating, etc.

A properly built cable with Neutrik connectors and strain relief lasts much longer than your cheap Guitar Center POS, but as far as signal goes, as long as your not going over 200' it doesn't make a noticable difference.",null,1,cdnfrd9,1rgzbv,askscience,top_week,1
EvilHom3r,"For digital (i.e. HDMI), no it does not matter at all. Digital either works or doesn't, there is no in between.

For analog (RCA, speaker wire, TRS wires), you will always get better quality (even if just slightly) with a better wire. However for the average user they will probably never notice the difference, and more likely than not the quality bottleneck is elsewhere in the system.",null,5,cdncqhj,1rgzbv,askscience,top_week,3
Dyson201,"Not exactly an Audio Engineer, but this isn't a difficult question from a signals standpoint.

Transferring signals through a medium (cable) can pose a variety of challenges that are handled in many different ways.  Without going into extreme detail lets just say that electromagnetic forces could possibly come into play, as well as capacitance to ground producing noise in the circuit, etc. etc.

Long story short, if you're replacing a 3' cable for sound, I highly doubt you'll notice a huge dip in quality between $100 cables and coat hangers.  Both are capable of transferring the signal, and while the expensive cable will transfer the signal with a much greater Signal to Noise ration (SNR), at 3' and with modern noise abatement technology, you would be hard pressed to hear a difference.

Now that being said, I wouldn't wire up your home surround system with soldered together coat hangers, as distance plays a huge factor in the quality of the transmitted signal.  Also, if you buy a cheap ass sound system, expect to hear a big difference in quality between expensive and poor cables, even at 3'.  

Finally, audio quality sound is a very low frequency, and does not travel well over distances with a good SNR.  Quality cables are the only way to increase sound quality over distances (relative term, we're talking meters here not miles).  Technology has come a long way towards discerning the signal from the noise, but any reduction in noise is a huge positive in the quality of the signal.",null,11,cdn6sul,1rgzbv,askscience,top_week,6
generalelectrix,"This is a very vague question.

For digital signals, yes this does matter.  Digital audio signals require significantly higher bandwidth and run at higher frequencies than the audio content they encode, so transmission line effects become important.  If the characteristic impedance of the cable you use to transmit a digital signal is not matched to the source and destination, you can get partial reflections or standing waves on the cable, which can definitely cause errors in the reconstructed signal at the destination.  This becomes more important with longer cables.

For analog signals, the frequency is low enough that the characteristic impedance isn't really important.  So long as the conductors you're using are low-resistance (copper is great), coat hangers should work just as well as fancy cable.  Shielding in cables is important for line-level interconnects to prevent the cables from picking up noise from the environment, though this usually isn't too big of a problem in a home environment.

The only real exception to this is for speaker cables (carrying post-amplifier level signals) for electrostatic speakers, as the load they present to the driving amplifier is largely capacative.  Then the details of the impedance of the cable driving the speaker become a bit more important.

I'm a physics PhD in quantum electronics with a minor hi-fi addiction.

Edit: I give an in-depth and accurate answer and get a ton of downvotes?  SCIENCE!",null,19,cdn89ly,1rgzbv,askscience,top_week,5
sever0us,"A meniscus is caused by the ratio of the strength of the cohesive forces of a fluids molecules to each other and the cohesive forces of the fluids molecules to the container wall.

If a fluid has a higher cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a concave meniscus.
If a fluid has a lower cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a convex meniscus.

Since gels behave is a solid-liquid hybrid way, the presence or absence of a meniscus would most likely depend on the physical properties of the gel. It really depends on weather the cohesive forces described above are enough to deform the gels structure.

TL;DR: It depends on the gel. 'Fluid' gels such as shower gel stand a much greater chance of presenting a meniscus than 'solid' gels like ballistics gel.",null,0,cdn8dfo,1rgzf8,askscience,top_week,5
sever0us,"A meniscus is caused by the ratio of the strength of the cohesive forces of a fluids molecules to each other and the cohesive forces of the fluids molecules to the container wall.

If a fluid has a higher cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a concave meniscus.
If a fluid has a lower cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a convex meniscus.

Since gels behave is a solid-liquid hybrid way, the presence or absence of a meniscus would most likely depend on the physical properties of the gel. It really depends on weather the cohesive forces described above are enough to deform the gels structure.

TL;DR: It depends on the gel. 'Fluid' gels such as shower gel stand a much greater chance of presenting a meniscus than 'solid' gels like ballistics gel.",null,0,cdn8dfo,1rgzf8,askscience,top_week,5
StringOfLights,"Yes, it's possible to have multiple ova fertilized by sperm from different men. Sperm can live for several days, and multiple ova can be released over the course of several days in a single ovulation cycle. That means it's possible for more than one ovum to be fertilized and implant, resulting in a pregnancy of multiples with different paternities (I've only ever heard of this happening with twins, but triplets, etc., aren't impossible).

As DNA testing has become more common case reports have come out verifying the different paternities of twins. [Here](http://www.nejm.org/doi/full/10.1056/NEJM197809142991108) is an example from the 1970s, and [here](http://www.fertstert.org/article/S0015-0282%2897%2981456-2/abstract) is one from the 1990s. 

The phenomenon of having two ova fertilized in two seperate coital events is often referred to as ""superfecundation"". It technically refers to any instance in which more than one egg is fertilized in more than one act. Instances where the paternity differs is referred to as ""heteropaternal superfecundation"". [One study estimated](http://www.ncbi.nlm.nih.gov/pubmed/7871943) that 1 in 12 sets of dizygotic twins born to married white women in the US were the result of superfecundation, while 1 in 400 were the result of heteropaternal superfecundation.

Edited for clarity.",null,3,cdn6dbz,1rgzjd,askscience,top_week,30
StringOfLights,"Yes, it's possible to have multiple ova fertilized by sperm from different men. Sperm can live for several days, and multiple ova can be released over the course of several days in a single ovulation cycle. That means it's possible for more than one ovum to be fertilized and implant, resulting in a pregnancy of multiples with different paternities (I've only ever heard of this happening with twins, but triplets, etc., aren't impossible).

As DNA testing has become more common case reports have come out verifying the different paternities of twins. [Here](http://www.nejm.org/doi/full/10.1056/NEJM197809142991108) is an example from the 1970s, and [here](http://www.fertstert.org/article/S0015-0282%2897%2981456-2/abstract) is one from the 1990s. 

The phenomenon of having two ova fertilized in two seperate coital events is often referred to as ""superfecundation"". It technically refers to any instance in which more than one egg is fertilized in more than one act. Instances where the paternity differs is referred to as ""heteropaternal superfecundation"". [One study estimated](http://www.ncbi.nlm.nih.gov/pubmed/7871943) that 1 in 12 sets of dizygotic twins born to married white women in the US were the result of superfecundation, while 1 in 400 were the result of heteropaternal superfecundation.

Edited for clarity.",null,3,cdn6dbz,1rgzjd,askscience,top_week,30
claireauriga,"There are definitely equations that can describe what is going on! Heat and mass transfer are an important part of physics and engineering. 

In order to melt, the ice must be raised to its melting point temperature, then given enough energy to melt into liquid. This energy needs to come from somewhere. Heat moves from hotter to colder places, so the warm air will give energy to the ice (and water) until they are the same temperature. 

There are some complications in calculating all this, however. For example, if the air is stagnant then it will get colder as it gives up energy, which means transfer to the sculpture will slow down. If the air is moving, we also have to think about how fast it's going and if it's removing some of the water as vapour too. 

There are many more and less detailed ways of describing what's going on, but in the very simplest terms, the bigger the temperature difference between the air and the ice, the faster energy will transfer. The lower the ice temperature is below its melting point, the more energy needs to be added to make it warm up and melt. ",null,0,cdngq51,1rgzx3,askscience,top_week,3
RelativisticMechanic,"&gt;Lets say you crush a planet down to mosquito size to form a blackhole.

Alright, we have a black hole of 1 Earth Mass.

&gt;Apparently it would evaporate really fast from your outside frame of reference.

Not really. The lifetime of a Schwarzschild black hole with the mass of the Earth would be about 500 trillion trillion trillion trillion years (as measured by those of us far from the event horizon for the duration).

&gt;But how could any effect pass over the event horizon to reduce the mass of the blackhole?

Nothing necessarily crosses the event horizon; rather, the curvature of spacetime near (but outside) the event horizon produces (nearly) thermal radiation that can be intercepted by those of us far from the black hole. In this process, the spacetime curvature relaxes, manifesting in a decrease in the surface area of the event horizon: the black hole shrinks.

One can, with suitable constructions, model this behavior as a tunneling process whereby particles from inside the event horizon tunnel out; this is analogous to other tunneling behavior wherein particles traverse a classically impenetrable barrier due to quantum mechanical effects.

&gt; I know that things can pass over the EH from their own reference frame - but not from an outside frame.

In fact they *can* cross into the black hole, even in a far-removed frame. The idea that they can't comes from an idealization where you neglect the mass of the infalling object (which we can reasonably assume is very, very small compared to the black hole mass). Even in that approximation, though, if we account for the quantization of light, there will be a final photon emitted from the infalling object. Once that photon is emitted, it will never again be seen by anything outside of the event horizon.",null,0,cdnag5o,1rh0ay,askscience,top_week,3
Daegs,"The simple version: Because the particle entering the event horizon has negative mass.

When the pair of virtual particles are ""created"", if one sticks around with positive mass, then the other must have a negative mass in order to cancel out (and they must cancel out, no free energy)

So the positive mass one shoots off away from the black hole, and the negative mass one enters the black hole which reduces its overall mass. ",null,0,cdn9pcd,1rh0ay,askscience,top_week,1
Nicked777,"The Hawking radiation is a deeply quantum mechanical effect, but here is an intuitive way to think about it. The uncertainty principle requires the creation of particle antiparticle pairs, everywhere, all the time. These particles locally violate conservation of energy, which is allowed in QM, as long as it happens on short enough time scales. This means the two particles annihilate very quickly, as if they were never there. 

The point of hawking radiation is if this happens very close to a black hole's event horizon, one of the particles can get sucked in, and the other will escape, albeit very reduced in energy from its trip. Because of this the Hawking radiation is believed to be very weak. A specialist in this topic could explain why it seems to be only the anti particles that fall in, and why we think this admittedly bizarre idea could me true, but I don't know off the top of my head. ",null,1,cdn9rz6,1rh0ay,askscience,top_week,1
EdwardDeathBlack,"Assuming you use the European convention of having a comma instead of a decimal point, you would get indeed 350,000 people. 

I find the idea of a 6.5 GWh plant weirdly low. A nuclear reactor can easily be a 1GW thermal, assuming 35% conversion efficiency, that's 350 MW electrical. Assuming 90% uptime , that'll be 365 * 24 * 350 * 0.9=~2800GWh. Most nuclear power plants have four or five reactors, so can easily generate 10,000GWh per nuclear plant per year. So a power plant with a total capacity of 6.5GWh per year certainly seems puny by modern energy use. Then again tidal is really not much of an energy source, more of a public relation toy , so maybe it is that puny.",null,1,cdn799s,1rh0eq,askscience,top_week,3
E_F_F_E_C_T,"So using this site for KWH/capita for china gave me 3,300 KWH/capita -http://data.worldbank.org/indicator/EG.USE.ELEC.KH.PC

Then using this site for the station's output - http://en.wikipedia.org/wiki/Jiangxia_Tidal_Power_Station

The instantaneous power of the station is 3,200KW (we'll ignore the solar stuff).

Multiply this by the amount of hours in a year gives you 28 GWH.

Dividing this by the 3300KWH/capita gives us roughly 8500 people.

Considering the second Wikipedia article states that ""The power station feeds the energy demand of small villages at a 20 km (12 mi) distance, through a 35-kV transmission line."" I feel that this isn't that unreasonable.

Hope this helps.",null,0,cdn7d47,1rh0eq,askscience,top_week,2
super-zap,"Compared to most other large power plants your favorite tidal power plant is tiny. 

http://en.wikipedia.org/wiki/List_of_largest_power_stations_in_the_world

It has 1000 times less generating capacity than most of the large ones and almost 6000 times less capacity than the largest power plant.

So, overall it is not surprising that it can generate power for only 350 000 people. I believe your math is correct.",null,0,cdn7dqd,1rh0eq,askscience,top_week,2
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,top_week,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,top_week,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,top_week,3
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,top_week,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,top_week,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,top_week,3
Platypuskeeper,"The color depends on the coordination environment of the Cu(II) ions that are formed. In a concentrated nitric acid solution, the copper ions coordinate to nitrate ions, giving a green/greenish-blue color. If the solution is more dilute (or diluted after oxidizing the copper), then you get a blue solution where the Cu(II) ions are coordinating to water instead.

And on a safety aside: Who the hell are these fools who play around with concentrated HNO3 outside of fume hood? That brown gas is toxic nitrogen dioxide!
",null,1,cdn9hbo,1rh4eg,askscience,top_week,4
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,top_week,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesn’t grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,top_week,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,top_week,1
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,top_week,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesn’t grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,top_week,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,top_week,1
Platypuskeeper,"Two reasons. 1) Most chemical reaction rates increase exponentially with temperature. Water leaching into some stuck food, or something dissolving are chemical reactions. 2) The solubility of most (solid) stuff tends to increase with temperature.
",null,1,cdndlk4,1rh5ch,askscience,top_week,7
SimpleBen,"The viscosity of fats is dramatically altered by temperature. Think about it. Bacon fat in the package is nearly solid, but at around 200 degrees F it is pretty liquid. Fat changes so much with temperature that it is by far the dominant reason that warm water cleans better than cold (not to mention the fats in the soaps!) ",null,0,cdngody,1rh5ch,askscience,top_week,4
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,18,cdnb7zo,1rh9np,askscience,top_week,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,top_week,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,top_week,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a “low heat” setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A “low heat” setting is more [energy] efficient than higher heat settings.    
&gt;7. A “less dry” setting is more [energy] efficient than “normal” or “more dry.” 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,top_week,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,top_week,2
null,null,null,2,cdnev7c,1rh9np,askscience,top_week,2
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,18,cdnb7zo,1rh9np,askscience,top_week,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,top_week,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,top_week,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a “low heat” setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A “low heat” setting is more [energy] efficient than higher heat settings.    
&gt;7. A “less dry” setting is more [energy] efficient than “normal” or “more dry.” 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,top_week,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,top_week,2
null,null,null,2,cdnev7c,1rh9np,askscience,top_week,2
Hiddencamper,"In a nuclear reactor we use the fission process to release energy by splitting the atom. 

For the case of uranium235 the fission process looks roughly as follows

U-235 + n -&gt; Fragment1 + Fragment2 + ~2.4 n + energy

Those fragments are also known as fission products and are somewhat random in size. There is a statistical probability of what you can get. See the image at the top of this Wikipedia page. The fission fragments are where you get all the well known products like xenon, iodine, strontium, cesium, etc

http://en.m.wikipedia.org/wiki/Fission_products_(by_element)

Basically in a nuclear reactor there are fission products that are a result of splitting the atom, and there are also transactinides. What also happens, is the U-238 and U-235 can absorb neutrons but not undergo fission, causing them to become other heavy elements through a series of decay chains. ",null,1,cdnccjy,1rhamu,askscience,top_week,5
Proxymace,"In the uranium mined from the earth the ""active"" isotope makes up appx 0.7% this is enriched to appx 8-10% depending on the type of plant that will use it so there is a substantial amount of material that will be irradiated and will then decay into different elements to the ""active"" one",null,3,cdnbga6,1rhamu,askscience,top_week,1
patchgrabber,"This is an unanswerable question. Different organisms mutate at different rates, we don't know exactly when life began (who knows how many different types of microorganisms were around near the beginning), the way a species is distinguished from another is inherently arbitrary, and we have no idea how many species have ever existed.",null,0,cdncaz4,1rhbru,askscience,top_week,6
biorad17,I've seen  estimates of this.  IIRC you only need one specieation event every million years or so to account for every species.  It's important to note that estimations like this are not necessarily biologically accurate.  They are mathematical models that provide parameters to begin thinking about evolution.,null,0,cdob0of,1rhbru,askscience,top_week,2
Karnivoris,There is not much change at the bottom of the trench by inspection if you look at the size of the globe in comparison to the depth of the trench.,null,1,cdnrtdc,1rhdel,askscience,top_week,3
vashoom,"You can use Newton's Universal Law of Gravitation to calculate a decent approximation.  If the average radius from the center of the Earth to the surface is 6,371 km and the trench is 10.9 km deep, simply plug in 6,371 - 10.9 = 6360.1 km.

Crunching the numbers (gravitational constant times mass of earth divided by that radius (6360100 m) squared, gives me 9.8473 m/s^2.  So just a tiny bit above the average gravitational acceleration on the surface.",null,3,cdndqjv,1rhdel,askscience,top_week,3
null,null,null,5,cdnia0n,1rhdel,askscience,top_week,5
eebootwo,"As said by 1992^^?, not much different. However, depending on the object underwater, it might accelerate upwards due to buoyancy, or downwards faster than GM/r^2 if it is denser than water: which would be if it were a gas compressed to greater density than 0.998 kgm^-3",null,5,cdnnrfp,1rhdel,askscience,top_week,1
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,top_week,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,top_week,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,top_week,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer – common)
8.5–8.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer – rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.5–8.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,top_week,8
null,null,null,3,cdn9iux,1rhdi6,askscience,top_week,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,top_week,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,top_week,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,top_week,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,top_week,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,top_week,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,top_week,2
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,top_week,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,top_week,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,top_week,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer – common)
8.5–8.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer – rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.5–8.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,top_week,8
null,null,null,3,cdn9iux,1rhdi6,askscience,top_week,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,top_week,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,top_week,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,top_week,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,top_week,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,top_week,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,top_week,2
medstudent22,"There are several known benefits to neonatal circumcision. 

- **It prevents penile cancer.** Squamous cell carcinoma of the penis is exceedingly rare in circumcised patients. Circumcision alone may not be the preventative measure. [Phimosis](http://en.wikipedia.org/wiki/Phimosis) (the inability to retract the foreskin) can only occur in non-circumcised individuals and is associated with a higher risk of penile cancer. Phimosis, in many cases, is preventable with adequate hygiene. It should also be noted that penile cancer is extremely rare 1-2 out of 200,000 men per year. Also worthwhile to note that somewhere between 909 and 322,000 circumcisions would need to be performed in order to prevent one case of penile cancer. 

- **It reduces the risk of UTIs in early life and up to 5 years of age.** Uncircumcised males are 20x more likely to develop a UTI during the neonatal period. It should be noted that 111 circumcisions must be performed to prevent one UTI though. Some cost analyses have shown that there is still a cost benefit to performing circumcisions when just considering UTIs though.  
 

There are some claimed benefits of circumcision with varying amounts of evidence. 
 
- **It may reduce the spread of HIV** (to men, in heterosexual relationships). This is based on several large African clinical trials. It was not found to reduce the risk of transmission to women and has not been shown to reduce the risk of transmission in homosexual male couples. 

- **It may reduce the transmission of HPV and herpes (HSV).** In a study of 3393 men (1684 who underwent circumcision), after two years, 7.8% of the circumcised men had HSV-2 antibodies, 10.3% of the uncircumcised group did. In the same study, 18% of the circumcised men had evidence of HPV, 27.9% of uncircumcised men did. ([Study](http://www.nejm.org/doi/full/10.1056/NEJMoa0802556)) It should be noted that this study was performed in Uganda. Also worthwhile to note that most individuals clear HPV spontaneously and also that a [vaccine is available](http://en.wikipedia.org/wiki/Gardasil) for the most common HPV strains. Also worthwhile to note that HPV is associated with penile cancer, but more importantly cervical cancer in women.  

The reason I tried to note the conclusions which were drawn based on African studies is that the underlying prevalence of disease has an effect on the study and these results may not be considered generalizable to other populations.  

Multiple groups have issued statements on neonatal circumcision which may contain more information that may be useful to you. 

[The American Academy of Pediatrics](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCwQFjAA&amp;url=http%3A%2F%2Fpediatrics.aappublications.org%2Fcontent%2Fearly%2F2012%2F08%2F22%2Fpeds.2012-1989&amp;ei=FrGUUpKsOZHlsATz34HABA&amp;usg=AFQjCNGNikptx2aRUdOftngXQ3JyIFWU5g&amp;sig2=MiBQ3NWjASCvuKD_PjKXHw) states: 
&gt;Evaluation of current evidence indicates that the health benefits of newborn male circumcision outweigh the risks and that the procedure’s benefits justify access to this procedure for families who choose it. 

[The American Urological Association](http://www.auanet.org/about/policy-statements/circumcision.cfm) states: 
&gt; neonatal circumcision has potential medical benefits and advantages as well as disadvantages and risks.",null,4,cdne4ln,1rhdmd,askscience,top_week,17
redallerd,"Yes. Since there is no limit as to how small fractions can be, there can be an infinite amount between two whole numbers. If you're finding it hard to understand, try adding halving fractions to see if you can get to a whole number for example : 1/2 + 1/4 + 1/8 + 1/16 and etc.",null,0,cdndbyb,1rhegr,askscience,top_week,10
Captain-Negative,"Yes. For example, consider all finite strings of digits beginning with ""1."" and ending with a varying number of twos.

1.2, 1.22, 1.222, 1.2222, 1.22222, etc.

If you count these numbers one by one, you'll notice that it goes on infinitely long. This is a type of ""countable infinity"" (alternatively, [aleph](http://en.wikipedia.org/wiki/Aleph_number)-zero or [beth](http://en.wikipedia.org/wiki/Beth_number)-zero), because it's an infinity arising from (surprise surprise) a list you can count through.

However, there are even bigger kinds of infinity -- each of which is ""uncountable"" -- one of which describes the number of numbers between 1 and 2. As it turns out, there's no way to come up with a method to list all numbers between 1 and 2 one-by-one simply because there are way-too-fucking-many of them. Perhaps paradoxically, though, you can show that describing all the numbers between 1 and 2 is just as hard as describing those between and 1000000, so the two infinities are actually said to be the same (specifically, it's called the ""beth-one"" infinity).

Very roughly speaking, it is unclear if there is an infinity between beth-zero (countable infinity) and beth-one (the type of incountable infinity we just discussed). Whether or not this ""beth-half"" exists depends on how you decide to model the world, and is the topic of something called ""[the continuum hypothesis](http://en.wikipedia.org/wiki/Continuum_hypothesis)"" in mathematics.",null,0,cdnee15,1rhegr,askscience,top_week,2
xavier_505,"This is likely an issue with the way the MPEG streams are configured. Generally MPEG-2 encoding (I am not especially familiar with MPEG 4 but I would hazard a guess that it is similar, or at a very minimum has great flexibility in its configuration) uses three types of frames: I, B and P.

- I (intra) frames are full frames of data encoded similarly to JPEG (~7:1 compression). These do not reference any other frames.

- P (predicted) frames only have information describing the difference between the preceding I or P frame (lower size than I frames, ~20:1 compression).

Both I and P frames are called ""reference frames"" since the information they describe can be referenced by other frames.

- B (bidirectional) frames are encoded from interpolation of preceding/following reference frames (even smaller than P frames, ~50:1 compression).

These various frame types are typically sent in the following way:

    I B B P B B I B B P B B I P B B....

Where the distance between I/P frames (in this case it is 3) is configurable as is the distance between I frames (in this case there are 6).

Why am I telling you this!? Well, a P frame cannot be decoded without its referenced I frame, and B frames cannot be decoded without all of their referenced I/P frame. So, the longer the distances mentioned above, the greater affect a lossy channel will have. You would see the behavior you are describing in the following case:

    MPEG2 GOP structure (m=2, n=4): I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B ....

    MPEG4 GOP structure (m=4, n=16): I B B B P B B B P B B B P B B B I B B B P B B B P B B B P B B B ....

A lost I frame on the lower stream would blow away 16 frames of data, while on the upper frame only 4 frames. Similar for lost P frames.

Edit: Cleaned up description of IBP frames.",null,0,cdnfw8p,1rhf15,askscience,top_week,3
chucklesMtheThird,"A total stab in the dark here....it could be that MPEG4 channels are sent on higher QAM constellation carriers, which are more susceptible to minute changes in signal quality, and the MPEG2 are sent over lower constellation carriers?

For example, QAM64 receivers have a much higher tolerance range in amplitude and phase angle error than do QAM256 or QAM512 because of the lower symbol density. The higher you go in constellation density, the less room there is for error.",null,1,cdney2o,1rhf15,askscience,top_week,2
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,top_week,32
null,null,null,1,cdnfj49,1rhi55,askscience,top_week,10
null,null,null,0,cdngnvj,1rhi55,askscience,top_week,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,top_week,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,top_week,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,top_week,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,top_week,1
null,null,null,1,cdng7fo,1rhi55,askscience,top_week,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,top_week,2
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,top_week,32
null,null,null,1,cdnfj49,1rhi55,askscience,top_week,10
null,null,null,0,cdngnvj,1rhi55,askscience,top_week,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,top_week,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,top_week,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,top_week,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,top_week,1
null,null,null,1,cdng7fo,1rhi55,askscience,top_week,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,top_week,2
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,20,cdndsjx,1rhpc1,askscience,top_week,75
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,top_week,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,top_week,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,top_week,15
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,20,cdndsjx,1rhpc1,askscience,top_week,75
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,top_week,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,top_week,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,top_week,15
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,top_week,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,top_week,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,top_week,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,top_week,1
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,top_week,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,top_week,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,top_week,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,top_week,1
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,2,cdng2f1,1rhpdk,askscience,top_week,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,top_week,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,top_week,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,top_week,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,top_week,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,top_week,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,top_week,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,top_week,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,top_week,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing…
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,top_week,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,top_week,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,top_week,2
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,2,cdng2f1,1rhpdk,askscience,top_week,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,top_week,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,top_week,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,top_week,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,top_week,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,top_week,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,top_week,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,top_week,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,top_week,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing…
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,top_week,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,top_week,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,top_week,2
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,9,cdndirh,1rhpj0,askscience,top_week,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,top_week,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,0,cdnf7mk,1rhpj0,askscience,top_week,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,top_week,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,top_week,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,top_week,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,top_week,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,top_week,1
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,9,cdndirh,1rhpj0,askscience,top_week,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,top_week,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,0,cdnf7mk,1rhpj0,askscience,top_week,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,top_week,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,top_week,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,top_week,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,top_week,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,top_week,1
Halysites,"Two reasons:

1. When a star is going through [fusion](http://en.wikipedia.org/wiki/Star#Formation_and_evolution), it will combine hydrogen to form heavier and heavier atoms. The basic reaction series would be: 

* hydrogen + hydrogen = helium
* helium + helium = carbon
* carbon + carbon = iron
* there will be other combinations of fusing atoms which would result in the creation of neon, silicon, etc. This is just a very simple list (I'm just a simple geologist and chemistry is not my speciality).

Once a star has burned all it's fuel to generate iron, it will undergo a supernova (or some other process, depending on it's size). Since iron was the last atom to be produced during the fusion process the star will be very rich with iron. If it's goes through a supernova it ends up ejecting most of it's material into the space around it; during this process heavier elements may form as well. So the space around it becomes enriched in a variety of elements, especially iron. This material is what will be used to form planets.

2. Planetary material begins to accrete from the rich stuff spewed out by a dying star. It is very hot and so the material is molten. As the planetoids get larger and larger, gravity becomes a stronger force. Gravity, coupled with a molten states, means that heavier elements are pulled towards the core of the planetary body quickly. Iron, being very heavy, will sink towards the core. Other heavy (metallic) elements will also sink to the core.

Viola, you have an iron-rich core for rocky planets. This process doesn't exactly apply to gas giants like Jupiter (which would be relatively depleted in iron).

Most of this information is from geology textbooks and courses I took in my undergrad. Although I imagine most of the info could be looked up on the internet.",null,1,cdnh1c9,1rhqp5,askscience,top_week,3
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,top_week,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,top_week,1
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,top_week,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,top_week,1
Trill-Nye,"What do you mean by ""the two angles?"" Electrons will be diffracted by a crystalline material at a number of angles, each corresponding to a certain crystallographic plane with a reflection allowed by the structure factor. So each diffracted beam is due to a different d-spacing. the different n values in the Bragg equation correspond to higher order reflections, but these can generally be ignored. Does this answer your question?",null,0,cdnfqvw,1rhu16,askscience,top_week,3
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,233,cdnhkj4,1rhu7r,askscience,top_week,1075
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,34,cdnfpuh,1rhu7r,askscience,top_week,203
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,7,cdng9z2,1rhu7r,askscience,top_week,25
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,7,cdnkfb7,1rhu7r,askscience,top_week,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,3,cdng7o4,1rhu7r,askscience,top_week,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,2,cdnqm5k,1rhu7r,askscience,top_week,12
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,1,cdnni7u,1rhu7r,askscience,top_week,10
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,top_week,7
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,2,cdnj69w,1rhu7r,askscience,top_week,7
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,1,cdnfgcp,1rhu7r,askscience,top_week,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,top_week,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,1,cdnpywc,1rhu7r,askscience,top_week,5
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,top_week,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,3,cdnfmxv,1rhu7r,askscience,top_week,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,1,cdnguqo,1rhu7r,askscience,top_week,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,3,cdnril0,1rhu7r,askscience,top_week,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,top_week,4
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,top_week,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,top_week,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,1,cdnwy1g,1rhu7r,askscience,top_week,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,top_week,6
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,top_week,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,top_week,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,1,cdnpfal,1rhu7r,askscience,top_week,3
null,null,null,0,cdns99f,1rhu7r,askscience,top_week,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,top_week,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,2,cdnt98m,1rhu7r,askscience,top_week,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,top_week,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,top_week,2
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,233,cdnhkj4,1rhu7r,askscience,top_week,1075
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,34,cdnfpuh,1rhu7r,askscience,top_week,203
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,7,cdng9z2,1rhu7r,askscience,top_week,25
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,7,cdnkfb7,1rhu7r,askscience,top_week,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,3,cdng7o4,1rhu7r,askscience,top_week,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,2,cdnqm5k,1rhu7r,askscience,top_week,12
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,1,cdnni7u,1rhu7r,askscience,top_week,10
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,top_week,7
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,2,cdnj69w,1rhu7r,askscience,top_week,7
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,1,cdnfgcp,1rhu7r,askscience,top_week,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,top_week,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,1,cdnpywc,1rhu7r,askscience,top_week,5
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,top_week,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,3,cdnfmxv,1rhu7r,askscience,top_week,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,1,cdnguqo,1rhu7r,askscience,top_week,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,3,cdnril0,1rhu7r,askscience,top_week,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,top_week,4
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,top_week,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,top_week,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,1,cdnwy1g,1rhu7r,askscience,top_week,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,top_week,6
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,top_week,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,top_week,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,1,cdnpfal,1rhu7r,askscience,top_week,3
null,null,null,0,cdns99f,1rhu7r,askscience,top_week,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,top_week,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,2,cdnt98m,1rhu7r,askscience,top_week,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,top_week,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,top_week,2
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,top_week,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,top_week,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,top_week,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,top_week,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,top_week,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,top_week,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,top_week,1
null,null,null,32,cdnftrg,1rhuln,askscience,top_week,29
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,top_week,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,top_week,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,top_week,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,top_week,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,top_week,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,top_week,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,top_week,1
null,null,null,32,cdnftrg,1rhuln,askscience,top_week,29
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,top_week,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,top_week,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,top_week,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,top_week,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,top_week,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,top_week,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,top_week,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,top_week,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,top_week,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,top_week,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,top_week,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,top_week,1
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,top_week,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,top_week,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,top_week,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,top_week,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,top_week,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,top_week,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,top_week,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,top_week,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,top_week,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,top_week,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,top_week,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,top_week,1
Astrokiwi,"The space between the stars in a galaxy is filled with a very thin gas - the ""interstellar medium"". Even between galaxies there is the ""intergalactic medium"" too. This means stars and galaxies aren't completely isolated in space - there's gas everywhere. This means you'll have matter and antimatter annihilating each other in the ""border regions"" between antimatter and matter galaxies. This would produce a constant stream of ~~\~2~~~1 GeV gamma rays in these border regions, which we'd be able to detect with our gamma ray telescopes. However, we don't see this.",null,2,cdnj4gy,1ri2no,askscience,top_week,9
rocketgolfer,"Antimatter functionally behaves the same as normal matter, it's just that there's much, much less of it and it annihilates as soon as it contacts normal matter. The parts of antimatter that are ""opposite"" are opposite only by convention (e.g. it doesn't matter whether we treat the electron as being positively charged or negatively charged, but it does matter that the proton has the opposite charge).",null,1,cdnkvjd,1ri2no,askscience,top_week,2
Astrokiwi,"The space between the stars in a galaxy is filled with a very thin gas - the ""interstellar medium"". Even between galaxies there is the ""intergalactic medium"" too. This means stars and galaxies aren't completely isolated in space - there's gas everywhere. This means you'll have matter and antimatter annihilating each other in the ""border regions"" between antimatter and matter galaxies. This would produce a constant stream of ~~\~2~~~1 GeV gamma rays in these border regions, which we'd be able to detect with our gamma ray telescopes. However, we don't see this.",null,2,cdnj4gy,1ri2no,askscience,top_week,9
rocketgolfer,"Antimatter functionally behaves the same as normal matter, it's just that there's much, much less of it and it annihilates as soon as it contacts normal matter. The parts of antimatter that are ""opposite"" are opposite only by convention (e.g. it doesn't matter whether we treat the electron as being positively charged or negatively charged, but it does matter that the proton has the opposite charge).",null,1,cdnkvjd,1ri2no,askscience,top_week,2
WhoH8in,"Its completely aritrary. There is no objective way to identify ""up"". We choose the Earth's negative pole as north and assign that to the rest of the solar system and orient our images of other planets to that. If when cartographers started drawing maps they had placed the positive node on top then we would think of Antarctica as being Arctica. The only other way to get bearings in the solar system to orient yourself to the orbits of the planets. If you are looking toward the sun and the planets are going left to right you are oriented ""upward"" if they go right to left you are ""upside down"". But in reality none of this matters.",null,0,cdnj6g9,1ri48m,askscience,top_week,2
stuthulhu,"&gt;I'm visualizing the Solar System as planets orbiting the Sun in a flat disc. If we imagine that the disc is like a dinner plate, the standard view of Earth is that Antartica is orientated toward the bottom of the dinner plate. Is this actually correct?

Pick one. From a vantage point above the north pole of the Earth, the Earth would appear to revolve in a counterclockwise direction about the Sun. From a vantage point above the south pole, the Earth would appear to revolve in a clockwise direction. ",null,0,cdnjmdd,1ri48m,askscience,top_week,2
DangerOnion,"The plane itself is tilted pretty severely relative to the plane of the galaxy, making terms like ""up"" kind of meaningless in the first place.  But the simple answer is that you're right. The equators of most planets are roughly aligned with their orbital planes, and we invented astronomy so we get to decide which way is up :)  If we decide that Antarctica is ""down,"" then so is the hemisphere of Jupiter with the GRS in it.  Most pictures of the planets, despite being taken by satellites with no particular orientation, are rotated to be consistent with the way we visualize ""up"" in our solar system.",null,0,cdnv8pl,1ri48m,askscience,top_week,2
Baloroth,"Putting a fan behind the space heater will produce forced convection, which will cool down the heater and heat up the room as a whole. This is the reason central furnaces have fans in the first place: it spreads the heat around (same for AC, but the reverse principle: you heat up the condenser and cool down the air).

What the net effect over a long period of time will be (i.e. if the room will end up warmer with or without the fan) depends on many factors, but generally I would venture that the room will be warmer overall with the fan than without. But short term, in a cold room, the fan will certainly speed up the process.",null,2,cdnilrg,1ri6wl,askscience,top_week,13
OlejzMaku,"It depends on the definition of ""heat up the room"". Do you want evenly districubuted heat or warmer area around the heater? How warm do you want the room to be? How big is the room? How well isolated it is? What temperature is outside?

If the room will be too big and/or badly isolated and/or it is very cold outside and/or your desired temperature is too high the fan might be contraproductive.",null,1,cdni8d1,1ri6wl,askscience,top_week,7
garycarroll,"You are correct that the result will be at least as much heat energy into the room, and more air movement (to a point) will result in more even heat. This may be better, or not. If the room is not sealed for instance, the door is open to the rest of the building) more heat may escape than if you had a warm side of the room away from the door. And as OlejzMaku implied, if the room is too large or cold, the space heater may be unable to make the whole thing comfortable but could heat one corner. 
Also, note that moving air may feel cooler than still air of the same temperature. 
It sounds like you are trying to heat the whole room. If so, the whole room will heat more evenly with better circulation, and this means the guy sitting next to the heater will not get warm as quickly. If I had brought the heater, I might prefer no fan.",null,0,cdnjfua,1ri6wl,askscience,top_week,3
Richard_Fitzsnuggly,"More information is needed as well as the previous responses.  Is the room a defined sealed space?  If not you will be attempting to heat fresh air instead of re-heated air as it circulates within the space.  The friction of the blades on the air does not impact the heat.  The speed in which the drag coefficient of the air on the blades versus the cooling affect of the ambient air, would need to be astronomically fast.
",null,0,cdnj58v,1ri6wl,askscience,top_week,2
expertunderachiever,"Ironically it could make the space around the heater hotter than desired as you cycle cool air over the heater basically nullifying the duty cycle [instead of shutting off for a bit it'll always be on].

From experience it will make the room hotter though.  I've used this trick in my basement on really cold days where you just need it to warm up.",null,1,cdniy0a,1ri6wl,askscience,top_week,2
DangerOnion,"Assuming an enclosed space with no open windows or anything, you'd be right.  The fan is producing a negligible amount of heat energy through friction, but it certainly can't reduce the temperature of the room.  He may be conflating the use of fans with the reason we use fans in the summer, which is that 1) moving air makes our skin feel cooler through evaporative cooling, but doesn't actually reduce the air temperature or 2) by circulating fresh outside air into a stuffy building, which doesn't apply here.  The temperature in one place might rise slower, but it's not reducing the amount of heat added to the room.",null,0,cdnuz1w,1ri6wl,askscience,top_week,1
OrbitalPete,"Tall mountains are generally a product of continental collision. That occurs when subduction processes close an ocean and collide the continents that  formed its margins.

When that ocean closes lots of the upper sediments get scraped off. They obviously get caught up in the collision zone, and involved in the thrust faulting and deformation that builds the resulting mountain range. Carbonates such as limestone are commonly among these uppermost sediments. Hence the fact you tend to see a lot of limestones at the top of mountain ranges.",null,0,cdnq1t9,1ri7ga,askscience,top_week,6
DangerOnion,"I don't think they're overrepresented.  The Alps and Rockies are mostly granite, and the Andes are chiefly igneous rock.  Like OrbitalPete says, mountain ranges are usually formed by the collision of tectonic plates, and whatever rock happens to constitute those plates is what gets shoved upwards.",null,0,cdnv4et,1ri7ga,askscience,top_week,3
bohr_exciton,"Yes, but only if you ionize the gas, manage to extract (at least in part) charge carriers of one type (say electrons), and then manage to somehow isolate the system such that charge neutrality cannot be re-established. In that case there will be a net charge within the gas, and the resulting repulsion would act as an effective increase in the pressure, which like you said could alternatively lead to a larger equilibrium volume if the container is flexible. ",null,0,cdnjcd7,1ri7it,askscience,top_week,1
TangentialThreat,"The charge will prefer to collect on the outside of the balloon, but the balloon material will repel itself and that may have the desired effect.

Is it cheating if I heat the contents of the balloon to 10,000 K? The rubber will melt, but for a brief moment you will have significantly increased the pressure using ionization.",null,0,cdnkify,1ri7it,askscience,top_week,1
__Pers,"Yes, but not in the way you think. In plasmas, as in ideal gases, 

P = n k_B T

If you ionize a gas to make a plasma, the density of independent particles n comprises electron density and ion density and the sum is higher than the neutral particle density prior to ionization. Also, in making a plasma from a gas, you generally make the temperature higher. Both will tend to increase the pressure. ",null,0,cdnsyil,1ri7it,askscience,top_week,1
thetripp,"Your son is basically describing the theory known as ""Tired Light.""  The reason we don't think tired light is true is that we've never been able to come up with a mechanism that would cause energy loss in photons, yet still match our observed data.

For a tired light phenomenon to be true, it would have to:

1) Explain energy loss of photons over long distances, and match the observed redshift.

2) Not scatter photons so much as to induce blurring (since we don't observe significant blurring of distant objects).

3) Also explain the observed time dilation of distant events

4) Cause the same effect in every wavelength band, or in other words photons must ""tire"" in the same way, regardless of their frequency.

The wikipedia page on [Tired Light](http://en.wikipedia.org/wiki/Tired_light) has a nice list of some of the historical proposals related to this theory and why they don't match the observed evidence.",null,1,cdnl44c,1ri89x,askscience,top_week,13
stimulatedecho,"The distance related red shift is *evidence* of an expanding universe.  There happens to be a mountain of other evidence (search this sub for this question being asked previously to find specifics, mostly related to the cosmic microwave background, I believe), that suggests the same thing, i.e. expanding universe.  So, we don't really *know* that expansion causes the observed red shift, but it is certainly one valid explanation (as you already know), and it also explains other things we observe.  Additionally, and potentially more importantly, we have no experimental evidence to the contrary. 

That said, the requirement of ""dark energy"" energy seems to be a bit of a blemish...there is no doubt something in the recipe we have no understanding of.  I guess we'll find out exactly what as we go!",null,1,cdnlgh3,1ri89x,askscience,top_week,6
florinandrei,"&gt; ""but what if light just lost energy steadily as it went, wouldn't that look the same?""

No, it would not.

You are talking about an entire class of alternative explanations of redshift, grouped under the umbrella of the ""tired light hypothesis"". The have pretty much been debunked in bulk.

You cannot have light become ""tired"" by magic. There has to be some physical mechanism for photons to lose energy. If so, the energy loss will tweak the properties of those photons a little. As a result, a series of effects would become apparent:

- images from distant objects would become a little blurred, due to the scattering of the photons via energy-sapping interactions

- distant events would be observed to take place at the same rate of time; there would be no time dilation, like in the relativistic redshift models

Other effects would also become observable, depending on the particular ""flavor"" of tired-light theory, none of which have ever been observed.

Bottom line: the expansion of a relativistic universe is the only model that accounts for everything we observe out there.

http://en.wikipedia.org/wiki/Tired_light",null,1,cdnlxim,1ri89x,askscience,top_week,3
WhoH8in,"Well light does loose energy as it goes, in a sense anyway, every time it doubles its distance its energy is 1/4 what it was because intensity dissipates. This does not affect wavelength though. There are other phenomena that affect wavelength though, like movement. If somehting is moving toward us the light it emits doesn't seem to hit us any ""harder""(b/c light only goes c, no faster, no slower, ever) but that energy is accounted for, the light decreases its wavelength. If it is moving away then the wavelength increases which makes it appear redder.

Now when looking out into the stars hubble noticed that the further an object was the redder it appeared to be compared to what we know that objects [emission spectrum](http://en.wikipedia.org/wiki/Emission_spectrum) *should* look like. Now if the Universe were static then we would expect that, overall, half of all objects must be moving toward us and half be moving away and some tiny minority not moving relative to us at all. It is incredibly unlikely that, in a static universe, all objects would be moving away form us, we would ahve to be a truly uniqe body to observe that, literally the center of the universe. If the universe is expanding though it makes perfect sense because every object percieves every other object as moving away from it (ignoring of course nearby objects).

Use the expanding balloon analogy to understand it. If you have a barely inflated balloon and you draw three dots on it then start to blow it up those dots appear to be moving away form eachother but none fo them are actually moving, the space between them is expanding. This is why we think redshift is caused by expansion.",null,6,cdnjekl,1ri89x,askscience,top_week,2
thetripp,"Your son is basically describing the theory known as ""Tired Light.""  The reason we don't think tired light is true is that we've never been able to come up with a mechanism that would cause energy loss in photons, yet still match our observed data.

For a tired light phenomenon to be true, it would have to:

1) Explain energy loss of photons over long distances, and match the observed redshift.

2) Not scatter photons so much as to induce blurring (since we don't observe significant blurring of distant objects).

3) Also explain the observed time dilation of distant events

4) Cause the same effect in every wavelength band, or in other words photons must ""tire"" in the same way, regardless of their frequency.

The wikipedia page on [Tired Light](http://en.wikipedia.org/wiki/Tired_light) has a nice list of some of the historical proposals related to this theory and why they don't match the observed evidence.",null,1,cdnl44c,1ri89x,askscience,top_week,13
stimulatedecho,"The distance related red shift is *evidence* of an expanding universe.  There happens to be a mountain of other evidence (search this sub for this question being asked previously to find specifics, mostly related to the cosmic microwave background, I believe), that suggests the same thing, i.e. expanding universe.  So, we don't really *know* that expansion causes the observed red shift, but it is certainly one valid explanation (as you already know), and it also explains other things we observe.  Additionally, and potentially more importantly, we have no experimental evidence to the contrary. 

That said, the requirement of ""dark energy"" energy seems to be a bit of a blemish...there is no doubt something in the recipe we have no understanding of.  I guess we'll find out exactly what as we go!",null,1,cdnlgh3,1ri89x,askscience,top_week,6
florinandrei,"&gt; ""but what if light just lost energy steadily as it went, wouldn't that look the same?""

No, it would not.

You are talking about an entire class of alternative explanations of redshift, grouped under the umbrella of the ""tired light hypothesis"". The have pretty much been debunked in bulk.

You cannot have light become ""tired"" by magic. There has to be some physical mechanism for photons to lose energy. If so, the energy loss will tweak the properties of those photons a little. As a result, a series of effects would become apparent:

- images from distant objects would become a little blurred, due to the scattering of the photons via energy-sapping interactions

- distant events would be observed to take place at the same rate of time; there would be no time dilation, like in the relativistic redshift models

Other effects would also become observable, depending on the particular ""flavor"" of tired-light theory, none of which have ever been observed.

Bottom line: the expansion of a relativistic universe is the only model that accounts for everything we observe out there.

http://en.wikipedia.org/wiki/Tired_light",null,1,cdnlxim,1ri89x,askscience,top_week,3
WhoH8in,"Well light does loose energy as it goes, in a sense anyway, every time it doubles its distance its energy is 1/4 what it was because intensity dissipates. This does not affect wavelength though. There are other phenomena that affect wavelength though, like movement. If somehting is moving toward us the light it emits doesn't seem to hit us any ""harder""(b/c light only goes c, no faster, no slower, ever) but that energy is accounted for, the light decreases its wavelength. If it is moving away then the wavelength increases which makes it appear redder.

Now when looking out into the stars hubble noticed that the further an object was the redder it appeared to be compared to what we know that objects [emission spectrum](http://en.wikipedia.org/wiki/Emission_spectrum) *should* look like. Now if the Universe were static then we would expect that, overall, half of all objects must be moving toward us and half be moving away and some tiny minority not moving relative to us at all. It is incredibly unlikely that, in a static universe, all objects would be moving away form us, we would ahve to be a truly uniqe body to observe that, literally the center of the universe. If the universe is expanding though it makes perfect sense because every object percieves every other object as moving away from it (ignoring of course nearby objects).

Use the expanding balloon analogy to understand it. If you have a barely inflated balloon and you draw three dots on it then start to blow it up those dots appear to be moving away form eachother but none fo them are actually moving, the space between them is expanding. This is why we think redshift is caused by expansion.",null,6,cdnjekl,1ri89x,askscience,top_week,2
s3c7i0n,"It does, were you to look at the sun directly in space, you'd be blinded. That's the point of those gold visors on space suits. The reason it doesn't appear to is that there's very little for the light to strike. Think of shining a flashlight into the air, it has effectively no visible effect. Now if you shine it at a tree, it gets nice and bright. It's the same amount of light, but it doesn't appear so because we can't see it travel. Now if you mean why isn't space blue, like the sky, that's because when sunlight filters through the atmosphere, it's interaction with the various gasses scatter the blue light most, giving us that nice hue. Space, obviously for the most part lacking an atmosphere, doesn't scatter the light, hence the black. ",null,0,cdnj4u9,1riapc,askscience,top_week,28
alltat,"It *does* make space bright. The only reason space looks black is because it's empty: it's not black because it's dark, but because there's nothing there. If you look at pictures of spaceships and satellites in space, you'll notice that they're all brightly lit with strong shadows. That's because space is bright, as long as you're close to a star.",null,0,cdnjy5p,1riapc,askscience,top_week,7
VA_guy,"There are two parts.  First, the sun's brightness decreases with the square of the distance.  Meaning when you're twice as far, it is 1/4 as bright.  Four times as far?  4*4=16, 1/16 as bright.  That's because there is a finite amount of light being cast over an ever increasing spherical area.  So the sun would look quite a bit less bright from Mars or Saturn than it would here simply because we're closer.
  
But if you're asking why space isn't glowing, you need to think about what would cause that to occur.  If you have a spotlight on a clear night, it will illuminate a path in front of it but it won't make the entire surrounding area bright, right?  But if you were to shine that same spotlight into a white room, it would do a much better job of making the whole area look bright.  That would be because there are reflections in the second case which case the light to come at you from all angles, appearing to illuminate you from everywhere.  
  
So in space it would be similar to that spotlight.  If you look directly into the sun, it would be very bright (depending on your distance).  But otherwise, there is nothing else out there for the light to reflect off of, so it won't be as if the entire area is glowing or light is coming from you at all directions.
  
Hope that helps.",null,0,cdnj62j,1riapc,askscience,top_week,6
stuthulhu,"Things are bright because light bounces off those things, and strikes your retina. There's relatively little in space for light to bounce off of, and get redirected towards your retina instead of traveling away. ",null,0,cdnjqys,1riapc,askscience,top_week,4
Gitsumkikin,"Oh it does! Only specialized cameras can turn towards the sun... If a regular camera were to be facing the sun all you would see is white. Same thing if you were facin it...Kiss yer eye sight goodbye! I think its somethin like 350 degrees if you are in direct sunlight in space, -350 out of. Space is so unimaginably huge that if your back was to the sun,(it better be!) the light probably wouldn't be noticeable at all...nothing for it to reflect off...as I said,mostly educated guesses here. Aside from the temp and the specialized cameras, those are facts, temp may be off one way or another, but, not by much.",null,0,cdnj45h,1riapc,askscience,top_week,3
s3c7i0n,"It does, were you to look at the sun directly in space, you'd be blinded. That's the point of those gold visors on space suits. The reason it doesn't appear to is that there's very little for the light to strike. Think of shining a flashlight into the air, it has effectively no visible effect. Now if you shine it at a tree, it gets nice and bright. It's the same amount of light, but it doesn't appear so because we can't see it travel. Now if you mean why isn't space blue, like the sky, that's because when sunlight filters through the atmosphere, it's interaction with the various gasses scatter the blue light most, giving us that nice hue. Space, obviously for the most part lacking an atmosphere, doesn't scatter the light, hence the black. ",null,0,cdnj4u9,1riapc,askscience,top_week,28
alltat,"It *does* make space bright. The only reason space looks black is because it's empty: it's not black because it's dark, but because there's nothing there. If you look at pictures of spaceships and satellites in space, you'll notice that they're all brightly lit with strong shadows. That's because space is bright, as long as you're close to a star.",null,0,cdnjy5p,1riapc,askscience,top_week,7
VA_guy,"There are two parts.  First, the sun's brightness decreases with the square of the distance.  Meaning when you're twice as far, it is 1/4 as bright.  Four times as far?  4*4=16, 1/16 as bright.  That's because there is a finite amount of light being cast over an ever increasing spherical area.  So the sun would look quite a bit less bright from Mars or Saturn than it would here simply because we're closer.
  
But if you're asking why space isn't glowing, you need to think about what would cause that to occur.  If you have a spotlight on a clear night, it will illuminate a path in front of it but it won't make the entire surrounding area bright, right?  But if you were to shine that same spotlight into a white room, it would do a much better job of making the whole area look bright.  That would be because there are reflections in the second case which case the light to come at you from all angles, appearing to illuminate you from everywhere.  
  
So in space it would be similar to that spotlight.  If you look directly into the sun, it would be very bright (depending on your distance).  But otherwise, there is nothing else out there for the light to reflect off of, so it won't be as if the entire area is glowing or light is coming from you at all directions.
  
Hope that helps.",null,0,cdnj62j,1riapc,askscience,top_week,6
stuthulhu,"Things are bright because light bounces off those things, and strikes your retina. There's relatively little in space for light to bounce off of, and get redirected towards your retina instead of traveling away. ",null,0,cdnjqys,1riapc,askscience,top_week,4
Gitsumkikin,"Oh it does! Only specialized cameras can turn towards the sun... If a regular camera were to be facing the sun all you would see is white. Same thing if you were facin it...Kiss yer eye sight goodbye! I think its somethin like 350 degrees if you are in direct sunlight in space, -350 out of. Space is so unimaginably huge that if your back was to the sun,(it better be!) the light probably wouldn't be noticeable at all...nothing for it to reflect off...as I said,mostly educated guesses here. Aside from the temp and the specialized cameras, those are facts, temp may be off one way or another, but, not by much.",null,0,cdnj45h,1riapc,askscience,top_week,3
Freeoath,"The eraser works in a way that when you rub it, it removes the graphite from the papers surface. The rubber is more ""sticky"" then the paper and thus the graphite preferes the eraser over the paper. Another way some erasers works is the eraser damges the top  layer of the paper effectively removing the graphite that way. 
You can't use an eraser on ink (what a pen leaves behind) because the paper more or less absorbs the ink deeper making the erasers funcion useless. For these you can use ink remover that either changes the chemical compound of the ink removing it from the paper, or dying it white

",null,1,cdnk132,1ricv9,askscience,top_week,11
Freeoath,"The eraser works in a way that when you rub it, it removes the graphite from the papers surface. The rubber is more ""sticky"" then the paper and thus the graphite preferes the eraser over the paper. Another way some erasers works is the eraser damges the top  layer of the paper effectively removing the graphite that way. 
You can't use an eraser on ink (what a pen leaves behind) because the paper more or less absorbs the ink deeper making the erasers funcion useless. For these you can use ink remover that either changes the chemical compound of the ink removing it from the paper, or dying it white

",null,1,cdnk132,1ricv9,askscience,top_week,11
stimulatedecho,"Hail forms in the presence of a strong updraft.  In this case, water precipitates, freezes and falls, but is blown back (continuously).  During this cycle more layers of ice form on the previously precipitated particles, until they get too heavy and eventually fall to the ground

If you get the chance, break open a hail piece (bigger the better) and you will find it is layered akin to an everlasting gobstopper.  ",null,0,cdnm2ax,1rid0v,askscience,top_week,2
ipostjesus,"basically, snow = ice from the beginning to the end of the process of water particles accumulating into larger structures. It doesnt always form hexagonal structures, there are many shapes of snow.

hail = a liquid or partially melted phase in the process, most likely involving a re-freezing event prior to reaching the ground. 

Ive never learnt much about snow formation, but i can tell you about hail. 
In a cumulonimbus cloud, water is cooled below freezing point but it hasnt frozen, called 'supercooled' water. supercooled water will freeze when it comes into contact with something that can start the crystal growth, such as a dust particle or some frozen water. So supercooled water is blowing around in the cloud, being pushed up by updraughts and falling back down when the updraughts cant hold its weight any more. It will cycle around the cloud falling and then riding updraughts back up again, all the while it will be accumulating water (some of it supercooled liquid water, some of it water vapour) until it doesnt find an updraught strong enough to hold it in the cloud and it falls out. Because the supercooled water is liquid and doesnt necessarily freeze instantly, the stone will be wet on the outside, which means stones will stick to each other by touching and then freezing. the sticking together of stones into larger stones makes the irregular surface of the ""random chunks"".
The larger a hail stone, the longer it spent accumulating while blowing around in the cloud. Which generally means larger clouds with stronger updraughts capable of suspending larger particles. ",null,0,cdnmhop,1rid0v,askscience,top_week,1
Jeffy_Weffy,"Slow chemical reactions are happening. At one end, the reaction wants to take in electrons. On the other end, the reaction wants to give up electrons. The only way for the electrons to flow to complete these reactions is to go through the device you're powering.",null,0,cdnkh2z,1rifop,askscience,top_week,11
chillichill,"Lithium-ion batteries are common so I'll use those as an example. A battery has 4 major components, a cathode, anode, electrolyte and wires to connect the electrodes and complete the circuit (plus all the other bits that hold them together). This circuit is attached to a power supply during charge (to provide electrons) and a device when discharging (to be powered by movement of electrons).  The cathode and the anode are materials which can hold Lithium, while also allowing it to leave the structure reversibly. The electrolyte is a material which allows lithium to pass through, but not electrons.
When a lithium battery is being charged lithium ions (Li+) move from the cathode to the anode, through the electrolyte. When the Li+ reaches the anode it takes an electron from the power supply to form a (relatively) stable state. The Li is stored in the anode until a device that requires power is connected (discharging the battery). 
When a device that needs power is attached, the Li in the anode releases an electron to form Li+ which moves to the cathode. The released electrons cannot travel through the electrolyte therefore travel around the circuit, powering the device. At the cathode the Li+ recombines with an electron from the circuit. Once all the Li has moved from the anode to the cathode, the battery is completely discharged. ",null,0,cdnoov8,1rifop,askscience,top_week,1
LuklearFusion,"It really depends on the physical implementation of the qubits, as there are many kinds. An incomplete list of things that people use as qubits are:

1. The electronic structure of ions or atoms, which will be confined to some region of space by some sort of electromagnetic ""trap"".

2. A single electron's spin, where the electron has been trapped in a solid state system; a so called ""quantum dot"".

3. Superconducting circuits which have Josephson junctions can also be used as qubits.

4. Optical qubits use polarization or optical modes in light as qubits.

Each kind of qubit is stored (I'm assuming by stored you mean kept free from noise) and manipulated differently. Is there a particular kind you're interested in?",null,0,cdnnt7i,1rifph,askscience,top_week,6
DanielSank,"Whenever people ask about this I recommend reading [this post](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp). It refers only to superconducting circuit qubits, but it's definitely worth a read.",null,0,cdnvaq9,1rifph,askscience,top_week,1
LuklearFusion,"It really depends on the physical implementation of the qubits, as there are many kinds. An incomplete list of things that people use as qubits are:

1. The electronic structure of ions or atoms, which will be confined to some region of space by some sort of electromagnetic ""trap"".

2. A single electron's spin, where the electron has been trapped in a solid state system; a so called ""quantum dot"".

3. Superconducting circuits which have Josephson junctions can also be used as qubits.

4. Optical qubits use polarization or optical modes in light as qubits.

Each kind of qubit is stored (I'm assuming by stored you mean kept free from noise) and manipulated differently. Is there a particular kind you're interested in?",null,0,cdnnt7i,1rifph,askscience,top_week,6
DanielSank,"Whenever people ask about this I recommend reading [this post](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp). It refers only to superconducting circuit qubits, but it's definitely worth a read.",null,0,cdnvaq9,1rifph,askscience,top_week,1
FlavaFlavivirus,"Yes! I work with Alphaviruses; these particles contain a fusion peptide which allows the contents of the capsid to enter the cytoplasm of the cell, by fusing the two membranes together and inducing a conformational change in the structural proteins. 
",null,0,cdnpmde,1rigsa,askscience,top_week,2
captsuprawesome,"I would not characterize it as ""catalyzing their own import"" but many proteins are capable of penetrating the cell membrane.  HIV-1 Tat is a well studied protein that can do such a thing.  You may be interested in this summary of [cell-penetrating peptides](http://en.wikipedia.org/wiki/Cell-penetrating_peptide).",null,0,cdnsl21,1rigsa,askscience,top_week,1
bearsnchairs,[Transferrin](http://en.wikipedia.org/wiki/Transferrin) is a neat protein for that. It carries iron into cells. When transferrin binds to its acceptor on the surface of cells it initiates endocytosis and is taken up by the cell. You can attach transferrin to a particle or protein of interest to incorporate it into cells. ,null,0,cdnvctl,1rigsa,askscience,top_week,1
skleats,"Check out the dog genome sequence - there's lots of great examples of the role of repeated sequences in the selection history of various breeds. For some good sources:

[Here's](http://genomebiology.com/2011/12/2/216) an overview of the dog genome, with some info about repeated sequences.

[Here's](http://www.pnas.org/content/107/3/1160.full) a good rundown of the genomic variety between breeds.

And, for all the money, [here's](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1356118/) a study showing that the diversity is linked to SINEs.

Evolution gets driven by selection on random variation, but repetitive sequences drive random variation.",null,0,cdnwr9q,1rih74,askscience,top_week,2
astazangasta,"Most of the genomes that are full of crap like LINEs and retrotransposable elements are higher eukaryotes. Prokaryotic genomes are usually much tighter and filter these sorts of things out. That is probably because higher eukaryotes can tolerate the addition of some extra sequence in the genome - over the course of a large multicellular organism's lifetime, replicating a few extra base-pairs worth of sequence every time a cell divides is not that big a deal. A prokaryote, on the other hand, can optimize over each division, since each replication cycle produces a new generation.

This probably means that repetitive elements in the (e.g.) human genome are mostly harmless crap - they are annoying, but not that important. But if we could get rid of them (like prokaryotes), we probably would.",null,3,cdnuuqn,1rih74,askscience,top_week,1
Astrokiwi,It's most likely flat. [This post in the FAQ](http://www.reddit.com/r/sciencefaqs/comments/v97po/is_the_universe_infinite/) should be useful.,null,0,cdnkzfg,1riij0,askscience,top_week,3
Arrogus,"Loud noises and bright lights are damaging because of the energy they carry; scents, on the other hand, are merely particles suspended in the air. Sure, many chemicals could do serious damage to your olfactory receptors if you inhaled them through your nose, but it would be because of their reactivity, not their pungency. In such a scenario, damage to your lungs would probably be your primary concern.",null,38,cdnmjtk,1rim55,askscience,top_week,199
Zukuto,"/u/Arrogus has it. it isnt that they are too *smelly* that breaks your nose, but that they are comprised of *toxic* fumes; sometimes they are smelly and other times not.

one time i had to clean out a Hair salon next to a business i was working for in a strip mall; i was the only one who posessed a mask that also had eye protection. the salon owner had tried to get a stain off the floor using Bleach and Ammonia. i let all her hoses run onto the floor and pushed the watery mustard gas out of the building before calling the Fire Dept and a HAZMAT team. 

i got a free haircut for my trouble. the salon owner got the shock of her life.",null,0,cdnymdg,1rim55,askscience,top_week,8
ubcokanagan,"No, you perceive smells when aromatic compounds bind neurons in your nasal passages.  The binding causes these neurons to fire which send a signal to your brain letting you know you just stepped in dog crap.  A very strong smell will innervate many neurons but it wont damage them.

If the odour is present for a long period of time, desensitization of the neurons will occur, and they will be less likely to fire in the absence of an increase in concentration of the aromatic molecules.  This is why smokers don't realize that they smell terrible all the time (well that and any damage caused by the smoke).

If the strong smell is caused by something toxic then yes it can cause damage, but I believe this would be caused by a property of the offending chemical, as opposed to it overstimulating a nerve.",null,0,cdo0u8c,1rim55,askscience,top_week,3
null,null,null,0,cdo28pj,1rim55,askscience,top_week,2
Philosophisation,"As with any sensory input it can be damaged via chemicals. Your chemoreceptors in your sinus will not however get destroyed by excessive use. Imagine a bathtub with the plug pulled. This would be ~chemoreceptor being overloaded with scent molecules or similarly shaped molecules at least. Anything that fits through goes through and is registered. But add oil and hey? It doesn't go through for a while. This is one reason for desensitized smell. Another is that the sensory nerve endings present to receive the signal from the sensory organelle(dendrites) fires so often that the brain starts filtering it out as useless signal, same as white noise. So no excessive safe smell will desensitize but only harmful molecules may ruin smell receptors.",null,0,cdo2jpc,1rim55,askscience,top_week,1
freeze4111,"All odors, indeed anything that gets in your nose, damages it to a very small extent. The strength of the odor isn't necessarily the measure for its destructive capability however; mostly it is corrosive acids or things like smoke which do the real damage: look up tobacco smoking and its damage on the sense of smell to get an idea: http://www.ncbi.nlm.nih.gov/pubmed/22776624. 

Heavy odors, like blue cheese or something like that, may briefly block a number of receptors making your sense of smell not as good as what it could be, but this is very temporary (no more than a few seconds). In addition, your brain can adapt or habituate to odors, making them less noticeable (this is harder to do as the odor gets stronger). You'll probably notice this with your own perfume/deodorant throughout the day. 

Any damage done to your sense of smell is much, much easier to recover from compared to other senses because the neurons involved can turnover and regenerate (these are the only neurons that can). Some environmental experiences can make the turnover slower (as can age) but overall your sense of smell will recover from anything you throw at it. 

Something I find interesting- the neurons that take information from your sense of smell transmit that information to the brain through neurons passing a structure called the cribiform plate via little holes. If these neurons are severed, they can regenerate, but usually can't find their way through the holes again; a case of this unique quality being utterly useless! 

",null,0,cdo3j4q,1rim55,askscience,top_week,1
paulHarkonen,"I work in the natural gas industry and thus work with odorant (that rotten eggs smell in gas).  Odorant is one of the most powerful smells around but all the health concerns surrounding it involve how your body reacts to strong odors.  Very strong negative smells can pose a nausea risk, along with some breathing concerns because your body expects strong bad smells to also be toxic.  None of the Msds information covers permanent damage to your sense of smell.  (Although there is a short term effect as your nose becomes overwhelmed by the one strong odor and stops caring about other weaker odors).

Its not a super scientific source, but there has been a fair bit of testing to create the MSDS information.",null,0,cdo8mi4,1rim55,askscience,top_week,2
Dr_JA,"The most important reason strong odors do not damage your sense of smell is because your odorant receptors are replaced regularly - all receptors are replaced every 2 weeks. Thus, even if there were chemicals that would covalently bind to your receptors and render them useless, these receptors will eventually be recycled and you're good to go. 

It is a myth that you destroy your sense of smell by working e.g. in an organic chemistry lab - it is just your brain getting used to it. If you took an extended break and came back, you would smell it like everyone else.

As you get older, your sense of smell diminishes because the bit in the skull where the nerves go through gets clogged, and the nerves can't transmit the signals properly anymore.",null,0,cdoz0ev,1rim55,askscience,top_week,1
Arrogus,"Loud noises and bright lights are damaging because of the energy they carry; scents, on the other hand, are merely particles suspended in the air. Sure, many chemicals could do serious damage to your olfactory receptors if you inhaled them through your nose, but it would be because of their reactivity, not their pungency. In such a scenario, damage to your lungs would probably be your primary concern.",null,38,cdnmjtk,1rim55,askscience,top_week,199
Zukuto,"/u/Arrogus has it. it isnt that they are too *smelly* that breaks your nose, but that they are comprised of *toxic* fumes; sometimes they are smelly and other times not.

one time i had to clean out a Hair salon next to a business i was working for in a strip mall; i was the only one who posessed a mask that also had eye protection. the salon owner had tried to get a stain off the floor using Bleach and Ammonia. i let all her hoses run onto the floor and pushed the watery mustard gas out of the building before calling the Fire Dept and a HAZMAT team. 

i got a free haircut for my trouble. the salon owner got the shock of her life.",null,0,cdnymdg,1rim55,askscience,top_week,8
ubcokanagan,"No, you perceive smells when aromatic compounds bind neurons in your nasal passages.  The binding causes these neurons to fire which send a signal to your brain letting you know you just stepped in dog crap.  A very strong smell will innervate many neurons but it wont damage them.

If the odour is present for a long period of time, desensitization of the neurons will occur, and they will be less likely to fire in the absence of an increase in concentration of the aromatic molecules.  This is why smokers don't realize that they smell terrible all the time (well that and any damage caused by the smoke).

If the strong smell is caused by something toxic then yes it can cause damage, but I believe this would be caused by a property of the offending chemical, as opposed to it overstimulating a nerve.",null,0,cdo0u8c,1rim55,askscience,top_week,3
null,null,null,0,cdo28pj,1rim55,askscience,top_week,2
Philosophisation,"As with any sensory input it can be damaged via chemicals. Your chemoreceptors in your sinus will not however get destroyed by excessive use. Imagine a bathtub with the plug pulled. This would be ~chemoreceptor being overloaded with scent molecules or similarly shaped molecules at least. Anything that fits through goes through and is registered. But add oil and hey? It doesn't go through for a while. This is one reason for desensitized smell. Another is that the sensory nerve endings present to receive the signal from the sensory organelle(dendrites) fires so often that the brain starts filtering it out as useless signal, same as white noise. So no excessive safe smell will desensitize but only harmful molecules may ruin smell receptors.",null,0,cdo2jpc,1rim55,askscience,top_week,1
freeze4111,"All odors, indeed anything that gets in your nose, damages it to a very small extent. The strength of the odor isn't necessarily the measure for its destructive capability however; mostly it is corrosive acids or things like smoke which do the real damage: look up tobacco smoking and its damage on the sense of smell to get an idea: http://www.ncbi.nlm.nih.gov/pubmed/22776624. 

Heavy odors, like blue cheese or something like that, may briefly block a number of receptors making your sense of smell not as good as what it could be, but this is very temporary (no more than a few seconds). In addition, your brain can adapt or habituate to odors, making them less noticeable (this is harder to do as the odor gets stronger). You'll probably notice this with your own perfume/deodorant throughout the day. 

Any damage done to your sense of smell is much, much easier to recover from compared to other senses because the neurons involved can turnover and regenerate (these are the only neurons that can). Some environmental experiences can make the turnover slower (as can age) but overall your sense of smell will recover from anything you throw at it. 

Something I find interesting- the neurons that take information from your sense of smell transmit that information to the brain through neurons passing a structure called the cribiform plate via little holes. If these neurons are severed, they can regenerate, but usually can't find their way through the holes again; a case of this unique quality being utterly useless! 

",null,0,cdo3j4q,1rim55,askscience,top_week,1
paulHarkonen,"I work in the natural gas industry and thus work with odorant (that rotten eggs smell in gas).  Odorant is one of the most powerful smells around but all the health concerns surrounding it involve how your body reacts to strong odors.  Very strong negative smells can pose a nausea risk, along with some breathing concerns because your body expects strong bad smells to also be toxic.  None of the Msds information covers permanent damage to your sense of smell.  (Although there is a short term effect as your nose becomes overwhelmed by the one strong odor and stops caring about other weaker odors).

Its not a super scientific source, but there has been a fair bit of testing to create the MSDS information.",null,0,cdo8mi4,1rim55,askscience,top_week,2
Dr_JA,"The most important reason strong odors do not damage your sense of smell is because your odorant receptors are replaced regularly - all receptors are replaced every 2 weeks. Thus, even if there were chemicals that would covalently bind to your receptors and render them useless, these receptors will eventually be recycled and you're good to go. 

It is a myth that you destroy your sense of smell by working e.g. in an organic chemistry lab - it is just your brain getting used to it. If you took an extended break and came back, you would smell it like everyone else.

As you get older, your sense of smell diminishes because the bit in the skull where the nerves go through gets clogged, and the nerves can't transmit the signals properly anymore.",null,0,cdoz0ev,1rim55,askscience,top_week,1
gredders,"**What makes a neutron stick to a proton?**

Neutrons and protons (collectively known as nucleons when discussing their role in nuclear physics) are bound in the nucleus by the strong nuclear force. This interaction is a powerful attractive force which acts over a range of only a few femtometres (which is roughly the diameter of a nucleon). Beyond that the force quickly drops to zero. 

**What determines the number of neutrons and protons?**

As you increase the number of protons in a nucleus you increase the repulsive electrostatic (Coulomb) force due to their positive charge. Now this Coulomb force is weaker than the nuclear force, but it also acts over a long range. That means that as the size of the nucleus increases the strong force 'saturates' while the coulomb repulsion just keeps getting bigger, and causes the nucleus to become unstable.

Adding neutrons, however, contributes to the strong force, but does not increase the coulomb repulsion (since the have neutral charge), therefore heavier nuclei typically need more neutrons than protons for it to remain stable. 

[See on this plot of proton number vs neutron number how the line of stable nuclei tends to bend round to the neutron-rich side of the chart](http://nuclearpowertraining.tpub.com/h1019v1/img/h1019v1_38_4.jpg)


However, it is obvious that this isn't the whole story, since too many neutrons makes a nucleus unstable, too. This is due to the Pauli exclusion principle, which forbids two particles from occupying the same quantum state. Protons and neutrons differ by their isospin quantum number, and can therefore share the same energy state without violating the exclusion principle. Two neutrons (or two protons) are indistinguishable from each other and cannot share the same energy state. Therefore when you add only one kind of nucleon to a nucleus they are forced to 'stack up' more quickly than adding both neutrons and protons, which is to say they have to occupy increasingly higher energy states. In this case, it isn't long until one of the nucleons is forced to occupy a state of higher energy than the binding energy of the nucleus, and it cannot form a bound system. 

In reality, the situation is more complex than this and is very difficult to model. The best so far is the [nuclear shell model](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). Nuclear structure is an active area of research (and one in which I am doing a PhD), and we do not have a complete picture of what goes on inside the nucleus at this point. ",null,3,cdnrwhp,1rivmw,askscience,top_week,8
iorgfeflkd,"1. A residual strong nuclear force.

2. Protons repel each other electrically, so as you add more protons, more neutrons are required to keep them apart (that's a simplistic explanation). Nuclei with too few neutrons break apart.",null,1,cdnpwlj,1rivmw,askscience,top_week,4
Ejb90,"
&gt;- what makes a neutron stick to a proton.

There are four fundamental forces in the universe. One of the more familiar ones is the electromagnetic force. This has two ""possibilities"" - positively charged or negatively charged.Another one, the one we're interested in here, is the ""Strong nuclear force"", which has three ""possibilities"" which we call colour charge (though that's just a name).
Protons and neutrons are a type of particle called hadrons, which means they're made up of three fundamental particles called quarks. These quarks are all oppositely colour charged so they ""stick"" together (analogous to how oppositely charged particles like protons and electrons attract). This force is very strong (hence the name), but acts over a short distance. Inside the nucleus there is some residue from each of the forces inside the protons and neutrons, which leaks out and also causes the protons and neutrons stick together. This is called the residual strong nuclear force.
This is simplified and ignoring some important facts but it's the nuts and bolts of it

For the number of neutrons in different isotopes of elements, this relies on the ""Nuclear Shell Model"" which is analogous to the electron shell model you're familiar with.
This revolves around the fundamental idea of the Fermi exclusion principle - that particles with certain identical properties can't be in the same state.
In the nucleus there are ""shells"" in which the nucleons sit. Sometimes however the exclusion principle stops the nucleons being in the lowest energy state, ah
Md hence that nucleon is unstable, and so those isotopes will get rid of it to become more stable - they will decay. Some however are stable in energy, so won't kick nucleons out.
Hope this helps!",null,1,cdnqo6w,1rivmw,askscience,top_week,5
paperanch0r,"You will experience half of the pain relief for the entire duration. This is an effect referred to as ""dose/response curve"", which is basically a comparison of the amount of substance to the strength of its effect. The half-life(time it takes a substance to break down in the body) of a substance remains the same regardless of dose, but its effects will change with dose.
 
Edit: grammar",null,5,cdnqx4y,1riwul,askscience,top_week,6
Dannei,"I'm almost certain 65mph would be more efficient for any car. Generally, it seems cars are most fuel efficient around 40-50mph, although there's a lot of conflicting results out there...

As you go at higher speeds, you have to start fighting air resistance more, which reduces your fuel economy for obvious reasons. As this increases with the square of speed, it's a much more important factor at high speed than low - 85mph has twice the air resistance of 60mph.

At the lower end, I believe you're less fuel efficient due to not being in top gear (I'd appreciate it if someone could find a decent source on that!). Once you're in that, your engine is at the minimum RPMs possible for your speed, meaning that you reduce losses within the engine itself - the faster the engine is turning, the more energy is lost to friction.

Therefore, you want to aim for a relatively low speed whilst in top gear, which is the minimum of the combination of mechanical losses and air resistance.",null,1,cdnrpcv,1rixux,askscience,top_week,11
wmeredith,"There's not really enough information to answer this. Energy efficiency is going to depend on how the speed was achieved and then sustained much more so than what the actual velocity is. This would be a combination of mechanical factors of gearing, engine type, thrust delivery, etc...",null,0,cdnptnn,1rixux,askscience,top_week,2
baldeagleNL,"I always learned that the most efficient way to drive is in the highest gear with the least amount of rotations per minute. In that way, the least energy is spilled on useless movement. However, the actual optimal speed is probably a bit more complicated. You'll have to find out at what RPM your engine has the best (*is there even an English word for* **rendement**).",null,1,cdnpw0p,1rixux,askscience,top_week,3
sweaterhead,"Assuming that the lower speed has more mpg's, and you are traveling the same distance, driving at the lower speed will be more efficient, because it's miles per gallon that defines fuel efficiency specifically, not miles per hour. ",null,1,cdnqi8y,1rixux,askscience,top_week,3
xavier_505,"If you are in any modern passenger vehicle, 65mph will be  more efficient than 80. The engine will be running for longer however the energy required to overcome drag increases with the cube of velocity, and consumer vehicles are not designed to have their peak efficiency at 80 mph.",null,0,cdns0vb,1rixux,askscience,top_week,1
DanielSank,"This is one of my favorite questions.

&gt; Why do mirrors flip along a vertical axis, while still preserving up/down orientation?

They don't! This is a result of your brain doing extra work and fooling itself. Think about this: when you look at another person face to face, you are used to seeing their left side on the right side of your field of vision. Of course this is true when you look at anything that has a ""front."" You always see what would be the left side in the object's own frame of reference on your visual field's right side. Because of this your brain is accustomed to mentally register the switch.

The vertical direction isn't flipped because normally you and the person (or object) you're looking at are both oriented upward from the ground (I guess you could say this is because we're all subject to the ground being on the same side).

When you look in a mirror, *nothing is flipped*. It's a plain old mirror image. It *looks* flipped because you're *not* seeing the side reversal that you're used to seeing when you face another person.

A good way to visualize this is as follows. Imagine standing in a room facing in one direction. Now imagine you make a copy of yourself right where you're standing. Now you take a step forward and to the side, and then turn around and face your copy. If you visualize this in action you'll see that your right eye is directly opposite your copy's left eye. However, your feet and your copy's feet are both on the ground. This is the reversal that you're used to seeing when you face other people and objects, and this reversal does *not* happen when you look in a mirror.

EDIT: grammar",null,7,cdnqa8v,1rixyo,askscience,top_week,29
IX-103,"The mirror is not flipping along the vertical axis, but instead in the front/back axis. You can see this by noting that if you point your hand out toward the mirror the reflection of your hand appears to be getting closer (movement away is changed to movement toward). You don't usually see people flipped front/back, so you perceive the person of your reflection as having their left and right sides swapped.",null,1,cdnxgtn,1rixyo,askscience,top_week,14
Dorcus0,"Take a picture of yourself. Go to a mirror, and compare the picture of yourself with your mirror image. Or, as a thought experiment, put gloves on your right hand, then look in the mirror.

A mirror image is more accurately thought of as you pushing yourself through the mirror, not a 180 degree rotation.",null,0,cdo4jr4,1rixyo,askscience,top_week,1
SMURGwastaken,"glucose doesn't require insulin to be metabolised; rather insulin causes cells to absorb glucose. It's a significant distinction since fructose and glucose metabolism are similar in that the same kind of chemistry occurs, albeit with different enzymes.

The short answer is that the small differences between fructose and glucose are enough for the body to differentiate them. IIRC, glucose has an aldehyde functional group whereas fructose has a ketone. 

It's more complicated than that however because insulin *does* affect fructose uptake in skeletal muscle cells, just not in the liver. This is because fructose is not normally metabolised in all cells and is processed by the liver first, so presumably it's beneficial for all fructose to enter the liver so that it can be phosphorylated into a form which can be used elsewhere.",null,1,cdnqmo2,1riyww,askscience,top_week,10
fartprince,"Just because chemicals are similar in structure doesn't mean they are necessarily processed the same way. Indeed, even differences in enantiomers (ie same exact chemical structure but just mirror-images of each other) can lead to very different effects. The classic case is thalidomide, where one enantiomer was teratogenic while the other wasn't. 

As for glucose, insulin's active site contains residues that can directly bind/interact with the chemical structure of glucose but probably not fructose. Why it evolved this way, one can only speculate. There are many other examples where similar chemical structures can have very different effects (and also very different chemical structures can elicit very similar effects).",null,0,cdnqqds,1riyww,askscience,top_week,5
mutatron,"What SMURGwastaken said. Molecules and the molecules that operate on them are like keys and locks. If you live in my apartment complex, my door key may have the same general shape as yours, but it won't unlock your door. It's only a small difference, but it's enough.

There are [a number of sugar molecule transport proteins](http://en.wikipedia.org/wiki/Glucose_transporter), which are embedded into the walls of cells and often only admit sugar molecules at the request of a third signaling molecule, or even a signaling complex of molecules. Also, the number of transporters in a cell's walls is not fixed, but can be regulated by other molecules.",null,0,cdnqw7e,1riyww,askscience,top_week,2
threegigs,"But, it doesn't *require* insulin to be metabolized. Insulin simply causes more GLUT4 expression, resulting in *increased* uptake.",null,0,cdnqfge,1riyww,askscience,top_week,1
Criticalist,"Yes, there is a wealth of evidence from animal and human studies that links cholesterol to heart disease.

In animals that have genetic defects that lead to high LDL cholesterol, tend to develop atherosclerosis, while those species that have low levels don’t. In humans, there is the [Framingham Heart Study](http://www.ncbi.nlm.nih.gov/pubmed/?term=9603539), [The Multiple Risk factor Intervention Trial](http://www.ncbi.nlm.nih.gov/pubmed/?term=3773199) and the [Lipid Research Clinics Trial](http://jama.jamanetwork.com/article.aspx?articleid=391065) which are large scale trials demonstrating a clear association between levels of LDL cholesterol and the development of heart disease. The higher the level, the greater the risk.

In people who have a genetic defect that leads to very high blood cholesterol levels there is a very early onset of heart disease, even if they have no other risk factors.

When LDL cholesterol is reduced in clinical trials there is a clear benefit, and this occurs with cholesterol lowering drugs other than statins, and lifestyle interventions. These include the [STARS trial](http://www.ncbi.nlm.nih.gov/pubmed/1347091), the [Lifestyle Heart Trial](http://www.ncbi.nlm.nih.gov/pubmed/?term=1973470) and the [NHLBI type II study](http://www.ncbi.nlm.nih.gov/pubmed/6360415).

LDL cholesterol has been comprehensively studied and the results are very strongly suggestive that it is an important, modifiable risk factor for heart disease. 

",null,0,cdo50t1,1rj0sn,askscience,top_week,9
Philosophisation,"Yes there are. Let me give the scientific basis for why ""cholesterol causes heart diseases"". Cholesterol is a steroid: it goes into cells and tells them what to make. There are two common types of cholesterol. Low density lipoprotein cholesterol or LDL-Cholesterol and High Density Lipoprotein Cholesterol or HDL-Cholesterol. Low density cholesterol is bad for you because your body has a harder time absorbing it and it will over time accumulate as plaque on your artery walls. Think of this process like a pipe getting clogged by kitchen fat. If you flush the fat in small chunks then the chance that it sticks to the sides is lower due to lower surface area. But if you do it as a bucket of fry fat then it will coat the sides and definitely leave some residue.So really you do not need a study to understand why and how LDL cholesterol impacts you. Here is a tip to maintain healthy cardiovascular system or ""heart"" as you put it: Eat foods low in both LDL and Mono-Sodium Glutamate or MSG which helps the cholesterol aglutamate (stick together).edit to pedants: Yes this is a simplification.",null,3,cdo2ocu,1rj0sn,askscience,top_week,1
_NW_,"Diamonds are the hardest material, but are brittle.  They can be cleaved with a hammer and chisel.  To get it to the final shape, they are ground on a wheel covered in oil and diamond dust.  Diamonds can also be cut with a steel blade lubricated with oil and diamond dust.",null,0,cdogkuv,1rj19e,askscience,top_week,2
Freeoath,"I saw noone had commented so I though I might explain it with what I know.

The index finger is the prefered finger amongs most people, some use the middle finger.
Why we use the index finger more than the other fingers is most likely due to the fact that it is more dextorious and flexible than any of our other finger. The index finger is also the most sensitive, giving back the best response to touch. 
Using the index finger to point or press does not strain the hand or any of the other fingers, like thr other fingers do.
The middle finger can be used but is universally seen as a insult. 
Even from the age of 1-2 kids prefer to use their index finger to point over their other fingers.

I have not studied human anathomy but this is what I know.
Sorry for spelling errors, on a phone and english is not my main language ",null,0,cdo3h5x,1rj19j,askscience,top_week,3
SMURGwastaken,"To elaborate on some of what /u/Freeoath said, the index finger is (aside from the thumb) the only one of the digits on our hand capable of wholly independent action. For example, try moving your little finger without moving your ring finger. You can't (or you shouldn't be able to anyway). Now make a fist and try extending your ring finger without moving you middle finger. Again, you can't (or shouldn't be able to). Now open your hand again so all your fingers are fully extended and attempt to touch your palm with your middle finger whilst keeping the ring finger fully extended. You can't (or shouldn't be able to). 

This is basically down to the fact that the tendons in your forearm control multiple finger functions, leaving the index finger as the only one which can operate independently of the others. This makes it the most flexible and best for precise operation, which is probably why it has evolved to be the most sensitive and responsive, and why we tend to use it above all others. The middle finger is the next best thing, which is why some people use that one too.",null,0,cdo55sf,1rj19j,askscience,top_week,2
SMURGwastaken,"I'm not sure I understand what you're asking... Plants do not conduct nitrogen fixation, rather bacteria do it for them (forming colonies in root nodules grown specially by the plant in the case of things like legumes). As for the 'advantage' of doing so, it simply comes down to the fact that plants (like all complex life) require proteins and are (generally) autotrophic organisms. Since proteins require Nitrogen and the only source of non-organic Nitrogen is the N2 in the air, it is beneficial for a plant to culture its own colonies of Nitrogen-fixing bacteria rather than relying entirely on those free in the soil.

I think what you mean by ""Nitrogen-fixing plants"" are plants that grow root nodules in which Nitrogen-fixing bacteria then grow. It only helps competing plants if the host plant dies, at which point it doesn't care - and in the mean time it effectively has its own nitrate factory.",null,0,cdnqz4c,1rj1hy,askscience,top_week,2
cladocerans,"First question: Yes and no. Plants that have symbiotic colonies of nitrogen-fixing bacteria primarily promote their own growth. The nitrogen-fixers are located in their roots, and they get first access to that resource. They do, however, facilitate the growth of competitors--some nitrogen is lost to the soil, and becomes available to other nearby plants. Also, more nitrogen is available to the community as the symbiotic plants are consumed and decomposed. It's basically a win-win situation--the symbiotic plant gets most of the N (minus the loss of some photosynthate for maintaining the bacteria), and the other plants get a fertilizer boost tangentially.

As for arising in homogenous or austere environments--I don't really understand what you are asking. Nitrogen is limiting in most terrestrial ecosystems (i.e. it is the one nutrient plants need the most relative to how common it is). The habitat doesn't have to be homogeneous or stressed for a N-fixing symbiosis to be advantageous.",null,0,cdnsl1l,1rj1hy,askscience,top_week,1
South_park_fan,"Look at the stability of the intermediate carbocation compared with the effects of steric hinderance, 1˚ (primary) carbons will form unstable carbocations, whilst 3˚ carbons form much more stable carbocations. This means that Sn1 is not favoured for 1˚ carbons, but is favoured for 3˚ carbons. Remember, Sn1 requires a stable intermediate carbocation. Conversely, 3˚ carbons have high steric hinderance and thus are not easily attacked by nucleophiles, whilst 1˚ carbons are easily attacked by nucleophiles as there is space for the nucleophile to come into contact with the carbon. So Sn2 is favoured by 1˚ carbons.

A solvent is not termed as a reactant as it is just what is used in order to make the reactants come into contact with each other.

Nucleophiles tend to be strong bases and will usually carry a negative charge. They always have a lone pair of electrons. ie OH-

You can find a substrate by looking for leaving groups, leaving groups are  weak conjugate bases (Cl-,Br-) ect. Your substrate will be some kind of organic chain and will always be fully saturated at the carbon at which the substitution is taking place.",null,0,cdo48c0,1rj285,askscience,top_week,4
__Pers,"I'm not sure what specific chaotic pendulum system you're describing, but let's consider a damped pendulum subject to a sinusoidal forcing, a chaotic pendulum yielding chaotic orbits. 

Orbits of the chaotic pendulum's dynamics will be 1D trajectories (curves) in some sort of smooth, abstract space (a ""manifold"" in the mathematical parlance). In the case above, the three ""axes"" of this space could be chosen to be phase, angular frequency, and phase of the driver, for example, with the understanding that phase and driver phase wrap back on themselves every 2 pi. A Poincare map is made by looking at the structure of the crossings of an orbit through a lower-dimensional surface embedded in the 3D. For instance, you could make a 2D map whose coordinates are the phase and angular frequency of the pendulum every time the driver's phase reaches 2 pi.

A reason one might make such a Poincare map (besides the fact that they are rather pretty) are that properties of the map such as behavior around fixed points reveal insight into the underlying dynamics. Also, they tend to be fractal sets whose dimensionality and other properties can reveal something of the chaotic nature of the underlying dynamics.",null,0,cdo80qu,1rj2ho,askscience,top_week,1
zmbbmz,Yes this happens all over the world.  This is from the clouds insolating the Earth.  The energy from the sun comes to Earth in the form of short wave energy being that the molecules are excited from the heat of the sun and move faster.  The gasses in the atmosphere allow these short wave energies to pass through.  When the short wave comes into contact with the ground the energy is taken in and released as long wave energy.  The gasses in the atmosphere do not allow this long wave energy to travel through as readily and hold most of it in.  So when it is cloudy there are more of these gasses in that area preventing more of the long wave energy getting out which in turn heats up the area.  This is the cause known as the 'greenhouse effect'.,null,1,cdnzgvg,1rj31i,askscience,top_week,1
RelativisticMechanic,"I've never seen a good one, but it's important to realize that even such a ""3D representation"" would fail to show the ever-so-important *time* dimension and the curvature thereof.",null,0,cdo280s,1rj34y,askscience,top_week,5
iorgfeflkd,"http://cdn-static.zdnet.com/i/story/50/00/018934/18935.jpg

http://d1jqu7g1y74ds1.cloudfront.net/wp-content/uploads/2008/02/noexcision_full_7.png

https://www.nikhef.nl/uploads/pics/Colliding_black_holes.jpg

But really, see /u/reativisticmechanic's reply.",null,0,cdo8wk8,1rj34y,askscience,top_week,2
uncleawesome,Gravity isn't something you can see. It is an invisible force. Those pictures represent gravity as a low spot in a plane. If you roll a ball on the plane it will roll to a low spot. If the ball is going faster it might roll thru part of the indent and move off in a new direction. It could also orbit around the low for a while until its speed slowed enough for it to fall down into the hole.,null,1,cdo2blz,1rj34y,askscience,top_week,1
iorgfeflkd,"The light-analogue of a conducting  (or insulating, semiconducting, etc) material is called a photonic crystal. There is a type of photonic crystal called a Whispering Gallery Resonator where the light can constantly be reflected around in a circle, being totally internally reflected from the edges. They don't last forever because if inherent imperfections in their construction.",null,1,cdo8qh4,1rj6t4,askscience,top_week,2
Osymandius,"I may have misinterpreted /u/Philosophisation's answer but I don't quite agree with all of it. He's right in that first we metabolise carbohydrates, then lipids, then protein, but some of the details don't quite sit right with me.

Initially we metabolise free blood glucose - this is plentiful after a meal, but insulin secretion rapidly triggers glycogenesis and blood glucose falls. We normally have enough glycogen to last us about 12-24 hours of starvation. By the time breakfast rolls around, you might /just/ be beginning to burn fat. 

Now we start mobilising adipose reserves. Stored fat (triacylglycerides) is mobilised by breaking it down into free fatty acids (FFAs) and glycerol. Glycerol is easily metabolised. FFAs are transported to the target tissue and imported into the mitochondria and undergo b-oxidation where they are broken down, 2 carbon chunks at a time, into acetyl CoA. This can be fed directly into the Krebs cycle and respiration proceeds as normal.

Eventually (now you're really starving) we start metabolising protein. This is bad for a number of reasons. Firstly and most importantly - we don't store energy as protein. Any protein in the body is there because it needs to be there, either structurally or functionally. Secondly, protein is full of nitrogen. We have to deaminate amino acids in order to metabolise them. This is energetically expensive and leads to conditions like hyperuricemia and other unpleasant ways to go.

Ketone bodies are also very important. The majority of your tissues are happy to burn anything more or less. The brain is considerably more picky. It will only burn glucose (either free blood glucose straight from food, or that stored in glycogen and remobilised) or ketone bodies. These are produced in the liver from fatty acid metabolism and circulated back into the blood. Their synthesis begins more or less simultaneously with the start of fat metabolism, albeit at a slow rate. As starvation proceeds, levels of ketone bodies continue to rise.

",null,0,cdo4e12,1rj98n,askscience,top_week,4
Philosophisation,Think of the order of magnitude for carbohydrates and also recall that monosaccharides are all that your cells eat. The bigger things are broken down in order to be consumed.Order of consumption:1Monosaccharides:GlucoseGalactoseFructose2Disaccharides:Sucrose= glu+fruLactose=gal+gluMaltose=glu+glu3Polysaccharides4Lipids5ProteinGetting to 5 is almost impossible and only occurs when there is extreme malnourishment as protein does not yield a large amount of net energy. ,null,0,cdo2rpi,1rj98n,askscience,top_week,2
medikit,"*Bordetella pertussis* is super infectious and was a significant cause of infant mortality. Tetanus is ubiquitous in the environment and a danger to all who are not vaccinated. Diphtheria used to be a significant cause of morbidity and mortality.

We try to combine vaccines when we can. It is much more efficient and also minimizes the amount of shots that people receive which improves the likelihood that people will comply with vaccination. There is not currently a single acellular pertussis vaccine separated from tetanus and diphtheria.

There are other formulations that include *haemophilus influenzae* type b (Hib), Polio, and even Hepatitis B. I assume you are familiar with Polio but Hib was quite deadly and has all but disappeared in the US after vaccination. Hepatitis B is more likely to cause chronic disease if acquired in infancy which can lead to Liver failure and/or Liver cancer.

Because pertussis (and measles) are so incredibly infectious (research this) you could make the best case for mandatory vaccination of these diseases. It would be possible to create a pertussis vaccine separate from Diphtheria and Tetanus so I wouldn't worry too much about this issue. You may want to know the difference precise difference between Tdap and DTaP (Hint: it has to do with concentration of each component).

This article will be helpful: http://www.npr.org/blogs/health/2013/09/25/226147147/vaccine-refusals-fueled-californias-whooping-cough-epidemic

Also be aware that we changed the pertussis vaccine to its current acellular version to avoid unwanted side effects but there is concern that it may not be as effective. Here is some very recent info about this: http://news.sciencemag.org/health/2013/11/whooping-cough-vaccine-does-not-stop-spread-disease-lab-animals",null,1,cdo05hx,1rj9fp,askscience,top_week,5
contactinhibition,"[DTaP](http://www.cdc.gov/vaccinesafety/vaccines/dtap/dtapindex.html) is short for Diptheria Tetanus acellular pertussis, which is composed of the Diptheria toxoid, Tetanus toxoid, and a protein formulated to mimic the pertussis bacteria. It replaced DTP, which used toxoids but whole pertussis bacteria that had been lysed (broken apart). Part of the effectiveness of the vaccine is how it is formulated-the toxoids are linked to the pertussis protein to increase effectiveness and provide protection. ",null,1,cdnyvq8,1rj9fp,askscience,top_week,3
Henipah,"Diphtheria and tetanus are also horrific diseases. Having single injections is going to reduce the likelihood of being covered for them. For instance after the pointless controversy over the MMR disease after Andrew Wakefield's fraudulent Lancet paper people tried getting separate measles, mumps and rubella vaccinations. This increases the chances of people missing the necessary jabs, e.g. rubella... which parents may be less worried about immediately for their children but with important public health implications. ",null,1,cdo33cf,1rj9fp,askscience,top_week,2
jamesinphilly,"&gt; Are there shots available on request to only give the pertussis vaccine?

Back in the old days when the vaccines were killed cells of the pathogen (called 'inactivated' vaccines), you could get the shots separate. This is because people often had allergies associated with them (really, many were just side effects, but that's how they got classified), so you'd get the vaccines a la carte. As we have moved to the acellular vaccines, you can't get the pertussis vaccine by itself. This is because the manufacturing process is much more expensive, there's no demand for it by itself, and there's no harm in getting an extra booster. And if you combine it with other vaccines (as other people have pointed out), it also means less shots and more compliance with doctor's visits. Also, storage is a pain, you have to chill these things or they denature easy. It's obviously much easier and cheaper to transport and keep chilled 1 vial vs 3. If our hospital had the option of a pertussis-only vaccine we would not buy it for this very reason.

Good luck on your essay",null,0,cdoz19a,1rj9fp,askscience,top_week,2
expandedthots,"You should think of schizophrenia as the typical psychotic disorder, as any other psychotic disorder is going to have a similar spectrum of symptoms as schizophrenia (bipolar with psychosis, psychotic disorder now etc). So the dopamine hypothesis says that some disruption in dopamine processing leads to dysfunctional signaling between multiple locations in the brain. The benefits of this that it does serve to decrease positive symptoms (hallucinations, delusions) and newer atypicals also are somewhat active in reducing negative symptoms (lack of speech, not caring about anything, no energy to do anything). The downside of this is that it disrupts dopamine signaling globally across the brain, and causes movement problems when the drug reaches high enough concentrations in these areas. Look at tardive dyskinesia for a picture into how debilitating these problems can be. 

Now, there are other theories about whats truly occurring in psychotic brains. Some people argue that dopamine antagonists are merely acting as symptomatic relief, meaning it isn't really targeting the mechanism of whats going wrong, just acting at a downstream site where the symptoms can be limited. I'd argue this could be true, because of the success of a specific atypical, clozapine, in treating treatment resistant schizophrenia. 

Clozapine is an antagonist of basically every receptor in your brain. Some people, such as Herbert Meltzer, argue that its actually an effect of clozapine on serotonin receptors that increases its efficacy over other antipsychotics. He has substantial evidence to back this up, but it again becomes a matter of mere receptor pharmacology. 

I'd suggest looking into the phospholipid theory of schizophrenia. It discusses how the neuronal composition of susceptible brains has a different composition of phosphoplipids, which can have significant downstream effects as far as inflammation and disrupted immune signaling and oxidative stress handling. Its probably a piece to the puzzle along with the receptor pharmacology. The reason I support it is because clozapine is the only antipsychotic that interacts truly with this system.

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3478065/
",null,0,cdpllb0,1rj9sv,askscience,top_week,1
syvelior,"Well, actually, the example you cite is a case of misperception in both directions.

Japanese doesn't have an /r/ sound - they have an alveolar tap /ɾ/ which is similar to the sound produced in most varieties of American English in the word butter (Tsujimura, 1996).

Bradlow et al. (1997) showed that extensive training on perceiving the /r/ - /l/ distinction with native Japanese speakers resulted in better production distinctions, strongly suggesting that incorrect productions stem from failing to perceive the differences between these sounds (and in fact, putting them in a category that is incorrect for both).


**References**:

Bradlow, A. R., Pisoni, D. B., Akahane-Yamada, R., &amp; Tohkura, Y. I. (1997). Training Japanese listeners to identify English /r/ and /l/: Some effects of perceptual learning on speech production. *The Journal of the Acoustical Society of America*, 101(4), 2299.

Tsujimura, N. (1996). *An Introduction to Japanese Linguistics.* Basil Blackwell: Oxford.",null,0,cdo3hnd,1rjast,askscience,top_week,6
paolog,"1. The sound doesn't exist in the speaker's language. For example, speakers of Romance languages may not be able to hear a difference between English /æ/ and /ʌ/.

2. Making a particular sound involves positioning the tongue, lips, etc in an unfamiliar way that a speaker, unlike a native speaker, has not been practising since childhood, so a more familiar sound that approximates it is substituted. English speakers learning French will often use /u/ for /y/ and /nj/ for /ɲ/.",null,0,cdo3suv,1rjast,askscience,top_week,1
shavera,"the positron's the only anti particle with its own special name. In diagrams though we'll just label it with an e^+ . It's just an old naming convention that's stuck around.

Mesons were so named because early particles seemed to fall into 3 categories, based on mass. Light ones were ""leptons,"" medium ones were ""mesons,"" and heavy ones were ""baryons,"" coming from the greek for light, middle, and heavy respectively. The muon was originally lumped in with the mesons because it has a mass similar to a pion.

Later on, after the discovery of the quark model, we realized that baryons had 3 quarks, mesons had a quark and an anti-quark, and leptons had no quarks (which reclassified the muon back into the lepton family where it belongs). 

Recently, we've also discovered 4-quark (2 quarks, 2 anti-quarks) bound states, but we've been rather dull and called them tetraquarks",null,0,cdnxn58,1rjc6k,askscience,top_week,3
MCMXCII,"A quark/antiquark pair is not necessarily bound. And a meson is not necessarily a quark/antiquark pair. A meson is a quark and an antiquark, but they don't have to be the same flavor.

What do you mean by ""an antiquark is not an anti particle like say an anti electron is a positron?"" Antiquarks *are* antiparticles just like positrons....",null,0,cdnxgkf,1rjc6k,askscience,top_week,2
blueyedlvrx01,"Ok, I think I understand your question. You are wondering if there is any gravitational pull on our atmosphere other than the gravitational pull of earth. The short answer is no. If there is such a thing, then the forcing is so weak that it is not considered in the governing equations we use to understand and forecast weather. There are terms that we do throw out due to them being relatively small in scale and complicated to calculate. Even with that, there is no term for what you are describing here. I

A few quick things to note that you may find interesting:

The atmosphere is deepest in the tropics due not only to the very warm temperatures experienced here, but also because of centripetal acceleration due to angular momentum from the rotation of the earth. Angular momentum is largely conserved by atmospheric motions, especially in the upper atmosphere. This is the reason for mid-latitude extratropical synoptic systems (aka cold-front/warm-front systems). 

I could also get into gravitational forces and buoyancy. A parcel of air (think of like a small cluster of air - I tend to think of a bubble of air for all intents and purposes) can become positively buoyant for many reasons. When this parcel becomes positively buoyant, it accelerates upward to try to become neutrally buoyant. It will overshoot this level of neutral buoyancy and then it will experience a downward acceleration. It will still overshoot this level it's trying to get to (the level where it will be neutrally buoyant) so it will  experience vertical motions over time much like a sine wave. Ever heard of gravity wave clouds? This is what is happening here. The downward acceleration it experiences is due to gravity. 

Edit:
Source: BS in meteorology and currently an atmospheric science graduate student. ",null,4,cdo3piw,1rjceb,askscience,top_week,4
blueyedlvrx01,"Ok, I think I understand your question. You are wondering if there is any gravitational pull on our atmosphere other than the gravitational pull of earth. The short answer is no. If there is such a thing, then the forcing is so weak that it is not considered in the governing equations we use to understand and forecast weather. There are terms that we do throw out due to them being relatively small in scale and complicated to calculate. Even with that, there is no term for what you are describing here. I

A few quick things to note that you may find interesting:

The atmosphere is deepest in the tropics due not only to the very warm temperatures experienced here, but also because of centripetal acceleration due to angular momentum from the rotation of the earth. Angular momentum is largely conserved by atmospheric motions, especially in the upper atmosphere. This is the reason for mid-latitude extratropical synoptic systems (aka cold-front/warm-front systems). 

I could also get into gravitational forces and buoyancy. A parcel of air (think of like a small cluster of air - I tend to think of a bubble of air for all intents and purposes) can become positively buoyant for many reasons. When this parcel becomes positively buoyant, it accelerates upward to try to become neutrally buoyant. It will overshoot this level of neutral buoyancy and then it will experience a downward acceleration. It will still overshoot this level it's trying to get to (the level where it will be neutrally buoyant) so it will  experience vertical motions over time much like a sine wave. Ever heard of gravity wave clouds? This is what is happening here. The downward acceleration it experiences is due to gravity. 

Edit:
Source: BS in meteorology and currently an atmospheric science graduate student. ",null,4,cdo3piw,1rjceb,askscience,top_week,4
MichaelHRender,"No, you can not sing a chord. A chord being a harmonic set of three or more notes.

But you can sing an interval. [Tuvan throat singers](http://en.wikipedia.org/wiki/Tuvan_throat_singing) are a classic example. When you create a tone with your larynx, it has a rich harmonic content. Meaning it is made up of a whole series of smaller waves of different frequencies. You can adjust your throat and mouth to amplify those frequencies (sometimes called partials) to make them distinct enough to be heard as an interval of the main frequency.",null,0,cdo4ony,1rjchg,askscience,top_week,1
mrmayo26,"Well this is not the case for all transmittable infections, but at least for HIV and things like bird flu, the idea is that at some point the infectious agent does something (like say change the structure or functioning of its outer membrane or the receptors on it which it uses to gain access into a cell) which allows it to start to infect another species. 

On a similar note there are some infections which have intermediate hosts which they infect but don't cause harm to (like mosquitos and malaria, and ducks with several other parasites) which a parasite can evolve to be part of it's life cycle where it matures. This also has the advantage that if all the final host species in an area die off, the parasite species can still stay alive in the intermediate host. I believe the general term for these is [Zoonosis](http://en.wikipedia.org/wiki/Zoonosis)

But as for your specific example of HIV, it came about after a particular form of SIV (simian immunodeficiency virus) which infects certain primates (simians being a classification of ""higher"" primates) found its way into the blood and/or body of a human and managed to somehow do its virus thing.

One thing to remember is that viruses, especially retroviruses like HIV and SIV, are very sloppy when it comes to replicating themselves and make a lot of uncorrected errors. Luckily for HIV/SIV and unluckily for us, if you increase the mutation /error rate you also increase the rate of adaptation / evolution which helped make the jump to humans. 

tl;dr  viruses are sloppy at replicating their genome, leads to more variety, one variety happens to be able to infect a new species ",null,0,cdnxm8y,1rjcxp,askscience,top_week,2
medikit,I think you might enjoy this radiolab: http://www.radiolab.org/story/169879-patient-zero/,null,1,cdnzn3t,1rjcxp,askscience,top_week,3
mzyos,"Just to add to this, they believe the transmission was via the preparation of ""bushmeat"", which can be different kinds of primates depending on the region it comes from. What is thought to have occurred is that a women preparing the meat underwent blood to blood contact with an infected primate. This may have happened quite a few times until the virus was able to transfer from human to human after a mutation. 

   This is the reason everyone was getting seriously worried about bird flu. It could infect someone who had contact with birds, but it couldn't pass from human to human. This will be a serious problem if it ever mutates and gains the ability to do so.",null,0,cdoj1ox,1rjcxp,askscience,top_week,1
abstrusey,"Viruses (IMO) offer the most interesting examples of infectious diseases that act this way (the majority of clinically relevant bacteria, fungi, and parasites tend to also exist elsewhere in the world, outside of humans, or typically exist in/on humans without causing disease). You have to remember that viruses are just little self-copying machines. They copy copy copy, and then copy some more. Like with a Xerox copier, sometimes the quality degrades, and it's hard to read the message. In the case of these viruses, it can sometimes be a copying alteration/error (aka mutation) that results in different abilities/limitations of the virus (e.g. now it's more infectious but it is also less likely to travel outside of the host species). Over time, the virus can become so ""host-specific"" that it is only transmitted between that species. Diseases that are too weak will be wiped out by the immune system, or they will not be able to replicate enough/fast enough to cause the disease to spread. On the other hand, infectious diseases that are too aggressive can kill the host before they are able to pass it on to others (especially before the modern era of the global community).

There is pretty convincing evidence that HIV comes from Central African chimps (it is transmitted through infected body fluids, like blood, which would be a likely original source if someone fought with or ate a chimp). 

Hepatitis C Virus is an interesting case study for your question, because the answer is still unknown. An interesting [paper on HCV](http://www.ncbi.nlm.nih.gov/pubmed/23463195) states: 

&gt;Going back a final step to the actual source of HCV infection in these endemic areas, non-human primates have been long suspected as harbouring viruses related to HCV with potential cross-species transmission of variants corresponding to the 7 main genotypes into humans. Although there is tempting analogy between this and the clearly zoonotic origin of HIV-1 from chimpanzees in Central Africa, no published evidence to date has been obtained for infection of HCV-like viruses in either apes or Old World monkey species. Indeed, a radical re-think of both the host range and host-specificity of hepaciviruses is now required following the very recent findings of a non-primate hepacivirus (NPHV) in horses and potentially in dogs. Further research on a much wider range of mammals is needed to better understand the true genetic diversity of HCV-like viruses and their host ranges in the search for the ultimate origin of HCV in humans.",null,0,cdpej1s,1rjcxp,askscience,top_week,1
Mn2,"&gt; I recently was reading about the late Stephen Jay Gould's objection to sociobiology (that is to say, that a relationship exists between human social behavior and evolution by natural selection), and how scientists like Richard Dawkins firmly advocate the theory and the correlation it suggests, but I must say that I don't fully understand the counterargument to it (as suggested by Gould and others).

To my knowledge, Gouldt mostly argues that many of the higher functions we have are by-products/indirect consequences of natural selection rather than something that was directly selected for. This is not the same as to say that natural selection has not have a major impact on human behavior. He did approve the theories of kin selection, for instance. 

I'm coming from the field of biology and within our field a lot  -  I would almost dare say majority -  of people are skeptical of sociobiology simply because you have to be very careful and thorough with what you do. It is way too easy for us to make unfounded assumptions (often mirroring our society and values) of how things were x number of years ago, to conclude that this lead to natural selection and that's why things are like they are today. We already know from first hand experience, how much our own values and expectations influence science (e.g. only recently did we start to acknowledge that homosexual behavior in animals is common and that females are in general pretty promiscuous). Hence, it often feels like sociobiology is repeating the same mistakes as ethology did.

There is also a lot of very poor quality sociobiology out there... It is often very simplistic (does not discuss what genes might be responsible, what the mechanism might be and if this is influenced by the environment) and non-critical (disregards a lot of cognitive psychology, neuroscience etc in how our brains interpret and react to stimuli, often forgets or ignores to take in count the cultural influence on our behavior and our assumptions, clumps several different biological functions into one and does not take in count all what we still don't know about biology). Unfortunately this really flies well within media because it is easy to understand and related to and it doesn't require any actual understanding of biology. Dawkins does, unfortunately, sometimes fall into this category.

However, absolutely not all of it is bad. For one, Robert M. Sapolsky has written high-quality stuff relating to sociobiology.

",null,0,cdo47wl,1rje0t,askscience,top_week,1
Jabra,"To see if a drug may be addictive, it is tested on animals. Rats for example. A strain of rats which has a high propensity toward addiction is typically used. Basically, these rats are put in a [Skinner box](http://en.wikipedia.org/wiki/Operant_conditioning_chamber). They receive a dose of the drug each time they press a lever. If the rats keep pressing the level to get a new dose, the drug might cause addiction. One can compare the drug to a placebo to make sure it is in fact the drug and not the rat's anticipation causing it to press the lever.

The animal model itself is validated by using known addictive substances such as cocaine and comparing that to a placebo. Rats with a propensity towards drug seeking behavoir will keep pressing the lever as long as it results in a new dose of cocaine, but will stop when the receiving placebo a few times.

On a related, unscientific note: A colleague of mine used to work with these rat models. He was in the business of validating these tests. In order to study other possible behavoiral effects of drugs they would put a rat on a large table. Normally, a rat will skirt around the edges of the table to see what is up. Once it gets bored, it will scoot back to the centre of the table. There it waits to be picked up and brought back to its cage. A rat on cocaine is something else. It starts skirting the edges of the table with lighting speed. It keeps running, faster and faster, until... it cannot make the turn at the end of the table and flies off. Then you have a rat on cocaine loose in your lab. Try catching one of those. Fun times...",null,0,cdo5t0l,1rjfqu,askscience,top_week,3
chuck10470,"The tires are very hard when cold. F1 speeds would melt normal tires, so they use tires that get soft and ""grippy"" at high speeds. Conversely, when the car is going slow, the hard tires have little grip, and are more akin to hockey pucks than tires. So, in an F1 car, if you go slow, you spin. ",null,1,cdo027b,1rjhk1,askscience,top_week,4
Ermagerd_cerpcerk,"Also F1 cars are designed to generate lots of downward force. The wings and aerodynamics of the car use the air whipping passed to force the car onto the ground. I can't verify, but I was told that if an F1 car was topped out, it would be generating enough downwards force to be able to drive upside down ( like in Men in Black when they drive on the the tunnel ceiling). So the faster you go, the more force the tires have on the road, resulting in higher friction/grip ",null,0,cdoawat,1rjhk1,askscience,top_week,2
fortunecooki,"F1 tires, when heated, soften and ""mould"" to the surface. The track is not completely smooth and every little tiny crevice in the track is a decrease in surface area for the tire to grip to. By melting/ softening the tires, the tire rubber moulds into these crevices, thereby increasing the surface area on which a friction force can act on. This increases the coefficient of friction. here is a nice pic:
http://insideracingtechnology.com/Resources/mechkey.gif
The more surface area, the more ""grip"" a tire has. 
As a side note, if you look at the normal racing tires, they are completely smooth compared to the ones we normally drive with day-to-day. The have more grip but are in serious trouble if there is a touch of water. there is very low coefficient of friction between water and rubber so the tires will have little to no grip. Hence, they swap to tires that have grooves in them to expel the excess water. Because we don't like changing our tires all the time, we always have grooved tires. This can also been seen on your dress shoes compared to your running shoes",null,0,cdoqhkq,1rjhk1,askscience,top_week,1
ControllerInShadows,"They do a variety of things depending on the species... Many bugs go into a dormant stage as adults or larvae. Ants for example will be dormant deep below the surface in very cold climates, while other bugs may find refuge in rotten logs or bark. Other bugs (such as the Praying Mantis) will lay eggs which survive the winter and 'hatch' in the spring. In such cases the adults will typically die.

If you find a rotten log in the winter and open it up, you'll likely see many slow moving bugs (mostly larvae) calling the log home for the winter.",null,1,cdo3bke,1rji2i,askscience,top_week,8
Platypuskeeper,"Well, there's no consensus on what 'atomic radius' means in the first place! Atoms have a diffuse cloud of electrons around them, and it's fairly arbitrary where you consider the 'end' of to be. E.g. the Bohr radius is the most probable radius for the electrons (note: not the same thing as the most probable _location_) Another possible definition (but seldom used) would be the radius that encloses a certain percent of the electron density. Or you could take half the bond distance when two identical atoms have a single bond (the covalent radius). Then there's the 'ionic radius' which exists in two different definitions by Shannon and Pauling. Or, you could use the 'effective radius' as determined from their interactions in the gas phase (read: the van der Waals gas law), which is the vdW radius.

Now since noble gases don't bond, you can't really use ionic or covalent radii. Van der Waals radii work well, because noble gases have close to ideal-gas behavior, but the vdW radii for anything is substantially larger than other measures, so you can't make a comparison that way. 

But the most concrete measure here would probably be the Bohr radius, as it's more directly related to the electron density. And from that perspective, adding electrons does increase repulsion, but it doesn't do so by an extra amount when you complete the shell. The effect of electron-electron repulsion on the radius is actually pretty small. This is illustrated pretty well by comparing hydrogen to helium. Hydrogen has a Bohr radius of 1 Bohr. Since helium has twice the charge, it would have exactly half of that - if the electrons didn't repel each other at all. The [actual radial density](http://www.rsc.org/ej/CP/2009/b901402k/b901402k-f5.gif) (upper curve) has its peak only a slight above that. In fact, it's hard to tell the difference in the radial density distribution between reality and [the situation where they don't repel at all](http://wiki.chemeddl.org/mediawiki/images/0/05/H_and_He_orbitals.gif) (leftmost curve, squashed a bit by a different horizontal scale).

This is not as dramatic for heavier elements where the relative increase in nuclear charge isn't as large, but it still tends to outweigh the increased e-e repulsion. By just about any measure related to the electron density, the noble gases will have a radius somewhat smaller than the preceding halogen.

The exception to this (and perhaps the reason why some people think the radius is bigger), is the van der Waals radius, which for noble gases is indeed larger than for the preceding halogen. But this goes to how the vdW radius is defined by the effective interactions, it's more a result of noble gases having much less mutual attraction than any other atoms. 
",null,0,cdo18bs,1rji79,askscience,top_week,4
Henipah,"If someone has a head injury they can potentially develop [bleeding](http://www.hakeem-sy.com/main/files/images/Location%20of%20epidural,%20subdural%20%20hematomas.JPG) inside the skull, for example an epi/extradural haematoma. This particular injury can be associated with a ""lucid interval"" where people seem fine, but then as pressure builds up they become confused and progress to coma and ultimately death. 

It used to be advised that people not go to sleep because this would prevent proper observation. These days generally people would not be sent home if they were thought to be high risk. ",null,0,cdo2son,1rjiho,askscience,top_week,4
SirGoo,"when you look at something, there is a split second lag between the light hitting your eye and your brain interpreting the image. Also, when you stare a light source for too long, and then look at a blank wall, you can see a spot that clouds your vision slightly. I assume that when you look at these images above, your eye is not holding perfectly still. Even when you try to focus on the exact center of a bar, your eye is slightly correcting the angle every once in a while. this causes a negative image of what you are looking at to flash for brief moments, superimposed behind the actual image you are trying to see. you have to realize all these ""trail"" phenomena are taking place in your head, not on the image, or in your eye. ",null,3,cdo1znz,1rjir4,askscience,top_week,6
chrisbaird,"Your eyes are always jittering around slightly without you noticing. Your eyes do this to compensate for the blind spot where the optic nerve exits through the retina. If your eyes did not jitter, you would always see an ugly blind spot in your field of vision. When you look at one bright color, it saturates the cone cells specific to that color so that they briefly don't work as well. If you then look at a different color, that color is perceived as skewed because of underperforming receptors. The eyes' jitters makes the receptors the are detecting the part of the image that is close to the border between two bright colors to jump back and forth across the border, getting saturated and skewed in the process.

The bottom line is that the effect is caused by the ways your eyes work and not by the way the computer screen works or the way your brain works.",null,0,cdoblgs,1rjir4,askscience,top_week,3
WhiteLightMods,"Try this for fun. Make 4 squares, one each of red, blue, green, yellow. Arrange them in a 2x2 arrangement with a small gap between, kind of like the Windows logo, on a white background. Hold your head still and stare at the center between the squares for 45 seconds. Shift your eyes over to a white sheet of paper. There will be an opposite image in your view.",null,0,cdoil25,1rjir4,askscience,top_week,1
SirGoo,"when you look at something, there is a split second lag between the light hitting your eye and your brain interpreting the image. Also, when you stare a light source for too long, and then look at a blank wall, you can see a spot that clouds your vision slightly. I assume that when you look at these images above, your eye is not holding perfectly still. Even when you try to focus on the exact center of a bar, your eye is slightly correcting the angle every once in a while. this causes a negative image of what you are looking at to flash for brief moments, superimposed behind the actual image you are trying to see. you have to realize all these ""trail"" phenomena are taking place in your head, not on the image, or in your eye. ",null,3,cdo1znz,1rjir4,askscience,top_week,6
chrisbaird,"Your eyes are always jittering around slightly without you noticing. Your eyes do this to compensate for the blind spot where the optic nerve exits through the retina. If your eyes did not jitter, you would always see an ugly blind spot in your field of vision. When you look at one bright color, it saturates the cone cells specific to that color so that they briefly don't work as well. If you then look at a different color, that color is perceived as skewed because of underperforming receptors. The eyes' jitters makes the receptors the are detecting the part of the image that is close to the border between two bright colors to jump back and forth across the border, getting saturated and skewed in the process.

The bottom line is that the effect is caused by the ways your eyes work and not by the way the computer screen works or the way your brain works.",null,0,cdoblgs,1rjir4,askscience,top_week,3
WhiteLightMods,"Try this for fun. Make 4 squares, one each of red, blue, green, yellow. Arrange them in a 2x2 arrangement with a small gap between, kind of like the Windows logo, on a white background. Hold your head still and stare at the center between the squares for 45 seconds. Shift your eyes over to a white sheet of paper. There will be an opposite image in your view.",null,0,cdoil25,1rjir4,askscience,top_week,1
paolog,"Because the yarn passes round the edges of the square in a curve rather than a sharp angle. This means that the yarn does not flatten out until it is a small distance in from the edge of the square. The next winding will lie on top of this curve and will form a looser curve and the yarn will only flatten out a bit further still from the edge of the curve. If you were winding in one direction only around the square, eventually these curves at the top and bottom edges of the square would become so wide that they would meet in the middle, in other word, the yarn would no longer lie flat between the two edges. The wound yarn would now resemble a cylinder. Winding in random directions around the square forms curves in all directions, which eventually become so large that they form a shape resembling a sphere.",null,0,cdo3vcp,1rjivn,askscience,top_week,2
FoolsShip,"The amount of dissociation that occurs is proportional to the ratio of dissociated product in the mixture. The more of the dissociated product is in the mixture, the less likely dissociation will occur. 

When burning hydrocarbons the dissociated products are obviously a result of combustion as well. Therefore they will be present during combustion. This means that there will always be more dissociation in a leaner mixture. In a slightly rich mixture, there will be a maximum ratio of dissociated products to mixture, and so further dissociation will not be favored by the mixture, and there will ultimately be more methane to oxidize.

TL;DR: More methane = less dissociation per mole of O2, meaning that in practice this will basically be the ""true"" stoichiometrically balanced system.",null,0,cdo4cjd,1rjjez,askscience,top_week,1
RelativisticMechanic,"To say that a differential equation is linear means that the differential operator defined by the (homogeneous part of) the differential equation is linear in the sense of linear algebra. This is also the sense in which the Laplace transform is linear. Loosely, ""being linear"" is the statement that you can ""split it across sums"" and ""pull out constants"", as they say.

More formally, given a vector space, V an operator L on that space is said to be linear if it satisfies two conditions:

1. It is additive. That is, L(v + w) = L(v) + L(w) for all vectors v and w;
2. It is homogeneous (of degree 1). That is, L(cv) = cL(v) for all scalars c and vectors v.

Note that under this definition, a function f(x) = mx + b will be linear if an only if b = 0 (the case for nonzero b is called ""affine"").

Now, let's look at your differential equations. The ""vectors"" here are twice-differentiable functions (assuming we're talking about a second order equation) and the ""linear operator"" is the differentiation operator. For example, consider the equation

y'' + 3y = 0.

If you define the operator L(y) = y'' + 3y, then this says

L(y) = 0.

Note that if we compute L(y + v) for two functions y and v, we get

L(y + v) = (y + v)'' + 3(y + v) = y'' + 3y + v'' + 3v = L(y) + L(v),

and if we compute L(cy) for some number c and function y, we get

L(cy) = (cy)'' + 3(cy) = c(y'' + 3y) = cL(y).

Thus, L is a linear operator on the space of twice-differentiable functions.

On the other hand, consider the differential equation

y'' + y^(2) = 0.

The differential operator here is L(y) = y'' + y^(2). What happens if we consider L(cy)? We get

L(cy) = (cy)'' + (cy)^2 = cy'' + c^(2)y^(2) = c(y'' + cy^(2)).

Note that this does *not* satisfy L(cy) = cL(y), so we conclude that this is *not* a linear operator.",null,1,cdo1x7y,1rjjua,askscience,top_week,9
MCMXCII,"Linear in the context of differential equations means that the variable of interest and all of its derivatives appear to the first (or zeroth) power in the equation. If you notice in the equation y = mx + b, the terms on the RHS of the equation contain x to the first and zeroth power respectively. So you can see why the term ""linear"" applies in both situations.

But what's so special about linear differential equations? For starters linear systems can't exhibit chaos. Chaos arises from systems described by nonlinear differential equations (although not all nonlinear differential equations exhibit chaos). Also linear differential equations obey the principle of superposition. That means if f(x) and g(x) are both solutions to a linear differential equation, any linear combination of f and g is also a solution. So f(x) + g(x) is a solution as well. Finally, and possibly most importantly, linear differential equations are relatively easy to solve. There are all kinds of techniques for solving linear differential equations. Nonlinear differential equations are in general much harder to solve.",null,3,cdo1udy,1rjjua,askscience,top_week,6
Dinstruction,"Linear is a term used to describe functions that satisfy f(x * y) = f(x) * f(y). Note that x and y could represent not only numbers, but functions as well. The operation * could also represent different binary operations such as addition, multiplication, or things like matrix addition, etc. Think of integration, differentiation, and the Laplace transform as functions on functions.

These functions behave rather nicely and show up often. For example, differentiation is linear. This is because d/dx (f(x) + g(x)) = d/dx f(x) + d/dx g(x). The Laplace transform is linear because L(f(x) + g(x)) = L(f(x)) + L(g(x)). You probably saw a rule like that when they were first introduced. Similar rules for scalar multiplication hold. i.e. If k is a constant, then L(kf(x)) = k L(f(x)).

Oddly enough, the familiar function f(x) = mx + b is only linear (as described above) when b = 0. When b = 0, and u and v are arbitrary real numbers, we have that f(u + v) = m(u + v) = mu + mv = f(u) + f(v). The function f(x) = 2x + 1 is not linear. As a counterexample, f(2+3) = f(5) = 11, yet f(2) + f(3) = 5 + 7 = 12. 

In more abstract mathematics, this concept is known as homomorphism. Such functions are said to ""preserve operations.""",null,1,cdo1v06,1rjjua,askscience,top_week,3
LoyalSol,"Since others have already defined what being linear is defined as,  I'll add on why we care.   The major reason it is called linear is that for equations or operators which satisfy linearity,  you can use a simple change of variables to generate a linear curve.  For instance,

     x+x^2

is one of the simplest examples.  But let's say we introduce a variable called y and let it be set to y=x^2 .   The above equation is reduced to

   x+y

So if you graphed the values of x+x^2 against x and y you will get a curve that falls on the 3D plane (3D version of a line) x+y.   Why do we care?  Well there are a host of techniques which work for linear systems and usually they are significantly easier than their non-linear counterparts. So if we can take advantage of any linearity it's a massive help.  Conversely if you had an equation like

   f(x)=exp(x+x^2 ) 

Even with a change of variables there no way to reform this equation to make a line while keeping the two variables separate.  Because even if you define y = exp(x) and z=exp(x^2 ) you will simply get

   f(y,z)=yz

Which is still not linear unless you plot with respect to yz.  But this may not actually be useful. ",null,0,cdo77yo,1rjjua,askscience,top_week,2
ACStellar,"I'm sure someone more qualified will come around and answer this but since nobody has at this time I'll give you the simple explanation.

It basically comes down to angular momentum. When the solar system first formed from a collapsing cloud of gas and dust it started spinning as material was pulled toward the center. As such the spinning caused the materials to flatten into a disk orbiting the star. It was from this disk that all the large planetary bodies formed, as such they're all generally contained within the same plane as a consequence of the nature of their formation. 

As far as I know the galaxy is similar. The rotation causes the galaxy to flatten into a roughly disk shape just as it did in the solar system.",null,0,cdnxthr,1rjk9l,askscience,top_week,2
Das_Mime,"See the FAQ

http://www.reddit.com/r/askscience/wiki/astronomy

They formed out of a rotating gas cloud, and when such an object collapses, it forms a rotating disk.",null,0,cdny2av,1rjk9l,askscience,top_week,2
chocapix,"No. They would hit the ground almost as hard.

This is because right before it hits the ground, the car isn't stationary.
Let's say you can get to about 10mph upward when jumping.
If you jump off a car that's falling towards the earth at a 100mph, you're now falling at 90mph.

",null,0,cdo3fle,1rjlcq,askscience,top_week,6
stephenhauskins,"Both the car and the person are accelerating in free fall.  Jumping off might gain you a very little change in downward acceleration but it would be insignificant.

All objects fall at the same rate in a gravitational field",null,0,cdobnp5,1rjlcq,askscience,top_week,1
DangerOnion,"Nope.  It's the same as the urban myth about jumping in a falling elevator.  Though your speed relative to the car is changing, your speed relative to the ground is still very high, and in a bad way.  As chocapix mentioned, unless you can take almost all of the speed of your fall off by jumping, you're going to hit almost as hard.",null,0,cdockbg,1rjlcq,askscience,top_week,1
SpaceEnthusiast,"There are two things we need to decide on before we can answer this question. First, what are we looking at the stars with? To make things easier, let's just consider stars we can view with the naked eye.Typical human eyes can see stars as dim as magnitude 6.5 – 7 ([Apparent Magnitude](http://en.wikipedia.org/wiki/Apparent_magnitude)). You are in luck because there is the [Yale Bright Star Catalog](http://en.wikipedia.org/wiki/Bright_Star_Catalogue) which contains the information on the 9110 objects of stellar magnitude 6.5 or higher. Now, 9096 of these objects are stars and I found [this spreadsheet]( http://handprint.com/ASTRO/XLSX/Yale_BSC.xlsx) (Warning: it’s an excel spreadsheet) that contains all of them. 

Second, we need to decide on the locations we are viewing the stars from. If you are at the equator you can see all the stars but not all of them at the same time (due to Earth's rotation). It'll be most useful to consider the [celestial sphere](http://en.wikipedia.org/wiki/Celestial_sphere). From the page ""the celestial sphere is an imaginary sphere of arbitrarily large radius, concentric with Earth. All objects in the observer's sky can be thought of as projected upon the inside surface of the celestial sphere, as if it were the underside of a dome or a hemispherical screen"". This makes things easy for us. The stars in the North are the ones that are on the Northern celestial hemisphere. The ones in the South are in the Southern celestial hemisphere. This is good because this information is contained in the spreadsheet I found. The parameter we need is the stars' [declination](http://en.wikipedia.org/wiki/Declination). A positive number indicates north and vice versa.

If we sort the numbers and just count the entries with plus or with a minus we get that there are 4428 stars with positive declination and 4668 stars with negative declination. That is, there are slightly more stars visible in the southern sky than in the northern sky (51.3% of the catalog’s stars have negative declination). So you can see a bit more stars in the southern sky.

We can also take a look at the [50 brightest stars](http://astropixels.com/stars/brightstars.html) and count that there are 22 stars with positive declination and 28 with negative declination. The ratio here is 56% of the 50 brightest stars are in the southern hemisphere. That’s not all though. The 3 brightest stars in the night sky all have negative declination.

Conclusion: The southern night sky wins by a small margin.",null,0,cdo3kku,1rjlhn,askscience,top_week,4
do_od,"Much of our understanding of the universe rests on the assumption that it is homogenous on large scales, meaning that it looks pretty much the same in all directions. However the [center of the Milky Way](http://en.wikipedia.org/wiki/Sagittarius_A*) is located at a declination of -30^o which means that we have a better view of the Milky Way from the southern hemisphere. ",null,1,cdo3ifu,1rjlhn,askscience,top_week,3
baloo_the_bear,"The damage to the retina caused by the sources you mention have to do with the **intensity** of the incoming light. The eye mechanism works to focus the incoming image on a specific area of the retina called the fovea, and if the intensity of the light is very high, it can cause damage. The best analogy I can give is that it's like using a magnifying glass to light something on fire: by focusing the sunlight down to a point, the light is intense enough to cause combustion. Now imaging that light being focused on the back of your eye. ",null,0,cdo57m1,1rjlv4,askscience,top_week,1
Platypuskeeper,"You're dealing with the equilibrium H2O &lt;--&gt; H^+ + OH^- so in pure water, the concentrations of H^+ and OH^- will always be the same. pH and pOH are the negative logarithms of the concentrations (actually activities but you don't need to know that yet), so they're equal too. 

Ka is the product of the two concentrations Ka = [H^(+)][OH^(-)], and pKw is the negative logarithm of that, so pKa = -log10([H^(+)][OH^(-)]) = -log10([H^(+)]) - log10([OH^(-)]) = pH + pOH.

pKw decreases with temperature (at least up to the boiling point at atmospheric pressure), meaning _more_ water molecules are breaking apart into ions. (just as low pH = more H^+ , remember it's the negative logarithm) 

So the pH decreases with temperature, but _so does the pOH_. The solution does not become more acidic or basic, rather it's the neutral point that's shifted.  
",null,0,cdo1gwl,1rjmlf,askscience,top_week,3
afranius,"A pixel is not a little square: http://www.cs.princeton.edu/courses/archive/spr06/cos426/papers/smith95b.pdf

That paper might be a little dense, so a simpler explanation is this: a real image is continuous, it is not composed of little boxes. But computers and digital TVs can't work with continuous signals very easily, so the continuous true image is discretized into samples. These samples are called pixels. For a (gross) oversimplification, you can think of these samples as ""probes"" that test the color of the underlying image at evenly spaced locations (in reality, they actually average the color of the image over a small area, but let's not worry about that for now).

Now the job of a TV or monitor is to take these samples (pixels) and reconstruct something that looks as close as possible to the original image. What mechanism the TV or monitor uses depends on the type: a CRT will excite a phosphorus coating by means of an electron gun, an LCD will change the opaqueness of the liquid crystal layer, etc. Some LCDs indeed have square or rectangular regions that change opacity to match the corresponding pixel, but their arrangement need not line up perfectly. So long as the final image faithfully reproduces the original samples.

For some images to illustrate what I mean, start on p19 of this presentation: https://graphics.stanford.edu/wikis/cs148-11-summer/FrontPage?action=AttachFile&amp;do=get&amp;target=148-1.pdf

In answer to your original question, 1080 just refers to the format of the input image that the TV can handle. How it reconstructs an image with 1080 samples on a side is up to the TV, and the number of physical pixels need not be 1080 (and they need not be square!).",null,0,cdo29o8,1rjplu,askscience,top_week,3
king_of_the_universe,"I don't understand the question.

    ...................
    ....#######........
    ....#     #........
    ....#     #........
    ....#######........
    ...................

The object is a square even though the aspect ratio of the screen is not square. And if you change the magnification in your browser (akin to changing the font size), you get a different screen size with an identical amount of pixels. (So yes, if a screen is bigger and has the same amount of pixels, the pixels are bigger.)",null,0,cdo47nd,1rjplu,askscience,top_week,2
Larsor,"A black hole is created in a supernova. A star that has a mass of about 20 times the mass of our sun can create a black hole when it ""dies"". Supernovas are really big explosions that occur when an star has run out of ""fuel"". The gravitational force of the star is creating an inward pressure. This pressure is canceled out by the energy released in the star. When the energy release is stopped the gravitational force wins and the star collapses.",null,0,cdo352o,1rjpt6,askscience,top_week,2
cowboysauce,"&gt; Do we have any evidence for a ""super galaxy""

There are numerous examples of galaxies merging together.

* [Anetnnae galaxies](http://en.wikipedia.org/wiki/Antennae_Galaxies)

* [Mice galaxies](http://en.wikipedia.org/wiki/Mice_Galaxies)

* [NGC 7318](http://en.wikipedia.org/wiki/NGC_7318)

* [NGC 2207](http://en.wikipedia.org/wiki/NGC_2207_and_IC_2163)

* [NGC 520](http://en.wikipedia.org/wiki/NGC_520)

Hell, the milky way is currently in the process of merging with at least one galaxy.

&gt;and what creates a black hole?

There's no consensus on how supermassive black holes form, but stellar mass black holes are formed by the death of large stars.",null,0,cdo32sp,1rjpt6,askscience,top_week,1
Philosophisation,No. Light is simultaneously a wave and a ray. It goes in one direction until absorbed by an electron if it is the right wavelength i.e. energy. It can also be re-emitted if the electron falls to a lower quantum state (energy level). The amount of light in one direction is not affected by any factor other than the amount emitted in that direction and whether it is absorbed. Advanced: Light will not be emitted if a ray is diametrically opposed to it. Rather the enrgy will be released as heat at the source. Other conditions prohibiting the normal release of light in a straight line: A singularity blocking its path or bending it. If an electron has no spin is spin neutral such as if it hawking radiation it will not absorb or emit light. Shining light on said electron will add energy until it disintegrates into multiple decay products including new photons. These photons will be released not upon wavelengths meaning they will not be interfereable with until they are normalized by absorption into another electron.,null,0,cdo2yav,1rjqg5,askscience,top_week,1
Osymandius,"If you're aware of how flu nomenclature works then excuse the patronising explanation. This is something I wrote for a post a while back:

&gt;Let's take flu as it's a virus we know a lot, if not the most, about. The influenza virus enters your cells by binding to sialic acid on your cell membrane. This is enabled by haemagglutinin, a surface protein on the viral capsid. Upon replication, the virions then need to get out your cell, and employ a sialic acid cleaver, neuraminidase. This is where the HXNX notation comes from for flu (H1N1, H3N1 and so on).

&gt;Small mutations (i.e. single amino acid substitutions) in the sequence is what is known as drift - this occurs continually - but the viral phenotype is unchanged. Large charges are rarer and are denoted by a change in nomenclature - H1N1 shifts to H9N1 for example. It is these large shifts that represent a sudden drop in immunity - and occasionally the beginning of a pandemic: Shifting to H1N1 brought about the Spanish Flu and the 08/09 pandemic.

&gt;Drift makes existing vaccines weaker (hence the annual requirement of flu vaccines compared to others lasting a lifetime) and memory T cells less effective as the previously stimulated and clonally expanded naive cells are no longer as specific for the correct epitope. However, it is no where near as drastic as shift which can render any previous cellular immunity totally useless.

So here we're looking at Influenza A and Influenza B - Influenza A is more virulent and seems to shift and drift faster than B. For any given year we normally get 2 strains of A and 1 strain of B knocking about - at any rate that's what the WHO recommend we vaccinate for!

The names are just the site of first identification - normally somewhere around the Pacific Rim - Japan, Malaysia, Australia, West Coast US. Lineage not determined is sort of what you've said, it could be either Yamagata, or Victoria, or any of the other classification sites. For example, in 2006 we were warned about:

A/New Caledonia/20/99 (H1N1)-like virus

A/Wisconsin/67/2005 (H3N2)-like virus (A/Wisconsin/67/2005 and A/Hiroshima/52/2005 strains)

B/Malaysia/2506/2004-like virus from B/Malaysia/2506/2004 and B/Ohio/1/2005 strains which are of B/Victoria/2/87 lineage

Influenza type/Origin/Strain type/Year of isolation/HXNX typing followed by any lineage data.

Does that help?",null,0,cdo4jbp,1rjqqf,askscience,top_week,1
Platypuskeeper,"Because it's in the next period, silicon has larger and more diffuse valence orbitals. The overlap between its p orbitals and those of the oxygen atom is less than in carbon, and so it forms a weaker pi (double) bond. The lower energy state for it then is to form four single bonds to oxygen atoms, and create this network rather than have double bonds to two oxygen atoms as in CO2. 

",null,1,cdo1mof,1rjryk,askscience,top_week,4
cowboysauce,"&gt;What kind of radiation is emitted?

Beta and gamma.

&gt;Why does it destroy the thyroid, but doesn't harm much else?

The thyroid actively takes up iodine, as it's required to synthesis thyroid hormones, removing it from general circulation and preventing too much damage to other parts of the body.

&gt;How exactly does radiation kill cells?

Ultimately, radiation damages DNA and if DNA is damaged too severely, cells cannot preform their necessary functions and die. ",null,0,cdo2w34,1rjsmy,askscience,top_week,4
duckdoodoo,"So, the important thing to know here is that the thyroid gland produces hormones which regulate metabolism in the body. The vital thing about these hormones is that they contain lots of iodine. When we take iodine up into the body, it concentrates in the thyroid gland because this is where the majority of iodine is used in the body.

Radioactive iodine will concentrate in the thyroid gland but will not concentrate anywhere else, sparing the rest of the body from damage.

Once the radioactive iodine concentrates in the thyroid gland, it releases beta radiation within the gland which attacks cells within a small radius. Only cells within the thyroid gland are killed because beta radiation cannot penetrate through our tissues well enough to damage any neighbouring structures. It also releases some gamma radiation too.

Radiation kills cells by damaging the DNA molecules. If enough of the DNA is damaged, the cell will no longer be able to function and will die.

I hope this helps!",null,0,cdox42g,1rjsmy,askscience,top_week,1
E13ven,"That all pretty much has to do with skull characteristics and things like that. We really can't know *exactly* how things sounded, but looking back through evolutionary history and comparing fossil samples of the past with current skulls and things that we have for extant species of the same lineage we can make a good educated guess on what kind of sounds things would have made. ",null,0,cdo1bir,1rjtgm,askscience,top_week,3
atomfullerene,"Mostly we don't.  But given that crocodylians and birds both vocalize, it's quite likely that dinosaurs did as well.  But with a few exceptions based on models of hadrosaur skulls, we have no idea at all what they sounded like.  

But having no sound is almost certainly even more wrong (for some dinosaurs, anyway)  

It's like color...we don't know what color dinosaurs were, but they certainly _weren't_ all some neutral gray color.  So you kind of have to add in something to give the right impression.",null,0,cdo7lz5,1rjtgm,askscience,top_week,1
King_Dynamo,"A person has multiple vocal cords that can vibrate to produce sound, but they can't be independently controlled, so no, not really.  The closest a single person can get to singing a chord is two notes at once using properties of overtones and mouth resonance to throat sing.",null,0,cdo1lc3,1rjub7,askscience,top_week,3
synchrony_in_entropy,"In the end, all memories are just representations that our brains make to learn about the world and every time you think about a memory you make it malleable again. This malleability is a large part of why repressed memories are so controversial and mostly considered to be pseudo-science. This malleability has been shown in a number of studies. Rat research has shown that you can actually erase memories if you inject the right compound when somebody is reflecting on a memory, while human researchers have shown that you can make people change their memories by describing elements of the memory that were never actually initially encoded. So, essentially, repressed memories are never pure and are probably not trustworthy.",null,0,cdo2rzk,1rjuzo,askscience,top_week,13
cowboysauce,"At perigee, the moon is ~ 363,000 km away, amateur telescopes have a a resolving power of about 0.5 arc seconds. Assuming that the telescope is perfectly calibrated and when the moon is closest to Earth, you could see details as small as 2 km.",null,0,cdo35om,1rjw6z,askscience,top_week,6
king_of_the_universe,"I can't answer that, just want to give this impression of the Moon's real distance - it's often imagined that the Moon circles the Earth at a distance of 5 Earth diameters or something like that, but the truth is much different:

http://upload.wikimedia.org/wikipedia/en/archive/7/73/20070429012601!Distance_From_Earth_to_Moon_In_Light_Seconds.gif",null,1,cdo2zfj,1rjw6z,askscience,top_week,5
null,null,null,95,cdoedmo,1rjwyn,askscience,top_week,376
GritsVids,"Not really. Most of the defining characteristics of a face are bone and cartilage. As a human, you're wired to notice mostly the eyes, nose shape, and mouth. These are the key areas that don't change, and why you're able to still recognize someone who's lost a lot of weight, or aged. 

From a silhouette perspective, fat is the only real augmenter, and even then it takes a significant change before morphing someone's features. As we age, those fatty deposits sag, adding to a look of jowls, or baggy eyes. But even then, since most of the key features are reliant on bony structures,  you are still able to recognize the person.

Purpose: Facial muscles aren't really designed to be ""load bearing"" aside from 2: The masseter and the temporalis. Both are used to help close the jaw, and both are laid very low and ""inside"" against the skull, behind your cheek bones a bit. (Look at the bridge behind the cheeks on a skull) Aside from those, the rest are pretty much designed around communication, both verbal and non-verbal. They need to move quickly, and rather precisely.

And lastly... perception. Human being are hard-wired to recognize faces and what they are communicating to us. Our social development decided to go a visual, instead of a smell-based recognition. Your brain has areas just for it. Even if you were able to cause some measurable change, it might just flat out go unnoticed.


TL;DR: No, the recognizable parts of the face are bone and cartilage, and face muscles don't work that way.  

Creds: facial animator for several years. 


edit: spelling",null,54,cdodbrm,1rjwyn,askscience,top_week,229
ren5311,"Hello, as a friendly reminder, please keep all comments civil, on topic, scientific and free from speculation, otherwise they will be removed in line with our [community determined guidelines!](http://www.reddit.com/r/askscience/wiki/index#wiki_answering_askscience)

If you have any questions or comments on AskScience policies, please message the moderators via the link on the sidebar as all off-topic conversation in this thread will be removed.",moderator,59,cdo4bf9,1rjwyn,askscience,top_week,149
null,null,null,9,cdo3lo3,1rjwyn,askscience,top_week,67
null,null,null,21,cdo36rm,1rjwyn,askscience,top_week,61
SpaceEnthusiast,"Sometimes the jaw muscle grow for no known reason at all. Take a look at [this paper](http://dmfr.birjournals.org/content/36/5/296.full.pdf) and [this page](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3275871/). The condition is called Idiopathic Masseter Muscle Hypertrophy. Sometimes it happens on [both sides](http://www.msrdc.ac.in/files/Dento_Quest/02_Dr_Ravleen_Oral_Med.pdf). It seems that Botulinum toxin A [helps](http://www.sciencedirect.com/science/article/pii/S0278239104012686) in reducing the effects of this conditions


",null,14,cdo4bgh,1rjwyn,askscience,top_week,39
null,null,null,10,cdo6n93,1rjwyn,askscience,top_week,21
null,null,null,6,cdo9bbf,1rjwyn,askscience,top_week,17
null,null,null,3,cdohvhz,1rjwyn,askscience,top_week,6
prestonhh,"The answer is a simple yes, provided you are interested in the perception of said facial appearance. The tone of given musculature, irritability of fibers is determined to a significant degree by the representation within the CNS, this of course is greatly shaped by dominant motor behavior. If a person has a cheerful disposition, they are connected to and express things on their face effectively we will perceive a subjectively different appearance. The face may appear more perky.

The levers in the face and the orientation of the musculature doesn't make for the ability to apply significant resistance very easy. There are therapeutic devices and modes of e-stim to encourage effective recruitment but the usage of such devices diminishes after a certain point.

Like the top comment said the biggest contributor to appearance are the fatty deposits and the bone structure. My hero Jack Lalanne was famous for his facial exercises and he has some nice ones on youtube. They are quite invigorating and are a great idea when you first wake up.

Source: I massage people with TMJ among other things for a living",null,3,cdo6ujg,1rjwyn,askscience,top_week,5
null,null,null,4,cdody38,1rjwyn,askscience,top_week,6
null,null,null,95,cdoedmo,1rjwyn,askscience,top_week,376
GritsVids,"Not really. Most of the defining characteristics of a face are bone and cartilage. As a human, you're wired to notice mostly the eyes, nose shape, and mouth. These are the key areas that don't change, and why you're able to still recognize someone who's lost a lot of weight, or aged. 

From a silhouette perspective, fat is the only real augmenter, and even then it takes a significant change before morphing someone's features. As we age, those fatty deposits sag, adding to a look of jowls, or baggy eyes. But even then, since most of the key features are reliant on bony structures,  you are still able to recognize the person.

Purpose: Facial muscles aren't really designed to be ""load bearing"" aside from 2: The masseter and the temporalis. Both are used to help close the jaw, and both are laid very low and ""inside"" against the skull, behind your cheek bones a bit. (Look at the bridge behind the cheeks on a skull) Aside from those, the rest are pretty much designed around communication, both verbal and non-verbal. They need to move quickly, and rather precisely.

And lastly... perception. Human being are hard-wired to recognize faces and what they are communicating to us. Our social development decided to go a visual, instead of a smell-based recognition. Your brain has areas just for it. Even if you were able to cause some measurable change, it might just flat out go unnoticed.


TL;DR: No, the recognizable parts of the face are bone and cartilage, and face muscles don't work that way.  

Creds: facial animator for several years. 


edit: spelling",null,54,cdodbrm,1rjwyn,askscience,top_week,229
ren5311,"Hello, as a friendly reminder, please keep all comments civil, on topic, scientific and free from speculation, otherwise they will be removed in line with our [community determined guidelines!](http://www.reddit.com/r/askscience/wiki/index#wiki_answering_askscience)

If you have any questions or comments on AskScience policies, please message the moderators via the link on the sidebar as all off-topic conversation in this thread will be removed.",moderator,59,cdo4bf9,1rjwyn,askscience,top_week,149
null,null,null,9,cdo3lo3,1rjwyn,askscience,top_week,67
null,null,null,21,cdo36rm,1rjwyn,askscience,top_week,61
SpaceEnthusiast,"Sometimes the jaw muscle grow for no known reason at all. Take a look at [this paper](http://dmfr.birjournals.org/content/36/5/296.full.pdf) and [this page](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3275871/). The condition is called Idiopathic Masseter Muscle Hypertrophy. Sometimes it happens on [both sides](http://www.msrdc.ac.in/files/Dento_Quest/02_Dr_Ravleen_Oral_Med.pdf). It seems that Botulinum toxin A [helps](http://www.sciencedirect.com/science/article/pii/S0278239104012686) in reducing the effects of this conditions


",null,14,cdo4bgh,1rjwyn,askscience,top_week,39
null,null,null,10,cdo6n93,1rjwyn,askscience,top_week,21
null,null,null,6,cdo9bbf,1rjwyn,askscience,top_week,17
null,null,null,3,cdohvhz,1rjwyn,askscience,top_week,6
prestonhh,"The answer is a simple yes, provided you are interested in the perception of said facial appearance. The tone of given musculature, irritability of fibers is determined to a significant degree by the representation within the CNS, this of course is greatly shaped by dominant motor behavior. If a person has a cheerful disposition, they are connected to and express things on their face effectively we will perceive a subjectively different appearance. The face may appear more perky.

The levers in the face and the orientation of the musculature doesn't make for the ability to apply significant resistance very easy. There are therapeutic devices and modes of e-stim to encourage effective recruitment but the usage of such devices diminishes after a certain point.

Like the top comment said the biggest contributor to appearance are the fatty deposits and the bone structure. My hero Jack Lalanne was famous for his facial exercises and he has some nice ones on youtube. They are quite invigorating and are a great idea when you first wake up.

Source: I massage people with TMJ among other things for a living",null,3,cdo6ujg,1rjwyn,askscience,top_week,5
null,null,null,4,cdody38,1rjwyn,askscience,top_week,6
TheCrazyOrange,"Nothing. Humans are just about the dumbest mammals on the face of the earth, when it comes to innate sense of direction.

The only animals springing to mind are things like the panda (dumb as a rock), sheep (also incredibly stupid), though not a mammal, the Kakapo. 

If someone says they have a good sense of direction, most likely they are just better able to visualize the route they took from a known location, and translate that into terms of their current location.


But if you drop a human and a panda in a maze, knowing only that the exit is to the north, the human will begin just as lost as the panda.",null,2,cdo16xv,1rjwyp,askscience,top_week,3
ITRAINEDYOURMONKEY,"I've heard that some researchers have found a few people (I think in some isolated tribe?) who do in fact have neurons that fire when the person is facing north.

This of course doesn't mean that they have a biological direction sensor. What's more likely is they've trained themselves so heavily to navigate, that part of their brain (conscious or subconscious) always keeps track of the direction they're facing, re-calibrating when it sees a reference point. A number of animals are thought to do this kind of positioning (though they'll keep track of both position and direction) where they estimate current position by integrating the movement from the last reference point. People who have a good sense of direction are usually good at using landmarks and their own previous movement to infer their current direction, but it's not an innate sense in the same way as touch/smell/etc.

If you want to see some impressive stuff, look up research on ants' navigation systems. It's likely they have very finely tuned pedometers and directional sense, though it's not settled what all the mechanisms at work are or how big of a role they play. Some of it is thought to be visual flow, and some is based on using the polarization of sunlight as a compass of sorts (they can see the direction in which light is polarized). They're very good at doing a long, wandering search for food, and then returning to the nest in a straight line after they find the food source.

I don't know how many different animals this applies to, but at least bats and rats can build a map of a region they've explored. After they've acclimated to an enclosed environment, they'll have specific neurons that will fire when they're in certain locations within their enclosure (look up ""place cells""). Not sure, but I believe they also have neurons firing based on the direction they're facing relative to the enclosure.",null,0,cdp0l55,1rjwyp,askscience,top_week,1
fartprince,"From Wikipedia:
""However, most spiders that lurk on flowers, webs, and other fixed locations waiting for prey tend to have very poor eyesight; instead they possess an extreme sensitivity to vibrations, which aids in prey capture. Vibration sensitive spiders can sense vibrations from such various mediums as the water surface, the soil or their silk threads. Changes in the air pressure can also be detected in search of prey.""
http://en.wikipedia.org/wiki/Spider_anatomy#Eyes.2C_vision.2C_and_sense_organs

They ""hear"" through their legs by the way. What we think of as the sense of hearing is basically a measurement in air pressure fluctuations. Spiders have sensory systems in place on their legs that detect similar vibrations in the air that, say, perhaps a moth might make while flapping its wings.

If you want further, non-wikipedia reading, here you go:
http://www.european-arachnology.org/proceedings/11th/010_Barth.pdf
",null,0,cdo8i4r,1rjxui,askscience,top_week,2
uncleawesome,Doubtful they sense them. They just make webs where they are luckier at finding bugs. You don't see webs where no bugs are because the spiders moved to a different location or died from starvation.  They move around until they find a spot that is satisfactory.,null,4,cdo2990,1rjxui,askscience,top_week,1
EvergreenStateofMind,"I don't believe so. The smells coming from your clothing may be caused by your normal microbial flora found on your skin. Although most bacteria are anaerobic, the ones flourishing on your skin are aerobic. This would mean that the oxygen present in the atmosphere would not kill these bacteria. I assume that once the nutrients that are on your soiled clothing are depleted the remnants of the bacteria could still cause your clothes to be dirty. I am but a measly microm student so please correct me if i am wrong.",null,0,cdo34yw,1rjz4b,askscience,top_week,5
FatSquirrels,"This very much depends on the type of soil you are talking about and where you hang it up.

If you are trying to remove stains or grime from your clothes then hanging them up won't do much.  Hanging the clothes in the sun and wind will help break up some of the chemicals that make up this stain/grime, but they are likely oils and other large molecules that aren't going to evaporate and won't break down enough to do any good in UV light.

If you have stuff growing on your clothes then hanging them in the sun could help.  UV radiation is pretty good at killing small organisms, especially once you have dried out their living environment.  It might not get rid of the oils or whatever they were feeding on, but it could kill the bacteria and temporarily remove the odors they were producing.",null,0,cdobvoh,1rjz4b,askscience,top_week,1
sharp12180,"Buoyancy is independent of the Earth's rotation. Gravity, however, is necessary for the buoyant force to exist. The buoyant force arises from the fact that, in a fluid, pressure increases as depth increases and this pressure exerts a force in all directions. This means that if an object were submerged there would be more pressure on the bottom of the object than at the top which causes the object to accelerate upward. If there were no gravity then there would be no pressure in the fluid and thus no buoyant force. This link shows a pretty clear diagram.
http://hyperphysics.phy-astr.gsu.edu/hbase/pbuoy.html",null,0,cdo19v6,1rk09y,askscience,top_week,2
chrisbaird,"The problem with time travel to the past is that it violates conservation of energy/mass. Say you time travel back three days and appear in your room. In the local reference frame, you were not there one second and then you are there the next second. Mass/energy has therefore been created out of nothing, which is not allowed. You hang out for three days and then time travel back again alongside your original self. You can see where this leads: you end up with an infinite number of you appearing out of no where, which is nonsense. This is not a psychological effect. Send back in time a rock and conservation of mass is still broken, and a runaway infinity is still possible. ",null,1,cdocv11,1rk11y,askscience,top_week,4
tagaragawa,"I recommend Sean Carroll's [From Eternity to Here](http://preposterousuniverse.com/eternitytohere/). He has a whole chapter on whether and how time travel would fit into our current understanding of physics. Basic outcome: probably not. Also: paradoxes do not happen, any model that would lead to a paradox is necessarily false.",null,0,cdomnzj,1rk11y,askscience,top_week,1
PHYS101,"As far as I know, General Relativity doesn't forbid travelling backwards in time, I am yet to take a course on it besides some reading in my own time.  

However, there is a problem with causality. If you go back in time, you can cause something to happen from before you were born. And then you can run into paradoxes. I have heard a quote that a paradox just means that your model is wrong, nature doesn't allow paradoxes.

My answer's vague and I should feel bad.",null,9,cdo44cu,1rk11y,askscience,top_week,5
sharp12180,"There are reasons to believe that if an object were to travel faster than the speed of light it would then travel back in time, but as far as we know objects cannot travel faster than the speed of light. However, it has been shown (by Richard Feynman) that antiparticles are regular particles traveling backwards in time. Cool Stuff",null,12,cdo1bh0,1rk11y,askscience,top_week,2
professor__doom,"It is called the Valsalva maneuver.  It does have effects on blood pressure and heart rate: http://en.wikipedia.org/wiki/Valsalva_maneuver

When my dad was in the Navy ROTC, he was told to perform it as part of the process of abandoning ship to both prepare for the rigors of the jump and prevent ear injury through equalization of pressure.",null,4,cdo1fiv,1rk2i6,askscience,top_week,27
lennardi,"The main action of holding your breath (after taking in a deep breath) is that it increases intra-abdominal pressure. This is important, especially when lifting, because it is harder to bend your abdomen forward when you have a full breath. It's the same reason that many weightlifters use a weightlifting belt - helps to keep your abdomen straight and can prevent injuries such as hernia which usually arise when you strain with a bent back.

The reason it increases blood pressure is because your torso is filled up with air, then your muscles contract, similar to squeezing a bottle of soda. Obviously, the more air the bottle has in it, the easier it is to squeeze. If the bottle is full of only soda (liquid, like most organs), then it is very hard to squeeze.
",null,5,cdo2sgu,1rk2i6,askscience,top_week,20
steamtrol1er69,"We hold our breath while straining/struggerling while we do complete an obsidious task due to our nervous system connecting and controlling the influences of our raspiratory system to incerase the output of our strength influxes.

source: studying PhD in Bio-Medical Sciences",null,7,cdo6tjw,1rk2i6,askscience,top_week,2
null,null,null,10,cdo1xwv,1rk2i6,askscience,top_week,1
professor__doom,"It is called the Valsalva maneuver.  It does have effects on blood pressure and heart rate: http://en.wikipedia.org/wiki/Valsalva_maneuver

When my dad was in the Navy ROTC, he was told to perform it as part of the process of abandoning ship to both prepare for the rigors of the jump and prevent ear injury through equalization of pressure.",null,4,cdo1fiv,1rk2i6,askscience,top_week,27
lennardi,"The main action of holding your breath (after taking in a deep breath) is that it increases intra-abdominal pressure. This is important, especially when lifting, because it is harder to bend your abdomen forward when you have a full breath. It's the same reason that many weightlifters use a weightlifting belt - helps to keep your abdomen straight and can prevent injuries such as hernia which usually arise when you strain with a bent back.

The reason it increases blood pressure is because your torso is filled up with air, then your muscles contract, similar to squeezing a bottle of soda. Obviously, the more air the bottle has in it, the easier it is to squeeze. If the bottle is full of only soda (liquid, like most organs), then it is very hard to squeeze.
",null,5,cdo2sgu,1rk2i6,askscience,top_week,20
steamtrol1er69,"We hold our breath while straining/struggerling while we do complete an obsidious task due to our nervous system connecting and controlling the influences of our raspiratory system to incerase the output of our strength influxes.

source: studying PhD in Bio-Medical Sciences",null,7,cdo6tjw,1rk2i6,askscience,top_week,2
null,null,null,10,cdo1xwv,1rk2i6,askscience,top_week,1
wazoheat,"The only thing that governs whether something floats or sinks in something else is the density. Almost all substances, liquid water included, have a positive [coefficient of thermal expansion](http://www.engineeringtoolbox.com/linear-expansion-coefficients-d_95.html), meaning as they warm up they get less dense, and vice versa. Warm water ""floats"" above cold water because the denser cold water pushes the warm water up.

Ice is a special case, however. During the phase transition from liquid to solid water, individual water molecules arrange themselves [in a hexagonal lattice](http://media.web.britannica.com/eb-media/90/62690-004-1FB5CC40.gif); an arrangement which is *less dense* than liquid water. Because ice is less dense than liquid water, it will float on it.",null,2,cdo30ss,1rk606,askscience,top_week,10
whatthefat,"Both are possible, and it's not entirely clear what causes one to happen rather than the other.

By ""sleep cycle"", I'm assuming you mean the cycle between rapid eye movement (REM) and non-REM (NREM) sleep that occurs during sleep. This cycle tends to occur with a period of ~90 minutes, although the period shortens slightly across the night, and the amount of REM sleep in each cycle increases across the night. However, the [textbook image](http://upload.wikimedia.org/wikipedia/commons/3/3e/Sleep_Hypnogram.svg) of a night of sleep is a gross simplification. In reality, the cycles are often fragmented, highly variable in period, and often difficult to even discern. It's not possible to predict when a cycle will start or end to a high level of accuracy.

A long awakening can restart the cycle, whereas a very short awakening generally does not. Most people briefly awaken ~20 times per night and then fall back to sleep. These brief awakenings seem not to have much effect on the overall structure of sleep. Many of these awakenings also naturally occur near the end of a NREM/REM sleep cycle.

For longer awakenings, things are more interesting. In [this study](http://www.sciencedirect.com/science/article/pii/0013469489901892), people were awoken 20 minutes into a NREM sleep period, and then kept awake for 10-90 minutes. When they went back to sleep, one of two things generally happened. Either they started a new cycle and went into REM sleep about an hour after they fell back to sleep, or they finished their cycle and went into REM sleep fairly quickly (within about 25 minutes). The latter outcome occurred about one third of the time, but it wasn't possible to predict which outcome would occur. Curiously, no relationship was detected between the outcome and the length of the sleep interruption.",null,0,cdodwpo,1rk6qj,askscience,top_week,4
DangerOnion,"Essentially, yes.  The reason your foot doesn't go through the ground is because of the electromagnetic interactions between matter.  The fact that they can't collide means that the kinetic energy of your shoe is transferred.  Some goes into the ground as heat, some goes back into your foot as heat and as reverberations in your muscles, and some goes into the air.  When that energy is applied to the air, it creates a compression wave in the form of sound.

TL;DR — You're not literally hearing the sounds of electrons colliding, but the interactions between them are somewhat responsible for the sound.",null,1,cdoch0t,1rk7f0,askscience,top_week,4
shavera,"Let's say a station is broadcasting at a full Megawatt of power (the strongest, to my knowledge are usually in the half-megawatt range). Let's say they have a dish (or array of dishes with equivalent collection area equal to) of *d* meters in radius. Their collection area is pi *d*^2 , but our signal is spread out over the sphere of 4.24 light years, that's about 4 x 10^16 meters. And the surface area of a sphere is given by 4 pi r^2 , so 6.4 x 10^33 pi meters. So the signal strength is approximately their collection area divided by the surface area of the sphere, so about d^2 / 6.4 x 10^33 . 

So there's a factor of 1.5625 x 10^-34 reduction in signal strength. A megawatt is 10^6 , so that leaves us 10^-28 . d^2 / 10000 Yoctowatts of power (we don't even have an SI prefix small enough to properly label it). 

So now, looking at wiki's orders of magnitudes of power, the Galileo satellite was giving a 70 m pickup about 10^-21 (zeptowatt) of power. So let's say that the alien SETI program needs to pick up .1 zeptowatt (to make the math easy). So it needs 10^3 yoctowatts, so d^2 = 10^8 , so d = 10^4 , they'd need something like a 10 kilometer dish to pick up our signal. Given all the above approximations, their dish maybe needs to be between a kilometer and 100 kilometers, just to be safe. 

So not *impossible* but seems like just barely possible. Certainly more distant objects are going to have even more trouble (remember again that signal decreases with square of distance)",null,2,cdo7e79,1rk7o8,askscience,top_week,19
Mortis7432,"Yep (http://news.bbc.co.uk/1/hi/7544915.stm)

Here is a chart to see what they would be watching. 

http://www.chartgeek.com/tv-signals-in-space/",null,2,cdo63pl,1rk7o8,askscience,top_week,4
TildeAleph,"Not an expert, and I think I might be being a bit too speculative, but here goes: Yes, you *could* detect radio signals, but not the average random stuff we produce on a day to day basis. Alpha Centauri is ~[4.3 light years away](http://en.wikipedia.org/wiki/Alpha_Centauri), so they would be listening to what we were broadcasting in 2009. Radio communication has only gotten more efficient over the decades, with less and less signal escaping into space. If someone near AC had something like the [Very Large Array](http://www.vla.nrao.edu/) and pointed it at earth they probably [wouldn't](http://zidbits.com/2011/07/how-far-have-radio-signals-traveled-from-earth/) be able to pick anything out of the background radiation. *But*, If we we tried broadcasting a message using something like the [Arecibo Radio Telescope](http://en.wikipedia.org/wiki/Arecibo_Observatory#The_Arecibo_message), and they were using their version of the Very Large Array to listen, then they could easily read us five-by-five.",null,1,cdo72go,1rk7o8,askscience,top_week,2
pspinler,"According to the SETI institute's FAQ on this question, most of our radio signals are too weak to be detected on interstellar distances, but there are some exceptions.  In particular, some military radars, and some deliberate broadcasts.  Here: http://www.seti.org/faq#obs12

",null,0,cdogc33,1rk7o8,askscience,top_week,2
null,null,null,3,cdo6n4a,1rk7o8,askscience,top_week,1
AK-Arby,"Because radio signals travel at the speed of light, in 4.367 years.

The only variable would be the transmission power, but the answer is almost assuredly yes.",null,5,cdo4w9l,1rk7o8,askscience,top_week,2
shavera,"Let's say a station is broadcasting at a full Megawatt of power (the strongest, to my knowledge are usually in the half-megawatt range). Let's say they have a dish (or array of dishes with equivalent collection area equal to) of *d* meters in radius. Their collection area is pi *d*^2 , but our signal is spread out over the sphere of 4.24 light years, that's about 4 x 10^16 meters. And the surface area of a sphere is given by 4 pi r^2 , so 6.4 x 10^33 pi meters. So the signal strength is approximately their collection area divided by the surface area of the sphere, so about d^2 / 6.4 x 10^33 . 

So there's a factor of 1.5625 x 10^-34 reduction in signal strength. A megawatt is 10^6 , so that leaves us 10^-28 . d^2 / 10000 Yoctowatts of power (we don't even have an SI prefix small enough to properly label it). 

So now, looking at wiki's orders of magnitudes of power, the Galileo satellite was giving a 70 m pickup about 10^-21 (zeptowatt) of power. So let's say that the alien SETI program needs to pick up .1 zeptowatt (to make the math easy). So it needs 10^3 yoctowatts, so d^2 = 10^8 , so d = 10^4 , they'd need something like a 10 kilometer dish to pick up our signal. Given all the above approximations, their dish maybe needs to be between a kilometer and 100 kilometers, just to be safe. 

So not *impossible* but seems like just barely possible. Certainly more distant objects are going to have even more trouble (remember again that signal decreases with square of distance)",null,2,cdo7e79,1rk7o8,askscience,top_week,19
Mortis7432,"Yep (http://news.bbc.co.uk/1/hi/7544915.stm)

Here is a chart to see what they would be watching. 

http://www.chartgeek.com/tv-signals-in-space/",null,2,cdo63pl,1rk7o8,askscience,top_week,4
TildeAleph,"Not an expert, and I think I might be being a bit too speculative, but here goes: Yes, you *could* detect radio signals, but not the average random stuff we produce on a day to day basis. Alpha Centauri is ~[4.3 light years away](http://en.wikipedia.org/wiki/Alpha_Centauri), so they would be listening to what we were broadcasting in 2009. Radio communication has only gotten more efficient over the decades, with less and less signal escaping into space. If someone near AC had something like the [Very Large Array](http://www.vla.nrao.edu/) and pointed it at earth they probably [wouldn't](http://zidbits.com/2011/07/how-far-have-radio-signals-traveled-from-earth/) be able to pick anything out of the background radiation. *But*, If we we tried broadcasting a message using something like the [Arecibo Radio Telescope](http://en.wikipedia.org/wiki/Arecibo_Observatory#The_Arecibo_message), and they were using their version of the Very Large Array to listen, then they could easily read us five-by-five.",null,1,cdo72go,1rk7o8,askscience,top_week,2
pspinler,"According to the SETI institute's FAQ on this question, most of our radio signals are too weak to be detected on interstellar distances, but there are some exceptions.  In particular, some military radars, and some deliberate broadcasts.  Here: http://www.seti.org/faq#obs12

",null,0,cdogc33,1rk7o8,askscience,top_week,2
null,null,null,3,cdo6n4a,1rk7o8,askscience,top_week,1
AK-Arby,"Because radio signals travel at the speed of light, in 4.367 years.

The only variable would be the transmission power, but the answer is almost assuredly yes.",null,5,cdo4w9l,1rk7o8,askscience,top_week,2
albasri,"You may be interested in my comment to a similar question:
http://www.reddit.com/r/askscience/comments/1phyv1/has_science_determined_the_most_effective_way_to/",null,0,cdobkit,1rk829,askscience,top_week,3
king_of_the_universe,"Follow-up question - Has the following hypothesis been scientifically verified/falsified?:

""If a person *wants* to learn a certain fact (e.g. 'Hm. Let me just Google that.'), the data is more likely to be stored in the person's head.""",null,0,cdov4l8,1rk829,askscience,top_week,1
__Pers,"I'm not sure what your specific question is, to be honest. The following is from Glasstone's book *The Effects of Nuclear Weapons* (my copy is the 1962 version). 

We have data on what would happen from an underwater burst from the Bikini BAKER event. As in air, a blast wave is created at the point of detonation, though the decrease in overpressure doesn't fall off as fast as in air. Also, because the burst is in water, the overpressure is much greater--peak overpressure at 3k feet from a 100 kt explosion is 2700 psi, compared with only a few psi from a surface or atmospheric burst. Shock waves, if the detonation is close enough to a surface (like a sea floor or the interface between sea and air), can reflect from the surface and surface waves can travel along the surface. Ships near the blast site in Bikini BAKER had superstructures damaged by these surface waves. Bikini BAKER excavated a crater about 2k feet across and (at maximum) about 30 feet deep on the bottom of the sea floor. 

An underwater blast releases a large bubble of superheated gas. In Bikini BAKER, this release led to the production of a large (100 ft. tall) solitary wave on the surface. Several smaller, subsequent waves were observed from the sloshing around as the void filled with water.",null,0,cdo5mko,1rk94s,askscience,top_week,3
c_programmer,"What you're talking about is similar to symbolic linking which is extensively done in both databases and file systems. The bottom line is that the whole data block has to be stored somewhere so you aren't compressing anything. You can arbitarially decide that the byte 01011001 is a 20gb file but that is worthless unless you always have that 20gb file on hand somewhere. In database systems you'll store the big file on some massive file system and store a link taking up a few hundred bytes to point to the actual file. 

Actual compression can be done though things like [Huffman Encoding](http://en.wikipedia.org/wiki/Huffman_coding) but it won't always work well. The bottom line is that there isn't always a way to reduce the amount of information used without losing some of it. ",null,0,cdofdgo,1rk9xa,askscience,top_week,4
lithiumdeuteride,"Since the data you receive is unspecified and could be absolutely anything (or it could be encrypted and thus indistinguishable from random data), every possible chunk could be encountered.

Suppose your chunks are 64 bits each.  The number of possible unique chunks is 2^64 = 18446744073709551616, so you need 2^64 different 'symbols' to represent all of the possible chunks.

How will these symbols be stored in the dictionary?  Well, you'd be crazy to use anything except binary data for the symbol names.  The number of bits needed to uniquely name 2^64 different symbols names is 64, so each symbol name will itself be a 64-bit number.  That means if you get a random 1024-bit file, it will take 16 symbols to encode it.  Each of those symbols itself requires 64 bits to identify, which brings the encoded file size to 1024 bits.  Note that this operation saved zero space.

Space can only be saved when you can break data into chunks without accounting for every possible chunk of that size (thus allowing the symbol names to be shorter than the data they represent).  For this to work, the data must be structured/repetitive, not random-looking.",null,0,cdolrm6,1rk9xa,askscience,top_week,2
MonadicTraversal,"The problem with this idea is that you'd need to decide what chunks of data you put in the database; even if you decide that 'k' expands to a PDF of War and Peace, that means that anybody whose PDF copy has a single bit change now has to store the entire text. Plus, it means that anybody who wants to expand their data has to have *the entire database*, which will be many times bigger than the data they wanted to compress in the first place.",null,0,cdo9qnk,1rk9xa,askscience,top_week,1
Smashfigs,"Things from cranberry juice are absorbed into the bloodstream, you just can't tell because they don't get you drunk!

Things you eat/drink enter the bloodstream through the intestinal wall. The molecules of the things you eat and drink are absorbed directly through the cells of your intestine and enter the bloodstream that way (in a process called transcytosis). Some molecules, such as complex proteins, need to be broken down a bit first as they are quite large. Ethanol is a component molecule of alcoholic drinks, and it's this that makes you drunk when it enters the bloodstream along with all the other molecules present in the drink.",null,0,cdoapz4,1rkatz,askscience,top_week,3
MCMXCII,"&gt;I assume it has something to do with the strong force and the structure of things in the nucleus.

Exactly. Unfortunately it's very complicated. But the nucleus (and everything in nature) wants to exist in the lowest energy state it possibly can. So for certain nuclei there exists a more energetically favorable configuration. If the nucleus can naturally reach the more stable state, it will (probably) do so. The rate at which this occurs can be expressed as a probability per unit time (the decay constant), or a time that it takes for half to decay (half life), or the time it takes for the sample to decay it 1/e times its original size (mean lifetime), among other less useful ways.",null,0,cdoairw,1rkb76,askscience,top_week,3
niceasimov,"yes, all the time. One cool example is the Lyme disease pathogen, *Borrelia burgdorferi*.  Bacteria in the genus *Borrelia* express a protein which elicits an antigenic response in vertebrate hosts (we mount an immune response), called [vlsE] (http://www.ncbi.nlm.nih.gov/pmc/articles/PMC165742/).  Once our immune system mounts an effective response against this antigen, the Borrelia circulating in our system undergo recombination at the vlsE locus resulting in a new antigenic ""strain"" of the bacteria which we no longer recognize, resulting in a persistent infection.  *Borrelia* have a number of unexpressed silent cassettes which recombine into the vlsE locus for just this purpose.    
You might also be interested in reading about the [Red Queen Hypothesis](http://www.sciencemag.org/content/333/6039/216.abstract) in relation to host-pathogen interactions.  ",null,0,cdo9z0k,1rkbad,askscience,top_week,4
stuthulhu,"No. The local cluster is gravitationally bound to the Milky Way, and other clusters of galaxies may be gravitationally bound together in a similar fashion, which is how we have events such as the likely eventual merger of the Milky Way and Andromeda. However, at larger scales the expansion of the universe dominates, and distant objects will recede from one another forever. ",null,0,cdo5fz6,1rke6g,askscience,top_week,2
SMURGwastaken,"Helium only floats upwards when surrounded by heavier particles; if you put a helium balloon in a container full of hydrogen, the balloon would sink to the floor because helium is heavier than hydrogen.

Consequently, in the absence of any other matter helium behaves like any other gas - i.e. it would expand to fill the container as far as was possible.",null,0,cdo51wu,1rkej7,askscience,top_week,8
Platypuskeeper,"Like any gas, it would expand to fill the container it's in.
",null,3,cdo4nt1,1rkej7,askscience,top_week,8
MustafaBei,"If the meteor is massive enough (this is also highly dependant on the meteor's elemental composition), it may hit the ground. But if it burns up like in your case, again, dependant on its elemental composition, it may totally evaporate, or pieces that are very small may still fall down scattered. Although the first and the latest rarely happen. Meteors usually totally evaporate before they can hit the earth. 

They cannot go back to space because of the earth's gravity (or whichever body's it is approaching to). In rare cases, and if it is not going directly towards the celestial body in question (i.e. it is grazing the celestial body), it may start to orbit the source of gravity.",null,1,cdo54em,1rkfch,askscience,top_week,3
DangerOnion,"Depends what it's made of.  Most of it will sprinkle to the ground very slowly, basically as dust.  If it has a component that's ice or frozen gases, then that would evaporate and be incorporated into the atmosphere.  There's really no way it could go back to space.",null,0,cdoc88y,1rkfch,askscience,top_week,2
KRoNlC,"Well actually depending on a few factors like composition and part of the atmosphere where evaporation occurs, i.e. thermosphere, some of the gas could escape back to space via photodissociation. But like I said, this would only occur with the elements light enough to freely escape (H, He). ",null,0,cdosms9,1rkfch,askscience,top_week,2
DrVentureWasRight,"As with everything in engineering, reliability and cost.

People are relatively cheap and replacing an injured stoker is easier than fixing a clogged hopper.  Your loading mechanism will also need to be able to withstand the heat, smoke and soot of an active boiler.

The next benefit is level loading.  For a hopper you need your fuel sufficiently above your boiler to give enough force to load into the boiler.  This will be quite tough on a train due to hight limits and potentially dangeous on a ship as you want your mass as low in the water as possible.

It is possible to use a mechanically driven loader, but that's one more thing that can break.  It also makes starting up the boiler that much harder since your machine as to be producing useful power to load in more fuel.

This systems are just expensive and complicate the design of an already complicated machine.  In the end, particularily at the time steam power was big, a person loading coal into a boiler is just cheaper than the equipment to do it automatically.

",null,1,cdoa63k,1rkfgd,askscience,top_week,8
apostate_of_Poincare,"**yes**

-Piano keys are weighted, and ""reset"" by gravity when you release the key, so that would be annoying (probably like having sticky keys).  

-There are some exotic water-based instruments that it would obviously affect.  Especially once the vessel started vibrating (might make some neat harmonics before the water escapes though)

---------------------
**No**

-Stringed instruments are based largely on the string's properties and perturbations on it.  

-Similarly, brass and reed instruments rely on perturbing instrument components with air, same with an accordion.

-Also similarly, drums are based on perturbing a stretched or solid material that vibrates.

---------------------

(if you have any more instruments in mind, feel free to ask).",null,0,cdoky85,1rkg9b,askscience,top_week,3
jakkes12,Unless you're in a sealed tank filled with air you wouldn't be able to hear what you're playing.As of the effects on the instrument while playing I'd say there is no. Perhaps trumpet since there's no air to blow,null,8,cdohmcl,1rkg9b,askscience,top_week,3
Catsplorer,"I don't believe we've reached a point where we can 'revive' tissue after cryopreservation. In the case of gametes and embryos, special freezing media is required to protect them from cold shock and freezing damage. Cryoprotectants such as low density lipoproteins and glycerol stabilise the plasma membrane during chilling and replace intracellular fluid to prevent ice crystals forming within the cell. As the follicular fluid does not have these properties, I doubt the oocytes would survive being frozen within the ovary itself.",null,0,cdoks61,1rkgwb,askscience,top_week,2
adamhstevens,"Well, for a start Venus is much closer to the Earth in size (0.95 x the radius) and Mass (0.815 x Earth mass). 

But obviously Venus still has a thick atmosphere. This is partially due to the runaway greenhouse, which has driven the carbonate-silicate cycle to an extreme, produced massive amounts of Carbon dioxide.

The other reason is that Mars' magnetosphere died a long time ago, allowing the solar wind to strip away the atmosphere.

In fact, Venus doesn't have a substantial magnetosphere either, but it's probable that it had one at some time in the past.",null,1,cdo63xe,1rkh30,askscience,top_week,7
DangerOnion,"They're not really of comparable size.  Mars is two-thirds the diameter and only an eighth of the mass of Venus.  Much of the Earth's atmosphere is maintained by several factors.  First, volcanoes erupt huge amounts of gas into the atmosphere, compensating for the atmosphere that we lose.  Second, we have a strong enough gravity to keep the atmosphere close, making it denser.  And third, we have a magnetosphere that diverts most of the solar wind around the atmosphere, protecting it from being torn away.

Mars has none of those things.  It cooled long ago, leaving no volcanic activity to replenish its atmosphere.  It has no global magnetic field, though it has very strong surface fields and it appears as though it might have had a global field in the past.  Regardless, what atmosphere it has is offered no protection.  And its gravity is only a third of that of Earth.

Venus, on the other hand, is large, has a sort of fake magnetic field created by the magnetic field of the Sun wrapping around it, and is hot enough to be replenishing its own gas supply.",null,0,cdobhtf,1rkh30,askscience,top_week,3
Javi2639,"Alkali ions like sodium and potassium serve incredibly important functions in the body. Since they are reactive,  they're usually found in salt form. As these salts are rare in nature, our bodies give us positive feedback when it is ingested, therefore it tastes good. ",null,0,cdoke1e,1rkizt,askscience,top_week,1
Henipah,"About 10% of the time you have a bowel movement and often when you floss your teeth you get a transient bacteraemia, i.e. bacteria circulating in your blood. Your immune system manages to clear them which is why we don't all die of sepsis. ",null,0,cdojl3m,1rkj5o,askscience,top_week,8
expandedthots,"The bacteria in your GI tract have become part of your normal flora, meaning they live and survive in your gut, without being attacked by your immune system, and actually help to digest food and in some ways regulate your immune system. 

So basically, your immune system doesn't fully recognize them as a threat in the same way it recognizes other bacteria, because the bacteria that make up your normal flora don't have the virulent versions of things like endotoxin and other toxins, which activate your immune system. Hence, your immune system recognizes these less threatening versions of bacteria and learn not to attack them like they would an infection. It also allows your body to recognize when something is bad more easily, because it has all this ""data"" on what isn't bad. 

Gets out of whack at times, so you see e. coli sepsis in really sick patients. 

Also, you don't get blood on toilet paper from wiping too hard. You probably have a hemorrhoid.",null,9,cdoa6e5,1rkj5o,askscience,top_week,12
Natolx,"&gt; Why don't all the pathogens which can cause disease get into the bloodstream and if so, why don't they effect us as much as they would elsewhere?

OP, please see my reply to one of the other posts in this thread, your question is addressed by the edit(supported by the information above)

http://www.reddit.com/r/askscience/comments/1rkj5o/why_dont_you_get_infected_when_you_wipe_too_hard/cdoq0eh


",null,0,cdoqfdz,1rkj5o,askscience,top_week,1
jyaron,"Your rectum, though external, is essentially meshed with the mucus membranes of your lower GI tract. As with essentially all mucus membranes in the body, the rectum is rich in immune cells representing the first line of defense against invading pathogens. In particular, the immune cells in these sorts of mucus membranes are especially good at tagging invading pathogens with Immunoglobulin A (IgA), which signal to professional phagocytes of the immune system that ""this is a bug that you should destroy."" As such, though we may bleed in the area, the bacteria that make their way into that particular wound have a hell of a time surviving for very long, as they are quickly destroyed by these ready-to-fight immune cells.

Search ""mucosal immunity"" for more info.",null,0,cdpra42,1rkj5o,askscience,top_week,1
Henipah,"About 10% of the time you have a bowel movement and often when you floss your teeth you get a transient bacteraemia, i.e. bacteria circulating in your blood. Your immune system manages to clear them which is why we don't all die of sepsis. ",null,0,cdojl3m,1rkj5o,askscience,top_week,8
expandedthots,"The bacteria in your GI tract have become part of your normal flora, meaning they live and survive in your gut, without being attacked by your immune system, and actually help to digest food and in some ways regulate your immune system. 

So basically, your immune system doesn't fully recognize them as a threat in the same way it recognizes other bacteria, because the bacteria that make up your normal flora don't have the virulent versions of things like endotoxin and other toxins, which activate your immune system. Hence, your immune system recognizes these less threatening versions of bacteria and learn not to attack them like they would an infection. It also allows your body to recognize when something is bad more easily, because it has all this ""data"" on what isn't bad. 

Gets out of whack at times, so you see e. coli sepsis in really sick patients. 

Also, you don't get blood on toilet paper from wiping too hard. You probably have a hemorrhoid.",null,9,cdoa6e5,1rkj5o,askscience,top_week,12
Natolx,"&gt; Why don't all the pathogens which can cause disease get into the bloodstream and if so, why don't they effect us as much as they would elsewhere?

OP, please see my reply to one of the other posts in this thread, your question is addressed by the edit(supported by the information above)

http://www.reddit.com/r/askscience/comments/1rkj5o/why_dont_you_get_infected_when_you_wipe_too_hard/cdoq0eh


",null,0,cdoqfdz,1rkj5o,askscience,top_week,1
jyaron,"Your rectum, though external, is essentially meshed with the mucus membranes of your lower GI tract. As with essentially all mucus membranes in the body, the rectum is rich in immune cells representing the first line of defense against invading pathogens. In particular, the immune cells in these sorts of mucus membranes are especially good at tagging invading pathogens with Immunoglobulin A (IgA), which signal to professional phagocytes of the immune system that ""this is a bug that you should destroy."" As such, though we may bleed in the area, the bacteria that make their way into that particular wound have a hell of a time surviving for very long, as they are quickly destroyed by these ready-to-fight immune cells.

Search ""mucosal immunity"" for more info.",null,0,cdpra42,1rkj5o,askscience,top_week,1
albasri,"This question has been asked several times before. Here a few relevant links that you might find interesting:

http://www.reddit.com/r/askscience/comments/1hwivf/is_there_a_documented_explanation_as_to_why_the/
http://www.reddit.com/r/askscience/comments/10kocv/why_do_neurons_decussate/
http://www.reddit.com/r/askscience/comments/1ljq6x/is_there_an_adaptive_purpose_to_the_different/
http://www.reddit.com/r/askscience/comments/1c2uqz/why_do_nerves_decussate_in_the_spinal_cord/
http://www.reddit.com/r/askscience/comments/14pgpx/why_do_nerve_pathways_to_and_from_the_brain_cross/
http://www.reddit.com/r/askscience/comments/lxb2t/is_there_an_evolutionary_biological_or_other/
http://www.reddit.com/r/askscience/comments/j2i6z/why_does_our_brain_cross_nerve_signals_ie_the/
http://www.reddit.com/r/askscience/comments/gqxid/why_does_the_left_hemisphere_of_the_brain_control/

Sometimes, a lot of really interesting questions and answers get buried because of the high volume (and long history!) of /r/askscience. The search bar can be a great way to search for answers to some of your questions (although getting just the right search terms in can sometimes be a bit tricky) and explore /r/askscience in general!",null,0,cdobfym,1rkj7a,askscience,top_week,1
_NW_,"The products of combustion are CO2 and water.  When the exhaust pipe is cold, the water vapor is condenced into water droplets and exits as a fog.  Once the exhaust pipe gets hot enough, the water stops condencing and exits the pipe as a clear water vapor instead of a fog.  The same amount of water is being produced in both cases.  It's just the state of the water that determines if it is visible or not.",null,1,cdode72,1rkjot,askscience,top_week,7
Sterlz,"The 'steam' that you're talking about is a mix of water vapor and other nitrogen compounds and carbon compounds as well as mostly carbon dioxide. The reason you see more exhaust when you start up is because the catalytic converter on your exhaust system hasn't been brought up to operating temperature. When the converter reaches operating temperature it is more efficient at turning the nitrogen and carbon compounds from the cylinders into water and CO2. 
The compounds that come out of a cold exhaust are more visible in the cold air than water vapor, or CO2 (essentially invisible).",null,0,cdodffx,1rkjot,askscience,top_week,2
Thermodynamicist,"When you first start the engine, the exhaust system is cold, so the exhaust is colder when it first hits the atmosphere, and its velocity is less because its volume is less (PV = mRT). 

[The saturation vapour pressure of water is a strong function of temperature.](http://en.wikipedia.org/wiki/Vapour_pressure_of_water)

Therefore the combination of improved mixing due to increased exhaust gas velocity, and the increased saturation vapour pressure due to increased exhaust  gas temperature mean that once the engine warms up, the exhaust diffuses out into the atmosphere fast enough that it doesn't ever intercept the saturation line, and no condensation occurs. ",null,0,cdp6l0v,1rkjot,askscience,top_week,2
flamoutan,"Yes. [According to their website](http://tellspec.com/howitworks/), TellSpec is in essence just a handheld Raman spectrometer. [Raman spectroscopy](http://en.wikipedia.org/wiki/Raman_spectroscopy) can be used to identify different molecules based on their unique vibrational spectra (thus allowing for identification of various ingredients by matching patterns with their online database). Various types of handheld spectrometers already exist on the market (you can see this through a quick search), so this is certainly technologically feasible.",null,0,cdoftym,1rkk8j,askscience,top_week,2
Jyesss,"rhEPO does not have a different number of amino acids than does human EPO. rhEPO is made by transfecting the human gene into yeast and expressing it. The fact that human EPO and rhEPO are genetically identical is what made separating them so difficult. They managed to find a way of separating them by detecting charge differences on the amino acids. rhEPO is glycosylated differently than human EPO because it is made in yeast and not humans. This allows the protein to be isolated by 2d visualization based on charge and molecular weight.

Check out this figure from Lasne et al:
http://imgur.com/RLF5Omn",null,0,cdoefap,1rklhj,askscience,top_week,2
expandedthots,"My guess is that when you draw labs looking for doping, you don't really need to analyze the structure of the hormone to see if its recombinant, you just need to measure the amount. If its sky high, then they're doping unless they have an EPO secreting tumor in their kidney. The big dopers obviously are aware of this, so they cycle before they have to be tested but this habit is decreasing because testing is becoming more random. In my mind, you could just look at a hematocrit as well, even if their EPO levels are normal, although this can be naturally elevated by training at high altitudes etc. 

Except for the most high profile cases and levels of competition, I doubt any lab would analyze the amino acid profile of EPO harvested from thousands of athletes unless they were paid very very well.",null,1,cdo9y7q,1rklhj,askscience,top_week,2
selfification,"You might want to retag this with Biology or Neuro.  It'll probably get the attention of folks who know more about action potentials and the electrical properties of peripheral nerves in general.  The question you're essentially asking is ""Is there a difference between having a positive voltage along a nerve and a negative one?"" and the doctors will probably have a better answer to that.",null,0,cdoak5j,1rko77,askscience,top_week,7
shavera,"First, omfg do I hate that video. Like you have no idea. It's so so so wrong.

To answer your question properly, no, it's not. We can get into semantics about what it means to be ""moving"" through time, but that's all a bit of philosophy about what the theory of relativity implies about the metaphysical nature of reality (ie beyond our capacity to experiment and thus describe scientifically). 

In particle physics, we end up with some really awful infinite sums of integrals. One ingenious way of solving these integrals was to rearrange them in a specific order and then rewrite them in a different mathematical ""language."" It turns out that new ""language"" is one of diagrams of particles moving and colliding and whatnot. Again though, these are just graphical representations of mathematical representations of physical behaviour, not necessarily ""reality itself."" (again more metaphysics here)

Well it turns out that in these *representations* of reality, antimatter can be treated *as if* it is matter travelling backwards in time in these diagrams. 

But what is often more wondered about the ""arrow of time"" is really the ""arrow of causality."" Where it seems like actions earlier in time *cause* actions at further points in time, but not in the reverse. However, causality isn't a rigorous scientific description of reality. It's another approximate representation of reality. And it's an approximation that doesn't really help when we're talking about particle physics. What matters is that at time **a** we observe particles {*A*} with energy and momentum and position and at time **b** we observe particles {*B*} with energy and momentum and position. ",null,2,cdo7ssl,1rkprv,askscience,top_week,38
chrisbaird,"The answer to this question is really one of philosophy and not physics. Let me explain why. There are certain symmetries in particle physics that make it possible to describe antimatter as backward-in-time traveling regular matter and still get the right answer in the end (if you are careful and do it right). *But*... antimatter still obeys all the usual conservation laws and obeys causality. All the interesting stuff in science fiction that becomes possible with time travel to the past ends up breaking conservation laws (conservation of energy, charge, etc.). Since antimatter obeys these laws, it can't be used to do anything interesting typically associated with time travel. You can't use antimatter to shoot your grandpa as a kid. So with regards to conservation laws and causality, antimatter acts like regular forward-in-time matter. With regards to the symmetry in particle interactions, it *can* act like backwards-in-time matter. Which one is it *really*? That type of question is better suited to philosophers and linguists. There is no *really* in science. There is just layers upon layers of observations.",null,0,cdobzxi,1rkprv,askscience,top_week,8
Alphaetus_Prime,"What that really means is that if you take the equations that govern the behavior of matter and you reverse both the charge and the direction of travel through time, the equations are still valid. An electron annihilating with a positron can be interpreted as an electron that spontaneously flips its charge and starts traveling backwards in time, giving off lots of energy in the process.",null,0,cdobkue,1rkprv,askscience,top_week,3
shavera,"First, omfg do I hate that video. Like you have no idea. It's so so so wrong.

To answer your question properly, no, it's not. We can get into semantics about what it means to be ""moving"" through time, but that's all a bit of philosophy about what the theory of relativity implies about the metaphysical nature of reality (ie beyond our capacity to experiment and thus describe scientifically). 

In particle physics, we end up with some really awful infinite sums of integrals. One ingenious way of solving these integrals was to rearrange them in a specific order and then rewrite them in a different mathematical ""language."" It turns out that new ""language"" is one of diagrams of particles moving and colliding and whatnot. Again though, these are just graphical representations of mathematical representations of physical behaviour, not necessarily ""reality itself."" (again more metaphysics here)

Well it turns out that in these *representations* of reality, antimatter can be treated *as if* it is matter travelling backwards in time in these diagrams. 

But what is often more wondered about the ""arrow of time"" is really the ""arrow of causality."" Where it seems like actions earlier in time *cause* actions at further points in time, but not in the reverse. However, causality isn't a rigorous scientific description of reality. It's another approximate representation of reality. And it's an approximation that doesn't really help when we're talking about particle physics. What matters is that at time **a** we observe particles {*A*} with energy and momentum and position and at time **b** we observe particles {*B*} with energy and momentum and position. ",null,2,cdo7ssl,1rkprv,askscience,top_week,38
chrisbaird,"The answer to this question is really one of philosophy and not physics. Let me explain why. There are certain symmetries in particle physics that make it possible to describe antimatter as backward-in-time traveling regular matter and still get the right answer in the end (if you are careful and do it right). *But*... antimatter still obeys all the usual conservation laws and obeys causality. All the interesting stuff in science fiction that becomes possible with time travel to the past ends up breaking conservation laws (conservation of energy, charge, etc.). Since antimatter obeys these laws, it can't be used to do anything interesting typically associated with time travel. You can't use antimatter to shoot your grandpa as a kid. So with regards to conservation laws and causality, antimatter acts like regular forward-in-time matter. With regards to the symmetry in particle interactions, it *can* act like backwards-in-time matter. Which one is it *really*? That type of question is better suited to philosophers and linguists. There is no *really* in science. There is just layers upon layers of observations.",null,0,cdobzxi,1rkprv,askscience,top_week,8
Alphaetus_Prime,"What that really means is that if you take the equations that govern the behavior of matter and you reverse both the charge and the direction of travel through time, the equations are still valid. An electron annihilating with a positron can be interpreted as an electron that spontaneously flips its charge and starts traveling backwards in time, giving off lots of energy in the process.",null,0,cdobkue,1rkprv,askscience,top_week,3
Smoothened,"Our skin cells are not as exposed to physical stress as bacteria are. The outermost layer of our skin, the stratum corneum, acts as a barrier against many types of damage, including that inflected by mechanical stress such as the nano-spikes in this material. The stratum corneum is composed of biologically dead but still active cells called corneocytes which are interlocked forming a thick hydrophobic matrix. Besides mechanical stress, it also protects the body against dehydration, infection, burns, chemicals, etc. Bacteria, on the other hand, are only protected by their cell envelope. 

Edit: Also bear in mind that the stratum corneum is composed of up to 20 layers of cells and its thickness is between 10 and 40 micrometers. That's at least 20 to 80 times thicker than the 500 nanometers spikes in this material. And even if the cells in the outermost layers are damaged, these cells are constantly being replaced. ",null,1,cdoc4k9,1rkqh7,askscience,top_week,3
chrisbaird,"The ultimate physical constraint on telescopes is diffraction. Due to diffraction, the best resolution is proportional to the ratio of wavelength observed to aperture size. Note that this just places limits on individual telescopes depending on aperture size and does not mean that there is one fixed limit for all telescopes. In principle, you could get ever better resolution if you built ever bigger telescopes. While a telescope the size of our solar system could in principle see with far better resolution than we currently have, such a feat is currently technologically unattainable.",null,0,cdociv9,1rkrla,askscience,top_week,5
selfification,"Yep:
http://en.wikipedia.org/wiki/Diffraction-limited_system
http://en.wikipedia.org/wiki/Rayleigh_criterion#Explanation

Light has a certain wavelength and at some point, you cannot extract more resolution out of it.  It would be like trying to map out where ships in the ocean are based on the tides...  the wavelength would simply be too high to resolve the image.",null,0,cdo9hj7,1rkrla,askscience,top_week,3
iorgfeflkd,"Their path is curved when they move perpendicular to strong gravitational fields, and their energy is shifted up or down if they move towards or away from a gravitational source.",null,0,cdo954c,1rksxv,askscience,top_week,4
patchgrabber,"There is no average, this question isn't answerable. The pace of evolution is mainly dictated by selection, which is constantly changing. Add in other factors like environmental changes such as formation of mountain ranges, not to mention the sheer fact that it takes thousands upon thousands of years for most organisms our size to progress enough to a point where we deem it (arbitrarily) to be another species, something that we cannot observe directly, and you can see why something like this cannot have a simple number value placed on it.",null,0,cdo8wg8,1rkt46,askscience,top_week,4
snusmumrikan,"A way that may help you visualise it is to think of species in the same way as you think of generations in your family. If you look at a father and a child you can confidently say they are of two different generations. However if you lined up 100 people all aged from 1-100, you wouldn't be able to pick exact points in which one generation ends and another begins.

Evolution gives rise to new species through small accumulative changes in the genome of an organism. It is only by looking back that we see that now an organism is incompatible with regards to mating with the ancestor.",null,0,cdow5gv,1rkt46,askscience,top_week,1
julesjacobs,"No, it would appear just as fuzzy to them as looking directly at the text (slightly fuzzier in fact, since now you also add the distance from the eyes to the mirror and back the the equation). Optometrists use the same effect for the opposite means. If you want to test the right glasses for a person who can't see far away objects, you would need to place objects really far away (like 10 meters). Usually an optometrist's office is too small for that, so what they do is place a mirror on one side of the room, and let the patient look via the mirror to the opposite side of the room. This way you only need a 5 meter room to display on object 10 meters away for the purposes of the measurement.",null,3,cdoa7nk,1rktky,askscience,top_week,21
Arladerus,"First, let's examine what is actually happening in the eye of a short-sighted person.  The function of the lens in the eye is to take light rays from objects around you and bend them so that they converge on your retina to form an image.  However, in a myopic eye, this image is actually formed in front of the retina, causing blurriness.

Now, let's discuss your scenario.  When looking through a mirror (we are assuming a typical, flat mirror), we see objects behind the mirror, which is called a virtual image.  Without going too deep into the matter, basically the light rays act as if they were really behind the mirror.  So, if we have a mirror one foot in front of us, and we see an object 20 feet behind us, the object would appear to be a total of 22 feet away from us in the mirror.  The important question is whether the light rays behave as if were from an object 22 feet away.  Because the mirror is flat, the answer is yes.  

To summarize, no, the object would not appear clear.",null,1,cdoace3,1rktky,askscience,top_week,6
stuthulhu,"&gt;if a short sighted person were to try to read some text 20 ft away using a reflection from a mirror a foot away from their face,

You see by the photons reflecting from the object into your retina. In this case, the photons reflect from the object, hit the mirror, bounce off, then hit your retina. Now the object will appear as though it is 21 feet away, because the photons have to travel all the way from the object to the mirror, then from the mirror to your eye. This would actually make the problem worse.

Think about it this way, if you go outside during a full moon, and you hold up a mirror a foot away from your face, would you expect to see the footprints on the moon?",null,1,cdoajn3,1rktky,askscience,top_week,3
yeast_problem,"I read the question as using a ""close up"" mirror, i.e a convex mirror usually used to enlarge the view of your face. If your eye was further from the mirror than its focal point, surely a short sighted person could see an inverted but corrected image? A diverging concave mirror would probably be more useful, creating a virtual image behind the mirror.",null,1,cdobslm,1rktky,askscience,top_week,2
julesjacobs,"No, it would appear just as fuzzy to them as looking directly at the text (slightly fuzzier in fact, since now you also add the distance from the eyes to the mirror and back the the equation). Optometrists use the same effect for the opposite means. If you want to test the right glasses for a person who can't see far away objects, you would need to place objects really far away (like 10 meters). Usually an optometrist's office is too small for that, so what they do is place a mirror on one side of the room, and let the patient look via the mirror to the opposite side of the room. This way you only need a 5 meter room to display on object 10 meters away for the purposes of the measurement.",null,3,cdoa7nk,1rktky,askscience,top_week,21
Arladerus,"First, let's examine what is actually happening in the eye of a short-sighted person.  The function of the lens in the eye is to take light rays from objects around you and bend them so that they converge on your retina to form an image.  However, in a myopic eye, this image is actually formed in front of the retina, causing blurriness.

Now, let's discuss your scenario.  When looking through a mirror (we are assuming a typical, flat mirror), we see objects behind the mirror, which is called a virtual image.  Without going too deep into the matter, basically the light rays act as if they were really behind the mirror.  So, if we have a mirror one foot in front of us, and we see an object 20 feet behind us, the object would appear to be a total of 22 feet away from us in the mirror.  The important question is whether the light rays behave as if were from an object 22 feet away.  Because the mirror is flat, the answer is yes.  

To summarize, no, the object would not appear clear.",null,1,cdoace3,1rktky,askscience,top_week,6
stuthulhu,"&gt;if a short sighted person were to try to read some text 20 ft away using a reflection from a mirror a foot away from their face,

You see by the photons reflecting from the object into your retina. In this case, the photons reflect from the object, hit the mirror, bounce off, then hit your retina. Now the object will appear as though it is 21 feet away, because the photons have to travel all the way from the object to the mirror, then from the mirror to your eye. This would actually make the problem worse.

Think about it this way, if you go outside during a full moon, and you hold up a mirror a foot away from your face, would you expect to see the footprints on the moon?",null,1,cdoajn3,1rktky,askscience,top_week,3
yeast_problem,"I read the question as using a ""close up"" mirror, i.e a convex mirror usually used to enlarge the view of your face. If your eye was further from the mirror than its focal point, surely a short sighted person could see an inverted but corrected image? A diverging concave mirror would probably be more useful, creating a virtual image behind the mirror.",null,1,cdobslm,1rktky,askscience,top_week,2
Ermagerd_cerpcerk,"I'm not 100%, but I believe that bad breath is caused in the mouth, not the lungs. Salivating cleans your mouth. This is why if you wake up with bad breath, then eat something, you won't have the morning dragon breath afterwards. While you sleep you don't salivate as much so your mouth gets ""stale"", causing bad breath. If you don't exhale from your mouth, the air coming from your lungs won't pick up the odors from your mouth",null,0,cdo9emq,1rktnc,askscience,top_week,1
Sterlz,"Oil is made up of much larger carbon chains approaching eight carbons (octane), while natural gas is usually composed of one carbon (methane).  Different underground pressures and differences in the surrounding chemical composition are most likely the reason that the carbon chains produced different lengths. ",null,0,cdod844,1rkumr,askscience,top_week,2
LuckyThursdays,"That biomass is made up of compounds (hydrocarbons) with varying chain lengths. The longer the chain, the more intermolecular forces form between the compounds which means there are more 'bonds' to break, requiring more energy and therefore the compounds have a lower boiling point. These are the ones which are liquid (or solid).
Smaller chains (eg. methane) have very low boiling points, below room temperature, so are naturally found as gases.
Longer chains tend to have formed under much higher pressure and temperatures as the atoms are forced together. Essentially, many methane molecules are 'squashed' to form longer hydrocarbon chains.",null,1,cdodaj4,1rkumr,askscience,top_week,3
albasri,"It used to be thought that chess masters had extraordinary memories, but this was shown to be untrue. Shown a board position for a very brief period of time, a chess master can reconstruct it from memory with much greater accuracy than a novice. However, if pieces are randomly arranged on the board, then experts are just as bad as novices. This was taken to indicate that the difference between experts and novices is that experts are able to rapidly extract features and detect patterns or structure in arrangements of chess positions, forming simpler representational units that can be encoded faster and with greater accuracy. These are typical characteristics of perceptual learning and exist in many domains of expertise, e.g. bird watchers, radiologists, etc. 

See Chase and Simon's chapter in *Visual information processing* (1973) and *Thought and choice in chess* by de Groot (1946/1978). The latter work showed that there was no difference between masters and controls in general cognitive abilities, only this memory difference for board positions. Another useful resource could be *The psychology of chess skill* by Holding (1985). 

(as an aside - I feel that this question is more appropriate for psychology than neuroscience…)

edit: grammar x2",null,22,cdoaul2,1rkutv,askscience,top_week,146
darkheritage,"There has been some buzz the last few years of research being done on chess' ability to increase cognitive ability, more specifically on how it relates to (oddly enough) Alzheimer's.  Studies have shown that playing chess can increase cognitive ability and help keep the disease in check.
Here is a brief overview:[From Chessbase](https://chessbase.com/post/checkmating-alzheimers-disease-210513)

Edit:  I know little about the subject, I am just an amateur chess player who has read some of the articles over the last few years.",null,6,cdobpox,1rkutv,askscience,top_week,16
honkycat1,"I don't think whether or not chess masters performance are due to nature of nurture quite answers your original question. 

Recent research has shown that cognitive games (those mental exercises, e.g. puzzles, mind games, etc) don't really help general mental ability. They only help one's ability to play those games in a very context specific way. This is a pretty sensitive topic, as a lot of companies have made a ton of money selling mind-exercises because of the argument that, like a muscle, you need to train the mind. While the general premise of this is true, empirical evidence does not show that mind games improve anything other than performance on those mind games. (Unfortunately, I don't have reference off the top of my head, but either way, I'm sure there are plenty of support on both sides of this argument) Therefore, I don't think playing chess will make someone smarter, in anything other than chess. But again, this is a hotly debated area, there is also contradicting evidence.

As for expert performance in chess. Hambrick is a colleague of mine so I am somewhat familiar with his work. He is on the camp of ""nature"" in the nature vs nurture debate. He has shown that working memory is related to expert performance of piano players even at the highest level, which indicate that regardless of practice, basic cognitive abilities is related to performance. My personal research is in job performance and research show that general mental ability can predict job performance even at the highest level of performance. Bottom line is, practice aside, your disposition will always play a role. 

When you ask ""In other words, it seems that I have a better chance of reaching a more ""beneficial"" conclusion if I could process 5 units of information with a superior ability to synthesize them, than if I could process 10 units of information with a poor ability to synthesize them. "" Of course, working memory alone doesn't predict your performance in chess. There are other factors, and there is also random noise. But holding other variables constant, someone higher in working memory will ON AVERAGE out perform in chess than someone who is lower. ",null,0,cdonux9,1rkutv,askscience,top_week,2
vincentrevelations,"As long as you're *learning* chess and studying moves you'll enhance your cognitive abilities. Once the game, or rather the opponents you encounter, begin to lack challenge the only thing you're training is your skill in recognizing chess set ups faster. 

Studies like the one in [this recent post](http://www.reddit.com/r/science/comments/1qdfvn/a_study_shows_that_older_adults_mean_age_72/) support this. I link the Reddit post because the discussion diverges to questions like yours.",null,1,cdogmyo,1rkutv,askscience,top_week,2
albasri,"It used to be thought that chess masters had extraordinary memories, but this was shown to be untrue. Shown a board position for a very brief period of time, a chess master can reconstruct it from memory with much greater accuracy than a novice. However, if pieces are randomly arranged on the board, then experts are just as bad as novices. This was taken to indicate that the difference between experts and novices is that experts are able to rapidly extract features and detect patterns or structure in arrangements of chess positions, forming simpler representational units that can be encoded faster and with greater accuracy. These are typical characteristics of perceptual learning and exist in many domains of expertise, e.g. bird watchers, radiologists, etc. 

See Chase and Simon's chapter in *Visual information processing* (1973) and *Thought and choice in chess* by de Groot (1946/1978). The latter work showed that there was no difference between masters and controls in general cognitive abilities, only this memory difference for board positions. Another useful resource could be *The psychology of chess skill* by Holding (1985). 

(as an aside - I feel that this question is more appropriate for psychology than neuroscience…)

edit: grammar x2",null,22,cdoaul2,1rkutv,askscience,top_week,146
darkheritage,"There has been some buzz the last few years of research being done on chess' ability to increase cognitive ability, more specifically on how it relates to (oddly enough) Alzheimer's.  Studies have shown that playing chess can increase cognitive ability and help keep the disease in check.
Here is a brief overview:[From Chessbase](https://chessbase.com/post/checkmating-alzheimers-disease-210513)

Edit:  I know little about the subject, I am just an amateur chess player who has read some of the articles over the last few years.",null,6,cdobpox,1rkutv,askscience,top_week,16
honkycat1,"I don't think whether or not chess masters performance are due to nature of nurture quite answers your original question. 

Recent research has shown that cognitive games (those mental exercises, e.g. puzzles, mind games, etc) don't really help general mental ability. They only help one's ability to play those games in a very context specific way. This is a pretty sensitive topic, as a lot of companies have made a ton of money selling mind-exercises because of the argument that, like a muscle, you need to train the mind. While the general premise of this is true, empirical evidence does not show that mind games improve anything other than performance on those mind games. (Unfortunately, I don't have reference off the top of my head, but either way, I'm sure there are plenty of support on both sides of this argument) Therefore, I don't think playing chess will make someone smarter, in anything other than chess. But again, this is a hotly debated area, there is also contradicting evidence.

As for expert performance in chess. Hambrick is a colleague of mine so I am somewhat familiar with his work. He is on the camp of ""nature"" in the nature vs nurture debate. He has shown that working memory is related to expert performance of piano players even at the highest level, which indicate that regardless of practice, basic cognitive abilities is related to performance. My personal research is in job performance and research show that general mental ability can predict job performance even at the highest level of performance. Bottom line is, practice aside, your disposition will always play a role. 

When you ask ""In other words, it seems that I have a better chance of reaching a more ""beneficial"" conclusion if I could process 5 units of information with a superior ability to synthesize them, than if I could process 10 units of information with a poor ability to synthesize them. "" Of course, working memory alone doesn't predict your performance in chess. There are other factors, and there is also random noise. But holding other variables constant, someone higher in working memory will ON AVERAGE out perform in chess than someone who is lower. ",null,0,cdonux9,1rkutv,askscience,top_week,2
vincentrevelations,"As long as you're *learning* chess and studying moves you'll enhance your cognitive abilities. Once the game, or rather the opponents you encounter, begin to lack challenge the only thing you're training is your skill in recognizing chess set ups faster. 

Studies like the one in [this recent post](http://www.reddit.com/r/science/comments/1qdfvn/a_study_shows_that_older_adults_mean_age_72/) support this. I link the Reddit post because the discussion diverges to questions like yours.",null,1,cdogmyo,1rkutv,askscience,top_week,2
yeast_problem,"Newtons law and the Heat Equation do apply to a hot surface in contact with air, at any instant in time.

But this is a complex thing to model mathematically when you add gravity and buoyancy. With no air movement the temperature difference would continue to reduce and the rate of heat transfer fall until the temperatures equalise, like any other heat transfer, in the absence of any further boundary conditions.  But with buoyancy the air will move away from the hot surface bringing in cold air again, so the rate of transfer is going to depend on how fast the buoyancy works, which is temperature dependent.

So Newtons law wont work for the heat transfer between the hot surface and the bulk of the air it is in contact with over a range of temperature differences. But is does apply if you were to integrate the the heat equation across the surface and the boundary layer of the air taking account of the buoyancy and viscosity. At moderate temperature differences the flow becomes turbulent making solutions harder.

Thermal modelling in buildings often assumes that Newtons law applies for simplicity, but more complex models will use a temperature dependent function to calculate the convection rate.
",null,0,cdockqu,1rkw0i,askscience,top_week,2
newoldwave,"Convective rate of heat transfer depends upon the coefficient of heat transfer of the two materials and the temperature difference between them. For a fluid moving across a solid, the temperature difference fluctuates of course. Coolant flowing through a radiator for instance.  ",null,0,cdo9db0,1rkw0i,askscience,top_week,1
therationalpi,"The two slit experiment is based on wave physics, so it works with sound waves, light waves, water gravity waves, etc. It's most noticeable when the wavelength is on the same order as the distance between the slits, and the slits are small with respect to the wavelength. For example, if I wanted to do this experiment with sound with a 1 m wavelength, then I'd want the slits no more than 10 meters apart, and I'd like the slits to be less than 10 cm.",null,1,cdo9vwh,1rky8r,askscience,top_week,4
fortunecooki," As the double slit experiment is based on a wave model, sound waves and even ocean waves show this too. Even in nature, you see the same effect when a wave goes through two rocks:
http://bsbh.wikispaces.com/file/view/diffraction.jpg/214660796/377x392/diffraction.jpg",null,1,cdodpvb,1rky8r,askscience,top_week,4
iorgfeflkd,"Yes it does. Any type wave, like surface waves in a lake, etc. There's nothing mystical or spooky about it.",null,1,cdo9tg2,1rky8r,askscience,top_week,2
Armadyll,"Firstly, you need to understand what 1440p means. the 1440 refers to the number of pixels in the vertical axis. The normal aspect ratio of HD screens is 16:9 making the horizontal axis 2560 pixels. This is a total of 3,686,400 pixels which is much higher than the 2,073,600 pixels of a 1080p screen. 
Our eyes can resolve (tell the difference between) things about 1' (1/60 of a degree) apart. The angle between the pixels depends, of course, upon the distance between the pixels and your eye. To test this look at your monitor from 10m away and see if you can resolve any of the pixels. You can't. Now try it with your face pressed against the screen. You probably can. The only thing that has changed was the distance between your eye and the pixels. What does all of this mean? You can make 1440p screens much larger than their 1080p counterparts without being able to resolve any of the pixels, and this is vital to maintaining a good picture quality. To answer your question, the limit to how many pixels per inch we can resolve depends on the distance at which the screen is viewed. ",null,1,cdofh1w,1rkyt0,askscience,top_week,15
dakami,"I've held in my hands a 20"" 4k tablet; images on it look like printed photographs (more accurately, they look like photographs printed onto plastic and backlit).

So size and distance really do matter.

Where things get interesting is when we get up to around 32K to 64K displays; those start getting dense enough that you can have pixels emitting at only certain angles.  That lets you actually have light field displays, i.e. monitors that have true 3D and appear as windows instead of surfaces.  I've seen early versions of these; they're amazing.",null,1,cdoi55u,1rkyt0,askscience,top_week,7
bunjay,"The detail we can see is always going to depend on pixel density and distance to the image vs your eyesight.

I think you might be trying to ask what resolution it would take for the individual pixels to be indistinguishable from one another at even the closest distance. Regular eyesight can distinguish high contrast line pairs about 0.003 inches apart at a distance of 10 inches. So if you had a display with about 700 lines per inch or more it could display high contrast detail at or above the resolving power of the average person's eyes even at the closest distance they can focus. This would shrink a 1080p display down to an inch and a half wide, and possibly as little as half that size would be required for someone with exceptional vision.",null,0,cdojmr5,1rkyt0,askscience,top_week,5
littlemisfit,"If your TV is less than 50 inchs and you don't sit closer than 10 feet from your screen then you won't even be able tell the difference between 720p and 1080p, so getting a 4000k TV is a complete waste of money unless you have a massive screen or sit really close to the TV.  [This site](http://www.digitaltrends.com/home-theater/720p-vs-1080p-can-you-tell-the-difference-between-hdtv-resolutions/) has a chart that shows what resolution is best based on the TV size and viewing distance.

Source: http://www.audioholics.com/hdtv-formats/1080p-and-the-acuity-of-human-vision",null,10,cdoffaa,1rkyt0,askscience,top_week,12
expertunderachiever,"Lot of good answers but honestly where things are lacking isn't in the DPI area but in the PNSR area.  It's not hard to see MPEG artifacts on a 40"" TV even at 10-14ft away and that will always trigger a ""this is a fake image"" response from your brain.

",null,0,cdovwel,1rkyt0,askscience,top_week,3
ITRAINEDYOURMONKEY,"Back in my undergrad Optics course (physics optics, not physiology), we did a kind of back-of-the-envelope approximation. Based only on the pupil size (not retina), we came up with about 5MP at arm's length. This is only based on treating the eye as a lens system, not as a CCD.

I read a post on here a couple months ago claiming much higher resolution based on the retina's physiology, but I'm afraid I can remember neither the numbers they gave nor the exact reasoning process to get there.",null,0,cdp04bn,1rkyt0,askscience,top_week,2
Armadyll,"Firstly, you need to understand what 1440p means. the 1440 refers to the number of pixels in the vertical axis. The normal aspect ratio of HD screens is 16:9 making the horizontal axis 2560 pixels. This is a total of 3,686,400 pixels which is much higher than the 2,073,600 pixels of a 1080p screen. 
Our eyes can resolve (tell the difference between) things about 1' (1/60 of a degree) apart. The angle between the pixels depends, of course, upon the distance between the pixels and your eye. To test this look at your monitor from 10m away and see if you can resolve any of the pixels. You can't. Now try it with your face pressed against the screen. You probably can. The only thing that has changed was the distance between your eye and the pixels. What does all of this mean? You can make 1440p screens much larger than their 1080p counterparts without being able to resolve any of the pixels, and this is vital to maintaining a good picture quality. To answer your question, the limit to how many pixels per inch we can resolve depends on the distance at which the screen is viewed. ",null,1,cdofh1w,1rkyt0,askscience,top_week,15
dakami,"I've held in my hands a 20"" 4k tablet; images on it look like printed photographs (more accurately, they look like photographs printed onto plastic and backlit).

So size and distance really do matter.

Where things get interesting is when we get up to around 32K to 64K displays; those start getting dense enough that you can have pixels emitting at only certain angles.  That lets you actually have light field displays, i.e. monitors that have true 3D and appear as windows instead of surfaces.  I've seen early versions of these; they're amazing.",null,1,cdoi55u,1rkyt0,askscience,top_week,7
bunjay,"The detail we can see is always going to depend on pixel density and distance to the image vs your eyesight.

I think you might be trying to ask what resolution it would take for the individual pixels to be indistinguishable from one another at even the closest distance. Regular eyesight can distinguish high contrast line pairs about 0.003 inches apart at a distance of 10 inches. So if you had a display with about 700 lines per inch or more it could display high contrast detail at or above the resolving power of the average person's eyes even at the closest distance they can focus. This would shrink a 1080p display down to an inch and a half wide, and possibly as little as half that size would be required for someone with exceptional vision.",null,0,cdojmr5,1rkyt0,askscience,top_week,5
littlemisfit,"If your TV is less than 50 inchs and you don't sit closer than 10 feet from your screen then you won't even be able tell the difference between 720p and 1080p, so getting a 4000k TV is a complete waste of money unless you have a massive screen or sit really close to the TV.  [This site](http://www.digitaltrends.com/home-theater/720p-vs-1080p-can-you-tell-the-difference-between-hdtv-resolutions/) has a chart that shows what resolution is best based on the TV size and viewing distance.

Source: http://www.audioholics.com/hdtv-formats/1080p-and-the-acuity-of-human-vision",null,10,cdoffaa,1rkyt0,askscience,top_week,12
expertunderachiever,"Lot of good answers but honestly where things are lacking isn't in the DPI area but in the PNSR area.  It's not hard to see MPEG artifacts on a 40"" TV even at 10-14ft away and that will always trigger a ""this is a fake image"" response from your brain.

",null,0,cdovwel,1rkyt0,askscience,top_week,3
ITRAINEDYOURMONKEY,"Back in my undergrad Optics course (physics optics, not physiology), we did a kind of back-of-the-envelope approximation. Based only on the pupil size (not retina), we came up with about 5MP at arm's length. This is only based on treating the eye as a lens system, not as a CCD.

I read a post on here a couple months ago claiming much higher resolution based on the retina's physiology, but I'm afraid I can remember neither the numbers they gave nor the exact reasoning process to get there.",null,0,cdp04bn,1rkyt0,askscience,top_week,2
outerspacepotatoman9,"Light does slow as it passes through a gas. It slows down as it passes through any medium with an index of refraction greater than one. Air, for example, has an index of refraction of 1.000293. So, the speed of light in air is .9997 times the speed of light in a vacuum.",null,0,cdohvxr,1rl04b,askscience,top_week,2
MasterPatricko,"A refractive index/slowing of light arises because the EM wave interacts with the charged parts of atoms. Gases, unlike liquids and solids, have a very low density of atoms so will always have a refractive index close to the vacuum -- *significant* slowing of light is unlikely.",null,0,cdowsbh,1rl04b,askscience,top_week,1
mc2222,"Recall that when we say ""polarization"" we're talking about the direction of the electric field of the EM wave.

A few observations/relevant comments:

* The double slit experiment is independent of polarization.

* Bragg diffraction is the molecular/atomic version of double (or multiple) slit interference, where the space between crystal planes act like a slit structure.  *The gaps are so small that they don't diffract visible light*, but rather diffract x-rays.  Hence the field of ""X-Ray Crystallography"".

Polarizing films are, as you indicate, sometimes made of long strings of molecules with gaps between them.  The spacing between molecules is so small the light won't be diffracted.  However, if the polarization of light is appropriate, the electric field will exert a force on the valence electrons in the molecule, driving them up and down along the molecular chain.  Since the optical energy is being converted to energy to drive the electrons back and forth, the EM wave is absorbed, and thus blocked.  

The component of polarization in the direction perpendicular to these long chains is unable to drive valence electrons back and forth (since there is no chain like structure for them to travel along), and thus, this component of the electric field is transmitted through the polarizing sheet.

It's worth mentioning that these polarizers work for things like sunglasses and inexpensive polarizers, but scientific grade polarizers rely on different methods of achieving good polarization.",null,0,cdoff2r,1rl2n8,askscience,top_week,6
failuer101,"that's a really good question. i think it's because the way the polarized lenses are designed which causes only one pattern of light through. the light that is let through will act like a wave but only in one direction so it doesn't interfere with the other beams of light that make it through.

[here](http://www.microscopyu.com/articles/polarized/images/polarizedlightfigure1.jpg) is an image of polarized light. now if you can imagine many sets of these vertical waves side by side, they wouldn't overlap, therefore they wouldn't interfere with each other. i think the double slit experiment doesn't polarize the light.

edit: just watch this video [sixty symbols video](https://www.youtube.com/watch?v=KM2TkM0hzW8)

",null,2,cdof8hl,1rl2n8,askscience,top_week,1
Gplads,"I may be looking in the wrong places, but the only stuff I've been able to find talks about protective instinct specifically in new mothers (and even then it's about mice.) Altruistic behavior in general though could be an example of costly signaling theory, in which an individual exposes themselves to risk or sacrifices resources (time, energy, food) in exchange for benefits such as social recognition or mating preference. I could imagine protective instinct as being an extension of this, in which an individual puts himself at risk to keep another member of their group alive, with the assumption that it would be done for them, if needed. I would really like to hear a more informed answer though.

Edit: here is an article proposing that altruistic behavior in humans is a costly signal for general intelligence.

https://lirias.kuleuven.be/bitstream/123456789/101194/1/Millet
",null,0,cdof0b1,1rl2qw,askscience,top_week,1
stuthulhu,"Planets are pulled into spherical shapes by the force of gravity. One of the defining features of a planet is that its mass is substantial enough that it overcomes the [compressive strength](http://en.wikipedia.org/wiki/Compressive_strength) of its material, forcing it into a spherical shape that is balanced by [hydrostatic equilibrium](http://en.wikipedia.org/wiki/Hydrostatic_equilibrium#Planetary_geology). ",null,0,cdoe7ut,1rl37t,askscience,top_week,11
hovissimo,"Well, planets **by definition** are spherical (or very close).

http://en.wikipedia.org/wiki/Definition_of_planet#Hydrostatic_equilibrium

http://en.wikipedia.org/wiki/IAU_definition_of_planet

There are plenty of objects orbiting the sun that definitely aren't spherical, but we don't call them planets.  There are even some spherical objects (like [Ceres](http://en.wikipedia.org/wiki/Ceres_%28dwarf_planet%29)) that we don't consider planets because they don't meet other planetary criteria.



___

To answer the question I think you intended to ask, it's because of gravitational attraction.  The largest bodies in our solar system have enough mass (enough *stuff*) in them that their mutual gravitation ""pulls"" them into a mostly spherical shape.

A good analogy would be a water droplet on a piece of glass, the water drop also ""pulls"" itself into a spherical-ish shape because the molecules of water mutually attract each other.  (Note: This is not a good example, because it's not gravity that makes water bead like this, but [Cohesion](http://en.wikipedia.org/wiki/Cohesion_%28chemistry%29).)

  That cohesion article on wikipedia has a picture of some water floating in microgravity in space, and you can see that the water is much closer to a perfect sphere than it gets here on the surface of Earth.

Edit: newlines",null,0,cdoeiri,1rl37t,askscience,top_week,5
natty_dread,"Because a sphere is the energetically most desirable state.

Gravity is the ""glue"" that holds the gas together, but there are some other things to consider:

The surface of a body (no matter if it is solid or a fluid) is on an energetically higher level than the particles inside the body.

Since a system always wants to be in the state of least potential energy, the system wants to have a surface as small as possible.

On the other hand, there is a pressure inside the sphere, due to the kinetic energy of the particles. This pressure wants the body to take a shape of maximum volume.

These two factors are working against each other, hence the system will choose a shape that has maximum volume while sustaining minimum surface.

The shape of the highest volume to surface ratio is the sphere. That is why Planets - or soap bubbles for that matter - take the shape of spheres.
",null,1,cdoiky2,1rl37t,askscience,top_week,2
null,null,null,7,cdody0j,1rl37t,askscience,top_week,2
dalgeek,"In an ideal world the wireless client associates with the AP with the loudest signal.  A ""perfect"" signal is around -35dBm (if you're directly next to the AP) and most wireless cards disconnect around -85dBm.  This range  is reduced if lower data rates are disabled; the lower the data rate, the further away you can be, but this reduces the performance of all clients.

This gets tricky when you mix 2.4GHz and 5GHz.  The 5GHz radios operate at half the power of 2.4GHz radios and the signal attenuates (degrades) faster than 2.4GHz, BUT even with a supposedly weaker signal a client can get faster speeds from 802.11a.

Roaming is also up to the client.  If the client can see two APs at -55dBm then it's pretty much a 50/50 chance on which it will pick.  The client will associate to one and only attempt the other if the signal level drops too far.  Some devices will maintain a death grip on an AP before roaming, while others will hop between APs like a jack rabbit.

A lot of this depends on the driver implementation.  For example, iDevices up until iOS 4 or 5 preferred 2.4GHz radios over 5GHz radios even if the 5GHz could give a higher data rate.  After that point the developers swapped the preference.  The same thing happened with Dell Inspiron wireless drivers.  The drivers also determine how the signal strength is calculated.

EDIT: Quick guide to frequencies and speeds (physical data rate)

* 802.11b - 2.4GHz - 1, 2, 5.5, 11 (Mbps)
* 802.11g - 2.4GHz - 6, 9, 12, 18, 24, 36, 48, 54 (Mbps)
* 802.11a - 5GHz - 6, 9, 12, 18, 24, 36, 48, 54
* 802.11a/n - 5GHz - 15, 30, 45, 60, 90, 120, 135, 150
* 802.11b/g/n - 2.4GHz - 7.2, 14.4, 21.7, 28.9, 43.3, 57.8, 65, 72.2

Note that ""n"" works on both 2.4 and 5GHz.  This is why it is important to look for a/n because a lot of consumer devices that advertise 802.11n are actually running at 2.4GHz.",null,0,cdoejwh,1rl3px,askscience,top_week,23
iBeReese,"Funfact: This is a great question that I would love to know the answer too, but is isn't actually a computer science question. CS is the field devoted to software that straddles the line of applied math and engineering. This question is in the realm of computer engineering and information technology more than CS.",null,0,cdomvyi,1rl3px,askscience,top_week,5
dalgeek,"In an ideal world the wireless client associates with the AP with the loudest signal.  A ""perfect"" signal is around -35dBm (if you're directly next to the AP) and most wireless cards disconnect around -85dBm.  This range  is reduced if lower data rates are disabled; the lower the data rate, the further away you can be, but this reduces the performance of all clients.

This gets tricky when you mix 2.4GHz and 5GHz.  The 5GHz radios operate at half the power of 2.4GHz radios and the signal attenuates (degrades) faster than 2.4GHz, BUT even with a supposedly weaker signal a client can get faster speeds from 802.11a.

Roaming is also up to the client.  If the client can see two APs at -55dBm then it's pretty much a 50/50 chance on which it will pick.  The client will associate to one and only attempt the other if the signal level drops too far.  Some devices will maintain a death grip on an AP before roaming, while others will hop between APs like a jack rabbit.

A lot of this depends on the driver implementation.  For example, iDevices up until iOS 4 or 5 preferred 2.4GHz radios over 5GHz radios even if the 5GHz could give a higher data rate.  After that point the developers swapped the preference.  The same thing happened with Dell Inspiron wireless drivers.  The drivers also determine how the signal strength is calculated.

EDIT: Quick guide to frequencies and speeds (physical data rate)

* 802.11b - 2.4GHz - 1, 2, 5.5, 11 (Mbps)
* 802.11g - 2.4GHz - 6, 9, 12, 18, 24, 36, 48, 54 (Mbps)
* 802.11a - 5GHz - 6, 9, 12, 18, 24, 36, 48, 54
* 802.11a/n - 5GHz - 15, 30, 45, 60, 90, 120, 135, 150
* 802.11b/g/n - 2.4GHz - 7.2, 14.4, 21.7, 28.9, 43.3, 57.8, 65, 72.2

Note that ""n"" works on both 2.4 and 5GHz.  This is why it is important to look for a/n because a lot of consumer devices that advertise 802.11n are actually running at 2.4GHz.",null,0,cdoejwh,1rl3px,askscience,top_week,23
iBeReese,"Funfact: This is a great question that I would love to know the answer too, but is isn't actually a computer science question. CS is the field devoted to software that straddles the line of applied math and engineering. This question is in the realm of computer engineering and information technology more than CS.",null,0,cdomvyi,1rl3px,askscience,top_week,5
adamsolomon,"The energy was lost to the expansion of the Universe. Energy isn't actually conserved as the Universe expands, because energy is only conserved when the laws of physics don't depend on time, whereas an expanding Universe clearly does change with time.

The Universe cools down as it expands because radiation (like light) *redshifts*, meaning the expansion of the Universe stretches light waves to longer and longer wavelengths. Those longer wavelengths correspond to lower energies, and energy corresponds to temperature, so the temperature of the radiation cools down. Similarly, the expansion slows down matter particles running around, so their kinetic energy and therefore their temperature also decreases with the expansion.",null,1,cdoh4h3,1rl406,askscience,top_week,7
Slave_to_Logic,"The ""white"" light coming from your flashlight is leaving the bulb and traveling towards the red plastic.  White light is of course all colors of light traveling together.  

When this white light hits the red plastic, the part of the light that is red continues through, but all of the other colors of the rainbow are blocked and absorbed into the plastic.

",null,0,cdodi6e,1rl4gp,askscience,top_week,13
ramk13,"To add to what /u/Slave_to_Logic said, the plastic is absorbing the non-red colors. The same is true for other colors of translucent/transparent materials. Here's a plot of what gets absorbed for different types of dyes (food coloring in this case):
http://www.nsta.org/images/news/legacy/jcst/0705/FavDemoFig5.jpg

The bar at the bottom is the color of the light at that wavelength. So red dyes absorb green light strongly and some blue light too. What's left over (what you see coming out of the flashlight) is the red light not absorbed.",null,0,cdodsy9,1rl4gp,askscience,top_week,3
wazoheat,"""Radiation"" is a scary word for the public because they only know of it in the context of nuclear bombs and nuclear power plants. In reality, ""radiation"" is just a word that means something emitted from something else. In most scientific contexts, the word ""radiation"" is referring to ""electromagnetic radiation"", which is the generic term for waves given off in the form of alternating electric and magnetic fields.

""Electromagnetic radiation"" actually covers a broad spectrum of things you experience in your day-to-day life. [X-rays, UV light, visible light, microwaves, and radio waves are all just electromagnetic waves at different energies](http://www2.chemistry.msu.edu/faculty/reusch/VirtTxtJml/Spectrpy/Images/emspec.gif): in reality they are all the same thing. The only difference is the wavelength of these electromagnetic waves: x-rays have the shortest wavelength (most energy), radio waves have the longest wavelength (least energy).

All objects in the universe emit electromagnetic radiation at all times; the energy of this radiation is directly related to the temperature of that object. On earth, this radiation is typically at [infrared](https://en.wikipedia.org/wiki/Infrared) wavelengths, which are less energetic than visible light. Hotter objects will emit higher-energy wavelengths, which our eyes can detect as ""visible light"". This is the reason why objects glow when you heat them up.

Electromagnetic radiation *can* be dangerous, but *only if* the frequency of the waves is high enough. X-rays and UV-light can be bad for you in high doses because at high energies, this radiation can break sensitive molecular bonds in your cells, particularly in your DNA. And while your body has mechanisms for repairing this damage, if you receive too much high-energy radiation in a short period of time the repair mechanisms can not keep up, leading to permanent damage, and possibly cancer if the DNA is damaged enough. **But they key point here is that this damage can only occur from high-energy electromagnetic radiation**: lower-energy EM radiation, such as visible light, microwaves, and radio waves, *do not have enough energy to break molecular bonds*, and so can not cause ill effects in this manner.

The ""radiation"" that electronic devices emit work in a wide range of wavelengths. Remote controls use signals in the infrared. Cell phones and other wireless devices tend to use microwave wavelengths. Broadcast TV and radio use radio waves. But **all of these signals are at lower energy than visible light**, and so can not cause the same ill effects that higher-energy EM radiation can.

**In summation: electronics emit electromagnetic radiation at lower energies than the radiation you get from everyday normal objects, and can not hurt living tissue.**",null,0,cdofa5v,1rl8a4,askscience,top_week,6
stefvonb,"There is always a danger with regards to radiation. These are called stochastic effects (the probability of the effect increases with the amount of exposure, or *dose*).

However, electronics usually give off radiation with low frequencies, such as visible light, microwaves, infrared, or even radio waves. These types of radiation are not primarily what we call *ionizing radiation*. It is ionizing radiation which has the power to strip the electrons off of the molecules in our bodies, like those present in DNA. Ionizing radiation is primarily responsible for the risk of cancer, as far as radiation is concerned. Electronics don't give off much ionizing radiation.

Yes, there is a lot of speculation, but electronics do not present much of a risk.

EDIT: This is more of a physics question. Hope my answer is satisfactory.",null,0,cdodzr2,1rl8a4,askscience,top_week,1
iamdelf,"These sorts of clinical trials are actually interesting to design.  http://www.ncbi.nlm.nih.gov/pubmed/9708750  Has the abstract of one of the original clinical trials.  Basically someone comes into a clinic and asks for emergency contraceptive.  The clinic asks the person if they are interested in participating in a clinical trial of a new medication.  They collect the results both as far as efficacy(pregnant/not pregnant) as well as side effects to compare it to currently available contraceptives.

The compounds used aren't new, its the same chemicals which are already approved for daily contraception.  It is a new indication trial and you compare it to the standard accepted treatment to see if it is any better than what is available.",null,9,cdoiopu,1rl9fc,askscience,top_week,67
null,null,null,21,cdohhj0,1rl9fc,askscience,top_week,52
takeandbake,"I am going to respond to the part of your question about efficacy of emergency contraception (EC) in women over 80kg.

Researchers were looking at the data sets for 2 randomized controlled clinical trials, each one about a different medication used as EC.  The purpose of the meta-analysis was to find risk factors for EC failure, which would be pregnancy.  They found that obese women, as defined by a body weight over 80kg, were more likely than non-obese women to become pregnant despite using EC.

Also note that it's not that it ""doesn't work"" for women over 80kg, it is less likely to be effective.",null,1,cdomofz,1rl9fc,askscience,top_week,6
housebrickstocking,"Just remember some ""testing"" in science is in fact mathematics, chemistry, and physics extrapolated with some real world observations to support the outcomes. A lot of people imagine testing as needing to be a lab experiment rather than just a mathematical proof.

Note: This is not really relevant to the topic, excepting that the recent news about Plan B comes from more or less the above method.",null,2,cdor76y,1rl9fc,askscience,top_week,3
null,null,null,4,cdoj2n1,1rl9fc,askscience,top_week,2
null,null,null,8,cdoi4z7,1rl9fc,askscience,top_week,1
null,null,null,20,cdoh6qd,1rl9fc,askscience,top_week,4
iamdelf,"These sorts of clinical trials are actually interesting to design.  http://www.ncbi.nlm.nih.gov/pubmed/9708750  Has the abstract of one of the original clinical trials.  Basically someone comes into a clinic and asks for emergency contraceptive.  The clinic asks the person if they are interested in participating in a clinical trial of a new medication.  They collect the results both as far as efficacy(pregnant/not pregnant) as well as side effects to compare it to currently available contraceptives.

The compounds used aren't new, its the same chemicals which are already approved for daily contraception.  It is a new indication trial and you compare it to the standard accepted treatment to see if it is any better than what is available.",null,9,cdoiopu,1rl9fc,askscience,top_week,67
null,null,null,21,cdohhj0,1rl9fc,askscience,top_week,52
takeandbake,"I am going to respond to the part of your question about efficacy of emergency contraception (EC) in women over 80kg.

Researchers were looking at the data sets for 2 randomized controlled clinical trials, each one about a different medication used as EC.  The purpose of the meta-analysis was to find risk factors for EC failure, which would be pregnancy.  They found that obese women, as defined by a body weight over 80kg, were more likely than non-obese women to become pregnant despite using EC.

Also note that it's not that it ""doesn't work"" for women over 80kg, it is less likely to be effective.",null,1,cdomofz,1rl9fc,askscience,top_week,6
housebrickstocking,"Just remember some ""testing"" in science is in fact mathematics, chemistry, and physics extrapolated with some real world observations to support the outcomes. A lot of people imagine testing as needing to be a lab experiment rather than just a mathematical proof.

Note: This is not really relevant to the topic, excepting that the recent news about Plan B comes from more or less the above method.",null,2,cdor76y,1rl9fc,askscience,top_week,3
null,null,null,4,cdoj2n1,1rl9fc,askscience,top_week,2
null,null,null,8,cdoi4z7,1rl9fc,askscience,top_week,1
null,null,null,20,cdoh6qd,1rl9fc,askscience,top_week,4
SyNNeR6x3,"The capacity of the human stomach varies, on average the stomach can expand to hold up to 4 L (4.2 qts.) of food, expanding to about 50 times its volume when empty. When it is completely filled, the human bladder can hold on average 680 ml of urine; the urge to urinate however can start at about 250 ml.

TL; DR: Stomach: 4L (4.2 qts) Bladder: 680 ml
",null,2,cdoijew,1rl9yg,askscience,top_week,21
SyNNeR6x3,"The capacity of the human stomach varies, on average the stomach can expand to hold up to 4 L (4.2 qts.) of food, expanding to about 50 times its volume when empty. When it is completely filled, the human bladder can hold on average 680 ml of urine; the urge to urinate however can start at about 250 ml.

TL; DR: Stomach: 4L (4.2 qts) Bladder: 680 ml
",null,2,cdoijew,1rl9yg,askscience,top_week,21
seroevo,"Are you asking whether it can in general, or whether it does for you specifically if you play driving games?

It's been shown to make a difference for people already, where those with extensive game time in Forza or GT can drive a track for the first time in RL and put up a significantly better time and display of skill than a person otherwise totally inexperienced. ",null,7,cdoem4e,1rla91,askscience,top_week,34
rtechnix,"Short answer, [yes](http://www.popularmechanics.com/cars/news/industry/can-you-learn-real-racing-skills-from-gran-turismo-16055980). Long answer, it's arguable that such people were bound to be good drivers naturally, but just as pilots train in simulators, a sufficiently realistic game should test/build up your reactions/reflexes that can translate to actual driving. In more realistic games like the Gran Turismo series, you will learn much more about how to pick a racing line as well and in general get a better grasp on how cars react to steering/braking/etc and changes to the traction. Of course, there will be a few real life things that simulators won't be able to emulate.

That and many racers and other with hefty experience in real racecars have often spoken highly of the realism of some games like [GT](http://en.wikipedia.org/wiki/Gran_Turismo_4#Reception) and [iRacing](http://www.iracing.com/testimonials/). In fact iRacing claims some racers even use it sometimes to get a feel of a track before they can actually drive there (though take it with a grain of salt like most promotional statements).

IIRC I think the biggest challenge from that one guy who went from winning the GT competition to an actual race in a real race car was that he wasn't ready for the forces you can experience and his neck hurt from the lateral g's.",null,4,cdok7j4,1rla91,askscience,top_week,15
null,null,null,0,cdogn4z,1rla91,askscience,top_week,2
null,null,null,4,cdoi33r,1rla91,askscience,top_week,8
null,null,null,2,cdoizca,1rla91,askscience,top_week,4
null,null,null,1,cdopck7,1rla91,askscience,top_week,4
null,null,null,2,cdop1vt,1rla91,askscience,top_week,2
null,null,null,1,cdom4yw,1rla91,askscience,top_week,1
Cyanotical,"playing a racing sim will not improve them per say as you can not get a proper feel for g-force and tire traction, but there has been at least one study that showed playing video games in general makes you a better driver due to increased situational awareness, reduced reaction time, and the ability to make faster on-the-fly decisions

I can't seem to find the study atm, [there are plenty of news articles,](http://www.theregister.co.uk/2010/09/14/action_games_make_you_a_finer_human_being/) but direct links are a 404",null,2,cdoo46q,1rla91,askscience,top_week,1
null,null,null,9,cdogowl,1rla91,askscience,top_week,4
null,null,null,11,cdoed4d,1rla91,askscience,top_week,5
null,null,null,10,cdoere2,1rla91,askscience,top_week,3
seroevo,"Are you asking whether it can in general, or whether it does for you specifically if you play driving games?

It's been shown to make a difference for people already, where those with extensive game time in Forza or GT can drive a track for the first time in RL and put up a significantly better time and display of skill than a person otherwise totally inexperienced. ",null,7,cdoem4e,1rla91,askscience,top_week,34
rtechnix,"Short answer, [yes](http://www.popularmechanics.com/cars/news/industry/can-you-learn-real-racing-skills-from-gran-turismo-16055980). Long answer, it's arguable that such people were bound to be good drivers naturally, but just as pilots train in simulators, a sufficiently realistic game should test/build up your reactions/reflexes that can translate to actual driving. In more realistic games like the Gran Turismo series, you will learn much more about how to pick a racing line as well and in general get a better grasp on how cars react to steering/braking/etc and changes to the traction. Of course, there will be a few real life things that simulators won't be able to emulate.

That and many racers and other with hefty experience in real racecars have often spoken highly of the realism of some games like [GT](http://en.wikipedia.org/wiki/Gran_Turismo_4#Reception) and [iRacing](http://www.iracing.com/testimonials/). In fact iRacing claims some racers even use it sometimes to get a feel of a track before they can actually drive there (though take it with a grain of salt like most promotional statements).

IIRC I think the biggest challenge from that one guy who went from winning the GT competition to an actual race in a real race car was that he wasn't ready for the forces you can experience and his neck hurt from the lateral g's.",null,4,cdok7j4,1rla91,askscience,top_week,15
null,null,null,0,cdogn4z,1rla91,askscience,top_week,2
null,null,null,4,cdoi33r,1rla91,askscience,top_week,8
null,null,null,2,cdoizca,1rla91,askscience,top_week,4
null,null,null,1,cdopck7,1rla91,askscience,top_week,4
null,null,null,2,cdop1vt,1rla91,askscience,top_week,2
null,null,null,1,cdom4yw,1rla91,askscience,top_week,1
Cyanotical,"playing a racing sim will not improve them per say as you can not get a proper feel for g-force and tire traction, but there has been at least one study that showed playing video games in general makes you a better driver due to increased situational awareness, reduced reaction time, and the ability to make faster on-the-fly decisions

I can't seem to find the study atm, [there are plenty of news articles,](http://www.theregister.co.uk/2010/09/14/action_games_make_you_a_finer_human_being/) but direct links are a 404",null,2,cdoo46q,1rla91,askscience,top_week,1
null,null,null,9,cdogowl,1rla91,askscience,top_week,4
null,null,null,11,cdoed4d,1rla91,askscience,top_week,5
null,null,null,10,cdoere2,1rla91,askscience,top_week,3
Smoothened,"Decussation, or cross-wiring in neural circuits is thought to confer some form of functional advantage because it's prevalent across animal taxa. The exact advantage is not entirely understood, but research suggests that decussation prevents wiring errors in complex 3D networks. [This paper](http://onlinelibrary.wiley.com/doi/10.1002/ar.20731/pdf), for example, shows that decussation makes complex wiring networks more robust and offers a mathematical explanation. As a molecular neuroscientist, trying to read the paper gave me a headache, but you might find it useful.  ",null,2,cdofogt,1rlc83,askscience,top_week,37
null,null,null,1,cdojhgk,1rlc83,askscience,top_week,5
howlin,"It is stranger than that.  Not only are our left and right crossed, but our front and back are also crossed.  We do sensory processing in the back of our brain (farthest away from the eyes), and our motor planning in the from of our brain (farthest away from our spinal cord).

It's believed this was an evolutionary accident that occurred when vertebrates and arthropods diverged. The arthropod nervous system runs along the inside of their bodies, while our nervous system runs along our back. Anatomically, the best way to explain this is that at some point, vertebrates flipped their body plan 180 degrees, but the brain remained stationary.  

You can read a little more about this in  ""The Upright Ape: A New Origin of the Species"" By Aaron G. Filler if you want a high-level review

",null,2,cdok66w,1rlc83,askscience,top_week,5
null,null,null,2,cdofdzn,1rlc83,askscience,top_week,1
null,null,null,3,cdou7rp,1rlc83,askscience,top_week,1
Smoothened,"Decussation, or cross-wiring in neural circuits is thought to confer some form of functional advantage because it's prevalent across animal taxa. The exact advantage is not entirely understood, but research suggests that decussation prevents wiring errors in complex 3D networks. [This paper](http://onlinelibrary.wiley.com/doi/10.1002/ar.20731/pdf), for example, shows that decussation makes complex wiring networks more robust and offers a mathematical explanation. As a molecular neuroscientist, trying to read the paper gave me a headache, but you might find it useful.  ",null,2,cdofogt,1rlc83,askscience,top_week,37
null,null,null,1,cdojhgk,1rlc83,askscience,top_week,5
howlin,"It is stranger than that.  Not only are our left and right crossed, but our front and back are also crossed.  We do sensory processing in the back of our brain (farthest away from the eyes), and our motor planning in the from of our brain (farthest away from our spinal cord).

It's believed this was an evolutionary accident that occurred when vertebrates and arthropods diverged. The arthropod nervous system runs along the inside of their bodies, while our nervous system runs along our back. Anatomically, the best way to explain this is that at some point, vertebrates flipped their body plan 180 degrees, but the brain remained stationary.  

You can read a little more about this in  ""The Upright Ape: A New Origin of the Species"" By Aaron G. Filler if you want a high-level review

",null,2,cdok66w,1rlc83,askscience,top_week,5
null,null,null,2,cdofdzn,1rlc83,askscience,top_week,1
null,null,null,3,cdou7rp,1rlc83,askscience,top_week,1
xtxylophone,"Right now, best we can say is we dont really know. We can't get observational evidence of them having planets so we must rely on models and those arent quite perfect either or suited for those conditions.

So to throw a stab at an answer. Probably not 'planets' but there may well have been smaller failed stars orbiting other stars.",null,2,cdojsvn,1rlf3l,askscience,top_week,4
GeoGeoGeoGeo,"To build a planet you need lots of rubble and that means lots of heavy elements – stuff more massive than atoms of hydrogen and helium. The prevailing wisdom had been that the magic of stellar alchemy didn’t produce enough useful “star-stuff” to build terrestrial worlds until at least six or seven billion years after the Big Bang. In this sense a host stars [metallicity](http://en.wikipedia.org/wiki/Metallicity) has been previously assumed to be directly correlated to what kinds of planets it can be host to which was backed up by studies of exoplanets, frequently finding worlds around stars with a “metallicity” (i.e. a heavy element abundance) equal to or greater than our Sun. This leads to the conclusion then that the 1st generation stars simply were not able to be host to terrestrial (rocky) or gas-giant type planets (also with rocky [metallic] cores. However, the correlation that a stars metallicity is a strong indicator for planets, has been weakened, at least for smaller, terrestrial planets - **relative to our suns metallicity**, a pop I star. 

&gt;We find that planets with radii less than four Earth radii form around host stars with a wide range of metallicities (but on average a metallicity close to that of the Sun), whereas large planets preferentially form around stars with higher metallicities. - [Nature](http://www.nature.com/nature/journal/v486/n7403/full/nature11121.html)

Surveys do detect a decrease in the number of planet-hosting stars with decreasing metallicity, but this drop is much shallower for terrestrial planets than it is for gas giants. According to our current understanding the very 1st stars, termed [population III stars](http://astronomy.swin.edu.au/cosmos/P/Population+III) would have had no planets; pop II stars have low-metallicity *relative to our sun* and created all the other elements in the periodic table (except the more unstable ones). Though some older pop II stars have been found with [gas-giants orbiting them](http://arxiv.org/abs/1011.4938), they are believed to 'adopted' planets. Pop I stars (such as our sun) are metal-rich and can therefore be host to rocky planets. 

In order to make planets you clearly need metals, and therefore it is extremely unlikely to have planets formed alongside their metal deficient or even metal-poor host star. ",null,0,cdos4nj,1rlf3l,askscience,top_week,3
EvanRWT,"&gt;There weren't any heavy elements to speak of, so there obviously couldn't have been rocky planets like our own. But what about gas giants?

Our current models of planet formation (through accretion) require the presence of heavy elements. Even for gas giants. All of the gas giants in our own solar system have cores which consist of heavy elements. Though they may small in comparison to the mass of the whole planet, they are needed for the planet to begin forming.

This was the status until a few years ago. Most astronomers would have said that planets were unlikely among very old stars, because there was a scarcity of heavy elements.

However, more recently some very old extrasolar planets have been discovered. For example, the [Methuselah Planet](http://en.wikipedia.org/wiki/PSR_B1620-26_b) was discovered by Hubble near the core of the globular cluster M4. This [planet](http://hubblesite.org/newscenter/archive/releases/2003/19) is a gas giant, about 2.5 times the size of Jupiter, and about 12.7 billion years old. So it was formed about a billion years after the big bang. The star it orbits is of very low metallicity, and is now a white dwarf.

So right now there is a bit of a confusion. Apparently, planets could form very very early in the universe, as early as a billion years after the big bang. The ratio of heavy elements was very low at this time, so our present accretion models of planet formation don't really explain how they were formed. This is an active area of research. Right now, nobody knows how they were formed, but we have good evidence of very old planets.

But even this doesn't go back to the **very first** generation of stars (population III). We really can't speculate much about them because so far population III is hypothetical; no population III stars have ever been found.

The idea is that population III stars were very large, about 100-150 solar masses. Stars this size have very short lifetimes, so they burned out quickly and supernova'd, producing the first heavy elements. Given what we suspect about them, it seems highly unlikely that they had any planets.",null,0,cdphhs1,1rlf3l,askscience,top_week,2
camCut,"There was only hydrogen and helium in the early universe, where first generation stars ""lived"" in, no heavier elements like carbon, nitrogenium  or silicium. and I am not sure if the first gen stars lived long enough to allow planets to form around them.
edit: im in a train, to lazy to go deeper  ;)",null,3,cdofps8,1rlf3l,askscience,top_week,3
iCookBaconShirtless,"The problem with addressing this question is that there is no natural and intrinsic way to decide whether a non-zero number is ""close to zero"" or ""far from zero.""  The constants you listed are closer to zero than 10^10 but further from zero than 10^-10 .  Who is to say where we should place to cutoff between numbers that are ""close to"" or ""far from"" from zero?

They may strike you as being close to zero, but this is a question of psychology not mathematics.  Perhaps we deal with numbers that are further from zero than pi more often than numbers that are closer to zero than pi, so pi seems close to zero?

I'll add that the precise values of those constants might not be as fundamental as you think.  The ratio of the surface area to the diameter of a sphere in n dimensions, for example, is only pi when n=2.  However, it can be written in terms of pi for other dimensions.  In fact, many people argue that tau=2*pi should be thought of as more fundamental than pi.  I personally believe that any rational multiple of pi is equally ""fundamental"", whatever that means.  We just settled on one of them out of convenience and for historical reasons.

As for your second question, the reason that small primes are denser than large primes is fairly well understood.  See [the prime number theorem](http://en.wikipedia.org/wiki/Prime_number_theorem).

Edit: Think about it this way. OP is intrigued that so many constants are smaller than 5 in magnitude. But any finite list of numbers has an upper bound.  The list given by OP happens to have 5 as an upper bound.  So what is so surprising about 5 versus some other upper bound? It's tempting to answer that a number smaller than 5 is unlikely to occur if you uniformly draw a random positive number. But there is actually no mathematical way to make sense of drawing a real number uniformly. The lack of scale is the real culprit. You cannot claim that a number is intrinsically ""large"" or ""small"" in any meaningful way without choosing something to compare that number to.


**EDIT 2: To anyone who still thinks that the list of constants given by OP (or any other finite list of numbers) is intrinsically small in magnitude, please provide a list of number that you think is not small in magnitude.**

I am spending a lot of time responding to commenters that might be glossing over some subtleties of my argument.  If you wish to debate my comment, please first answer the question I posed in EDIT 2.  I think the doomed effort to answer this question will reveal some of the subtleties of the point I am making.

EDIT 3: I am in no way trying to discredit OP's question.  I agree with OP that the fact that so many named constants are less than 5 is surprising in a psychological sense.  But I contend that there is no way to answer the question of whether it is surprising mathematically.  The reasons for this are actually a bit more subtle than they first appear.  The problem is that *every* finite list of numbers has an upper bound.  So how surprising is it that there are upper bounds on OP's list that are less than 5?  Answering this would require defining some sense of a probability distribution on the positive real numbers.  But every probability distribution on the positive real numbers artificially imposes a scale because there is no uniform distribution on the positive reals.  So **you can only really ask whether a list of numbers is surprisingly close to zero with respect to some arbitrarily chosen scale.** 

",null,508,cdooolp,1rlfdu,askscience,top_week,1622
Vietoris,"I will try to give a different point of view on this question. It will be more geometrical and not exactly answering the question but that might still give some insight.

I will talk about [Constructible numbers](http://en.wikipedia.org/wiki/Constructible_number). The principle is the following. You start with the two points at distance 1 on a plane, and you have a compass and a straightedge. At each step of the process you will get new points as follows :

Imagine you are at step n and you have already constructed N points. You draw all possible lines passing through two of your N points.  You also draw all the circles centered at one of your N points with a radius equal to the distance between two of your N points. You mark all the intersections created between lines, circles or both and add all these points to your list of N points. 

For convenience I will see the points in the plane as complex numbers. A number is constructible if it can be obtained by this process. 

Let's give the first examples : 

* From the first two points 0 and 1, I can draw the line passing though these two points, the two circles of radius 1 centered on 0 and 1 respectively. This gives me 4 new points : -1, 2 , e^ipi/6 , e^-ipi/6

* From this new set of six points, I can draw 10 different lines. And I have four possible radius for my circles (1,2,3,sqrt(3)) and 6 possible centers which gives me more or less 24 new circles. So I get more than 100 new points (see the [picture](http://imgur.com/bE3dlfG) )

* And so on ... the number of points explodes very quickly.

Ok what was the point of all this ? Well it is interesting to note that all the (non-zero) numbers we can construct in 2 steps all have norm between 0.2 and 5. And more importantly, most (like 95% may be, I didn't compute it) have norm between 0.5 and 4. (see the [picture](http://imgur.com/bE3dlfG) )

When I go on with the steps I see that there is always a lower bound and an upper bound (obvious), and that most of the numbers have norm between 0.5 and 4. (less obvious but it's because, when you draw all the circles around your existing points, most of them will stay in the same range ...).

Now that we understand that, what next ? Well, suppose we could see the property of being ""interesting"" as a random occurence among numbers (this is my hypothesis, I am not exactly proving that this means anything). And to construct interesting numbers we have to do a certain number of steps. Equivalently, I construct all possible points in n steps and pick randomly some of them  (to say that they are interesting). With the arguments above, this would prove that :

* a majority (I have no idea how to exactly compute it) of the interesting numbers constructible in n steps lie in a rather restricted range between 0.5 and 5. 

* a few will be larger or smaller than that ... 

* And no interesting number constructible in n steps will have norm smaller than 10^-n or be bigger than 10^n (because no  number constructible in n steps can be that big)

&gt; Tl; DR : If I randomly pick numbers that could be defined in a certain number of steps, then most of the numbers I picked will be in the [0.5 , 5] range

Now, back to mathematical constant. How are they defined ? Well, we have to do several steps. Much more complicated steps than for constructible numbers, but still a certain number of steps. (I could look at the numbers that can be defined in a certain number of words, to get an idea, even if this definition is problematic).

So I would guess (and this is wishful thinking) that the principle stay the same. Of all the numbers that can be defined in a certain number of steps, most of them should in the zone around 0.5 and 5. So the same should be true for interesting numbers ...

Thank you for your attention. I hope this was not too confusing.

EDIT : I changed the phrasing of the sentence defining constructible numbers.",null,42,cdolc72,1rlfdu,askscience,top_week,195
ReyJavikVI,"First, I wanted to link [this math.stackexchange post](http://math.stackexchange.com/questions/120780/why-are-all-the-interesting-constants-so-small), which discusses the topic.

There's a constant that no one has mentioned: the order of the monster group, which is 808017424794512875886459904961710757005754368000000000 (seriously). I don't know much of the math behind it, but as far as I know the monster group is the largest of the sporadic finite simple groups, and this is the number of elements in it. Surely not close to zero!",null,18,cdog7cz,1rlfdu,askscience,top_week,57
penorio,"A lot of times you are working with ratios that are not the actual constant, but a multiple of the constant. For example the ratio between radius and circumference is 2*pi, not pi. So any other multiple would work the same, but I guess it's more useful to choose a small one.

For why there are bigger gaps for primes as numbers grow the [Sieve of Eratosthenes](http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) is a really nice way to generate prime numbers and visualize it. There's an animation of the method in that wikipedia link.",null,14,cdohd6r,1rlfdu,askscience,top_week,46
null,null,null,12,cdops2y,1rlfdu,askscience,top_week,47
SpaceEnthusiast,"This is probably going to get buried deeply but I need to give an example as why some constants tend to be small. Take the Gamma function Γ(x) for example. It is defined recursively as Γ(x+1) = xΓ(x) so for example take

Γ(3/2) = (1/2)Γ(1/2) = pi^(1/2)/2

Or take Γ(5/2) = (3/2)Γ(3/2) = (3/2)(1/2)Γ(1/2)

In a similar manner

Γ(m/n) = qΓ(1/n)

where q is a rational number obtained by the process above. In essence, when it comes to the Gamma function, you only need to know the values of the function for x between 0 and 1 to know all other real values. When it comes to the Gamma function, the smallness of the constants comes from the recursive nature of the Gamma function. 

I would argue that a lot of small constants come from an anthropometric perspective - the objects we usually consider are somewhat idealized, small, less complicated things. There is only so far you can go with a limited application of the basic operations. 

I would also argue that if you are given a larger constant, more often then not you'll be able to break it down into simpler/smaller constants.",null,9,cdoky4q,1rlfdu,askscience,top_week,38
null,null,null,7,cdopotb,1rlfdu,askscience,top_week,29
NemoKozeba,"I think I know the answer to your question. Or answers, because there are two.

First, whether you realize it or not, all of your examples are ratios. Even pi, in a way. It's a question of scale and balance. Nature tends toward balance. Larger ratio's tend to be unstable. For example imagine a rectangle crystal which grew to a ratio of 22:7062. Which would be a constant 321. It would take a very odd molecule to form such a crystal. And should the molecule exist, nature would have to provide space an materials 321 times longer than wide. That's just not how nature tends to work. 1:1 is most common. Any shift from that becomes less and less likely. 

For the second reason you should read about [Benford's law](http://en.wikipedia.org/wiki/Benford%27s_law). Which basically states that lower numbers are more common than higher numbers. And goes even farther to show that even in larger numbers, lower digits are many times more common. There are many theories as to why. My belief is simple. One comes before two. Two is twice as hard to reach as one so one is twice as common. Two is twice as common as four. Etc... 

",null,17,cdom8rd,1rlfdu,askscience,top_week,37
disconcision,"'the set of important mathematical constants' does not appear to be well-defined. all of the lists provided seem to amount to lists of numbers which particular humans find particularly interesting. is there even a single number on these lists whose membership is uncontroversial? why pi as opposed to tau? if both, why not all rational multiples of pi, eliminating the perceived bias? if you ask a random person to list numbers they find interesting i'd guess that the list would be similarly biased.",null,4,cdoglzb,1rlfdu,askscience,top_week,21
null,null,null,13,cdon75p,1rlfdu,askscience,top_week,23
antonfire,"One somewhat reasonable answer (or at least a relevant comment) is that a lot of these constants have have some interpretations as probabilities, and they are probabilities of events that are not all that unlikely. For example, 1/e is roughly the probability that a random permutation of the set {1, ..., n} will not fix an element. 6/pi^(2) is roughly the probability that two random numbers in the set {1, ..., n} are relatively prime. 1/phi is the probability that the sum of a random number in the set [0,1] and its square is less than 1. I suspect you can relate Feigenbaum's constants to some event relating to periods of a dynamical system as well.",null,4,cdolwdp,1rlfdu,askscience,top_week,13
monstertofu,"If we look at pi, well that's the ratio of the circumference of a circle to its diameter.... in Euclidean space.  It's certainly not the case in a highly negatively curved geometry.  In fact, in a very weird negatively curved geometry, the circumference-diameter ratio could be absurdly large.  Now why don't we live in a world where we see that to be the case?

Now we're running into something that seems like the anthropic principle.  Namely, if the ratio were absurdly large, how would the ancients have discovered this relationship?  Remember even counting up to a large number requires a lot of mathematical sophistication.  One could argue that if the ratio were not around 3 (as with pi) then the relationship would either not have been discovered or it would not have been considered useful enough to merit noting.  

One could argue the same way with e.  So e arose as the limiting value  as one continuously compounds interest on $1 (at a 100% annual rate).  If compound interest behaved in such a weird way that the $1 would become a gazillion dollars under continuous compounding, then it's unlikely continuous compounding would have become a focus of interest to us.  ",null,0,cdoo5xa,1rlfdu,askscience,top_week,8
ENORD,"These numbers are close to 1 because they are *defined* as ratios of one (in most cases). A ratio is not a number but an equivalence class over pairs of numbers that satisfy the constraints in question (i.e. 1 to pi / 2 to tau are the ratio of the circumference of a circle to its radius/diameter).

A number close to one is chosen because we are fond of multiplying and any ratio represented as some ratio of one (1) multiplied with any number will yield a number that satisfies the constraints of the equivalence class of the ratio in question in a way that is often easy to observe.

Tl;Dr: Numbes are arbitrary.",null,5,cdooypi,1rlfdu,askscience,top_week,12
nihilaeternumest,"There's a lot of good responses here and this will undoubtedly remain buried forever, but I feel like adding my two cents.

It makes sense that constants would be close to zero or one because those numbers are unique. Zero and one are non-random* numbers. They are the additive and multiplicative identities, respectively. Constants are non-random by definition, so it intuitively makes sense for them to be close to other non-random numbers (ie. 0 and 1).

Obviously, this idea does not hold up to mathematical or scientific scrutiny, but it doesn't need to. ""Close"" is not a mathematical term, so it makes sense for the answer to be non-mathematical. We're talking about how these numbers ""feel"" to us, after all.

\*Because this term is difficult to actually define, I'm cheating a little bit and defining a random number as any number that is a reasonable\** multiple of a mathematical constant.

** Yes, I realize I am defining non-randomness using the concept of non-randomness. This is why this is hard to define.

edit: making \* not make things italic",null,3,cdovupd,1rlfdu,askscience,top_week,8
adequate_potato,"iCookBaconShirtless' answer is spot-on in terms of us not being able to say that a number is either ""close to"" or ""far from"" zero.

That said, I think your question comes more from these being numbers in the range of 10^0, which are numbers similar in scale to those we deal with on a day-to-day basis. 

The reason so many constants are like this is because they describe relationships between things we deal with - the diameter and circumference of the same circle, for instance. Most of these numbers are small as a result of the components we use to describe them being similar in scale because that's how we've defined them to be.

In fact, the ones not like this tend to be very large or small in magnitude compared to constants on the scale of 10^0. Consider the gravitational constant (10^-11), the speed of light (10^8), Boltzmann Constant (10^-23), and many, many others, all of which relate things of very small scale with things of very large scale. ",null,0,cdok579,1rlfdu,askscience,top_week,6
null,null,null,1,cdotvlr,1rlfdu,askscience,top_week,7
fleetingshadow,"I think you're spot on with your hunch actually - 0 and 1 are the most important numbers.  
 
Sqrt(2)? Comes from the hypotenuse of a triangle with short sides both 1 unit in length.  
  
Pi comes from the use of the unit circle as the way that pi is defined - its recipe already has parameters normalized to 1 and 0. (origin 0, radius 1).  
  
In fact, you can view 2*pi as the number of different triangles you can create with a hypotenuse of 1 in two dimensions.  
  
e? Similarly defined. 1+(1/n!)^n  
    
Out of my depth on the Feigenbaum constants...  
  
As for other (typically physical constants), we'vegone through a process over the centuries of using the normalized versions as much a possible - you set what you think the most fundamental constant is to 1, and scale the related ones. Why? It makes the math easier.  
  
So while it doesn't hold for all constants, there is a relationship there - even if we've deliberately created one - and it's all because of normalization.
",null,1,cdojh84,1rlfdu,askscience,top_week,5
Parametrize,"To add to other people's answers:

How do you define 'close'? We are using the standard metric - the norm on R. But I'll ask you a few questions: Why is it fair to say that 0.01 is just as close from 5 as 9.99 is? Sure, they are the same 'distance' apart... but 0.01 and 5 are orders of magnitude apart while 5 and 9.99 are on the same order of magnitude... (This is why we use the geometric as well as the arithmetic mean). 

On another note:
Some of the constants on the wikipedia page are chosen arbitrarily. Why are we choosing a certain constant over a different constant? And you can find plenty of functions which will give you arbitrarily large constants. (Look up Ramsey theory for a few of them, if you want).",null,2,cdoqdyn,1rlfdu,askscience,top_week,6
bigbad486,"I may not really be qualified to answer this, but it seems like the people who are more so are being purposely obtuse about the word ""close"".  I think a simple answer might be that many of the important constants are the most basic possible part of a pattern, a common piece of a certain type of equation.  That is to say, they are as small as it is possible to make them, because it makes more sense to multiply them to reach larger values as needed than to divide a larger number for smaller ones.  Like the OP I'm sure that I'm not phrasing this in an ideal way.",null,0,cdokvz9,1rlfdu,askscience,top_week,3
long-shots,"I think it is because the differentiation among lower numbers (those ""closer to zero"") seems more significant.  Like the distance from one to two is double in quantity, but the difference between one billion vs. One billion and one is almost negligible.  So, finding constants around zero might not be surprising because numbers closer to zero offer greater ""number power"", and by that I mean quantitative significance.   From the standpoint of a philosopher who doesn't do much math, I can't see a better explanation; be warned I am not nearly an expert theoretician in the art of numbryng and, so, these words may be explosive. ",null,1,cdoqbo7,1rlfdu,askscience,top_week,4
Onechrisn,"It seems like a lot of people are shooting around, but not actually hitting upon, the idea of [Bedford's Law](http://en.wikipedia.org/wiki/Benford's_law). 
Bedford's Law: 
&gt; refers to the frequency distribution of digits in many (but not all) real-life sources of data. In this distribution, the number 1 occurs as the leading digit about 30% of the time, while larger numbers occur in that position less frequently.

Basically, one would expect constants and a lot of other numbers to cluster around 1 and drop off logarithmically from there. In fact, studying logarithms is [how Bedford's Law was discovered.](http://www.rexswain.com/benford.html)",null,2,cdoqo9n,1rlfdu,askscience,top_week,4
NicknameAvailable,"Disregarding the point of trying to define ""close to zero"" on an infinite scale and taking it for what you seemed to intend:

Because the vast majority of people aren't smart enough to come up with relations that would equate to something larger and those that are have no need to do so as of yet.  Something like a circle's circumferene divided by it's diameter, though useful, is completely arbitrary.  We could have used the radius and it would have been twice the size, we could have used 1/nth the radius and it would be that much larger.  We could do the same sort of thing for multidimensional geometric shapes and the number again would be larger - we simply don't have the need to do so.  You could take the planck length vs the diameter of the universe if you wanted a really big number.",null,2,cdorjy3,1rlfdu,askscience,top_week,3
professor__doom,"It can be proven, stemming from axiomatic definitions not related to numeric value, that e^pi*i = -1 .  This is called Euler's Identity.

Instinct leads me to believe that in order for this equality to hold to within a given tolerance, the constants e and pi must have values within a certain neighborhood--either relative to zero, or they must at least be reasonably close to each other.  I have not verified this, but I would be willing to bet that this can be shown via numeric methods.

Somebody better versed than I am in numeric methods, please jump in and try it, I'd love to see your findings.

The ""golden ratio"" phi also has an absolute numerical definition in terms of another irrational number: phi=1+sqrt(5)/2.  Using the extremely intuitive inequality 1&lt;sqrt(x)&lt;x for x&gt;1, this definition gives us an obvious set of possible bounds for phi: 1&lt;phi&lt;3

Now there ARE some dimensionless constants with ""real world"" applications that are enormous numbers.  For example, the Eddington Number is the number of protons in the universe--just a number, not expressed in terms of meters or miles or anything like that--and is approximately 137*2^256 .",null,2,cdovit7,1rlfdu,askscience,top_week,4
ShadowKing94,"A lot of them are ratios (golden ratio, pi).
Ratios tend to be ""smaller"" numbers because they describe the ratio between two things that are related to each other and thus wont be hugely different.

Fe the radius and circumference of a circle have pi as a ratio and not a number in the thousands because they cant differ that much. 

May not be the most scientific answer, but I think its a very valid one. ",null,0,cdowej3,1rlfdu,askscience,top_week,2
turtle889,"Most people on this thread refer to an arbitrary definition of close as an explanation for OP's question, but I find this unsatisfying. From a mathematical perspective, while the real numbers are uncountable, and so 'closeness' *is* somewhat arbitrary, it doesn't address the fact that 1.618, for example, is a value that explains how much longer one line is than another. These numbers express a relative scaling between two things, and so rescaling these values to a different number system would only change our impression of them, not the absolute ratio. 

For the constants that you mentioned, I suspect their small values result from our **tendency to compare things on the same order of magnitude**. It would be ridiculous, for example, for us to compare area of a circle to a millionth of its diameter. This would in fact change our 'fundamental' value for pi, but it would make no sense. So we choose to compare things that we can relate to each other visually or conceptually - a circle's area to the line that bisects it, for example.",null,0,cdoxczg,1rlfdu,askscience,top_week,2
SigaVa,"I'll give a perspective from physics:

Dimensionless constants are ratios.  To observe a ratio you have to measure, in some sense, the relative effect of two ""similar"" things; similar meaning having the same units.

Due to our historic inability to measure things of hugely varying magnitudes we tend to measure stuff of around the same size ... stuff on the size scale we can directly observe.  When you then take a ratio of two such measurements, you're going to get a number around 1.

If you look at constants with dimension, there's a huge variation in size.  In our normal units, the speed of light is ~10^8, the charge of a proton is ~10^-19, the mass of an electron is ~10^-30, Planck's constant is ~10^-33",null,0,cdonrj5,1rlfdu,askscience,top_week,2
kenhill,"I think the question would be better phrased a question of maximizing a function. Given a set I of ""important mathematical constants"" in C and a ball B in C centered at point P of radius r define the density of important constants as the number of elements of I that are also in B divided by the volume of B.  For a given set of constants I what point P and radius r yields the greatest density.  We could also ask, for each element i of I what is the largest ball centered on i that contains no other elements I.  We could also ask, what is the smallest ball that contains all elements of I or any non-empty subset of I.  Lots more questions.  Keep in mind there are a lot of undefined issues still in my questions (what distance metric do we use to define a ball, what function do we use to define volume, etc.)
",null,0,cdonug8,1rlfdu,askscience,top_week,1
CuriousMetaphor,"The ""number line"" is a way to graph the real numbers while highlighting their properties as a field with the operation of addition.  It goes from negative infinity to positive infinity.  0 is the additive identity, so it is in the middle.  Each number and its additive inverse are equal distances from 0.

But there are other ways to do it.  You could graph the positive real numbers while emphasizing their properties as a field with the operation of multiplication.  This alternate number line would go from 0 to infinity.  The number 1, being the multiplicative identity, would be in the middle, and each number and its multiplicative inverse would be equal distances from 1.

Neither of these number lines is more ""correct"" than the other.  They are both graphical representations of an abstract concept.

If graphed on the second number line, all the constants you mention are on the right side so are ""closer"" to infinity than they are to 0.

Usually, there is some latitude when choosing a mathematical constant.  The constants you mentioned could all be replaced by their multiplicative inverses without becoming less ""fundamental"".  For example, 3.142 is the ratio between a circle's circumference and its diameter, but 0.3183 is the ratio between a circle's diameter and its circumference.  The reason we choose numbers greater than 1 rather than numbers less than 1 as constants probably has to do with the fact that most of the numbers we are most familiar with, the natural numbers, are greater than 1.  Also, positive numbers are easier to work with than negative numbers.

Another point: ""close"" doesn't mean anything unless you are comparing it to something else.  Number lines are one-dimensional objects and so they look exactly the same when zoomed in or out.  If you draw an (additive) number line with tick marks every 10 units, then yes those constants seem close to 0.  But if you draw the same number line with tick marks every 0.01 units, those constants suddenly seem very far from 0.  

There are equal numbers of real numbers in any two segments of the number line.  You can draw a one-to-one correspondence between them.  For example, there are exactly the same number of real numbers between 0.2 and 0.3 as there are between 1 and infinity.",null,1,cdoosdu,1rlfdu,askscience,top_week,2
iCookBaconShirtless,"The problem with addressing this question is that there is no natural and intrinsic way to decide whether a non-zero number is ""close to zero"" or ""far from zero.""  The constants you listed are closer to zero than 10^10 but further from zero than 10^-10 .  Who is to say where we should place to cutoff between numbers that are ""close to"" or ""far from"" from zero?

They may strike you as being close to zero, but this is a question of psychology not mathematics.  Perhaps we deal with numbers that are further from zero than pi more often than numbers that are closer to zero than pi, so pi seems close to zero?

I'll add that the precise values of those constants might not be as fundamental as you think.  The ratio of the surface area to the diameter of a sphere in n dimensions, for example, is only pi when n=2.  However, it can be written in terms of pi for other dimensions.  In fact, many people argue that tau=2*pi should be thought of as more fundamental than pi.  I personally believe that any rational multiple of pi is equally ""fundamental"", whatever that means.  We just settled on one of them out of convenience and for historical reasons.

As for your second question, the reason that small primes are denser than large primes is fairly well understood.  See [the prime number theorem](http://en.wikipedia.org/wiki/Prime_number_theorem).

Edit: Think about it this way. OP is intrigued that so many constants are smaller than 5 in magnitude. But any finite list of numbers has an upper bound.  The list given by OP happens to have 5 as an upper bound.  So what is so surprising about 5 versus some other upper bound? It's tempting to answer that a number smaller than 5 is unlikely to occur if you uniformly draw a random positive number. But there is actually no mathematical way to make sense of drawing a real number uniformly. The lack of scale is the real culprit. You cannot claim that a number is intrinsically ""large"" or ""small"" in any meaningful way without choosing something to compare that number to.


**EDIT 2: To anyone who still thinks that the list of constants given by OP (or any other finite list of numbers) is intrinsically small in magnitude, please provide a list of number that you think is not small in magnitude.**

I am spending a lot of time responding to commenters that might be glossing over some subtleties of my argument.  If you wish to debate my comment, please first answer the question I posed in EDIT 2.  I think the doomed effort to answer this question will reveal some of the subtleties of the point I am making.

EDIT 3: I am in no way trying to discredit OP's question.  I agree with OP that the fact that so many named constants are less than 5 is surprising in a psychological sense.  But I contend that there is no way to answer the question of whether it is surprising mathematically.  The reasons for this are actually a bit more subtle than they first appear.  The problem is that *every* finite list of numbers has an upper bound.  So how surprising is it that there are upper bounds on OP's list that are less than 5?  Answering this would require defining some sense of a probability distribution on the positive real numbers.  But every probability distribution on the positive real numbers artificially imposes a scale because there is no uniform distribution on the positive reals.  So **you can only really ask whether a list of numbers is surprisingly close to zero with respect to some arbitrarily chosen scale.** 

",null,508,cdooolp,1rlfdu,askscience,top_week,1622
Vietoris,"I will try to give a different point of view on this question. It will be more geometrical and not exactly answering the question but that might still give some insight.

I will talk about [Constructible numbers](http://en.wikipedia.org/wiki/Constructible_number). The principle is the following. You start with the two points at distance 1 on a plane, and you have a compass and a straightedge. At each step of the process you will get new points as follows :

Imagine you are at step n and you have already constructed N points. You draw all possible lines passing through two of your N points.  You also draw all the circles centered at one of your N points with a radius equal to the distance between two of your N points. You mark all the intersections created between lines, circles or both and add all these points to your list of N points. 

For convenience I will see the points in the plane as complex numbers. A number is constructible if it can be obtained by this process. 

Let's give the first examples : 

* From the first two points 0 and 1, I can draw the line passing though these two points, the two circles of radius 1 centered on 0 and 1 respectively. This gives me 4 new points : -1, 2 , e^ipi/6 , e^-ipi/6

* From this new set of six points, I can draw 10 different lines. And I have four possible radius for my circles (1,2,3,sqrt(3)) and 6 possible centers which gives me more or less 24 new circles. So I get more than 100 new points (see the [picture](http://imgur.com/bE3dlfG) )

* And so on ... the number of points explodes very quickly.

Ok what was the point of all this ? Well it is interesting to note that all the (non-zero) numbers we can construct in 2 steps all have norm between 0.2 and 5. And more importantly, most (like 95% may be, I didn't compute it) have norm between 0.5 and 4. (see the [picture](http://imgur.com/bE3dlfG) )

When I go on with the steps I see that there is always a lower bound and an upper bound (obvious), and that most of the numbers have norm between 0.5 and 4. (less obvious but it's because, when you draw all the circles around your existing points, most of them will stay in the same range ...).

Now that we understand that, what next ? Well, suppose we could see the property of being ""interesting"" as a random occurence among numbers (this is my hypothesis, I am not exactly proving that this means anything). And to construct interesting numbers we have to do a certain number of steps. Equivalently, I construct all possible points in n steps and pick randomly some of them  (to say that they are interesting). With the arguments above, this would prove that :

* a majority (I have no idea how to exactly compute it) of the interesting numbers constructible in n steps lie in a rather restricted range between 0.5 and 5. 

* a few will be larger or smaller than that ... 

* And no interesting number constructible in n steps will have norm smaller than 10^-n or be bigger than 10^n (because no  number constructible in n steps can be that big)

&gt; Tl; DR : If I randomly pick numbers that could be defined in a certain number of steps, then most of the numbers I picked will be in the [0.5 , 5] range

Now, back to mathematical constant. How are they defined ? Well, we have to do several steps. Much more complicated steps than for constructible numbers, but still a certain number of steps. (I could look at the numbers that can be defined in a certain number of words, to get an idea, even if this definition is problematic).

So I would guess (and this is wishful thinking) that the principle stay the same. Of all the numbers that can be defined in a certain number of steps, most of them should in the zone around 0.5 and 5. So the same should be true for interesting numbers ...

Thank you for your attention. I hope this was not too confusing.

EDIT : I changed the phrasing of the sentence defining constructible numbers.",null,42,cdolc72,1rlfdu,askscience,top_week,195
ReyJavikVI,"First, I wanted to link [this math.stackexchange post](http://math.stackexchange.com/questions/120780/why-are-all-the-interesting-constants-so-small), which discusses the topic.

There's a constant that no one has mentioned: the order of the monster group, which is 808017424794512875886459904961710757005754368000000000 (seriously). I don't know much of the math behind it, but as far as I know the monster group is the largest of the sporadic finite simple groups, and this is the number of elements in it. Surely not close to zero!",null,18,cdog7cz,1rlfdu,askscience,top_week,57
penorio,"A lot of times you are working with ratios that are not the actual constant, but a multiple of the constant. For example the ratio between radius and circumference is 2*pi, not pi. So any other multiple would work the same, but I guess it's more useful to choose a small one.

For why there are bigger gaps for primes as numbers grow the [Sieve of Eratosthenes](http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) is a really nice way to generate prime numbers and visualize it. There's an animation of the method in that wikipedia link.",null,14,cdohd6r,1rlfdu,askscience,top_week,46
null,null,null,12,cdops2y,1rlfdu,askscience,top_week,47
SpaceEnthusiast,"This is probably going to get buried deeply but I need to give an example as why some constants tend to be small. Take the Gamma function Γ(x) for example. It is defined recursively as Γ(x+1) = xΓ(x) so for example take

Γ(3/2) = (1/2)Γ(1/2) = pi^(1/2)/2

Or take Γ(5/2) = (3/2)Γ(3/2) = (3/2)(1/2)Γ(1/2)

In a similar manner

Γ(m/n) = qΓ(1/n)

where q is a rational number obtained by the process above. In essence, when it comes to the Gamma function, you only need to know the values of the function for x between 0 and 1 to know all other real values. When it comes to the Gamma function, the smallness of the constants comes from the recursive nature of the Gamma function. 

I would argue that a lot of small constants come from an anthropometric perspective - the objects we usually consider are somewhat idealized, small, less complicated things. There is only so far you can go with a limited application of the basic operations. 

I would also argue that if you are given a larger constant, more often then not you'll be able to break it down into simpler/smaller constants.",null,9,cdoky4q,1rlfdu,askscience,top_week,38
null,null,null,7,cdopotb,1rlfdu,askscience,top_week,29
NemoKozeba,"I think I know the answer to your question. Or answers, because there are two.

First, whether you realize it or not, all of your examples are ratios. Even pi, in a way. It's a question of scale and balance. Nature tends toward balance. Larger ratio's tend to be unstable. For example imagine a rectangle crystal which grew to a ratio of 22:7062. Which would be a constant 321. It would take a very odd molecule to form such a crystal. And should the molecule exist, nature would have to provide space an materials 321 times longer than wide. That's just not how nature tends to work. 1:1 is most common. Any shift from that becomes less and less likely. 

For the second reason you should read about [Benford's law](http://en.wikipedia.org/wiki/Benford%27s_law). Which basically states that lower numbers are more common than higher numbers. And goes even farther to show that even in larger numbers, lower digits are many times more common. There are many theories as to why. My belief is simple. One comes before two. Two is twice as hard to reach as one so one is twice as common. Two is twice as common as four. Etc... 

",null,17,cdom8rd,1rlfdu,askscience,top_week,37
disconcision,"'the set of important mathematical constants' does not appear to be well-defined. all of the lists provided seem to amount to lists of numbers which particular humans find particularly interesting. is there even a single number on these lists whose membership is uncontroversial? why pi as opposed to tau? if both, why not all rational multiples of pi, eliminating the perceived bias? if you ask a random person to list numbers they find interesting i'd guess that the list would be similarly biased.",null,4,cdoglzb,1rlfdu,askscience,top_week,21
null,null,null,13,cdon75p,1rlfdu,askscience,top_week,23
antonfire,"One somewhat reasonable answer (or at least a relevant comment) is that a lot of these constants have have some interpretations as probabilities, and they are probabilities of events that are not all that unlikely. For example, 1/e is roughly the probability that a random permutation of the set {1, ..., n} will not fix an element. 6/pi^(2) is roughly the probability that two random numbers in the set {1, ..., n} are relatively prime. 1/phi is the probability that the sum of a random number in the set [0,1] and its square is less than 1. I suspect you can relate Feigenbaum's constants to some event relating to periods of a dynamical system as well.",null,4,cdolwdp,1rlfdu,askscience,top_week,13
monstertofu,"If we look at pi, well that's the ratio of the circumference of a circle to its diameter.... in Euclidean space.  It's certainly not the case in a highly negatively curved geometry.  In fact, in a very weird negatively curved geometry, the circumference-diameter ratio could be absurdly large.  Now why don't we live in a world where we see that to be the case?

Now we're running into something that seems like the anthropic principle.  Namely, if the ratio were absurdly large, how would the ancients have discovered this relationship?  Remember even counting up to a large number requires a lot of mathematical sophistication.  One could argue that if the ratio were not around 3 (as with pi) then the relationship would either not have been discovered or it would not have been considered useful enough to merit noting.  

One could argue the same way with e.  So e arose as the limiting value  as one continuously compounds interest on $1 (at a 100% annual rate).  If compound interest behaved in such a weird way that the $1 would become a gazillion dollars under continuous compounding, then it's unlikely continuous compounding would have become a focus of interest to us.  ",null,0,cdoo5xa,1rlfdu,askscience,top_week,8
ENORD,"These numbers are close to 1 because they are *defined* as ratios of one (in most cases). A ratio is not a number but an equivalence class over pairs of numbers that satisfy the constraints in question (i.e. 1 to pi / 2 to tau are the ratio of the circumference of a circle to its radius/diameter).

A number close to one is chosen because we are fond of multiplying and any ratio represented as some ratio of one (1) multiplied with any number will yield a number that satisfies the constraints of the equivalence class of the ratio in question in a way that is often easy to observe.

Tl;Dr: Numbes are arbitrary.",null,5,cdooypi,1rlfdu,askscience,top_week,12
nihilaeternumest,"There's a lot of good responses here and this will undoubtedly remain buried forever, but I feel like adding my two cents.

It makes sense that constants would be close to zero or one because those numbers are unique. Zero and one are non-random* numbers. They are the additive and multiplicative identities, respectively. Constants are non-random by definition, so it intuitively makes sense for them to be close to other non-random numbers (ie. 0 and 1).

Obviously, this idea does not hold up to mathematical or scientific scrutiny, but it doesn't need to. ""Close"" is not a mathematical term, so it makes sense for the answer to be non-mathematical. We're talking about how these numbers ""feel"" to us, after all.

\*Because this term is difficult to actually define, I'm cheating a little bit and defining a random number as any number that is a reasonable\** multiple of a mathematical constant.

** Yes, I realize I am defining non-randomness using the concept of non-randomness. This is why this is hard to define.

edit: making \* not make things italic",null,3,cdovupd,1rlfdu,askscience,top_week,8
adequate_potato,"iCookBaconShirtless' answer is spot-on in terms of us not being able to say that a number is either ""close to"" or ""far from"" zero.

That said, I think your question comes more from these being numbers in the range of 10^0, which are numbers similar in scale to those we deal with on a day-to-day basis. 

The reason so many constants are like this is because they describe relationships between things we deal with - the diameter and circumference of the same circle, for instance. Most of these numbers are small as a result of the components we use to describe them being similar in scale because that's how we've defined them to be.

In fact, the ones not like this tend to be very large or small in magnitude compared to constants on the scale of 10^0. Consider the gravitational constant (10^-11), the speed of light (10^8), Boltzmann Constant (10^-23), and many, many others, all of which relate things of very small scale with things of very large scale. ",null,0,cdok579,1rlfdu,askscience,top_week,6
null,null,null,1,cdotvlr,1rlfdu,askscience,top_week,7
fleetingshadow,"I think you're spot on with your hunch actually - 0 and 1 are the most important numbers.  
 
Sqrt(2)? Comes from the hypotenuse of a triangle with short sides both 1 unit in length.  
  
Pi comes from the use of the unit circle as the way that pi is defined - its recipe already has parameters normalized to 1 and 0. (origin 0, radius 1).  
  
In fact, you can view 2*pi as the number of different triangles you can create with a hypotenuse of 1 in two dimensions.  
  
e? Similarly defined. 1+(1/n!)^n  
    
Out of my depth on the Feigenbaum constants...  
  
As for other (typically physical constants), we'vegone through a process over the centuries of using the normalized versions as much a possible - you set what you think the most fundamental constant is to 1, and scale the related ones. Why? It makes the math easier.  
  
So while it doesn't hold for all constants, there is a relationship there - even if we've deliberately created one - and it's all because of normalization.
",null,1,cdojh84,1rlfdu,askscience,top_week,5
Parametrize,"To add to other people's answers:

How do you define 'close'? We are using the standard metric - the norm on R. But I'll ask you a few questions: Why is it fair to say that 0.01 is just as close from 5 as 9.99 is? Sure, they are the same 'distance' apart... but 0.01 and 5 are orders of magnitude apart while 5 and 9.99 are on the same order of magnitude... (This is why we use the geometric as well as the arithmetic mean). 

On another note:
Some of the constants on the wikipedia page are chosen arbitrarily. Why are we choosing a certain constant over a different constant? And you can find plenty of functions which will give you arbitrarily large constants. (Look up Ramsey theory for a few of them, if you want).",null,2,cdoqdyn,1rlfdu,askscience,top_week,6
bigbad486,"I may not really be qualified to answer this, but it seems like the people who are more so are being purposely obtuse about the word ""close"".  I think a simple answer might be that many of the important constants are the most basic possible part of a pattern, a common piece of a certain type of equation.  That is to say, they are as small as it is possible to make them, because it makes more sense to multiply them to reach larger values as needed than to divide a larger number for smaller ones.  Like the OP I'm sure that I'm not phrasing this in an ideal way.",null,0,cdokvz9,1rlfdu,askscience,top_week,3
long-shots,"I think it is because the differentiation among lower numbers (those ""closer to zero"") seems more significant.  Like the distance from one to two is double in quantity, but the difference between one billion vs. One billion and one is almost negligible.  So, finding constants around zero might not be surprising because numbers closer to zero offer greater ""number power"", and by that I mean quantitative significance.   From the standpoint of a philosopher who doesn't do much math, I can't see a better explanation; be warned I am not nearly an expert theoretician in the art of numbryng and, so, these words may be explosive. ",null,1,cdoqbo7,1rlfdu,askscience,top_week,4
Onechrisn,"It seems like a lot of people are shooting around, but not actually hitting upon, the idea of [Bedford's Law](http://en.wikipedia.org/wiki/Benford's_law). 
Bedford's Law: 
&gt; refers to the frequency distribution of digits in many (but not all) real-life sources of data. In this distribution, the number 1 occurs as the leading digit about 30% of the time, while larger numbers occur in that position less frequently.

Basically, one would expect constants and a lot of other numbers to cluster around 1 and drop off logarithmically from there. In fact, studying logarithms is [how Bedford's Law was discovered.](http://www.rexswain.com/benford.html)",null,2,cdoqo9n,1rlfdu,askscience,top_week,4
NicknameAvailable,"Disregarding the point of trying to define ""close to zero"" on an infinite scale and taking it for what you seemed to intend:

Because the vast majority of people aren't smart enough to come up with relations that would equate to something larger and those that are have no need to do so as of yet.  Something like a circle's circumferene divided by it's diameter, though useful, is completely arbitrary.  We could have used the radius and it would have been twice the size, we could have used 1/nth the radius and it would be that much larger.  We could do the same sort of thing for multidimensional geometric shapes and the number again would be larger - we simply don't have the need to do so.  You could take the planck length vs the diameter of the universe if you wanted a really big number.",null,2,cdorjy3,1rlfdu,askscience,top_week,3
professor__doom,"It can be proven, stemming from axiomatic definitions not related to numeric value, that e^pi*i = -1 .  This is called Euler's Identity.

Instinct leads me to believe that in order for this equality to hold to within a given tolerance, the constants e and pi must have values within a certain neighborhood--either relative to zero, or they must at least be reasonably close to each other.  I have not verified this, but I would be willing to bet that this can be shown via numeric methods.

Somebody better versed than I am in numeric methods, please jump in and try it, I'd love to see your findings.

The ""golden ratio"" phi also has an absolute numerical definition in terms of another irrational number: phi=1+sqrt(5)/2.  Using the extremely intuitive inequality 1&lt;sqrt(x)&lt;x for x&gt;1, this definition gives us an obvious set of possible bounds for phi: 1&lt;phi&lt;3

Now there ARE some dimensionless constants with ""real world"" applications that are enormous numbers.  For example, the Eddington Number is the number of protons in the universe--just a number, not expressed in terms of meters or miles or anything like that--and is approximately 137*2^256 .",null,2,cdovit7,1rlfdu,askscience,top_week,4
ShadowKing94,"A lot of them are ratios (golden ratio, pi).
Ratios tend to be ""smaller"" numbers because they describe the ratio between two things that are related to each other and thus wont be hugely different.

Fe the radius and circumference of a circle have pi as a ratio and not a number in the thousands because they cant differ that much. 

May not be the most scientific answer, but I think its a very valid one. ",null,0,cdowej3,1rlfdu,askscience,top_week,2
turtle889,"Most people on this thread refer to an arbitrary definition of close as an explanation for OP's question, but I find this unsatisfying. From a mathematical perspective, while the real numbers are uncountable, and so 'closeness' *is* somewhat arbitrary, it doesn't address the fact that 1.618, for example, is a value that explains how much longer one line is than another. These numbers express a relative scaling between two things, and so rescaling these values to a different number system would only change our impression of them, not the absolute ratio. 

For the constants that you mentioned, I suspect their small values result from our **tendency to compare things on the same order of magnitude**. It would be ridiculous, for example, for us to compare area of a circle to a millionth of its diameter. This would in fact change our 'fundamental' value for pi, but it would make no sense. So we choose to compare things that we can relate to each other visually or conceptually - a circle's area to the line that bisects it, for example.",null,0,cdoxczg,1rlfdu,askscience,top_week,2
SigaVa,"I'll give a perspective from physics:

Dimensionless constants are ratios.  To observe a ratio you have to measure, in some sense, the relative effect of two ""similar"" things; similar meaning having the same units.

Due to our historic inability to measure things of hugely varying magnitudes we tend to measure stuff of around the same size ... stuff on the size scale we can directly observe.  When you then take a ratio of two such measurements, you're going to get a number around 1.

If you look at constants with dimension, there's a huge variation in size.  In our normal units, the speed of light is ~10^8, the charge of a proton is ~10^-19, the mass of an electron is ~10^-30, Planck's constant is ~10^-33",null,0,cdonrj5,1rlfdu,askscience,top_week,2
kenhill,"I think the question would be better phrased a question of maximizing a function. Given a set I of ""important mathematical constants"" in C and a ball B in C centered at point P of radius r define the density of important constants as the number of elements of I that are also in B divided by the volume of B.  For a given set of constants I what point P and radius r yields the greatest density.  We could also ask, for each element i of I what is the largest ball centered on i that contains no other elements I.  We could also ask, what is the smallest ball that contains all elements of I or any non-empty subset of I.  Lots more questions.  Keep in mind there are a lot of undefined issues still in my questions (what distance metric do we use to define a ball, what function do we use to define volume, etc.)
",null,0,cdonug8,1rlfdu,askscience,top_week,1
CuriousMetaphor,"The ""number line"" is a way to graph the real numbers while highlighting their properties as a field with the operation of addition.  It goes from negative infinity to positive infinity.  0 is the additive identity, so it is in the middle.  Each number and its additive inverse are equal distances from 0.

But there are other ways to do it.  You could graph the positive real numbers while emphasizing their properties as a field with the operation of multiplication.  This alternate number line would go from 0 to infinity.  The number 1, being the multiplicative identity, would be in the middle, and each number and its multiplicative inverse would be equal distances from 1.

Neither of these number lines is more ""correct"" than the other.  They are both graphical representations of an abstract concept.

If graphed on the second number line, all the constants you mention are on the right side so are ""closer"" to infinity than they are to 0.

Usually, there is some latitude when choosing a mathematical constant.  The constants you mentioned could all be replaced by their multiplicative inverses without becoming less ""fundamental"".  For example, 3.142 is the ratio between a circle's circumference and its diameter, but 0.3183 is the ratio between a circle's diameter and its circumference.  The reason we choose numbers greater than 1 rather than numbers less than 1 as constants probably has to do with the fact that most of the numbers we are most familiar with, the natural numbers, are greater than 1.  Also, positive numbers are easier to work with than negative numbers.

Another point: ""close"" doesn't mean anything unless you are comparing it to something else.  Number lines are one-dimensional objects and so they look exactly the same when zoomed in or out.  If you draw an (additive) number line with tick marks every 10 units, then yes those constants seem close to 0.  But if you draw the same number line with tick marks every 0.01 units, those constants suddenly seem very far from 0.  

There are equal numbers of real numbers in any two segments of the number line.  You can draw a one-to-one correspondence between them.  For example, there are exactly the same number of real numbers between 0.2 and 0.3 as there are between 1 and infinity.",null,1,cdoosdu,1rlfdu,askscience,top_week,2
Nantosuelta,"Also, female [Eclectus Parrots] (http://en.wikipedia.org/wiki/Eclectus_Parrot) are scarlet and purple, while males are a uniform green. There are many species of birds where females are larger than males (especially among birds of prey, shorebirds, and some flightless birds), but phalaropes and eclectus parrots are the only ones I can think of where the females are *brighter*.",null,0,cdoik0f,1rlg2l,askscience,top_week,5
monster_cookie,Yes. In the genus *Phalaropus* females are bigger and more brightly colored. This is because they are polyandrous (one female mates with multiple males) and males provide most parental care. So females have to look good while for males is better to not be detected by predators. ,null,1,cdogslb,1rlg2l,askscience,top_week,5
99trumpets,"One other example (in addition to the phalaropes and parrots already mentioned) is the belted kingfisher. That's the kingfisher that's commonly seen zipping around suburban lakes in North America, the one with the rattling call. [Females](http://www.shutterpoint.com/photos/T/566377-Female-Belted-Kingfisher_view.jpg) have a red belly band, [males](http://www.birdwatchersdigest.com/site/resizedImages/20070521113133_kingfisher.jpg) do not.
",null,0,cdopb6u,1rlg2l,askscience,top_week,2
iCookBaconShirtless,"Mandelbrot sets are not actually very mathematically significant in the sense that they don't help us solve many (if any) mathematical or physical problems that arise naturally.  [EDIT: Experts in fractal geometry, please correct me if this is wrong.]

They are mostly famous for historical reasons and because they're really cool: they were some of the first examples of sets that are defined by very simple rules, but have very complex properties.  The discovery of such sets (and also functions with similar properties) spawned the study of chaos theory and fractal dimensions, which have since grown to be extremely significant fields of mathematics (especially the former).  Many similarly complicated sets and functions which can be defined by simple rules have been discovered and intensely studied since the discovery of Mandelbrot sets.  Some of these have important applications/consequences in other fields of math and in the sciences.",null,9,cdogzh2,1rlhst,askscience,top_week,37
KerSan,"I always find this kind of question interesting. It is a question of the form ""X mathematical concept is cool and all, but why does anyone care?"" I find this interesting because no one ever asks this about art, or television, or even astronomy. Everyone seems to think that mathematics is supposed to have some purpose outside of pure enjoyment, even though they do not apply those same rules to all sorts of other human endeavours. I have yet to come up with a good explanation for why this is the case.

Nevertheless, it just so happens that the Mandelbrot set is extremely significant in a way that no one in this discussion seems to have realized. As /u/AsterJ [points out](http://www.reddit.com/r/askscience/comments/1rlhst/why_are_mandelbrot_sets_so_significant_what_can/cdop10f), the Mandelbrot set is the collection of points in the complex plane whose associated Julia set is connected.  This statement needs a bit of explanation before I can discuss its significance.

A Julia set is, to put it almost too simply, the collection of chaotic points of a given rational function of complex numbers. It's best to understand this by example. [This famous image](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) shows the Julia set for the function f(z) = z^3 - 1. The idea is that the non-coloured points bounce around in a chaotic fashion under iteration of this map. The Mandelbrot set is the set of points c such that the Julia set for f(z) = z^2 + c is connected (a technical term that means roughly what you think it means).

I don't know your level of education so I'm not explaining further because I don't want to condescend. This information is on Wikipedia and in books, so you can take the time to understand these concepts a little better if you're interested. You didn't ask for an explanation of the Mandelbrot set, after all, so I'm assuming that you've thought a bit about it. I'm here to answer to the significance of the concept.

[This image I linked before](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) is significant for the validity of the Newton's method for finding the roots of a polynomial equation -- specifically, z^3 = 1. When you seed Newton's method with an initial guess, which root will the method eventually approximate? The answer depends, of course, on where you started. What is surprising, at least at first, is that the root you find might be quite far away from your initial point, even when there are other perfectly reasonable roots much nearer. In other words, this Julia set is telling you about when and how to use Newton's method, and what kind of things you should worry about. The Mandelbrot set is an example of how to classify Julia sets, which in turn helps you extract useful information from a given Julia set.

More generally, the value of so-called chaos theory (for which the Mandelbrot set is rather foundational) is, to me, in analyzing the performance of numerical algorithms. Describing the real-world uses of numerical algorithms is about as difficult as describing the real-world uses of writing.

Edit: some grammar and other minor fixes",null,1,cdoykpe,1rlhst,askscience,top_week,7
EvOllj,"fractals and recursive functions allow a line to have near unlimited length within a limited area, the length only limited only by the possible resolution and possible accuracy to display/measure. They are pretty uniform where they are recursive. There are some geometrical and symmetrical properties interesting for some math and physics.",null,7,cdojbqs,1rlhst,askscience,top_week,5
iCookBaconShirtless,"Mandelbrot sets are not actually very mathematically significant in the sense that they don't help us solve many (if any) mathematical or physical problems that arise naturally.  [EDIT: Experts in fractal geometry, please correct me if this is wrong.]

They are mostly famous for historical reasons and because they're really cool: they were some of the first examples of sets that are defined by very simple rules, but have very complex properties.  The discovery of such sets (and also functions with similar properties) spawned the study of chaos theory and fractal dimensions, which have since grown to be extremely significant fields of mathematics (especially the former).  Many similarly complicated sets and functions which can be defined by simple rules have been discovered and intensely studied since the discovery of Mandelbrot sets.  Some of these have important applications/consequences in other fields of math and in the sciences.",null,9,cdogzh2,1rlhst,askscience,top_week,37
KerSan,"I always find this kind of question interesting. It is a question of the form ""X mathematical concept is cool and all, but why does anyone care?"" I find this interesting because no one ever asks this about art, or television, or even astronomy. Everyone seems to think that mathematics is supposed to have some purpose outside of pure enjoyment, even though they do not apply those same rules to all sorts of other human endeavours. I have yet to come up with a good explanation for why this is the case.

Nevertheless, it just so happens that the Mandelbrot set is extremely significant in a way that no one in this discussion seems to have realized. As /u/AsterJ [points out](http://www.reddit.com/r/askscience/comments/1rlhst/why_are_mandelbrot_sets_so_significant_what_can/cdop10f), the Mandelbrot set is the collection of points in the complex plane whose associated Julia set is connected.  This statement needs a bit of explanation before I can discuss its significance.

A Julia set is, to put it almost too simply, the collection of chaotic points of a given rational function of complex numbers. It's best to understand this by example. [This famous image](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) shows the Julia set for the function f(z) = z^3 - 1. The idea is that the non-coloured points bounce around in a chaotic fashion under iteration of this map. The Mandelbrot set is the set of points c such that the Julia set for f(z) = z^2 + c is connected (a technical term that means roughly what you think it means).

I don't know your level of education so I'm not explaining further because I don't want to condescend. This information is on Wikipedia and in books, so you can take the time to understand these concepts a little better if you're interested. You didn't ask for an explanation of the Mandelbrot set, after all, so I'm assuming that you've thought a bit about it. I'm here to answer to the significance of the concept.

[This image I linked before](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) is significant for the validity of the Newton's method for finding the roots of a polynomial equation -- specifically, z^3 = 1. When you seed Newton's method with an initial guess, which root will the method eventually approximate? The answer depends, of course, on where you started. What is surprising, at least at first, is that the root you find might be quite far away from your initial point, even when there are other perfectly reasonable roots much nearer. In other words, this Julia set is telling you about when and how to use Newton's method, and what kind of things you should worry about. The Mandelbrot set is an example of how to classify Julia sets, which in turn helps you extract useful information from a given Julia set.

More generally, the value of so-called chaos theory (for which the Mandelbrot set is rather foundational) is, to me, in analyzing the performance of numerical algorithms. Describing the real-world uses of numerical algorithms is about as difficult as describing the real-world uses of writing.

Edit: some grammar and other minor fixes",null,1,cdoykpe,1rlhst,askscience,top_week,7
EvOllj,"fractals and recursive functions allow a line to have near unlimited length within a limited area, the length only limited only by the possible resolution and possible accuracy to display/measure. They are pretty uniform where they are recursive. There are some geometrical and symmetrical properties interesting for some math and physics.",null,7,cdojbqs,1rlhst,askscience,top_week,5
cowboysauce,"Iron doesn't kill stars, it's just that stars cannot use iron as fuel because the fusion of iron doesn't release energy. When iron accumulates in the core of a star, it means that the star has run out of fuel.

It's a similar relation to ashes and a fire. Ashes aren't what kill a fire.",null,1,cdoga2p,1rljcb,askscience,top_week,37
drzowie,"The Sun has a significant amount of iron in it already! [The solar corona is about 0.01% iron by number](http://arxiv.org/pdf/astro-ph/0004007v1.pdf) (about 0.5% by mass), and the deep interior is within a factor of 4 of that number.

In other words, the Sun may already contain as much as 10^28 kg of iron.  That's a Hell of a lot of iron.  (Consider that the Earth only has 6x10^24 kg of stuff total -- so the Sun contains literally thousands of Earth-masses of iron).

But the Sun keeps chugging along just fine.

The fact that there's so much iron in the Sun, but it isn't even close to out of fuel, implies that it's a second-generation star:  the iron must have been present in the material from which the Sun was made, because it's hard to make a lot of iron in a star that is still 'burning' hydrogen and helium -- the core just doesn't get hot enough to drive the fusion reactions that make iron.  

We can also tell that the Sun is second-generation, because we have lots of iron, enough gold to use it for jewelry, and small quantities of exotic fissile materials right here on Earth.  Those heavy elements are only produced in large quantities by endothermic fusion in supernovae or by neutron capture in very metal rich stars.  Hence the accretion disk that became the solar system must have *already* been processed through the interior of a large, metal-rich star.  

So the facts that your car is made of steel (because iron is plentiful and therefore cheap), your wedding band is made of gold (because gold can be found in small but non-microscopic quantities), and we have a nuclear proliferation problem (because there are fissile elements on Earth) all stem from the same root cause -- as Carl Sagan pointed out, ""We're all starstuff"" that must have at one time been in the middle of a supernova.

But the titanic quantity of iron already in the Sun isn't quenching fusion, because there's still plenty of energy to be had by fusing light elements together.  The core is thought to be about halfway through its initial hydrogen load.  ",null,0,cdopgyd,1rljcb,askscience,top_week,10
inventor226,"There are several factors here.

1) The mass of the star

2) The age of the star

3) The mass of the iron (to have any effect it needs to be a significant fraction of the stars mass)


But if we are taking about pieces of iron small enough to be controlled by any traditional scifi civilization it will have no effect. Stars produced today are not pure H/He, they are some fraction metal (in astronomy if it isn't H/He it is a metal). They have something called a [metallicity](http://en.wikipedia.org/wiki/Metallicity), a ratio of some element to another (usually Fe to H).",null,1,cdogg18,1rljcb,askscience,top_week,8
pigeon768,"If you were to inject a [1.44 solar mass](https://en.wikipedia.org/wiki/Chandrasekhar_limit) slug of iron into a star's core, and were able to displace anything else that might be potential fuel, it would die. The iron would collapse into a neutron star, and the rest of the star would likely go supernova. The entire thing would either collapse into a black hole or add to the mass of the ""seed"" neutron star.

Injecting 1.44 solar masses of iron directly into a star's core is a difficult engineering problem.",null,2,cdolehb,1rljcb,askscience,top_week,8
cowboysauce,"Iron doesn't kill stars, it's just that stars cannot use iron as fuel because the fusion of iron doesn't release energy. When iron accumulates in the core of a star, it means that the star has run out of fuel.

It's a similar relation to ashes and a fire. Ashes aren't what kill a fire.",null,1,cdoga2p,1rljcb,askscience,top_week,37
drzowie,"The Sun has a significant amount of iron in it already! [The solar corona is about 0.01% iron by number](http://arxiv.org/pdf/astro-ph/0004007v1.pdf) (about 0.5% by mass), and the deep interior is within a factor of 4 of that number.

In other words, the Sun may already contain as much as 10^28 kg of iron.  That's a Hell of a lot of iron.  (Consider that the Earth only has 6x10^24 kg of stuff total -- so the Sun contains literally thousands of Earth-masses of iron).

But the Sun keeps chugging along just fine.

The fact that there's so much iron in the Sun, but it isn't even close to out of fuel, implies that it's a second-generation star:  the iron must have been present in the material from which the Sun was made, because it's hard to make a lot of iron in a star that is still 'burning' hydrogen and helium -- the core just doesn't get hot enough to drive the fusion reactions that make iron.  

We can also tell that the Sun is second-generation, because we have lots of iron, enough gold to use it for jewelry, and small quantities of exotic fissile materials right here on Earth.  Those heavy elements are only produced in large quantities by endothermic fusion in supernovae or by neutron capture in very metal rich stars.  Hence the accretion disk that became the solar system must have *already* been processed through the interior of a large, metal-rich star.  

So the facts that your car is made of steel (because iron is plentiful and therefore cheap), your wedding band is made of gold (because gold can be found in small but non-microscopic quantities), and we have a nuclear proliferation problem (because there are fissile elements on Earth) all stem from the same root cause -- as Carl Sagan pointed out, ""We're all starstuff"" that must have at one time been in the middle of a supernova.

But the titanic quantity of iron already in the Sun isn't quenching fusion, because there's still plenty of energy to be had by fusing light elements together.  The core is thought to be about halfway through its initial hydrogen load.  ",null,0,cdopgyd,1rljcb,askscience,top_week,10
inventor226,"There are several factors here.

1) The mass of the star

2) The age of the star

3) The mass of the iron (to have any effect it needs to be a significant fraction of the stars mass)


But if we are taking about pieces of iron small enough to be controlled by any traditional scifi civilization it will have no effect. Stars produced today are not pure H/He, they are some fraction metal (in astronomy if it isn't H/He it is a metal). They have something called a [metallicity](http://en.wikipedia.org/wiki/Metallicity), a ratio of some element to another (usually Fe to H).",null,1,cdogg18,1rljcb,askscience,top_week,8
pigeon768,"If you were to inject a [1.44 solar mass](https://en.wikipedia.org/wiki/Chandrasekhar_limit) slug of iron into a star's core, and were able to displace anything else that might be potential fuel, it would die. The iron would collapse into a neutron star, and the rest of the star would likely go supernova. The entire thing would either collapse into a black hole or add to the mass of the ""seed"" neutron star.

Injecting 1.44 solar masses of iron directly into a star's core is a difficult engineering problem.",null,2,cdolehb,1rljcb,askscience,top_week,8
Henipah,"Plastics are polymers of very long molecules bound by covalent bonds. Ionising radiation can break these bonds. Metals have a completely different molecular structure, a bunch of nuclei in a sea of electrons. If you bash a metal it just deforms but stays together. ",null,0,cdojqjl,1rlu8n,askscience,top_week,5
Platypuskeeper,"Metals don't degrade because of the nature of how metals work. The valence electrons (the outmost ones who do bonding) are free to move about within the metal. In plastic, the stuff is made up of molecules, and the electrons aren't bound to their places(bonds) within the molecule. A UV excitation with break the bond, and then the molecule comes apart. 

In a metal, the atoms are packed much more densely in a crystal lattice. And since the electrons are are free to move about, if an electron is excited by UV (even to the extent of leaving the metal), then other electrons can quickly take their place. Removing a few electrons temporarily doesn't necessarily break any bonds, and the atoms can't move even if you do temporarily break a bond to it, as it's tightly packed to it's neighbors. 

If you break the C-C bond of a carbon chain in a polymer though, there's smaller chance of the two atoms finding each other and managing to bond again, as they're free to move about.
",null,0,cdorfor,1rlu8n,askscience,top_week,3
joca63,"From what I've read in the comments, the simplest answer is touched upon, but is largely missing.

Rubbers and plastics are made of very long carbon chains. This is actually a fairly unfavoured state for the system, entropy would prefer short chains. So when they are hit eith enough energy to break bonds they will tend to make smaller chains. It happens that UV light has enough energy to do this, leading to slow degradation of the plastic.

Metals on the other hand can't really be though of as having bonds. At least not covalent bonds like the ones in rubber and plastic. So first of all there is no real bond that can break. Secondly metals exist as a lattice. This is the lowest energy state for an elemental metal at room temperature and presssure (save mercury), so there is no more favoured state for the metal to be in.

Another effect that happens particularly with aluminum is the creation of a passivation layer. With the plastic, the molecules are exposed directly with air, allowing reaction with the air itself. (in theory if you leave plast long enough in air it would turn to carbon dioxide. it would just take an insanely long time because of the activation barrier) The instant a metal such as aluminum is exposed to air it forms a very thin layer of aluminum oxide which prevents further reaction. ",null,0,cdq1obi,1rlu8n,askscience,top_week,2
NotFreeAdvice,"I would like to add to this one point that is missing so far.  

Metals (because they are great electrical conductors) do not allow for deep penetration of light into their bulk.  Thus, the interaction of metals with light (such as UV) usually take place at the surface.  

What this means is that, if there were to be any light-initiated chemistry, it would also need to take place at the surface.  However, most metals are pretty reactive, actually, and their surface will already be coated with a number of molecules (or -- even more likely -- will have formed an oxide layer).  These molecules then protect the surface of the metal from further reaction (most of the time).  

At any rate, this is not *universally* true, and the other explanations below provide more fundamental reasons why metals are not reactive under UV-light, but I thought it would be interesting to point out a more practical reason as well.  ",null,1,cdowsew,1rlu8n,askscience,top_week,2
bipnoodooshup,"It is my uneducated understanding that any organic substance is susceptible to ultraviolet rays if it does not use them to produce food (ie photosynthesis). That's why we get skin cancer, plastics get weaker, pigments lose their vibrancy, etc. Meanwhile metals seem to only be susceptible to other elements. Mercury eats aluminum",null,6,cdoj942,1rlu8n,askscience,top_week,3
afcagroo,"When video is slowed down, you just see fewer still frames per second.  Each frame is imaged the same way as before. The motion has been sampled, essentially.   
  
Music is slowed down for real (in the analog domain), so a given pitch changes at a slower number of cycles per second (Hertz). ",null,0,cdojpxg,1rluan,askscience,top_week,5
selfification,"Color does change when you change speeds.  It's called relativistic doppler shifting.  It just works in a different order of magnitude and also slightly differently because sound works with a fixed medium.  We can actually tell how fast things are moving towards or away from us based on how much their colors are shifted from what their supposed to be.  Hubble's law is based off of such measurements and the age of the universe is derived for this principle.

Oh wait, you meant when a video is slowed down...  well in that case /u/afcagroo has the right answer.  Also note that if you did digital music playback where the music is actually not time-stretched but instead chopped up and replayed at regular speed, you'll hear no pitch change.  You'll instead here very choppy audio of the right frequency instead (which is what you're seeing in video - more choppiness).",null,0,cdozyme,1rluan,askscience,top_week,4
iorgfeflkd,"You'd start accelerating until you reached the center, then start decelerating until you reach the other side. You'd get to about 8 km/s in the middle, and the whole trip would take about 38 minutes. 

If you controlled your descent so that you're weren't accelerating the whole time, and there was some kind of hollowed out cave in the center, you could conceivably float there.

You can read more about the details [here](http://arxiv.org/pdf/1308.1342.pdf)",null,3,cdol1lz,1rm1ho,askscience,top_week,18
LibertasEtSerenitas,"Whoa, hold on here.  I'm having trouble with some air resistance calculations. If you were to assume the air in the hole is not affected by the heat of the earth, e.g. insulated adiabatic tunnel walls, isn't it still super hot because of the compression from the weight of the air above it?  Not only is it high pressure air, but also high temperature?",null,1,cdopqex,1rm1ho,askscience,top_week,1
listens_to_galaxies,"The calculation of electron speeds in a conductor is a pretty classic exercise in electromagnetism.  It depends on the current (charge per second) passing through your conductor/cable and the cross section (thickness) of the cable.  More current requires the electrons to move faster, while thicker cables hold more electrons so they don't need to move as fast.  The formula, and a sample calculation, can be found [here on Wikipedia](http://en.wikipedia.org/wiki/Drift_velocity).  Bear in mind that this assumes DC current; for an AC current the electrons will oscillate back and forth.

For typical currents of a few amperes, and typical wire thicknesses, electron speeds are generally on the order of millimetres per second.  So it would take on the order of hours (1000s of seconds) to travel distances of ~6 feet (a few meters).

This may seem really weird, because when you flip a light switch or plug in a device, it works instantly rather than taking hours to turn on.  This is because the signal is carried by the electric field that moves the electrons.  The electrons themselves act more like carriers of the signal/power, rather than the source, just like sound waves can propagate through air or water much faster than the individual air/water atoms.  The electric field propagates at the speed of light, so there isn't a noticeable delay between plugging something in and it working.",null,1,cdou7vk,1rmgbh,askscience,top_week,8
fizixx,"It takes a LONG time. Many people think it travels at the speed of light, which is not the case. I did a calculation many years ago, based on my guess at how far (how much wiring) extended from a light switch to a fluorescent light in the room. It came out to almost 40 hours for an electron leaving the switch and traveling through all the wiring to the light.",null,0,cdpl7aa,1rmgbh,askscience,top_week,1
paolog,"Things that happen inside the eye as seen by the owner of the eye, such as [floaters](http://en.wikipedia.org/wiki/Floater), for example. Of course, these could be captured by a camera by viewing the inside of the eye, but they aren't captured as seen by the person.",null,0,cdougep,1rmgp4,askscience,top_week,7
FriendlyCraig,"There are certain optical illusions which are lost when photographed, although it's more the brain seeing than the eye. If you ever look at a picture of the moon near the horizon, it looks, awfully small while in person it's huge.",null,1,cdp2tu5,1rmgp4,askscience,top_week,3
samloveshummus,"Both dinosaurs and mammals are examples of clades, groups of organisms defined by a common ancestor, so that much is correct. As to your specific example, Stegosaurus belongs to the family Stegosauridae while Velociraptor belongs to the family Dromaeosauridae while both humans and chimps belong to the family Hominidae, so we're much closer than a typical pair of mammal species. Moreover, while chimpanzees and us are contemporaries, Stegosauruses were as ancient to Velociraptors (another 75 million years) as Velociraptors are to us (75 million years old).",null,0,cdovp8g,1rmgph,askscience,top_week,4
stuthulhu,"Dinosaurs have distinguishing physical characteristics and common ancestors as you can see [here](http://en.wikipedia.org/wiki/Dinosaur#Definition)

Interestingly, this includes birds, and excludes other things commonly thought of as dinosaurs, for instance [plesiosaurs](http://en.wikipedia.org/wiki/Plesiosauria), [icthyosaurs](http://en.wikipedia.org/wiki/Ichthyosaurs) and other marine reptiles, flying reptiles such as [pterosaurs](http://en.wikipedia.org/wiki/Pterodactyls), or animals such as the [dimetrodon](http://en.wikipedia.org/wiki/Dimetrodon) which were actually more closely related to mammals.",null,0,cdoz6xi,1rmgph,askscience,top_week,2
patchgrabber,"If you're asking can wind make water freeze, the answer is no, unless the air temperature is below freezing. For inanimate objects and things like water, the only thing wind does is cool its present temperature to ambient air temperature. It cannot cool anything more than the ambient temperature.",null,0,cdoyf5b,1rmgtz,askscience,top_week,1
TangentialThreat,"Your basic fan can move heat around in a room, but it won't produce temperatures below ambient unless you happen to have chosen to point it at something like a sweaty human or a bowl of water AND the humidity is not 100%.

Evaporation will produce a cooling effect. The most energetic water molecules get kicked out, leaving behind low-energy ones. Near standard conditions this is mild and will not freeze a bowl of water in a warm room, but [here's a paper](http://www.sciencedirect.com/science/article/pii/S0017931003000723) saying you could produce ice this way by drying incoming air with desiccant. Taken to its limit and using a fluid other than water, this is also close to an adsorption heat pump.
",null,0,cdozk2p,1rmgtz,askscience,top_week,1
fizixx,"I think you have to have a certain initial set of conditions, but.....yes, you can. 

High speed air moving pas a vessel of water can reduce its temperature to freezing. The fast air reduces the pressure in its vicinity. Based on the Idea Gas law, that means the temperature will also go down.",null,0,cdpleq7,1rmgtz,askscience,top_week,1
TITS_ME_UR_PM_PLS,"Well, [Chile](http://earthquake.usgs.gov/earthquakes/world/events/1960_05_22.php) had a magnitude 9.5 in 1960.

Here's the thing: Richter magnitude [was originally developed](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Development) for a tiny part of California, on a specific type of instrument (a Wood-Anderson torsion seismograph). It's been adapted for use elsewhere, using other instrumentation, based on total energy release.

Note from [here](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Examples) that the highest recorded incident- the 9.5 in Chile- released 11 EJ (exojoules) of energy, equivalent to the release of 2.7 gigatons of TNT.

A 10.0 on that scale would be 63 EJ, or 15 gigatons of TNT, and that one has ""Never [been] recorded, equivalent to an earthquake rupturing a very large, lengthy fault, or an extremely rare/impossible mega-earthquake, shown in science fiction.""

Anyway- depth is a big consideration here, as a fault that is close to or at the surface will release energy more efficiently to us surface-dwellers than a very deep one, [such as this 8.3 recorded at 378 miles below the seafloor.](http://www.livescience.com/34671-russian-earthquake-deepest-ever.html)",null,1,cdp49vq,1rmj44,askscience,top_week,6
dakami,"At a certain point, the earth can't move any more energy, and so Richter intensity starts referring to time.  So, maybe an M7 that goes on for minutes and minutes?  (I live through the SF quake of 89, that 7.1 lasted for but 18 seconds.)",null,2,cdozu2t,1rmj44,askscience,top_week,5
chuck10470,"The Richter Scale is exponential. A ""1"" level increase,  say from 7 to 8 is actually 10 times more powerful in terms of total energy release. A 9 would be 100 times as strong as a 7. A 10 on the scale is a staggering amount of energy. But this isn't solely what makes earthquakes dangerous. Focus depth, the distance beneath the surface that the epicenter is located is just as important. Shallow quakes deliver more of that energy to the surface in a concentrated area,  so it is possible to have more damage from a weaker quake. ",null,0,cdpeuf8,1rmj44,askscience,top_week,2
TITS_ME_UR_PM_PLS,"Well, [Chile](http://earthquake.usgs.gov/earthquakes/world/events/1960_05_22.php) had a magnitude 9.5 in 1960.

Here's the thing: Richter magnitude [was originally developed](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Development) for a tiny part of California, on a specific type of instrument (a Wood-Anderson torsion seismograph). It's been adapted for use elsewhere, using other instrumentation, based on total energy release.

Note from [here](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Examples) that the highest recorded incident- the 9.5 in Chile- released 11 EJ (exojoules) of energy, equivalent to the release of 2.7 gigatons of TNT.

A 10.0 on that scale would be 63 EJ, or 15 gigatons of TNT, and that one has ""Never [been] recorded, equivalent to an earthquake rupturing a very large, lengthy fault, or an extremely rare/impossible mega-earthquake, shown in science fiction.""

Anyway- depth is a big consideration here, as a fault that is close to or at the surface will release energy more efficiently to us surface-dwellers than a very deep one, [such as this 8.3 recorded at 378 miles below the seafloor.](http://www.livescience.com/34671-russian-earthquake-deepest-ever.html)",null,1,cdp49vq,1rmj44,askscience,top_week,6
dakami,"At a certain point, the earth can't move any more energy, and so Richter intensity starts referring to time.  So, maybe an M7 that goes on for minutes and minutes?  (I live through the SF quake of 89, that 7.1 lasted for but 18 seconds.)",null,2,cdozu2t,1rmj44,askscience,top_week,5
chuck10470,"The Richter Scale is exponential. A ""1"" level increase,  say from 7 to 8 is actually 10 times more powerful in terms of total energy release. A 9 would be 100 times as strong as a 7. A 10 on the scale is a staggering amount of energy. But this isn't solely what makes earthquakes dangerous. Focus depth, the distance beneath the surface that the epicenter is located is just as important. Shallow quakes deliver more of that energy to the surface in a concentrated area,  so it is possible to have more damage from a weaker quake. ",null,0,cdpeuf8,1rmj44,askscience,top_week,2
MasterPatricko,"It's not the fact there are electrons moving that's important, it's that *electromagnetic charge* is moving. Any moving/changing charge (electrons, holes, ions, protons...) is a current and is associated with electric and magnetic fields.

There are other types of charge - colour, weak force stuff - but those forces have a different structure (not inverse square law) so there is no analogy with electromagnetic forces. 

There is a particular view of gravity (which is inverse square law) which allows you to write it analogously to EM -- where mass is the charge -- and there you can talk about gravitoelectric and gravitomagnetic fields. If I remember correctly that's only an approximation to full GR though.",null,0,cdow50o,1rmjmp,askscience,top_week,5
Dannei,"Any charged particle is affected by the laws of electromagnetism, and so can form electric currents and will be affected by (or generate) magnetic fields.

(However, you can't just start throwing protons down the wires into your lightbulb - it's a bit more complicated than that!)",null,0,cdouxgs,1rmjmp,askscience,top_week,2
iorgfeflkd,"There are ionic currents based on charged atoms. In your body, nerve signals are conducted by having ions move in and out of cells. This produces an extremely small magnetic field that can be detected in magnetoencephalography.",null,0,cdoxke2,1rmjmp,askscience,top_week,2
MasterPatricko,"Jello is a [gel](http://en.wikipedia.org/wiki/Gel), a substance which is a fluid trapped in a network of bonds.

Basically it's a solid until you put a force on it, when it flows like a liquid.

A similar concept is a paste.

There is no requirement that mixtures/suspensions like gels and pastes fall neatly into 'solid' or 'liquid'.",null,0,cdowkbh,1rmloo,askscience,top_week,2
fizixx,Depending on the concentrations I would say yes. The sugar needs proximity to the water molecules to dissolve. If there are other particles in the water (salt) the sugar would not have the frequent/constant contact with the water as it would otherwise.,null,0,cdplrgf,1rmn9e,askscience,top_week,1
joca63,"In general, no it wouldn't. For the concentration of one solute to effect another they must have a common ion. Since salt only gives sodium and chloride ions it wouldnt have any effect on the ability for sugar to dissolve. There may be some supersaturated case where this is not true, but I don't know of any.",null,0,cdq0p5u,1rmn9e,askscience,top_week,1
CosmotheSloth,"A lot of biochemists nowadays turn to computational chemistry methods to obtain potential molecules for drug uses.

Large libraries of different molecules are searched by different methods to give results that best match your requirements. You can search via different criteria; by looking at current drugs that are in use or by looking for specific functional groups or shapes of molecules, for example. The main methods of molecular modelling are docking or pharmacophores.

Pharmacophores are computational descriptions of molecules that specify the positions relative to one another of the main functional groups in space.

Docking is a method use to see if the various orientations of a known molecule have a preferred fit with a desired receptor molecule, much like the lock and key model of enzymes and substrates in biochemistry.

More simulations are then used to determine whether the synthesis and testing of the molecule (drug) is viable financially (as essentially all computational chemistry and pharmaceuticals comes down to money!).

Source: 3rd Year Degree Level Chemist remembering notes from last years Cheminformatics module.",null,0,cdovhni,1rmnfn,askscience,top_week,6
rupert1920,"An example of drug development is this:

We have a known site we want to bind to - usually it's a receptor, sometimes it's a protein binding site. To find a drug candidate, one uses computational chemistry: DOCKing studies uses a huge library of potential drug candidates, and examples how well they bind to the site we're interested in. Based on this data, we have a list of molecules that bind the strongest.

The next step is where medicinal chemists spend most of their time: to make a drug candidate into a successful drug, one must make sure that it makes the transition from ""the computer says it binds strongly"" to ""it will make it through your body's metabolism and actually do its job"". Often time, many drug candidates work well _in vitro_, but fails _in vivo_. Even if they seem to work well in animal models, most failures occur in Phase II clinical trials, when the the candidate was shown to be no better than placebo.

So the medicinal chemist is tasked with modifying your drug candidate to survive the journey to the site of action. This is often done by replacing functional groups we know will undergo chemical changes - for example, an orally administered drug shouldn't contain ester groups, as they are easily hydrolyzed in the stomach. They will replace this with a non-hydrolyzable group, such as a straight chain ketone, which retains the carbonyl group but does away with the other oxygen.

Other replacement strategies are more complex. Peptides, for example, are easily hydrolyzed by peptidases, but depending on the conformation, the backbone can be completely replaced. One strategy is using aryl chains as a mimetic for alpha helixes.

In short, there are many considerations just for addressing the pharmacokinetics of a drug. A general rule for a drug is [Lipinski's rule of five](http://en.wikipedia.org/wiki/Lipinski%27s_rule_of_five), which lists some general characteristics that'll make a drug lipophilic enough to be absorbed easily, and hydrophilic enough to be distributed easily.

There can be other changes one makes to attempt to increase binding with the site. For example, knowing the landscape of the receptor, one make attempt to introduce further interactions by tagging on extra groups onto your drug candidate. For example, if a binding site has an adjacent pocket formed by amino acids with hydrogen bond acceptors, you can add hydrogen bond donors onto your molecule to attempt to increase the interaction.

To address your last part question: no, not all drugs are synthetic copies of a biologically active chemical. There are many drugs that have their own, unique interaction with the body. For example, the classic chemotherapy drug [cisplatin](http://en.wikipedia.org/wiki/Cisplatin) doesn't work by mimicking a molecule in the body - it works by chemical reaction with DNA.

That said, from the sample description of a drug development above, you can see how the strategy would _lead_ to a compound being chemically close to the natural compound it's attempting to mimic.",null,0,cdowocn,1rmnfn,askscience,top_week,3
System09,"Things vibrate differently due to 3 things: their shape, material they are made of and forces acting on them.

 Calculating these modes of vibration isn't easy and even simple shapes, like a sphere, can vibrate in many complicated ways. The sound you hear is a combination of these modes, with primary being most important, as it has the highest amplitude. 

Easiest thing to imagine is a string on a guitar. Different thicknesses give different sounds. Using your finger, to shorten the cord, also gives a different sound. You can also tune the strings by adjusting the tension in the head of the guitar. ",null,0,cdovo20,1rmnul,askscience,top_week,3
natty_dread,"&gt; What would I be looking at? 

The thing is, that the limit is not a matter of precision of the microscope. It is inherently impossible to ""see"" atoms.

The reason for this is, that the wavelength of visible light is ~400-700 nm = 4 x 10^-7 - 7 x 10^-7 m.  
Atoms have roughly the size of ~1Å = 1 x 10^-10 m. 

As you can clearly see, the shortest wavelength of visible light is still more than 3 orders of magnitudes bigger than an atom. Thus, visible light will never be able to represent Atoms.

What we can do, and what we are already doing, is using particles of shorter wavelength than visible light, like electrons.

This will result in an representation of Atoms in visible light.

&gt; If I zoomed into an electron when would I be able to tell it is made up of quarks? For that matter what would the inside of an electron really look like if I dissected it.. 

Electrons are not made up of quarks. In fact, as far as we know, electrons are not ""made up"" of anything. They are fundamental particles.  
According to the standard Model of Particles there are three kinds of fundamental particles:  

*  Quarks  - Those are the particles that make up protons and neutrons

* Leptons - The most famous of which is the electron. There are also neutrinos etc...

* Gauge Bosons - Those are force carriers

(And then there's the Higgs Boson which is a whole other story)",null,0,cdow6ua,1rmo1g,askscience,top_week,5
chrisbaird,"First, of all, atoms are too small to see using visible light. The wavelength of visible light is simply too large to interact with single atoms in a predictable, individual way. But that does not mean we can't ""see"" them.

Secondly, atoms are fuzzy quantum clouds of wavefunctions and don't have hard boundaries, so you can't see them in the sense of seeing a sharp silhouette or a distinct surface. It's more like seeing a cirrus cloud in the sky. You can see where the cloud is and where the cloud isn't, but the *size* and *shape* of the cloud depends somewhat on how you define the border of the cloud, how you tweak the contrast in your image, and how you calibrate your camera.

With that said, we can see atoms by bouncing electrons off of them instead of light:

http://www.research.ibm.com/articles/madewithatoms.shtml#fbid=i02OkJ97gWy

http://researcher.ibm.com/researcher/view_project_subpage.php?id=4251

http://education.mrsec.wisc.edu/background/STM/images/stm17.jpg

http://images.iop.org/objects/phw/news/13/8/19/afm1.jpg

Also, electrons are fundamamental according to our current knowledge. They are points particles composed of nothing else and can't be dissected.",null,0,cdpovwy,1rmo1g,askscience,top_week,1
yeast_problem,"The answer is resistance to metal fatigue. A single strand could have a defect  in the crystal structure causing a point of weakness which will spread, while the probability of multiple strands having a defect in the same location is very low.

Stranded cables are more flexible for the same diameter.
",null,1,cdow0m8,1rmpm7,askscience,top_week,13
Benginieur,"Tensile strength in engineering is one-directional and means there is no bending influence. It depends on the material and is proportional to the area of the cable. So a group of thin cables twisted together will be weaker, because there will be small gaps between them that don't add to the tensile strength.

Now for cutting foam the force from pressing the wire against it will result in a change in force distribution inside the cable with compression on the side of the foam and tension on the other side of the cable with a neutral fibre in the center (http://toolboxes.flexiblelearning.net.au/demosites/series10/10_01/content/bcgbc4010a/04_struct_members/01_beams/page_003.htm).

A single strand will be more stiff as the diameter factors in heavily (Try bending a ruler along both axis and you'll see the effect. Same area but very different flexibility.) So, in my opinion, the reason to use many twisted cables would be if the bending of the single strand would plastically deform it near the edge, where forces are strongest, and thus destroy it over time. Thin twisted cables should distribute the forces more evenly, but they won't be stronger in general with the same diameter.

",null,0,cdp7q90,1rmpm7,askscience,top_week,4
WorldIsImagination,"I would think a single strand would be stronger in theory and here's why:

If you were to set two metal rods down on a piece of concrete, one twisted and one regular, and you set a rock on both of varying weights until one broke, the spiral would break first. The weight of the object bearing down on the spiral would crack one of the thinner strands before the single strand. ",null,0,cdpeqfx,1rmpm7,askscience,top_week,1
walexj,"If you consider a bundle of untwisted small diameter wires which together form the same overall diameter of a single wire of the exact same material, they're strength will be equivalent. The reason that things like suspension bridges and other cables with tensile loads are made of several strands is due to the likelihood of failure. A single wire design with a safety factor of 1.5 may have a material defect that can propogate through the material and cause a catastrophic failure. The multi-wire cable of the same overall diameter may have a single strand with a material failure that causes the single strand to fail, but then you still have 99/100 other strands to take the load. Because your safety factor was 1.5, your cable is still safe by a factor of around 1.49.

Things get a little more complicated when you twist, or braid, the wires as that introduces things like bending stress. But overall, they're pretty close to being similarly strong and can handle the same tensile stresses. The difference is that multi-wire cables are tougher and more reliable. They're also way easier to manufacture as well, so win-win.",null,0,cdpq4ey,1rmpm7,askscience,top_week,1
yeast_problem,"The answer is resistance to metal fatigue. A single strand could have a defect  in the crystal structure causing a point of weakness which will spread, while the probability of multiple strands having a defect in the same location is very low.

Stranded cables are more flexible for the same diameter.
",null,1,cdow0m8,1rmpm7,askscience,top_week,13
Benginieur,"Tensile strength in engineering is one-directional and means there is no bending influence. It depends on the material and is proportional to the area of the cable. So a group of thin cables twisted together will be weaker, because there will be small gaps between them that don't add to the tensile strength.

Now for cutting foam the force from pressing the wire against it will result in a change in force distribution inside the cable with compression on the side of the foam and tension on the other side of the cable with a neutral fibre in the center (http://toolboxes.flexiblelearning.net.au/demosites/series10/10_01/content/bcgbc4010a/04_struct_members/01_beams/page_003.htm).

A single strand will be more stiff as the diameter factors in heavily (Try bending a ruler along both axis and you'll see the effect. Same area but very different flexibility.) So, in my opinion, the reason to use many twisted cables would be if the bending of the single strand would plastically deform it near the edge, where forces are strongest, and thus destroy it over time. Thin twisted cables should distribute the forces more evenly, but they won't be stronger in general with the same diameter.

",null,0,cdp7q90,1rmpm7,askscience,top_week,4
WorldIsImagination,"I would think a single strand would be stronger in theory and here's why:

If you were to set two metal rods down on a piece of concrete, one twisted and one regular, and you set a rock on both of varying weights until one broke, the spiral would break first. The weight of the object bearing down on the spiral would crack one of the thinner strands before the single strand. ",null,0,cdpeqfx,1rmpm7,askscience,top_week,1
walexj,"If you consider a bundle of untwisted small diameter wires which together form the same overall diameter of a single wire of the exact same material, they're strength will be equivalent. The reason that things like suspension bridges and other cables with tensile loads are made of several strands is due to the likelihood of failure. A single wire design with a safety factor of 1.5 may have a material defect that can propogate through the material and cause a catastrophic failure. The multi-wire cable of the same overall diameter may have a single strand with a material failure that causes the single strand to fail, but then you still have 99/100 other strands to take the load. Because your safety factor was 1.5, your cable is still safe by a factor of around 1.49.

Things get a little more complicated when you twist, or braid, the wires as that introduces things like bending stress. But overall, they're pretty close to being similarly strong and can handle the same tensile stresses. The difference is that multi-wire cables are tougher and more reliable. They're also way easier to manufacture as well, so win-win.",null,0,cdpq4ey,1rmpm7,askscience,top_week,1
arble,"Combustion requires a source of heat. In an engine, there's easily enough heat for everything to combust but there often isn't quite enough oxygen. Outside of the engine, there's plenty more oxygen but there's no longer the heat, so the partially combusted products remain as they are thereafter.",null,0,cdotbcr,1rmpo9,askscience,top_week,5
arble,"Ozone is made in the upper atmosphere by the interaction of normal diatomic oxygen (O*_2_*) with ultraviolet light. The light splits apart an oxygen molecule to form two highly reactive oxygen radicals which can attach themselves to an O*_2_* molecule to form O*_3_*.

In the lab, ozone can be made a number of ways, but the most common is usually exposing oxygen to a strong electrical discharge.",null,0,cdotlk7,1rmug0,askscience,top_week,3
rocketsocks,"Ozone is the product of almost any energetic reaction involving Oxygen. A common way of producing ozone is to simply run a spark of electricity through the air. Ozone is also a common byproduct of ordinary combustion, as in an automobile, though catalytic converters and electric engine control reduce ozone production substantially.",null,0,cdotpqq,1rmug0,askscience,top_week,2
patchgrabber,"I'm not sure about a chemical reaction, but a tesla coil produces a decent amount of ozone, you can even use it to decontaminate water (lots of water treatment is done with ozone). Laser printers also produce smaller quantities of ozone through those electrical voltages as well.",null,0,cdovql2,1rmug0,askscience,top_week,1
NeverQuiteEnough,"followup question, at my school we have a physics club and we were operating our tesla coil as part of a demonstration for some kids.

after a few hours it stopped working.  I suspected it had reacted with all of the nearby oxygen, turning it into ozone.  Told our captain, put a fan on it and it worked when we turned it back on.  Was my guess likely to be right?",null,0,cdowyg5,1rmug0,askscience,top_week,1
Platypuskeeper,"This guy copy-and-pasted this [from here](http://www.syhdee.com/repositoryx-24-103-1.html) and his submissions history consists entirely of links to that website.

",null,0,cdp8dao,1rmvtm,askscience,top_week,4
rohrspatz,"The main problem is that MHC signaling also involves a *lot* of ""co-stimulation"". The T cell has to be able to bind to a lot of different cell surface receptors at the same time in order to be properly activated. ""Loose"" MHC floating around in the intercellular spaces wouldn't provide the necessary stimulation. 

Additionally, CD8 T cells trigger apoptosis by releasing chemicals directly onto the surface of the cell they're bound to - so even if loose MHC could stimulate them, they'd cause a lot more local damage than normal, because they would just be indiscriminately releasing toxins into the environment instead of ""targeting"" tumor cells. You'd basically end up with some kind of big ulcerated hole, not a neat little swiss-cheese with all the cancer gone and the normal stuff preserved. They would also be unable to seek out tumor cells that weren't in the local environment where the MHC had been injected, so the efficacy would still depend on the doctor being able to find and manually target every last cancerous cell.

It really wouldn't be any more effective than just cutting out the tumor mass or injecting drugs into it, because all of the limitations that make tumors ""inoperable"" would also apply to the hypothetical MHC injection treatment.",null,0,cdp430s,1rmw8k,askscience,top_week,2
SquirrelSoul,"I assume you are referring to the fact that birds/reptiles, and amphibians expel fecal matter and urine through a single opening - the cloaca, while mammals, with the exception of the monotremes and some marsupials, have separate openings. (Interestingly, the cloaca is also used for fertilization during sexual reproduction in most birds).

In biology we distinguish between ""elimination"" which is for substances that never cross a membrane and are expelled as feces, and ""excretion"" which is for substances that do cross a membrane and are expelled as a nitrogenous waste such as urine.

Reptiles (which includes birds) actually do have an excretory system. They have kidneys which direct uric acid to the intestinal tract via ureters. That waste then exits through the cloaca along with the feces - so effectively, reptiles practice elimination and excretion simultaneously.

Why do they do this instead of the way we do it? 

Essentially, the answer is evolution. Mutations for the creation of two exits occurred by chance in mammals, were passed on to offspring, and were selected for. The cloaca still worked just fine for birds and reptiles though so it wouldn't disappear entirely.",null,2,cdp5cr4,1rmyt8,askscience,top_week,5
dukwon,"It's Lorentz force at its simplest.

Charges moving perpendicular to magnetic fields (that little silver thing at the bottom is a magnet) will experience a force perpendicular to both the motion of the charge and the direction of the field.

Let's say that the magnetic field lines go parallel to the battery, then the current flowing through the straight sections of the wire at the bottom is perpendicular to the field, so the moving electrons experience a horizontal force perpendicular to the wire, causing it to rotate.

There will be some magnetic field induced by the rotation, but this will oppose motion",null,0,cdp3qlq,1rn09r,askscience,top_week,9
Platypuskeeper,"The activation energy is a [saddle-point](http://www.chm.bris.ac.uk/pt/harvey/elstruct/pics/pot_surface.jpeg). It's the highest point of the total energy when going from reactant to product through the path that requires the least energy. That includes all forms of energy in the system, but not all forms of energy in the system will necessarily change with the reaction coordinate. 

In practice, the 'reaction coordinate' is an abstracted form of the actual spatial coordinates of the atoms as they move from the reactant structure to the transition state to the product. So the form of energy required to overcome a transition state barrier is the kinetic energy of the atoms. 
",null,0,cdp6ak7,1rn24f,askscience,top_week,3
lokim,"I wrote a reentry simulator in javascript some years ago. It mainly deals with the ballistic decent in an atmosphere, http://mdj.dk/project/atmospheric-reentry-simulator/


I just realised that the angle of decent is not a configurable variable in the simulator, I might add it if you want to play around with it to see why survival depends on the right decent.

In practice there are many factors that can determine the landing point, have a look at http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA483248&amp;Location=U2&amp;doc=GetTRDoc.pdf for instance.

",null,0,cdowyc1,1rn2y4,askscience,top_week,18
Julian_Berryman,"[Check this out](http://www.faa.gov/other_visit/aviation_industry/designees_delegations/designee_types/ame/media/Section%20III.4.1.7%20Returning%20from%20Space.pdf).

It seems the main factors to consider are deceleration of the craft, heating due to atmospheric friction and the precision of your desired landing.",null,0,cdoz46a,1rn2y4,askscience,top_week,4
lokim,"I wrote a reentry simulator in javascript some years ago. It mainly deals with the ballistic decent in an atmosphere, http://mdj.dk/project/atmospheric-reentry-simulator/


I just realised that the angle of decent is not a configurable variable in the simulator, I might add it if you want to play around with it to see why survival depends on the right decent.

In practice there are many factors that can determine the landing point, have a look at http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA483248&amp;Location=U2&amp;doc=GetTRDoc.pdf for instance.

",null,0,cdowyc1,1rn2y4,askscience,top_week,18
Julian_Berryman,"[Check this out](http://www.faa.gov/other_visit/aviation_industry/designees_delegations/designee_types/ame/media/Section%20III.4.1.7%20Returning%20from%20Space.pdf).

It seems the main factors to consider are deceleration of the craft, heating due to atmospheric friction and the precision of your desired landing.",null,0,cdoz46a,1rn2y4,askscience,top_week,4
rohrspatz,"I unfortunately can't remember where I heard this, but one of my favorite explanations is: ""Evolution is not an engineering process"".

In other words, there's nobody sitting there evaluating things and going, ""hm, it would be better to do that, so let's make it happen"". A population of insects has no way of knowing that it would be better off with stripes and then achieving that effect. Traits emerge completely by chance, and if they *happen* to be beneficial, they persist.

But which traits emerge depends on the genetics of the organism and what traits it currently has. Let's imagine an insect that synthesizes a black pigment via a three step reaction: Chemical A is colorless, Chemical B is yellow, and Chemical C is black. A single mutation that disables the B-&gt;C enzyme will cause that reaction chain to stop at B, and all the pigment it produces will be yellow. That's definitely a trait that could emerge and then stick around. But then, let's imagine an insect that produces C directly from A. In order to produce B, it would have to somehow acquire the entire gene for the A-&gt;B enzyme out of thin air, and for the yellow pigment to be visible and not just masked by the black, it would also have to mutate/disable the A-&gt;C enzyme. That's really, really, really unlikely to happen spontaneously.

Although it's hard to shoehorn into this example, there are also lots of examples of traits that would require multiple mutations to arrive at the final ideal state, but the intermediate stages are so disadvantageous that all the mutations would have to happen simultaneously (the odds of which are seriously miniscule).

**TL;DR: basically, a more ideal genetic composition for an organism may exist in theory, but the actual range of possibilities for how it will evolve in real life is very much limited by the fact that it has to ""get where it's going"" through spontaneous mutations of the genes it already has.**",null,0,cdp31vs,1rn39a,askscience,top_week,4
polistes,"Apart from what rohrspatz has already typed, that evolution does not always lead to the optimal solution, there is an answer for this specific question as well. This is a strategy that does not work if ALL insects/animals have it. 

First of all, lets talk about why warning symbols work. Wasps can sting and have venom, and by also displaying strong colors, predators learn to associate these strong colors with the nasty sting. This association means they will leave the wasps alone, which is good for the wasps. This is the same for many species of colourful caterpillars or for example ladybirds; they taste disgusting so by displaying warning colours predators will not even try to eat them. 

Now, imagine you are a syrphid fly. You are not venomous and don't sting, but you have mimicked the wasps' colours. This means that the predators that associate your colours with the wasps will also leave you alone, which is great for you too!

But, imagine that ALL species of insects have these colours. Predators will now no longer learn to associate the colours with danger/disgust, because most of the prey they eat with those colours are not dangerous or disgusting at all. This makes the use of these warning colours obsolete. 

In fact, this has been studied a lot for poison dart frogs. There are many species of them, of which a part is poisonous and a part is just mimicking the poisonous ones. In areas in which the majority is members of the poisonous species, birds quickly learn not to eat the colourful frogs because eating them makes them very sick. They will avoid these frogs, and the mimic also thrives. Now imagine that in another area the number of mimics far exceeds the number of poisoned frogs. The bird will eat a few mimics and learns that it is good food and will eat more of them. The warning strategy does not work anymore, not even for the poisonous frog.

Tl;dr : If all insects display warning colours, predators no longer associate those colours with danger and will eat you regardless of them.  ",null,0,cdpnzut,1rn39a,askscience,top_week,3
GOD_Over_Djinn,"The multiplicative inverse of a real number n is the unique real number m such that nm=1. Hence, if you want a multiplicative inverse for ∞, you're going to need to define multiplication by ∞. You'll run into a hundred million problems here.

The fundamental issue is that the familiar operation of division is defined for real numbers, and real numbers have the [Archimedean property](http://en.wikipedia.org/wiki/Archimedean_property). Roughly speaking this says that for any real number x, there is a strictly bigger real number y. Since this wouldn't be true if we put ∞ into our real number system, trying to put ∞ in breaks everything.

It is often convenient to write 1/∞=0 as a shorthand for lim 1/n = 0 with the limit is as n-&gt;∞. This is a nice way to save paper, but that's about it.",null,1,cdp4ogo,1rn3pg,askscience,top_week,7
gababa,"Ok, . I hope this isn't too much, I kinda got carried away with this answer, but I just can't stop myself when I'm writing about math :P


First I would like to point out, that infinity is not a number, it is a concept used in math to describe mathematical objects that are ascending without an upper limit. For example the following series of numbers (1,2,3, ...) is ascending, and it has no upper limit, because for any number you may choose (lets say m), i can find one in this series that is bigger (lets say m rounded up + 1) and as such it is defined that its limit is infinity.

Notice now, that properties you listed of infinity are actually derived from it's definition. For example it is not changed because of multiplication by a finite numbers because, if you take the series from before (1,2,3, ...) and divide each number by a finite number (lets call it r), it still holds that for any number(m) you can through at me I can still find a number in the series that is bigger (now its m rounded up + 1 divided by r) and as such the limit remains the same. And because this is true for any such mathematical object with no upper limit, we say that infinity divided by any number remains infinity.

Now, although infinity is not a number, it has an intuitive concept for humans as something very very large, and this intuitive concept is on par with the mathematical definition for example - The universe is infinite because no matter how far you go, you can still go farther. (Just like in the series where for any number you choose, you can find a bigger one in the series.)

To proceed let us explore now what 1/inf actually means. If we take our example of the ever ascending series (1,2,3, ...), we said that it's limit is infinity, so 1/inf is what limits the series - (1/1, 1/2, 1/3, ...). This series is limited by 0, because, for any number you can give me, be it close to zero as you'd like, by going down the series, I can find a number in this series that is closer to zero. 

Notice, how 1/inf isn't actually zero, this thing we just calculated is not 'nothing at all', and it is not exactly a number. Just like infinity, we calculated a concept of something very very small, this is called an [infinitesimal](http://en.wikipedia.org/wiki/Infinitesimal). (Or at least it's close enough). It is the concept of something so small it can not be measured, so small that no matter what number you give me, it is always smaller, but still not nothing. And this infinitesimal displays some of the same properties infinity does, because they are the same kind of concept, and are mathematically defined similarly.",null,0,cdpg8yw,1rn3pg,askscience,top_week,3
sufferingplanet,"Because ""infinite"" is not actually a number. This is why you cannot have ""infinity plus one"" or ""infinite minus one"" as they are still infinite, regardless of how you modify them. An ""inverted infinite"" would remain infinite.",null,2,cdp1r7l,1rn3pg,askscience,top_week,3
MWVaughn,"Infinity isn't a number; it's a concept. This allows it to do its freaky stuff where certain ""infinities"" can be larger than others (for example, the number of numbers between 0 and 1 is infinite, but the number of numbers between 0 and 2 is also infinite, but twice as large too.) Because infinity isn't a number, it has no inverse. You can say the same for 0. 1/0=NaN, and 1/∞=NaN.",null,0,cdpzu4g,1rn3pg,askscience,top_week,1
SquirrelSoul,"Technically, insomnia could be caused by any chemical that when ingested acts as a [stimulant](http://en.wikipedia.org/wiki/Stimulant). Stimulants are substances that increase physiological arousal and often do this by acting on the sympathetic nervous system - by increasing the levels or activity of norepinephrine or dopamine.

The illicit drugs that increase arousal are amphetamines like methamphetamines (""meth"") or MDMA (""ecstasy""), or norepinephrine reuptake inhibitors like cocaine.

The milder stimulants that you are more likely to encounter are caffeine, nicotine, or pseudophedrine (Sudafed).",null,0,cdp5oi8,1rn4ev,askscience,top_week,2
kooksies,"Basically you use the distance the band travelled down the gel (Rm), to estimate the size of the DNA fragments in base pairs (bp) by comparing it with known standards; a marker ladder.  

You can then use the fragment sizes from each lane to determine what restriction enzymes (RE) were used by comparing the Rm values with a table of known values. 

When using agarose gel electrophoresis with RE analysis, the bands always represent size because RE linearise circular DNA. Unless the RE had no activity for it.   
However, you can use electrophoresis to determine the species of the DNA too, i.e. open circular, linear, covalently closed circular etc..   

So you should be aware that plasmid DNA of the same size can move at different speeds down the gel depending on what species it is and what buffer is being used. ",null,0,cdp3zfk,1rn4h9,askscience,top_week,4
Pombologist,"While your gel will show you the sizes of the DNA fragments in your digest, what is more important in this context is the information you can deduce from your results.

What were you trying to determine when you did these digests? For example, were you looking for the presented of a cloned piece of DNA that had been ligated into the vector? How do your results give you this information?",null,0,cdpah1v,1rn4h9,askscience,top_week,2
cladocerans,"Yes, basically. What you are doing is sorting pieces of DNA by weight.  Since DNA is a sequence of bases, sorting by weight means you get longer pieces at the bottom and shorter sequences at the top. You are pulling DNA through gel--longer pieces are going to move more slowly and end up at the top (where you put the product in), while short sequences can move more quickly.

Also, intensity of the bands indicates roughly how much of that size you have. (more intense = more DNA). This is important in PCR, when you are aiming to replicate many copies of one particular part of the genome.",null,1,cdp20mx,1rn4h9,askscience,top_week,1
McMillan_Astro,"Images [like this one](http://www.spitzer.caltech.edu/images/1925-ssc2008-10b-A-Roadmap-to-the-Milky-Way-Annotated-) are built from a mixture of things.

First and foremost they are drawn under the assumption that the Milky Way is a lot like other nearby galaxies, so how they look is a useful guide to what the Milky Way would look like from outside.

Then different known parts of the Milky Way are put in. We know there's a [fat bulge-like bar component of our Galaxy](http://adsabs.harvard.edu/abs/1998ApJ...492..495F), and we know its rough size and alignment so that gets put in. We know that there's a [longer, thinner bar](http://adsabs.harvard.edu/abs/2007A%26A...465..825C) at a similar angle as well, so that gets put in.

We know that the bulk of the stars are in a disc, and we know that it has spiral arms. We have some information about where those are from [a variety of sources](http://adsabs.harvard.edu/abs/2008AJ....135.1301V). Measurements of gas velocities give us indirect information, as does observed Milky Way star counts at infrared wavelengths (IR is less blocked by dust) as we look at different lines of sight through the Galaxy. The most exciting source of information is from objects known as [maser sources](http://www3.mpifr-bonn.mpg.de/staff/abrunthaler/BeSSeL/index.shtml) which can be viewed to such high accuracy by radio telescopes that we can determine the distance to them very accurately from their parallaxes. These masers are expected to lie mostly within spiral arms.

Putting all of this information together can give the artist information about where the spiral arms might lie. This information is mostly about the spiral arms on the Sun's side of the Milky Way - on the far side a lot of extrapolation is required.

The details of what these spiral arms then look like (as opposed to where they are) is almost entirely artistic license, guided by observations of other galaxies to give them an idea what to expect.",null,55,cdozjrd,1rn5lm,askscience,top_week,343
Samply,"This was my the first question on Reddit. Great answers and nice discussion -thanks.However more pressure on accuracy part: What are uncertainties on scales or distances? +-light years?-""We have plenty to do on this planet too, but we really should work more on these star maps.""",null,5,cdp51sl,1rn5lm,askscience,top_week,19
xenophonf,"If I recall correctly, astronomers measured the redshift of the hydrogen line in the (radio) spectrum of stars in the Milky Way.  They even figured out that we live in a barred spiral.  If I weren't so full of Thanksgiving turkey (tryptophan poisoning FTW!), I'd chase down the citations for you, but you should be able to find them yourself on Wikipedia.  Start with the articles on the Milky Way and the hydrogen emissions spectrum.",null,0,cdpax2v,1rn5lm,askscience,top_week,3
null,null,null,0,cdp34t3,1rn5lm,askscience,top_week,1
null,null,null,11,cdoxv4q,1rn5lm,askscience,top_week,5
McMillan_Astro,"Images [like this one](http://www.spitzer.caltech.edu/images/1925-ssc2008-10b-A-Roadmap-to-the-Milky-Way-Annotated-) are built from a mixture of things.

First and foremost they are drawn under the assumption that the Milky Way is a lot like other nearby galaxies, so how they look is a useful guide to what the Milky Way would look like from outside.

Then different known parts of the Milky Way are put in. We know there's a [fat bulge-like bar component of our Galaxy](http://adsabs.harvard.edu/abs/1998ApJ...492..495F), and we know its rough size and alignment so that gets put in. We know that there's a [longer, thinner bar](http://adsabs.harvard.edu/abs/2007A%26A...465..825C) at a similar angle as well, so that gets put in.

We know that the bulk of the stars are in a disc, and we know that it has spiral arms. We have some information about where those are from [a variety of sources](http://adsabs.harvard.edu/abs/2008AJ....135.1301V). Measurements of gas velocities give us indirect information, as does observed Milky Way star counts at infrared wavelengths (IR is less blocked by dust) as we look at different lines of sight through the Galaxy. The most exciting source of information is from objects known as [maser sources](http://www3.mpifr-bonn.mpg.de/staff/abrunthaler/BeSSeL/index.shtml) which can be viewed to such high accuracy by radio telescopes that we can determine the distance to them very accurately from their parallaxes. These masers are expected to lie mostly within spiral arms.

Putting all of this information together can give the artist information about where the spiral arms might lie. This information is mostly about the spiral arms on the Sun's side of the Milky Way - on the far side a lot of extrapolation is required.

The details of what these spiral arms then look like (as opposed to where they are) is almost entirely artistic license, guided by observations of other galaxies to give them an idea what to expect.",null,55,cdozjrd,1rn5lm,askscience,top_week,343
Samply,"This was my the first question on Reddit. Great answers and nice discussion -thanks.However more pressure on accuracy part: What are uncertainties on scales or distances? +-light years?-""We have plenty to do on this planet too, but we really should work more on these star maps.""",null,5,cdp51sl,1rn5lm,askscience,top_week,19
xenophonf,"If I recall correctly, astronomers measured the redshift of the hydrogen line in the (radio) spectrum of stars in the Milky Way.  They even figured out that we live in a barred spiral.  If I weren't so full of Thanksgiving turkey (tryptophan poisoning FTW!), I'd chase down the citations for you, but you should be able to find them yourself on Wikipedia.  Start with the articles on the Milky Way and the hydrogen emissions spectrum.",null,0,cdpax2v,1rn5lm,askscience,top_week,3
null,null,null,0,cdp34t3,1rn5lm,askscience,top_week,1
null,null,null,11,cdoxv4q,1rn5lm,askscience,top_week,5
rohrspatz,"In broad terms, heart performance is controlled by contrasting/opposing input from the sympathetic (""fight or flight"") and parasympathetic (""rest and digest"") nerves. Sympathetic input increases the heart rate and makes the heart muscles contract more strongly, while parasympathetic input decreases the heart rate. If sympathetic signaling is too strong/intense, or if it's even moderately strong in someone with certain kinds of heart disease, it can cause the heart muscles to work so hard that they consume oxygen faster than it can be replenished through the coronary arteries.

Normally, this effect is observed due to physical activity - exercise increases sympathetic stimulation of the heart. But from what I can see about mental stress ischemia, the effect is pretty much exactly analogous to physical stress ischemia. Strong emotions also tend to increase the activity of the sympathetic nervous system, and if the person has some kind of predisposition, apparently mental stress ischemia is the result.

However, there really are just these two ends of the scale. The neurons of the brain interact in much more complex ways to differentiate dozens of emotions, but the signals that get sent into the rest of the body ""feed into"" relatively few pathways - the nerves that control various parts of the body typically only release one of a few activating or inhibitory signals that have specific physiological effects. Ever noticed that ""happy"" anticipation and ""fearful"" anticipation have a lot of the same physical signs, even though you can clearly tell the difference between the two emotions? That's why.

I seriously doubt that different emotions have different effects on heart function, except possibly to the extent that certain emotions could activate sympathetic signaling more strongly or weakly than others. But I'd also bet that that effect is hugely variable among different people, and given our current understanding of the precise neurological signaling mechanisms that result in precise emotional states (i.e. basically none at all), it's probably not a good topic to study.",null,0,cdp3ng7,1rn5n6,askscience,top_week,2
Dr_JA,"It depends. There are some plants that you can grow under 24h light condition and they'll just be fine, whereas others will get stressed and not so happy. In the polar circle there are enough plant species growing, and they seem to be doing fine with 24h periods of light during the summers there - they will most certainly be photosynthesizing all day, although probably not at a constant rate.

In principle, there's nothing against perpetual photosynthesis - the enzymes that do this whole process will need to be turned-over obviously, but that is normally an ongoing process.

The only thing that is probably an influence in this, is the ability to deal with reactive oxygen species, or ROS. ROS are generated in the photosynthesis process, and need to be cleaned-up by certain enzymes. I guess this normally happens in the dark, so these ROS can be cleaned-up. Continues photosynthesis will probably require that ROS scavenging is increased, which plants should have the plasticity for, depending a bit on the species.
Hope this made things a little clear.",null,0,cdpa4az,1rn6m6,askscience,top_week,3
MarineLife42,"Plants do need, and take, rest.  
Depending on species and location, plants will only photosynthesize between 8 and 14 hours every day, independent on daylight. Outside that time, their metabolism will switch from photosynthesis, producing sugar and oxygen, to breathing just like animals, i.e. using sugar and producing CO2.  
Only the small amount of sugar that has been permanently built into the plant structure represents a net withdrawal of CO2 from the atmosphere and a net gain of Oxygen. ",null,1,cdpxiys,1rn6m6,askscience,top_week,1
naijizaknayr,"They probably mean dont do things like take it from the oven and put it in the sink with water.  The dish will heat up slowly in the oven because air is a poor conductor of heat...but if you take it from the oven and run water over it, water is a much better conductor so the dish will cool rapidly and could crack.",null,4,cdp426u,1rn7aa,askscience,top_week,28
Apollo_Felix,"Pyrex today is made of tempered glass. Pyrex used to be a low thermal expansion borosilicate glass, according to Wikipedia. Now glass subjected to a fast change in temperature breaks because glass is a thermal insulator. If you cool a surface of the glass quickly, you induce a large variation of temperature along the inside of the glass, which for a dish can be pretty large. The cool side will shrink, but the hot side won't. This will cause stress inside the glass dish, which can be enough to break the glass. Since original Pyrex had a low thermal expansion, it would induce lower stresses and not break. Tempered glass is a resistant glass that is already stressed in a way to make it more resistant. It also breaks into very small pieces when shattered. However it still expands or contracts enough to break when subjected to sudden large temperature changes.",null,0,cdp7hco,1rn7aa,askscience,top_week,8
PostGradWarrior,"In general amorphous materials such as glass don't deal well with rapid temperature changes. They are insulators, so they don't transfer heat very well. If the surface is suddenly cooled by water, the outer surface will want to contract and the inner surface will want to stay the same. The lack of a crystalline structure (Think of it like a solid trapped as a liquid), makes it very hard to resist shear stress. A contraction of the top surface due to a temperature difference will create a shear stress and this can cause it to crack.",null,0,cdpfxjh,1rn7aa,askscience,top_week,3
FieryPianos,"As was mentioned before, Pyrex is currently made of tempered glass. Tempered glass is manufactured in high temperatures, in which the glass suffers compression forces on its surface and traction forces in its interior. It is then cooled very quickly, so as to mantain those forces and also keep its glass transition temperature (the temperature above which a glass starts behaving a lot like a liquid) very high. The result of these forces is that superficial fissures on tempered glass rarely ever spread and cause it to break, making it incredibly resistant. Before the change to tempered glass, Pyrex was made of borossilicate glass. The reason for the resistance of that material is that the boric oxyde that is added occupies empty spaces in the glass and helps keep the silicate, aluminum oxyde and sodium oxyde tight together. It is also created at very high temperatures, again to keep its glass transition temperature high.",null,0,cdpitsv,1rn7aa,askscience,top_week,2
null,null,null,0,cdpygwf,1rn7aa,askscience,top_week,2
null,null,null,3,cdp1twh,1rn7aa,askscience,top_week,1
naijizaknayr,"They probably mean dont do things like take it from the oven and put it in the sink with water.  The dish will heat up slowly in the oven because air is a poor conductor of heat...but if you take it from the oven and run water over it, water is a much better conductor so the dish will cool rapidly and could crack.",null,4,cdp426u,1rn7aa,askscience,top_week,28
Apollo_Felix,"Pyrex today is made of tempered glass. Pyrex used to be a low thermal expansion borosilicate glass, according to Wikipedia. Now glass subjected to a fast change in temperature breaks because glass is a thermal insulator. If you cool a surface of the glass quickly, you induce a large variation of temperature along the inside of the glass, which for a dish can be pretty large. The cool side will shrink, but the hot side won't. This will cause stress inside the glass dish, which can be enough to break the glass. Since original Pyrex had a low thermal expansion, it would induce lower stresses and not break. Tempered glass is a resistant glass that is already stressed in a way to make it more resistant. It also breaks into very small pieces when shattered. However it still expands or contracts enough to break when subjected to sudden large temperature changes.",null,0,cdp7hco,1rn7aa,askscience,top_week,8
PostGradWarrior,"In general amorphous materials such as glass don't deal well with rapid temperature changes. They are insulators, so they don't transfer heat very well. If the surface is suddenly cooled by water, the outer surface will want to contract and the inner surface will want to stay the same. The lack of a crystalline structure (Think of it like a solid trapped as a liquid), makes it very hard to resist shear stress. A contraction of the top surface due to a temperature difference will create a shear stress and this can cause it to crack.",null,0,cdpfxjh,1rn7aa,askscience,top_week,3
FieryPianos,"As was mentioned before, Pyrex is currently made of tempered glass. Tempered glass is manufactured in high temperatures, in which the glass suffers compression forces on its surface and traction forces in its interior. It is then cooled very quickly, so as to mantain those forces and also keep its glass transition temperature (the temperature above which a glass starts behaving a lot like a liquid) very high. The result of these forces is that superficial fissures on tempered glass rarely ever spread and cause it to break, making it incredibly resistant. Before the change to tempered glass, Pyrex was made of borossilicate glass. The reason for the resistance of that material is that the boric oxyde that is added occupies empty spaces in the glass and helps keep the silicate, aluminum oxyde and sodium oxyde tight together. It is also created at very high temperatures, again to keep its glass transition temperature high.",null,0,cdpitsv,1rn7aa,askscience,top_week,2
null,null,null,0,cdpygwf,1rn7aa,askscience,top_week,2
null,null,null,3,cdp1twh,1rn7aa,askscience,top_week,1
SheepShepherd01,"The mass of an animal is a function of the mean density of its tissues times its volume. When increasing an animals size, its weight will grow as a cubic function. The weight comes into play when determining the momentum of the animal just before it hits the ground - a heavy animal dropped from the same height as a smaller animal will, while reaching the same velocity as the lightweight animal, have a much larger momentum. However, the force of the impact is just applied to the area of the animals bottom side, which will behave like a quadratic function when going from small to big animals.

So, the larger an animal gets, the the higher gets the ratio weight/area of its bottom side. In a larger animal, more energy is applied to a relativly smaller area when it falls to the ground, so it will get hurt more than a smaller animal.",null,0,cdp3gqu,1rn7k4,askscience,top_week,5
Platypuskeeper,"I think 'all the above' would work as an answer here. 

DNA bases (deoxyribonucleotides) are synthesized (by [Ribonucleotide reductase](http://en.wikipedia.org/wiki/Ribonucleotide_reductase)) from ribonucleotides, those used as bases in RNA. Which is one of the reasons the dominant theory is that [RNA evolved first](http://en.wikipedia.org/wiki/RNA_world_hypothesis). 

Unlike DNA, RNA is single-stranded. But it can still form a double helix, if part of the strand has a set of complimentary bases, it can loop back and form a helix with itself, which it does in [tRNA](http://en.wikipedia.org/wiki/Transfer_RNA) for example. I would say (or speculate) that this is mainly for structural and chemical stability there. But note that RNA that carry genetic information (mRNA) don't usually (or at least not necessarily) form helices as far as I know. 

Why bother with DNA in the first place? RNA can already carry genes (and e.g. [some viruses](http://en.wikipedia.org/wiki/RNA_virus) only have RNA), so it might seem redundant. It also costs energy to produce DNA bases from RNA ones. The apparent answer to this would be that DNA is more stable (both chemically and against mutations), which is something that's gets increasingly important as the amount of genetic material increases. From that perspective, it also makes sense why we'd still retain RNA for [transcribing](http://en.wikipedia.org/wiki/Transcription_%28genetics%29) genes, since they're just short fragments of genetic material - there's no need for the extra stability, so it's not worth the cost.

So, given that DNA is double-stranded (I'll get to that), it's sort-of a consequence of it's structure that it would be able to form helices like RNA does when it has a complementary strand. (However, there's a bit of difference in that RNA only forms 'A-type' helices, while DNA is usually 'B-type'. There are at least 3 types of DNA helices - [A](http://en.wikipedia.org/wiki/Z-DNA), [B](http://en.wikipedia.org/wiki/Z-DNA) and [Z](http://en.wikipedia.org/wiki/Z-DNA), so in order to not get bogged down in detail, I'll just leave it at 'forming a helix' here)

Now, the complementary strand, which is a key to helix-forming here, is a big difference to RNA. It does provide stability both against mutations and chemical reactions by having the bases inwards, facing each other, shielding them a bit from the environment. But there's more than just that, the redundancy allows for '[proofreading](http://en.wikipedia.org/wiki/Proofreading_%28biology%29)' of the DNA, helping correct errors during replication.

So there are benefits to having a complementary strand, and having a helix likely helps with the aforementioned shielding of the bases. But in a way it's also side-effect of the structure. I don't think you could possibly 'flatten' it into a ladder-like structure without putting an implausible amount of strain on the chemical bonds. 

",null,1,cdp883i,1rn8mb,askscience,top_week,5
darkgrenchler,"A couple reasons: Size, chemical protection, and readability by proteins.

[DNA is actually coiled multiple times](http://i.imgur.com/sOjvbBS.png). this supercoiling allows for space conservation inside cells. A fully uncoiled human genome in a cell would be a meter or two in length. This isn't directly associated with the double-helix nature, but helps nonetheless.

the double helix nature of DNA helps protect DNA from being damaged, while still being able to be unzipped by proteins who *need* to access the DNA for some reason. Each (single) strand is built by covalent bonds, but the two strands in a helix use hydrogen bonds to keep eachother attached. H-bonds are weak alone, but a billion H-bonds acting at once are strong enough to keep a DNA strand intact. The twisting nature of the double helix also puts the hydrogen bonds on the ""inside"" of the helix, increasing its protection.

[The spacing in the coils](http://i.imgur.com/Yckvfgp.jpg) are only a few angstroms in width, so there aren't many things that could get in there and mess around.",null,0,cdp87ur,1rn8mb,askscience,top_week,1
andrewjkwhite,"Since there has been no answer posted.
As a layman if I had to guess, residue on the window. Outside of the wiper streak probably had wax or some other residue that was a poor surface for crystallization but inside the streak that will have been worn off and the crystals could form directly on the glass. 

I know I'm breaking the rules and I would love to know the real answer.",null,0,cdp2ocl,1rn8r3,askscience,top_week,4
princetonwu,"Pathologically speaking, the cells of the urinary tract epithelium (ie the lining of the bladder walls) do not contain the same properties as the cells lining the colon. Secondly, the purpose of the bladder is to hold the metabolic wastes after urine is filtered from the kidney; so it would serve no purpose to additionally reabsorb more water when the kidney is already efficient enough to reabsorp water. Thirdly, if a person is severely dehydrated, the body acts to minimize fluid loss through the reabsorption mechanisms of the kidney (and not the bladder). ",null,1,cdp34qm,1rn9nv,askscience,top_week,8
Sterlz,"The melting point of a material is independent of how the material is shaped. The only thing that effects the melting point is pressure. The minute change in crystalline structure from machining might have a negligible effect, but nothing close to a noticeable change.",null,0,cdp5zdd,1rncb0,askscience,top_week,2
columbium,"Yes.  If you have a material which splits into multiple phases upon cooling, the rate of cooling can dictate how much of a given element diffuses into each phase.  The resulting variation in compositions will change the melting point of each phase.

Also, the cooling rate of a polymer will affect its glass transition temperature.  Cool faster and the glass transition temperature increases.  This isn't a melting point, but for practical use, you probably wouldn't want to use a polymer above its glassy transition temperature for structural applications. ",null,0,cdpnksz,1rncb0,askscience,top_week,1
NHsniper5689,"The answer is simple. It does not make the water boil faster, actually it does just the opposite... the boiling temperature of the water is raised so that the pasta (or whatever you're cooking) cooks faster due to the water temperature being higher). Sorry for the not so sciency answer.",null,1,cdp1lv6,1rncfh,askscience,top_week,5
TheNextDoctorWho,"Took me some time to find the english word, but the term to look up is the [boiling-point elevation](http://en.wikipedia.org/wiki/Boiling_point_elevation) (In german, it's ""molale Siedepunktserhöhung"".

This property is inherent to ANY solution with solved particles in it. The nature of the particles is irrelevant, only the number counts - hence the term ""molale"" in german, because it is dependet on the molality of any solute.

A liquid is boiling if its vapor pressure is equal to the surrounding atmospheric pressure. If you have a couple of non-solvent molecules in the solution, the concentration of the solvent molecules gets diluted, therefore the vapor pressure of the solvent is lower. That in turn means that you have to bring the solution to a higher temperature than usual, so that the 'lower' vapor pressure can catch up to the atmospheric pressure. 



",null,0,cdp5n05,1rncfh,askscience,top_week,2
Alexis_deTokeville,"By adding salt to the water, you increase its entropy by diluting the water particles and therefore decrease the *change* in entropy needed to change phases from a liquid to a gas. Smaller change in entropy means less heat/temperature is required. I believe the entropy effect also corresponds to a lower vapor pressure.",null,3,cdpbgu1,1rncfh,askscience,top_week,1
SingleMonad,"&gt;..., so if they are experiencing acceleration ...

Yes, *if*.  They are not.  That's the reason they do not radiate.

More broadly, classical radiation such as the kind generate by an accelerating charge is proportional to the square of the acceleration.  The expectation value of the electron's position is constant (this is a feature of all energy eigenstates, and the reason they are also called *stationary states*).  Related to this is the fact that an atom in its ground state has no classical dipole moment, and no way to generate one in a transition to a lower state of energy (because no lower state exists).  

It seems odd, since there is generally a nonzero probability to find the electron throughout the atom.  But no classical acceleration is at work, i.e., the second derivative of the expectation value of position is zero.


You might wonder what's different about an atom in (say, just for concreteness) its *first excited state*.  You know that decays, and emits radiation.  What is different about it?  The answer is subtle (and beautiful):  the atom in its excited state is **not** in an eigenstate of the coulomb Hamiltonian (from the nucleus).  It's in an eigenstate of the *total* Hamiltonian of the system.  The total Hamiltonian includes the electromagnetic field.  The true energy eigenstates are *superpositions* of atomic states.  That excited state I mentioned is really a super position of (mostly) the excited state plus a little bit of the ground state.  This superposition state does possess a dipole moment, and hence radiates when the atom falls down to the ground state.

Your question is what kept the pressure on to find a better atomic theory than the Bohr model.

Edit: it's-its stuff",null,0,cdp5h3u,1rng96,askscience,top_week,6
serweet,"The best way to think of this in classical terms is in electron shells, which are a set of allowed state for an electron to be in. These shells are based on intrinsic quantum numbers of the electrons (such as angular momentum and spin), with each further shell increasing in energy. Due to the Pauli exclusion principle, only one electron may occupy each state.

Now, to get onto your question, if an electron were to radiate energy, then it must be an exact quantity to move it from a higher shell to a lower, but if the electrons are all in the ground state (the state of lowest energy), then they cannot physically move to a lower state, thus radiating photons. Electrons tend to sit in their ground state configuration, but do absorb photons of exact energies to move up into higher states, and then radiate photons as they drop down the shells.",null,2,cdp2vfv,1rng96,askscience,top_week,4
mc2222,"&gt;They are charged particles, so if they are experiencing acceleration they should

Well, they do.  When an electron looses energy, it emits light as an EM wave (or photon depending on your philosophical bent).  

&gt;I understand that in quantum physics, due to Schroedinger equation, electrons are static fields of probability but that seems to only take into account their wave properies, completely neglecting the fact that they are also particles.

It's not that the *electron* is a static field of probability, it's that the electron's *location* can be described in terms of the probability of where *you will find it*.  It's the electron's *location* that is described in terms of probability.

",null,2,cdp2y5t,1rng96,askscience,top_week,3
RetraRoyale,"There doesn't need to be a classical explanation because classical physics doesn't apply at that scale. It's only true at larger scales as an approximation.

&gt;I understand that in quantum physics, due to Schroedinger equation, electrons are static fields of probability around the nucleus and so are their charges, but that seems to only take into account their wave properies, completely neglecting the fact that they are also particles.

~~First, electrons are not 'static' fields of probability, they're quite dynamic. In fact, the quantum mechanical notion of energy typically manifests a sort of complex cycling-of-the-wave effect, much like a vibrating string.~~ (This is probably not a useful thing to say.)

Second, electrons don't have 'wave properties' and 'particle properties', they *only* have wave properties (or tautologically, 'electron properties'.) To call it a particle means that the EM field has quantized, localized features, not that these features are *actually* a single indivisible entity.
",null,3,cdp2w8d,1rng96,askscience,top_week,3
RelativisticMechanic,"[TL;DR]

It depends on just what you mean, but the usual definition of black hole temperature comes from Hawking radiation, in which case if we express our mass in multiples of solar masses (so the sun would have M = 1, a star ten times as massive would have M = 10), the temperature is

T = (0.0000000617/M) Kelvin.

So, for example, a black hole the mass of the sun would have a ""temperature"" of approximately 0.0000000617 Kelvin. One that was ten times as massive would have one tenth the temperature.

[Comment Proper]

When dealing with curved spacetimes, you have to be careful with the definition of things like ""temperature"". When dealing with black holes in particular, you also have to be careful about how you interpret questions that you expect naïvely to depend on the behavior of stuff inside the event horizon.

One of the difficulties here is that we can't speak to the microscopic behavior of matter inside of the black hole. We do not have a consistent model for describing quantum systems in such spacetime regions, and any proper microscopic description is going to require a quantum theory. Thus, we can't really talk about temperature in the sense of ""motion of particles"".

However, we *can* apply quantum field theory in a blackhole spacetime provided, roughly, that we stay away from the singularity and our field doesn't carry too much energy. In particular, if we're interested in describing events outside of the event horizon, we can use these techniques as a good first approximation. We can also apply quantum statistical mechanics/thermodynamics to this process. When you do this, you find that if an observer falling across the event horizon sees themselves in a vacuum, an observer far from the black hole must see a flow of thermal radiation coming from the black hole. This radiation has the same spectrum as a blackbody source of some definite temperature, so we can associate it with a temperature. It turns out that, for a black hole of mass M, the temperature T associated with this radiation is

T = (ħc^(3) / [8πGk_*B*_])/M,

where ħ is the reduced Planck's constant, c is the speed of light, G is Newton's gravitational constant, and k_*B*_ is Boltzmann's constant. That factor is just a constant that we can evaluate, and it comes out to roughly

T = 1.227\*10^(23) kg K / M = (0.0000000617/M) Kelvin,

where the second form comes about by expressing M as a multiple of the solar mass.

",null,18,cdp2hll,1rngpn,askscience,top_week,157
dumb_,"Simplified answer:

*The temperature of a black hole is determined by the 'black body radiation temperature' of the radiation which comes from it. (e.g., If something is hot enough to give off bright blue light, it is hotter than something that is merely a dim red hot.)*

*For black holes the mass of our Sun, the radiation coming from it is so weak and so cool that the temperature is only one ten-millionth of a degree above absolute zero. This is colder than scientists could make things on Earth up until just a few years ago (and the invention of a way to get things that cold won the Nobel prize this year). Some black holes are thought to weigh a billion times as much as the Sun, and they would be a billion times colder, far colder than what scientists have achieved on Earth.*

*However, even though these things are very cold, they can be surrounded by extremely hot material. As they pull gas and stars down into their gravity wells, the material rubs against itself at a good fraction of the speed of light. This heats it up to hundreds of millions of degrees. The radiation from this hot, infalling material is what high-energy astronomers study.* 

[^Via: ^NASA's ^Ask ^An ^Astrophysicist](http://imagine.gsfc.nasa.gov/docs/ask_astro/answers/971111e.html)",null,1,cdp3rrp,1rngpn,askscience,top_week,25
RelativisticMechanic,"[TL;DR]

It depends on just what you mean, but the usual definition of black hole temperature comes from Hawking radiation, in which case if we express our mass in multiples of solar masses (so the sun would have M = 1, a star ten times as massive would have M = 10), the temperature is

T = (0.0000000617/M) Kelvin.

So, for example, a black hole the mass of the sun would have a ""temperature"" of approximately 0.0000000617 Kelvin. One that was ten times as massive would have one tenth the temperature.

[Comment Proper]

When dealing with curved spacetimes, you have to be careful with the definition of things like ""temperature"". When dealing with black holes in particular, you also have to be careful about how you interpret questions that you expect naïvely to depend on the behavior of stuff inside the event horizon.

One of the difficulties here is that we can't speak to the microscopic behavior of matter inside of the black hole. We do not have a consistent model for describing quantum systems in such spacetime regions, and any proper microscopic description is going to require a quantum theory. Thus, we can't really talk about temperature in the sense of ""motion of particles"".

However, we *can* apply quantum field theory in a blackhole spacetime provided, roughly, that we stay away from the singularity and our field doesn't carry too much energy. In particular, if we're interested in describing events outside of the event horizon, we can use these techniques as a good first approximation. We can also apply quantum statistical mechanics/thermodynamics to this process. When you do this, you find that if an observer falling across the event horizon sees themselves in a vacuum, an observer far from the black hole must see a flow of thermal radiation coming from the black hole. This radiation has the same spectrum as a blackbody source of some definite temperature, so we can associate it with a temperature. It turns out that, for a black hole of mass M, the temperature T associated with this radiation is

T = (ħc^(3) / [8πGk_*B*_])/M,

where ħ is the reduced Planck's constant, c is the speed of light, G is Newton's gravitational constant, and k_*B*_ is Boltzmann's constant. That factor is just a constant that we can evaluate, and it comes out to roughly

T = 1.227\*10^(23) kg K / M = (0.0000000617/M) Kelvin,

where the second form comes about by expressing M as a multiple of the solar mass.

",null,18,cdp2hll,1rngpn,askscience,top_week,157
dumb_,"Simplified answer:

*The temperature of a black hole is determined by the 'black body radiation temperature' of the radiation which comes from it. (e.g., If something is hot enough to give off bright blue light, it is hotter than something that is merely a dim red hot.)*

*For black holes the mass of our Sun, the radiation coming from it is so weak and so cool that the temperature is only one ten-millionth of a degree above absolute zero. This is colder than scientists could make things on Earth up until just a few years ago (and the invention of a way to get things that cold won the Nobel prize this year). Some black holes are thought to weigh a billion times as much as the Sun, and they would be a billion times colder, far colder than what scientists have achieved on Earth.*

*However, even though these things are very cold, they can be surrounded by extremely hot material. As they pull gas and stars down into their gravity wells, the material rubs against itself at a good fraction of the speed of light. This heats it up to hundreds of millions of degrees. The radiation from this hot, infalling material is what high-energy astronomers study.* 

[^Via: ^NASA's ^Ask ^An ^Astrophysicist](http://imagine.gsfc.nasa.gov/docs/ask_astro/answers/971111e.html)",null,1,cdp3rrp,1rngpn,askscience,top_week,25
cladocerans,"Yes, this is a real problem if you are not protecting the soil from erosion and also doing nothing to build up topsoil (adding compost, leaving standing crop, etc). The US is losing approximately 6.9 billion tons of topsoil every year: http://anrcatalog.ucdavis.edu/pdf/8196.pdf",null,1,cdp2608,1rnhwj,askscience,top_week,1
McMillan_Astro,"If Hubble's constant (H0) were actually constant that would be correct - the relative velocity of two objects a distance s apart is v = H0 * s, so using some maths (apologies if you don't know calculus):

v = ds/dt, so ds/dt = H0 * s. This is a differential equation with solution s = const * exp(H0 * t).

That means that the universe would be exponentially expanding, as you suspected, and that never goes to zero size however far back in time you go.

**However**, Hubble's ""constant"" changes with time! We believe it is constant in space (it is the same whichever direction we look), but it changes over time. The evolution of Hubble's ""parameter"" (which has the value known as Hubble's constant today) is governed by the [Friedmann equations](http://en.wikipedia.org/wiki/Friedmann_equations).

Broadly speaking these mean that when the universe is dominated by matter, so it's density and pressure decreases over time, then the Hubble parameter decreases over time - this has been the case for most of the history of the universe. In this scenario we can get back to everything in the universe being at the same point long ago (the big bang).
",null,2,cdp3c3z,1rni8w,askscience,top_week,15
Dendrimer14,"For every million parsecs of distance from the observer, the rate of expansion increases by about 74 kilometers per second.  This is equivalent to ~2.4 femtometers/sec/meter (if the expansion rate is linear over distance)   2 meters would expand twice as fast as 1 meter.  1 meter would take about 26.4 million years to double in size.
",null,0,cdp43kg,1rni8w,askscience,top_week,7
McMillan_Astro,"If Hubble's constant (H0) were actually constant that would be correct - the relative velocity of two objects a distance s apart is v = H0 * s, so using some maths (apologies if you don't know calculus):

v = ds/dt, so ds/dt = H0 * s. This is a differential equation with solution s = const * exp(H0 * t).

That means that the universe would be exponentially expanding, as you suspected, and that never goes to zero size however far back in time you go.

**However**, Hubble's ""constant"" changes with time! We believe it is constant in space (it is the same whichever direction we look), but it changes over time. The evolution of Hubble's ""parameter"" (which has the value known as Hubble's constant today) is governed by the [Friedmann equations](http://en.wikipedia.org/wiki/Friedmann_equations).

Broadly speaking these mean that when the universe is dominated by matter, so it's density and pressure decreases over time, then the Hubble parameter decreases over time - this has been the case for most of the history of the universe. In this scenario we can get back to everything in the universe being at the same point long ago (the big bang).
",null,2,cdp3c3z,1rni8w,askscience,top_week,15
Dendrimer14,"For every million parsecs of distance from the observer, the rate of expansion increases by about 74 kilometers per second.  This is equivalent to ~2.4 femtometers/sec/meter (if the expansion rate is linear over distance)   2 meters would expand twice as fast as 1 meter.  1 meter would take about 26.4 million years to double in size.
",null,0,cdp43kg,1rni8w,askscience,top_week,7
nohoxe,"Let's think of the factors that effect tire ware. Heat will effect how soft the tire is and how quickly it wares. Rain would cool the tire making it much less elastic and more durable. Though it might be that the vulcanized rubber on the tire wouldn't change it's elasticity much at even the highest temperatures that tires are subject to on the rode. 
",null,0,cdp1yqh,1rnkz1,askscience,top_week,2
elsjaako,"You need to define what ""acting like a sphere"" means.

A coin can only flip over if it bounces, a six sided die already sort of rolls over a hard surface, a twenty sided dice rolls (I don't feel like what they do needs a qualifier). A 100 sided die moves almost like a smooth ball, but it will land on one of 100 discrete points.",null,127,cdp5a68,1rnn07,askscience,top_week,721
thepsyborg,"Your question presumes that we can answer a binary question (""Does it act like a sphere?"" Yes/no) about a given shape. Without specifying an arbitrary threshold for spheroid behavior, the most we can say is that behavior approaches that of a sphere as shape approaches that of a sphere.

The largest number of sides a single brand-new die could have while still rolling fairly is limited only by the precision &amp; accuracy of its manufacturing process. The upper limit on number of facets for a _usable_ fair-rolling die would also rely on the resistance of its material to wear and deformation, if we expect it to remain essentially fair for a reasonable number of rolls.",null,30,cdp7es9,1rnn07,askscience,top_week,164
dracho,"The answer is obvious.  None.  

If an object has 1,000,000,000,000,000,000 sides, it still doesn't act like a sphere.  It's acts *more* like a sphere than a cube, depending on what metrics you're measuring, but it's still not a sphere.

The only thing that ""acts like a sphere"" is a sphere.

There is some discussion about molecules, and how in a real-world scenario, a sphere made of ordinary matter isn't truly a sphere... I'm ignoring this and focusing on the pure math.",null,30,cdp6v7x,1rnn07,askscience,top_week,108
sperho,"OP's post reminds me of the [MythBuster's segment](http://dsc.discovery.com/tv-shows/mythbusters/videos/square-wheels-angle-2.htm) on putting square wheels on a truck and seeing if driven at high speeds they would ""act like a"" round wheel and produce a smooth ride. spoiler alert:  not really.",null,6,cdp1ngu,1rnn07,askscience,top_week,37
null,null,null,7,cdp6586,1rnn07,askscience,top_week,30
Hokararu,"To clarify what I mean by 'act like a sphere' I am referring to the point at which an object will maintain its current position when the angle of the surface it rests on is adjusted.

For example, if I roll a die onto a table and then lift that table off the ground at one end, the die may slide down the table but is unlikely to change the side it rests on. A sphere however, would roll down the table making it useless as a die in this situation.

Math me...",null,11,cdp5j36,1rnn07,askscience,top_week,26
AdmiralTroll,"Not an expert on this, but my understanding is that everything, aside from 'the perfect sphere' holds a number of angles, even if it is down to an molecular level. Therefore every smooth rubber ball has a finite number of sides and so technically is a die in its own right?

On another note: surely then a perfect sphere that had no angled side to land on would; on a completely flat smooth surface, continue rolling without needing more force applied?",null,3,cdp6q8p,1rnn07,askscience,top_week,13
John_Duh,"If you only consider a Dice one of the Platonic Solids you can only go as high as 20 sided. They are the only polyhedron that has equal sized sides, so the question is not really possible to answer.

You could though take a sphere and cut of parts of it to create surfaces but it would not be considered a Platonic Solid.",null,3,cdp7li9,1rnn07,askscience,top_week,12
Pitboyx,"Since you probably won't be rolling on a perfectly clean 100% smooth surface, the die would be considered uselessly inaccurate between 8-11 thousand sides (estimated through blender icosphere) because at that point, the die will be tipped on an edge or corner instead of a face by particles of dust, grooves, and bumps in the surface as if you were spinning a regular 6-sided die on a corner.

From an even more practical viewpoint, it would be near impossible to decide which face is facing upwards with the naked eye after 500-700 faces. 

A 80 sided icosphere would probably be the ideal size for having the most results per die. The more reasonable choice would be to simply use more dice.",null,7,cdpalpy,1rnn07,askscience,top_week,16
vortik,"As others have mentioned, the term ""act like a sphere"" is a bit vague.  So when I read this, I started wondering if there was a way that we could quantify this in terms of three-dimensional packing densities.

A cube can be arranged to fill a three-dimensional space completely.  But as you ""subdivide"" the sides, on your way toward turning it into a sphere, your maximum packing density decreases.  For example:

| Solid | Maximum known packing density |
| ------ | ------------------- |
| octahedra | 0.947003 |
| dodecahedra | 0.904002 |
| icosahedra | 0.836315 |

[Wiki reference](http://en.wikipedia.org/wiki/Packing_problem#Packings_of_Platonic_solids_in_three_dimensions)

As these objects become more sphere-like, the maximum known packing density is conjectured to approach that of the [Kepler conjecture](http://en.wikipedia.org/wiki/Kepler_conjecture) density of ~0.74 for best-possible sphere packing in 3d.  My understanding is that much of these ""best known"" values have been computed via computer simulation, and no rigorous proofs of this transition have been forthcoming.

If we assume that this trend is correct (frankly, I think it is, but I don't think it has been formally proven), then what I was thinking was that the OP may be able to quantify (to some extent) what ""close to a sphere"" means.  For example, he could take the given icosahedron considered in [this paper](http://www.nature.com/nature/journal/v460/n7257/full/nature08239.html), and say that it is ""0.83/0.74 ~= 12%"" away from being a ""sphere"", and this is what it looks like, and that is how well it packs.  

I do not know if the above would answer the OP's question.  A simpler answer would probably be:  ""a 20-sided die rolls pretty well"", but others have tried to tackle this.  I suspect that you'd have to consider the effects of friction between the sides of the die against the contact surface (compared to the infinite case for a sphere?), but that's out of my purview.


*Edit: Formatting.",null,0,cdp2oz3,1rnn07,askscience,top_week,7
null,null,null,1,cdpbqy3,1rnn07,askscience,top_week,6
Jackeddaniels69,"Hi Hokararu, if we assume that each side is a congruent regular polygon then you can actually find the spherical deviation (relatively how close it is to being a sphere) for a Polyhedra (Basically a symmetrical 3D Object)!
If we take a 3 sided object connected by equilateral triangles we get a tetrahedron. Use the formula δ(v) = 360 - (sum interior angles surrounding a single vertex) which will be, δ(v) = 360 - (180). 180 was derived by each of the three interior angles of the equilateral triangle 3(60). 

δ(v) = 180 = Spherical Deviation. The lower the number the ""more spherical"". 0 would be a sphere. Unfortunately you can only have a certain number of side that are regular and congruent fitting the description of a dice.

The largest number of sides to still be considered a 'dice' and be as close to 'spherical' as possible is 20 Sides.
http://en.wikipedia.org/wiki/Icosahedron 
Devils advocate though, these are a thing.
http://en.wikipedia.org/wiki/Zocchihedron

Edit&gt; Also a dices vertices connect edges all with the same interior angle. so Catalan or Semi-regular polyhedra would not count in my opinion. So it really depends on your perspective of what a dice really is.",null,0,cdp7hl7,1rnn07,askscience,top_week,5
OneTwoTriangle,"This is more a physical answer, than a mathematical one. I am assuming a dice in our reality.

When the surfaces of the sides are a couple of molecules big the dice will start to behave like a sphere. It is basically a ball. To calculate the amount of sides you calculate the surface of a sphere with (almost) the same diameter as your dice. Divide this number by the surface area of your dice side. This gives the amount of sides it can have.

~~A problem however is the regularity of your dice. A twenty sided dice is the biggest dice you can make on which all sides have equal probability. After that you need different kinds of shapes to create the dice. This follows by the fact that there are only five platonic solids.~~ 

EDIT: That last part is wrong see the comment by eli5--answer. 

",null,2,cdpe2r1,1rnn07,askscience,top_week,4
88888888333,"Technically, nothing is a perfect sphere. In fact, all perfect geometric shapes are only mathematical concepts. You cannot create a perfect sphere out of atoms. So the answer to your question is that what you perceive to ""act like a sphere"" is actually similar to a die with *many* sides, since there is no such thing as a perfect sphere. ",null,0,cdpezbt,1rnn07,askscience,top_week,3
gubasaurus,"To me (an engineer, not a mathematician), it's an issue of stability. Will the dice ever reach a position in which it is stable? When perturbed, will it want to go back to its original state? To 'behave like a sphere' the smallest angular perturbation will cause the sphere to move and keep moving. A dice with sides of equal surface areas that is perturbed by a small enough will go back to its original position. So my answer to your question would be that, to behave like a sphere, an object cannot have sides, and so cannot be a dice.",null,0,cdp7zu5,1rnn07,askscience,top_week,3
bangsecks,"I think OP is asking how many sides could be added to a die before it no longer would settle and rest on a discrete surface, not how many facets are possible to have on one.  This comes down in part to the volume and mass of the die, as the die gets larger and heavier, whatever inertia from the roll acting on its mass will pull it in one direction or another such that it doesn't find a resting position, at least in a reasonable amount of time continuing to move about like a ball does.  The greater the mass the fewer surfaces possible before it just rolls.

Before we get the volume and mass to certain quantities such that we can fit a lot on there and still have it behave as a die we'd probably run into the problem of not being able to clearly tell what surface is the value surface, or the side that's telling you what number you have both because it would be hard to tell just which is directly at the top when they're so small and because as there are more and more sides the numbers will have to get to digit strings so large they'd be difficult to fit on shrinking facets.",null,0,cdp8bd3,1rnn07,askscience,top_week,2
huggybear0132,"This is a 3D version of making a polygon into a circle. You could consider a circle to be a regular polygon with an infinite number of sides, and likewise a sphere a regular prism with an infinite number of sides. That is the theoretical side. Now physically, what you mean by ""act like"" is ambiguous. Do spheres not roll and then eventually rest on a ""side"" (the point tangent to the surface they are resting on)? In that sense, all dice act like a sphere already.  I am going to go ahead and say the number of sides n has to tend towards infinity.",null,0,cdpe3en,1rnn07,askscience,top_week,2
ImAVibration,"Here's a crazy idea, a perfectly smooth sphere has an infinite number of sides it can rest on. But if you increase its diameter, it will still have an infinite number of 'sides', but it will also have infinitely more than the smaller sphere.",null,0,cdpfg2x,1rnn07,askscience,top_week,2
bloonail,"Neutron stars be damned -0- the question is about dice and ""acting like a sphere"".

That question seems three parts:
1) what makes a dice act like a dice
2) when is a dice no longer acting like a dice
3) at what level do facets make an object non spherical.

A dice provides evenly random results based on the number of facets of the dice.

Dice no longer act like dice when the randomness is predicable or favors a group of facets. Six sided dice are well understood. They are hard to roll in a predictable way.. No one really trusts the ones with more sides. There are too many ways to mess with the spin and bowling-like parameters of the many sided dice.

However a regularly multifaceted object would likely be considered dice-like even if the number of facets were very large. A comet with 90 near regular sides would be dice-like. ",null,0,cdpkpgh,1rnn07,askscience,top_week,2
broofa,"tl;dr: **1,876 faces for a die to behave like a sphere on a smooth wooden surface** *

Here's the rational for this ...

As others have noted the behavior of a die depends on the surface it is rolled on. Intuitively we know that rolling a die on a granite countertop is very different than rolling it on a shag carpet. Thus, to get to a specific answer we have to make some assumptions.

For starters, from [this size chart](http://dicegamedepot.com/dice-sizes/) we see that the most common die is 16mm across.  And to avoid some nasty dynamic analysis, let's just say the die will ""begin to act like a sphere"" when the size of a face is &lt;= 10x the imperfections in the surface.  [This is rather arbitrary but this is a vague enough question that if we can get to within +/- 10x we probably have a ""good enough"" answer.]

For the surface, let's assume we're on a finished wood counter top with [surface irregularities on the order of ~0.05mm](http://www.nyme.hu/fileadmin/dokumentumok/fmk/acta_silvatica/cikkek/Vol04-2008/08_magoss_p.pdf).

We can approximate our die as a sphere of radius = 8mm, with faces of radius = 0.25mm.  To get our answer, we then calculate how many faces can be packed into the surface area provided by our sphere.  To simplify this problem, we use the fact the face size is small relative to the die size, meaning we can just treat the sphere surface like a flat plane.  Thus, our equation for determing the # of faces is this:

    # of faces = AS / AF * D

Where ...

* AS = Area of sphere = 4 * pi * 8mm ^2 = 804mm^2
* AF = Area of die face  = 2 * pi * 0.25mm^2 = 0.39mm^2
* D = [Packing density of circles](http://en.wikipedia.org/wiki/Circle_packing#Packings_in_the_plane) = 0.91

Plug those in and you get the answer at top of post.

(*) Your Mileage May Vary, depending on size of die, material and finish of surface, and extent to which you actually give a damn about accuracy of this answer.

[Edit: move answer to top]",null,0,cdp8o78,1rnn07,askscience,top_week,3
abz_eng,"I think this is question of angles between sides 

A cube 6 sided has 90 degrees between sides whereas a sphere essentially has 180 - (1/infinity)? 

Now as the number of sides increases the angle increases and the angle at which the dice flips decreases (centres of mass and gravity) - cube flips at 45 degrees whereas a sphere is just over zero. You need to determine subjectively in each situation the angle you require.

(I've ME/CFS guys so this is the best I can do)",null,0,cdp9c65,1rnn07,askscience,top_week,1
Larrysbirds,"Think about the changes between a triangle to a square to a pentagon to a hexagon, etc. The number of sides increase by 1, the theoretical maximum amount of sides (or infinite) would be shaped as a circle. To answer your question: infinite in theory.
I speak in 2D, but it translates into 3D the same.",null,0,cdp9cz1,1rnn07,askscience,top_week,1
godless117,"technically with no limit specified on the surface area of each facet, there is no limit, so long as you have the manufacturing means you can make one giant die with 1,000,000,000 facets. Perhaps a better question would be what is the minimum surface area, or surface area of facet to number of facet ratio, that a facet of a die must possess in order to still give specific rolls?",null,0,cdpb4vt,1rnn07,askscience,top_week,1
mao_zedonk,"Figured I'd take a quick stab at a ""back-of-the-napkin"" calculation for this one. One can imagine that a polyhedron with edge lengths one atom apart would be indistinguishable from the best physical approximation we could possibly make of a sphere. I made some assumptions here: 
-all bond lengths are an average of 135 picometers (mid-range for carbon-carbon bonds you would find in a plastic die).
-a polyhedron can be constructed from ""n"" approximately equilateral triangles (would love some input on this from the mathematicians out there)

The surface area of a standard 6-sided die with 16mm edges is 1.536E-3 square meters. The area of an equilateral triangle with edge lengths of 135pm is 3.490E-12 square meters. So.... to construct a n-sided polyhedron composed of approximately equilateral triangular sides and having the same surface area as a standard die, we would need... 4.40E17 sides.

That is, 440,000,000,000,000,000 sides. Granted, some of my assumptions may have over-reached, thoughts anyone?",null,0,cdpbihm,1rnn07,askscience,top_week,1
ademnus,"Whatever it is, it must be 6 or higher.

No one needs a 1 sided die. I can predict what it would land on with 100% certainty anyway, if you could somehow possess one. 2 sided dice are otherwise known as coins and they toss or flip better than they roll, unless you wish to chase one under the vending machine. I have tried oddly-shaped 3-5 sided dice and they do not roll well. They're best shaken in closed, cupped hands and tossed down. Once you hit a 6 sider, however, it's all downhill from there.
",null,0,cdpctvm,1rnn07,askscience,top_week,1
KillingSloth,"I think what OP wants to know is this : http://www.toplessrobot.com/2009/04/the_10_most_shameful_rpg_dice.php 

And from experience, 100 gives you pretty much a sphere (a ball if you want), you could say 1000 sided die for maximum crazyness (I'm sure it will come out one day with 3D printing) ",null,0,cdpczox,1rnn07,askscience,top_week,1
kobescoresagain,"Something to take into consideration as well is the size of the sphere like object.  As size increases the amount of sides required to approach sphere like properties would increase unless you also changed how the outside interacted with the sphere like object.  For instance if you had a 20 sided die similar to ones used in card games, you could easily roll it like a sphere.  However of it were much larger it would become increasingly more difficult to use it like a sphere.  ",null,0,cdpdmh9,1rnn07,askscience,top_week,1
GBBerg88,"Technically speaking that answer to this must be that the dice would have to be able to land on a single square, but also be readable. 

The size in itself could technically be very large, but not infinite because at some point you have to account for the hardness of the material and make sure you don't have any kind of failiure either in the ground or in the dice itself. But then you'd go in to extremes and you couldn't actually throw it, only roll it over and see what square it landed on. 

So limiting the size to something reasonable, such as a ""handful"" at max, an adult sized handball for instance. And then accounting for readability and it landing reasonably on one square. I'd say something like 350 sides would be possible. 

But for that to work properly you need a perfect surface. ",null,0,cdpek01,1rnn07,askscience,top_week,1
WorldIsImagination,"I don't know the exact formula, but I think it would be something like""

A constantly changing formula based on the speed of growth of our universe. 

Meaning the largest dice would be slightly smaller than the exact dimension of our (spherical??) universe. As the universe expands the answer to the question would change. ",null,0,cdpgxp6,1rnn07,askscience,top_week,1
king_of_the_universe,"Well, there would certainly be a number of sides that makes it impossible to easily determine what number the dice throw resulted in. From this perspective, ""acting like a sphere"" might already have begun with a 100-sided die.",null,0,cdphwbd,1rnn07,askscience,top_week,1
TITAN_,"Having played a lot of percentile dice based RPG systems, the closest I've seen a dice come is the d100, a 100 sided dice. It rolls nearly like a ball, so easy to roll off the table. Which is why you usually opt for 2d10 instead where one die is the tens and the other the single numbers.",null,0,cdpiix3,1rnn07,askscience,top_week,1
ohmz2,"The determining factors here are the surface finish and geometrical integrity of the die and the surface it's being rolled on. We can use microfabrication techniques such as photolithography and micromachining, which can produce structures with tolerances at the micron level (1/1000 of a mm, ~0.000039 inches). You can make a die with an innumerable amount of super flat sides, as well as super-flat surface to roll it on. Dust and other finite particles would interfere with our microfabricated die (dust particles range from ~5-1000 microns), so we'd enclose it in a vacuum chamber. The super-flatness of our surface would mean that the die would be subjected to minimal frictional/damping forces, so it would roll around and slide for a while. Potential and kinetic energy will eventually dissipate as the die hits the inner walls of our vacuum chamber. Ultimately, the die will reach mechanical equilibrium, and the most elevated side of our die from the super-flat surface (the side should also be parallel with the right number of sides and geometry) will indicate the number we rolled.   

Keep in mind, just because it's microfabricated, doesn't mean the die as a whole has to be microscopic. The sides themselves will be on micron scale, but the whole die itself can be as big as a basketball, or larger. But let's stick to the former for example's sake: Let's make each side of our die a hexagon with sides 10 microns in length, and let's take a regulation basketball having a surface area of roughly 0.184838 m^2. Dividing the basketball's surface area by the surface area of our hexagon means we'd have 1,422,927,945 possible sides to roll.   


Totally possible. Incredibly unfeasible. I can't think of all the problems such a project would present, but the first thing that comes to mind is material requirements which are problematic. We'd need a material that could withstand the impacts of bouncing around without shattering or compromising the integrity of the die's incredibly finite geometry.   

Hope I managed to provide some insight into that.",null,0,cdp504i,1rnn07,askscience,top_week,1
elsjaako,"You need to define what ""acting like a sphere"" means.

A coin can only flip over if it bounces, a six sided die already sort of rolls over a hard surface, a twenty sided dice rolls (I don't feel like what they do needs a qualifier). A 100 sided die moves almost like a smooth ball, but it will land on one of 100 discrete points.",null,127,cdp5a68,1rnn07,askscience,top_week,721
thepsyborg,"Your question presumes that we can answer a binary question (""Does it act like a sphere?"" Yes/no) about a given shape. Without specifying an arbitrary threshold for spheroid behavior, the most we can say is that behavior approaches that of a sphere as shape approaches that of a sphere.

The largest number of sides a single brand-new die could have while still rolling fairly is limited only by the precision &amp; accuracy of its manufacturing process. The upper limit on number of facets for a _usable_ fair-rolling die would also rely on the resistance of its material to wear and deformation, if we expect it to remain essentially fair for a reasonable number of rolls.",null,30,cdp7es9,1rnn07,askscience,top_week,164
dracho,"The answer is obvious.  None.  

If an object has 1,000,000,000,000,000,000 sides, it still doesn't act like a sphere.  It's acts *more* like a sphere than a cube, depending on what metrics you're measuring, but it's still not a sphere.

The only thing that ""acts like a sphere"" is a sphere.

There is some discussion about molecules, and how in a real-world scenario, a sphere made of ordinary matter isn't truly a sphere... I'm ignoring this and focusing on the pure math.",null,30,cdp6v7x,1rnn07,askscience,top_week,108
sperho,"OP's post reminds me of the [MythBuster's segment](http://dsc.discovery.com/tv-shows/mythbusters/videos/square-wheels-angle-2.htm) on putting square wheels on a truck and seeing if driven at high speeds they would ""act like a"" round wheel and produce a smooth ride. spoiler alert:  not really.",null,6,cdp1ngu,1rnn07,askscience,top_week,37
null,null,null,7,cdp6586,1rnn07,askscience,top_week,30
Hokararu,"To clarify what I mean by 'act like a sphere' I am referring to the point at which an object will maintain its current position when the angle of the surface it rests on is adjusted.

For example, if I roll a die onto a table and then lift that table off the ground at one end, the die may slide down the table but is unlikely to change the side it rests on. A sphere however, would roll down the table making it useless as a die in this situation.

Math me...",null,11,cdp5j36,1rnn07,askscience,top_week,26
AdmiralTroll,"Not an expert on this, but my understanding is that everything, aside from 'the perfect sphere' holds a number of angles, even if it is down to an molecular level. Therefore every smooth rubber ball has a finite number of sides and so technically is a die in its own right?

On another note: surely then a perfect sphere that had no angled side to land on would; on a completely flat smooth surface, continue rolling without needing more force applied?",null,3,cdp6q8p,1rnn07,askscience,top_week,13
John_Duh,"If you only consider a Dice one of the Platonic Solids you can only go as high as 20 sided. They are the only polyhedron that has equal sized sides, so the question is not really possible to answer.

You could though take a sphere and cut of parts of it to create surfaces but it would not be considered a Platonic Solid.",null,3,cdp7li9,1rnn07,askscience,top_week,12
Pitboyx,"Since you probably won't be rolling on a perfectly clean 100% smooth surface, the die would be considered uselessly inaccurate between 8-11 thousand sides (estimated through blender icosphere) because at that point, the die will be tipped on an edge or corner instead of a face by particles of dust, grooves, and bumps in the surface as if you were spinning a regular 6-sided die on a corner.

From an even more practical viewpoint, it would be near impossible to decide which face is facing upwards with the naked eye after 500-700 faces. 

A 80 sided icosphere would probably be the ideal size for having the most results per die. The more reasonable choice would be to simply use more dice.",null,7,cdpalpy,1rnn07,askscience,top_week,16
vortik,"As others have mentioned, the term ""act like a sphere"" is a bit vague.  So when I read this, I started wondering if there was a way that we could quantify this in terms of three-dimensional packing densities.

A cube can be arranged to fill a three-dimensional space completely.  But as you ""subdivide"" the sides, on your way toward turning it into a sphere, your maximum packing density decreases.  For example:

| Solid | Maximum known packing density |
| ------ | ------------------- |
| octahedra | 0.947003 |
| dodecahedra | 0.904002 |
| icosahedra | 0.836315 |

[Wiki reference](http://en.wikipedia.org/wiki/Packing_problem#Packings_of_Platonic_solids_in_three_dimensions)

As these objects become more sphere-like, the maximum known packing density is conjectured to approach that of the [Kepler conjecture](http://en.wikipedia.org/wiki/Kepler_conjecture) density of ~0.74 for best-possible sphere packing in 3d.  My understanding is that much of these ""best known"" values have been computed via computer simulation, and no rigorous proofs of this transition have been forthcoming.

If we assume that this trend is correct (frankly, I think it is, but I don't think it has been formally proven), then what I was thinking was that the OP may be able to quantify (to some extent) what ""close to a sphere"" means.  For example, he could take the given icosahedron considered in [this paper](http://www.nature.com/nature/journal/v460/n7257/full/nature08239.html), and say that it is ""0.83/0.74 ~= 12%"" away from being a ""sphere"", and this is what it looks like, and that is how well it packs.  

I do not know if the above would answer the OP's question.  A simpler answer would probably be:  ""a 20-sided die rolls pretty well"", but others have tried to tackle this.  I suspect that you'd have to consider the effects of friction between the sides of the die against the contact surface (compared to the infinite case for a sphere?), but that's out of my purview.


*Edit: Formatting.",null,0,cdp2oz3,1rnn07,askscience,top_week,7
null,null,null,1,cdpbqy3,1rnn07,askscience,top_week,6
Jackeddaniels69,"Hi Hokararu, if we assume that each side is a congruent regular polygon then you can actually find the spherical deviation (relatively how close it is to being a sphere) for a Polyhedra (Basically a symmetrical 3D Object)!
If we take a 3 sided object connected by equilateral triangles we get a tetrahedron. Use the formula δ(v) = 360 - (sum interior angles surrounding a single vertex) which will be, δ(v) = 360 - (180). 180 was derived by each of the three interior angles of the equilateral triangle 3(60). 

δ(v) = 180 = Spherical Deviation. The lower the number the ""more spherical"". 0 would be a sphere. Unfortunately you can only have a certain number of side that are regular and congruent fitting the description of a dice.

The largest number of sides to still be considered a 'dice' and be as close to 'spherical' as possible is 20 Sides.
http://en.wikipedia.org/wiki/Icosahedron 
Devils advocate though, these are a thing.
http://en.wikipedia.org/wiki/Zocchihedron

Edit&gt; Also a dices vertices connect edges all with the same interior angle. so Catalan or Semi-regular polyhedra would not count in my opinion. So it really depends on your perspective of what a dice really is.",null,0,cdp7hl7,1rnn07,askscience,top_week,5
OneTwoTriangle,"This is more a physical answer, than a mathematical one. I am assuming a dice in our reality.

When the surfaces of the sides are a couple of molecules big the dice will start to behave like a sphere. It is basically a ball. To calculate the amount of sides you calculate the surface of a sphere with (almost) the same diameter as your dice. Divide this number by the surface area of your dice side. This gives the amount of sides it can have.

~~A problem however is the regularity of your dice. A twenty sided dice is the biggest dice you can make on which all sides have equal probability. After that you need different kinds of shapes to create the dice. This follows by the fact that there are only five platonic solids.~~ 

EDIT: That last part is wrong see the comment by eli5--answer. 

",null,2,cdpe2r1,1rnn07,askscience,top_week,4
88888888333,"Technically, nothing is a perfect sphere. In fact, all perfect geometric shapes are only mathematical concepts. You cannot create a perfect sphere out of atoms. So the answer to your question is that what you perceive to ""act like a sphere"" is actually similar to a die with *many* sides, since there is no such thing as a perfect sphere. ",null,0,cdpezbt,1rnn07,askscience,top_week,3
gubasaurus,"To me (an engineer, not a mathematician), it's an issue of stability. Will the dice ever reach a position in which it is stable? When perturbed, will it want to go back to its original state? To 'behave like a sphere' the smallest angular perturbation will cause the sphere to move and keep moving. A dice with sides of equal surface areas that is perturbed by a small enough will go back to its original position. So my answer to your question would be that, to behave like a sphere, an object cannot have sides, and so cannot be a dice.",null,0,cdp7zu5,1rnn07,askscience,top_week,3
bangsecks,"I think OP is asking how many sides could be added to a die before it no longer would settle and rest on a discrete surface, not how many facets are possible to have on one.  This comes down in part to the volume and mass of the die, as the die gets larger and heavier, whatever inertia from the roll acting on its mass will pull it in one direction or another such that it doesn't find a resting position, at least in a reasonable amount of time continuing to move about like a ball does.  The greater the mass the fewer surfaces possible before it just rolls.

Before we get the volume and mass to certain quantities such that we can fit a lot on there and still have it behave as a die we'd probably run into the problem of not being able to clearly tell what surface is the value surface, or the side that's telling you what number you have both because it would be hard to tell just which is directly at the top when they're so small and because as there are more and more sides the numbers will have to get to digit strings so large they'd be difficult to fit on shrinking facets.",null,0,cdp8bd3,1rnn07,askscience,top_week,2
huggybear0132,"This is a 3D version of making a polygon into a circle. You could consider a circle to be a regular polygon with an infinite number of sides, and likewise a sphere a regular prism with an infinite number of sides. That is the theoretical side. Now physically, what you mean by ""act like"" is ambiguous. Do spheres not roll and then eventually rest on a ""side"" (the point tangent to the surface they are resting on)? In that sense, all dice act like a sphere already.  I am going to go ahead and say the number of sides n has to tend towards infinity.",null,0,cdpe3en,1rnn07,askscience,top_week,2
ImAVibration,"Here's a crazy idea, a perfectly smooth sphere has an infinite number of sides it can rest on. But if you increase its diameter, it will still have an infinite number of 'sides', but it will also have infinitely more than the smaller sphere.",null,0,cdpfg2x,1rnn07,askscience,top_week,2
bloonail,"Neutron stars be damned -0- the question is about dice and ""acting like a sphere"".

That question seems three parts:
1) what makes a dice act like a dice
2) when is a dice no longer acting like a dice
3) at what level do facets make an object non spherical.

A dice provides evenly random results based on the number of facets of the dice.

Dice no longer act like dice when the randomness is predicable or favors a group of facets. Six sided dice are well understood. They are hard to roll in a predictable way.. No one really trusts the ones with more sides. There are too many ways to mess with the spin and bowling-like parameters of the many sided dice.

However a regularly multifaceted object would likely be considered dice-like even if the number of facets were very large. A comet with 90 near regular sides would be dice-like. ",null,0,cdpkpgh,1rnn07,askscience,top_week,2
broofa,"tl;dr: **1,876 faces for a die to behave like a sphere on a smooth wooden surface** *

Here's the rational for this ...

As others have noted the behavior of a die depends on the surface it is rolled on. Intuitively we know that rolling a die on a granite countertop is very different than rolling it on a shag carpet. Thus, to get to a specific answer we have to make some assumptions.

For starters, from [this size chart](http://dicegamedepot.com/dice-sizes/) we see that the most common die is 16mm across.  And to avoid some nasty dynamic analysis, let's just say the die will ""begin to act like a sphere"" when the size of a face is &lt;= 10x the imperfections in the surface.  [This is rather arbitrary but this is a vague enough question that if we can get to within +/- 10x we probably have a ""good enough"" answer.]

For the surface, let's assume we're on a finished wood counter top with [surface irregularities on the order of ~0.05mm](http://www.nyme.hu/fileadmin/dokumentumok/fmk/acta_silvatica/cikkek/Vol04-2008/08_magoss_p.pdf).

We can approximate our die as a sphere of radius = 8mm, with faces of radius = 0.25mm.  To get our answer, we then calculate how many faces can be packed into the surface area provided by our sphere.  To simplify this problem, we use the fact the face size is small relative to the die size, meaning we can just treat the sphere surface like a flat plane.  Thus, our equation for determing the # of faces is this:

    # of faces = AS / AF * D

Where ...

* AS = Area of sphere = 4 * pi * 8mm ^2 = 804mm^2
* AF = Area of die face  = 2 * pi * 0.25mm^2 = 0.39mm^2
* D = [Packing density of circles](http://en.wikipedia.org/wiki/Circle_packing#Packings_in_the_plane) = 0.91

Plug those in and you get the answer at top of post.

(*) Your Mileage May Vary, depending on size of die, material and finish of surface, and extent to which you actually give a damn about accuracy of this answer.

[Edit: move answer to top]",null,0,cdp8o78,1rnn07,askscience,top_week,3
abz_eng,"I think this is question of angles between sides 

A cube 6 sided has 90 degrees between sides whereas a sphere essentially has 180 - (1/infinity)? 

Now as the number of sides increases the angle increases and the angle at which the dice flips decreases (centres of mass and gravity) - cube flips at 45 degrees whereas a sphere is just over zero. You need to determine subjectively in each situation the angle you require.

(I've ME/CFS guys so this is the best I can do)",null,0,cdp9c65,1rnn07,askscience,top_week,1
Larrysbirds,"Think about the changes between a triangle to a square to a pentagon to a hexagon, etc. The number of sides increase by 1, the theoretical maximum amount of sides (or infinite) would be shaped as a circle. To answer your question: infinite in theory.
I speak in 2D, but it translates into 3D the same.",null,0,cdp9cz1,1rnn07,askscience,top_week,1
godless117,"technically with no limit specified on the surface area of each facet, there is no limit, so long as you have the manufacturing means you can make one giant die with 1,000,000,000 facets. Perhaps a better question would be what is the minimum surface area, or surface area of facet to number of facet ratio, that a facet of a die must possess in order to still give specific rolls?",null,0,cdpb4vt,1rnn07,askscience,top_week,1
mao_zedonk,"Figured I'd take a quick stab at a ""back-of-the-napkin"" calculation for this one. One can imagine that a polyhedron with edge lengths one atom apart would be indistinguishable from the best physical approximation we could possibly make of a sphere. I made some assumptions here: 
-all bond lengths are an average of 135 picometers (mid-range for carbon-carbon bonds you would find in a plastic die).
-a polyhedron can be constructed from ""n"" approximately equilateral triangles (would love some input on this from the mathematicians out there)

The surface area of a standard 6-sided die with 16mm edges is 1.536E-3 square meters. The area of an equilateral triangle with edge lengths of 135pm is 3.490E-12 square meters. So.... to construct a n-sided polyhedron composed of approximately equilateral triangular sides and having the same surface area as a standard die, we would need... 4.40E17 sides.

That is, 440,000,000,000,000,000 sides. Granted, some of my assumptions may have over-reached, thoughts anyone?",null,0,cdpbihm,1rnn07,askscience,top_week,1
ademnus,"Whatever it is, it must be 6 or higher.

No one needs a 1 sided die. I can predict what it would land on with 100% certainty anyway, if you could somehow possess one. 2 sided dice are otherwise known as coins and they toss or flip better than they roll, unless you wish to chase one under the vending machine. I have tried oddly-shaped 3-5 sided dice and they do not roll well. They're best shaken in closed, cupped hands and tossed down. Once you hit a 6 sider, however, it's all downhill from there.
",null,0,cdpctvm,1rnn07,askscience,top_week,1
KillingSloth,"I think what OP wants to know is this : http://www.toplessrobot.com/2009/04/the_10_most_shameful_rpg_dice.php 

And from experience, 100 gives you pretty much a sphere (a ball if you want), you could say 1000 sided die for maximum crazyness (I'm sure it will come out one day with 3D printing) ",null,0,cdpczox,1rnn07,askscience,top_week,1
kobescoresagain,"Something to take into consideration as well is the size of the sphere like object.  As size increases the amount of sides required to approach sphere like properties would increase unless you also changed how the outside interacted with the sphere like object.  For instance if you had a 20 sided die similar to ones used in card games, you could easily roll it like a sphere.  However of it were much larger it would become increasingly more difficult to use it like a sphere.  ",null,0,cdpdmh9,1rnn07,askscience,top_week,1
GBBerg88,"Technically speaking that answer to this must be that the dice would have to be able to land on a single square, but also be readable. 

The size in itself could technically be very large, but not infinite because at some point you have to account for the hardness of the material and make sure you don't have any kind of failiure either in the ground or in the dice itself. But then you'd go in to extremes and you couldn't actually throw it, only roll it over and see what square it landed on. 

So limiting the size to something reasonable, such as a ""handful"" at max, an adult sized handball for instance. And then accounting for readability and it landing reasonably on one square. I'd say something like 350 sides would be possible. 

But for that to work properly you need a perfect surface. ",null,0,cdpek01,1rnn07,askscience,top_week,1
WorldIsImagination,"I don't know the exact formula, but I think it would be something like""

A constantly changing formula based on the speed of growth of our universe. 

Meaning the largest dice would be slightly smaller than the exact dimension of our (spherical??) universe. As the universe expands the answer to the question would change. ",null,0,cdpgxp6,1rnn07,askscience,top_week,1
king_of_the_universe,"Well, there would certainly be a number of sides that makes it impossible to easily determine what number the dice throw resulted in. From this perspective, ""acting like a sphere"" might already have begun with a 100-sided die.",null,0,cdphwbd,1rnn07,askscience,top_week,1
TITAN_,"Having played a lot of percentile dice based RPG systems, the closest I've seen a dice come is the d100, a 100 sided dice. It rolls nearly like a ball, so easy to roll off the table. Which is why you usually opt for 2d10 instead where one die is the tens and the other the single numbers.",null,0,cdpiix3,1rnn07,askscience,top_week,1
ohmz2,"The determining factors here are the surface finish and geometrical integrity of the die and the surface it's being rolled on. We can use microfabrication techniques such as photolithography and micromachining, which can produce structures with tolerances at the micron level (1/1000 of a mm, ~0.000039 inches). You can make a die with an innumerable amount of super flat sides, as well as super-flat surface to roll it on. Dust and other finite particles would interfere with our microfabricated die (dust particles range from ~5-1000 microns), so we'd enclose it in a vacuum chamber. The super-flatness of our surface would mean that the die would be subjected to minimal frictional/damping forces, so it would roll around and slide for a while. Potential and kinetic energy will eventually dissipate as the die hits the inner walls of our vacuum chamber. Ultimately, the die will reach mechanical equilibrium, and the most elevated side of our die from the super-flat surface (the side should also be parallel with the right number of sides and geometry) will indicate the number we rolled.   

Keep in mind, just because it's microfabricated, doesn't mean the die as a whole has to be microscopic. The sides themselves will be on micron scale, but the whole die itself can be as big as a basketball, or larger. But let's stick to the former for example's sake: Let's make each side of our die a hexagon with sides 10 microns in length, and let's take a regulation basketball having a surface area of roughly 0.184838 m^2. Dividing the basketball's surface area by the surface area of our hexagon means we'd have 1,422,927,945 possible sides to roll.   


Totally possible. Incredibly unfeasible. I can't think of all the problems such a project would present, but the first thing that comes to mind is material requirements which are problematic. We'd need a material that could withstand the impacts of bouncing around without shattering or compromising the integrity of the die's incredibly finite geometry.   

Hope I managed to provide some insight into that.",null,0,cdp504i,1rnn07,askscience,top_week,1
Astrokiwi,"Most things in the solar system go at 10s of km/s. Earth goes at about 0.01% of the speed of light, or about 30 km/s. Even Mercury is going at less than 60 km/s. So if ISON was going at 0.1% of the speed of light, it's going at 300 km/s, which is more than 5 times faster than the next fastest thing.

This speed basically comes from the Sun's gravity: the comet has ""dropped"" towards the Sun and picked up a lot of speed. The greatest speed you can get from falling downwards (assuming you start pretty stationary) is the same as the escape velocity from the Sun's surface - a bit over 600 km/s. So you'll not likely see anything natural in the solar system going at 600 km/s or faster.",null,5,cdp33ut,1rnn7b,askscience,top_week,35
null,null,null,34,cdp4gsf,1rnn7b,askscience,top_week,5
Astrokiwi,"Most things in the solar system go at 10s of km/s. Earth goes at about 0.01% of the speed of light, or about 30 km/s. Even Mercury is going at less than 60 km/s. So if ISON was going at 0.1% of the speed of light, it's going at 300 km/s, which is more than 5 times faster than the next fastest thing.

This speed basically comes from the Sun's gravity: the comet has ""dropped"" towards the Sun and picked up a lot of speed. The greatest speed you can get from falling downwards (assuming you start pretty stationary) is the same as the escape velocity from the Sun's surface - a bit over 600 km/s. So you'll not likely see anything natural in the solar system going at 600 km/s or faster.",null,5,cdp33ut,1rnn7b,askscience,top_week,35
null,null,null,34,cdp4gsf,1rnn7b,askscience,top_week,5
Psygnosis911,"The proteins in the egg denature and congeal when heated, that is what causes them to solidify. It's the same process that causes meat to firm up when cooked.

[Wikipedia page on protein denaturation](http://en.wikipedia.org/wiki/Denaturation_%28biochemistry%29)",null,1,cdp1wop,1rnokt,askscience,top_week,8
UpboatOrNoBoat,"Well, for starters, you're trying to compare an egg to a homogenous mixture, which has phase states and transitions that are clearly defined.

Eggs are a vast mixture of proteins, lipids, and other molecules. The general solidification is due to the denaturization and reassembly of the vast amount of proteins in the egg that cause it to solidify. Basically the proteins in the liquid form break down due to heating or chemical reasons and as they break, they form strong bonds with surrounding proteins. Eventually, these protein bonds force water molecules out and the egg starts to solidify.",null,0,cdp1wzl,1rnokt,askscience,top_week,4
Trying_2help,"The main difference between gas giants and rocky planets is that rocky planets are primarily composed of solid materials and gas giants are not, even though the atmosphere of a planet like Venus is very dense it's still only a small part of the total mass. 

A planet like Venus has a solid surface but a gas giant doesn't have a surface at all (at least not a solid one), the gas just gets denser as you get closer to the center. A gas giant can still have a molten core or have that core dispersed throughout the planet, but that only makes up a small part of the total mass.",null,0,cdp2901,1rnpru,askscience,top_week,3
UpboatOrNoBoat,"Gas giants are more similar to stars than they are planets like Venus or Earth. They are basically failed stars, they've accumulated such massive amounts of gas that they have a very hot and active core, but their mass is so large that any heavy elements don't survive long enough to accumulate due to the intense heat. Either they disperse through the atmosphere of the planet or they stay at the molten core.


You are wrong about their cores, they are completely molten. Gas giants are almost completely comprised of Hydrogen and Helium, and completely lack a surface. Rather, strata are distinguished by the density of the atmosphere.",null,2,cdp21vq,1rnpru,askscience,top_week,4
dukwon,"Neutrinos are absolutely affected by gravity. In fact, even photons are.

According to General Relativity, all particles will follow the curvature of spacetime: i.e. they are affected by gravity whether they have mass or not.

If an approximately massless neutrino travelling at approximately the speed of light approaches the Earth tangentially to its surface: assuming it doesn't interact, it will be deflected through an angle of about 0.16 millionths of a degree. (Using the gravitational lensing formula).

This is an infinitesimally small angle, and I've chosen the case where it's largest.
The angle is zero if the neutrino passes through the exact centre of mass of the Earth.

Edit: A note about neutrino beams: it is very hard to control the angle at which neutrinos are emitted. To a good approximation, all processes that emit neutrinos do so isotropically (in all directions) in the rest frame of that process. We can use such devices as magnetic focusing horns to direct particles that are likely to give off neutrinos (e.g. charged pions and muons) towards the detector. You don't even need (or sometimes want) to point the beam directly at the detector, for example T2K uses a 2.5 degree offset in order to understand well the range of energies that the neutrinos have.",null,0,cdp2sst,1rnq5s,askscience,top_week,5
dukwon,"Without the immense gravity, the neutrons would beta decay into protons (and electrons and antineutrinos).

The question really is how much gravitational energy per proton was originally required to force them to electron-capture in the first place. I'm not sure how to calculate it, but it should give you the initial temperature of the resulting matter.

Presumably it's hydrogen, but I guess it's dense enough for fusion to occur, if there's enough energy

edit: Apparently neutron star material is already incredibly hot, so enjoy your explosion.",null,1,cdp6etq,1rnqya,askscience,top_week,5
super-zap,"The thing that keeps it so compact, is immense gravity, once that is gone it should expand very very rapidly. At the same time the chunk of neutron star is so dense that it should be able to push through any material (if it stayed compact ).",null,0,cdp26i0,1rnqya,askscience,top_week,3
RelativisticMechanic,"Cosmological expansion only causes things that are already sufficiently far apart to expand away from one another. For things that are relatively close to one another, like the galaxies in the Local Group, the behavior is much more closely approximated by the standard picture of attractive gravity.

In other words, gravitationally bound systems (like galaxy groups) don't experience cosmological expansion. It's only when you consider widely separated galaxies that you find the distance between them increasing due to expansion.",null,13,cdp2yr1,1rnrll,askscience,top_week,90
yosata,"Basically the gravitational force is much stronger than the expansion force of the universe. Only if the distance between two objects is sufficiently large, then the objects will move away from each other.

In billions of years our cluster of galaxies will be the only thing visible to us in the sky. Without a telescope there wouldn't be much different as the stars keep are kept together by gravity.
The galaxies that are far away will have moved so far away that we can't see them anymore.",null,9,cdp6c1r,1rnrll,askscience,top_week,22
RelativisticMechanic,"Cosmological expansion only causes things that are already sufficiently far apart to expand away from one another. For things that are relatively close to one another, like the galaxies in the Local Group, the behavior is much more closely approximated by the standard picture of attractive gravity.

In other words, gravitationally bound systems (like galaxy groups) don't experience cosmological expansion. It's only when you consider widely separated galaxies that you find the distance between them increasing due to expansion.",null,13,cdp2yr1,1rnrll,askscience,top_week,90
yosata,"Basically the gravitational force is much stronger than the expansion force of the universe. Only if the distance between two objects is sufficiently large, then the objects will move away from each other.

In billions of years our cluster of galaxies will be the only thing visible to us in the sky. Without a telescope there wouldn't be much different as the stars keep are kept together by gravity.
The galaxies that are far away will have moved so far away that we can't see them anymore.",null,9,cdp6c1r,1rnrll,askscience,top_week,22
Thermodynamicist,"At the most basic level, explosions happen because of rapid exothermic processes, which produce extremely large transient power densities. 

Extremely high power densities imply high temperatures, which means that the primary initial method of energy transfer is going to be radiation, because [radiative power varies as T^4](http://en.wikipedia.org/wiki/Radiative_heat_transfer#Radiative_power) and therefore it tends to dominate when T is large. 

If you detonate a nuke in space, you get a big flash of gamma rays, which for practical purposes come from a point source.

If you detonate a nuke in the atmosphere, the gamma rays get absorbed by the atmosphere, heating it up, and then re-radiated at a slightly longer wavelength. Eventually, the wavelength hits an atmospheric window and the flash ""escapes"". What you actually see, if you look at the high-speed footage, is a flash of light which comes from a sphere of finite radius and is composed of a variety of wavelengths of light approximating a Boltzmann distribution (obviously it's going to be missing sundry spectral absorption lines).

This sphere of really hot air then obviously expands violently, due to the gas law, and produces a [*wave of abrupt disturbance*](http://homepages.abdn.ac.uk/h.tan/pages/teaching/explosion-engineering/Rankine.pdf), which is responsible for the blast effects. 

At a high level of abstraction, what's happening is that the really low entropy energy from the explosion is being converted into higher entropy energy by interaction with the medium through which it is being transmitted. 

What this does is to localize the effects of the explosion to a greater degree than would be expected from the inverse square law of [radiation flux](http://en.wikipedia.org/wiki/Radiation_flux).

Shrapnel released in the atmosphere is subject to aerodynamic drag, which converts its kinetic energy into heat. 

Shrapnel released in space just keeps on going until it hits something. 

What this means is that explosions in space can cause radiative damage (e.g. to sensors) at very long distances, but will generally be much less likely to cause physical damage at short to medium ranges because there are no localizing effects. 

It also means that there is a small probability of devastating shrapnel impact out to arbitrary range. 

Overall, the effects are more stochastic, because the absence of localized interactions with a coupling medium such as a fluid or a solid mean that all the work is being done by a smaller number of higher energy particles. ",null,36,cdp6d37,1rnt7q,askscience,top_week,189
xtxylophone,"There would be no shockwave, all the energy would be in electromagnetic radiation of varying wavelengths i.e. visible light and infrared. Also it would accelerate any part of the bomb left over.

So the only way an explosion would do damage is if it were actually touching the target on detonation, it was hit by shrapnel from the blast or the radiation is intense enough to damage the target.

EDIT: Also as /u/super-zap said, expanding gas",null,10,cdp43xx,1rnt7q,askscience,top_week,25
Ferociousaurus,"Semi-related question. I've often heard there would be no fire in an explosion in space, and movies often get this wrong. Let's use the Death Star as an example. If the Death Star is presumably mostly sealed and full of oxygen, wouldn't there briefly be some fire in an explosion, albeit quickly snuffed? Same with destroyed fighter ships on a smaller scale? Or would the vacuum disperse anything that could catch fire too quickly?",null,2,cdp5pr1,1rnt7q,askscience,top_week,10
TangentialThreat,"The shrapnel usually has a larger kill radius than the overpressure.

With no atmosphere and in microgravity, shrapnel would *never slow down*. Only a real dick would set off a pipe bomb in orbit, and if you did it would work perfectly because the expanding hot gas is confined.

Armor-piercing shaped charges should also function properly as the expanding gas does not have to go very far.",null,1,cdp4r42,1rnt7q,askscience,top_week,6
Your_ish_granted,An explosion and the resulting shock wave is a rapid energy release being propagated through mass (air or water). So if there is no mass (in a vacuum) the energy is transfered in the fragments of the bomb (hot gasses etc) and as energy in wavelength form (ultraviolet etc). ,null,2,cdp4en0,1rnt7q,askscience,top_week,4
0hLaVache,"Since we're talking space explostions... 

Say we're trying to deflect an incoming earth-bound asteroid, and for whatever reasons have decided to go with an explosive deflection.  

What kind of explosion would be best to push the asteroid?  Atomic?  Thermonuclear? Conventional?  

Given the relative weakness of space explosions described in this thread, what is the biggest asteroid at what distance which could reasonably be nudged away from our poor little planet?  

Thanks!",null,0,cdp9lez,1rnt7q,askscience,top_week,1
null,null,null,9,cdp4hz3,1rnt7q,askscience,top_week,1
Thermodynamicist,"At the most basic level, explosions happen because of rapid exothermic processes, which produce extremely large transient power densities. 

Extremely high power densities imply high temperatures, which means that the primary initial method of energy transfer is going to be radiation, because [radiative power varies as T^4](http://en.wikipedia.org/wiki/Radiative_heat_transfer#Radiative_power) and therefore it tends to dominate when T is large. 

If you detonate a nuke in space, you get a big flash of gamma rays, which for practical purposes come from a point source.

If you detonate a nuke in the atmosphere, the gamma rays get absorbed by the atmosphere, heating it up, and then re-radiated at a slightly longer wavelength. Eventually, the wavelength hits an atmospheric window and the flash ""escapes"". What you actually see, if you look at the high-speed footage, is a flash of light which comes from a sphere of finite radius and is composed of a variety of wavelengths of light approximating a Boltzmann distribution (obviously it's going to be missing sundry spectral absorption lines).

This sphere of really hot air then obviously expands violently, due to the gas law, and produces a [*wave of abrupt disturbance*](http://homepages.abdn.ac.uk/h.tan/pages/teaching/explosion-engineering/Rankine.pdf), which is responsible for the blast effects. 

At a high level of abstraction, what's happening is that the really low entropy energy from the explosion is being converted into higher entropy energy by interaction with the medium through which it is being transmitted. 

What this does is to localize the effects of the explosion to a greater degree than would be expected from the inverse square law of [radiation flux](http://en.wikipedia.org/wiki/Radiation_flux).

Shrapnel released in the atmosphere is subject to aerodynamic drag, which converts its kinetic energy into heat. 

Shrapnel released in space just keeps on going until it hits something. 

What this means is that explosions in space can cause radiative damage (e.g. to sensors) at very long distances, but will generally be much less likely to cause physical damage at short to medium ranges because there are no localizing effects. 

It also means that there is a small probability of devastating shrapnel impact out to arbitrary range. 

Overall, the effects are more stochastic, because the absence of localized interactions with a coupling medium such as a fluid or a solid mean that all the work is being done by a smaller number of higher energy particles. ",null,36,cdp6d37,1rnt7q,askscience,top_week,189
xtxylophone,"There would be no shockwave, all the energy would be in electromagnetic radiation of varying wavelengths i.e. visible light and infrared. Also it would accelerate any part of the bomb left over.

So the only way an explosion would do damage is if it were actually touching the target on detonation, it was hit by shrapnel from the blast or the radiation is intense enough to damage the target.

EDIT: Also as /u/super-zap said, expanding gas",null,10,cdp43xx,1rnt7q,askscience,top_week,25
Ferociousaurus,"Semi-related question. I've often heard there would be no fire in an explosion in space, and movies often get this wrong. Let's use the Death Star as an example. If the Death Star is presumably mostly sealed and full of oxygen, wouldn't there briefly be some fire in an explosion, albeit quickly snuffed? Same with destroyed fighter ships on a smaller scale? Or would the vacuum disperse anything that could catch fire too quickly?",null,2,cdp5pr1,1rnt7q,askscience,top_week,10
TangentialThreat,"The shrapnel usually has a larger kill radius than the overpressure.

With no atmosphere and in microgravity, shrapnel would *never slow down*. Only a real dick would set off a pipe bomb in orbit, and if you did it would work perfectly because the expanding hot gas is confined.

Armor-piercing shaped charges should also function properly as the expanding gas does not have to go very far.",null,1,cdp4r42,1rnt7q,askscience,top_week,6
Your_ish_granted,An explosion and the resulting shock wave is a rapid energy release being propagated through mass (air or water). So if there is no mass (in a vacuum) the energy is transfered in the fragments of the bomb (hot gasses etc) and as energy in wavelength form (ultraviolet etc). ,null,2,cdp4en0,1rnt7q,askscience,top_week,4
0hLaVache,"Since we're talking space explostions... 

Say we're trying to deflect an incoming earth-bound asteroid, and for whatever reasons have decided to go with an explosive deflection.  

What kind of explosion would be best to push the asteroid?  Atomic?  Thermonuclear? Conventional?  

Given the relative weakness of space explosions described in this thread, what is the biggest asteroid at what distance which could reasonably be nudged away from our poor little planet?  

Thanks!",null,0,cdp9lez,1rnt7q,askscience,top_week,1
null,null,null,9,cdp4hz3,1rnt7q,askscience,top_week,1
mrrp,"Who says they're not flying towards the moon (or otherwise using the moon for navigation?)

If you're out in a field and you want to walk in a straight line you might adopt a strategy of keeping the moon on your right. If you replace the moon with a lamp you're going to be circling the lamp. If you keep the lamp at 45 degrees rather than 90 you're going to quickly spiral right into it.",null,2,cdpdd3z,1ro0cm,askscience,top_week,3
piantado,"I believe that the concensus is that they fly at a constant angle to the brightest light, which happens to be the moon in their natural environment. Because the moon is so far away, this keeps them flying in a generally straight line. But, when artificial lights are around, they try to fly at a constant angle to them too (mistaking them for the moon), and end up spiraling inwards.",null,0,cdpzamw,1ro0cm,askscience,top_week,1
Das_Mime,"The planets of the solar system formed out of an accretion disk of gas and dust. These disks are usually flared, meaning that they get thicker as you go outward from the center. Thus there is a larger amount of material at larger radii.

The heaviest elements were preferentially sorted toward the inner parts of the disk, while radiation pressure tended to push the light elements outward. This is what resulted in the inner planets being low-mass objects comprised of heavier elements while the outer planets are higher mass with lighter elements.",null,0,cdpgnc8,1ro139,askscience,top_week,1
null,null,null,4,cdpglmr,1ro139,askscience,top_week,1
zeug,"Very rarely proton-proton (or proton-antiproton) collisions at a high enough energy will produce top quarks. Typically these come in a top and antitop pair. Some even more rare collisions will result in a single top quark without a corresponding antitop. These are the ""single top quark"" events originally observed at Fermilab.

In either case, the top quark, or top-antitop pair, is not directly observed. What is seen in the detector is a spray of hadrons called a ""jet"". The very brief existence of a top quark is inferred from the patterns of jets and other observed particles produced in certain collision events. ",null,1,cdpe9v2,1ro2zy,askscience,top_week,5
cppdev,"Combining multiple flash units together is actually already how SSDs are made. For example, if you buy a 128 GB SSD, chances are it has 8 16 GB NAND chips wired together. 

As for why you can't combine thousands of little SSD chips to get very fast speeds, the answer is somewhat technical. In digital circuits, each time you have what's called ""fan in"", or multiple wires converging together there is associated with it some input capacitance which is proportional to the number of wires that are converging. So if you have 1000 wires from 1000 flash chips converging to one output (e.g., your SATA cable) you're going to have a very high input capacitance. However, the speed at which you can operate a circuit is inversely proportional to the input capacitance. 

TLDR: You could connect 1000 SATA chips in parallel, but it would end up being much slower than you'd expect.",null,0,cdph1pe,1ro8bz,askscience,top_week,2
TangentialThreat,"You may be looking for [this paper](http://grad.physics.sunysb.edu/~amarch/Walborn.pdf).

The single-photon experiments are just creepy and feel very intuitively wrong; there's no way to interpret it without a particle being in more than one place at once. The worst of them all is probably the [delayed-choice version of this experiment](http://www.youtube.com/watch?v=P2cMFrXqbYM) wherein the which-path information is erased *after* the photon has hit the screen.

",null,0,cdpfccj,1rodeo,askscience,top_week,6
Platypuskeeper,"Yes, you see that interference patterns you see in Young's double-slit experiment or a Michelson interferometer are still there even when you work with individual photons. 

",null,1,cdph2jb,1rodeo,askscience,top_week,5
The-Grim-Reefer,"If you do this your fish will most likely die. Marine fish have evolved to produce very concentrated urine while absorbing most of the water they take. While some marine fish may be able to tolerate a freshwater environment (called euryhaline fish), most will absorb too much water, creating a hypotonic solution within the fish, effectively killing it. My advice to you is to do some research on your particular fish and see if they can tolerate living in a freshwater environment. If they can be sure not to place them directly in freshwater because it will shock and kill them.",null,0,cdpef1o,1roeuz,askscience,top_week,4
patchgrabber,"It depends on the fish. Some fish are euryhaline, meaning they can live in a differing concentration of salt in water. Salmon are one such example; they are born in freshwater rivers but spend most of their lives in the sea, returning to spawn. But they have to go through an acclimation period where they slowly lower the concentration of salt in water. I wouldn't try this with your aquarium fish, though.",null,0,cdpj5ib,1roeuz,askscience,top_week,3
yankee333,"Probably not....

This depends on the species of fish, but most have physiologically adapted to the salinity of their environment. They have specialized ion channels in their cells that regulate osmotic pressure. When you change those gradients the cells can swell or burst, basically killing the fish.",null,1,cdpr6bz,1roeuz,askscience,top_week,1
NotFreeAdvice,"The answer has to do with the idea of bubble *nucleation*.

So, it turns out that the release of carbonation (ie. the bubbles) is thermodynamcally favorable.  This means that the CO2 would ""like"" to exit as bubbles.  

There is just one problem: to get a bubble, you must first form a bubble.  The *start* of bubble formation (what we call nucleation) is *not* thermodyanmically favorable.

The reason is entropic.  There is much more ordering at the surface of a bubble than inside the bulk bubble or the bulk water.  This presents a barrier to bubble formation that is related to the surface area of the bubble.

At the same time, there is a enthalpic reason to form bubbles, and this related to the *volume* of the bubble. 

The end result is this: when the bubbles are quite small, the effects from the surface area of the bubble, dominate the effects from the bulk of the bubble.  And the bubble will not form -- or forms very slowly.  

Once a bubble forms, then it will grow more easily.  So the bottleneck, if you will, is the initial formation of the bubble.  

By shaking the beverage, you introduce a bunch of gas bubbles into the beverage, and this provides sites for bubble growth -- and an increase in released carbonation.  Essentially, you are providing the mean by which to skip the slow ""nucleation"" step.

Hope that makes sense!",null,0,cdppqxy,1rofgc,askscience,top_week,2
Das_Mime,"Yes, there are stars that are very close to each other. In environments like the nucleus of our galaxy and globular clusters, stars can be packed extremely tight. Globular cluster cores can have dozens of stars within a cubic light year. By contrast, the nearest star to us (Proxima Centauri) is 4.2 light years away.",null,0,cdpcsvj,1roh5n,askscience,top_week,4
trebuday,"Yes, you can see storm clouds from the International Space Station!  See [this](http://i.huffpost.com/gen/1231373/thumbs/o-STORM-CLOUDS-900.jpg?6) cool photo, from July 4, 2013, showing storm clouds over the Atlantic ocean.  This was shot with a 50mm lens, which approximates the width of view of human vision quite well.

However, while you are seeing the bottom of a storm cloud, the ISS astronauts only see the top!  [This](http://www.youtube.com/watch?v=KaOC9danxNo) music video by former Commander Chris Hadfield of the ISS has some nice shots of the view of Earth from the ISS.

According to [wikipedia](http://en.wikipedia.org/wiki/Visual_acuity#Normal_vision), 20/20 vision is defined as being able to see an object 1 arc-minute wide.  This means a person with 20/20 vision can see something a little over an inch tall at 100 meters away.  The ISS orbits at 370km, which, according to [wolframalpha.com](http://www.wolframalpha.com/input/?i=one+arc-minute+at+370km), means someone looking down from ISS could see something as small as 107.6m wide, or about the size of an American Football field. 

",null,0,cdpdlnu,1roi9d,askscience,top_week,3
iorgfeflkd,"Yeah, the Earth gets hit all the time by cosmic rays which are mostly high energy protons. Older TV and computer monitors use beams of electrons to produce images on the screen. Free neutrons come from radioactive decay and nuclear reactors.",null,0,cdpdpya,1roisb,askscience,top_week,8
null,null,null,0,cdpfvyd,1roisb,askscience,top_week,3
Muh-Freedoms,"A loose proton flying around is called Hydrogen.
Neutrons may exist outside of a nucleus, but they are unstable and [decay](https://en.wikipedia.org/wiki/Free_neutron#Free_neutron_decay).
Electrons may be free as well; cathode ray tubs in your old TVs shoot electrons, for example.

",null,2,cdpdt9k,1roisb,askscience,top_week,3
joca63,"Neutrons are produced by nuclear decay in bombs, and reactors.
Electrons are produced in a similar manner (termed beta decay).
The bare helium nucleus is produced by alpha decay, again in nuclear reactions.
Protons are a bit different, they can also be expelled in radioactive decay, but they are also seen frequently in acid base reactions. To be clear, if a chemist uses H+ they are referring to the reactive equilivent. An example would be HCL + H2O --&gt; H+  + CL-  What actually happens would be more accurately described as HCL + H2O --&gt; H3O+  + CL-  With the proton being bound to the oxygen in a water molecule. The reason it is so frequently described as just H+ is that the speed at which the protons bounce from solvent molecule to solvent molecule is insanely fast. That proton doesnt have a ""home"" oxygen that it sticks around so it can be described as a free proton in the solvent
",null,0,cdq15pt,1roisb,askscience,top_week,1
remarcsd,"Seebeck

The materials with the greatest difference in Seebeck effect are the ones from which the greatest energy can be harvested. Of the current standard thermocouples, Type E (chromel constantan) gives the greatest output",null,0,cdpd38b,1roj3s,askscience,top_week,1
LukeSkyWRx,"The two primary material properties that can be manipulated for thermoelectric are thermal and electrical conductivity. Ideally for a given material you would want a low thermal conductivity and a high electrical conductivity. An easy way to decrease thermal conductivity is reducing the grain size of the material but that also typically hurts electrical conductivity so it depends on which property changes fastest.

As to what material is best, that depends on the temperature you want to operate at. The best material at one temperature will not be the best at a higher or lower temp so your heat source temp plays a big role.",null,0,cdpue22,1roj3s,askscience,top_week,1
gingerkid1234,"I don't know if that shape has a name.  However, shapes whose volume formula is unknown can have it calculated by integrating over the volume of the shape.  All that's required is knowing equations that describe the boundaries of that shape.  It's worth mentioning that this is how the volume equations of shapes you're familiar with are generally derived.  The following may be a bit confusing if you don't know calculus.  In case you don't, the basic principle of integrating a function is that you add the infinitesimally small heights under a curve to get an area, or the areas under a curve to get a volume.

I'll use the xy plane for the cross section, with the flat surface along x, and z for the length dimension of the shape, and assume that the rate of flattening is linear, and that it tapers to a point.  The easiest way to integrate it, it seems to me, is to break it up into triangles in the yz plane, and integrate along z.

I'll use r as the radius at the un-squished end, and L as the length.  I'll also put the center of the un-squished cross-section at the origin.  Now, on the xy plane, a y coordinate along the boundary can be expressed as y=sqrt(r^2 - x^2 ).  If the squishing of the shape is linear, the yz sections will be triangles with height 2 * sqrt(r^2 - x^2 ) and length L.  The area of each will be L * sqrt(r^2 - x^2 ).  A brief sanity check shows that the areas are 0 at x=r, where the height tapers from zero to zero.

By integrating these areas along x the volume can be determined.  The integral in question is the integral of L * sqrt(r^2 - x^2 ) with respect to x as x goes from -r to r, which is equal to V.

This integral isn't terribly easy to solve, but there's a way around actually doing it out.  By inspection, sqrt(r^2 - x^2) is just the y coordinate of a circle.  The integral of this from -r to r should just be the area of a semicircle, pi * r^2 /2.  And plugging the definite integral into wolframalpha, it is!

Plugging that back in and cancelling the 2, V = 1/2 * L * pi * r^2, or half the area of a cylinder.  And intuitively, that makes sense--each cross-section of it is a triangle with the same base and height as full rectangle.  And that shape made of rectangles is a cylinder.  So the volume will be half that.

An important practical note is that squishing a real-life cylinder will result in something significantly different, because just squishing the end won't produce a constant taper throughout the cylinder.

tl;dr it can be determined by summing all the arbitrarily thin cross-sectional volumes of the shape, each of which is an area times an arbitrarily small increment in the direction we're slicing.  By doing out this calculus, the area is A = 1/2 * L * pi * r^2 , or half the volume of a cylinder.  This makes sense when you think about how a cylinder and this shape differ.",null,36,cdpdfg9,1rokpj,askscience,top_week,176
bzishi,"Don't get drawn in, don't get drawn in. Damn.

The key point nobody is discussing is that when you pinch a cylinder, the circumference of the cross section remains the same. You can't simply pretend it is an ellipse where you vary the semi-minor axis from r to 0 while the semi-major axis stays the same. On a plus note, you don't have to use the pain-in-the-ass ellipse circumference equation for substitution, since you know the end points and can write a linear function. The flat 'ellipse' (or line) at the end will simply have a half-length of r*pi/2. The semi-major axis (x counting up from the pinch) will then be 

a = (r-r * pi/2) * x / L + r * pi/2. 

Let q = r * pi/2 =&gt; 

a = (r-q) * x/L + q =  rx/L -q(x-L)/L. 

The semi-minor axis will be 

b = rx/L

The area is then 

A(x)= pi * a * b = pi * (rx/L - q(x-L)/L)(rx/L) = pi/L^2 * (r^2 x^2 -qrx(x-L)) = pi/L^2 * (r^2 x^2 -qrx^2 + qrxL)

Integrating:

V = pi/L^2 * (1/3 * r^2 * L^3 -1/3qrL^3 + 1/2 * qrL^3 ) = pi * L * (1/3 * (r^2 - qr)+qr/2) = pi * L * (1/3 * r^2 + 1/6 * qr) = pi * L * (1/3 * r^2 +1 /12 * pi * r^2 ) = pi * r^2 * L * (1/3 +pi/12) =

V = 0.595 * pi * r^2 * L (my best estimate)

Check my work, because I might have had some Thanksgiving rum and we all know not to drink and derive.",null,4,cdpik0n,1rokpj,askscience,top_week,48
yeti_manetti,"For a ""tube"" of length L where the radius of the circular end is R, I get:
V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

So its the volume of the cylinder multiplied by approximately 0.595.

My approach differs from gingerkid1234. We make the same assumptions but the expression of y and z are simpler I believe. 

Lets orient the shape  with the end of the tube that is a line lying on the y axis centred at 0, and the tube going out in the positive x direction. 

The xy plane cross-section should be a quadrilateral through the points (0, (pi * R)/2), (L, R), (L, -R), (0, -(pi * R)/2). So the line from (0, (pi * R)/2) to (L, R) defines the semi-major axis of the elipses that make up zy cross sections of the tube. The line defined here is: y = (R/L) * (1 - pi/2) * x + (pi * R)/2.

In the xz plane, we have a triangle defined by (0,0), (L, R), (L, -R). So we see the semi-minor axis of the eliptical cross-sections is defined by z = (R/L) * x.

Using the formula for the area of an elipse A = pi * x * y where x is the length of the semi-major axis and y is the length of the semi-minor axis. [ a = b for a circle hence A = pi * r^2 ]

If we plug in the formulas for the lines defining the semi-major and semi-minor axes and integrate over x from 0 to L, we get:

V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

EDIT: Since I see gingerkid got something different, I looked it over again and found some differences. I'm open to critiques!",null,2,cdpdiqc,1rokpj,askscience,top_week,14
anon5005,"I am thinking that if the flattening is happening linearly in one dimension, then the area of the cross section each proportion r of the way from the squished end to the ordinary end has r times the area of the ordinary end.

Meaning, the cross section halfway up has half the area, and so-on. 

As always, the volume is the height times the average cross sectional area. Here, since the area goes linearly it is the same as the height times the area when it is cut at the half way point.

So the squishing cuts the volume in half.

You can see an example if you do it to other shapes, like a 1x1x1 cube.  Then you get what looks from the side like a triangle of area 1/2 thickened up to have thickness 1. So the volume is 1/2 x 1 = 1/2.",null,1,cdpejjk,1rokpj,askscience,top_week,3
brewsan,"Here's my take.. 

The cross-section is an ellipse (the very bottom being a circle and the very top being an ellipse with the y radius being 0 i.e. a line).  Let's put that on the XY plane. Note that x never changes from the radius(R) at the bottom, regardless of where along z we are. 

So the area of the ellipse is A=pi * x * y = pi * R * y

y changes as we travel up the z axis linearly from R at the bottom(z=0) to 0 at the top (Z=H) so y=R-(R/H)z = R * H/H -R/H * z = R/H(H-z)

Subbing in the A=pi * R * (R/H(H-z)) = pi * R²/H(H-z)
Now to get the volume we need to integrate over z (think of this as adding up all the cross sections of ellipses).

V = Int[z=0 to H]pi*R²/H(H-z)dz 

= pi * R²/H * Int[z=0 to H](H-z)dz 

= pi * R²/H (H * z - 1/2z²) |z=0 to H

= pi * R²/H (H² - 1/2*H²) 

= pi * R² * 1/2 * H

= 1/2 * pi * H * R²
",null,0,cdpfaz9,1rokpj,askscience,top_week,3
YouDoNotWantToKnow,"O      --

Orient your axis at the center of the circle at the bottom of the cylinder. The circle is radius r and let it exist in the x-y plane, so cylinder extends into the z plane. Let the line at the other end of the shape, the ""squish"" point, also be described by y=0, z=h where h is the height of the cylinder.

Now look at the y-z plane in slices for x in {-r,r}. For any given x in this range, the y-z plane contains a triangle. That triangle has a height of h and a base of 2\*sqrt(r^2 - x^2 ).

Now you have a simple calculus integral.

Integrate [sqrt(r^2 - x^2 ) \* h]/2 for x from 0 to r and multiply that by 2 (and to make that simpler, there's a /2 in the integral so just pull that out and divide it out right away).

There's probably a more elegant way of going from here, but to brute force it you can look up (or derive using trig substitution) that the indefinite integral of [A - x^2]^(1/2) is I = 1/2(x(A-x^2)^(1/2) + A tan^(-1)(x(A-x^2)^(-1/2)

Looks messy, but just plug in the integration points. A = r^2 btw, so the definite integral from 0 to r is I(r) - I(0). I(0) is easy, there are x's in both numerators, so I(0) = 0.

I(r) causes the first time to drop out, but the second term inside the inverse tangent goes to infinity. tan^(-1) of infinity is pi/2.

So the integral comes out to just A\*pi/2, don't forget the height constant we left out earlier and that A=r^2.. the final solution is:

V = h\*pi\*r^(2)/2.

Does that make sense? It's half the volume of a full cylinder. So that is the right direction (less). Is it correct? Well, do you think we could form the same shape out of the volume we took out? Think about each slice we made at the beginning. Initially they would have been rectangles instead of triangles right? What part of each rectangle is missing? The area of the triangle is b*h/2. The area of the rectangle is b*h. So the missing area is bh - bh/2 = bh/2. Half the rectangle. So it's pretty clear this makes sense and is the correct result.


And now I read the other top reply and it has a similar derivation and the same solution. Sweet.",null,0,cdpfd5k,1rokpj,askscience,top_week,3
CoorsLightNTits,"You have two sections here that you can effectively add together if you cannot come up with equations and integrate.  The shape is a cylinder sitting beneath a cone, if I am to understand your description accurately.  This is simple enough a manner to describe the volume. 

Cylinder volume = Vc = .25*(pi*hd^2)

Cone volume = V▲ = bh▲/3

This assumes straight sides with the pinched end. Introducing curvature maked this approximation less accurate, but it's useful enough to get close for most any application. 

Furthermore, to simplify, b = d/2, thus V▲ = dh▲/6

TL; DR Approximately V = .25*(pi*hd^2) + dh▲/6",null,0,cdpgvp7,1rokpj,askscience,top_week,3
null,null,null,0,cdpeg4o,1rokpj,askscience,top_week,1
Diogenes_Laertius,"Would anyone be able to get the equation for a 3D Plot of this shape? I would prefer it to be in the program ""Grapher"" on Macintosh. It will be used in a philosophy class to demonstrate how perspective changes conclusions. If you look at it from one direction, you see a disc. If you look from another, you see a triangle, and from a third angle you see a square. ",null,0,cdpgxnt,1rokpj,askscience,top_week,1
null,null,null,1,cdpg49w,1rokpj,askscience,top_week,1
gingerkid1234,"I don't know if that shape has a name.  However, shapes whose volume formula is unknown can have it calculated by integrating over the volume of the shape.  All that's required is knowing equations that describe the boundaries of that shape.  It's worth mentioning that this is how the volume equations of shapes you're familiar with are generally derived.  The following may be a bit confusing if you don't know calculus.  In case you don't, the basic principle of integrating a function is that you add the infinitesimally small heights under a curve to get an area, or the areas under a curve to get a volume.

I'll use the xy plane for the cross section, with the flat surface along x, and z for the length dimension of the shape, and assume that the rate of flattening is linear, and that it tapers to a point.  The easiest way to integrate it, it seems to me, is to break it up into triangles in the yz plane, and integrate along z.

I'll use r as the radius at the un-squished end, and L as the length.  I'll also put the center of the un-squished cross-section at the origin.  Now, on the xy plane, a y coordinate along the boundary can be expressed as y=sqrt(r^2 - x^2 ).  If the squishing of the shape is linear, the yz sections will be triangles with height 2 * sqrt(r^2 - x^2 ) and length L.  The area of each will be L * sqrt(r^2 - x^2 ).  A brief sanity check shows that the areas are 0 at x=r, where the height tapers from zero to zero.

By integrating these areas along x the volume can be determined.  The integral in question is the integral of L * sqrt(r^2 - x^2 ) with respect to x as x goes from -r to r, which is equal to V.

This integral isn't terribly easy to solve, but there's a way around actually doing it out.  By inspection, sqrt(r^2 - x^2) is just the y coordinate of a circle.  The integral of this from -r to r should just be the area of a semicircle, pi * r^2 /2.  And plugging the definite integral into wolframalpha, it is!

Plugging that back in and cancelling the 2, V = 1/2 * L * pi * r^2, or half the area of a cylinder.  And intuitively, that makes sense--each cross-section of it is a triangle with the same base and height as full rectangle.  And that shape made of rectangles is a cylinder.  So the volume will be half that.

An important practical note is that squishing a real-life cylinder will result in something significantly different, because just squishing the end won't produce a constant taper throughout the cylinder.

tl;dr it can be determined by summing all the arbitrarily thin cross-sectional volumes of the shape, each of which is an area times an arbitrarily small increment in the direction we're slicing.  By doing out this calculus, the area is A = 1/2 * L * pi * r^2 , or half the volume of a cylinder.  This makes sense when you think about how a cylinder and this shape differ.",null,36,cdpdfg9,1rokpj,askscience,top_week,176
bzishi,"Don't get drawn in, don't get drawn in. Damn.

The key point nobody is discussing is that when you pinch a cylinder, the circumference of the cross section remains the same. You can't simply pretend it is an ellipse where you vary the semi-minor axis from r to 0 while the semi-major axis stays the same. On a plus note, you don't have to use the pain-in-the-ass ellipse circumference equation for substitution, since you know the end points and can write a linear function. The flat 'ellipse' (or line) at the end will simply have a half-length of r*pi/2. The semi-major axis (x counting up from the pinch) will then be 

a = (r-r * pi/2) * x / L + r * pi/2. 

Let q = r * pi/2 =&gt; 

a = (r-q) * x/L + q =  rx/L -q(x-L)/L. 

The semi-minor axis will be 

b = rx/L

The area is then 

A(x)= pi * a * b = pi * (rx/L - q(x-L)/L)(rx/L) = pi/L^2 * (r^2 x^2 -qrx(x-L)) = pi/L^2 * (r^2 x^2 -qrx^2 + qrxL)

Integrating:

V = pi/L^2 * (1/3 * r^2 * L^3 -1/3qrL^3 + 1/2 * qrL^3 ) = pi * L * (1/3 * (r^2 - qr)+qr/2) = pi * L * (1/3 * r^2 + 1/6 * qr) = pi * L * (1/3 * r^2 +1 /12 * pi * r^2 ) = pi * r^2 * L * (1/3 +pi/12) =

V = 0.595 * pi * r^2 * L (my best estimate)

Check my work, because I might have had some Thanksgiving rum and we all know not to drink and derive.",null,4,cdpik0n,1rokpj,askscience,top_week,48
yeti_manetti,"For a ""tube"" of length L where the radius of the circular end is R, I get:
V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

So its the volume of the cylinder multiplied by approximately 0.595.

My approach differs from gingerkid1234. We make the same assumptions but the expression of y and z are simpler I believe. 

Lets orient the shape  with the end of the tube that is a line lying on the y axis centred at 0, and the tube going out in the positive x direction. 

The xy plane cross-section should be a quadrilateral through the points (0, (pi * R)/2), (L, R), (L, -R), (0, -(pi * R)/2). So the line from (0, (pi * R)/2) to (L, R) defines the semi-major axis of the elipses that make up zy cross sections of the tube. The line defined here is: y = (R/L) * (1 - pi/2) * x + (pi * R)/2.

In the xz plane, we have a triangle defined by (0,0), (L, R), (L, -R). So we see the semi-minor axis of the eliptical cross-sections is defined by z = (R/L) * x.

Using the formula for the area of an elipse A = pi * x * y where x is the length of the semi-major axis and y is the length of the semi-minor axis. [ a = b for a circle hence A = pi * r^2 ]

If we plug in the formulas for the lines defining the semi-major and semi-minor axes and integrate over x from 0 to L, we get:

V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

EDIT: Since I see gingerkid got something different, I looked it over again and found some differences. I'm open to critiques!",null,2,cdpdiqc,1rokpj,askscience,top_week,14
anon5005,"I am thinking that if the flattening is happening linearly in one dimension, then the area of the cross section each proportion r of the way from the squished end to the ordinary end has r times the area of the ordinary end.

Meaning, the cross section halfway up has half the area, and so-on. 

As always, the volume is the height times the average cross sectional area. Here, since the area goes linearly it is the same as the height times the area when it is cut at the half way point.

So the squishing cuts the volume in half.

You can see an example if you do it to other shapes, like a 1x1x1 cube.  Then you get what looks from the side like a triangle of area 1/2 thickened up to have thickness 1. So the volume is 1/2 x 1 = 1/2.",null,1,cdpejjk,1rokpj,askscience,top_week,3
brewsan,"Here's my take.. 

The cross-section is an ellipse (the very bottom being a circle and the very top being an ellipse with the y radius being 0 i.e. a line).  Let's put that on the XY plane. Note that x never changes from the radius(R) at the bottom, regardless of where along z we are. 

So the area of the ellipse is A=pi * x * y = pi * R * y

y changes as we travel up the z axis linearly from R at the bottom(z=0) to 0 at the top (Z=H) so y=R-(R/H)z = R * H/H -R/H * z = R/H(H-z)

Subbing in the A=pi * R * (R/H(H-z)) = pi * R²/H(H-z)
Now to get the volume we need to integrate over z (think of this as adding up all the cross sections of ellipses).

V = Int[z=0 to H]pi*R²/H(H-z)dz 

= pi * R²/H * Int[z=0 to H](H-z)dz 

= pi * R²/H (H * z - 1/2z²) |z=0 to H

= pi * R²/H (H² - 1/2*H²) 

= pi * R² * 1/2 * H

= 1/2 * pi * H * R²
",null,0,cdpfaz9,1rokpj,askscience,top_week,3
YouDoNotWantToKnow,"O      --

Orient your axis at the center of the circle at the bottom of the cylinder. The circle is radius r and let it exist in the x-y plane, so cylinder extends into the z plane. Let the line at the other end of the shape, the ""squish"" point, also be described by y=0, z=h where h is the height of the cylinder.

Now look at the y-z plane in slices for x in {-r,r}. For any given x in this range, the y-z plane contains a triangle. That triangle has a height of h and a base of 2\*sqrt(r^2 - x^2 ).

Now you have a simple calculus integral.

Integrate [sqrt(r^2 - x^2 ) \* h]/2 for x from 0 to r and multiply that by 2 (and to make that simpler, there's a /2 in the integral so just pull that out and divide it out right away).

There's probably a more elegant way of going from here, but to brute force it you can look up (or derive using trig substitution) that the indefinite integral of [A - x^2]^(1/2) is I = 1/2(x(A-x^2)^(1/2) + A tan^(-1)(x(A-x^2)^(-1/2)

Looks messy, but just plug in the integration points. A = r^2 btw, so the definite integral from 0 to r is I(r) - I(0). I(0) is easy, there are x's in both numerators, so I(0) = 0.

I(r) causes the first time to drop out, but the second term inside the inverse tangent goes to infinity. tan^(-1) of infinity is pi/2.

So the integral comes out to just A\*pi/2, don't forget the height constant we left out earlier and that A=r^2.. the final solution is:

V = h\*pi\*r^(2)/2.

Does that make sense? It's half the volume of a full cylinder. So that is the right direction (less). Is it correct? Well, do you think we could form the same shape out of the volume we took out? Think about each slice we made at the beginning. Initially they would have been rectangles instead of triangles right? What part of each rectangle is missing? The area of the triangle is b*h/2. The area of the rectangle is b*h. So the missing area is bh - bh/2 = bh/2. Half the rectangle. So it's pretty clear this makes sense and is the correct result.


And now I read the other top reply and it has a similar derivation and the same solution. Sweet.",null,0,cdpfd5k,1rokpj,askscience,top_week,3
CoorsLightNTits,"You have two sections here that you can effectively add together if you cannot come up with equations and integrate.  The shape is a cylinder sitting beneath a cone, if I am to understand your description accurately.  This is simple enough a manner to describe the volume. 

Cylinder volume = Vc = .25*(pi*hd^2)

Cone volume = V▲ = bh▲/3

This assumes straight sides with the pinched end. Introducing curvature maked this approximation less accurate, but it's useful enough to get close for most any application. 

Furthermore, to simplify, b = d/2, thus V▲ = dh▲/6

TL; DR Approximately V = .25*(pi*hd^2) + dh▲/6",null,0,cdpgvp7,1rokpj,askscience,top_week,3
null,null,null,0,cdpeg4o,1rokpj,askscience,top_week,1
Diogenes_Laertius,"Would anyone be able to get the equation for a 3D Plot of this shape? I would prefer it to be in the program ""Grapher"" on Macintosh. It will be used in a philosophy class to demonstrate how perspective changes conclusions. If you look at it from one direction, you see a disc. If you look from another, you see a triangle, and from a third angle you see a square. ",null,0,cdpgxnt,1rokpj,askscience,top_week,1
null,null,null,1,cdpg49w,1rokpj,askscience,top_week,1
U235EU,"Yes, it is known as the polar route. Virgin Airlines has been cleared to fly almost directly over the North Pole for a UK to Fiji flight. See more information here: 

http://www.dailymail.co.uk/news/article-2078301/Mind-sleigh-Airlines-given-permission-fly-North-Pole-time-slashing-hours-exotic-destinations.html",null,4,cdpcf1j,1rom21,askscience,top_week,13
howardcoombs,"Here is an old discussion thread showing various routes over the poles 
http://www.airliners.net/aviation-forums/general_aviation/read.main/1918010/",null,0,cdpewwa,1rom21,askscience,top_week,1
trebuday,"Nope!  Take a looks at wikipedia's article on [elevation](http://en.wikipedia.org/wiki/Elevation), which describes how the average elevation of all of the land is around 0.8km, whereas the average depth of the oceans is 3.7km.  Since the oceans comprise about 71% of the Earth's surface, (approximate 2.4:1 ratio of ocean to land), we can estimate that if all the land above sea level were put move to the bottom of the ocean, the average ocean depth would decrease by only 0.33km (to ~3.4km).  

I am now really interested in what the average elevation of the entire planet is...  ",null,1,cdpcq2c,1romjn,askscience,top_week,3
chicken_slaad,"Most water has various gases dissolved in it, like oxygen or carbon dioxide.  The warmer the water is, the less gas it is able to hold--that's why your soda foams up so much more when it's hot than when cold.  Also, the less pressure the water is under, the less gas it's able to hold--that's why your soda foams up when you open the can and release the pressure.

Water comes out of the tap, where it was cold and under pressure, and starts to warm up.  These gases come out of solution, forming bubbles.  The bubbles formed at the wall of the glass tend to stick there until they get big enough to break away, which sometimes never happens.",null,2,cdpd5xy,1ronfo,askscience,top_week,14
sammmmmmmmmm,"The air bubbles stick to the 'dirt' on your glass. Try drinking from that glass a couple of times, letting it dry inbetween, you should notice more and more bubbles forming as your glass gets 'dirtier'. These bubbles should mostly dissapear after properly washing the glass. ",null,11,cdpend5,1ronfo,askscience,top_week,3
chicken_slaad,"Most water has various gases dissolved in it, like oxygen or carbon dioxide.  The warmer the water is, the less gas it is able to hold--that's why your soda foams up so much more when it's hot than when cold.  Also, the less pressure the water is under, the less gas it's able to hold--that's why your soda foams up when you open the can and release the pressure.

Water comes out of the tap, where it was cold and under pressure, and starts to warm up.  These gases come out of solution, forming bubbles.  The bubbles formed at the wall of the glass tend to stick there until they get big enough to break away, which sometimes never happens.",null,2,cdpd5xy,1ronfo,askscience,top_week,14
sammmmmmmmmm,"The air bubbles stick to the 'dirt' on your glass. Try drinking from that glass a couple of times, letting it dry inbetween, you should notice more and more bubbles forming as your glass gets 'dirtier'. These bubbles should mostly dissapear after properly washing the glass. ",null,11,cdpend5,1ronfo,askscience,top_week,3
null,null,null,110,cdpdcox,1ronxr,askscience,top_week,513
yogurtsurprise,"&gt;why don’t your teeth heal on your own like other bones in your body?

The crown of teeth (the part that is visible) are made of two primary materials, enamel on the outer surface and dentin which makes up most of the tooth structure (with pulp in the centre which contains blood vessels, lymph and nerves). When the crown forming there are made of two major cells types. Ameloblasts and odontoblasts. First the dentin in formed by the odontoblasts at the centre of the tooth and dentin is formed outwards. The ameloblasts start from the DEJ (dentinoenamel junction where enamel and dentin meet) and move towards the outside of the crown. They are lost during tooth eruption (when the tooth comes through the gums). 
The tooth is unable to heal since it cannot produce enamel after eruption because there are no cells to make new enamel.

Dentin on the other hand can and does form after the tooth is already in the mouth (secondary dentin forms after the tooth is in the mouth and more dentin is made towards the centre of the tooth; tertiary dentin is made in response to inflammation of the pulp often due to cavities). So in one sense the tooth is healing. However, it doesn't heal like bones since often a tooth fractures in the middle of the tooth and the odontoblasts cannot move through already formed dentin to make new tooth structure on top of old tooth structure (and as the ameloblasts are gone, no new enamel can be produced). 

While I'm not as familiar with bone healing, bone is a vascularized (blood containing) cellular tissue. Osteoblasts (which synthesis new bone), and osteoclasts (which eat old bone) are always working even when there is no damage. So when the bone is broken, these cells can work to remodel and repair the bone. 

I hope this helps answer your question",null,29,cdpf0fv,1ronxr,askscience,top_week,164
Swoondoc,"Saliva is there to help protect your teeth and even has some immunoglobulin A in it along with enzymes.  So you can make the argument that your immune system DOES fight off bacteria that causes cavities.

The constant flow of saliva helps clean food debris from your teeth, and the keep the pH levels slighly basic to cancel out any acids that can wear away at the enamel.
",null,22,cdpf1ca,1ronxr,askscience,top_week,107
null,null,null,11,cdpeut3,1ronxr,askscience,top_week,34
handle8,"Your body does have some minimal defenses against bacteria. Saliva has some antibodies, specifically IgA antibodies which help prevent bacterial absorption, but mostly through mucous membranes rather than bone.

Saliva also has defensins or proteins that have anti-bacterial activity.

However, the mouth is an extremely unsterile area. There are a myriad of bacteria that grow in the mouth naturally. On teeth, bacteria can form biofilms, or collections of bacteria that become thick and have special secretions that prevent binding of antibodies or other natural host defenses. This is a problem even with decent blood supply to provide these host defenses to the inside of teeth.",null,4,cdpeyob,1ronxr,askscience,top_week,23
El_Dentistador,"So far all the other comments are spot on.  The cells that form the enamel and dentin in teeth are ameleoblasts and odontoblasts respectively.  The ameleoblasts work from the DEJ out while the odontoblasts work from the DEJ in.  The cell bodies of the odontoblasts are mostly at the edge of the pulp only extending processes through tubules within the dentin, and your ameleoblasts are long gone once they finish forming the enamel. This leaves enamel and dentin without any significant intrinsic method of healing, and your immune system has no access to these two tissues either as they are avascular.
By the time your immune system gets to see the infection the bacteria have breached the pulp chamber. Much like the brain, pulpal tissue  cannot handle inflammation well since it's completely encased by hard tissue.  Your own inflammatory response increases the intrapulpal pressure dramatically, and it hurts.  The pulpal tissue will necrose and the bacteria now have an easy path through the canals to the periapical tissues, which can lead to more serious infections like cellulitis and osteomyelitis.  There have actually been cases where odontogenic infections have led to fatalities, which should never happen.  The most famous case is Deamonte Driver. 
",null,1,cdpgs6c,1ronxr,askscience,top_week,13
mkav8,"Also the teeth have an outer layer called the pellicle. The pellicle is where plaque formation first starts and eventually can become calculus. A question I thought of when first beginning cariology (study of cavities) was why don't antibiotics stop caries? And the answer is because bacteria in plaque and calculus form a biofilm. This biofilm forms a protective layer and also has channels between adjacent biofilms. These channels allow the transfer of genes which can cause some minor resistance in these strains. 

Enamel is formed by ameloblasts which deposit enamel in an inside to outside direction. After the enamel is deposited the ameloblasts either die off or can form a cuticle layer. Dentin however is formed by odontoblasts and can be deposited through a lifetime in an inward direction. So in a way teeth can heal to a small degree because if there is insult or resorption of dentin possibly due to inflammation or trauma, the tooth can lay down tertiary dentin in order to repair it as long as it is on the pulpal side. ",null,0,cdph1sv,1ronxr,askscience,top_week,7
We_have_no_future,"A reason why we get cavities is because several thousands of years ago our diet changed. Evolutionary we were not designed to base our diet on carbs (such as rice, beans, corn, and other cereals). 

http://www.abc.net.au/science/articles/2013/02/18/3691558.htm
&gt;Mesolithic hunter-gatherers living on a meat-dominated, grain-free diet had much healthier mouths that we have today, with almost no cavities and gum disease-associated bacteria, a genetic study of ancient dental plaque has revealed.",null,10,cdpfj24,1ronxr,askscience,top_week,16
ThatInternetGuy,"&gt; Why doesn’t your immune system fight off the bacteria that causes cavities?

Saliva contain enzymes such as Lysozyme that kill bacteria. Saliva also contains mucus which contains IgA antibodies. While antibodies don't kill bacteria, they serve the first line defend by finding and marking the bacteria as unsafe foreign bodies so that the white blood cells can finish them off as soon as possible *if* the bacteria or virus from our mouth enter our body (through gum for instance).

Cavities are caused by acid in your mouth, consumed by you (phosphoric acid in sodas) and converted from sugar to acid by some bacteria in your mouth.

&gt; why don’t your teeth heal on your own like other bones in your body? 

Tooth enamel do heal. It's called remineralization. The flouride ions in toothpaste together with minerals in your saliva help regenerate teeth enamel. [More info](http://health.howstuffworks.com/wellness/oral-care/procedures/remineralization-of-teeth.htm)",null,2,cdpfy0z,1ronxr,askscience,top_week,6
jgrun,"Just to be clear its not the bacteria in your mouth itself that directly causes cavities. Rather its the byproduct of their metabolic processes, which are acidic, that wear away at your enamel. Some people are more prone to cavities than others because their saliva can neutralize that acid less effectively.",null,0,cdph40n,1ronxr,askscience,top_week,4
Dymethyltryptamine,"I need to mention that tooth decay is not only the result of bacteria and acidity. You can have a really good dental hygiene but still suffer from tooth decay because of dietary deficiencies. You need certain minerals such as calcium and phosphorous along with vitamins and other minerals that regulate the absorption of these. If lacking, your body will leech minerals from your teeth to deposit into your skeleton. ",null,0,cdpivip,1ronxr,askscience,top_week,4
crankfair,"We did not evolve to eat so much refined grain, which sticks between teeth.  The worst I have ever had is corn flakes or frosted flakes- they leave a layer ON your molars, not just in between.

That becomes a food source for the bacteria.  So our immune systems cant handle it because they have not been selected to since that was not a factor in our traditional environment of evolutionary adaptation.",null,0,cdpkbn4,1ronxr,askscience,top_week,3
flansiro2zero,"I have never had a cavity and im now into my 4th decade on this earth. I always wondered about the science behind this. I brush my teeth once a day if I remember and that's about the height of my dental hygiene routine.
A dentist told me a few years ago that I was part of a small proportion of the worlds population that was ""immune"" to cavities. Any credence to this theory or am I just a lucky guy?",null,0,cdpmxql,1ronxr,askscience,top_week,1
PumPumPumpkin,"When enamel gets damaged/broken, the stuff that makes it gets damaged/broken too. But it also starts to get harder over time as soon as it starts to get damaged, so that's pretty cool I guess. 

And our immune system can't exactly reach the outside of our teeth. Unless you would want us to suddenly start oozing some sort of immune-system goo out of our teeth/tongue to clean them, that's why we have tooth-paste, like a friend for your immune system. Saliva helps too, but not a lot in the long run.

It's not as sciencey as some other answers, but just felt like chipping in.",null,2,cdpgwrw,1ronxr,askscience,top_week,1
wampa-stompa,"Your teeth will rebuild enamel as long as your saliva has not become acidic (due to too much sugar being broken down) and fluoride is present. Also, there can't be a hard layer of plaque in the way for obvious reasons. This is why brushing and flossing is so important, and why water supplies are treated with fluoride (not mind control). It pays to keep this in mind.",null,1,cdplkyv,1ronxr,askscience,top_week,1
InfinitePS,"No one here has pointed out the importance of the oral microflora. It seems that ""good"" bacteria that coevolved with humans and reside in our mouths actually have the power to fight off other ""bad"" bacteria. Of course, fluoride in our toothpaste kills off all bacteria whether good or bad. In fact, a particular strain of bacterium Lactobacillus reuteri has been scientifically shown to reduce tooth decay. 
This strain is patented and I have no personal interest in promoting it (One can assume that there are a lot of other bacteria that behave similarly). Keep in mind, we live in a capitalist society; I may be a little cynical, but the American dentist association really has no particular interest in finding real strategies to fight off tooth decay. But it seems that the oral microflora may definitely play a huge role.",null,1,cdpm2vh,1ronxr,askscience,top_week,1
null,null,null,9,cdpgd9k,1ronxr,askscience,top_week,5
zadzad,"Here's whats really going on.

Look up phytic acid and cavities in google. Mostly its bread and nuts that cause cavities as it eats enamel. Once the enamel is eroded sugar will quickly eat to the pulp (sugar is also acidic but much weaker than those found in oatmeal, wholewheat bread etc.). 

Conversely, if you have tartar thats a more alkaline condition and has different health problems. Not cavities though more gingivitis.

The answers in my post will pretty much reverse any tooth sensitivity. You can try eating oatmeal frequently and see how your teeth feel. Mind you milk is alkaline so it will slow the effect but not stop it.

",null,18,cdpf8fc,1ronxr,askscience,top_week,5
null,null,null,110,cdpdcox,1ronxr,askscience,top_week,513
yogurtsurprise,"&gt;why don’t your teeth heal on your own like other bones in your body?

The crown of teeth (the part that is visible) are made of two primary materials, enamel on the outer surface and dentin which makes up most of the tooth structure (with pulp in the centre which contains blood vessels, lymph and nerves). When the crown forming there are made of two major cells types. Ameloblasts and odontoblasts. First the dentin in formed by the odontoblasts at the centre of the tooth and dentin is formed outwards. The ameloblasts start from the DEJ (dentinoenamel junction where enamel and dentin meet) and move towards the outside of the crown. They are lost during tooth eruption (when the tooth comes through the gums). 
The tooth is unable to heal since it cannot produce enamel after eruption because there are no cells to make new enamel.

Dentin on the other hand can and does form after the tooth is already in the mouth (secondary dentin forms after the tooth is in the mouth and more dentin is made towards the centre of the tooth; tertiary dentin is made in response to inflammation of the pulp often due to cavities). So in one sense the tooth is healing. However, it doesn't heal like bones since often a tooth fractures in the middle of the tooth and the odontoblasts cannot move through already formed dentin to make new tooth structure on top of old tooth structure (and as the ameloblasts are gone, no new enamel can be produced). 

While I'm not as familiar with bone healing, bone is a vascularized (blood containing) cellular tissue. Osteoblasts (which synthesis new bone), and osteoclasts (which eat old bone) are always working even when there is no damage. So when the bone is broken, these cells can work to remodel and repair the bone. 

I hope this helps answer your question",null,29,cdpf0fv,1ronxr,askscience,top_week,164
Swoondoc,"Saliva is there to help protect your teeth and even has some immunoglobulin A in it along with enzymes.  So you can make the argument that your immune system DOES fight off bacteria that causes cavities.

The constant flow of saliva helps clean food debris from your teeth, and the keep the pH levels slighly basic to cancel out any acids that can wear away at the enamel.
",null,22,cdpf1ca,1ronxr,askscience,top_week,107
null,null,null,11,cdpeut3,1ronxr,askscience,top_week,34
handle8,"Your body does have some minimal defenses against bacteria. Saliva has some antibodies, specifically IgA antibodies which help prevent bacterial absorption, but mostly through mucous membranes rather than bone.

Saliva also has defensins or proteins that have anti-bacterial activity.

However, the mouth is an extremely unsterile area. There are a myriad of bacteria that grow in the mouth naturally. On teeth, bacteria can form biofilms, or collections of bacteria that become thick and have special secretions that prevent binding of antibodies or other natural host defenses. This is a problem even with decent blood supply to provide these host defenses to the inside of teeth.",null,4,cdpeyob,1ronxr,askscience,top_week,23
El_Dentistador,"So far all the other comments are spot on.  The cells that form the enamel and dentin in teeth are ameleoblasts and odontoblasts respectively.  The ameleoblasts work from the DEJ out while the odontoblasts work from the DEJ in.  The cell bodies of the odontoblasts are mostly at the edge of the pulp only extending processes through tubules within the dentin, and your ameleoblasts are long gone once they finish forming the enamel. This leaves enamel and dentin without any significant intrinsic method of healing, and your immune system has no access to these two tissues either as they are avascular.
By the time your immune system gets to see the infection the bacteria have breached the pulp chamber. Much like the brain, pulpal tissue  cannot handle inflammation well since it's completely encased by hard tissue.  Your own inflammatory response increases the intrapulpal pressure dramatically, and it hurts.  The pulpal tissue will necrose and the bacteria now have an easy path through the canals to the periapical tissues, which can lead to more serious infections like cellulitis and osteomyelitis.  There have actually been cases where odontogenic infections have led to fatalities, which should never happen.  The most famous case is Deamonte Driver. 
",null,1,cdpgs6c,1ronxr,askscience,top_week,13
mkav8,"Also the teeth have an outer layer called the pellicle. The pellicle is where plaque formation first starts and eventually can become calculus. A question I thought of when first beginning cariology (study of cavities) was why don't antibiotics stop caries? And the answer is because bacteria in plaque and calculus form a biofilm. This biofilm forms a protective layer and also has channels between adjacent biofilms. These channels allow the transfer of genes which can cause some minor resistance in these strains. 

Enamel is formed by ameloblasts which deposit enamel in an inside to outside direction. After the enamel is deposited the ameloblasts either die off or can form a cuticle layer. Dentin however is formed by odontoblasts and can be deposited through a lifetime in an inward direction. So in a way teeth can heal to a small degree because if there is insult or resorption of dentin possibly due to inflammation or trauma, the tooth can lay down tertiary dentin in order to repair it as long as it is on the pulpal side. ",null,0,cdph1sv,1ronxr,askscience,top_week,7
We_have_no_future,"A reason why we get cavities is because several thousands of years ago our diet changed. Evolutionary we were not designed to base our diet on carbs (such as rice, beans, corn, and other cereals). 

http://www.abc.net.au/science/articles/2013/02/18/3691558.htm
&gt;Mesolithic hunter-gatherers living on a meat-dominated, grain-free diet had much healthier mouths that we have today, with almost no cavities and gum disease-associated bacteria, a genetic study of ancient dental plaque has revealed.",null,10,cdpfj24,1ronxr,askscience,top_week,16
ThatInternetGuy,"&gt; Why doesn’t your immune system fight off the bacteria that causes cavities?

Saliva contain enzymes such as Lysozyme that kill bacteria. Saliva also contains mucus which contains IgA antibodies. While antibodies don't kill bacteria, they serve the first line defend by finding and marking the bacteria as unsafe foreign bodies so that the white blood cells can finish them off as soon as possible *if* the bacteria or virus from our mouth enter our body (through gum for instance).

Cavities are caused by acid in your mouth, consumed by you (phosphoric acid in sodas) and converted from sugar to acid by some bacteria in your mouth.

&gt; why don’t your teeth heal on your own like other bones in your body? 

Tooth enamel do heal. It's called remineralization. The flouride ions in toothpaste together with minerals in your saliva help regenerate teeth enamel. [More info](http://health.howstuffworks.com/wellness/oral-care/procedures/remineralization-of-teeth.htm)",null,2,cdpfy0z,1ronxr,askscience,top_week,6
jgrun,"Just to be clear its not the bacteria in your mouth itself that directly causes cavities. Rather its the byproduct of their metabolic processes, which are acidic, that wear away at your enamel. Some people are more prone to cavities than others because their saliva can neutralize that acid less effectively.",null,0,cdph40n,1ronxr,askscience,top_week,4
Dymethyltryptamine,"I need to mention that tooth decay is not only the result of bacteria and acidity. You can have a really good dental hygiene but still suffer from tooth decay because of dietary deficiencies. You need certain minerals such as calcium and phosphorous along with vitamins and other minerals that regulate the absorption of these. If lacking, your body will leech minerals from your teeth to deposit into your skeleton. ",null,0,cdpivip,1ronxr,askscience,top_week,4
crankfair,"We did not evolve to eat so much refined grain, which sticks between teeth.  The worst I have ever had is corn flakes or frosted flakes- they leave a layer ON your molars, not just in between.

That becomes a food source for the bacteria.  So our immune systems cant handle it because they have not been selected to since that was not a factor in our traditional environment of evolutionary adaptation.",null,0,cdpkbn4,1ronxr,askscience,top_week,3
flansiro2zero,"I have never had a cavity and im now into my 4th decade on this earth. I always wondered about the science behind this. I brush my teeth once a day if I remember and that's about the height of my dental hygiene routine.
A dentist told me a few years ago that I was part of a small proportion of the worlds population that was ""immune"" to cavities. Any credence to this theory or am I just a lucky guy?",null,0,cdpmxql,1ronxr,askscience,top_week,1
PumPumPumpkin,"When enamel gets damaged/broken, the stuff that makes it gets damaged/broken too. But it also starts to get harder over time as soon as it starts to get damaged, so that's pretty cool I guess. 

And our immune system can't exactly reach the outside of our teeth. Unless you would want us to suddenly start oozing some sort of immune-system goo out of our teeth/tongue to clean them, that's why we have tooth-paste, like a friend for your immune system. Saliva helps too, but not a lot in the long run.

It's not as sciencey as some other answers, but just felt like chipping in.",null,2,cdpgwrw,1ronxr,askscience,top_week,1
wampa-stompa,"Your teeth will rebuild enamel as long as your saliva has not become acidic (due to too much sugar being broken down) and fluoride is present. Also, there can't be a hard layer of plaque in the way for obvious reasons. This is why brushing and flossing is so important, and why water supplies are treated with fluoride (not mind control). It pays to keep this in mind.",null,1,cdplkyv,1ronxr,askscience,top_week,1
InfinitePS,"No one here has pointed out the importance of the oral microflora. It seems that ""good"" bacteria that coevolved with humans and reside in our mouths actually have the power to fight off other ""bad"" bacteria. Of course, fluoride in our toothpaste kills off all bacteria whether good or bad. In fact, a particular strain of bacterium Lactobacillus reuteri has been scientifically shown to reduce tooth decay. 
This strain is patented and I have no personal interest in promoting it (One can assume that there are a lot of other bacteria that behave similarly). Keep in mind, we live in a capitalist society; I may be a little cynical, but the American dentist association really has no particular interest in finding real strategies to fight off tooth decay. But it seems that the oral microflora may definitely play a huge role.",null,1,cdpm2vh,1ronxr,askscience,top_week,1
null,null,null,9,cdpgd9k,1ronxr,askscience,top_week,5
zadzad,"Here's whats really going on.

Look up phytic acid and cavities in google. Mostly its bread and nuts that cause cavities as it eats enamel. Once the enamel is eroded sugar will quickly eat to the pulp (sugar is also acidic but much weaker than those found in oatmeal, wholewheat bread etc.). 

Conversely, if you have tartar thats a more alkaline condition and has different health problems. Not cavities though more gingivitis.

The answers in my post will pretty much reverse any tooth sensitivity. You can try eating oatmeal frequently and see how your teeth feel. Mind you milk is alkaline so it will slow the effect but not stop it.

",null,18,cdpf8fc,1ronxr,askscience,top_week,5
remarcsd,"Providing that the heat is not lost quicker than it can be created--so a really well insulated container may be required.

Note: centrifugal pumps--really common--have an impeller in them. If there is no liquid flow through the pump, and the impeller remains spinning in the same volume of liquid, it may only take a few minutes until the liquid is dangerously hot.",null,0,cdpcjv2,1ronyp,askscience,top_week,6
wesramm,"Immediately.  Adding work is thermodynamically equivalent to adding heat in increasing the internal energy of a fluid.  Since heat transfer takes time, the excess heat will escape to the environment and reach equilibrium, but instantaneously, the temperature will certainly increase.  A whole slew of Victorian era scientists in the US, UK and Germany performed careful experiments with paddle-wheels immersed in water tanks.  The paddle wheels were powered by falling weights, and to within a very high degree of correlation, it was found that the amount of temperature increase was proportional to the work added to the system following the first law of Thermodynamics.  Here is a link for you that talks about it quite nicely:
[Joule](http://en.wikipedia.org/wiki/James_Prescott_Joule)",null,0,cdpjng1,1ronyp,askscience,top_week,4
leftoveroxygen,"Yes.

Source:

I once made a camp shower with a 250-watt submersible pump in a 5-gallon bucket.

To my surprise, the pump became a *very effective* 250-watt water heater if I let the water circulate for about 20 minutes. All that energy has to go somewhere, and it ends up as heat.",null,1,cdpffco,1ronyp,askscience,top_week,2
darthmunkeys,"It will approach and equilibrium temperature assuming it is not insulated or perfectly insulated. Basically it will only get to room temperature. the shaking you do to it will not noticeably change it.

If the water starts cold then it will tend toward room temp, in a non insulated canteen. In a perfect insulated canteen the air and water would reach an equilibrium temp for just the inside of the perfect canteen. But in the real world, the outside air transfers energy into the canteen wall which is transferred to the water on the surface of the canteen. By mixing the water, through shaking the canteen, the water more quickly gains the energy from the canteen wall, thus bring the temperature up. 

In you could shake it extremely fast, then the energy you supply by shaking becomes more and more of a factor in the overall temperature of the water. If you could shake it at near lightspeed, which would mean lots of acceleration changes, then the water might turn into a gas then plasma as you shake the molecule apart into ions, at this point though shaking anything would do that. 


TL;DR shaking the canteen will make cold water reach room temp faster than letting it sit. It is a convection thing. ignore my last paragraph, im tired. ",null,2,cdpcq3y,1ronyp,askscience,top_week,3
MarineLife42,"When they say things in documentaries like 'sharks can smell blood for five kilometres' then this is at least misleading.  
Smelling is a chemical interaction so to do that, the molecules that make up the odor must be present in the olfactory organ - at zero distance. 
However, sharks have a very acute sense of smell and can smell blood that has been greatly diluted in water. Thus they can follow a trace of blood for several kilometres that will (hopefully) lead them to a meal.  
Like you and me only being able to smell something downwind from a source, sharks can only pick up a smell that a current brought to them, or was artificially laid out by people on a moving boat throwing fish remains into the water. ",null,0,cdpxd5q,1roywu,askscience,top_week,1
Entropius,"Not to the best of my knowledge.  I've seen different sources and textbooks give slightly different ranges for even common visible colors.  They'd disagree on exactly where green or blue begin &amp;amp;amp;amp; end.  But not by much, mind you.


You might think this would be a problem for scientists since they might disagree on their definitions for x-Ray ranges but in practice there's no problem since you're going to be referring to light by it's exact numeral wavelength, not broad terms like ""x-Ray"".",null,0,cdpr0t4,1royxd,askscience,top_week,2
Manzikert,"There's no fundamental difference between different categories of EM radiation. Energy increases proportionally with frequency/inversely with wavelength, regardless of what arbitrary category it slots into. ",null,0,cdpvfj0,1royxd,askscience,top_week,1
The-Grim-Reefer,"The dinosaurs never really went extinct. Almost all of today's modern birds are a descendant of the dinosaur *Archaeopteryx*. They just look much different now. Certain qualities allow organisms to live when the environment changes. If a species or even a select few members of a species can deal with a changing environment and reproduce they will live on. Most dinosaurs could not do this in the environment presented to them so they died off. Some lived on and genetic mutations throughout members of these species have changed their phenotypic qualities. In order for a species  to ""re-evolve"" after it has died off would require an organism to go through the same genetic mutations that originally brought that particular species to life. 
The chances of this actually happening are slim to none. If the environment is the reason for extinction, the qualities possessed by that species are not advantageous to qualities held by other organisms, therefore the other species will live on and continue to evolve with different phenotypic qualities than the ones held by the dinosaurs",null,0,cdpnejy,1royyw,askscience,top_week,3
GenL,"Dinosaurs evolved from an ancient reptile that no longer exists. Current reptiles might superficially resemble those dino ancestors, but they are very different. The starting point that you're imagining revisiting doesn't exist any more.

Could a new family of giants descended from reptiles arise again? Yes. But they wouldn't be dinosaurs. They would end up being different in many ways.",null,0,cdpqq81,1royyw,askscience,top_week,3
The-Grim-Reefer,No it actually helps to moisten the turkey meat. The salt from the solution gets absorbed by the muscle tissue which loosens up the fiber proteins. This creates more space within the meat causing it to swell with the juices of the solution that are pulled into the swelling meat.,null,0,cdpl4i2,1rozc0,askscience,top_week,3
yankee333,"Cells have many ion channels -&gt; the sodium/chloride diffuse as well, into the cells, bringing water with them osmotically.

Theoretically, if you blocked all sodium or chloride channels first, with drugs, you could dehydrate your turkey very well.",null,1,cdpr4rl,1rozc0,askscience,top_week,1
patchgrabber,"This happens more often than you'd think. Consider naked mole rats' eyes: they live underground in complete darkness nearly their entire lives, but they have non-functional (or extremely limited functional) eyes rather than no eyes.",null,0,cdpk4o2,1rozdg,askscience,top_week,3
yankee333,"No such thing as ""devolution"". Things are selected for and then not selected for. Removal of physical features over time indicates that their presence is not being selected for. This isn't devolution, but rather another form of evolution.",null,2,cdpr3h4,1rozdg,askscience,top_week,3
haysoos2,"There are many examples of traits that were advantageous for an organism being selected against later in the phylogeny, and the population losing that trait.

One of the more visible ones is flightless birds.  There are numerous lineages of birds - penguins, ratites, and many island species - who have secondarily lost the ability to fly that their ancestors had.  

In the case of flight, the metabolic costs to maintain that ability can be quickly outweighed if the advantages this gives (especially predator avoidance) are no longer required.

In our own lineage, the massive teeth and powerful jaw muscles that some of our ancestors had were no longer an advantage, and were selected against.  ",null,0,cdpkeed,1rozdg,askscience,top_week,1
anthropophobe,"My answer is going to be off topic.  Here are three examples of regression of technology:

1. Supersonic passenger aircraft (the Concorde) no longer exist.

2. There is still not a plane in regular service that is as fast as the SR-71 was.

3. The space shuttle was a financial sink hole and is now gone.",null,1,cdpv54v,1rozgj,askscience,top_week,1
wazoheat,"I can think of a few problems meteorologically:

* [The Mediterranean Sea is an important source of atmospheric moisture for southern Europe](http://www.atmos-chem-phys.net/10/5089/2010/acp-10-5089-2010.html). With it removed, not only would the resulting land be desert, but much of Europe's existing arable land would become desert as well.
* Not only would the resulting area be dry, it would be incredibly hot. [Much of the Mediterranean is over 1000 m (3300 ft) deep](http://www.geologie.ens.fr/spiplabocnrs/IMG/gif/CarteEastmed.gif), some of it more than 5000 m deep. With the sea removed, there would now be an incredibly large valley as much as 5000 m (16400 ft) deep. As wind blew down into this newly formed, incredibly large valley, it would heat up due to compression according to the [dry adiabatic lapse rate](https://en.wikipedia.org/wiki/Lapse_rate#Dry_adiabatic_lapse_rate), which is about 10°C (18°F) per 1000m. That means that the air temperature in the deepest parts of the valley could potentially reach 70-80°C (160-180°F) on hot days!",null,0,cdpgko7,1rozo5,askscience,top_week,8
Gargatua13013,"Sure, it is even believed it has happened in the past, a mere 6 or 7 MY ago (see: http://en.wikipedia.org/wiki/Messinian_salinity_crisis).

What you have to remember is that the mediterranean basin is, overall, evaporitive. This means that the annual sum of the water from all the streams and rivers draining into it is less than the amount lost annually to evaporation. The difference is made up by seawater coming in through the Gibraltar straights.

So all you'd have to do is dam up the Gibraltar straights and the med basin would evaporate down to the seafloor; exception made of a few residual ""Dead Sea"" type hypersaline lakes. 

But would you want to?

wazoheat brings up some excellent meteorological caveats.

Geologically, you've got to wonder just how fertile that ground would be. All the current salt load of the Med would precipitate, leaving behind thick layers of evaporite minerals (Salt, gypsum and anhydrite). And given the omnipresence of Messinian evaporites, I doubt you'd have much joy hoping for a usefefull aquifer.

So you'd basically get an ultrahot hypersaline desert bordering a rapidly desertifying southern Europe.",null,0,cdpk57n,1rozo5,askscience,top_week,4
Mightycoz,"TL;DR: Yes. Satellites in LEO and MEO don't see this as a major perturbation since Earth's masstrons and atmospheric drag dominate their orbits, but satellites in GEO must correct their orbits or their inclination gets amplified, requiring north-south station keeping maneuvers. 

Since the Moon is not orbiting Earth's equator, it drags at satellites either north or south, depending on its relative position as the Earth rotates. Over time this can become significant, moving the satellite ""off station"" and out of its assigned ""box"", which would require repointing of ground satellite dishes. There can also be east-west perturbation, which combines with other orbital effects to drag satellites off station. Uncontrolled GEO satellites are considered a Bad Thing, since that requires everyone else's satellites to predict conjunctions and take avoidance steps.",null,0,cdpkmhc,1rp09t,askscience,top_week,7
Davecasa,"20/20 vision is an angular resolution of one arc minute. A human pupil runs into the diffraction limit at 15-20 arc seconds, and it's impossible to do better without having a larger eye. 15-20 arc seconds is equivalent to 20/5 to 20/7.",null,0,cdprtiy,1rp0s1,askscience,top_week,3
The-Grim-Reefer,"There really is no limit to how good someone's eyesight can be. The 20/X scale means that the person being tested can see things at 20 feet that an average person can see at X feet. So someone with 20/10 can see something at 20 feet that the average person can see at 10 feet. 20/10 is the best vision on the visual acuity scale that we use. Although Theoretically someone could have 20/1 vision, but I have never heard of an instance where someone exceeded 20/10",null,0,cdpl0k0,1rp0s1,askscience,top_week,1
jayman419,"You might want to try /r/asksciencefiction. There are theories for using diamagnetism to generate a field that may simulate gravity, but it's probably unsafe for humans, definitely unsafe for any ferromagnetic materials. Or some kind of sci-fi gravity generator/controller.

But mass and/or motion are the only two that are really solid and you've already ruled them out.

Why not have the mission parameters begin with a constant acceleration, then some sort of accident or collision which knocks the ship into the eccentric orbit and also takes out the gravity. Then they can use like magnetic boots or something after they flail around for a bit.

edit: That way you can work in the set pieces you have in mind specifically taking place in gravity, explain the orbit and keep the ship in the solar system, still have the characters able to move around the environment throughout the game, and not have to go too far from the hard sci-fi approach you want to use.",null,0,cdpjzp2,1rp0ya,askscience,top_week,1
in4real,"Acceleration and gravity are equivalent from the subject's point of view.

Linear acceleration and centripetal force are both forms of acceleration (the later is a change in the velocity vector which by definition is acceleration).

Any change in velocity vector is going to simulate acceleration.

Acceleration aside I suppose you could recreate gravity on the ship with a captured black hole or maybe some neutron star material.  You'd have to make up some way to contain it.  Magnetic field or some such thing.",null,1,cdpjztw,1rp0ya,askscience,top_week,1
OnceReturned,"Monsanto started a company called Beelogics to developed genetic technologies to protect bees, mostly with an eye towards Colony Collapse Disorder and Israeli Acute Paralysis Virus, which pesticides likely play a roll in but which seem to involve a complex interplay of a variety of factors.  Their website is here:

http://www.beeologics.com/

They're working primarily on gene expression manipulation through RNA interference.  So, different than one what you're talking about, but the closest thing to it going on today as far as I know.

We simply don't yet understand the problem well enough to stick a gene into the bees and wait for it to proliferate, and it may very well turn out that a less heavy handed approach could deliver satisfactory results.

Edit: Monsanto actually bought them in 2011, they didn't start the company.",null,0,cdpssa2,1rp1tp,askscience,top_week,1
Philsutty,"Assuming you were lifting with both arms equidistant from the centre they'd be lifting half each, if you were to place your hands at different distances it would be different.  Imagine lifting one arm in the centre and one at an end, the bar would be balanced around the centre, so that arm would be taking all the weight and you could theoretically take the hand at the end off. If you had one arm  at position x from the centre and the other at 2x from the centre in the opposite direction the first arm would be lifting twice the weight of the second, totalling the overall weight",null,0,cdpl9nt,1rp2hf,askscience,top_week,2
chrisbaird,There is a very easy way to test this. Try lifting the bar with two hands and then try lifting the exact same bar with one hand in the center. Is it any harder? Does it feel any heavier? ,null,0,cdpobjr,1rp2hf,askscience,top_week,1
OnceReturned,"You could get into some deep water here thinking about what randomness really is, and randomness vs. unpredictability.  

I think in conventional usage though, and in the sense that you seem to be using it here, things can be random without all possible outcomes having equal probabilities.  If it was 1's on five sides and a 6 on the last, the outcome would be random with rolling a six having a probability of 1/6 and rolling a 1 having a probability of 5/6.  A better example might be picking a fruit out of a barrel with 99 pears and single apple.  Your selection could be random, even though one outcome is much more likely than another.

On the other hand, on any given roll of the die, if you knew enough about the conditions of the roll and properties of the die itself, the surface you rolled it on, and the air you tossed it through, there are quite a few people who would argue that you could determine the outcome with certainty beforehand, in which case it wouldn't be random. ",null,0,cdpvhnv,1rp2py,askscience,top_week,2
paolog,"The outcome of rolling a die many times would still be random, but the probabilities of getting each of the numbers from 1 to 6 may be affected by the indentations on the sides. In particular, the die's centre of gravity will be slightly closer to the 1 face than the 6 face, which would make throwing a 6 a tiny bit more likely than throwing a 1.",null,0,cdpkfo6,1rp2py,askscience,top_week,1
xxx_yyy,"* Geology: One can look at old stuff (e.g., rocks) on Earth to see if chemical processes were the same then.  I'm not very familiar with geology.

* Astrophysics: One can look at distant galaxies (""looking back in time"") to see if atomic properties (e.g., transition energies) are the same as now.  Also, the abundances of the light elements (hydrogen, helium, lithium, beryllium) created in the first few minutes after the big bang depend on nuclear processes that we can measure.  Agreement with modern measurements is an indication that the laws haven't changed.",null,0,cdpkb6l,1rp33e,askscience,top_week,4
adamsolomon,"&gt;And since science doesn't allow for assumptions

That's definitely not true! We make assumptions all the time - the point is then to figure out how to test them.

As xxx_yyy said, there are a few ways of testing the variation of constants, and none of these tests (with a few controversial exceptions) have turned up statistically significant evidence that any constants of nature have varied. These tests span a fairly wide range of places and times, all the way back to a mere few seconds after the Big Bang. But of course, that's only within present experimental precision: there's always the possibility that there is some smaller variation of some presumed constant.",null,0,cdpkjp5,1rp33e,askscience,top_week,2
chrisbaird,"You seem to be misunderstanding how science works. If we discover that something was different a million years ago, we don't say that the rules of physics has changed. We say that our model was wrong. We intentionally create a new model to account for the differing situation a million years ago such that the model is the same for all periods of time. Science is always evolving to account for all observations, and not the other way around.

For instance, let's say that we discovered that a million years ago the gravity created by a unit mass was twice as strong as the gravity created by a unit mass today. We would not say that, ""The rules of physics have changed themselves! Our assumption has been wrong all along!"". We would simply say, ""We misunderstood the rules of physics in thinking the relationship between gravity and mass is unchanging. We now know that this relationship is not fixed, so here is a more accurate, *unchanging* rule that describes how gravity has evolved through the history of the universe."" We would not have two separate rules of physics; one for a million years ago and one for today. We would instead create a better, single, theory that accounts for all ages.",null,0,cdpo6f9,1rp33e,askscience,top_week,2
aggasalk,"this is a complicated question. closest to your meaning, I think, is the issue of filling-in of surface color - the [Cornsweet illusion] (http://en.wikipedia.org/wiki/Cornsweet_illusion) is usually taken as one piece of evidence that the visual system uses boundary properties to infer properties of surfaces. the [watercolor illusion] (http://en.wikipedia.org/wiki/Watercolor_illusion) is another example at the same level. these illusions can be taken to demonstrate the fact that surface features are at least in part inferred from non-surface properties. there's a whole system of 'lightness and brightness' illusions that show this in different ways. [color constancy] (http://en.wikipedia.org/wiki/Color_constancy) is also at a similar level, showing that what you see as a particular color depends on other structure in the same scene. 

but more broadly speaking, *objects* are also perceptual inferences. 'dog', 'pillow', 'chair' - these are mental constructs. there is a physical reality, but it doesn't contain identities or meanings or clear-cut distinctions between clumps of matter. it's the brain that decides *this* clump and *this* clump are different (pillow vs dog), while *this* clump and *that* clump are similar (pillow vs another pillow).

so under this theory, you can see any clump of stuff as any object that is *known* to you, and while you'll usually get it right, you will often make mental mistakes. so you might see a clump and interpret it as 'dog', and then shift your interpretation to 'pillow' - what has happened is that the identity of objects is something that is filled in by context and features, just as is the color of surfaces, etc; and if the context strongly suggests 'dog place', you might be inclined to mistake a non-dog object for a dog, just as the border in the Cornsweet illusion inclines you to mistake the two sides of the display as being different colors. the difference is maybe that the dog/pillow has internal structure, as well as contextual relations, that help you to make further disambiguations upon a little more analysis.",null,1,cdpqiqf,1rp418,askscience,top_week,2
jowofoto,"Each person has a 'blind spot' due to the optic nerve. In terms of percentage, the area of the human retina is 1,094 mm^2 (average). A typical sized optic nerve is about 1.86mm x 1.75mm, so 3.255 mm^2 . To give a percentage of the optic nerve 'blind spot' would be something like 0.298%. However, the distance between each rod and cone as well as how large the retina is is also something to consider and your brain also 'fills in' those tiny gaps as well. ",null,0,cdpmcv5,1rp418,askscience,top_week,1
paashpointo,"They will not be able to for the same reason we dont know if other universes are out there.

If there is a thing that is impossible to observe(interact with in any way ever) no matter what then it effectively doesnt exist in your universe.


For such a universe that has expanded such that no light will ever reach the next galaxy, you could extrapolate backwards only if there was interaction. So it would be theoretically possible with a perfect grasp of gravity to prove that a long time ago other things that dont currently appear anywhere in the universe to have been there interacting. But to calculate with this sort of precision would require more memory than exists in the known universe. 

Fyi I am just a layman that likes stuff so my details might not be perfect or technically correct but that is how I understand it.",null,0,cdpkc61,1rp4aw,askscience,top_week,2
Entropius,"The Big Bang wasn't like a supernova.  A supernova is just an explosion *within*  space.  That would imply it was just throwing matter and energy through an already existing void.

The Big Bang was an explosion that created space &amp;amp; time (well maybe, it's up for interpretation).

The theory was derived from a catholic priest who also happened to be a physicist, [Georges Lemaître](http://en.wikipedia.org/wiki/Georges_Lema%C3%AEtre). From Hubble's observations of galaxies expanding away from one another, and Einstein's General Theory of Relativity, Lemaître figured if that's all true then all matter and energy had to have originally been at a single position some time in the past.

",null,0,cdprsmc,1rp4cp,askscience,top_week,1
calibri00010,"We don't know what happened at the EXACT moment of the big bang, but we are confident we know what happened .000000000000000000000000000000000000000000001 seconds after the ""bang"". Scientists currently believe that the Big Bang was the moment gravity broke away from the other unified forces (nuclear strong, nuclear weak, electromagnetic) causing the physics of our observable universe to ""lock"" in to place. As soon as the basic forces of nature were locked in to place; an immediate expansion of space caused intense heat and light (photons). The particles were so excited(hot) that they combined in to matter. Eventually the universe began to cool, at this point the universe was made of 75% hydrogen and 25% helium. For one billion years nothing existed but gas, but at some point gas clouds became so heavy they collapsed and ignited the first stars. Stars clustered to become galaxies etc. ",null,0,cdpxw9v,1rp4cp,askscience,top_week,1
haysoos2,"Some of the latest exoplanet discoveries have included huge gas giants orbiting much closer to their suns than we previously thought possible.

So at the moment, it appears our models of how planets form and arrange in a solar system are incorrect and need to be modified.

In short:  We have no idea.",null,0,cdpkseh,1rp4n7,askscience,top_week,1
adamsolomon,"Good question! There are two parts to this answer.

First, gravity is not always attractive. And it turns out that on the largest cosmic scales, it's repulsive: the expansion of the Universe isn't slowing down, as you'd expect, but is speeding up. This means that either the Universe is filled with some exotic stuff (""dark energy"") which has repulsive gravity, or the laws of gravity differ from what we thought they were, in such a way that at large distances/late times, gravity between normal objects becomes repulsive. If the Universe were to one day stop expanding and collapse on itself, it would first have to be expanding but with an expansion that's slowing down. The fact that the expansion isn't slowing down means that collapse is likely not in our future.

But in fact, before we knew about this bizarre speeding-up expansion, we still weren't sure there would be a Big Crunch. The reason is that even if gravity is always attractive, it doesn't necessarily make everything collapse. Think about it in terms of a rocket ship: if it's launched with an initial speed greater than some value (the escape velocity, neglecting air resistance), then the Earth's gravitational pull will constantly slow the rocket down, but never slow it down so much that it stops going up and falls back down.

Similarly, if galaxies were moving away from each other at sufficiently high speeds, then even ignoring this weird repulsive gravity stuff, they still wouldn't ever start moving towards each other. The expansion would slow down but never quite stop. This was considered a good candidate for our own Universe before we discovered the accelerating expansion, which put a Big Crunch way out of reach.",null,1,cdpkdng,1rp4yk,askscience,top_week,5
Pombologist,"These genes aren't binary. It's not a matter of having this gene will make you an alcoholic, not having this gene means you can't ever become alcoholic. Rather, it means that these genes give you a certain predisposition to those conditions. You are *more likely* to develop them if you have that genetic marker.

In terms of implications, it means that the patient can become aware of the potential for that problem before they get it, and therefore be aware of early warning signs or have better detection for it. For example, if you have a genetic marker associated with colon cancer, your doctor may recommend that you get a colonoscopy more often. Genomic is quickly becoming a more important part of medicine.",null,0,cdpnqxw,1rp54b,askscience,top_week,4
snusmumrikan,"One of the biggest misconceptions with the human genome project and genetics in general is that people think that finding out what gene(s) are responsible for something means we can fix it. That's not true, it might help develop treatments in the future, but on it's own it gives no real advantage. 

It's like having a boat with a leak - if it is a small leak you might be able to treat the symptoms of the leak (water in the boat) with a bucket twice a day and leave it at that. If the leak is more serious you will want to know why, however finding out that 'there's a flaming massive hole in the bottom of the boat cap'n!' makes no difference if your only tool is still one bucket. 

We have limited gene-therapy treatments as they need a way to counteract the latent problem in all of the cells affected in your body. For cystic fibrosis, which is most commonly caused by a single gene mutation in an ion channel in the lungs, they have tried to insert the correct gene with viral transmission vectors with [limited success](http://www.ncbi.nlm.nih.gov/pubmed/16296753)

There is also promising work with [non-coding DNA](http://en.wikipedia.org/wiki/Morpholino) injections for [Duchenne Muscular Dystrophy treatment](http://www.ncbi.nlm.nih.gov/pubmed/19288467) - another single gene disease. 

Other than those kinds of approaches the most promising future seems to be systemic treatments like bone marrow transplants and stem-cell therapies, however both of those are relatively infant fields. 

P.S. sorry for the awful boat analogy. ",null,0,cdpwni2,1rp54b,askscience,top_week,2
expertunderachiever,"Signals in a wire can only travel at most that fast (in reality it's much slower).

So your 4GHz CPU means that a signal can travel 74mm per clock period ( http://www.wolframalpha.com/input/?i=%28light+second+%2F+4*10^9%29+in+mm ) but nothing in your CPU is a straight line and not all gates react evenly to signaling [there is slack/skew/etc].  So to meet timing the distance between latched pieces of logic must be shorter than the distance light can travel in that period.

Ignoring other electrical properties at the very least to get to 8GHz you'd have to have half as much length per latch otherwise your circuit won't meet timing.",null,1,cdpmcyy,1rp6wm,askscience,top_week,2
wesramm,"Yes, paint ""drying"" is the release of the volatile solvent from the paint.  An analogous term is ""off-gassing"".  On earth, if you want to ""off-gas"" a solvent, you put the part or article containing the solvent into a vacuum chamber.  The lower pressure causes any volatile material to (whether it is a petrochemical or water) boil off at a much lower temperature, leaving only the solids.  Its the same with paint.  So, in fact, the process should be much faster in space, but I'd expect the process to be nearly explosive, resulting in spotty coverage, as the boiling would be extremely violent...  ;)
",null,5,cdpjlhm,1rp7i5,askscience,top_week,29
ashary,No. Paints contain unsaturated oils which react with oxygen to form a cross linked polymer. As space doesn't have any oxygen the polymer will not form and hence the paint will not dry,null,1,cdpik9n,1rp7i5,askscience,top_week,6
Dominus_,"Because of the near total vacuum of space, [the boiling point of any liquid is much lower](http://www.youtube.com/watch?v=rM04U5BO3Ug), causing water or other liquids in paint to evaporate much more quickly. 

So yes, paint would probably dry almost instantly, in a violent boiling fashion.",null,0,cdpizgy,1rp7i5,askscience,top_week,5
wesramm,"Yes, paint ""drying"" is the release of the volatile solvent from the paint.  An analogous term is ""off-gassing"".  On earth, if you want to ""off-gas"" a solvent, you put the part or article containing the solvent into a vacuum chamber.  The lower pressure causes any volatile material to (whether it is a petrochemical or water) boil off at a much lower temperature, leaving only the solids.  Its the same with paint.  So, in fact, the process should be much faster in space, but I'd expect the process to be nearly explosive, resulting in spotty coverage, as the boiling would be extremely violent...  ;)
",null,5,cdpjlhm,1rp7i5,askscience,top_week,29
ashary,No. Paints contain unsaturated oils which react with oxygen to form a cross linked polymer. As space doesn't have any oxygen the polymer will not form and hence the paint will not dry,null,1,cdpik9n,1rp7i5,askscience,top_week,6
Dominus_,"Because of the near total vacuum of space, [the boiling point of any liquid is much lower](http://www.youtube.com/watch?v=rM04U5BO3Ug), causing water or other liquids in paint to evaporate much more quickly. 

So yes, paint would probably dry almost instantly, in a violent boiling fashion.",null,0,cdpizgy,1rp7i5,askscience,top_week,5
thegreatgazoo,"It has been a while but an LED has a voltage that is applied to it (around .7 volts) that raises electrons from one energy level to another. When the electron 'falls back down' it releases a photon corresponding to the wavelength/distance that it 'fell down'. The wavelength depends on the material used which is why some are red, green, yellow, or blue. 



",null,0,cdpi9ju,1rp7zv,askscience,top_week,4
Ejb90,"thegreatgazoo gave a good answer, but to expand:

Light can be considered as waves of electromagnetic radiation OR a particle, dependent on what it's doing. This duality is counterintuitive, but it's a fundamental consequence of quantum mechanics.
The issue is that light can behave with different properties we consider as ""particulate"" or ""wave-like"" depending on what it's doing. Hence we just say it's a duality between the two, and pick what we need for the situation.

In a light bulb the electrons pass through a filament. However the filament has some resistance to the electrons passing, as it's full of atoms the electrons need to push their way through. The actual mechanics of how this works is complex (it requires a bit of band theory and quantum condensed matter), but you can think of it as the electrons pushing each other into higher energy levels whilst trying to get through. These electrons are a bit unstable in the higher energies, so they emit some energy to drop down into a more stable shell. This energy it releases is in a packet a ""quanta"" of energy, which we call a photon. Hence it produces light. The differences between the energy levels is different for different materials, and it determines the colour of the light emitted.

The beta-decay process is a bit different to what you describe. the reaction path is:
(up, down, down) -&gt; (up, up, down) + electron + electron antineutrino + photon
neutron -&gt; proton + electron + electron antineutrino + photon

The mass lost from the neutron to the proton is (sort of) the mass of the antineutrino and electron.
There is also beta+ decay, which you can look up.
You can think about the products of these reactions by making sure several quantities are unchanged, such as the energy, angular momentum, charge, lepton number etc.
Hence here the charge is conserved, as is the lepton number (the number of leptons in a system, funnily enough).

Your point about the positive and negative charges producing a stream of photons is slightly misguided, but I think you're talking about virtual particles. It's (in a way) true that there is some photons that produce the electromagnetic force, called virtual photons, but these aren't physical, tangible photons we can observe. It's more of a mechanism for dealing with subatomic particles.
",null,0,cdpiham,1rp7zv,askscience,top_week,4
Spirko,"In an incandescent light bulb, the electrons that form the current can basically flow where they want, but they keep running into things.  This stops their progress along the wire and heats up the wire.  Some energy of the charges that form the current is given to the atoms in the wire, and they vibrate around randomly.  Any object emits [Blackbody Radiation](http://en.wikipedia.org/wiki/Black-body_radiation), which is what we see from ""regular"" light bulbs.

In an LED, the electrons in the semiconductor (the business part of the LED) are confined to particular orbitals.  Because there are so many atoms and the orbitals overlap, they are called bands.  (Imagine drawing the same line across a page many times.  Eventually, you just have a big stripe of ink.)  At one point, the material changes, which changes the energies of the bands, and the electrons must find new bands to occupy to cross that point.  They must lose some energy.  The energy released is given off in the form of photons that have that amount of energy.  Because this is a microscopic process with conservation of momentum and energy, the energy basically all has to go to one photon, and the color comes from the amount of energy.  The simplest case to think about is a single atom's [Spontaneous Emission](http://en.wikipedia.org/wiki/Spontaneous_emission), but the LED's case isn't that different.

How does it happen?  It's probably best to start thinking about electric and magnetic fields, go through the ideas of [Displacement Current](http://en.wikipedia.org/wiki/Displacement_current) and [Electromagnetic Induction](http://en.wikipedia.org/wiki/Electromagnetic_induction), and understand [Maxwell's Equations](http://en.wikipedia.org/wiki/Maxwell%27s_equations).  Any time-varying electric or magnetic field will stimulate the other, and they will oscillate together into the distance as an EM wave.  Classically, any accelerating charge should emit EM radiation (i.e. light) and that includes the ones in the LED.  As an electron changes speed, its electric field changes, etc.  The restrictions of the energies of the electrons and the photons (i.e. electron bands and photon energies) come from quantum mechanics.",null,0,cdpkjif,1rp7zv,askscience,top_week,2
PRBLM2,"Technically, a shadow is an area where direct light from a light source cannot reach due to obstruction by an object.  So, anything that blocks visible light and creates a shadow would also have to be visible.

However, in the spirit of your question, it is possible to use a lens (while not completely invisible) to create a ""shadow"".  If you've ever used an incandescent flashlight, you've experienced this type of ""shadowing"".  [Here's a picture to illustrate my point.](http://www.spcmarketing.co.uk/acatalog/dead_spot_beams.jpg) I don't know if there's a name for this type of dark spot, but it isn't technically a shadow since the light is being refracted and not obstructed.  ",null,2,cdpi7gc,1rp86x,askscience,top_week,14
Fyrefli,"Carbon dioxide can cast a shadow, but not because it blocks the light.  Rather, due to its higher density (relative to nitrogen), the carbon dioxide refracts light, causing darker patches (i.e. a  shadow).

Edit: at about the 0:40 mark of [this video](http://www.bbc.co.uk/learningzone/clips/an-introduction-to-carbon-dioxide/1614.html), you can see the shadow.",null,0,cdpittk,1rp86x,askscience,top_week,2
jayman419,"The Wardenclyffe Tower's main job was to be a wireless telegraph. It was his entry in the race with Marconi. A secondary purpose was a large scale proof-of-concept for the wireless transmission of energy, something Tesla had already demonstrated in smaller scale experiments.

But the version he had up and running wasn't necessarily unlimited or free. It was a method of power *transmission* ... not power *generation*.

http://www.teslauniverse.com/nikola-tesla-article-teslas-wireless-light

The ""World Wireless System"", had he been able to work on it with continued iterations, might have someday been something functional. It might have taken mile-high antennas and mile-deep geothermal pits to generate a strong enough field to cover a city, but who knows what he had in mind for the final version?

Wireless energy is something they're working on today. They use microwaves for longer-distance transmission though, because there is less loss due to atmospheric variability and water vapor. And his basic theories for the wireless transmission of energy were sound, for the most part. 

While not a great source, [the wiki](http://en.wikipedia.org/wiki/Wireless_power#Electric_energy_transfer) goes over some of the different ways it's been proven to work, and can go over the basic terminology to give you some better search terms.",null,0,cdpkfrx,1rp9gl,askscience,top_week,2
adamsolomon,"As SilentCast said, it's mass - not physical size - that determines gravity. But it's also mass which, to an extent, determines whether what you have is a star or a planet. If you have something as massive as a star, its gravity is going to be strong enough to force hydrogen atoms in its core to fuse into helium, producing light. That's pretty much the definition of a star. So you wouldn't have a star-planet system, but a binary star system.",null,0,cdpll5a,1rpabd,askscience,top_week,7
iorgfeflkd,"It is possible for a planet to have a larger radius than its star (if the star is a white dwarf or neutron star), but not more massive. Anything that's more massive than a star will become a star.",null,0,cdpmj08,1rpabd,askscience,top_week,6
SilentCastHD,"Well, the thing that makes gravity work is not size, but mass.

Just to be clear about that.

radius:

1 | Jupiter | 69173 km

2 | Saturn | 57316 km

3 | Uranus | 25266 km

4 | Neptune | 24553 km

5 | Earth | 6367.5 km

6 | Venus | 6051.9 km

7 | Mars | 3386 km

8 | Mercury | 2439.7 km

Mass:

1 | Jupiter | 1.8988×10^27 kg

2 | Saturn | 5.685×10^26 kg

3 | Neptune | 1.0278×10^26 kg

4 | Uranus | 8.6625×10^25 kg

5 | Earth | 5.9721986×10^24 kg

6 | Venus | 4.869×10^24 kg

7 | Mars | 6.4191×10^23 kg

8 | Mercury | 3.3022×10^23 kg

So for exampel as you see, uranus is bigger that neptune (chuckle) but still has less mass, since the density is way off:

1 | Earth | 5.515 g/cm^3

2 | Mercury | 5.43 g/cm^3

3 | Venus | 5.24 g/cm^3

4 | Mars | 3.94 g/cm^3

5 | Neptune | 1.76 g/cm^3

6 | Jupiter | 1.33 g/cm^3

7 | Uranus | 1.3 g/cm^3

8 | Saturn | 0.7 g/cm^3

So uranus would gravitate around neptune (not really, since even planets make our sun wobble around, since they really gravitate around each other but neptune would move a little less around uranus than uranus around neptune)

So with that out of the way: 

Yes, planets can be bigger (not heavier) than stars:

There are for exsample a Neutron star has a radius of only 12 km but have a density of 3.7×10^17 to 5.9×10^17 kg/m3

So wikipedia states: This density is approximately equivalent to the mass of a Boeing 747 compressed to the size of a small grain of sand.

So as you see, every planet in our solar system would be bigger than this Neutron star, but they could still gravitate around it, since this star is around 2.6×10^14 to 4.1×10^14 times the density of the Sun at 1/60000 of the radius.",null,0,cdpl75m,1rpabd,askscience,top_week,3
iorgfeflkd,"It's possible to use an electromagnetic field to vaporize a solid. Powerful lasers can do this (I personally have used a ~1 Watt 1064 nm laser to melt ice). However, the mechanism of melting or vaporization isn't as you described.",null,0,cdpja5b,1rpaf7,askscience,top_week,4
__Pers,"This is routinely done with ultra-intense (intensity greater than 10^18 W/cm^2 for ~1-micron wavelength light) lasers through a process called [Coulomb explosion](http://en.wikipedia.org/wiki/Coulomb_explosion). Typically, the samples exploded in this way are rather small (~10s to 100s of nm across) and the lasers are rather short in duration (picosecond to tens of femtoseconds).  

There are also other, related means of particle acceleration from lasers that operate on similar principles, yet in different parameter regimes; target normal sheath acceleration is one such example. 

Source: I work in this field. ",null,0,cdpkvdp,1rpaf7,askscience,top_week,2
NAG3LT,"Well, changing electric and magnetic fields produce electromagnetic waves. Light is also an electromagnetic wave and can be used to vaporise solid. Also, you almost never strip out all electrons from the more charged nuclei, but stripping few electrons out of outer shell is usually enough to break bonds between atoms. When you shine light at a material and it is absorbed, most of its energy first goes into electrons, and then redistributed to the nuclei. The kinetic energy of nuclei manifests itself as a heat and acts the same. 


To put a lot of energy in a form of light into a solid, you can use laser. If you're using a laser that shines continuously, you can melt and then vaporise some part of a molten metal. Depending on the reflectance, thickness and focusing few Watts of laser power may be enough for cutting. However, when you use a non-pulsed laser, even 50 kW lasers will still leave some parts molten, as the absorbed heat has time to diffuse around the spot you shine at. 


There is a way to vaporise metal with minimal melting, however. Going straight from solid to a vapour is called ablation, and it can be achieved with a very short laser pulses. It is currently possible to generate pulses on the order of picoseconds (10^(-12) s) and less. The energy you put into the pulse is absorbed so quickly that the thermal diffusion into surrounding area cannot move it away fast enough. One of the values I've found was done with a 100 fs (100*10^(-15) s) [laser pulse](http://link.springer.com/article/10.1007%2Fs00339-010-5766-1). The flux required to ablate copper was 1.7 J/cm^2, which means a peak irradiance of 17 TW/cm^2. The peak electric field strength is 11 GV/m (Gigavolts per meter). That's a lot, as making an electric spark go through the air requires ~1000 times weaker field. 


So, if you want to vaporise material from a solid state, you're looking at very strong electric fields. However, that won't strip all electrons, only the least bound ones on the outer shell. Removing all the electrons gets harder the further you go along the periodic table. Unfortunately I can't give you any numbers for those.",null,0,cdpjek4,1rpaf7,askscience,top_week,1
rupert1920,"[Mass spectrometry](http://en.wikipedia.org/wiki/Mass_spectrometry) is a technique where gas-phase ions are separated by their mass-to-charge ratio, so both things you mentioned are employed: moving the sample into the gas phase, and making it into an ion.

One way to do this is basically what you're describing - using a strong electric field (at the kilovolt range) to both ionize the sample and to expel it into the gas phase. This ionization technique is called [field desorption](http://en.wikipedia.org/wiki/Field_desorption).

As mentioned by others, there are other ways involving actual _electromagnetic_ fields (i.e., light). You can use a laser to ablate - vaporizing a solid - your sample embedded in a matrix. The laser also acts to ionize the matrix, which then ionizes your sample. See [matrix-assisted laser desorption/ionization (MALDI)](http://en.wikipedia.org/wiki/Maldi).

As a final note, neither of these methods will strip _all_ of the electrons from a solid. It's unstable to have a huge excess of charge - and if that occurs, the molecule will fragment instead.",null,0,cdpjvb3,1rpaf7,askscience,top_week,1
jakkes12,Imagine yourself the zipper has a few points it will produce a sound on whenever you cross it. Each time you cross one of these marks it'll produce one sound wave.Thus the frequency will be determined by how many marks you cross each second. Higher speed means more marks crossed per second equals more sound waves per second (higher frequency) which means higher pitch.,null,4,cdpiz4t,1rpbvb,askscience,top_week,28
jakkes12,Imagine yourself the zipper has a few points it will produce a sound on whenever you cross it. Each time you cross one of these marks it'll produce one sound wave.Thus the frequency will be determined by how many marks you cross each second. Higher speed means more marks crossed per second equals more sound waves per second (higher frequency) which means higher pitch.,null,4,cdpiz4t,1rpbvb,askscience,top_week,28
expertunderachiever,"To put it in simple algebraic terms, suppose computing 1/x was a hard problem but somehow I had an algorithm to do it efficiently.  You could know me as the ""guy who can compute modular inverses"" if I take a string and give you the inverse of it ... e.g.

    r = hash(msg)

then I hand you 
 
    s = 1/r

Which you can verify by comparing s * hash(msg) == 1.  You could encrypt a message m to me by picking a random r and sending me c = rm and r.  Since 1/r is hard to compute [in this scenario...] I can easily compute 1/r multiply against c and get m back.

In this case you're dealing with problems like RSA where you have a modulus n = pq where the private key owner knows the factors.  Because they know the factors they can compute two exponents

    e = something like 17 or 65537

and

    d = 1/e modulo the order of the multiplicative group mod n

The order of the group is given by lcm(p-1,q-1).

So now we have 

    m^ed mod n == m^1 == m

So if I send you

    c = m^e mod n

you can compute

    m = c^d mod n

since

    c^d == ( m^e )^d == m^ed == m^1 == m

Since nobody knows d but the private key owner they can also easily compute
 
    s = hash(msg)^d mod n

And we can verify with

    hash(msg) == s^e mod n

Computing the order from n = pq is a hard problem believed to be as hard as factoring, so is computing the e'th root.

[note: messages are padded in RSA to avoid other forms of non-random attacks on the operation...]
",null,0,cdpkgma,1rpcqt,askscience,top_week,3
paashpointo,"What it means is if I have a plain text of something I want the world to know and I put whatappears to be gibberish at the end that has been generated using my private key, then anyone with my public key can prove that my private key was used and thus it has to be me.

So for example if you and I were doing business and I just said in plain language hey this is paashpointo send me my money at account number and so on, you would have no way of verifying it was me and not a hacker. 

But instead I could send the same message and at the end attach paashpointorulesandreallyistheguyyouaretalkingtotrustme after running it through my private key and it would look like gibberish but then running it through the public key would restore it. So it is only good at verifying to the public that they are talking to who they say they are. Hope this helps.",null,0,cdpkgp8,1rpcqt,askscience,top_week,2
blackhattrick,"In asymetric encription, usually RSA, you have a pair of keys: the private and the public key. You can encrypt with the public key and decrypt with the private key and viceversa. When you encrypt something with a private key, which no one have, certifies at the moment of decryption with the public key that this message was encrypted by you. If someone sends a message with a different key saying it is you and someone else tries to decrypt it with his public key, he will get garbage data. So the person trying to decrypt this message would know this isn't you.",null,0,cdprplv,1rpcqt,askscience,top_week,2
linozeros,"Somebody asked a similar question and already got it answered:
[Did viruses that are lethal to humans evolve to kill us, or is that a side effect of their existence?](http://www.reddit.com/r/askscience/comments/17h29d/did_viruses_that_are_lethal_to_humans_evolve_to/)

Hope this is what you were looking for",null,0,cdpl0qo,1rpcs5,askscience,top_week,7
WhenTheRvlutionComes,"A microorganisms success isn't determined by whether or not it kills someone. We just tend to think that way since harmful microorganisms are typically the only ones of interest to us - such that, in the common lexicon, ""virus"" essentially only means something harmful. But there are plenty of benign or beneficial microorganisms that are very successful as species. Killing the host is just a side effect, undesirable in its own right, sure, but tolerable in evolutionary terms if it's a necessary part of some overall strategy that leads to new hosts and new chances for reproduction.",null,0,cdpqr73,1rpcs5,askscience,top_week,2
yankee333,"Many of these pathogens have large environmental reservoirs. While not a virus, vibrio cholerae is a good example. There is little need for the vibrio to maintain a chronic infection in a host because it already has an environmental reservoir to maintain itself. This is very much an indirect process, as all evolution is.",null,1,cdpqxy6,1rpcs5,askscience,top_week,1
casonthemason,"It depends how you define 'success.'  If the goals of a virus are to infect, replicate, and spread, then viruses which quickly kill their hosts would actually be regarded as unsuccessful.  Generally speaking, the viruses that are most lethal to humans tend to come from non-human reservoirs (for example: ebola, influenza).",null,0,cdpsfcg,1rpcs5,askscience,top_week,1
ucstruct,"Not really, even these two have somewhat indirect effects on mineral absorption. Vitamin C acts as an [anti-oxidant](http://en.wikipedia.org/wiki/Vitamin_C) that helps keep iron in a more soluble form and [vitamin D](http://en.wikipedia.org/wiki/Vitamin_D) stimulates production of an ion channel that can facilitate calcium absorption. 

Vitamins generally function by being cofactors in enzymatic reactions or signaling molecules (except for D, which signals, and [E](http://en.wikipedia.org/wiki/Vitamin_E) which mainly is an anti-oxidant) by performing chemistry difficult to perform with just amino acids. Mineral absorption is handled by transporters and channels located at the cell cell surface that are specific to each mineral. These include phosphate, calcium (upregulated by D), manganese, magnesium, zinc, iron, as well as all of the nutrients we require like amino acids or sugars.  ",null,0,cdpu3xh,1rpdnd,askscience,top_week,1
MrQuiver,"The light we observe from it has been traveling towards us for 2500 years. We are observing it right now as it was 2500 years ago. We do not have to send it anything to observe it, nor to determine its composition. We are passively receiving the light that has been radiating out from it. This light contains the information that tells us of its composition.

There is no difference between this and how you observe, say, an airplane in the sky above you. You do not have to travel to the plane to observe it. You just have to observe the light that is arriving at your eyes from the plane. The only difference in your example is that the thing you are observing is much further away, such that the light that is arriving at your eyes was emitted a long time ago.",null,0,cdplcfr,1rpf7x,askscience,top_week,4
DanielSank,"This excellent question is at the root of *many* other questions that show up on /r/askscience.

There are aspects of this question that nobody really understands yet. However, we understand a lot more than popular science and even some physicists will lead you to believe. This will be a somewhat long post but possibly worth your time to read.

Before I say anything else, I want to say that your statement

&gt; How does it ""know"" it was observed? By its interaction with surrounding particles, of course.

is absolutely essential. I hope to explain why below.

**Structure of physical theory**

Any theory of physics has two parts.

* (1) The first part is a representation of the natural system under consideration. For example, if you are making a theory of balls rolling down hills you represent each ball with three numbers (x,y,z) indicating the ball's position in space. You may also have (Vx, Vy, Vz) representing its velocity in all three directions.

* (2) The second part is some way of predicting how this representation changes as time goes by [1]. For the case of rolling balls this could just be [Newton's laws of motion](http://en.wikipedia.org/wiki/Newton%27s_laws_of_motion).

In classical physics these two parts were all we needed to go happily about our scientific lives. However, there's actually a crucially important third part:

* (3) We have to have a prescription for how the representation put forth in part (1) relates to our personal/human/cognitive observations of the natural system. In other words, ""what does a group of numbers (x,y,z) have to do with my observation of the system?""

In the case of rolling balls this is pretty simple. I might use Newton's laws to compute that my ball should be at position

(1 meter, 0.5 meters, 0 meters) at time=1.3 minutes

and we all just *know* that this means that we should see a ball at the specified location and time. This works just fine and indeed in classical mechanics the link between part (1) and (3) is usually pretty straightforward. Note that we never mentioned anything about what we meant by ""see a ball"", because in classical physics we implicitly assume that it doesn't matter. In quantum mechanics it does matter and I'll now try to explain why.

**Quantum mechanics**

The first two parts of quantum mechanics are 

* (1) We represent the natural world with a [wave function](http://en.wikipedia.org/wiki/Wave_function) (or [state vector](http://en.wikipedia.org/wiki/Quantum_state)).

* (2) The wave function evolves according to [Schrodinger's equation](http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) (or [Heisenberg's equation](http://en.wikipedia.org/wiki/Heisenberg_picture)).

These two together are just as good as (x,y,z) coordinates and Newton's laws. The place where everyone gets confused is in what the wave function means in terms of what we observe in experiments. To answer this question you hear people say things like ""when you make a measurement the wave function collapses into a definite position, chosen at random, and that's what you see."" The crucial thing to understand is that this statement modifies part (1). It says that Schrodinger's equation determines the evolution until some vaguely defined ""observation"" happens, at which time Schrodinger's equation stops being correct, the state collapses probabilistically, and then Schrodinger's equation goes back into effect. This breaks the theory's self-consistency. Suppose I make a measurement and say that the atom's state collapsed. Now suppose that me and my experiment are in a box, which is actually the experiment of some greater alien. For the theory to be true for the alien, there can't be any state collapse until *he* does the observation. Thus we see that state collapse as stated above can't work.

The issue is that as stated above, state collapse modifies part (1) of the theory, whereas we *should* have been looking for a part (3) type statement. It turns out that if we *leave part (1) alone* and make an extra statement like this:

* (3) When a human's brain collects information about a physical system that information will be randomly distributed according to the square of the wave function, and that system will henceforth appear to have collapsed in accordance with the result of the human's information.

then we have a self-consistent theory. I emphasize that it is both falsifiable and self-consistent and therefore must be admissible as a theory of Nature. That said, its not very satisfying. because of this bizarrely anthropocentric rule (3) which seems much more weird than the sort of obvious equivalent rule (3) in classical physics [2].

**Decoherence**

Amazingly we can *almost* get the quantum rule (3) to be as *obvious* as the classical one. The trick is to really consider what happens when we make ""measurements"". In fact whenever you measure something you are connecting that thing to many extra physical elements. If I measure the position of an atom with a CCD camera, I am coupling the wave function of the atom to an enormous number of electrons etc. in the camera's inner workings. Certainly I do not know the wave functions of all those electrons. **This is the crux of the entire story**: It turns out that if you actually compute what happens to the wave function of the atom in the case that it couples to a bunch of extra stuff whose wave functions you *don't know*, then from your perspective the wave function of the atom becomes a classical probability distribution. I emphasize that this comes about *entirely from Schrodinger's equation and nothing else*. From the perspective in which the wave functions of the electrons are known, the atom's wave function *does not collapse*. Therefore, collapse can be understood as an apparent phenomenon, predicted by pure quantum mechanics, that shows up with only part of the information in a system's wave function is available. This is always the applicable case for humans because our measurement apparatuses contain many uncontrollable degrees of freedom.

When the wave function appears to have collapsed for one observer, it may not appear that way for another one who has more information.

This is really interesting because it gets us really really close to being able to explain rule (3) via rule (1). We know that when only some information of a system is available, that information appears probabilistic. What we haven't explained is why humans see only one of those possible results. Nobody knows the answer to that.

**Answer to the original question**

So, how does a particle ""know"" when it has definite position? It doesn't. It never does. Definite position is just an apparent property of things that comes up when we don't have complete information about the system.

**Conclusion**

We have a self-consistent theory of quantum mechanics using wave functions and Schrodinger's equation. Part of the theory is the statement that when humans have access to the prabability distributions furnished by partial access to a complete system wave function, they perceive one particular value, for some reason that we don't know.

[1] For the physicists out there I'd say more generally equations of motion are constraints on how the representation at one point in space-time is related to the representation at other points in space-time.

[2] However I challenge anyone to state a truly self-consistent theory of physics without such a rule.

EDIT: For down voters: If you could express what you think is unscientific, wrong or irrelevant about this post I'd appreciate it.

EDIT: Formatting",null,1,cdppbdo,1rpid6,askscience,top_week,13
DanielSank,"Since my other post was so long:

&gt; If none of the particles around it have definite positions either, how does any particle in the universe ""know"" for sure that it was observed?

They don't. Particles don't do anything special when they're observed. We can propose a self-consistent theory of quantum mechanics in which the wave functions don't actually collapse and take on a definite position. The *appearance* of definite position only happens for in cases where only subsets of the system's information is available, which happens to be the case relevant when humans are involved. From the perspective of someone with fuller information about the system, the collapse that *you* see may not happen.

The falsifiability of this theory (described more fully in my other post) is definitely open for discussion. I'd love to have that discussion with anyone else who's interested.",null,0,cdppng9,1rpid6,askscience,top_week,5
RetraRoyale,"DanielSank's description is very good, so you probably want to read it over carefully. 

Essentially, ""measurement"" and ""observation"" are poor words for describing what happens. An experiment with observed particle is *fundamentally* different from an experiment with an unobserved particle because the observer is made out of a huge number of particles.

If you have a train barreling down the track in one experiment, and in another you build a huge wall in the middle of the track, it's silly to ask how the train ""knows"" to derail it'self when you build a wall there. *You built a huge wall on the track*! That's how it 'knows' to derail itself.",null,0,cdpricr,1rpid6,askscience,top_week,3
foreveraloneirl,Depends on what quality footage you would find acceptable. Smaller lens/sensor = poorer quality. This is why professional photographers despite many years of technological advancements still need to haul around massive lenses and cameras.,null,0,cdpm3xx,1rpiq9,askscience,top_week,10
asdfWriter,It all depends on the amount of pixels you want do have. Let's say the minimum resolution you accept is 1000px times 1000px. According to [this paper](http://isl.stanford.edu/groups/elgamal/abbas_publications/C072.pdf) the optimal (and minimal) pixel width in a CMOS-sensor is 6.5um -- that would result in a minimum sensor-width of 1000px x 6.5um/px = 6500um = 6.5mm. Of course future technoligies might be able to shrink the pixel width.,null,1,cdpnkpq,1rpiq9,askscience,top_week,5
mcnubbin,"This question reminds me of the semi-autobiographical Phillip Dick novel, A Scanner Darkly.  


""What does a scanner see? Into the head? Down into the heart? Does it see into me? Into us? Clearly or darkly? I hope it sees clearly because I can't any longer see into myself. I see only murk. I hope for everyone's sake the scanners do better, because if the scanner sees only darkly the way I do, then I'm cursed and cursed again.""",null,11,cdpokzn,1rpiq9,askscience,top_week,1
From_Ashes_Rize,"The answer is sort of. You can not create an extremely small flame from a source such as a traditional candle. But there is a way to create micro flames.  This is called microcombustion and they are flames smaller than the width of a millimeter.  They are typically made in small hard ceramic combustors (such as alumina oxide) and they burn at temperatures of up to 2000 C. The combustors have small channels through which the fuel flows and the flame burns. They are typically fueled by hydrocarbons and oxygen. Methane, butane, propane, etc.

The two inlet flows of the fuels meet and combine at some point and then you have your combustible gas. The point at which the two flows meet exhibits turbulent flow and this is the limiting factor at ignition for the combustor due to wall conditions and fluid dynamics not worth explaining here. Turbulent flow tends towards extinction for a flame. Typically a large candle type flame will be seen at the end of the exhaust channel.

As the combustor ramps up to its max temp (steady state) the walls of the exhaust channel heat up and begin to provide the thermal energy necessary for the gases within the channel to ignite. You will hear loud pops as ignition events begin to occur within the channel. As the temperature of the wall increases due to the heat seen from the exhaust flame, the ignition events begin occurring closer to the mixing points of the 2 fuels.

Finally at steady state the walls of the combustor at the mixing point of the fuels is so hot that the gases ignite immediately upon mixing. Due to the turbulent nature of the flow, they go out almost immediately.  They are also almost immediately replaced by fresh uncombusted fuel which them ignites. This cycle repeats itself many times per second and you end up with a sustained micro flame. The ratio of gases determines how many times per second ignition occurs and you can actually say that the flame has a frequency (I.e. You can hear the flame buzz while it is lit).

So in short, yes micro flames are possible, just not from a candle.


Tl;dr
Teeny candles don't work. Burn hydrocarbons in a ceramic combustor for super hot, buzzing micro flames. Science is awesome.",null,106,cdplrw5,1rpive,askscience,top_week,596
Freemantic,"How sustainable do you want it?

Because technically little miniature ""fires"" are happening all the time at the molecular level. So if you count those, the smallest fire would be that.

The sustainable fire is different, and would change depending on what planet you were on and depending where on Earth you were because pressure changes. The limiting factors being volume, scale, and pressure.

You can scale a flame down indefinitely, but there will be a certain point where it can't sustain a flame. The amount of heat a flame produces/loses isn't a 1:1 ratio, and as it shrinks it loses more heat than it produces. As far as an exact size? Honestly, you're not going to get much smaller than a match. 2-3mm is probably the smallest ""real"" flame without cheating.
",null,1,cdplpdz,1rpive,askscience,top_week,35
dizekat,"If we are speaking of a steady, hot flame in the air at room temperature and atmospheric pressure, there is a limit because the energy loss at a given temperature would be proportional to the surface area (size squared) while the output power is proportional to the volume. 

Consequently, very small flames lose more heat at the combustion temperature than they produce. Unless of course you use a gas mix that combusts at a very low temperature, in which case the flame could be arbitrarily small (but it will be cold and it is not clear if you should call that a flame). 

Personally, the smallest actual in-air flame that I have seen was smaller than 1 mm, at the end of a hypodermic needle feed oxygen and hydrogen mixture. You can probably go somewhat smaller with hydrocarbons and oxygen (e.g. acetylene and oxygen). Maybe smaller still with chlorine trifluoride or something similar as an oxidizer, but it would be difficult to make a nozzle that can withstand it.

It should be possible to make a smaller flame at higher temperature, or in a less thermally conductive environment than air.",null,0,cdpqfsu,1rpive,askscience,top_week,12
LakeSolon,"Some good comments so far but to put it simply:

Surface area and volume do not scale the same. When a flame gets small enough it doesn't have enough combustion volume to overcome the heat loss, so it cools too fast and dies out.

That's most of the answer for most of the situations.",null,0,cdpp37h,1rpive,askscience,top_week,9
borthuria,"Well you have to know that a flame is also a plasma (the fourth state of the matter (solid liquid and gazeous being the other three)) wich consist of a cloud of electron et ion (atoms that lost their electron).

It would be possible to create a plasma as small as you want to do, as long as you can control it.  Frome here on, we would need an experienced plasma physicist.  You can control a plasma with a magnetic field (since it's only electron and Ions, it's ""pretty"" easy thing to do (we know how to do it and we can do it)).

I'm a physic teacher and in a microfabrication lab, we create a plasma in a vaccuum chamber to clean a silicium chip.",null,0,cdpt5kj,1rpive,askscience,top_week,5
leftylouis,"well it depends on what you would call a flame but in theorie you can let just a few atoms reacts and they will release light and heat doing this (a flame). so id say that it is possible at an atomic level. 

Edit: But a candle ofourse has a lot of limiting factors. it needs the right amount of energy to get the reaction started and to keep it going. if the flame gets to small then it cant supply itself of enough parrafine and it will stop burning. if the fuse is too short then the surface area to burn with enough energy to sustain. 
but then also you got the temprature of the enviroment and the  amount of oxygen and the pressure etc.. 

Oh and about the reguar/birthday candle flame size. the size of the candle doesnt really matter it is the fuel and oxygen supply that matter. 
",null,1,cdpk6ay,1rpive,askscience,top_week,4
Conotor,"Viscous effects scale with 1/VL (part of Reynolds Number) where V is a typical speed in your scenario, and L is a typical length.  Viscosity tends to mix and split up flows, so at very small scales your flame would become less smooth and steady.",null,0,cdpr74b,1rpive,askscience,top_week,3
jowofoto,"Just thought I'd put this out there.. Cavitation is something that we've all seen, heard and even felt. It causes a microscopic 'light show' and temperatures yielding thousands of degrees kelvin. It is a big reason why water pumps and propellers fail. 
[Cavitation](http://en.wikipedia.org/wiki/Cavitation)
[Cool video](http://www.youtube.com/watch?v=G5D0F0tDYZQ)",null,0,cdpm10s,1rpive,askscience,top_week,2
WillNotBeAttending,"I always used to wonder about this as a kid, because in books like The Borrowers series the characters would have a tiny campfire made out of twigs. It never made sense to me because twigs burn really quickly, it didn't seems possible to have a small fire without controlling the flow of oxygen to the flame to slow down combustion.",null,0,cdpotjs,1rpive,askscience,top_week,3
From_Ashes_Rize,"The answer is sort of. You can not create an extremely small flame from a source such as a traditional candle. But there is a way to create micro flames.  This is called microcombustion and they are flames smaller than the width of a millimeter.  They are typically made in small hard ceramic combustors (such as alumina oxide) and they burn at temperatures of up to 2000 C. The combustors have small channels through which the fuel flows and the flame burns. They are typically fueled by hydrocarbons and oxygen. Methane, butane, propane, etc.

The two inlet flows of the fuels meet and combine at some point and then you have your combustible gas. The point at which the two flows meet exhibits turbulent flow and this is the limiting factor at ignition for the combustor due to wall conditions and fluid dynamics not worth explaining here. Turbulent flow tends towards extinction for a flame. Typically a large candle type flame will be seen at the end of the exhaust channel.

As the combustor ramps up to its max temp (steady state) the walls of the exhaust channel heat up and begin to provide the thermal energy necessary for the gases within the channel to ignite. You will hear loud pops as ignition events begin to occur within the channel. As the temperature of the wall increases due to the heat seen from the exhaust flame, the ignition events begin occurring closer to the mixing points of the 2 fuels.

Finally at steady state the walls of the combustor at the mixing point of the fuels is so hot that the gases ignite immediately upon mixing. Due to the turbulent nature of the flow, they go out almost immediately.  They are also almost immediately replaced by fresh uncombusted fuel which them ignites. This cycle repeats itself many times per second and you end up with a sustained micro flame. The ratio of gases determines how many times per second ignition occurs and you can actually say that the flame has a frequency (I.e. You can hear the flame buzz while it is lit).

So in short, yes micro flames are possible, just not from a candle.


Tl;dr
Teeny candles don't work. Burn hydrocarbons in a ceramic combustor for super hot, buzzing micro flames. Science is awesome.",null,106,cdplrw5,1rpive,askscience,top_week,596
Freemantic,"How sustainable do you want it?

Because technically little miniature ""fires"" are happening all the time at the molecular level. So if you count those, the smallest fire would be that.

The sustainable fire is different, and would change depending on what planet you were on and depending where on Earth you were because pressure changes. The limiting factors being volume, scale, and pressure.

You can scale a flame down indefinitely, but there will be a certain point where it can't sustain a flame. The amount of heat a flame produces/loses isn't a 1:1 ratio, and as it shrinks it loses more heat than it produces. As far as an exact size? Honestly, you're not going to get much smaller than a match. 2-3mm is probably the smallest ""real"" flame without cheating.
",null,1,cdplpdz,1rpive,askscience,top_week,35
dizekat,"If we are speaking of a steady, hot flame in the air at room temperature and atmospheric pressure, there is a limit because the energy loss at a given temperature would be proportional to the surface area (size squared) while the output power is proportional to the volume. 

Consequently, very small flames lose more heat at the combustion temperature than they produce. Unless of course you use a gas mix that combusts at a very low temperature, in which case the flame could be arbitrarily small (but it will be cold and it is not clear if you should call that a flame). 

Personally, the smallest actual in-air flame that I have seen was smaller than 1 mm, at the end of a hypodermic needle feed oxygen and hydrogen mixture. You can probably go somewhat smaller with hydrocarbons and oxygen (e.g. acetylene and oxygen). Maybe smaller still with chlorine trifluoride or something similar as an oxidizer, but it would be difficult to make a nozzle that can withstand it.

It should be possible to make a smaller flame at higher temperature, or in a less thermally conductive environment than air.",null,0,cdpqfsu,1rpive,askscience,top_week,12
LakeSolon,"Some good comments so far but to put it simply:

Surface area and volume do not scale the same. When a flame gets small enough it doesn't have enough combustion volume to overcome the heat loss, so it cools too fast and dies out.

That's most of the answer for most of the situations.",null,0,cdpp37h,1rpive,askscience,top_week,9
borthuria,"Well you have to know that a flame is also a plasma (the fourth state of the matter (solid liquid and gazeous being the other three)) wich consist of a cloud of electron et ion (atoms that lost their electron).

It would be possible to create a plasma as small as you want to do, as long as you can control it.  Frome here on, we would need an experienced plasma physicist.  You can control a plasma with a magnetic field (since it's only electron and Ions, it's ""pretty"" easy thing to do (we know how to do it and we can do it)).

I'm a physic teacher and in a microfabrication lab, we create a plasma in a vaccuum chamber to clean a silicium chip.",null,0,cdpt5kj,1rpive,askscience,top_week,5
leftylouis,"well it depends on what you would call a flame but in theorie you can let just a few atoms reacts and they will release light and heat doing this (a flame). so id say that it is possible at an atomic level. 

Edit: But a candle ofourse has a lot of limiting factors. it needs the right amount of energy to get the reaction started and to keep it going. if the flame gets to small then it cant supply itself of enough parrafine and it will stop burning. if the fuse is too short then the surface area to burn with enough energy to sustain. 
but then also you got the temprature of the enviroment and the  amount of oxygen and the pressure etc.. 

Oh and about the reguar/birthday candle flame size. the size of the candle doesnt really matter it is the fuel and oxygen supply that matter. 
",null,1,cdpk6ay,1rpive,askscience,top_week,4
Conotor,"Viscous effects scale with 1/VL (part of Reynolds Number) where V is a typical speed in your scenario, and L is a typical length.  Viscosity tends to mix and split up flows, so at very small scales your flame would become less smooth and steady.",null,0,cdpr74b,1rpive,askscience,top_week,3
jowofoto,"Just thought I'd put this out there.. Cavitation is something that we've all seen, heard and even felt. It causes a microscopic 'light show' and temperatures yielding thousands of degrees kelvin. It is a big reason why water pumps and propellers fail. 
[Cavitation](http://en.wikipedia.org/wiki/Cavitation)
[Cool video](http://www.youtube.com/watch?v=G5D0F0tDYZQ)",null,0,cdpm10s,1rpive,askscience,top_week,2
WillNotBeAttending,"I always used to wonder about this as a kid, because in books like The Borrowers series the characters would have a tiny campfire made out of twigs. It never made sense to me because twigs burn really quickly, it didn't seems possible to have a small fire without controlling the flow of oxygen to the flame to slow down combustion.",null,0,cdpotjs,1rpive,askscience,top_week,3
abt137,"Yes, if you follow the Formula 1 this is a common issue. Drivers in pursuit of another tend to have issues with engine temperature when they are not able to overtake the rival. The more laps they stay behind another car the bigger the overheating risk, and this overheating also applies to other car systems like tires and brake discs (tip: F1 car have air cooled engines). This is referred as being in the ""dirty air"" or ""suck dirty air"", this overheating apart from represent a risk for the engine tends to manifest itself faster in tires and breaks causing unexpected degradation in the tires and malfunctioning of the brakes and also alter the aerodynamic flow so your wings, spoilers etc will not create the desired grip force pushing the car down against the ground.
When the situation above happens you could see drivers getting out of the drag of the car in ahead not trying to overtake but simply to suck fresh air.
This refers to F1 specifically and i can't say about common street cars but I'd say is unlikely to happen since they are liquid cooled; if you are in the mid of a traffic jam the fan starts and cools down the engine temp. Cannot say about other Motorsports as I do not follow them.",null,1,cdpkp45,1rpiyx,askscience,top_week,5
patchgrabber,"This question is a little vague; there are a great deal of hormones affected by light, and ""affect a plant cell on a molecular level"" is incredibly broad, and has different implications depending on the age of the plant in question, not to mention differences between taxa.",null,0,cdpmnxr,1rpj0x,askscience,top_week,3
yankee333,"Like patchgrabber said, very broad question. Generally though, light intensity is actually the number of photons, i.e. quanta. So increasing light intensity would activate more light-sensitive proteins, and lead to a stronger physiological effect as more photons activate more light-sensitive proteins through biological conversion of the photon to an electron.",null,2,cdpqz6p,1rpj0x,askscience,top_week,1
Davecasa,"The space elevator concept involves putting something really heavy in geostationary orbit around Earth (a bit beyond this, actually, so that it generates a centrifugal force to hold up the cable). Geostationary orbit means that you're still orbiting (and still in free fall), you just happen to be falling around Earth once every time the planet rotates, so you stay above the same point on the surface. The altitude you need to be in for your orbit to have this property is 35,786 km. The ISS is only at 416 km, so it moves very quickly relative to Earth's surface.

In terms of orbital mechanics, there's nothing special about a geostationary orbit. You would still experience weightlessness.",null,2,cdpkwrp,1rpj3t,askscience,top_week,8
Fringe_Worthy,"Looking at [http://en.wikipedia.org/wiki/Space_elevator](http://en.wikipedia.org/wiki/Space_elevator) you want your space elevator to be balanced just past the Geostationary orbit. 

It looks like if you're connected to the elevator, you'd be feeling gravity opposed by centrifugal force.  It would go from 1g towards earth at the base, decreasing to 0g at the geostationary orbit (35,786 km) and then increasing, away from earth as you keep on going upwards past there.

A space elevator with a center of mass at ISS level would be either be in the final process of falling down, or have dangly bits moving at ISS orbital velocity ( 7km/s? ) That seems like it would generate excessive amounts of drag, debris, and/or self destruction.",null,0,cdpm2aq,1rpj3t,askscience,top_week,5
stuthulhu,"Zero gravity and weightlessness are very different things. People in space appear weightless because they are in free fall. They still are experiencing gravity. If the gravity of the Earth did not reach as high as orbit, things would not orbit. Ergo, the moon would fly away.",null,2,cdpl33k,1rpj3t,askscience,top_week,3
nucleomancer,"Basically No. Remember that Felix BaumThingies. He was way up there and he still fell like a brick. (Proof number one) (There is a small reduction in gravity as you go up, but that is near to nothing.)

Secondly. Weightlessness in space is only relative to the shuttle/station. And is caused by its high velocity. As the shuttle passed overhead, gravity is pulling it in under just the right force to make it go in circles.

To be more precise the shuttle is traveling at just the right speed where the pull of gravity causes it to go in circles.

Same thing goes for the moon by the way. All the way out there. THAT heavy and still going neatly in a ""circle"" around the earth.

The moon goes round every month, the shuttle every couple of hours.

The idea with an elevator is that it is forced to go at the speed of the earths rotation, even though it's distance would allow for a lower speed, thereby pulling the wire taught and allowing ""small"" objects to travel up. When the object gets all the way up there it travels at the same speed and can be released into orbit.",null,7,cdpk4vx,1rpj3t,askscience,top_week,2
Broes,"Gravity is every, even if you would be 1.000.000.000 miles from the sun, it would still exert a pull on you. The pull will however be small.
If you would mean weightless, you can become 'weightless' just by jumping of something. During your fall you will be weightless... until you hit something. To be permanetly weightless, all that is required is that you somehow avoid hitting something that will stop you from falling. ",null,7,cdplp6i,1rpj3t,askscience,top_week,2
MotoEnduro,"Yes, warm early spring conditions can cause buds on trees to open prematurely. If the buds open and begin to develop and then there is a hard frost the buds can die. The results of this can be damaging. For example if flower buds freeze there will be no flowers to fertilize, and therefore no fruit. With leaf buds, the bud may die, but dormant buds will usually awake and sprout new leaves, but this takes a serious toll on the vigor of the tree as It has already used its stored energy from last season to produce this seasons leaves. This happened in north America 2 springs ago, where there was a week in march with 70 degree days, followed by a hard freeze, and resulted in a 50% decrease in apple production in many areas.
Source: I am a forester.",null,7,cdplsrs,1rpkur,askscience,top_week,64
LetsSmokeWeedAboutIt,"According to this site,http://northernwoodlands.org/outside_story/article/how-do-trees-know-when-to-leaf-out-in-the-spring, trees have to go through a warming phase and then a refreezing phase before bud break occurs.  So they've basically developed a system to account for thawing in late winter so they aren't fooled into budding before another frost.  Of course some plants and trees will get tricked because weather can vary a lot, there can be multiple thaws and refreezes. Those plants might not die necessarily, but they waisted however much energy it took them to make the buds they lost.  But obviously most species seem to be pretty good or we wouldn't have as many plants as we do in northern climates.  

I'm sure theres more to it even that, like hormones, circadian rhythms, and light cycles stuff like that.  Plant's are just way more awesome then we give them credit for too.  If you have the time take a botany class at your local college, I'm in one right now and I get my mind blown to pieces all the time its great.",null,2,cdpm3kz,1rpkur,askscience,top_week,6
MotoEnduro,"Yes, warm early spring conditions can cause buds on trees to open prematurely. If the buds open and begin to develop and then there is a hard frost the buds can die. The results of this can be damaging. For example if flower buds freeze there will be no flowers to fertilize, and therefore no fruit. With leaf buds, the bud may die, but dormant buds will usually awake and sprout new leaves, but this takes a serious toll on the vigor of the tree as It has already used its stored energy from last season to produce this seasons leaves. This happened in north America 2 springs ago, where there was a week in march with 70 degree days, followed by a hard freeze, and resulted in a 50% decrease in apple production in many areas.
Source: I am a forester.",null,7,cdplsrs,1rpkur,askscience,top_week,64
LetsSmokeWeedAboutIt,"According to this site,http://northernwoodlands.org/outside_story/article/how-do-trees-know-when-to-leaf-out-in-the-spring, trees have to go through a warming phase and then a refreezing phase before bud break occurs.  So they've basically developed a system to account for thawing in late winter so they aren't fooled into budding before another frost.  Of course some plants and trees will get tricked because weather can vary a lot, there can be multiple thaws and refreezes. Those plants might not die necessarily, but they waisted however much energy it took them to make the buds they lost.  But obviously most species seem to be pretty good or we wouldn't have as many plants as we do in northern climates.  

I'm sure theres more to it even that, like hormones, circadian rhythms, and light cycles stuff like that.  Plant's are just way more awesome then we give them credit for too.  If you have the time take a botany class at your local college, I'm in one right now and I get my mind blown to pieces all the time its great.",null,2,cdpm3kz,1rpkur,askscience,top_week,6
MrQuiver,"The gaps in the cage just have to be significantly smaller than the wavelength of the radiation. You know that metal screen that covers the front ""window"" of your microwave? Faraday cage. Microwave radiation has a wavelength of about 1 cm. Visible light /starts/ at 0.0005 mm! That's a big difference, so you can definitely block microwaves and let in visible light. Make a cage with a reasonably thick conductor (more than one layer of tin-foil) and gaps of 0.5 cm or less.",null,0,cdpl0o9,1rpl5q,askscience,top_week,7
yankee333,"picture 1: both

picture 2: both

drawing option 1 is correct

Fibronectin does not embed itself inside plasma membranes, but binds to integrins that themselves traverse the membranes. So it would be on the outside of the membrane, connected to a protein that is itself embedded in the lipid bilayer.

The reason why both pictures could indicate both is because basement membranes contain collagen.",null,1,cdpqwe0,1rpq89,askscience,top_week,2
walexj,"When the ball falls off as it's stationary, you notice that it tips to one side.

The same thing happens when the ball is spinning. It begins to tip to one side. But wait! The ball is spinning, so the side that it was tipping toward is now on the opposite side of your finger, so the ball starts 'falling' back toward the axis of spin. So, basically, so long as the ball is spinning fast enough, it will right itself every time it begins to tip. If it's right on the cusp of spinning too slowly to correct itself in time then it begins to wobble. When it's too slow, it falls off your finger as expected.

This is one of the features of angular momentum and the gyroscopic effect.

And the Earth's rotation has nothing to do with it, but if the Earth stopped spinning, we'd observe ~1000 km/h winds if we weren't also thrown into the nearest wall at ~1000 km/h ourselves, while every building (save for the few near the poles) would get torn off their foundation.",null,0,cdppoyr,1rpvm6,askscience,top_week,3
rcs_thruster,"For the earth question: The Earth would start to spin again, because of the Moon is revolving around the Earth. At the same time, the Moon would lose orbital velocity and eventually fall to the Earth. Feel free to correct me if I'm wrong.",null,0,cdpt270,1rpvm6,askscience,top_week,1
_ask_,"When a ball spins, centripetal force occurs, and centripetal force always points towards the center of the object it is emmiting from. If your finger is on the center of the ball, and if the ball is spinning, the ball will attempt to stay on your finger.",null,2,cdprpqs,1rpvm6,askscience,top_week,2
MrQuiver,"The water level stays the same. As soon as something is floating on a liquid, it will displace a volume of that liquid equal to the weight of itself and everything in it. This means it raises the water level by the same amount as if one had added an amount of liquid equal to its weight.

The ice is inside the boat, so the water level has already raised up to account for an amount of liquid water equal to the weight of the ice. When the ice is dumped overboard it is still floating in the water, so nothing changes. When the ice melts, it turns into an amount of water equal to its own weight (obviously, no mass can be lost in the melting process). So, no change to the water level.",null,1,cdpod85,1rpwbu,askscience,top_week,6
null,null,null,1,cdpop5d,1rpwbu,askscience,top_week,2
0ndem,"When the block goes into the water the level will go up.

When the block melts it will go down but should still be above the original level because you did add material.

You can test this with a glass of water and an ice cube.


This is assuming the block has a large enough size to impart a noticeable difference and the water that was frozen was the same as that which is in the pool.

",null,13,cdpobyf,1rpwbu,askscience,top_week,2
erath_droid,"Half of your genes will come from your father, half will come from your mother. (What actually happens is a bit more complex than that, but this is close enough for this discussion.) This means that any single parent/child combination will have half of the same genes.

When it comes to siblings (from the same parents) you can have as many as 0% of the same inherited genes or as much as 100% of the same inherited genes.

For example, say for one particular allele the father has type Aa and the mother has type Bb. This means that the child can have one of four allele combinations: AB, Ab, Ba and ab. It's entirely possible for one child to inherit the combination AB and the other inherit ab. This would be a case of zero shared genes for that particular trait.

However, in the real world, due to the large number of genes that are inherited siblings will share close to 50% of the same (inheritable) genes making them in general as genetically similar to each other as they are to their parents.",null,4,cdpp17m,1rpxrq,askscience,top_week,33
null,null,null,4,cdpok54,1rpxrq,askscience,top_week,25
Xinlitik,"You are 50% related to mom and dad--this is basically static, although de novo changes and certain recombination events do account for a small deviation. Your non-identical siblings are theoretically 0-100% related to you, but since the genome is so huge, it's going to be close to 50%. That is, you could somehow get the exact 50% of dad's genome that your brother did not get, but the chance is extremely low, just like how flipping a coin 1000 times is probably not going to give you 1000 heads.",null,1,cdpq1t9,1rpxrq,askscience,top_week,5
OnceReturned,"In the average, oversimplified, model case, you will share 50% of your variable genes with both your siblings and your parents, making you equally similar to both, though the 50% that you share with each is not likely to be the same 50%.

However, you could think about the question with an eye towards probabilities and find that the two cases are not quite as similar as they seem.  Specifically, if the question is, ""Are you more likely to find a sibling pair that is &lt; 50% identical than you are to find a parent-child pair that is &lt; 50% identical,""  the answer is yes.  Here's why:

The rule that we're using to figure this out is that each parent contributes ~50% of their genes to their children.  

Because the genome is large, there are many, many different combinations of contributions from each parent that satisfy this rule of thumb of each parent having provided 50% - ranging from the same 50% from each parent in each sibling (making the siblings identical), to the opposite 50% from each parent to each sibling (making the siblings have 0% of their variable genes in common).  

In the case of your own child, no matter which 50% of your variables genes you contribute to their genome, you share 50% of variable genes with them.  

Therefore, as long as the rule of thumb of 50% parental contribution holds true, you are guaranteed to be 50% similar to your offspring.  While under the same circumstances, you may be anywhere from 0-100% similar to your siblings.  In other words, there is far more variability in the similarity between siblings than there is between parents and their children, though all that variability centers around the same point that the parent-child pairs exhibit consistently.",null,0,cdpsa4f,1rpxrq,askscience,top_week,2
LietKynes62,"On average, you're slightly more related to your parents. Since the creation of your siblings involved an entirely separate meiosis and fertilization process, they have had an additional opportunity for random mutation to occur. Meaning, they have twice as many mutations different from you than your parents would. Also taking into account sex chromosome differences in males in females, there's other variables involved. Practically speaking, though, you share approx 50% DNA with your parents AND your siblings.

To simplify it -- you are guaranteed to get 50% of your DNA from either parent. You may have less in common with your sibling depending on mutations and gender(since the X chromosome complicates things).",null,0,cdq3vfp,1rpxrq,askscience,top_week,1
GProteins,"They developed them separately. Current knowledge is that birds evolved from small dinosaurs, who had normal toothed jaws. This theoretically happened around the [mesozoic era](http://en.wikipedia.org/wiki/Mesozoic), which started around 250 million years ago.

Squids and octopi, on the other hand, are evolved from earlier forms of mollusks, which separated themselves out from other life forms in the some time during the [cambrian era](http://www.jakobvinther.com/Mollusc_evolution.html), which ended about 450 million years ago.",null,0,cdppwko,1rpxux,askscience,top_week,30
LietKynes62,"They developed seperately, which would be an example of analogous structures. This means they have similar appearance and function, but developed independently. A common example involved the octopus is their eye compared to the mammalian eye. We know, because of the organization of its layers, that the octopus eye developed independently from the mammalian eye. However, they share similar appearance and function.

Another example would be comparing an insect's wings to a bird's wings -- they look similar, are both used for flying, but have different structures and evolved separately.",null,0,cdq3rnj,1rpxux,askscience,top_week,2
GProteins,"They developed them separately. Current knowledge is that birds evolved from small dinosaurs, who had normal toothed jaws. This theoretically happened around the [mesozoic era](http://en.wikipedia.org/wiki/Mesozoic), which started around 250 million years ago.

Squids and octopi, on the other hand, are evolved from earlier forms of mollusks, which separated themselves out from other life forms in the some time during the [cambrian era](http://www.jakobvinther.com/Mollusc_evolution.html), which ended about 450 million years ago.",null,0,cdppwko,1rpxux,askscience,top_week,30
LietKynes62,"They developed seperately, which would be an example of analogous structures. This means they have similar appearance and function, but developed independently. A common example involved the octopus is their eye compared to the mammalian eye. We know, because of the organization of its layers, that the octopus eye developed independently from the mammalian eye. However, they share similar appearance and function.

Another example would be comparing an insect's wings to a bird's wings -- they look similar, are both used for flying, but have different structures and evolved separately.",null,0,cdq3rnj,1rpxux,askscience,top_week,2
OnceReturned,"A comprehensive answer to that question would have many components, but in animals there are a few dominant factors:

DNA - Cell division is not perfect, and DNA damage occurs naturally due to environmental factors and normal cellular processes.  Genomic robustness and DNA repair capacity is a huge factor in lifespan determination.  DNA damage is generally considered the leading cause of aging.  Species with more robust genomes and better DNA repair mechanisms tend to have greater lifespans.

Physical fragility - There are many potentially fatal events which individuals have some probability of experiencing in any given time period.  These include things like harsh environmental conditions (i.e. a bad winter), infectious diseases, and encounters with predators.  Every organism has some set of such events which are likely to be fatal, and which occur with some probably, such that the more time passes, the more likely the organism is to have been exposed to such an event.  The more sensitive an organism is to these things, the shorter its lifespan is expected to be.  For example, small animals are particularly susceptible to predation, so the odds that they have a fatal encounter with a predator in any time period are higher than the those same odds for larger animals that are higher on the food chain.  

Biological wear and tear:  Tissue and organs get worn out through use.  Small animals generally have higher metabolisms, which amounts to something similar to more ""friction"" or ""wear and tear"" on all of their systems (i.e. more heartbeats per minute).  So, they don't typically live as long as typical larger creatures with slower metabolisms. 

Generally, larger animals live longer.  Dogs are something of an exception.  It's not exactly clear why large dogs don't live as long, but it likely has something to do with the fact that domesticated dogs are highly inbred.  This means they have genetic risk factors that are much less common in wild animals.  This manifests in a variety of ways.  For example, large dogs are more likely to get cancer, and this is likely because they simply grow more cells throughout their lives.",null,0,cdptcay,1rqeob,askscience,top_week,2
RetraRoyale,"According to General Relativity (as I understand it), and in particular, [Mach's Principle](http://en.wikipedia.org/wiki/Mach's_principle), A rotating universe would be indistinguishable from a non-rotating one in which you yourself were spinning. That is, if the universe were spinning, it would emit gravitational waves in just the right way that made you think you were spinning and the universe wasn't.

As for everything else you've said, it's not very clear what you're talking about because our description of viscosity and friction don't really work for General Relativity, so you're inappropriately generalizing those concepts.",null,3,cdpykkm,1rqeus,askscience,top_week,19
Sexy_Philistine,"Godel had some interesting solutions to the field equations that implied a rotating universe (and time travel!), though they don't describe our universe.  Still interesting in that it's a consistent description of a hypothetical universe 

http://en.wikipedia.org/wiki/G%C3%B6del_metric#Cosmological_interpretation",null,0,cdq2pgn,1rqeus,askscience,top_week,2
null,null,null,1,cdpwy9i,1rqeus,askscience,top_week,2
UnkeptPorpoise,"What you are describing with matter bending space time is, I believe part of special relativity? Not sure how that relates to the universe as a whole spinning. The question would be spinning in relation to what? Maybe I just misunderstood your question. I am by no means an expert though.",null,9,cdpwzj7,1rqeus,askscience,top_week,5
