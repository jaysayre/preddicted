author,comment,distinguished,downs,id,post,subreddit,type,ups
Arrogus,"Loud noises and bright lights are damaging because of the energy they carry; scents, on the other hand, are merely particles suspended in the air. Sure, many chemicals could do serious damage to your olfactory receptors if you inhaled them through your nose, but it would be because of their reactivity, not their pungency. In such a scenario, damage to your lungs would probably be your primary concern.",null,36,cdnmjtk,1rim55,askscience,top_week,192
Zukuto,"/u/Arrogus has it. it isnt that they are too *smelly* that breaks your nose, but that they are comprised of *toxic* fumes; sometimes they are smelly and other times not.

one time i had to clean out a Hair salon next to a business i was working for in a strip mall; i was the only one who posessed a mask that also had eye protection. the salon owner had tried to get a stain off the floor using Bleach and Ammonia. i let all her hoses run onto the floor and pushed the watery mustard gas out of the building before calling the Fire Dept and a HAZMAT team. 

i got a free haircut for my trouble. the salon owner got the shock of her life.",null,0,cdnymdg,1rim55,askscience,top_week,7
ubcokanagan,"No, you perceive smells when aromatic compounds bind neurons in your nasal passages.  The binding causes these neurons to fire which send a signal to your brain letting you know you just stepped in dog crap.  A very strong smell will innervate many neurons but it wont damage them.

If the odour is present for a long period of time, desensitization of the neurons will occur, and they will be less likely to fire in the absence of an increase in concentration of the aromatic molecules.  This is why smokers don't realize that they smell terrible all the time (well that and any damage caused by the smoke).

If the strong smell is caused by something toxic then yes it can cause damage, but I believe this would be caused by a property of the offending chemical, as opposed to it overstimulating a nerve.",null,0,cdo0u8c,1rim55,askscience,top_week,3
null,null,null,0,cdo28pj,1rim55,askscience,top_week,2
Philosophisation,"As with any sensory input it can be damaged via chemicals. Your chemoreceptors in your sinus will not however get destroyed by excessive use. Imagine a bathtub with the plug pulled. This would be ~chemoreceptor being overloaded with scent molecules or similarly shaped molecules at least. Anything that fits through goes through and is registered. But add oil and hey? It doesn't go through for a while. This is one reason for desensitized smell. Another is that the sensory nerve endings present to receive the signal from the sensory organelle(dendrites) fires so often that the brain starts filtering it out as useless signal, same as white noise. So no excessive safe smell will desensitize but only harmful molecules may ruin smell receptors.",null,0,cdo2jpc,1rim55,askscience,top_week,1
freeze4111,"All odors, indeed anything that gets in your nose, damages it to a very small extent. The strength of the odor isn't necessarily the measure for its destructive capability however; mostly it is corrosive acids or things like smoke which do the real damage: look up tobacco smoking and its damage on the sense of smell to get an idea: http://www.ncbi.nlm.nih.gov/pubmed/22776624. 

Heavy odors, like blue cheese or something like that, may briefly block a number of receptors making your sense of smell not as good as what it could be, but this is very temporary (no more than a few seconds). In addition, your brain can adapt or habituate to odors, making them less noticeable (this is harder to do as the odor gets stronger). You'll probably notice this with your own perfume/deodorant throughout the day. 

Any damage done to your sense of smell is much, much easier to recover from compared to other senses because the neurons involved can turnover and regenerate (these are the only neurons that can). Some environmental experiences can make the turnover slower (as can age) but overall your sense of smell will recover from anything you throw at it. 

Something I find interesting- the neurons that take information from your sense of smell transmit that information to the brain through neurons passing a structure called the cribiform plate via little holes. If these neurons are severed, they can regenerate, but usually can't find their way through the holes again; a case of this unique quality being utterly useless! 

",null,0,cdo3j4q,1rim55,askscience,top_week,1
paulHarkonen,"I work in the natural gas industry and thus work with odorant (that rotten eggs smell in gas).  Odorant is one of the most powerful smells around but all the health concerns surrounding it involve how your body reacts to strong odors.  Very strong negative smells can pose a nausea risk, along with some breathing concerns because your body expects strong bad smells to also be toxic.  None of the Msds information covers permanent damage to your sense of smell.  (Although there is a short term effect as your nose becomes overwhelmed by the one strong odor and stops caring about other weaker odors).

Its not a super scientific source, but there has been a fair bit of testing to create the MSDS information.",null,0,cdo8mi4,1rim55,askscience,top_week,2
Astrokiwi,It's most likely flat. [This post in the FAQ](http://www.reddit.com/r/sciencefaqs/comments/v97po/is_the_universe_infinite/) should be useful.,null,0,cdnkzfg,1riij0,askscience,top_week,3
skleats,"Check out the dog genome sequence - there's lots of great examples of the role of repeated sequences in the selection history of various breeds. For some good sources:

[Here's](http://genomebiology.com/2011/12/2/216) an overview of the dog genome, with some info about repeated sequences.

[Here's](http://www.pnas.org/content/107/3/1160.full) a good rundown of the genomic variety between breeds.

And, for all the money, [here's](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1356118/) a study showing that the diversity is linked to SINEs.

Evolution gets driven by selection on random variation, but repetitive sequences drive random variation.",null,0,cdnwr9q,1rih74,askscience,top_week,2
astazangasta,"Most of the genomes that are full of crap like LINEs and retrotransposable elements are higher eukaryotes. Prokaryotic genomes are usually much tighter and filter these sorts of things out. That is probably because higher eukaryotes can tolerate the addition of some extra sequence in the genome - over the course of a large multicellular organism's lifetime, replicating a few extra base-pairs worth of sequence every time a cell divides is not that big a deal. A prokaryote, on the other hand, can optimize over each division, since each replication cycle produces a new generation.

This probably means that repetitive elements in the (e.g.) human genome are mostly harmless crap - they are annoying, but not that important. But if we could get rid of them (like prokaryotes), we probably would.",null,3,cdnuuqn,1rih74,askscience,top_week,1
SMURGwastaken,"The towel is definitely a good idea. Wash + towel is probably the best strategy, since I've done fingerprint tests in the micro-lab that have shown far more microbial growth on washed hands compared to unwashed ones (with certain soaps). You'd be surprised how readily the natural flora on some people's skin will attack anything new, so if you wipe out everything and then introduce something pathogenic it's likely to stick around.",null,0,cdnr8qa,1rih2r,askscience,top_week,2
cHEatsKYJelly,"Wet objects tent to hold things easier, for example wash your hands then stick them in sand.  We did an experiment in nursing school with the 5 second rule. What we did was got different objects and food, then drop it on the ground to see what holds more bacteria. The wettest objects had the most, jello, wet bread, stuff like that. The cleaner of the objects were the m&amp;ms, pills (tablets and caps). Hope that helps.",null,0,cdnswik,1rih2r,askscience,top_week,2
5amlawnshit,"A towel definitely helps - it serves as a physical barrier between your hand and the door knob. Therefore, the germs on the door knob do not come into contact with your hand.

Now to address whether a dry hand or wet hand transfer more germs from the door knob to your hand, I do not have a definite answer that wouldn't be based off speculation. 

tl;dr: Always wash your hands",null,1,cdnqksj,1rih2r,askscience,top_week,2
FlavaFlavivirus,"Yes! I work with Alphaviruses; these particles contain a fusion peptide which allows the contents of the capsid to enter the cytoplasm of the cell, by fusing the two membranes together and inducing a conformational change in the structural proteins. 
",null,0,cdnpmde,1rigsa,askscience,top_week,2
captsuprawesome,"I would not characterize it as ""catalyzing their own import"" but many proteins are capable of penetrating the cell membrane.  HIV-1 Tat is a well studied protein that can do such a thing.  You may be interested in this summary of [cell-penetrating peptides](http://en.wikipedia.org/wiki/Cell-penetrating_peptide).",null,0,cdnsl21,1rigsa,askscience,top_week,1
bearsnchairs,[Transferrin](http://en.wikipedia.org/wiki/Transferrin) is a neat protein for that. It carries iron into cells. When transferrin binds to its acceptor on the surface of cells it initiates endocytosis and is taken up by the cell. You can attach transferrin to a particle or protein of interest to incorporate it into cells. ,null,0,cdnvctl,1rigsa,askscience,top_week,1
LuklearFusion,"It really depends on the physical implementation of the qubits, as there are many kinds. An incomplete list of things that people use as qubits are:

1. The electronic structure of ions or atoms, which will be confined to some region of space by some sort of electromagnetic ""trap"".

2. A single electron's spin, where the electron has been trapped in a solid state system; a so called ""quantum dot"".

3. Superconducting circuits which have Josephson junctions can also be used as qubits.

4. Optical qubits use polarization or optical modes in light as qubits.

Each kind of qubit is stored (I'm assuming by stored you mean kept free from noise) and manipulated differently. Is there a particular kind you're interested in?",null,0,cdnnt7i,1rifph,askscience,top_week,6
DanielSank,"Whenever people ask about this I recommend reading [this post](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp). It refers only to superconducting circuit qubits, but it's definitely worth a read.",null,0,cdnvaq9,1rifph,askscience,top_week,1
Jeffy_Weffy,"Slow chemical reactions are happening. At one end, the reaction wants to take in electrons. On the other end, the reaction wants to give up electrons. The only way for the electrons to flow to complete these reactions is to go through the device you're powering.",null,0,cdnkh2z,1rifop,askscience,top_week,11
chillichill,"Lithium-ion batteries are common so I'll use those as an example. A battery has 4 major components, a cathode, anode, electrolyte and wires to connect the electrodes and complete the circuit (plus all the other bits that hold them together). This circuit is attached to a power supply during charge (to provide electrons) and a device when discharging (to be powered by movement of electrons).  The cathode and the anode are materials which can hold Lithium, while also allowing it to leave the structure reversibly. The electrolyte is a material which allows lithium to pass through, but not electrons.
When a lithium battery is being charged lithium ions (Li+) move from the cathode to the anode, through the electrolyte. When the Li+ reaches the anode it takes an electron from the power supply to form a (relatively) stable state. The Li is stored in the anode until a device that requires power is connected (discharging the battery). 
When a device that needs power is attached, the Li in the anode releases an electron to form Li+ which moves to the cathode. The released electrons cannot travel through the electrolyte therefore travel around the circuit, powering the device. At the cathode the Li+ recombines with an electron from the circuit. Once all the Li has moved from the anode to the cathode, the battery is completely discharged. ",null,0,cdnoov8,1rifop,askscience,top_week,1
stimulatedecho,"Hail forms in the presence of a strong updraft.  In this case, water precipitates, freezes and falls, but is blown back (continuously).  During this cycle more layers of ice form on the previously precipitated particles, until they get too heavy and eventually fall to the ground

If you get the chance, break open a hail piece (bigger the better) and you will find it is layered akin to an everlasting gobstopper.  ",null,0,cdnm2ax,1rid0v,askscience,top_week,2
ipostjesus,"basically, snow = ice from the beginning to the end of the process of water particles accumulating into larger structures. It doesnt always form hexagonal structures, there are many shapes of snow.

hail = a liquid or partially melted phase in the process, most likely involving a re-freezing event prior to reaching the ground. 

Ive never learnt much about snow formation, but i can tell you about hail. 
In a cumulonimbus cloud, water is cooled below freezing point but it hasnt frozen, called 'supercooled' water. supercooled water will freeze when it comes into contact with something that can start the crystal growth, such as a dust particle or some frozen water. So supercooled water is blowing around in the cloud, being pushed up by updraughts and falling back down when the updraughts cant hold its weight any more. It will cycle around the cloud falling and then riding updraughts back up again, all the while it will be accumulating water (some of it supercooled liquid water, some of it water vapour) until it doesnt find an updraught strong enough to hold it in the cloud and it falls out. Because the supercooled water is liquid and doesnt necessarily freeze instantly, the stone will be wet on the outside, which means stones will stick to each other by touching and then freezing. the sticking together of stones into larger stones makes the irregular surface of the ""random chunks"".
The larger a hail stone, the longer it spent accumulating while blowing around in the cloud. Which generally means larger clouds with stronger updraughts capable of suspending larger particles. ",null,0,cdnmhop,1rid0v,askscience,top_week,1
Freeoath,"The eraser works in a way that when you rub it, it removes the graphite from the papers surface. The rubber is more ""sticky"" then the paper and thus the graphite preferes the eraser over the paper. Another way some erasers works is the eraser damges the top  layer of the paper effectively removing the graphite that way. 
You can't use an eraser on ink (what a pen leaves behind) because the paper more or less absorbs the ink deeper making the erasers funcion useless. For these you can use ink remover that either changes the chemical compound of the ink removing it from the paper, or dying it white

",null,0,cdnk132,1ricv9,askscience,top_week,11
s3c7i0n,"It does, were you to look at the sun directly in space, you'd be blinded. That's the point of those gold visors on space suits. The reason it doesn't appear to is that there's very little for the light to strike. Think of shining a flashlight into the air, it has effectively no visible effect. Now if you shine it at a tree, it gets nice and bright. It's the same amount of light, but it doesn't appear so because we can't see it travel. Now if you mean why isn't space blue, like the sky, that's because when sunlight filters through the atmosphere, it's interaction with the various gasses scatter the blue light most, giving us that nice hue. Space, obviously for the most part lacking an atmosphere, doesn't scatter the light, hence the black. ",null,0,cdnj4u9,1riapc,askscience,top_week,28
alltat,"It *does* make space bright. The only reason space looks black is because it's empty: it's not black because it's dark, but because there's nothing there. If you look at pictures of spaceships and satellites in space, you'll notice that they're all brightly lit with strong shadows. That's because space is bright, as long as you're close to a star.",null,0,cdnjy5p,1riapc,askscience,top_week,7
VA_guy,"There are two parts.  First, the sun's brightness decreases with the square of the distance.  Meaning when you're twice as far, it is 1/4 as bright.  Four times as far?  4*4=16, 1/16 as bright.  That's because there is a finite amount of light being cast over an ever increasing spherical area.  So the sun would look quite a bit less bright from Mars or Saturn than it would here simply because we're closer.
  
But if you're asking why space isn't glowing, you need to think about what would cause that to occur.  If you have a spotlight on a clear night, it will illuminate a path in front of it but it won't make the entire surrounding area bright, right?  But if you were to shine that same spotlight into a white room, it would do a much better job of making the whole area look bright.  That would be because there are reflections in the second case which case the light to come at you from all angles, appearing to illuminate you from everywhere.  
  
So in space it would be similar to that spotlight.  If you look directly into the sun, it would be very bright (depending on your distance).  But otherwise, there is nothing else out there for the light to reflect off of, so it won't be as if the entire area is glowing or light is coming from you at all directions.
  
Hope that helps.",null,0,cdnj62j,1riapc,askscience,top_week,6
stuthulhu,"Things are bright because light bounces off those things, and strikes your retina. There's relatively little in space for light to bounce off of, and get redirected towards your retina instead of traveling away. ",null,0,cdnjqys,1riapc,askscience,top_week,4
Gitsumkikin,"Oh it does! Only specialized cameras can turn towards the sun... If a regular camera were to be facing the sun all you would see is white. Same thing if you were facin it...Kiss yer eye sight goodbye! I think its somethin like 350 degrees if you are in direct sunlight in space, -350 out of. Space is so unimaginably huge that if your back was to the sun,(it better be!) the light probably wouldn't be noticeable at all...nothing for it to reflect off...as I said,mostly educated guesses here. Aside from the temp and the specialized cameras, those are facts, temp may be off one way or another, but, not by much.",null,0,cdnj45h,1riapc,askscience,top_week,3
thetripp,"Your son is basically describing the theory known as ""Tired Light.""  The reason we don't think tired light is true is that we've never been able to come up with a mechanism that would cause energy loss in photons, yet still match our observed data.

For a tired light phenomenon to be true, it would have to:

1) Explain energy loss of photons over long distances, and match the observed redshift.

2) Not scatter photons so much as to induce blurring (since we don't observe significant blurring of distant objects).

3) Also explain the observed time dilation of distant events

4) Cause the same effect in every wavelength band, or in other words photons must ""tire"" in the same way, regardless of their frequency.

The wikipedia page on [Tired Light](http://en.wikipedia.org/wiki/Tired_light) has a nice list of some of the historical proposals related to this theory and why they don't match the observed evidence.",null,1,cdnl44c,1ri89x,askscience,top_week,13
stimulatedecho,"The distance related red shift is *evidence* of an expanding universe.  There happens to be a mountain of other evidence (search this sub for this question being asked previously to find specifics, mostly related to the cosmic microwave background, I believe), that suggests the same thing, i.e. expanding universe.  So, we don't really *know* that expansion causes the observed red shift, but it is certainly one valid explanation (as you already know), and it also explains other things we observe.  Additionally, and potentially more importantly, we have no experimental evidence to the contrary. 

That said, the requirement of ""dark energy"" energy seems to be a bit of a blemish...there is no doubt something in the recipe we have no understanding of.  I guess we'll find out exactly what as we go!",null,1,cdnlgh3,1ri89x,askscience,top_week,6
florinandrei,"&gt; ""but what if light just lost energy steadily as it went, wouldn't that look the same?""

No, it would not.

You are talking about an entire class of alternative explanations of redshift, grouped under the umbrella of the ""tired light hypothesis"". The have pretty much been debunked in bulk.

You cannot have light become ""tired"" by magic. There has to be some physical mechanism for photons to lose energy. If so, the energy loss will tweak the properties of those photons a little. As a result, a series of effects would become apparent:

- images from distant objects would become a little blurred, due to the scattering of the photons via energy-sapping interactions

- distant events would be observed to take place at the same rate of time; there would be no time dilation, like in the relativistic redshift models

Other effects would also become observable, depending on the particular ""flavor"" of tired-light theory, none of which have ever been observed.

Bottom line: the expansion of a relativistic universe is the only model that accounts for everything we observe out there.

http://en.wikipedia.org/wiki/Tired_light",null,1,cdnlxim,1ri89x,askscience,top_week,3
WhoH8in,"Well light does loose energy as it goes, in a sense anyway, every time it doubles its distance its energy is 1/4 what it was because intensity dissipates. This does not affect wavelength though. There are other phenomena that affect wavelength though, like movement. If somehting is moving toward us the light it emits doesn't seem to hit us any ""harder""(b/c light only goes c, no faster, no slower, ever) but that energy is accounted for, the light decreases its wavelength. If it is moving away then the wavelength increases which makes it appear redder.

Now when looking out into the stars hubble noticed that the further an object was the redder it appeared to be compared to what we know that objects [emission spectrum](http://en.wikipedia.org/wiki/Emission_spectrum) *should* look like. Now if the Universe were static then we would expect that, overall, half of all objects must be moving toward us and half be moving away and some tiny minority not moving relative to us at all. It is incredibly unlikely that, in a static universe, all objects would be moving away form us, we would ahve to be a truly uniqe body to observe that, literally the center of the universe. If the universe is expanding though it makes perfect sense because every object percieves every other object as moving away from it (ignoring of course nearby objects).

Use the expanding balloon analogy to understand it. If you have a barely inflated balloon and you draw three dots on it then start to blow it up those dots appear to be moving away form eachother but none fo them are actually moving, the space between them is expanding. This is why we think redshift is caused by expansion.",null,6,cdnjekl,1ri89x,askscience,top_week,2
bohr_exciton,"Yes, but only if you ionize the gas, manage to extract (at least in part) charge carriers of one type (say electrons), and then manage to somehow isolate the system such that charge neutrality cannot be re-established. In that case there will be a net charge within the gas, and the resulting repulsion would act as an effective increase in the pressure, which like you said could alternatively lead to a larger equilibrium volume if the container is flexible. ",null,0,cdnjcd7,1ri7it,askscience,top_week,1
TangentialThreat,"The charge will prefer to collect on the outside of the balloon, but the balloon material will repel itself and that may have the desired effect.

Is it cheating if I heat the contents of the balloon to 10,000 K? The rubber will melt, but for a brief moment you will have significantly increased the pressure using ionization.",null,0,cdnkify,1ri7it,askscience,top_week,1
__Pers,"Yes, but not in the way you think. In plasmas, as in ideal gases, 

P = n k_B T

If you ionize a gas to make a plasma, the density of independent particles n comprises electron density and ion density and the sum is higher than the neutral particle density prior to ionization. Also, in making a plasma from a gas, you generally make the temperature higher. Both will tend to increase the pressure. ",null,0,cdnsyil,1ri7it,askscience,top_week,1
OrbitalPete,"Tall mountains are generally a product of continental collision. That occurs when subduction processes close an ocean and collide the continents that  formed its margins.

When that ocean closes lots of the upper sediments get scraped off. They obviously get caught up in the collision zone, and involved in the thrust faulting and deformation that builds the resulting mountain range. Carbonates such as limestone are commonly among these uppermost sediments. Hence the fact you tend to see a lot of limestones at the top of mountain ranges.",null,0,cdnq1t9,1ri7ga,askscience,top_week,6
DangerOnion,"I don't think they're overrepresented.  The Alps and Rockies are mostly granite, and the Andes are chiefly igneous rock.  Like OrbitalPete says, mountain ranges are usually formed by the collision of tectonic plates, and whatever rock happens to constitute those plates is what gets shoved upwards.",null,0,cdnv4et,1ri7ga,askscience,top_week,3
Baloroth,"Putting a fan behind the space heater will produce forced convection, which will cool down the heater and heat up the room as a whole. This is the reason central furnaces have fans in the first place: it spreads the heat around (same for AC, but the reverse principle: you heat up the condenser and cool down the air).

What the net effect over a long period of time will be (i.e. if the room will end up warmer with or without the fan) depends on many factors, but generally I would venture that the room will be warmer overall with the fan than without. But short term, in a cold room, the fan will certainly speed up the process.",null,2,cdnilrg,1ri6wl,askscience,top_week,13
OlejzMaku,"It depends on the definition of ""heat up the room"". Do you want evenly districubuted heat or warmer area around the heater? How warm do you want the room to be? How big is the room? How well isolated it is? What temperature is outside?

If the room will be too big and/or badly isolated and/or it is very cold outside and/or your desired temperature is too high the fan might be contraproductive.",null,1,cdni8d1,1ri6wl,askscience,top_week,7
garycarroll,"You are correct that the result will be at least as much heat energy into the room, and more air movement (to a point) will result in more even heat. This may be better, or not. If the room is not sealed for instance, the door is open to the rest of the building) more heat may escape than if you had a warm side of the room away from the door. And as OlejzMaku implied, if the room is too large or cold, the space heater may be unable to make the whole thing comfortable but could heat one corner. 
Also, note that moving air may feel cooler than still air of the same temperature. 
It sounds like you are trying to heat the whole room. If so, the whole room will heat more evenly with better circulation, and this means the guy sitting next to the heater will not get warm as quickly. If I had brought the heater, I might prefer no fan.",null,0,cdnjfua,1ri6wl,askscience,top_week,3
Richard_Fitzsnuggly,"More information is needed as well as the previous responses.  Is the room a defined sealed space?  If not you will be attempting to heat fresh air instead of re-heated air as it circulates within the space.  The friction of the blades on the air does not impact the heat.  The speed in which the drag coefficient of the air on the blades versus the cooling affect of the ambient air, would need to be astronomically fast.
",null,0,cdnj58v,1ri6wl,askscience,top_week,2
expertunderachiever,"Ironically it could make the space around the heater hotter than desired as you cycle cool air over the heater basically nullifying the duty cycle [instead of shutting off for a bit it'll always be on].

From experience it will make the room hotter though.  I've used this trick in my basement on really cold days where you just need it to warm up.",null,1,cdniy0a,1ri6wl,askscience,top_week,2
DangerOnion,"Assuming an enclosed space with no open windows or anything, you'd be right.  The fan is producing a negligible amount of heat energy through friction, but it certainly can't reduce the temperature of the room.  He may be conflating the use of fans with the reason we use fans in the summer, which is that 1) moving air makes our skin feel cooler through evaporative cooling, but doesn't actually reduce the air temperature or 2) by circulating fresh outside air into a stuffy building, which doesn't apply here.  The temperature in one place might rise slower, but it's not reducing the amount of heat added to the room.",null,0,cdnuz1w,1ri6wl,askscience,top_week,1
WhoH8in,"Its completely aritrary. There is no objective way to identify ""up"". We choose the Earth's negative pole as north and assign that to the rest of the solar system and orient our images of other planets to that. If when cartographers started drawing maps they had placed the positive node on top then we would think of Antarctica as being Arctica. The only other way to get bearings in the solar system to orient yourself to the orbits of the planets. If you are looking toward the sun and the planets are going left to right you are oriented ""upward"" if they go right to left you are ""upside down"". But in reality none of this matters.",null,0,cdnj6g9,1ri48m,askscience,top_week,2
stuthulhu,"&gt;I'm visualizing the Solar System as planets orbiting the Sun in a flat disc. If we imagine that the disc is like a dinner plate, the standard view of Earth is that Antartica is orientated toward the bottom of the dinner plate. Is this actually correct?

Pick one. From a vantage point above the north pole of the Earth, the Earth would appear to revolve in a counterclockwise direction about the Sun. From a vantage point above the south pole, the Earth would appear to revolve in a clockwise direction. ",null,0,cdnjmdd,1ri48m,askscience,top_week,2
DangerOnion,"The plane itself is tilted pretty severely relative to the plane of the galaxy, making terms like ""up"" kind of meaningless in the first place.  But the simple answer is that you're right. The equators of most planets are roughly aligned with their orbital planes, and we invented astronomy so we get to decide which way is up :)  If we decide that Antarctica is ""down,"" then so is the hemisphere of Jupiter with the GRS in it.  Most pictures of the planets, despite being taken by satellites with no particular orientation, are rotated to be consistent with the way we visualize ""up"" in our solar system.",null,0,cdnv8pl,1ri48m,askscience,top_week,2
Astrokiwi,"The space between the stars in a galaxy is filled with a very thin gas - the ""interstellar medium"". Even between galaxies there is the ""intergalactic medium"" too. This means stars and galaxies aren't completely isolated in space - there's gas everywhere. This means you'll have matter and antimatter annihilating each other in the ""border regions"" between antimatter and matter galaxies. This would produce a constant stream of ~~\~2~~~1 GeV gamma rays in these border regions, which we'd be able to detect with our gamma ray telescopes. However, we don't see this.",null,2,cdnj4gy,1ri2no,askscience,top_week,9
rocketgolfer,"Antimatter functionally behaves the same as normal matter, it's just that there's much, much less of it and it annihilates as soon as it contacts normal matter. The parts of antimatter that are ""opposite"" are opposite only by convention (e.g. it doesn't matter whether we treat the electron as being positively charged or negatively charged, but it does matter that the proton has the opposite charge).",null,1,cdnkvjd,1ri2no,askscience,top_week,2
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,top_week,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,top_week,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,top_week,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,top_week,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,top_week,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,top_week,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,top_week,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,top_week,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,top_week,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,top_week,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,top_week,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,top_week,1
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,top_week,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,top_week,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,top_week,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,top_week,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,top_week,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,top_week,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,top_week,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,top_week,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,top_week,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,top_week,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,top_week,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,top_week,1
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,top_week,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,top_week,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,top_week,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,top_week,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,top_week,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,top_week,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,top_week,1
null,null,null,32,cdnftrg,1rhuln,askscience,top_week,29
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,top_week,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,top_week,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,top_week,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,top_week,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,top_week,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,top_week,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,top_week,1
null,null,null,32,cdnftrg,1rhuln,askscience,top_week,29
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,235,cdnhkj4,1rhu7r,askscience,top_week,1077
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,33,cdnfpuh,1rhu7r,askscience,top_week,199
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,5,cdng9z2,1rhu7r,askscience,top_week,26
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,5,cdnkfb7,1rhu7r,askscience,top_week,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,0,cdng7o4,1rhu7r,askscience,top_week,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,3,cdnqm5k,1rhu7r,askscience,top_week,11
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,0,cdnni7u,1rhu7r,askscience,top_week,9
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,top_week,6
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,0,cdnj69w,1rhu7r,askscience,top_week,6
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,0,cdnfgcp,1rhu7r,askscience,top_week,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,top_week,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,0,cdnpywc,1rhu7r,askscience,top_week,4
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,top_week,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,2,cdnfmxv,1rhu7r,askscience,top_week,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,0,cdnguqo,1rhu7r,askscience,top_week,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,1,cdnril0,1rhu7r,askscience,top_week,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,top_week,3
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,top_week,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,top_week,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,0,cdnwy1g,1rhu7r,askscience,top_week,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,top_week,5
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,top_week,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,top_week,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,0,cdnpfal,1rhu7r,askscience,top_week,3
null,null,null,0,cdns99f,1rhu7r,askscience,top_week,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,top_week,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,1,cdnt98m,1rhu7r,askscience,top_week,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,top_week,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,top_week,2
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,235,cdnhkj4,1rhu7r,askscience,top_week,1077
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,33,cdnfpuh,1rhu7r,askscience,top_week,199
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,5,cdng9z2,1rhu7r,askscience,top_week,26
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,5,cdnkfb7,1rhu7r,askscience,top_week,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,0,cdng7o4,1rhu7r,askscience,top_week,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,3,cdnqm5k,1rhu7r,askscience,top_week,11
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,0,cdnni7u,1rhu7r,askscience,top_week,9
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,top_week,6
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,0,cdnj69w,1rhu7r,askscience,top_week,6
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,0,cdnfgcp,1rhu7r,askscience,top_week,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,top_week,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,0,cdnpywc,1rhu7r,askscience,top_week,4
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,top_week,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,2,cdnfmxv,1rhu7r,askscience,top_week,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,0,cdnguqo,1rhu7r,askscience,top_week,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,1,cdnril0,1rhu7r,askscience,top_week,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,top_week,3
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,top_week,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,top_week,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,0,cdnwy1g,1rhu7r,askscience,top_week,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,top_week,5
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,top_week,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,top_week,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,0,cdnpfal,1rhu7r,askscience,top_week,3
null,null,null,0,cdns99f,1rhu7r,askscience,top_week,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,top_week,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,1,cdnt98m,1rhu7r,askscience,top_week,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,top_week,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,top_week,2
Trill-Nye,"What do you mean by ""the two angles?"" Electrons will be diffracted by a crystalline material at a number of angles, each corresponding to a certain crystallographic plane with a reflection allowed by the structure factor. So each diffracted beam is due to a different d-spacing. the different n values in the Bragg equation correspond to higher order reflections, but these can generally be ignored. Does this answer your question?",null,0,cdnfqvw,1rhu16,askscience,top_week,3
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,top_week,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,top_week,1
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,top_week,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,top_week,1
Halysites,"Two reasons:

1. When a star is going through [fusion](http://en.wikipedia.org/wiki/Star#Formation_and_evolution), it will combine hydrogen to form heavier and heavier atoms. The basic reaction series would be: 

* hydrogen + hydrogen = helium
* helium + helium = carbon
* carbon + carbon = iron
* there will be other combinations of fusing atoms which would result in the creation of neon, silicon, etc. This is just a very simple list (I'm just a simple geologist and chemistry is not my speciality).

Once a star has burned all it's fuel to generate iron, it will undergo a supernova (or some other process, depending on it's size). Since iron was the last atom to be produced during the fusion process the star will be very rich with iron. If it's goes through a supernova it ends up ejecting most of it's material into the space around it; during this process heavier elements may form as well. So the space around it becomes enriched in a variety of elements, especially iron. This material is what will be used to form planets.

2. Planetary material begins to accrete from the rich stuff spewed out by a dying star. It is very hot and so the material is molten. As the planetoids get larger and larger, gravity becomes a stronger force. Gravity, coupled with a molten states, means that heavier elements are pulled towards the core of the planetary body quickly. Iron, being very heavy, will sink towards the core. Other heavy (metallic) elements will also sink to the core.

Viola, you have an iron-rich core for rocky planets. This process doesn't exactly apply to gas giants like Jupiter (which would be relatively depleted in iron).

Most of this information is from geology textbooks and courses I took in my undergrad. Although I imagine most of the info could be looked up on the internet.",null,1,cdnh1c9,1rhqp5,askscience,top_week,3
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,10,cdndirh,1rhpj0,askscience,top_week,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,top_week,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,1,cdnf7mk,1rhpj0,askscience,top_week,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,top_week,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,top_week,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,top_week,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,top_week,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,top_week,1
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,10,cdndirh,1rhpj0,askscience,top_week,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,top_week,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,1,cdnf7mk,1rhpj0,askscience,top_week,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,top_week,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,top_week,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,top_week,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,top_week,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,top_week,1
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,1,cdng2f1,1rhpdk,askscience,top_week,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,top_week,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,top_week,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,top_week,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,top_week,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,top_week,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,top_week,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,top_week,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,top_week,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,top_week,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,top_week,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,top_week,2
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,1,cdng2f1,1rhpdk,askscience,top_week,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,top_week,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,top_week,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,top_week,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,top_week,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,top_week,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,top_week,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,top_week,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,top_week,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,top_week,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,top_week,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,top_week,2
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,top_week,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,top_week,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,top_week,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,top_week,1
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,top_week,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,top_week,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,top_week,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,top_week,1
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,18,cdndsjx,1rhpc1,askscience,top_week,77
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,top_week,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,top_week,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,top_week,14
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,18,cdndsjx,1rhpc1,askscience,top_week,77
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,top_week,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,top_week,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,top_week,14
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,top_week,32
null,null,null,1,cdnfj49,1rhi55,askscience,top_week,10
null,null,null,0,cdngnvj,1rhi55,askscience,top_week,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,top_week,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,top_week,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,top_week,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,top_week,1
null,null,null,1,cdng7fo,1rhi55,askscience,top_week,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,top_week,2
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,top_week,32
null,null,null,1,cdnfj49,1rhi55,askscience,top_week,10
null,null,null,0,cdngnvj,1rhi55,askscience,top_week,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,top_week,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,top_week,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,top_week,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,top_week,1
null,null,null,1,cdng7fo,1rhi55,askscience,top_week,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,top_week,2
xavier_505,"This is likely an issue with the way the MPEG streams are configured. Generally MPEG-2 encoding (I am not especially familiar with MPEG 4 but I would hazard a guess that it is similar, or at a very minimum has great flexibility in its configuration) uses three types of frames: I, B and P.

- I (intra) frames are full frames of data encoded similarly to JPEG (~7:1 compression). These do not reference any other frames.

- P (predicted) frames only have information describing the difference between the preceding I or P frame (lower size than I frames, ~20:1 compression).

Both I and P frames are called ""reference frames"" since the information they describe can be referenced by other frames.

- B (bidirectional) frames are encoded from interpolation of preceding/following reference frames (even smaller than P frames, ~50:1 compression).

These various frame types are typically sent in the following way:

    I B B P B B I B B P B B I P B B....

Where the distance between I/P frames (in this case it is 3) is configurable as is the distance between I frames (in this case there are 6).

Why am I telling you this!? Well, a P frame cannot be decoded without its referenced I frame, and B frames cannot be decoded without all of their referenced I/P frame. So, the longer the distances mentioned above, the greater affect a lossy channel will have. You would see the behavior you are describing in the following case:

    MPEG2 GOP structure (m=2, n=4): I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B ....

    MPEG4 GOP structure (m=4, n=16): I B B B P B B B P B B B P B B B I B B B P B B B P B B B P B B B ....

A lost I frame on the lower stream would blow away 16 frames of data, while on the upper frame only 4 frames. Similar for lost P frames.

Edit: Cleaned up description of IBP frames.",null,0,cdnfw8p,1rhf15,askscience,top_week,3
chucklesMtheThird,"A total stab in the dark here....it could be that MPEG4 channels are sent on higher QAM constellation carriers, which are more susceptible to minute changes in signal quality, and the MPEG2 are sent over lower constellation carriers?

For example, QAM64 receivers have a much higher tolerance range in amplitude and phase angle error than do QAM256 or QAM512 because of the lower symbol density. The higher you go in constellation density, the less room there is for error.",null,1,cdney2o,1rhf15,askscience,top_week,2
redallerd,"Yes. Since there is no limit as to how small fractions can be, there can be an infinite amount between two whole numbers. If you're finding it hard to understand, try adding halving fractions to see if you can get to a whole number for example : 1/2 + 1/4 + 1/8 + 1/16 and etc.",null,0,cdndbyb,1rhegr,askscience,top_week,10
Captain-Negative,"Yes. For example, consider all finite strings of digits beginning with ""1."" and ending with a varying number of twos.

1.2, 1.22, 1.222, 1.2222, 1.22222, etc.

If you count these numbers one by one, you'll notice that it goes on infinitely long. This is a type of ""countable infinity"" (alternatively, [aleph](http://en.wikipedia.org/wiki/Aleph_number)-zero or [beth](http://en.wikipedia.org/wiki/Beth_number)-zero), because it's an infinity arising from (surprise surprise) a list you can count through.

However, there are even bigger kinds of infinity -- each of which is ""uncountable"" -- one of which describes the number of numbers between 1 and 2. As it turns out, there's no way to come up with a method to list all numbers between 1 and 2 one-by-one simply because there are way-too-fucking-many of them. Perhaps paradoxically, though, you can show that describing all the numbers between 1 and 2 is just as hard as describing those between and 1000000, so the two infinities are actually said to be the same (specifically, it's called the ""beth-one"" infinity).

Very roughly speaking, it is unclear if there is an infinity between beth-zero (countable infinity) and beth-one (the type of incountable infinity we just discussed). Whether or not this ""beth-half"" exists depends on how you decide to model the world, and is the topic of something called ""[the continuum hypothesis](http://en.wikipedia.org/wiki/Continuum_hypothesis)"" in mathematics.",null,0,cdnee15,1rhegr,askscience,top_week,2
medstudent22,"There are several known benefits to neonatal circumcision. 

- **It prevents penile cancer.** Squamous cell carcinoma of the penis is exceedingly rare in circumcised patients. Circumcision alone may not be the preventative measure. [Phimosis](http://en.wikipedia.org/wiki/Phimosis) (the inability to retract the foreskin) can only occur in non-circumcised individuals and is associated with a higher risk of penile cancer. Phimosis, in many cases, is preventable with adequate hygiene. It should also be noted that penile cancer is extremely rare 1-2 out of 200,000 men per year. Also worthwhile to note that somewhere between 909 and 322,000 circumcisions would need to be performed in order to prevent one case of penile cancer. 

- **It reduces the risk of UTIs in early life and up to 5 years of age.** Uncircumcised males are 20x more likely to develop a UTI during the neonatal period. It should be noted that 111 circumcisions must be performed to prevent one UTI though. Some cost analyses have shown that there is still a cost benefit to performing circumcisions when just considering UTIs though.  
 

There are some claimed benefits of circumcision with varying amounts of evidence. 
 
- **It may reduce the spread of HIV** (to men, in heterosexual relationships). This is based on several large African clinical trials. It was not found to reduce the risk of transmission to women and has not been shown to reduce the risk of transmission in homosexual male couples. 

- **It may reduce the transmission of HPV and herpes (HSV).** In a study of 3393 men (1684 who underwent circumcision), after two years, 7.8% of the circumcised men had HSV-2 antibodies, 10.3% of the uncircumcised group did. In the same study, 18% of the circumcised men had evidence of HPV, 27.9% of uncircumcised men did. ([Study](http://www.nejm.org/doi/full/10.1056/NEJMoa0802556)) It should be noted that this study was performed in Uganda. Also worthwhile to note that most individuals clear HPV spontaneously and also that a [vaccine is available](http://en.wikipedia.org/wiki/Gardasil) for the most common HPV strains. Also worthwhile to note that HPV is associated with penile cancer, but more importantly cervical cancer in women.  

The reason I tried to note the conclusions which were drawn based on African studies is that the underlying prevalence of disease has an effect on the study and these results may not be considered generalizable to other populations.  

Multiple groups have issued statements on neonatal circumcision which may contain more information that may be useful to you. 

[The American Academy of Pediatrics](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCwQFjAA&amp;url=http%3A%2F%2Fpediatrics.aappublications.org%2Fcontent%2Fearly%2F2012%2F08%2F22%2Fpeds.2012-1989&amp;ei=FrGUUpKsOZHlsATz34HABA&amp;usg=AFQjCNGNikptx2aRUdOftngXQ3JyIFWU5g&amp;sig2=MiBQ3NWjASCvuKD_PjKXHw) states: 
&gt;Evaluation of current evidence indicates that the health benefits of newborn male circumcision outweigh the risks and that the procedures benefits justify access to this procedure for families who choose it. 

[The American Urological Association](http://www.auanet.org/about/policy-statements/circumcision.cfm) states: 
&gt; neonatal circumcision has potential medical benefits and advantages as well as disadvantages and risks.",null,4,cdne4ln,1rhdmd,askscience,top_week,17
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,top_week,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,top_week,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,top_week,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer  common)
8.58.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer  rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.58.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,top_week,8
null,null,null,3,cdn9iux,1rhdi6,askscience,top_week,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,top_week,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,top_week,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,top_week,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,top_week,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,top_week,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,top_week,2
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,top_week,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,top_week,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,top_week,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer  common)
8.58.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer  rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.58.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,top_week,8
null,null,null,3,cdn9iux,1rhdi6,askscience,top_week,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,top_week,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,top_week,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,top_week,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,top_week,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,top_week,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,top_week,2
Karnivoris,There is not much change at the bottom of the trench by inspection if you look at the size of the globe in comparison to the depth of the trench.,null,1,cdnrtdc,1rhdel,askscience,top_week,3
vashoom,"You can use Newton's Universal Law of Gravitation to calculate a decent approximation.  If the average radius from the center of the Earth to the surface is 6,371 km and the trench is 10.9 km deep, simply plug in 6,371 - 10.9 = 6360.1 km.

Crunching the numbers (gravitational constant times mass of earth divided by that radius (6360100 m) squared, gives me 9.8473 m/s^2.  So just a tiny bit above the average gravitational acceleration on the surface.",null,3,cdndqjv,1rhdel,askscience,top_week,3
null,null,null,5,cdnia0n,1rhdel,askscience,top_week,5
eebootwo,"As said by 1992^^?, not much different. However, depending on the object underwater, it might accelerate upwards due to buoyancy, or downwards faster than GM/r^2 if it is denser than water: which would be if it were a gas compressed to greater density than 0.998 kgm^-3",null,5,cdnnrfp,1rhdel,askscience,top_week,1
patchgrabber,"This is an unanswerable question. Different organisms mutate at different rates, we don't know exactly when life began (who knows how many different types of microorganisms were around near the beginning), the way a species is distinguished from another is inherently arbitrary, and we have no idea how many species have ever existed.",null,0,cdncaz4,1rhbru,askscience,top_week,6
biorad17,I've seen  estimates of this.  IIRC you only need one specieation event every million years or so to account for every species.  It's important to note that estimations like this are not necessarily biologically accurate.  They are mathematical models that provide parameters to begin thinking about evolution.,null,0,cdob0of,1rhbru,askscience,top_week,2
Hiddencamper,"In a nuclear reactor we use the fission process to release energy by splitting the atom. 

For the case of uranium235 the fission process looks roughly as follows

U-235 + n -&gt; Fragment1 + Fragment2 + ~2.4 n + energy

Those fragments are also known as fission products and are somewhat random in size. There is a statistical probability of what you can get. See the image at the top of this Wikipedia page. The fission fragments are where you get all the well known products like xenon, iodine, strontium, cesium, etc

http://en.m.wikipedia.org/wiki/Fission_products_(by_element)

Basically in a nuclear reactor there are fission products that are a result of splitting the atom, and there are also transactinides. What also happens, is the U-238 and U-235 can absorb neutrons but not undergo fission, causing them to become other heavy elements through a series of decay chains. ",null,1,cdnccjy,1rhamu,askscience,top_week,5
Proxymace,"In the uranium mined from the earth the ""active"" isotope makes up appx 0.7% this is enriched to appx 8-10% depending on the type of plant that will use it so there is a substantial amount of material that will be irradiated and will then decay into different elements to the ""active"" one",null,3,cdnbga6,1rhamu,askscience,top_week,1
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,20,cdnb7zo,1rh9np,askscience,top_week,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,top_week,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,top_week,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a low heat setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A low heat setting is more [energy] efficient than higher heat settings.    
&gt;7. A less dry setting is more [energy] efficient than normal or more dry. 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,top_week,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,top_week,2
null,null,null,2,cdnev7c,1rh9np,askscience,top_week,2
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,20,cdnb7zo,1rh9np,askscience,top_week,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,top_week,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,top_week,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a low heat setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A low heat setting is more [energy] efficient than higher heat settings.    
&gt;7. A less dry setting is more [energy] efficient than normal or more dry. 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,top_week,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,top_week,2
null,null,null,2,cdnev7c,1rh9np,askscience,top_week,2
Platypuskeeper,"Two reasons. 1) Most chemical reaction rates increase exponentially with temperature. Water leaching into some stuck food, or something dissolving are chemical reactions. 2) The solubility of most (solid) stuff tends to increase with temperature.
",null,1,cdndlk4,1rh5ch,askscience,top_week,7
SimpleBen,"The viscosity of fats is dramatically altered by temperature. Think about it. Bacon fat in the package is nearly solid, but at around 200 degrees F it is pretty liquid. Fat changes so much with temperature that it is by far the dominant reason that warm water cleans better than cold (not to mention the fats in the soaps!) ",null,0,cdngody,1rh5ch,askscience,top_week,4
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,top_week,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesnt grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,top_week,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,top_week,1
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,top_week,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesnt grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,top_week,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,top_week,1
Platypuskeeper,"The color depends on the coordination environment of the Cu(II) ions that are formed. In a concentrated nitric acid solution, the copper ions coordinate to nitrate ions, giving a green/greenish-blue color. If the solution is more dilute (or diluted after oxidizing the copper), then you get a blue solution where the Cu(II) ions are coordinating to water instead.

And on a safety aside: Who the hell are these fools who play around with concentrated HNO3 outside of fume hood? That brown gas is toxic nitrogen dioxide!
",null,0,cdn9hbo,1rh4eg,askscience,top_week,4
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,top_week,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,top_week,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,top_week,3
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,top_week,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,top_week,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,top_week,3
EdwardDeathBlack,"Assuming you use the European convention of having a comma instead of a decimal point, you would get indeed 350,000 people. 

I find the idea of a 6.5 GWh plant weirdly low. A nuclear reactor can easily be a 1GW thermal, assuming 35% conversion efficiency, that's 350 MW electrical. Assuming 90% uptime , that'll be 365 * 24 * 350 * 0.9=~2800GWh. Most nuclear power plants have four or five reactors, so can easily generate 10,000GWh per nuclear plant per year. So a power plant with a total capacity of 6.5GWh per year certainly seems puny by modern energy use. Then again tidal is really not much of an energy source, more of a public relation toy , so maybe it is that puny.",null,1,cdn799s,1rh0eq,askscience,top_week,3
E_F_F_E_C_T,"So using this site for KWH/capita for china gave me 3,300 KWH/capita -http://data.worldbank.org/indicator/EG.USE.ELEC.KH.PC

Then using this site for the station's output - http://en.wikipedia.org/wiki/Jiangxia_Tidal_Power_Station

The instantaneous power of the station is 3,200KW (we'll ignore the solar stuff).

Multiply this by the amount of hours in a year gives you 28 GWH.

Dividing this by the 3300KWH/capita gives us roughly 8500 people.

Considering the second Wikipedia article states that ""The power station feeds the energy demand of small villages at a 20 km (12 mi) distance, through a 35-kV transmission line."" I feel that this isn't that unreasonable.

Hope this helps.",null,0,cdn7d47,1rh0eq,askscience,top_week,2
super-zap,"Compared to most other large power plants your favorite tidal power plant is tiny. 

http://en.wikipedia.org/wiki/List_of_largest_power_stations_in_the_world

It has 1000 times less generating capacity than most of the large ones and almost 6000 times less capacity than the largest power plant.

So, overall it is not surprising that it can generate power for only 350 000 people. I believe your math is correct.",null,0,cdn7dqd,1rh0eq,askscience,top_week,2
RelativisticMechanic,"&gt;Lets say you crush a planet down to mosquito size to form a blackhole.

Alright, we have a black hole of 1 Earth Mass.

&gt;Apparently it would evaporate really fast from your outside frame of reference.

Not really. The lifetime of a Schwarzschild black hole with the mass of the Earth would be about 500 trillion trillion trillion trillion years (as measured by those of us far from the event horizon for the duration).

&gt;But how could any effect pass over the event horizon to reduce the mass of the blackhole?

Nothing necessarily crosses the event horizon; rather, the curvature of spacetime near (but outside) the event horizon produces (nearly) thermal radiation that can be intercepted by those of us far from the black hole. In this process, the spacetime curvature relaxes, manifesting in a decrease in the surface area of the event horizon: the black hole shrinks.

One can, with suitable constructions, model this behavior as a tunneling process whereby particles from inside the event horizon tunnel out; this is analogous to other tunneling behavior wherein particles traverse a classically impenetrable barrier due to quantum mechanical effects.

&gt; I know that things can pass over the EH from their own reference frame - but not from an outside frame.

In fact they *can* cross into the black hole, even in a far-removed frame. The idea that they can't comes from an idealization where you neglect the mass of the infalling object (which we can reasonably assume is very, very small compared to the black hole mass). Even in that approximation, though, if we account for the quantization of light, there will be a final photon emitted from the infalling object. Once that photon is emitted, it will never again be seen by anything outside of the event horizon.",null,0,cdnag5o,1rh0ay,askscience,top_week,3
Daegs,"The simple version: Because the particle entering the event horizon has negative mass.

When the pair of virtual particles are ""created"", if one sticks around with positive mass, then the other must have a negative mass in order to cancel out (and they must cancel out, no free energy)

So the positive mass one shoots off away from the black hole, and the negative mass one enters the black hole which reduces its overall mass. ",null,0,cdn9pcd,1rh0ay,askscience,top_week,1
Nicked777,"The Hawking radiation is a deeply quantum mechanical effect, but here is an intuitive way to think about it. The uncertainty principle requires the creation of particle antiparticle pairs, everywhere, all the time. These particles locally violate conservation of energy, which is allowed in QM, as long as it happens on short enough time scales. This means the two particles annihilate very quickly, as if they were never there. 

The point of hawking radiation is if this happens very close to a black hole's event horizon, one of the particles can get sucked in, and the other will escape, albeit very reduced in energy from its trip. Because of this the Hawking radiation is believed to be very weak. A specialist in this topic could explain why it seems to be only the anti particles that fall in, and why we think this admittedly bizarre idea could me true, but I don't know off the top of my head. ",null,1,cdn9rz6,1rh0ay,askscience,top_week,1
claireauriga,"There are definitely equations that can describe what is going on! Heat and mass transfer are an important part of physics and engineering. 

In order to melt, the ice must be raised to its melting point temperature, then given enough energy to melt into liquid. This energy needs to come from somewhere. Heat moves from hotter to colder places, so the warm air will give energy to the ice (and water) until they are the same temperature. 

There are some complications in calculating all this, however. For example, if the air is stagnant then it will get colder as it gives up energy, which means transfer to the sculpture will slow down. If the air is moving, we also have to think about how fast it's going and if it's removing some of the water as vapour too. 

There are many more and less detailed ways of describing what's going on, but in the very simplest terms, the bigger the temperature difference between the air and the ice, the faster energy will transfer. The lower the ice temperature is below its melting point, the more energy needs to be added to make it warm up and melt. ",null,0,cdngq51,1rgzx3,askscience,top_week,3
StringOfLights,"Yes, it's possible to have multiple ova fertilized by sperm from different men. Sperm can live for several days, and multiple ova can be released over the course of several days in a single ovulation cycle. That means it's possible for more than one ovum to be fertilized and implant, resulting in a pregnancy of multiples with different paternities (I've only ever heard of this happening with twins, but triplets, etc., aren't impossible).

As DNA testing has become more common case reports have come out verifying the different paternities of twins. [Here](http://www.nejm.org/doi/full/10.1056/NEJM197809142991108) is an example from the 1970s, and [here](http://www.fertstert.org/article/S0015-0282%2897%2981456-2/abstract) is one from the 1990s. 

The phenomenon of having two ova fertilized in two seperate coital events is often referred to as ""superfecundation"". It technically refers to any instance in which more than one egg is fertilized in more than one act. Instances where the paternity differs is referred to as ""heteropaternal superfecundation"". [One study estimated](http://www.ncbi.nlm.nih.gov/pubmed/7871943) that 1 in 12 sets of dizygotic twins born to married white women in the US were the result of superfecundation, while 1 in 400 were the result of heteropaternal superfecundation.

Edited for clarity.",null,5,cdn6dbz,1rgzjd,askscience,top_week,30
sever0us,"A meniscus is caused by the ratio of the strength of the cohesive forces of a fluids molecules to each other and the cohesive forces of the fluids molecules to the container wall.

If a fluid has a higher cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a concave meniscus.
If a fluid has a lower cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a convex meniscus.

Since gels behave is a solid-liquid hybrid way, the presence or absence of a meniscus would most likely depend on the physical properties of the gel. It really depends on weather the cohesive forces described above are enough to deform the gels structure.

TL;DR: It depends on the gel. 'Fluid' gels such as shower gel stand a much greater chance of presenting a meniscus than 'solid' gels like ballistics gel.",null,1,cdn8dfo,1rgzf8,askscience,top_week,5
Dominus_,"When you're wiring your home surround system, no, pretty much not at all. But over long distances like on a concert where some cables run several tenths of meters, sometimes even a hundred meters, the resistance and interference has to be reduced, or else you're going to end up with artifacts and noise. ",null,7,cdnacxc,1rgzbv,askscience,top_week,46
thegreatgazoo,"For just about anything in your house, lamp cord is an excellent choice of speaker wire. Just make sure one side is marked so you keep the polarity correct. 

Anjou Pear speaker wires (and anything similar) are for delusional people who have too much money. 

",null,1,cdnckf0,1rgzbv,askscience,top_week,7
littlegreenalien,"yes.. and no. It's not so much the cable that's the problem, rather the interference it can pick up on the way. The longer the cable the more issues come into play (cable resistance, etc as mentioned already). But at short cable distances it's mostly interference from power cables, and what not.",null,0,cdnb62y,1rgzbv,askscience,top_week,5
Kriemore,"Computer engineer here: wires are important, but if your question is 'should I spend $90 to get these cables I found at best buy for my home theatre?' Then probably not.

A bad cable will degrade audio quality significantly in addition to causing all manner of other problems with cutting out etc. 

Of course, expensive audio cables were famously compared to a coat hanger with no noticeable difference.


Now, if you're playing a massive theatre... these things start to matter a lot more.",null,0,cdngnf0,1rgzbv,askscience,top_week,4
jgrun,Ultimately there is always a degradation of signal quality when transmitting over a long distance. But since a digital signal is just binary 0s and 1s and you're only sending it 4 or 6 feet to the TV or stereo it doesn't matter. The receiving end will read the signal very clearly because it's hard to mistake a 1 for a 0.,null,7,cdn7wlm,1rgzbv,askscience,top_week,9
thisispointlessshit,"Not really. I just like getting cables that don't feel cheap... If that makes sense. The wire itself tends to wear over time if it has cheap shielding when I'm constantly coiling and uncoiling. Something with decent shielding usually lasts longer for me. For home use it might not be as much of an issue, because it's plugged in and never really moved.

In terms of sound quality? No difference.",null,1,cdng5vb,1rgzbv,askscience,top_week,3
lucaxx85,"You need to distingush three applications: 
1)analog signals in home setups
2)digital signals in home setups
3) live concert signals and similars...

For 1) everything works. Including coat hangers (for the power signals. You need shielding for line ones). The resistence and the impedence of such cables are such that they cannot affect in any way the final signal, which has a very low bandwidth. Only thing to be careful is to have cables large enough for the amp-speaker connection, if you have a very high power system (but I'd guess that you can still forget about this in any practical situation). 

2) Digital signals are more complicated. The bandwidth here is much higher, especially if you're also carrying video. That's why you have maximum lenghts and building them needs lots of care. Still almost any commercial cable is good if you're not trying to do something you shouldn't (e.g.: a 10 meters HDMI connection). In these case of course a 15'000 $ cable made from the finest rhodesian zinc, soldered in a full moon night by an african zoroastrian priest would work as badly as the cheapest one in the store.

3) For concerts and other applications cables can give actual problems. Still not those ""lamented"" by audiophiles. The first thing you look for in a concert cable is the *mechanical* resistance, especially of the connectors. Most of the cables break for a mechanical injury! Those things get torn everywhere. 
Then there is a problem with microphones/guitars signals. They're *extremely* weak. So they're sensitive to interferences. But, like before, those cables that claim to feature platinum in their alloy or even to have a special cristalline structure that favours the signal in a specific direction (how on earth would that work??!?!??!!) won't make *any* difference.   There are other tricks to solve the problem (balanced signals, preamplification before long transmissions etc...)

So you actually need a lot of care and you have a number of problems... But they're so not what the audiophiles claim!",null,0,cdni471,1rgzbv,askscience,top_week,3
Cyanmonkey,"I find the build of the cable more important than impedance rating, etc.

A properly built cable with Neutrik connectors and strain relief lasts much longer than your cheap Guitar Center POS, but as far as signal goes, as long as your not going over 200' it doesn't make a noticable difference.",null,1,cdnfrd9,1rgzbv,askscience,top_week,1
EvilHom3r,"For digital (i.e. HDMI), no it does not matter at all. Digital either works or doesn't, there is no in between.

For analog (RCA, speaker wire, TRS wires), you will always get better quality (even if just slightly) with a better wire. However for the average user they will probably never notice the difference, and more likely than not the quality bottleneck is elsewhere in the system.",null,5,cdncqhj,1rgzbv,askscience,top_week,3
Dyson201,"Not exactly an Audio Engineer, but this isn't a difficult question from a signals standpoint.

Transferring signals through a medium (cable) can pose a variety of challenges that are handled in many different ways.  Without going into extreme detail lets just say that electromagnetic forces could possibly come into play, as well as capacitance to ground producing noise in the circuit, etc. etc.

Long story short, if you're replacing a 3' cable for sound, I highly doubt you'll notice a huge dip in quality between $100 cables and coat hangers.  Both are capable of transferring the signal, and while the expensive cable will transfer the signal with a much greater Signal to Noise ration (SNR), at 3' and with modern noise abatement technology, you would be hard pressed to hear a difference.

Now that being said, I wouldn't wire up your home surround system with soldered together coat hangers, as distance plays a huge factor in the quality of the transmitted signal.  Also, if you buy a cheap ass sound system, expect to hear a big difference in quality between expensive and poor cables, even at 3'.  

Finally, audio quality sound is a very low frequency, and does not travel well over distances with a good SNR.  Quality cables are the only way to increase sound quality over distances (relative term, we're talking meters here not miles).  Technology has come a long way towards discerning the signal from the noise, but any reduction in noise is a huge positive in the quality of the signal.",null,11,cdn6sul,1rgzbv,askscience,top_week,6
generalelectrix,"This is a very vague question.

For digital signals, yes this does matter.  Digital audio signals require significantly higher bandwidth and run at higher frequencies than the audio content they encode, so transmission line effects become important.  If the characteristic impedance of the cable you use to transmit a digital signal is not matched to the source and destination, you can get partial reflections or standing waves on the cable, which can definitely cause errors in the reconstructed signal at the destination.  This becomes more important with longer cables.

For analog signals, the frequency is low enough that the characteristic impedance isn't really important.  So long as the conductors you're using are low-resistance (copper is great), coat hangers should work just as well as fancy cable.  Shielding in cables is important for line-level interconnects to prevent the cables from picking up noise from the environment, though this usually isn't too big of a problem in a home environment.

The only real exception to this is for speaker cables (carrying post-amplifier level signals) for electrostatic speakers, as the load they present to the driving amplifier is largely capacative.  Then the details of the impedance of the cable driving the speaker become a bit more important.

I'm a physics PhD in quantum electronics with a minor hi-fi addiction.

Edit: I give an in-depth and accurate answer and get a ton of downvotes?  SCIENCE!",null,19,cdn89ly,1rgzbv,askscience,top_week,5
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,2,cdn5cpe,1rgx9w,askscience,top_week,9
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,2,cdn5cpe,1rgx9w,askscience,top_week,9
Physics_Cat,"In order:

Technically, yes. But the technical definition of temperature isn't what you think it is. More on that in a moment. 

Absolute zero is exactly the same as zero Kelvin. 

Who told you that the temperature of a black hole is absolute zero? That's certainly not correct. In fact, it's not possible for any matter to be at a temperature of exactly zero kelvin, due to the zero-point motion inherent in quantum mechanics. We can get incredibly close in a laboratory (somewhere in the range of hundreds of picoKelvin) but it's not possible to attain exactly zero kelvin. 

As for negative temperature: the colloquial understanding of temperature is something like ""temperature is the average kinetic energy of the constituent particles in a material."" That's a very useful tool for intuitively understanding things like heat capacity, but it's not the ""real"" definition. In thermodynamics, temperature is defined as the partial derivative of internal energy with respect to entropy (not sure how to format that symbolically, so I won't try). There are some kinda-convoluted, not-entirely-realistic examples of physical scenarios with negative temperature. That is, you add a bit of energy to the system, and the entropy goes down. For example, suppose you have N light switches, and each ""quanta"" of energy is represented as turning on one light switch. The entropy of a system is related to the number of configurations (microstates) that lead to the same macroscopic result, so let's say that you have N-1 switches turned on. Then the entropy is, more or less, N (since there are N ways to have N-1 switches turned on in a collection on N switches). Now you add one ""quanta"" of energy and turn on the last light switch. How many microstates are there now? Only one. There's exactly one way to have N out of N light switches turned on. Since we added a unit of energy and saw the entropy decrease, the system could be said to have ""negative temperature"" if you like. There are physical systems that come close to this analogy, but I think the ""light bulb scenario"" is easier to digest.",null,2,cdn51ib,1rgv22,askscience,top_week,9
fishify,"Absolute zero and 0 K are the same temperature.

Negative absolute temperatures are actually *hotter* than any possible positive temperature.  When you look at the mathematics, at any positive temperature, more energetic states are less likely to be populated than less energetic states (though at higher temperatures, the difference between those likelihoods is not as large as at lower temperatures); what you find is that if you had negative absolute temperature is that it would correspond to a situation in which more energetic states were *more* likely to be populated than less energetic ones.  (Lasers are a place where you might see such population inversions.)

Black holes have positive temperature, inversely proportional to their mass.",null,0,cdn54pg,1rgv22,askscience,top_week,4
auralucario2,"First, the statement about black holes is completely false.

Now, according to the law of thermodynamics, it is impossible to reach absolute zero, which is the same as zero kelvin. However, quantum mechanics butts its head in here and offers a workaround (kind of). It would be theoretically possible to achieve a temperature of some negative kelvin by having particles achieve a quantum state in which their entropy actually *decreases* as energy is added to the system. Needless to say, this doesn't exactly happen all the time, but it is possible.",null,0,cdnvd3b,1rgv22,askscience,top_week,1
brickses,"Zero kelvin and absolute zero are the same thing. There is no such thing as negative temperatures except in advanced thermodynamics exams.

Black holes are actually hotter than zero kelvin, like all warm things, they radiate (the same way humans radiate in infrared).",null,3,cdn4wu1,1rgv22,askscience,top_week,2
mingy,"I think you are right, but it is a minor error that probably got by the editors. I once read the final draft of a textbook written by a renowned expert in optics (long story) and found several errors (mostly units and arithmetic) and I knew maybe 1% of what the author had forgotten. He was grateful nonetheless.

In any event, even the 10 billion bits are wrong. The base pairs are grouped into 3s so you have 64 permutations, however this is not a binary or quaternary system, there are redundant codons and start and stop (http://en.wikipedia.org/wiki/DNA_codon_table) so there are 22 symbols of the 64 permutations.",null,0,cdn4k62,1rgu88,askscience,top_week,4
selfification,"Yeah that was a mistake in a way.  Each nucleotide base pair carries 2 bits of info.  So 5 billion base pairs carries 10 billion bits of info.

But there is the flip side that you have 2 separate sequences.  Each base pair is 2 codons.  Now you can consider that just 1 letter (because one of them precisely specifies the other) but I guess one could consider them 2 separate letters.  I mean...  2 copies of a file have twice the number of bits, even if the *information content* hasn't increased.  So in that interpretation, each base pair contains 4 bits of info...   and that would make Sagan's calculation make sense.",null,0,cdn4nl4,1rgu88,askscience,top_week,2
iorgfeflkd,"Each base-4 base can represent 00, 01, 10, or 11. So there is 4 times as much information as just binary.",null,4,cdn3qce,1rgu88,askscience,top_week,1
iorgfeflkd,"Yes, it's both. Just being still in a gravitational field (like we are now, on the Earth) causes time dilation relative to freefall, and orbiting satellites have to take both into account (this is the famous GPS relativity correction).",null,0,cdn3rku,1rgt18,askscience,top_week,4
Platypuskeeper,"There cannot be such a thing as a 'non-cohesive liquid'. A liquid is by definition a state where the attraction between the molecules is strong enough that the thermal energy is insufficient to let most of them leave the liquid. But unlike a solid, the molecules are still able to move about. 

If you have no intermolecular forces, you have a gas. 
",null,0,cdn6cb2,1rgslj,askscience,top_week,3
LoyalSol,"The curve itself, not really.  The function, definitely. There are so many uses it is hard to list them all. ",null,0,cdnph1q,1rgo2l,askscience,top_week,1
iorgfeflkd,"It's not changing its constant, it's just changing your units. If you use meter-kilogram-seconds unit then hbar is something like 10^-34 m^2 kg /s but if you use Planck units then hbar is 1, G is 1, and c is 1, and you can measure lengths in terms of (hbar G/c^3 )^(1/2), for example. This makes it easier to do theoretical work because you don't have to keep track of all these constants, but you'll have to do more work to get your results in measurable quantities.",null,0,cdn34jk,1rgmvd,askscience,top_week,14
MonadicTraversal,"&gt; Any faster or slower, closer or farther, or difference in direction of travel and the body would de-orbit, spiralling toward the planet or star it's orbiting or flinging off into outer space.

This isn't true. If you smacked a huge meteor into the Earth, you wouldn't knock it into the sun, you'd just change the shape of its orbit a bit. Spiral orbits don't actually exist under inverse-square forces such as gravity; you can show that the only possible orbits are circles, ellipses, parabolas, and hyperbolas. Spiral orbits don't exist except if there's some kind of drag force or whatever dissipating energy from the system; on an interplanetary scale drag doesn't matter. (Note that this is somewhat complicated by the fact that, e.g., Jupiter affects the orbit of the Earth, but in general the perturbations due to planet-planet interactions are small enough to not matter for stability purposes).

&gt; Isn't it difficult for us to keep our own man-made satellites in a stable orbit, requiring periodic adjustments? And yet, the moon is huge and it seems to be in an orbit that will last billions of years with no intervention.

Many man-made satellites are orbiting at an altitude where Earth's atmosphere can still exert some small amount of drag. The moon is so far away from Earth that the drag is essentially negligible. We also want the satellites to be kept in a *predictable* orbit; for a geosynchronous satellite, we want that orbit to be such that it's always above the same spot on the equator. The moon doesn't 'have' to be in any particular orbit, it just orbits wherever it orbits.",null,0,cdn0pyg,1rgi2m,askscience,top_week,9
iorgfeflkd,"For a circular orbit it has to be that precise, but many more initial configurations will lead to stable elliptical orbits, which are stable due to a balance of gravity and angular momentum. We live in a universe where the force of gravity decays with the square of distance, which is related to the fact that we live in three spatial dimensions. It turns out, there are only two types of forces that can produce stable orbits: inverse square, and linear (harmonic, like a spring). So, basically, we live in a universe where stable orbits can exist. Because of that, the fact that we do see stable orbits is not surprising.",null,0,cdn0bqk,1rgi2m,askscience,top_week,5
dirtpirate,"You seem to be misunderstanding the interaction. Comic book guy is asking for a very high number X, and mister Burns is retorting to Smithers ""Give hime Y"", where Y is much smaller than X. Thus a typical haggling scenario. 

The joke isn't that the two numbers are the same, just that instead of comic book guy saying ""I want a billion"", and Burns replying ""I'll give you a million"", they are instead using physical constants. ",null,0,cdnbei2,1rgi0o,askscience,top_week,7
iorgfeflkd,"The Faraday constant is the charge of a mole of electrons or protons, measured in Coulombs. Avogadro's number is 6x10^23 and a Coulomb is 6.2x10^19 fundamental charges, and the ratio is 96485 Coulombs per mole.",null,3,cdn0eae,1rgi0o,askscience,top_week,7
ecopoesis,"Metrics such as temperature describe the behavior of a system that is made up of components.  These types of properties are termed emergent properties because they are derived from the behavior of the system as a whole and are not observable if you were to look only at the components.

So, for your specific question, individual molecules do not have temperature.  They are not ""hot"" or ""cool"" exactly, although they do have energy that is zipping them around their surroundings.  Molecules with more energy will move faster and collide with other matter more frequently and with more force.  It is only when you begin to look at a system of molecules that ideas such as temperature start to be meaningful.  In that sense, a group of molecules with a certain amount of energy will correspond to a certain temperature.  If these molecules are ""hotter"" than other molecules, then they will be moving about much more rapidly and they will be less dense than the latter group of molecules.  Properties such as temperature and density are emergent from the system of molecules interacting with each other and interacting with their surroundings.",null,1,cdn5v46,1rgf8v,askscience,top_week,7
The_Evil_Within,"&gt;an area of hot air becomes less dense, and so it rises above colder areas of air. 

First, you need to look at it the other way around - hot air doesn't rise, cold air sinks.  As it sinks, it forces the hotter air upwards.

Now, think of a mess (and I do mean 'mess' for the imagery, not 'mass') of cold air, with the molecules fairly still and fairly dense.  Then, something heats up a bit of it near the bottom - what's going to happen?

The molecules of hot air will bounce around a lot more than the cold, and sometimes they're going to bounce up.  When they do, the less active cold air is more likely to fall into the gap than to move in another direction, and now there's nowhere for that hot air molecule to go because it will only bounce off the cold air molecule if it bounces downward again. (Transferring some heat in the process, but we can ignore that for the purposes of this explanation)

Multiply this by unimaginable numbers of interactions, and you end up with a column of hot air rising while all the cold air around it rushes in to fill the gap at the bottom.",null,4,cdn6q0h,1rgf8v,askscience,top_week,8
AltoidNerd,"&gt;&gt;But what if there was a single constituent molecule from that wood existing in the water? Would it float? It is neither densely nor sparsely aggregated, existing all by itself.

My reaction to this is no not really - a single molecule would have dynamical behavior that isn't familiar like the bouyant force example you gave.  I have no idea how to describe what that situation *would* be like - but I'm positive it would be invalid to treat it like a whole plank of wood floating.",null,1,cdn9dzb,1rgf8v,askscience,top_week,5
ramk13,"Wanted to add that at the scale of single molecules, static interactions are much more important than buoyant forces. A single molecule of wood will dissolve and behave like another molecule in solution. Even a few molecules of wood together will still be influenced by the hydrogen bonding between water and its external oxygen groups more than the buoyant force on the particle as a whole. All of this applies to your wooden plank example.

To answer your question: And if so, why do they act like an aggregated 'body' with those molecules around them, just because they are at the same temperature?

It's because even in air at atmospheric pressure molecules have a limited mean free path. In air it's [68 nanometers](http://en.wikipedia.org/wiki/Mean_free_path#Mean_free_path_in_kinetic_theory). That is that an oxygen or nitrogen molecule only travels so far before it hits another molecule and they bounce off each other. The molecules collide often enough that they influence each other over that short length. That influence leads to aggregate properties, as each collision redistributes kinetic energy.",null,0,cdnt7nx,1rgf8v,askscience,top_week,1
5secondstozerotime,"I do not think the rocket is directly launching into Geostationary orbit. Rather, it is going into a geostationary transfer orbit (GTO) that will then allow it to go into a geostationary orbit.

I cannot find a reason why this needs a window, however what you are saying about the rocket is wrong. 

[This article talks exstensivly about it](http://www.americaspace.com/?p=45686).",null,0,cdn8uw4,1rgf3r,askscience,top_week,4
Nicked777,"A little known consequence of orbital mechanics is that you must be directly under an orbital path to launch into it. SpaceX do not launch from the equator, so they cannot go straight into GEO, they have to start with a transfer orbit, and then do a plane change somewhere. They can only launch into their transfer orbit when this orbital path is directly overhead, which means waiting for the earth to rotate Cape Canaveral into the right spot, thus the launch window troubles. ",null,0,cdn973l,1rgf3r,askscience,top_week,3
neverdonebefore,"There is a bit more to it than that.  

In FWD cars, the front wheels are doing both the steering and applying the engine torque to the road.  And RWD, the rear wheels are only applying that torque to the road.  Essentially, your fwd cars are 'pulling' while rwd are 'pushing'.  

As you drive down a straight road, you are applying longitudinal force to the road to propel you forward.  As you enter a curve in the road, you add a rotational component to your travel.  The center of mass of the vehicle has to move laterally through the curve, while the vehicle itself has to rotate about that center of mass in order to be pointed straight as you exit the curve.  With a fwd vehicle, the direction of the force applied to the road by the tires changes as you turn your steering wheel, and the back wheels will follow in that path. Fwd vehicles have a tendency to understeer.  An object in motion wants to stay in motion: the inertia of the car in the longitudinal direction makes it want to keep going straight.  The tires want to follow the path on which they are pointed.  If the lateral acceleration into the curve cannot overcome the forward inertia, the car will understeer, or take a path with a larger radius than the curve.  In a rwd vehicle, the the tendency is to oversteer, or turn at a smaller radius than the curve.  This is because the force on the road by the rear wheels is along the path of the vehicles inertia.  The front wheels will want to follow their path around the curve, but the rear wheels will want to keep going straight.  This means it is easier to rotate about the center of mass.  This is how fish tailing and drifting (and spin outs) occur.
",null,1,cdn5t9m,1rgew2,askscience,top_week,8
wwarnout,"In a rear-drive car, when you accelerate, the center of gravity shifts toward the rear.  So, if the only consideration was getting good traction during acceleration, this would be the preferable configuration.

However, since most cars have engines in the front, a front-drive car will have better traction is slippery conditions because more of the weight is over the front wheels.",null,1,cdn0w9f,1rgew2,askscience,top_week,4
AltoidNerd,Take a shopping cart at the grocery store and compare pushing and pulling the cart.  This especially is useful if the back wheels of the cart don't rotate (in analogy to the car).,null,1,cdn9faf,1rgew2,askscience,top_week,2
Das_Mime,"The idea is that another star's gravity will tug the comet farther out at first, and then when the star passes (or just when the comet continues on its newly more-elliptical orbit), the comet falls back inward.

It should be noted that the Oort Cloud is very poorly understood, not really directly detected, and is basically used as an explanation for long-period comets. Most comets are on highly elliptical orbits, so even if several of them are perturbed by the same star, their orbits will be altered in different ways. Even if multiple comets are sent into the inner solar system in this way, they might arrive years or centuries apart.",null,0,cdn86xc,1rgeiw,askscience,top_week,3
Dyolf_Knip,"East takes you out, out takes you west, **west takes you in**, in takes you east.

The bold one is relevant here.  The star does attract the comet, but does so in a way to slow its velocity relative to the sun.  After the star passes by, the comet assumes an orbit suitable to its new velocity, which means it drops into the inner solar system.",null,0,cdne36w,1rgeiw,askscience,top_week,1
iorgfeflkd,"Nothing particularly interesting would happen. Light by itself isn't affected by temperature, and if the light is passing through a vacuum then temperature isn't a meaningful quantity. Depending on the medium that the passes through, its temperature can have effects on how the light absorbs it. For example, in an extremely cold dilute gas it is possible for the atoms to absorb light and stay in that configuration for a brief period of time, so the light is in effect trapped. This is the temperature's effect on the medium, however, not the temperature's effect on the light.",null,0,cdmyy7i,1rgcdh,askscience,top_week,4
stuthulhu,"Another thing to consider, even if the photons *could* be frozen you would not see your display freeze as though stuck in time. You would simply not see your display, since the photons responsible for creating that image are no longer able to *move* to your eye. 

An easy way to simulate what a room would look like if all the photons became frozen in space is to put a box over your head. ",null,0,cdndsia,1rgcdh,askscience,top_week,2
Das_Mime,"Light won't stop moving, even if it's going through a medium which is at absolute zero.

Temperature is about the thermal motion of particles which have mass, like electrons. The colder you get, the less kinetic energy they have. But light has no mass and its energy is proportional to frequency, so it usually doesn't make a great deal of sense to talk about light having a temperature in the same way that a physical material does (although a spectrum of light can certainly have a characteristic black body temperature, lower energy light doesn't travel any slower than high energy light).

Light, on the other hand, is comprised of massless photons. If they're passing through a medium (like water or air), then they will go somewhat slower than the speed of light in a vacuum. This change in speed can be affected (slightly) by the temperature of the medium, which is why you see effects like heat shimmers. Light can't stop moving, although [certain materials can slow it down to extremely slow speeds](http://www.news.harvard.edu/gazette/1999/02.18/light.html).",null,0,cdmz3om,1rgcdh,askscience,top_week,2
musubk,"Contrary to the other answers, the length of the day decreases at a near constant rate for most of the year. It isn't a sine wave, people! It looks more like a triangle wave with the points lopped off and rounded off. It superficially looks like a sine wave if you view it for lower latitudes because the amplitude is too small to see the shape, but try it for somewhere further north. Fairbanks, AK is a good choice. I just wrote a quick IDL routine to read the daylight hours tables the USNO website gives for a chosen latitude, [here it is for Fairbanks (65 North)](http://i.imgur.com/YMvVGf5.png).

And if you go even further north, like 85 degrees, [you get something silly like this](http://i.imgur.com/bHWfVqK.png).

The point being that the days don't start shortening at a slower rate as you would think for coming over the edge of a sine wave, the rate that days are shortening is actually constant over the majority of the year for a majority of the planet. This is still true at lower latitudes, and if you scale the graphs right you can see that:

[50 degrees latitude](http://i.imgur.com/na9TE3D.png)

[35 degrees latitude](http://i.imgur.com/8BLfwwu.png)

[20 degrees latitude](http://i.imgur.com/PrqgIpq.png)

[5 degrees latitude](http://i.imgur.com/OLRDXDP.png)",null,0,cdnbz0h,1rgcbc,askscience,top_week,6
iamtheonewhotokes,"As we approach Dec. 21 the days will shorten at a slower and slower rate. Similarly as you approach the summer solstice in June days will get longer at a slower rate the closer you get. And as you approach an equinox (in March or Sept.), the rate increases. 

See chart here: http://cycletourist.com/Miscellany/Length_of_day.html (the slope of the curve is the rate)",null,3,cdn12gy,1rgcbc,askscience,top_week,6
iorgfeflkd,"They're not actually instantaneous, they're just treating them that way because it's much simpler to do so in an intro to physics class. Real objects are made of compressible materials, and when they collide the objects deform.",null,0,cdmy3fx,1rgap2,askscience,top_week,4
cylon37,"Let's be clear here. Two events that are simultaneous in one frame of reference may not necessarily be simultaneous in another frame ONLY if the two events are separated by some distance. Conversely, if two simultaneous events happen at the same point in space, they are simultaneous in all frames of reference. A collision as described above is a single point in space-time. The two 'events' that you describe, A transferring momentum to B and B transferring momentum to A happen at the same location and are therefore simultaneous in all frames of reference.",null,0,cdn0i76,1rgap2,askscience,top_week,4
iorgfeflkd,If the mother and father were half-siblings.,null,3,cdmycqc,1rganp,askscience,top_week,8
ohheytherewhatsup,"No. Crossover events during Meiosis 1 are required to generate tension in the meiotic apparatus.  Without crossover, division will not occur, and crossovers cause mixing of chromosomes from the grandparents.  Each of your chromosomes is a chimera of your two grandparents DNA.",null,1,cdn9ihk,1rganp,askscience,top_week,5
laika84,"Although this would not add up to 50%, the child of a mother with Down's syndrome, (men are essentially infertile and women with DS can have a child but they are less fertile than those without DS,) there would be a 50% chance that the child receives the extra chromosome.

Since this chromosome resulted from a non-disjunction event in one of the grandparents, the child would have more than 25% of his/her genetic material from one grandparent.  Again, not 50%, but still interesting.",null,1,cdn6qe0,1rganp,askscience,top_week,3
null,null,null,0,cdn18w5,1rganp,askscience,top_week,1
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,top_week,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,top_week,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,top_week,2
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,top_week,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,top_week,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,top_week,2
CosmicWaffle5,"It's called positional alcohol nystagmus. Basically, there are these things in your ears called semicircular canals that are responsible for your sense of balance. The semicircular canals are supported inside of a fluid that is usually the same density as the semicircular canals, but when you drink alcohol it changes the density of the fluid surrounding the membranes and throws your balance system out of walk. 

http://en.m.wikipedia.org/wiki/Positional_alcohol_nystagmus",null,0,cdn7xz4,1rg8rs,askscience,top_week,7
Merrilin,"Anything with a temperature (a.k.a. all matter) is constantly emitting **blackbody radiation**. 

You can think of temperature of an object as being proportional to how much each constituent atom vibrates. The more intense it's vibration, the hotter it is. The short of it is that this vibration causes the release of a photon, which carries with it some energy from the atom, decreasing it's temperature. More on that if I ever get home. 

It so happens that the hotter something is, the higher frequency radiation, on average, it emits. That's why a piece of metal visibly glows when you make it very hot. At room temperature it is emitting light at a range of frequencies, but almost none in the visible light range. As you make the piece of metal hotter, it's blackbody radiation in the visible light range becomes significant enough that a human eye can detect it. 

So, no, matter cannot have temperature without also emitting some frequency of light. And there is no such thing as matter without temperature, so matter is always emitting light. ",null,0,cdmy126,1rg6wj,askscience,top_week,9
thumbs55,"Excellent question.

First of all what is heat and what is temperature? Are they not the same thing?

[Heat](http://en.wikipedia.org/wiki/Heat) is a measure of thermal energy (measurable in joules like all energies), it can be a measure of the ammount of (highly disordere heat typed energy) energy moving from one body to another.

[Temperature](http://en.wikipedia.org/wiki/Temperature) is a measure of the hottness or coldness of a body, two bodies with the same temp will not exchange any net heat and if one body is hotter than the other then the hotter will give energy to the colder in the form of heat.

&gt;everything I can think of that has heat also has light. Stars, lightbulbs, lava, fire, hot metal,

This type of light is called [black body radiation](http://en.wikipedia.org/wiki/Black-body_radiation).

&gt;Metal only emits light after it heats up past a certain temperature.

While it is true that the light becomes visible after a certain temperature is reached, the metal is actually always emitting invisible ""light"" (electromagnetic radiation) at any finite temperature due to said black body radiation.

All of space has [Cosmic microwave background radiation](http://en.wikipedia.org/wiki/Cosmic_microwave_background) which gives ""empty"" space a temperature. (It is not empty of [real] particles if you include the photons giving it said temperature).

The temperature of space (away from stars and such) is around 3 kelvin, so if you have something hotter than that it in space will get colder and if you have something colder than that it will actually warm up.

The heat energy is often stored in the energy of the jiggling of molecules. But for it to move from one place to another it mainly moves in the form of photons (light) but also phonons (sound). If you engineered a particularly exotic system that only exchanged energy in phonons then this system would emit heat with no light.

Nutrenos are also an example of a form of heat (they carry energy from the sun in a manner that is not work) nutrenos are not light. But many forms of nutreno generation would also produce photons.",null,1,cdmyhyr,1rg6wj,askscience,top_week,4
AltoidNerd,How about your hands.,null,1,cdn9ibx,1rg6wj,askscience,top_week,3
Chuk,"Metal only emits light after it heats up past a certain temperature. It can get very hot but still not be glowing. (That is, assuming you are only thinking of visible light.) Living creatures also emit heat without light, as do many other chemical reactions.",null,7,cdmxi6u,1rg6wj,askscience,top_week,3
uberhobo,There is no such thing as relative humidity above the boiling point of water.  It will all stay a gas in any proportion with air.,null,0,cdnb8t8,1rg6ug,askscience,top_week,3
whatsup4,it depends if there is something for the water to condense on. Basically think of it like cloud formation. Air high in the atmosphere can sometimes be super saturated and achieve higher than 100% rh because it is hard for the water to form droplets without a surface to form on. Given a large enough decrease in temperature you can see cloud formation.,null,1,cdnaiz3,1rg6ug,askscience,top_week,1
BoxAMu,"The energy of a photon is proportional to frequency, but this energy must match the energy of some transition in the absorbing matter.  The electrons in bonds in glass have transitions in the UV, but not the visible range.

This is the same reason why high energy X-rays are used for imaging: muscle and tissue are mostly transparent to X-rays, while the calcium in bones absorbs them.",null,0,cdmy00a,1rg4zy,askscience,top_week,4
therationalpi,"Basically a whistle is a resonator. You either have a Helmholtz resonator (like a beer bottle) or a standing wave resonator (like an organ pipe).

Driving the resonator is the variable airflow through the whistle. In most whistles you will have a hole with a blade shaped edge on it. When the edge is blown on, it creates turbulent airflow in the form of vortexes. These vortexes alternate from side to side in what is called a ""vortex street."" There's a good picture of that [here.](http://www.grc.nasa.gov/WWW/Acoustics/code/adpac/sample/CYLINDER_VORTEX_SHEDDING/) The alternating vortexes create a varying positive and negative acoustic pressure, setting up a wave in the resonator. The resonator, as a result, forces the frequency of the vortexes to align with the whistle's natural frequency. In this way the whistle amplifies the normally irregular vortex variations into a sound loud enough to be heard at a distance.

The reason you must blow at the correct angle is that the flow vortexes will depend greatly on how the moving air stream hits the blade. You must hit the wedge shaped part of the whistle fast enough to create unstable flow, otherwise the wave will not be generated.

Hope that helps!",null,0,cdmz9jk,1rg4zj,askscience,top_week,1
AltoidNerd,"It's puffy.  If highly energetic, roughly spherical.  

You can get fireworks to discharge in predetermined shapes by the way you pack the explosives.   By analogy, the shape of a space flame would depend likewise on the shape of the source,  

Spherical enough of course to feel good about 4/3  r^2 in a physics calculation.",null,0,cdn9gny,1rg4lz,askscience,top_week,2
Nicked777,"The fire will indeed be spherical, this has actually been tried in Space before, it looks pretty cool (I'm on my phone so I won't link it.)

The flame changes colour because the lack of convection causes diffusion to be the dominant transport mechanism. Compared to a terrestrial flame this means the flame burns with more complete combustion, with less soot. (Glowing hot soot is the reason most terrestrial flames are yellow.)

Edit: More information here: http://carambola.usc.edu/research/microgravity.html
",null,0,cdn9k0r,1rg4lz,askscience,top_week,1
do_od,"Buoyancy is a force acting on a body as to oppose gravity when that body is immersed in a fluid. This force is proportional to the weight of the volume of fluid displaced. In zero gravity, the fluid has no weight and there is no direction in which buoyancy could act. Buoyancy requires gravity... or more generally a reference frame under acceleration. Example: If you spin a bucket of water on a string in outer space, a ball could be buoyant in the water. That would not be useful for propulsion though. ",null,0,cdmv0go,1rfxwf,askscience,top_week,10
blacksheep998,"Here's a good video answering your question. http://www.youtube.com/watch?v=bgC-ocnTTto

In it an astronaut places an alka-seltzer tablet into a spherical water drop. Without gravity the only major force affecting the bubbles is surface tension, which causes most of the bubbles to combine with each other and eventually form one large bubble in the middle of the water sphere.

There's also this video, http://www.youtube.com/watch?v=QPf5MJluhvo in which an astronaut injects an air bubble into a water sphere, and then injects small water droplets into the bubble.",null,0,cdn17rs,1rfxwf,askscience,top_week,4
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,4,cdmv2r3,1rfwje,askscience,top_week,45
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,3,cdn6q1y,1rfwje,askscience,top_week,14
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,4,cdmv2r3,1rfwje,askscience,top_week,45
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,3,cdn6q1y,1rfwje,askscience,top_week,14
Das_Mime,"&gt;I recall reading something along the lines of observing the orbit of any natural satellite of the object, but a more detailed explanation would be nice. 

If you can see a natural satellite of the object and you can reasonably assume the satellite to have much much lower mass than the planet*, then you can use mechanics to work out the host's mass. In this case I'll also assume a circular orbit, but you can also work out the mass from non-circular orbits.

The force of the planet's gravity on the moon is **F = G m*_p_* m*_m_* / r^(2)** where G is the gravitational constant, r is the orbital radius, and the m's are the masses of planet and moon. In the case of  circular motion, the force on the moon is equal to **F = m*_m_* v^(2) / r** where v is the orbital velocity of the moon. Set these forces equal to each other, and you get:

**G m*_p_* m*_m_* / r^(2) = m*_m_* v^(2) / r**

Canceling out common factors, you get

**m*_p_*  = v^(2) r / G** 

So if you know the distance of the planet and it has a moon (which for Solar System objects can be readily obtained via parallax methods), then you can directly calculate the planet's mass. 

Calculating the mass of a body without natural satellites is a bit more work. Prior to the Space Age, Venus and Mercury's masses were not well constrained, because the best way to measure mass is to measure its gravitational effect on other objects. Venus also exerts a gravitational influence on other planets such as the Earth, and so if you have sufficiently accurate position measurements of both bodies and if you know Earth's mass then you can calculate Venus's mass, but this is still not an ideal method.

Our best measurements of Venus' mass come primarily from spacecraft like the Mariners 2, 5, &amp; 10 (American) and Venera (Soviet) probes sent to Venus. [From analyzing their trajectories](http://adsabs.harvard.edu/full/1968AJS....73R.162A)--some of them were flybys, some were orbiters (e.g. Soviet Venera 15 &amp; 16, American Magellan and ESA *Venus Express*), and some have landed on the surface--you can determine Venus' mass to a high level of accuracy, but in the end this is essentially the same method as the first-- measuring the planet's tug on nearby objects. 

Finally, you can make guesses at the composition of Venus, and since its radius is easily measured with a telescope, you can get an estimate of its mass. This is much less accurate, of course, since it depend entirely on the accuracy of your guess about the composition. 

*true for all Solar System planet/moon pairs except the dwarf planet Pluto and its moon Charon, which is about 12% of Pluto's mass
",null,0,cdmyrqu,1rfuon,askscience,top_week,3
Ejb90,"Even for the simplified case of a planet-star system there are a few ways to find the mass of a planet. I'll describe a common one, [Astrometry](http://en.wikipedia.org/wiki/Astrometry).

From observations we can usually deduce the distance of the star from earth, the ""apparent magnitude"" (how bright it is to us) and the spectral class - what types of elements it's made up of by looking at the light received.
We can also find the period of orbit of the planet around the star by several methods - the transit method is most popular, though the Doppler shift method and others are used dependent on the circumstances).
The next part is the difficult part. The velocity of the star wobbling backwards and forwards must be measured. 
The planet doesn't actually orbit a stationary star - they both orbit their combined centre of gravity, though for the star, which is much more massive, this is relatively close to its centre of gravity. Hence the star itself wobbles backwards and forwards. This speed can be measured from earth via the doppler effect - the light when the star is shifting towards us is shifted slightly up in frequency and then when it is moving away gets shifted down slightly. This can be used to calculate the speed.

From the distance and apparent magnitude we can calculate the ""absolute magnitude"" - how bright it it from a standard distance (30ly IIRC). Using these and some hefty thermodynamics/ fluid mechanics/ stellar structure knowledge (or the simplified [mass-luminosity relation](http://en.wikipedia.org/wiki/Mass%E2%80%93luminosity_relation) or extrapolating roughly from the [Hertzsprung-Russell diagram](http://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram)) to find the mass of the star.
Also from the period of or it we can use Kepler's third law to find the radius of orbit.
Now, the speed of the body can be calculated as the distance it travels and the time it takes is known, and the speed and the mass of the star are known.
Finally these can be used with the conservation of momentum to calculate the mass of the planet.

This technique has several issues. Firstly, this only gives a lower limit, as the orbit may not be head on, so the star may be moving faster than expected. Also, some of the measurements needed aren't possible in some cases. Also, it must be noted that Kepler's laws and the mass determination of the star isn't exact. Finally the issue of having more than one body in the system. Because there are a large amount of bodies in the system, the equations aren't analytically solvable, so there is some error in the determination process.

The mass of moons we can observe is calculated much in the same way, using the planet as the main mass.
I'm not sure what you read about observing moons of planets. This certainly isn't possible with exoplanets - we're only just on the verge of being able to see the very biggest exoplanets as off this year.
",null,0,cdmxscf,1rfuon,askscience,top_week,2
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,top_week,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,top_week,5
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,top_week,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,top_week,5
siliconlife,"Actually what you suggest does happen, but it's not called subduction because continental crust is too buoyant to descend into the mantle like cold ocean crust.

The Himalayan orogeny actually is so intense that a process called [underplating](http://www.sciencemag.org/content/325/5946/1371/F2.large.jpg) actually takes place. Underplating is the positioning of crust or magma beneath an overriding crust. In the case of the Himalayas, the Indian continental crust is being thrust so strongly that it ends up completely beneath the Eurasian crust. [Link to paper](http://www.sciencemag.org/content/325/5946/1371.abstract)",null,0,cdmvwch,1rfs95,askscience,top_week,6
oloshan,"The Indian plate is indeed being subducted under the Eurasian plate. The Himalayas are the uplift of Eurasian crust, not Indian crust - although their elevation is certainly enhanced by the effects of having the Indian plate shoved beneath the Eurasian at the same time. But not only was the Indian plate subducted, the speed of the collision may have actually driven it deeper than typically subducted plates (probably meaning that it has had less time to melt since being subducted, and so can still be discerned beneath the Eurasian plate).

In addition, a fair amount of lighter continental sediments were essentially ""scraped off"" onto Eurasia by the collision, during the initial phases when the Tethys Sea closed. A similar process happened along the Pacific plate margins as well, and has contributed to the formation of Alaska, Japan, and other ""accreted"" terranes.",null,0,cdo7rsl,1rfs95,askscience,top_week,2
fastparticles,"The Himalayas are being lifted at least in part by this collision, however we do not have a specific mechanism worked out for it. The difficulty with this collision is that this is a continent on continent collision, and both are very buoyant. When you think of a traditional subduction zone you have oceanic crust hitting continental crust, and the oceanic crust is denser and so it sinks. In this case both are continental crust so there is little/no density contrast and India can't just sink.",null,2,cdmvdf1,1rfs95,askscience,top_week,3
hikaruzero,"It's pretty simple -- photons alone aren't the cause of attraction/repulsion.  It is the *fields themselves* that cause charged objects to attract or repel eachother.  Photons are created when charges accelerate, but if you have a bunch of stationary charges and no actual photons, those charges will still begin to accelerate and attract or repel eachother without emitting or absorbing any photons amongst themselves.

In the context of perturbative theories, this effect can be explained by saying that the vacuum is filled with virtual photons, and that the virtual photons end up exerting a force on the stationary charges.  But virtual photons are not detectable the way real photons are, and also virtual particles do not appear in non-perturbative treatments of electrodynamics, so it is something of a matter of debate whether they even exist at all (indeed in the theory of the strong force, perturbative calculations frequently end up being *wrong*).  Virtual particles can be thought of as simply a mathematical tool for calculating approximate answers -- it's best to just say it is the *fields* that cause charges to accelerate.

Now, real photons themselves are *disturbances* of the fields, and if the fields change, that will cause a change to the acceleration of a charged object, so real (detectable) photons *do* accelerate charged objects, but strictly speaking it is the field that is ""doing the work,"" the presence of photons isn't necessary for attraction and repulsion -- it's not like there have to be a bunch of photons flying around from one particle to the next in order for charged objects to accelerate (though if they are flying around and being absorbed or emitted, they will change how those charged objects are moving via transfers of momentum).

So it's the fields that do the acceleration, whether you want to interpret fields as being made up of virtual particles is something of a matter of philosophy, and not something that experiment can tell us is definitely true or false.

Hope that helps.  Some further (but more technical) reading:  [Wikipedia:  Static forces and virtual-particle exchange](http://en.wikipedia.org/wiki/Static_forces_and_virtual-particle_exchange) and [Wikipedia: Force carrier (particle and field viewpoints)](http://en.wikipedia.org/wiki/Force_carriers#Particle_and_field_viewpoints)",null,0,cdmsq9j,1rfqqd,askscience,top_week,5
aziridine86,"Wikipedia is an OK place to start, but I believe that the most basic answer to the 'why' question is this:

The hydrocarbons that we get from the earth come in a huge variety from gases like methane, ethane, and propane, all the way to thick waxes and tars. 

Because of the prevalence of internal combustion engines used in cars, trucks, planes, etc., we have a much higher demand for gasoline, diesel, and jet fuel than for other hydrocarbons which are heavier or lighter. 

Cracking is one way we can turn less desirable hydrocarbons like high-boiling petroleum into more desirable products such as those used in gasoline. 

If your talking specifically about using kerosene as the feed stock, then the products will contain larger amounts of small (C2-C5) products. For example, [this](http://pubs.acs.org/doi/abs/10.1021/i200024a026?journalCode=iepdaw) paper (full text not free) says that cracking of kerosene yielded significant amounts of ethene, propene, butene, and butadiene. 

These chemicals have many different uses, but a major use of this class of chemical (often called olefins) is to make plastics like polyethylene and polypropylene. ",null,0,cdnc9mz,1rfpcs,askscience,top_week,3
sf_torquatus,"The products of catalytic cracking are smaller hydrocarbons. The catalyst (usually a strong acid zeolite or precious metal) cleaves the C-C bond. You will find a distribution of products corresponding to the temperature, pressure, and catalyst. Kerosene itself is a product of catalytic cracking. One would want to crack it further to produce smaller hydrocarbons. 

Regarding the ""why"" - I'm a bit sketchier on these details, so I'd ask anyone with a better understanding to pitch in. Kerosene is used as jet fuel, and I found a patent that described cracking kerosene to yield gas-phase products, but I don't understand the advantages of such a process versus fuel injection, unless such a process improved the injection in some way.",null,0,cdmu5xz,1rfpcs,askscience,top_week,2
Surf_Science,"Everything in your cell is doing what is thermodynamically favourable. Proteins involved in transcription bind to a gene because that binding is favourable, they function because that is energetically favourable. The produced proteins bind each other causing actions that occur because those are also energetically favourable. ",null,0,cdms0zn,1rfooz,askscience,top_week,3
sparky_1966,"DNA alone can't determine what a cell does, you can think of it as storing information. That information can be turned into RNA, some of which regulates genes, proteins and a few specific reactions. The proteins handle most of the actual work.

So, in the simples example if a bacteria that can use multiple sugar types for energy is sitting in an environment that has no lactose sugar, it usually doesn't waste energy making enzymes to break it down. If suddenly lactose becomes available, a receptor protein can bind the lactose and either activate transcription of lactose digesting enzyme from the DNA, or more commonly in bacteria, change shape and fall off the DNA, allowing the gene to be expressed. As the enzymes break down all the lactose, eventually the receptor protein wont have any to bind to, and will switch shape again to turn off the gene. There are many other levels of regulation, but that's the simplest example.

As far as viruses, there are a number of different strategies they use to take over a cell. Almost never is it just a piece of naked DNA floating around, since the environment and cells are full of enzymes to destroy those fragments. Probably the easiest system to understand is a DNA virus with a protein coat. The protein coat protects the DNA, but also makes sure it gets delivered. The protein is usually shaped to bind to the bacteria or cell it infects. On binding, the proteins change shape and make a path through the cell membrane for the DNA to get in. There is energy stored in the shape of the protein and the winding of the DNA (taken from the last cell) that allows injection of the DNA without other sources of energy. Once in the cell, the DNA gets replicated and transcribed in to viral proteins and more viruses like any other DNA. That's the simple version, there are any number of different virus types, some use DNA, some RNA, some large viruses carry most of the proteins they need to begin replicating so they can shut down most of the hosts protein production, etc.  ",null,1,cdmsir9,1rfooz,askscience,top_week,1
darksingularity1,"Technically it's not DNA. That determines what a cell does. Think of it as a master blueprint for a house. It contains a great idea, but it's not actually contributing to the building of the house. The workers (proteins) are who/what do everything. The architect might be the direct liaison to the blueprint. He reads it and converts it into instructions for a worker function. Technically new workers are created in the analogy sense, but I'm sure you get what I mean. The proteins are what actually create changes in the cell. In fact, certain proteins even act on DNA to control the expression of other proteins. The DNA does nothing.",null,0,cdndkh6,1rfooz,askscience,top_week,1
chrisbaird,"You seem to be confusing length (m) with acceleration (m/s^2) which are different things. If gravitational acceleration is very small, that does not imply there is anything in the system with a small length scale. It just means the gravity is very weak. Quite the opposite case is more important actually: quantum effects and gravitational effects should intersect when there is a large amount of gravity in a very small volume (such as in a black hole).",null,0,cdmt1ks,1rfo90,askscience,top_week,1
steeeeve,"If you had a rigid bottle, a difference in pressure would build up as you travel further into the ocean. This difference in pressure will cause greater net forces on the water at the mouth of the bottle, causing it to enter the bottle more rapidly. The amount that this happens will depend on some complex fluid dynamics, as the air needs to leave the bottle as well as the water entering it. 

If the bottle were pressurized with air so that the pressure was at equilibrium between the inside and outside of the bottle at the bottom of the ocean, then only the difference in buoyancy will cause the bottle to fill, similar to the case of a few feet of water. In this case, the amount of time would be similar for both cases, though perhaps not exactly the same due to changes in the viscosity of the water and air at those pressures.

The collapse of an air-filled bottle would depend on what kind of bottle is being used. For a typical soda bottle, the bottle can be collapsed just by sucking the air out of it (say, using your lungs). This means that a pressure difference of less than one atmosphere will cause the bottle to begin to crumple. The pressure increases with depth at ~1atm/10m of depth, so the bottle would crumple long before reaching the bottom of the ocean.",null,0,cdmxqyd,1rfo1d,askscience,top_week,3
creepy_old_grampa,"Police Radar is tied to their speedometer and decremented from the total, Source, I used to convert old cop cars to taxis, and I could always find the speedometer signal wire spliced under the dashboard already when I went to put in a meter.",null,0,cdmttof,1rfltg,askscience,top_week,6
EpicEvslarg,"So a car is travelling at 100 km/h North

A police car is travelling at 100 km/h South

The relative velocity would be 200 km/h

So the police radar would either know what speed the police car is going at, and automatically calculate the velocity of the other car, or the policeman would have to do it in his head by looking at the radar, and his speedometer. 

In this example the radar would either say 100 km/h or 200 km/h, so it would be easy to calculate.

I hope I solved your question.",null,0,cdmsce3,1rfltg,askscience,top_week,3
Dyolf_Knip,"That's just a matter of subtracting out their own velocity.  The real problem is angles.  If you were traveling at 100 mph perpendicular to the beam of the radar gun, it would register your speed at basically zero, because it only measures relative velocity along the path of the beam.  Any deviation from that decreases the measured speed.  So what cops do is position themselves as much as possible such that are directly in the path of incoming traffic.  I.e., right at a bend in the road, on an overpass, etc.

Area radar systems get around that limitation by being smart.  The radio beam can't really tell you how fast something is going, but it can tell you where it is.  An attached computer says ""5 seconds ago it was there, now it's here, x miles away, ergo it's moving this fast"".",null,0,cdnjjys,1rfltg,askscience,top_week,1
Doener_wa,"I can tell you something about the Langmuir isotherm. To get to this equation you have some asumptions to make: first is you have an isotherm system, which means your temperature is constant and second your gas which will be adsorbed formes a mono-layer on your surface (there are equations which involve multi-layer adsorption). Therefor you get the coverage of your surface and your Langmuir-isotherme can describe how much you may adsorb until your surface is fully covered. Also Langmuir isotherms are used to describe how well an adsorber adsorbs a specific gas or a mixture of gases (all will adrob differently). This is very useful because you are now able to characterize reactions which are done using a (heterogene) catalyst or to cunstruct an adsorber like it is used in many chromatographie-applications. 
To your Freundlich isotherm: I think this must be a similar concept just using other asumptions.
I don't want to go in detail now, if you have further questions, just ask on.
Source: I am a graduated chemical engineer and I am currently visiting a lecture about adsorption.",null,0,cdniulx,1rflnd,askscience,top_week,1
openLIKEeuchromatin,"The WHY part of the question:
First think of it in terms of fitness (this is always a good idea when navigating through these types of organismal biology questions). The number one goal for life from a biological perspective is to reproduce and pass on your alleles. With that in mind, try to think of why these birds have all grouped together and are ""chatting"" away. Keeping fitness in mind (#1 goal in life is to reproduce) you know that the grouping and social communication behavior of these birds must be important in order pass on their alleles. Since these behaviors (phenotype) are important to the survival of the crow species, then they must have evolved via natural selection.

The HOW part of the question:
Birds calls have evolved for millions of years acted on by natural selection. The chirps, coos and shrieks you hear everyday are a product of that. Many birds have developed a communication system that allows them to recognize individual calls within that population. Much like humans can tell the difference between each others voices. Look at it from the birds perspective. Birds have very sensitive ears and a respiratory system with many airways that allows them to make the complex calls. Try not to fall into the ""anthropomorphism trap"". A large crowd of crowing birds in the eyes of a crow is very different than in the eyes of a human. A ""noisy crowd conversation"" from a humans perspective is loud and hard to decipher what an individual is saying (i.e. sporting events, concerts, etc.). This is not the case for birds. Some birds are able to pick up on some of the slightest changes in frequency to hear exactly who is calling and what the call is about (i.e. food, mate, predators, etc.). Many birds have a critical period during development where they learn specific calls usually unique to that  population. Some bird call are even genetically ""hardwired"" and do not require learning. Not all birds are social though and communication does vary from species. 

Last point:
Calling and crowing is not the only way birds communicate. A wide range of behavioral displays are used in junction with the calls in order to send a complete message to another bird (the receiver). 
",null,0,cdn0pf1,1rfkwr,askscience,top_week,3
chrisbaird,"Not enough to notice. You can test whether gravity has any noticeable effect easily. Pluck a guitar string whole holding it upside down and see it sounds any different from when plucking it upright. The relevant force for these instruments is the tension in the strings and drum membranes, which is enough stronger than gravity that you can ignore gravitational effects. 

Note you only asked about lack of gravity. I am assuming you mean there is still normal air pressure provided by a pressurized compartment. If there were no air, or lower air pressure, than that would definitely effect sound propagation. ",null,0,cdmt7yy,1rfkjv,askscience,top_week,3
lvachon,"An acoustic guitar has been on the ISS for a while.  According to Cmdr Hadfield, the only thing that required changing was his play style since he no longer had the weight of his arm to help move down the fret board.

Source : http://www.youtube.com/watch?feature=player_detailpage&amp;v=gWTndmDHZQc#t=59",null,0,cdn9no2,1rfkjv,askscience,top_week,1
Ocaiman,"No, plants cannot survive without oxygen.  They respire on O2 just like any other living thing.  O2 is a byproduct of energy production using photosynthesis and plants eventually give off more O2 than they take in to breath.

To your question, a plant needs oxygen to germinate and grow until it begins photosynthesis.  They do not store O2, thus they cannot live in an atmosphere of CO2 or they would suffocate (the O2 would diffuse into the environment).  They can live in a clear sealed container with access to light, as they continuously reuse the CO2 and O2 that was sealed in with them, but they reach a limit in growth.",null,0,cdmsibb,1rfkeu,askscience,top_week,5
iorgfeflkd,Its engine was cut off a long time ago. It is on a trajectory that takes it beyond the solar system.,null,2,cdmr7xs,1rfk5l,askscience,top_week,30
Gprime5,"I think you might have misinterpreted something because your description doesn't make sense.

Voyager 1 doesn't have any actual engines, only small thrusters that keep it pointed towards Earth. The craft is in a hyperbolic trajectory meaning it has enough velocity travelling away from the sun that it will never come back.",null,1,cdmrakq,1rfk5l,askscience,top_week,10
PorchPhysics,"http://www.jaymaron.com/asteroid/tour-l.jpg

As the others said, its on a hyperbolic path out of the solar system.  This means its its velocity is greater than or equal to the escape velocity required for the sun.  

As for your idea that ""we're always moving around something"" is not really true at all, but in the case of voyager, it is now and interstellar probe, no longer orbiting our sun or being considered part of our solar system, it now orbits the galactic center.",null,2,cdms1nn,1rfk5l,askscience,top_week,10
Osymandius,"You're right - there are lots of ways to kill bacteria. Antibiotics are selective ""weapons"" against bacteria which is why they're so important. Because they're specific to bacterial components, they're safe to give to patients without destroying their own cells.

Let's take another example of a way to kill bacteria: heat. Most bacteria give up at about 50/60^o C, some thermostable bacteria (see T. aquaticus) are good for a bit more - up to 85/90^o C. Yes - all antibiotic resistant bacteria will be killed by a 100oC burst, but then you've got the put the patient through that! 

Take any method that will kill bacteria that isn't antibiotics, and it'll probably do some damage to the host. Irradiation, particulate disruption, salt membrane disruption, electrostatic membrane disruption, intense dehydration etc.",null,1,cdmradq,1rfk3g,askscience,top_week,10
thedveeeee,"There's actually only a fairly limited number of ways to kill bacteria. To list a few, you can kill them through targeting protein synthesis, targeting DNA replication, and using cell wall synthesis/growth inhibitors. Some newer antibiotics are being produced that target ATP synthase, an enzyme that produces ATP for the bacteria.

Unfortunately, the specificity in these antibiotics lies in the fact that we can't administer compounds that are toxic to human cells. Many antibiotics (like methicillin) are mildly toxic to us so they must be modified. That being said, it takes years and millions of dollars to come up with solutions to these problems. 

Edit: To touch on antibiotic resistance, and this is a very simple explanation; when bacteria are exposed to sublethal doses of antibiotic, selective pressure can cause a change in their genome, in which the most advantageous traits are passed on. This leads to strains of bacteria that are resistant to antibiotics, and these bacteria can pass their advantageous genes on to other bacteria. You may have heard of the incredibly famous MRSA group of bacteria; Methicillin Resistant Staph Aureus. This is a strain of Staph aureus (a natural flora of bacteria found on your skin; it's very common) that has evolved to resistant methicillin antibiotics. ",null,13,cdmsn6g,1rfk3g,askscience,top_week,21
justin3003,"The big problem is that there are only so many ways to attack bacteria effectively. Many of our antibiotics center on attacking replication or protein synthesis, two areas of significant difference between humans and bacteria. This makes most modern antibiotics much less toxic to humans than they are to bacteria. Also, some bacteria are totally resistant to many antibiotics simply by their biology (ie. the drug cannot interact with it, etc.), limiting the available options to only a few drugs.

Unfortunately, because we only have these limited points of difference, antibiotic use over time tends to lend itself to the selection of bacteria that are not able to be killed by these mechanisms. As these elements become more resistant, we have more and more limited options to further address this problem. It is further compounded by the fact that antibiotics are not specific to the pathogen you are trying to treat; to eliminate one infectious pathogen you bathe all of the other bacteria in your body with the same drug. Thus you don't just drive resistance of pathogenic bacteria but also harmless bacteria in your body that, under the right circumstances, may become harmful. 

So, to get to your question, that is why we are terribly worried about antibiotic resistance. Bacteria are a constant presence in the environment and evolving faster than we can create effective, tolerable treatments.",null,9,cdmsg9e,1rfk3g,askscience,top_week,12
fazedx,"The most difficult part of drug design and discovery is to kill the thing you want to kill without harming ""healthy"" cells in the body. Most anti-bacterials are beta-lacatam antibiotics. That means they work by interfering with the building of the cell wall of bacteria. To put it simply, it disrupts penicillin binding proteins that are necessary for cross-linking of bacterial cell walls (kind of like the mortar in brick and mortar - without the mortar, the wall would not hold). Without the ability to reconstruct and expand cell walls, bacteria cannot grow or reproduce.

beta-lactam antibiotics work because they have similar structure to the penicillin binding proteins, but do not actually hold cell walls together. The bacteria use these to make their cell walls, but because they don't hold, the cell wall breaks down. It's kind of like giving a bricklayer sand instead of mortar to build a house. 

Some bacteria can produce beta-lactamase, which cleaves the beta-lactam ring and renders it ineffective. ",null,0,cdmv9no,1rfk3g,askscience,top_week,2
foamerc,"The short answer is there are many ways to kill bacteria, but few that discriminate between bacterial and human cells. Bacteria are cells too, and  they share many similarities with human cells, and a few differences here and there. Antibiotics exploit such differences such as bacteria have a cell wall and human cells don't. 

When discussing about killing them after they've infected someone within the body, you're pretty much left with antibiotics, which there are many subtypes working in different manners, but for all intents and purposes are chemicals ingested/injected into a human for the purpose of killing specific bacteria.

In addition you don't want to indiscriminately kill off all bacteria because that's how you select for resistant organisms, kill off normal helpful bacteria, and some nasty ones grow in their place. Look up C. difficile infections - a relatively new cure is to eat processed shit of other people.",null,0,cdo6tkv,1rfk3g,askscience,top_week,1
gfpumpkins,This isn't really an answerable question. The normal bacteria found in humans is incredibly unlikely to be pathogenic to ants. ,null,0,cdmr746,1rfix6,askscience,top_week,3
proule,"Your question seems to be based around the assumption that humans are bigger than ants, thus, our bacteria must somehow be stronger than the bacteria that colonize ant bodies.

There's no fundamental difference between bacteria that colonize ant bodies and those that colonize human bodies. Human bacteria don't need to be ""stronger"" to colonize humans; they're adapted to colonize humans just as bacteria in ants are adapted to colonize ants.",null,0,cdnh2jd,1rfix6,askscience,top_week,2
iorgfeflkd,"It's an amorphous solid, which basically means that it behaves like a solid (as most people would interpret them) but the atoms aren't arranged in a crystal lattice. This makes a difference if you try to measure heat transfer through the material, for example, or look at the diffraction of x-rays through it.

[Diagram](http://www.steelguru.com/uploads/reports/sss1-29-08-2008.jpg)",null,0,cdmq2n8,1rfih8,askscience,top_week,11
botanist2,"III&gt;Trees and plants existed millions of years before the first oxygen producing creatures

Photosynthetic organisms (mostly cyanobacteria that form [stromatolites](http://en.wikipedia.org/wiki/Stromatolites)) existed millions of years before the first oxygen producing creatures, but trees and plants as we know them today didn't evolve until much, much later.  

As to the rest of your question, there are a lot of other ways to make CO2 than just living organisms and one of the most likely sources of CO2 was volcanic activity.",null,4,cdmq5zo,1rfia5,askscience,top_week,10
sparky_1966,"I think you meant before the first oxygen consuming creatures. Trees and other plants weren't around for a long time after the start of making oxygen. The first photosynthetic organisms were single celled. When photosynthesis started, the atmosphere was thought to be a reducing atmosphere, so the excess oxygen taken up by iron and made rust, and there was a lot of methane that UV light made into CO2. Carbon was not necessarily limiting, since all the carbonate (limestone) had yet to form, and the oxygen comes from splitting water molecules. The oxygen cycle today is not necessarily the oxygen cycle at the beginning. ",null,1,cdms085,1rfia5,askscience,top_week,2
foamster,"Well, volcanic activity alone 'produces' a *lot* of atmospheric CO2. 

My understanding was that the atmosphere had very little oxygen initially, but plenty of CO2 at around the time that photosynthesis began to take off. Animal life wasn't really able to develop until the atmospheric oxygen concentration was high enough to allow for sufficient metabolic rates -- oxygen produced almost exclusively by algal photosynthesis.",null,1,cdmscd2,1rfia5,askscience,top_week,1
florinandrei,"Any battery has an internal resistance. Any resistance, when a current passes through it, it heats up. Therefore, a battery will heat up (or at least become a bit warmer) any time you either charge it or discharge it.

The higher temperature of a charging battery is not an indicator of it charging, it merely indicates that some current is passing through it. But same would happen during discharge.",null,1,cdmrmjz,1rfi8e,askscience,top_week,4
Guanglais_disciple,"The chemical reaction is endothermic and then exothermic (li-ion for example) but the joule heating (current ^ 2 * resistance) usually dominates. Since joule heating isn't a function of current direction, you see heating in both cases. For very low current, though, the chemical reaction dominates and it cools slightly. ",null,1,cdmtg6y,1rfi8e,askscience,top_week,4
dudds4,"It would be interesting if that was the case, but no. It'll help to understand why there is heat produced.

Basically the transfer of energy into the battery is not perfectly efficient. some energy is lost. Where does it go though, ( energy can not be destroyed) ? Heat, among other things, is the answer. 

Imagine a flowing stream of water. Some of the water laps up on either side of the stream, and gets absorbed by the land. Not all ( although nearly all) of the water makes it down the stream. Here the water getting absorbed is what we observe as heat


Same goes for discharging energy, it's just another form of energy transfer, and not perfectly efficient

",null,0,cdn4tzd,1rfi8e,askscience,top_week,2
polkasalad,"On discharge the batteries heat up due to the internal resistance.  Internal resistance increases as temperature decreases as well, which is why batteries last longer near room temperature, so if it were to cool the battery would actually lose capacity faster as you used it. Consequently, heating up the battery too much will damage the cell.  

I'm sure someone can offer more info, I'm in a graduate course relating batteries to hybrid-electric cars right now which is where I got my info from. ",null,2,cdmpz1e,1rfi8e,askscience,top_week,2
Weed_O_Whirler,"You would barely notice a difference. 

The main reason the magnet in the motor needs to keep being pushed isn't due to friction, but due to [Back EMF](http://en.wikipedia.org/wiki/Counter-electromotive_force) force. When spinning the magnet in the coil, a current is produced in the coil, and a counter-emf voltage opposes the current. These will always be of the same amount of energy- thus even without friction a magnet will very quickly slow down, as you will not be able to extract more energy from the magnet than you used to get it spinning in the first place. 

It is good to think of how these generators are not ""making"" energy, they just ""convert"" it. So, we burn stuff in order to move pistons, the moving piston spins a magnet, and the moving magnet makes electricity. Even without any friction or losses in the burning process, you'll never get more energy out of the generator than you put in. ",null,0,cdmqz7c,1rfhiw,askscience,top_week,7
CoryCA,"Only in that all life on Earth is related, and that they are both plants. (Though a mango stone reminds me more of a peach stone.)

A pumpkin is a squash variety of the species Cucurbita pepo of the family Cucurbitaceae. Acorn squash are also a C. pepo variety (the species is highly variable), zucchini are a different species in the same genus, and watermelons and cucumbers are part of the same family.

Mangos of genus Manifera of the family Anacardiaceae which also includes cashew, poison ivy, sumac and pistachio.",null,0,cdmrzhg,1rfgwc,askscience,top_week,2
proule,"There are many examples of ""convergent evolution"" in the world. That is, evolution that has caused very distantly related organisms to take on a similar appearance in some fashion. Another example of this is flying insects, birds and bats. Obviously you can see a large difference between insects, birds and bats, but birds and bats may seem like they're more related than not, right?

Birds are more closely related to reptiles than bats, which are mammals. If you look at the parts of their bodies specialized for flight, they *look* similar at a base level, but: A bird's wings are modified forelimbs (arms), and they still have distinct, separated feet. A bat's wings are a leathery extension of skin that stretches between the modified forelimbs, and actually reaches down to the legs. [Here's a picture to illustrate my point](http://upload.wikimedia.org/wikipedia/commons/3/38/Homology.jpg).

In biology, function is very tied to structure. Two structures can evolve to look very similar based on sharing the same end function, however, this does not necessarily imply a close relation.",null,0,cdnhdlw,1rfgwc,askscience,top_week,2
Trill-Nye,"In this case, it's better to think of color as a result of light absorption and emission, rather than reflection. When light hits a gas, it can be absorbed by various processes. Visible light just happens to be the right energy to excite the electrons bound to atomic nuclei in some molecules, such as those making up chlorine gas. These excited electrons, which have been given energy by a photon, then relax to their original energies, giving off new photons of a particular wavelength (and therefore color).

Electrons are unusual in that, due to quantum effects, they can have only certain discrete energies. This is determined by the structure and composition of the atom, and its interactions with other nearby atoms. Gasses that are not colored do not have electron excitation mechanisms of the correct energy to be excited by visible light, then give off light of a specific color.

If a gas were black, it would have to absorb most incoming photons, then give off accumulated energy as something other than visible light, such as photons of a wavelength that cannot be observed by the human eye. ",null,0,cdmsy9a,1rfggp,askscience,top_week,4
AznInvasian,"In easier terms to understand:

     Light wave goes into gas atom, energizes an electron and pushes it to a higher energy orbital. The electron doesn't like this, and returns to its original orbital, emitting that same amount of energy it absorbed. This makes it glow this specific colour (corresponding to the wavelength of light it absorbed).",null,0,cdnddj3,1rfggp,askscience,top_week,2
dontgothatway123,"In a specific practical sense when actively measuring the cardiac output (CO) of a person it is important to factor in the persons size.  This makes the CO calculation more relative.  For instance the average CO for a adult male is 5.6L/min (the volume of blood ejected from the heart every minute).  Now we'll introduce two people. One man is 5'2"" (157cm) 105lbs (47kg) with a BSA of 1.44m^2.  The other is 6'6"" (198cm) 285lb (129kg) with a BSA of 2.67m^2.  If we just considered CO (stroke volume x heart rate) would it make intuitive sense that if both of these individuals had a CO of 5.6L/min that would be ok for both?  No, some form of individualization is necessary.  This is obtained by taking the CO and including the BSA into the calculation.  This measurement is called the cardiac index (CI).  Clinically/practically it serves a better purpose and indicator for monitoring hemodynamic states in controlled situations.  Using the examples above the first man would have a CI of 3.89L/min/m^2 and the second man would have a CI of 2.1L/min/m^2.  Considering the normal CI ranges from 2.6-4.2L/min/m^2 the man in the second scenario is about a hairs breadth away from cardiogenic shock.

Hopefully that helps shows the significance of BSA inclusion within a certain situation.  As for whether there are better alternative parameters I am unsure. In research you tend to see body measurement index (BMI), ideal body weight (IBW), lean muscle mass calculations, body fat percentage (BF%), and body surface area (BSA) measurements used a bit.  Each has their own benefits and pitfalls.",null,0,cdn0ahy,1rfg3b,askscience,top_week,2
atomfullerene,"Height is  highly dependent on the amount and quality of food one receives as a child.  Poor farmers are often quite short.  People living in modern countries with plenty of food are taller.  Interestingly, skeletons of hunter gatherers before the dawn of agriculture were also often quite a bit taller than their immediate farmer descendants (though height does recover in the farmers somewhat over time) owing to the better nutrition of the hunter-gatherers as compared to the early farmers.

Farther back in prehistory early protohumans were often shorter than modern people.",null,0,cdmt3vc,1rfff0,askscience,top_week,12
Infinite_Ambiguity,"If galaxies are close enough to start with (as in clustered together, relatively speaking), then there's sufficient gravitational force between them to bring them together and to overcome inflation/expansion.  

To use an extreme example, inflation/expansion doesn't tear the earth apart, or the solar system apart, our own galaxy apart, or any other individual galaxy because the gravitational fields win each such cluster is sufficient to keep everything together.  Same concept between galaxies that are relatively close together.  

Many cosmologies believe that, I. Billions of years, our night sky will be totally dark and telescopes will be insufficient to see anything, except for the galaxies in our own cluster (which, I think, total something like 36 total galaxies).  ",null,0,cdmovvo,1rff0z,askscience,top_week,5
DarkLather,"Galaxies exist in groups. Galaxies within the same group can be gravitationally bound to each other. They can orbit each other and collide. Our Milky Way and the Andromeda galaxy, both members of the ""Local Group"", are currently on a collision course. ",null,0,cdmqwqr,1rff0z,askscience,top_week,2
keithb,"You are short-sighted. I can tell becasue your glasses have ""negative"" lenses, which cause a beam of light passing through them to diverge, to spread out. You can see the light which has been diverted in the brighter halo around the shadow of your glasses. If you were long-sighted you would glasses with lenses which are ""positive"", or converging, and there would be a bright spot in the middle of the shadow of the lens rather than a bright rim. 

The soft shadow of the lens appears darker than the carpet around it because the light passing through the lens is spread out over a larger area. The lenses will absorb a little bit of the light passing through them, but mainly they redistribute the light. That's what lenses are for.",null,1,cdmqwmz,1rfepp,askscience,top_week,18
iorgfeflkd,"There's a way of approximating functions called a Taylor series, where you add up diminishing terms with higher and higher powers. For example, the cosine of x can be approximated as 1-x^2 /2 + x^4 /24 - x^6 /720 ...

The tangent is the ratio of the opposite and adjacent sides of a right triangle, and for a 45 degree angle the tangent is 1. 45 degrees in radians is Pi/4. This means that the inverse tangent of 1 is Pi/4.

That series for Pi is based on the Taylor series of the inverse tangent function, substituting x=1 so that it equals Pi/4 (x=1 greatly simplifies the math because 1^anything is 1).

So basically, it's another way of saying that the tangent of 45 degrees is 1.",null,0,cdmpzwi,1rfdy0,askscience,top_week,15
Jetamors,"We've known about cancers for a very long time. [The oldest known description is Case 45 from this Egyptian papyrus from 1600 BC](http://archive.nlm.nih.gov/proj/ttp/flash/smith/smith.html), though I don't think it theorized about the cause. There's a great article about old Greco-Roman treatments [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820670/). According to the article, the oldest known theory about cancer (written by Galen) attributed tumors to an accumulation of black bile, due to the black veins that appear around many tumors. Galen was working off the [four humors theory](http://en.wikipedia.org/wiki/Four_humors), which was predominant in Western medicine from antiquity to about the 1800s.

Edit: I should correct myself, Galen's theory is the oldest one in the *Greco-Roman tradition*. I don't know much about medicine in other cultures, but I wouldn't be surprised if they (China particularly, but probably others as well) theorized about the origin of tumors at about that time or earlier.",null,0,cdmt0zr,1rfdsx,askscience,top_week,13
iorgfeflkd,"In [this](http://iopscience.iop.org/0143-0807/16/4/005) paper, they measured how likely it is for toast to land on the buttered side down, and found it was 62% (with thousands of tests), significantly more likely than random chance.",null,2,cdmozzv,1rfdo6,askscience,top_week,8
null,null,null,0,cdmubzf,1rfdo6,askscience,top_week,1
atomfullerene,"Murphy's Law is usually phrased ""Anything that can go wrong, will go wrong, and at the worst possible moment"".

It's meant to be taken tongue-in-cheek, it's not a physical law, but somewhere between a joke and a superstition.  If it was literally true, we'd all be dead. But it does have some level of validity, especially in the engineering context it was invented for.  Complex machines have lots of parts, and often only work right if _all_ the parts work together properly.  The probabilities that each part will fail get multiplied, making it more likely that something will go wrong.  And parts are more likely to fail under stress, which means while the machine is operating--often the worst time.  Eg, it's much worse if the wings fall off your test plane in the air than if it's sitting on the ground.  ",null,0,cdngwpa,1rfdo6,askscience,top_week,1
botanist2,"For the sake of reference [here's](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078092) the original article about these stone-tipped spears that you're asking about.  To clarify, they're not talking about aliens using spears, they're talking about different species of *Homo* (e.g., we're *Homo sapiens*, they could be talking about *Homo erectus*).  These spear tips were dated from the substrate in which they were found, they weren't dating the specific material of the spear (which could be much older). 

There's lots of ways to date different materials and the choice depends on what you're trying to test.  Carbon-14 is used predominately for organic materials, the article in question used [argon-argon](http://en.wikipedia.org/wiki/40Ar/39Ar) dating that is good for dating metamorphic and igneous minerals.   

&gt;When it comes to radio active decay, does it magically start over when you shape the object?

Not when it comes to minerals.  Radioisotope dating methods for minerals can only give you an idea of how long its been since the last time they cooled below their closure temperature (the temperature at which its assumed that isotopes aren't flowing in and out).  They tested the age of the substrate where the spear points were found because they wanted to know how long it had been since the points came to rest in that spot (and presumably when they were last used by their owner), not the age of the stones that were used to create the spear points.",null,0,cdmtews,1rfd9c,askscience,top_week,5
descabezado,"For radiometric dating in general, the clock starts once the object stops exchanging atoms with its surroundings.  For rocks, this means when the minerals of interest crystallized; for organic remains, it means when the creature died and stopped taking in air.  So, what they probably mean here is that the spear handle is made of wood that died 500000 years ago.  You are correct that dating the stone spear head would not be useful.

An interesting consequence of this is that you have to be very clear about what you've dated.  If you date pages of a book to be 2500 years old, it means the paper is that old, not the writing on it.  If you date a sedimentary rock to be 200 million years old using U-Pb dating with zircons, it means that the zircons were eroded out of 200 Ma old crystalline rock, but the timing of their erosion and deposition (i.e., the age of the sedimentary rock) could be any time between 200 Ma and yesterday.",null,0,cdmt83q,1rfd9c,askscience,top_week,3
tin_can_conspiracy,"Artifacts such as these are usually only dated by how deep they're buried (similar to how we date the dinosaurs) or by carbon dating artifacts found in the same site as the stone tools (wooden spear handles, bones, and such.)",null,1,cdmpbgx,1rfd9c,askscience,top_week,2
omgdonerkebab,"It's just a convex mirror.  The mirror is curved toward you, so that the rays of light that get to your eye come from a larger angle.  (Kind of like [this image](http://0.tqn.com/w/experts/Physics-1358/2009/06/Convex-Mirror.jpg), but with the directions of the arrows reversed.)  

This allows you to see a wider angle of stuff behind you, which has its obvious uses when driving.  But it also means that this larger angle is squished into a smaller area on the mirror, so the objects look smaller on the mirror.  Your brain might interpret it as the object being farther away, which would be wrong.  The object is closer than it appears to be.",null,0,cdmpcpr,1rfd8w,askscience,top_week,7
xenneract,"Sure. [You can hire a plant to do it for you.](https://en.wikipedia.org/wiki/Photosynthesis)

If that's not chemical enough for you, there is also active research in making [artificial photosynthetic cells](https://en.wikipedia.org/wiki/Artificial_photosynthesis) that perform the reaction you are describing.",null,0,cdmrnvi,1rfd6j,askscience,top_week,6
sodium_dodecyl,"We *can*, but it's not going to be terribly efficient (or necessarily fast, I don't have any kinects data). An example of a possible pathway: Reduce [Reduce CO2](http://en.wikipedia.org/wiki/Sabatier_reaction) to CH4 + H2O, then use electrolysis to split H2O --&gt; O2",null,0,cdmrezw,1rfd6j,askscience,top_week,1
steeeeve,"Yes, it is possible. However, the reason we make CO2 is because reacting carbon with oxygen to form CO2 bonds releases energy. The same amount of energy has to go into the bond to break it. Since power plants are not 100% efficient (and never can be) the re-separation will always cost more energy then we got from burining the fuel in the first place (assuming the fuel is almost all carbon, like in coal)",null,0,cdmxv3u,1rfd6j,askscience,top_week,1
kyaj21,"Technically, yes. CO2 is just 1 part carbon, 2 parts oxygen, as any school child who has taken introductory chemistry would be able to tell you. Yes, we could extract oxygen from the carbon dioxide, but the carbon would still be there. Reducing carbon emissions is a whole other matter, as in order to reduce carbon emissions, we would have to change the fuel sources or at the very minimum how we process them, and what we would do with the carbon once we extracted the oxygen from the carbon dioxide.",null,5,cdmrfta,1rfd6j,askscience,top_week,1
proule,"Curiosity drives you to ask questions, which, in being answered, can improve your chances of surviving. This ingenuity is perhaps the most evolutionarily successful means of avoiding death due to outside influence. Other evolutionary tactics would include simply being bigger than anything that could otherwise hurt you.

In animals capable of higher learning, curiosity is fundamentally a desire to learn and understand the world you interact with. At the most basic level, curiosity is important to be able to accomplish the key tasks for each living being: Survive and produce offspring.
",null,0,cdnh79b,1rfd57,askscience,top_week,2
spryspring,"Curiosity is a behavior that has probably been selected for in some animals by natural selection, or at least has not been selected against. Suppose that a ""gene for curiosity"" (I'm sure in reality it's not nearly that simple) arose in the ancestors of cats. Proto-cats that had this gene tended to have more offspring than those who didn't (we might guess that they, in being more curious, found more food).

Or maybe it's totally a learned behavior, I don't know. But in any case that's how behaviors can arise.  ",null,2,cdn1stc,1rfd57,askscience,top_week,1
jadiusatreu,"Great answers from the beekeepers. To add a little more information apart from honeybees, not all bees make a honeycomb. Bumblebees make honey pots in which they store their honey.  These bees make a cylindrical, sometimes round, pot. Just a little more information for you.",null,0,cdmtjth,1rfcsm,askscience,top_week,3
HCOOH,"There are so many wild-types of bees... they don't make nests.
And the ""normal"" honeybees make round shapes, but because of the melting of these round shapes thexy become hexagonal. The whole thing is more.. a succes through error",null,1,cdmp126,1rfcsm,askscience,top_week,2
steeeeve,"There's no 'up' and 'down'. However, your brain is somewhat accustomed to zero-g; it happens whenever we're falling. The fluid isn't really ""floating"" because to the best of my knowledge there's no air in the part of your ear that controls balance. Rather, there's hairs in the ear that will 'flex' under a current that is induced when you accelerate. ",null,0,cdmxa5d,1rfcei,askscience,top_week,1
Manhigh,"The only mechanism for heat transfer from the space station is through radiation.  In general, all of the electrical components on a spacecraft and solar incidence (when in sunlight) produce excess heat which needs to be shed.  If you look at a picture of the space station you'll see a series of panels that are perpendicular to the solar panels.  While it is generally desirable that solar arrays always face the sun, it's generally desirable to have the radiators edge-on to the sun, facing deep space.

Coolant passing through the radiators is cooled and then passed back inside to keep removing heat from the station.  If you wanted to heat the station, you could have the radiators face the sun slightly.

In this photo, the radiators are the white  accordion-like structures:  http://milesobrien.files.wordpress.com/2010/08/iss1.jpg",null,0,cdmsvb7,1rfc91,askscience,top_week,6
Truck43,"That's really two questions, whether or not the shell will act as a faraday cage, I'll leave to another, but, the microwaves will induce currents in the case that will produce enough heat to start a pretty serious fire, and probably cause catastrophic failure in the battery. ",null,0,cdn3oke,1rfc8w,askscience,top_week,2
auralucario2,"From my limited experience in putting metal in microwaves, I think that the shell itself would begin sparking, due to the movement of electrons caused by the energy of the microwaves. As for the insides, it would probably escape direct harm from the microwaves, but the heat and electricity thrown off of the casing would probably do some serious damage.

Please don't try it though.",null,0,cdnv98v,1rfc8w,askscience,top_week,1
Aniridia,"Yes, overweight women are 1.5 times more likely, and obese women 3 times more likely to become pregnant than women of normal BMI, regardless the type of hormonal contraception used.

[From this article:](http://www.ccjm.org/content/79/11/771.long)

&gt; Q: True or false? Hormonal emergency contraception is more likely to fail in obese patients.

&gt; A: True. Most recent evidence shows that whichever oral emergency contraceptive drug is taken, the risk of pregnancy is more than 3 times greater for obese women (OR 3.60, 95% CI 1.966.53) and 1.5 times greater for overweight women (OR 1.53, 95% CI 0.752.95).16 Of all covariates tested, those that were shown to increase the odds of failure of the emergency contraception were higher body mass index, further unprotected intercourse, and conception probability (based on time of fertility cycle). In fact, among obese women treated with levonorgestrel, the observed pregnancy rate was 5.8%, which is slightly above the overall pregnancy rate expected in the absence of emergency contraception, suggesting that for obese women levonorgestrel-based emergency contraception may even be ineffective.

&gt;This is in line with recent reports suggesting that oral contraceptives are less effective in obese women. More effective regimens such as an IUD or ulipristal might be preferred in these women. However, obesity should not be used as a reason not to offer emergency contraception, as this is the last chance these women have to prevent pregnancy.",null,0,cdmrmwo,1rfbsd,askscience,top_week,3
stevenstevenstevenst,"The most serious affect upon the body due to exposure to lower or zero gravity is atrophy of the muscles.  As you will weigh less or nothing at all, you muscles have to work much less and thus will begin to degrade.  This is the reason individuals on the ISS need to work out regularly by running on a treadmill or though other means.

As blood circulation is negatively affected by reduced gravity (due to the way this system has evolved to partially utilize gravity in its function), other health problems may potentially be associated with manned spaceflight, such a neurodegeneration- although this research is ongoing.",null,0,cdmo0v8,1rfbi7,askscience,top_week,2
mzyos,"There is some worry at NASA currently about Optic nerve atrophy. This is where the nerve carrying signals for sight from the eye starts to deteriorate. It seems that about a 3rd of astronauts have this, if they have experienced long bouts of zero G. They are studying on ISS at the moment using a goldmann tonometer which measures eye pressure. They don't really understand what is going on just yet, but it might be due to the lack of gravity causing some of the eye, and it's nerve's blood supply being slowed, or stopped in one way or another. ",null,0,cdojdvm,1rfbi7,askscience,top_week,1
bohr_exciton,"&gt;If we know the wavelength of a polarized photon... then why cant we determine where exactly a given photon will interact with the resist? I'm guessing something here will touch upon wave-particle duality...

Right, specifically it's the wave aspect that sets the limit. Light passing through a specific aperture or lens will not arrive in one infinitely sharp point but in a disk (e.g. the so called Airy disk for circular apertures). For far-field light, the size of this disk will be determined by a number of factors such as diffraction and the aberrations in the imaging system. The best possible case using simple far field optics is to obtain the diffraction limited spot, which is on the order of half the wavelength of the incident light. 

&gt;Part2: If we've got vapor deposition for things like gold ... why can't we vapor deposit a single atom later of the resist ... again being able to do away with the complex mask?

I'm not really sure I understand this question. Under certain circumstances it's possible to deposit metals uniformly for a desired number of monolayers. However, you need a mask if want something other than a uniform layer, e.g. patterning for an integrated circuit. ",null,0,cdmsmcx,1rfapt,askscience,top_week,1
LeoPanagiotopoulos,"The limit of the situation you're describing is a ratio of 1 where the planet and moon are indistinguishable because they're the same mass. It's unlikely but possible. You're correct in your suggestion that the distance from the 3rd, larger mass in the system is important. If The distance between our twin planets (or moons? or ploons? or [manets](http://2.bp.blogspot.com/_gJ6d5yFc7fw/TL72k9N-pqI/AAAAAAAAB_I/Gbu1fWmRxPU/s400/g013v_manet_lemon.jpg)?) is comparable to the distance to the larger object in the system, their orbits around each other will be unstable. 

[Consider reading about triple star systems](http://en.wikipedia.org/wiki/Star_system#Triple_star_systems). The situation we're talking about is labeled C on the linked diagram. It's true that interactions between stars that are very close to each other can be a little more complex that cold, non-fusing rocks (planets), but in most cases the dynamics are comparable. 

Almost forgot: the 3rd object is more often smaller and orbiting the two inner objects, which are orbiting each other. Still your situation is possible. ",null,0,cdnk8oc,1rf9zl,askscience,top_week,2
amvakar,"The first (and inescapable) factor in the large size of source code compared to the compiled binaries is the lack of information density inherent in any plain-text format -- you've got to keep things human-readable, which means that you're restricted to the alphabet plus enough special characters for basic formatting and organization. Each operation will involve reasonably-descriptive names as opposed to the pointers to their location in memory that the processor will see. Documentation will also be present. In short: you're describing what the computer will do so a person could understand it, while the computer will only need to be told the bare minimum about the operation to complete it. To see this in action, run the source through any compression algorithm -- the size will go down significantly.

The second factor in large software projects is the presence of code that might never actually be used. For an operating system, you'll end up with drivers for devices you don't have or support for CPUs you're not using. For applications, you might have support for different APIs or just functionality you choose not to include in the finished product. For debugging purposes there may be tests and additional information so that problems can be tracked down, and in debugging builds optimization may be turned off. 

In short: source code is far more descriptive than binary for human purposes and includes a lot of things that you may never end up using in the final build.",null,2,cdmstpb,1rf8w9,askscience,top_week,14
Platypuskeeper,"The [Golden Rule](http://en.wikipedia.org/wiki/Fermi%27s_golden_rule) says that transition probabilities depend on the overlap between the initial and final states. In a Rydberg atom, you're in a highly excited state where the electron is far from the nucleus, and its overlap with the ground state and lowest-energy states is quite poor. So direct transitions back down to there are improbable. 

",null,0,cdmr8jt,1rf8ta,askscience,top_week,3
Daegs,"On earth you can see million miles away yourself, right now!!! Just look at the stars.

Remember the sun is ~93 million miles away, most of the stars you see are orders of magnitude further away. 

We can see the andromeda galaxy with our naked eyes, so that is 14,696,249,500,000,000,000 miles away!

In space, you wouldn't have the atmosphere filtering photons coming from stars, so you'd be able to see even more.

This is why we have the hubble telescope in space, to avoid earth's atmosphere. ",null,0,cdmtpir,1rf86r,askscience,top_week,6
king_of_the_universe,"http://www.uitti.net/stephen/astro/essays/farthest_naked_eye_object.shtml

says:

&gt; Bode's Galaxy (M81), at 12 million (12,000,000) light years has been spotted by several people. This [page at SEDS on M81](http://www.seds.org/messier/m/m081.html) has a description of how to see it.

&gt; The trouble is, at Magnitude 6.9, M81 is dimmer than most consider naked eye. It depends on whose eye it is, and also where the feet are standing. It has to be an exceptionally dark sky site, probably at some altitude, at the right time of year, etc.

WolframAlpha's answer to ""12,000,000 lightyears in miles"" is 7.05410^19 miles, which is 70,540,000,000,000,000,000 miles. (Take that, Daegs! ;)

The text also says that there could be bright events like super novas that could even be visible with the naked eye from further away for a few days.",null,0,cdncboq,1rf86r,askscience,top_week,2
stuthulhu,"&gt; once you pass the outer layers of our atmosphere you are weightless - why cant we achieve that speed?

Weightlessness is a state achieved when no force other than gravity is acting upon you. When a vehicle is accelerating/decelerating, that force will be acting upon you, and you will not feel weightless. You would feel pushed against the back of the vehicle by the force of the acceleration.

The shuttle must burn fuel to leave our inertial motion, and burn fuel to match that of its destination. Being likely far more massive, both become more expensive actions, and the more fuel required to do either action increases the weight even further. ",null,0,cdmtslm,1rf5vk,askscience,top_week,1
WendyMouse,"The shuttle is bigger.  A LOT bigger.

New Horizons is a very light spacecraft-- about the size of a grand piano, launched from a very powerful rocket.  It was the combination of the two that made it travel so fast, faster than anything else humanity had ever launched.  New Horizons does not have enough propellant to slow itself down to enter into Pluto's orbit.  The fuel to do that would be too heavy.


Escape velocity from Earth is everything.  Humanity hasn't mastered launching a bunch of things in pieces, merging them and having another separate launch in space yet. 

Just because something doesn't have weight, (you are not in zero gravity in space, you are in microgravity), doesn't mean it doesn't have mass or momentum.

",null,0,cdnst3v,1rf5vk,askscience,top_week,1
tigertealc,"Catalysis by definition is a process by which a substoichiometric reagent promotes a reaction by lowering the activation barrier of the reaction. So that would be the common denominator, I suppose. 

Working out the mechanism of a catalytic reaction is not always straightforward. Most often, mechanisms are proposed to follow mechanistic steps that have been determined for related systems, or using intuition. But a number of different control experiments must be run to differentiate between different possibilities. Often these experiments involve the kinetics of the reaction, whether it involves determining the rate law of the reaction or determining a kinetic isotope effect. Isotopically labeled reagents can also assist, by seeing where they end up in the product. Computation can certainly aid in the assignments of mechanisms, but empiricism is the main method. And of course, the exact experiments that one is able to run to elucidate the mechanism is largely dependent upon the specific reaction. 

If you have any specific questions about specific reactions, feel free to ask. ",null,0,cdmotdy,1rf5ro,askscience,top_week,4
Platypuskeeper,"There is no common denominator other than the fact that catalysts catalyze. One reaction might be catalyzed by acid, the presence of H^+ , which participate in the reaction but are released on a later step. Another reaction might be catalyzed by a Lewis base, where the base temporarily donates an electron pair to a reacting atom. Those two scenarios really have nothing in common other than that they fulfill the definition of 'catalyst'. The word describes a role something plays in a reaction, but the reactions can be as different as any chemical reactions. There's no general theory of reactions either.

",null,0,cdmt6k1,1rf5ro,askscience,top_week,4
Daegs,"This is not a 3D gif. 

A 3D gif would either require:

* two stereoscopic panels which you could view by changing the focus of your eyes so that the panels merge

* A single panel using red / blue shading and 3D glasses

* A single panel and special display to work along with polarized glasses.

This **non-3D** gif simply give perspective by being displayed over the ""break"" and the changing focus which cues our brain that there is 3d information being presented.

In other words, there is nothing special about 2 breaks, 3 breaks, 4 breaks, whatever.... the breaks are just used so that the gun can go ""over"" something that we perceive as flat. ",null,1,cdmtkie,1rf5et,askscience,top_week,6
ramk13,"Though diffusion is slower at lower temperatures, lowered vapor pressure is a much bigger influence. Most odors are either small solid particles or vaporized compounds. The equilibrium vapor pressure of a compound is exponentially dependent on temperature, so when it's colder a lot less of the compound gets into the air. Since it doesn't vaporize as much you smell less of it.

Also, most of the stuff you smell is more likely to be transported by convection (movement by temperature induced density gradients) or advection (forced movement) than diffusion.

For an empirical relationship between vapor pressure and temperature, you can use the [Antoine Equation](http://en.wikipedia.org/wiki/Antoine_equation) which is derived from the principles of the [Clausius-Clapeyron relation](http://en.wikipedia.org/wiki/Clausius-Clapeyron_relation).",null,0,cdmrge3,1rf3jg,askscience,top_week,3
stevenstevenstevenst,"At lower temperatures, vibration of particles is decreased due to the decreased energy of the system.  As diffusion of gases relies upon random vibrational motion for the even dispersal of a compound, gaseous compounds (such as any odor) will spread increasingly more slowly with decreasing temperature.",null,0,cdmp1lq,1rf3jg,askscience,top_week,1
Osymandius,"Contrary to the answers below ATP **is** produced within the chloroplast. ATP synthase is located in the thylakoid membrane/space and does make use of the proton motive force generated by either cyclic or non cyclic photophosphorylation. But - the ATP produced in the chloroplast just isn't enough to compared to the amount produced in the mitochondria. We move relatively minimal numbers of protons across the membrane during photosynthesis - the really important product of non-cyclic photophosphorylation is the generation of reducing equivalents (NADPH). This can then be used to fuel the Calvin cycle and the production of triose phosphates and sugar derivatives.

Once we have produced TP/sugars, these can be metabolised to produce NADH in the mitochondria. The proton motive force produced by the electron transport chain is considerably greater, and much more ATP can be generated than relying on chloroplasts alone.",null,0,cdmrffo,1rf3cf,askscience,top_week,3
quantum_lotus,"As /u/Osymandius says, both organelles can produce ATP (the most useful form of stored energy for a cell), but that mitochondria are much more efficient at making it.

But there is another consideration.  Evolutionary data and model point to chloroplasts being acquired *after* mitochondria.  So the cells that eventually became the plant lineage already had mitochondria in them before they captured chloroplasts.  ",null,0,cdn40cu,1rf3cf,askscience,top_week,2
botanist2,"No.  The purpose of the chloroplasts is to make the energy needed for respiration, they don't have the ""machinery"" necessary to put the energy in the most usable form like what happens in the mitochondria.  Your question is kind of like asking ""Why can't the gas tank run the car?""",null,2,cdmplym,1rf3cf,askscience,top_week,2
null,null,null,414,cdmn013,1rf2b3,askscience,top_week,1927
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,271,cdmlcrf,1rf2b3,askscience,top_week,1452
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,38,cdmmor1,1rf2b3,askscience,top_week,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,top_week,65
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,11,cdmlptk,1rf2b3,askscience,top_week,52
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,24,cdmrqlw,1rf2b3,askscience,top_week,54
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,8,cdmpnpe,1rf2b3,askscience,top_week,21
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,top_week,10
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,top_week,17
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,17,cdmm72d,1rf2b3,askscience,top_week,23
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,top_week,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,7,cdmqbvb,1rf2b3,askscience,top_week,14
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,top_week,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,0,cdmn10w,1rf2b3,askscience,top_week,5
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,8,cdmpswn,1rf2b3,askscience,top_week,13
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,7,cdmpwee,1rf2b3,askscience,top_week,13
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,top_week,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,0,cdmq96f,1rf2b3,askscience,top_week,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,9,cdmr4on,1rf2b3,askscience,top_week,12
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,top_week,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,top_week,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,8,cdmmecq,1rf2b3,askscience,top_week,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,2,cdmmxgj,1rf2b3,askscience,top_week,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,8,cdmn15l,1rf2b3,askscience,top_week,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,0,cdmn6cx,1rf2b3,askscience,top_week,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,8,cdmngbx,1rf2b3,askscience,top_week,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,8,cdmnh29,1rf2b3,askscience,top_week,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,8,cdmnzr4,1rf2b3,askscience,top_week,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,top_week,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,8,cdmo9dv,1rf2b3,askscience,top_week,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [RayleighBnard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,top_week,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,0,cdmphjt,1rf2b3,askscience,top_week,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,top_week,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,top_week,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,0,cdmtzvb,1rf2b3,askscience,top_week,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,top_week,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,1,cdmw6gb,1rf2b3,askscience,top_week,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,top_week,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,top_week,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,0,cdn5ejj,1rf2b3,askscience,top_week,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,top_week,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,0,cdna4y1,1rf2b3,askscience,top_week,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,top_week,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,9,cdmnnog,1rf2b3,askscience,top_week,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,top_week,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,top_week,1
null,null,null,414,cdmn013,1rf2b3,askscience,top_week,1927
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,271,cdmlcrf,1rf2b3,askscience,top_week,1452
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,38,cdmmor1,1rf2b3,askscience,top_week,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,top_week,65
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,11,cdmlptk,1rf2b3,askscience,top_week,52
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,24,cdmrqlw,1rf2b3,askscience,top_week,54
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,8,cdmpnpe,1rf2b3,askscience,top_week,21
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,top_week,10
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,top_week,17
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,17,cdmm72d,1rf2b3,askscience,top_week,23
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,top_week,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,7,cdmqbvb,1rf2b3,askscience,top_week,14
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,top_week,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,0,cdmn10w,1rf2b3,askscience,top_week,5
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,8,cdmpswn,1rf2b3,askscience,top_week,13
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,7,cdmpwee,1rf2b3,askscience,top_week,13
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,top_week,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,0,cdmq96f,1rf2b3,askscience,top_week,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,9,cdmr4on,1rf2b3,askscience,top_week,12
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,top_week,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,top_week,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,8,cdmmecq,1rf2b3,askscience,top_week,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,2,cdmmxgj,1rf2b3,askscience,top_week,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,8,cdmn15l,1rf2b3,askscience,top_week,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,0,cdmn6cx,1rf2b3,askscience,top_week,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,8,cdmngbx,1rf2b3,askscience,top_week,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,8,cdmnh29,1rf2b3,askscience,top_week,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,8,cdmnzr4,1rf2b3,askscience,top_week,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,top_week,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,8,cdmo9dv,1rf2b3,askscience,top_week,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [RayleighBnard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,top_week,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,0,cdmphjt,1rf2b3,askscience,top_week,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,top_week,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,top_week,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,0,cdmtzvb,1rf2b3,askscience,top_week,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,top_week,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,1,cdmw6gb,1rf2b3,askscience,top_week,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,top_week,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,top_week,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,0,cdn5ejj,1rf2b3,askscience,top_week,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,top_week,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,0,cdna4y1,1rf2b3,askscience,top_week,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,top_week,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,9,cdmnnog,1rf2b3,askscience,top_week,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,top_week,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,top_week,1
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,top_week,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,top_week,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,top_week,13
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,top_week,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,top_week,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,top_week,13
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,26,cdmlm65,1rew42,askscience,top_week,167
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,8,cdmokr1,1rew42,askscience,top_week,20
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,26,cdmlm65,1rew42,askscience,top_week,167
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,8,cdmokr1,1rew42,askscience,top_week,20
bbqbollocks,"Because there are two ways a stm works. Constant current and constant height.

With constant current, the distance between the tip and the sample changes to keep the current flowing through the tip the same. This maps the topography of the surface. 

If the sample is flat enough then you can use the constant height mode. The constant height mode will keep the distance between the tip and sample fixed as it scans across the surface. So if you have 35 xenon atoms writhing range cor quantum tunneling to take place then a current flows where the atoms are. So no nickel atoms can be viewed. This mode looks at the density of states on the surface. ",null,0,cdmlwie,1revqt,askscience,top_week,11
breadmaniowa,"The real reason you feel the need to breathe is because of the carbon dioxide building up in your blood. Taking in oxygen removes the dissolved carbon dioxide from your body. So basically, the real reason you can't hold your breath for very long is that you need to expel the carbon dioxide from your body. You actually have plenty of oxygen still in your blood when you feel the need to breathe.",null,5,cdmlr1p,1revb2,askscience,top_week,12
fazedx,"There are two drivers in the human body that tells it to breathe. The first one is concentration of carbon dioxide in the blood, and the second one (backup, if you will) is the concentration of oxygen.

Carbon dioxide (CO2) is allowed to pass the blood brain barrier. High concentrations of CO2 diffuse into your cerebral spinal fluid (CSF), dissociates into hydrogen ions and lowers your CSF pH. This is picked up by chemoreceptors and signals your central nervous system to increase ventilation. This is your central, or main control of breathing.

Peripheral control is based on pO2 in arterial blood. If it drops below a certain point, it will send signals to your brain to start breathing.

You can reduce the pain from holding your breathe by hyperventilating before you hold your breathe, thus reducing the buildup of acid and the prolonging the time it takes for your brain to signal to you to breathe.

http://www.winona.edu/biology/adam_ip/misc/assignmentfiles/respiratory/Control_of_Respiration.pdf is a good source/summary",null,0,cdmuuor,1revb2,askscience,top_week,3
breadmaniowa,"The real reason you feel the need to breathe is because of the carbon dioxide building up in your blood. Taking in oxygen removes the dissolved carbon dioxide from your body. So basically, the real reason you can't hold your breath for very long is that you need to expel the carbon dioxide from your body. You actually have plenty of oxygen still in your blood when you feel the need to breathe.",null,5,cdmlr1p,1revb2,askscience,top_week,12
fazedx,"There are two drivers in the human body that tells it to breathe. The first one is concentration of carbon dioxide in the blood, and the second one (backup, if you will) is the concentration of oxygen.

Carbon dioxide (CO2) is allowed to pass the blood brain barrier. High concentrations of CO2 diffuse into your cerebral spinal fluid (CSF), dissociates into hydrogen ions and lowers your CSF pH. This is picked up by chemoreceptors and signals your central nervous system to increase ventilation. This is your central, or main control of breathing.

Peripheral control is based on pO2 in arterial blood. If it drops below a certain point, it will send signals to your brain to start breathing.

You can reduce the pain from holding your breathe by hyperventilating before you hold your breathe, thus reducing the buildup of acid and the prolonging the time it takes for your brain to signal to you to breathe.

http://www.winona.edu/biology/adam_ip/misc/assignmentfiles/respiratory/Control_of_Respiration.pdf is a good source/summary",null,0,cdmuuor,1revb2,askscience,top_week,3
paolog,"The ""criss-cross"" distance between two points is called the Manhattan distance between the points, while the straight-line distance is called the Euclidean distance. What you're asking is whether the limit of the Manhattan distance as the grid gets finer is equal to the Euclidean distance. It's easy to show that this is not the case.

Let's take a 1 x 1 square. The Manhattan distance from one corner to the other is 2 (length of bottom edge + length of right edge, for example), while the Euclidean distance is, by Pythogoras' theorem, 2.

Now subdivide the square into a 2 x 2 grid of four squares. To get from one corner to another, we have to zigzag along four edges of the small squares, each of which is 1/2 a unit long. So the total distance is 4 x 1/2, or 2.

It's not hard to show that however we subdivide the square into smaller squares (or even rectangles), the shortest corner-to-corner distance measured along the edges of these squares will always be 2 and will never get anywhere near 2. Hence no matter how many turns we make, the Manhattan distance never equals the Euclidean distance.

So no, nothing changes as the resolution of the grid becomes finer. Furthermore, a diagonal is not imaginary - it is just different from walking along the edges.

EDIT: removed repetition",null,3,cdml7vm,1reu8x,askscience,top_week,21
Professor_Snuggles,"The fundamental point here is this: two curves can be visually similar yet have very different lengths. Imagine a bug taking inch long steps in a long zig-zag across the line. What you're doing is similar to saying that the bug can instead cut closer to the line and decrease the forward distance covered with each zig-zag. This could give a path that stays closer to the original line overall, but has the same length because it wiggles more.

The moral of the story is that curves that stay close to each other do not have to have lengths that stay close to each other. As for real life: yes there is a difference traveled if you take a small step up, then a small step right, etc. compared to directly walking the diagonal. This is easiest to see if you have a robot or something that you can guarantee will travel at a constant speed and a timer. A real life diagonal is not necessarily ""imaginary"", it's just that traveling near it in any way you want is not going to get the same results as traveling *on* it, or other paths that more accurately approximate doing so.",null,1,cdmrs35,1reu8x,askscience,top_week,7
jeff0,"The size of the grid doesn't matter. Say your rectangle is a 1 mile x 1 mile square. The length of the diagonal from A to B is sqrt(2) =~ 1.4 miles. If you instead alternate walking due north with walking due east, you end up walking a total of 1 mile east and 1 mile north = 2 miles total. The size of the grid will only effect the number of times that you turn.

The same idea underlies the [troll math](http://knowyourmeme.com/forums/meme-research/topics/8029-troll-math) meme.",null,0,cdmleqb,1reu8x,askscience,top_week,2
wgunther,"Just to prove the bit more formally instead of showing some examples: if you divide a 1x1 square into an n by n grid then the distance you are traveling in total n*(1/n+1/n); that is, imagining you are in the bottom left corner, you have to go up distance 1/n and right distance 1/n for each subdivision, and there's are n of them. Therefore, the distance you must travel is 2. 

Intuitively it makes sense: all your motion is either purely vertical or purely horizontal. You must move horizontally distance 1 and vertically distance 1. Therefore you must move distance 2. ",null,1,cdmmhao,1reu8x,askscience,top_week,3
ignorant_,"A diagonal line bi-sects a square at 45degrees. The question asked is regarding using horizontal and vertical lines to travel toward the opposite corner. These lines are at 90degrees. Suppose we used intermediate angles. 89 degrees, then 88 degrees, etc., and work our way down toward 45degrees. Wouldn't my distance begin to decrease as the angle approaches 45 degrees, and have a limit of square root of 2?",null,1,cdmw7ub,1reu8x,askscience,top_week,1
yeast_problem,"Quantum Mechanics would bring a limit to this, as the grid size gets smaller the uncertainty principle would mean your momentum could not be zero in the  direction perpendicular to travel. It would become impossible to say that you were actually travelling along the grid lines, at scales around 10^-34 meters.",null,4,cdmvy76,1reu8x,askscience,top_week,1
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,top_week,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,top_week,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,top_week,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,top_week,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,top_week,27
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,top_week,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,top_week,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,top_week,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,top_week,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,top_week,27
wazoheat,"No. Food and drink go bad due to [spoilage](https://en.wikipedia.org/wiki/Food_spoilage), which is usually due to the growth of bacteria and/or fungus, none of which will grow in plain water.",null,0,cdmi5o5,1remei,askscience,top_week,3
ides_of_june,As wazoheat said water doesn't spoil. It's possible that the container that the water is stored in could undergo thermal degradation making the water unfit for consumption (or at least undesirab. Also if the water is stored open to the environment it can become contaminated though in an indoor environment it's unlikely to become unfit for consumption.,null,0,cdmo21b,1remei,askscience,top_week,1
ramk13,"Depending the temperature ranges you could breakdown the disinfectant residual (usually chlorine or monochloramine at ~1 mg/L) that is normally present in tap water. If the residual is gone, then organisms (e.g. algae) are much likely to grow in your water if spores are present. 

Also if the temperature gets high enough you can have interactions between the water and its container. Metals are more likely to corrode and leach, and probably more relevant, plastics can leach plasticizers into the water. I don't know that there are studies that have quantified whether there are documented effects in animals or humans for tap/drinking water stored in plastic bottles, but many people are concerned about it.

The water itself won't spoil.",null,0,cdmrq6r,1remei,askscience,top_week,1
drzowie,"Jovian interference.  The asteroids are near a couple of major resonances with Jupiter; that gives them enough of a nudge to prevent them from coalescing.  (Source:  while I am not a planetary scientist I work in a lab with a passel of 'em).

A bit more: Small-ratio resonance orbits with major bodies typically have nothing in them, because over time the larger body kicks the smaller ones out of that orbit.  Think of pushing a swing, or operating a cyclotron:  you can transfer a *lot* of energy to an oscillating body just by kicking it gently in some pattern with a harmonic relationship to the oscillation.  Major bodies typically clear out their own orbits over time due to the 1:1 resonance with anything else in that orbit -- anything at, say, 0.999 AU would eventually have a near encounter with Earth, and get ejected.  That effect is why Ceres and Pluto are considered ""dwarf planets"" and not ""planets"" -- the dynamical process is part of our modern definition of a planet.  The 2:1 and 4:1 resonances with Jupiter define reasonable approximate boundaries of the asteroid belt, and there are noticeable gaps near small-integer ratios of Jupiter's period between those values.
",null,0,cdmoip2,1remcu,askscience,top_week,3
The_Evil_Within,"Jupiter exerts enough of a disruptive force on the asteroid belt to keep it thinned out - and due to their relative motion, individual asteroids are at least as likely to smash into *more* debris than they are to coalesce into one bigger mass.

At least, so I was informed when I asked this question here a while ago.  The detailed explanation was kind of over my head, as you might expect.

Given that explanation, I still have trouble understanding how Ceres could form in the first place, yet still not be capable of 'finishing' and collecting the remaining mass of the asteroid belt.",null,1,cdmojfi,1remcu,askscience,top_week,3
GumbyTastic,"Well you have to look at it like this. Why does saturn have rings? Why doesn't all that mass floating around it just make a new moon? Usually the mass doesn't have enough force to coldine and make new objects or there's not enough force keeping the mass together. The asteroid belt (don't quote me) like a big ring like saturn. It's just full of rocks and debris that get caught up in the suns gravitational pull. It looses mass and gains mass when new objects are knocked out and sucked in. Correct me if I'm wrong on anything. I enjoy learning and never see much coverage, hell I never see anything about the asteroid belt!!",null,4,cdmj64t,1remcu,askscience,top_week,3
bobbycorwin123,"bah, I cant find any links to the exact reason. I believe its because of the rotations of mars and Jupiter and the way the gravity of the two bodies prevent stuff from gathering too much...

All I remember is that Jupiter and Saturn have a 2/1 rotation ratio...which helps not at all in this",null,6,cdmjcle,1remcu,askscience,top_week,3
I_Gargled_Jarate,"Gravity isnt strong enough to compress asteroids into larger planets. It takes a high velocity collision for asteroids to fuse together. Gravity does play a part by attracting large bodies which may be potentially travelling at very high velocities, but just sitting next to each other is not enough to form larger objects.",null,4,cdmk4yf,1remcu,askscience,top_week,1
svarogteuse,"
$60 is not worth spending on a telescope. You will end up with a very low end wobble device and be disappointed.  Buy a set of binoculars first. If you decide that you aren't that into astronomy later binoculars have other uses a telescope really doesn't. Next go hang out with the local Astronomical society. Look at what they are using get to know their equipment before you make a purchase more than binoculars.
The smallest scope regularly used in our society 10 years ago was an 6"", well above the $60 price tag and the 2-3"" of the ones you mentioned.

15x70s are huge binoculars. You are going to have problems keeping them steady unless you invest in some sort of [mount for them](http://www.telescope.com/Orion-Paragon-Plus-Binocular-Mount-and-Tripod/p/5379.uts).  With those binoculars you will be able to see the moons of Jupiter, the ears on Saturn, maybe Titan if you are in a good spot, and clusters. They aren't really designed to see galaxies except the brightest ones. The standard binoculars used are 10x50s. Light enough to hold steady, or balance on a chair but powerful enough to see binocular objects (bright clusters, comets, birds). We really don't use binoculars for planetary observing not enough detail. I would recommend a set of 10x50s before the 10x70s. I have never owned nor known anyone to own such large binoculars except for special purposes like comet hunting and defiantly not without a mount.

Neither of those scopes are really worth using for more than a causal, hey that's the moon kind of use. I used a 6"" for many years around 2000 and it was the smallest scope in the group. 8"" is a standard entry level amateur scope. What matters in a telescope is aperture the size of the main lens or mirror. The larger the aperture the more light is concentrated onto your eye, the fainter an object can be seen. You want to spend your money on aperture! Magnification doesn't matter, most observing is done with relatively low magnification but the higher aperture the better.  

Long time amateur astronomer (30+ years), previous president local astronomical society. ",null,0,cdmnyge,1reljs,askscience,top_week,2
_NW_,"Having both works better.  Do the binoculars have a tripod mount?  At that kind of magnification, it's going to be difficult to hold steady.  Also, after a few minutes, your arms are going to start getting really tired.  My first telescope was a 60 mm Tasco.  A good pair of binoculars worked better.  Years later, I bought a 6 inch reflector and finally got see all the things that I couldn't with the Tasco.  I have a pair of 10x50 binoculars that I use alone or with the telescope.  When looking for something in the sky, it helps to find it with binoculars first before using the telescope.  Or, sometimes I just go look with the binoculars just because it's easy.  Also, because it's more than enough to see several galaxies, star clusters, nebulas, and Jupiters moons.  Actually, the Andromeda, LMC, SMC, and a few other galaxies are visable even without binoculars.  Stop at a store and pick up a copy of ""Sky and Telescope"" or ""Astronomy"" Magazine.  Both have a star map that shows what you can see for that month.",null,0,cdmzcq2,1reljs,askscience,top_week,2
botanist2,"I do a bit of bird watching and very amateur stargazing, so I have some experience in this issue.  One of the biggest problems with using binoculars for anything like bird watching or stargazing is that your arms aren't very steady, which isn't that much of an issue at lower magnifications (e.g. looking at birds in the tree above you), but is really bad at higher magnifications (e.g. trying to look at ducks way out in the pond).  I would suggest getting a telescope with a tripod because you'll get a lot more stability and you'll be able to see things more clearly as a result.",null,0,cdmmve6,1reljs,askscience,top_week,1
SilentCastHD,"Well, first of all, you have to differentiate subtractive color from additive color.

In the first case, all the colors give you black, in the second, all the colors give you white.

So to make that clear: If you take a flashlight, and shine it onto a white paper, you see white light. - Duh...

If you take a red marker and mark the page, you strip away ""all the light"" that isn't red and absorb it, so only the red light reflects. The dye subtracts the [wavelengths](http://upload.wikimedia.org/wikipedia/commons/c/c4/Rendered_Spectrum.png) that don't correspond to red.

So you transform white light to red light using the filter ""red marker dye"".
Going forward, with blue and yellow, you strip awaay more and more of the light, until no light is relected anymore, leaving you with black.

The other way around, you [add up colored light](http://upload.wikimedia.org/wikipedia/commons/2/28/RGB_illumination.jpg) to make white light.

So you shine red light onto a white wall, the reflected light is red. If you overlay it with the other colors, you'll get white again.

(This is why green looks black in ""pure red light"", since there is [no refelection of red light on green leafs](http://1.bp.blogspot.com/-hHcuVK0TGHg/TyVDCInAFWI/AAAAAAAAAT0/3tU3h7p1Zbw/s1600/Lights%2B1%2B-%2BOriginal.jpg))

So with that out of the way: What is grey?

Grey is the achromatic color between black and white.

So, since you get the two different colour-systems now, you see that grey in [RGB](http://en.wikipedia.org/wiki/RGB) displays (additive color) has to be different from the grey in a printed picture in [CMYK](http://en.wikipedia.org/wiki/CMYK_color_model)(subtractive color)

So, as you can see [here](http://www.aksiom.net/rgb.html) at the bottom the RGB value for grey is always something where R=G=B, and the stronger the individual light gets, the more you go up to white.

I hope this helps :)",null,0,cdmjc3a,1rekov,askscience,top_week,6
LoyalSol,"There isn't really one universal answer since different materials will react differently with acids/bases, but a large majority of them dissolve because of either oxidation like in the case of metals or through catalyzed reactions (the acid/base speeds up a reaction that normally would occur slowly).

Oxidation is pretty straight forward.  The metals have electrons taken away by the acid and once that happens they form stable ions which can be freely dissolved into solution.  In catalytic reactions the acid/base comes in and binds to a functional group on a molecule (usually organic molecules) and stabilizes the molecule in a way that it can undergo further reactions.  

http://www.organic-chemistry.org/namedreactions/fischer-esterification.shtm

That's an example of the forward reaction, but the reverse reaction is similar.    In large scale a organic molecules such as proteins, each peptide in the chain is linked together by an functional ground (amide group for proteins, O=C-N) and the acid/base will attack these links causes the chain to break apart.  Which is why they are generally detrimental to biological organisms. ",null,0,cdmm2e5,1rehln,askscience,top_week,2
NotFreeAdvice,"Answering your second question, glass is often used for two reasons.  First, the Si-O bonds that are the structure of silica compounds (like glass) are relatively inert.  Thus, they do not like to be broken by other compounds/chemicals.  Second, it is amorphous, which adds both strength to the vessel and well as a reduction in reactivity that can occur at the edges of crystal faces.  Hence, the amorphous nature renders the glass less reactive than it would be if it were crystalline silica.  

There are some things that are not good to store in glass, however.  Potassium hydroxide will etch away the glass, and hydrofluoric acid will do the same.  These are just two examples, but there are a number of chemicals that are not inert, with respect to glass. 

Hope that helps!",null,1,cdmmctc,1rehln,askscience,top_week,2
drzowie,"A superadiabatic gradient is what *drives* convection -- the free energy that gets converted to mechanical flow comes from the positive difference between the gradient and the adiabatic lapse rate.  Convection will happen at *some* level with any nonzero excess in the lapse rate above the adiabatic rate, since the material is a fluid.

In practice, the actual lapse rate doesn't get driven exactly to the adiabatic rate, but it's pretty darned close.  The actual offset is driven by the balance between heat flux and (effective turbulent) viscosity in the fluid.  Since stellar plasmas aren't known for their high viscosity, and the scales are large, the offset turns out to miniscule (negligible by orders of magnitude) in nearly all cases -- so you can treat the adiabatic lapse rate as a strict limit, and be good to go.

Let's apply your example of 10^-6 superadiabaticity to the Sun.   [The convection zone spans about 6 orders of magnitude, or about 14 scale heights, in density](http://solarphysics.livingreviews.org/open?pubNo=lrsp-2009-2&amp;amp;page=articlese1.html).  If the lapse rate differs from adiabatic by 1 part in 10^6, that corresponds to a temperature differential factor of e^(14x2/3x1.000001) compared to e^(14x2/3) across the whole convection zone - so if you assumed the lapse rate was exactly adiabatic, but it was really 1+1x10^-6 times the adiabatic rate (and you knew the photospheric temperature exactly), your calculation of the temperature at the base of the convection zone would be off by a factor or (1+1x10^-5).  Other effects (like convective overshoot and dynamo action) enter at the 10^-3 level, so the superadiabaticity is negligible.",null,0,cdmo4bd,1reh81,askscience,top_week,1
bkkgirl,"Well nothing's stopping you from using it except that few people know how to use it, and very little has been translated to it.

Also, people with different accents would _write_ differently. This is critically important in languages such as Chinese, where the differences would render every dialect mutually unintelligible, and somehwat important in languages like English, becuz eugeniks an da lik wud mak ritin litrl spekn had. Written language preserves etymology, whereas the IPA, which would produce different forms for the same word, does not.

Additionally, what is transcribed in the IPA is not entirely uniform, so representations would be ambiguous even among speakers of the same dialect.

Since people usually read by identifying words as a whole, direct transcription of what was said would be counterproductive and difficult to follow, and since that's what the IPA is for, it would be too.

Disclaimer: I can't speak AAVE, so my transliteration is probably shitty as fuck.",null,0,cdmkd36,1refad,askscience,top_week,9
protestor,"A thing about phonetic alphabets is that often two different sounds are interpreted as being the same phoneme in a given language (they are [allophones](http://en.wikipedia.org/wiki/Allophone)), but on a different language they might be distinguished. On a given language the preferred allophone might depend on region, for example. The fact that two sounds may be interchangeable is called [free variation](http://en.wikipedia.org/wiki/Free_variation):

&gt; When phonemes are in free variation, speakers are sometimes strongly aware of the fact (especially where such variation is only visible across a dialectal or sociolectal divide), and will note, for example, that tomato is pronounced differently in British and American English, or that either has two pronunciations which are fairly randomly distributed.

[Each language has its own set of phonemes](http://en.wikipedia.org/wiki/Phoneme#Numbers_of_phonemes_in_different_languages). Some languages don't use tone to distinguish phonemes (but use them for other things), others use a lot.

This kind of non-uniformity may negate any advantage in uniformizing our writing system.

I also find the latin alphabet pretty convenient to type in a keyboard, but the IPA is less so, because it has too much symbols. (also, IPA is sometimes too specific - how to represent a word that we don't know how to pronounce?)

(ps: I suppose you're suggesting we use IPA to substitute alphabets already in use, instead of using IPA just for phonetic transcription)",null,0,cdmqbkr,1refad,askscience,top_week,2
fishify,"Hybrids have both an internal combustion engine and an electric drive system, which enables them to achieve better efficiencies in a few ways. One is that they recapture energy that would otherwise be lost; regenerative breaking allows the energy lost to waste heat in a standard car to instead be used to store energy in the hybrid's batteries. Another is that the internal combustion engine can be smaller, and thus operate more efficiently more of the time, since the electric motor is available for peak demands. In addition, the internal combustion engine can be turned off in situations in which a car is idling.",null,0,cdmfog1,1recy4,askscience,top_week,2
Pachacamac,"Actually you can't carbon date stone at all. Carbon dating needs organic materials with carbon-14 in them (an unstable isotope of carbon), so we need floral or faunal material. Burned seeds or charcoal are the best, but other organic materials can be dated. There are other dating methods that can date non-organic things and can date much older things that radiocarbon dating (which maxes out at 75,000-100,000 years), and some of these are useful to archaeologists/paleo-anthropologists, but radiocarbon dating is the most common method that archaeologists use. 

I'll mention here that there is one method, obsidian hydration dating, that can actually determine how long it's been since a piece of obsidian (volcanic glass commonly used for stone tools) was broken, which happens when the tool is being made (basically you start with a larger rock and chip away at it to shape it into what you want), but this method has a lot of problems and isn't always reliable. It's about the only way to directly date stone tools that I can think of, though.

So, we can't date the actual. How do we determine how old something like a stone tool is? We rely on one of the key assumptions in archaeology that things found together were probably made and used at roughly the same time (radiocarbon dating has an error range of 25-100 years anyway, so ""same time"" can mean same decade or same century). If you find a stone tool within a fire pit, say, then you assume that someone threw it in there during a fire, and the fire pit will have lots of organic material that we can date, So we date the pit and assume that the tool is as old as the pit. That is the most straightforward example I can think of, but the basic idea, dating by association, is how we get specific calendar dates for most of our sites. Same thing if we get a stone tool and a piece of charcoal at roughly the same depth in a site that we know has not been disturbed, you can date it by association.

Edit: just took a look at the article you linked. They've dated those tools to 280,000 years ago, not 85,000 years ago, so they would definitely not be using radiocarbon dating. I don't know what they used. The article is a bit hyperbolic but just keep in mind that, especially with those really early sites, there is a lot of room for error or unknown things complicating the picture, and a ton of room for interpretation, so the big claims that the article makes might be a bit presumptuous. As always, more research is required.",null,0,cdmfnvd,1rectr,askscience,top_week,2
Solivaga,"There's a wide range of radiometric dating techniques, but as /u/Pachacamac points out, you can't use radiocrbon dating on inorganic materials (such as stone), and radiocarbon dating is only really accurate back to around 50k BP, and completely fails much beyond 75k BP.

The short answer is that we use context and stratigraphy to securely sequence artefacts and features - in turn this allows us to identify material as being conteporaneous.  This enables us to date other material that's from the same phase of occupation or activity as the stone tools.

Dating techniques that stretch further back than C14 include Potassium Argon, Uranium Series, Fission Track, Electron Spin Resonance, abd Obsidian Hydration.  The problem with many of these is that they date natural events (including volcanic rock formation, formation of calcium-carbonate etc.), so often we'll be using these dates natural events to constrain the archaeological materials - i.e. we know that this palaeolithic site was occupied sometime between x and y.

",null,0,cdndjg2,1rectr,askscience,top_week,2
Platypuskeeper,"&gt; Hybridization is a generally good theory, but it doesn't explain properties like magnetism.

Valence-bond theory actually explains the paramagnetism of oxygen, if that's what you're referring to. (and has since the start, it's in Pauling's ""The nature of the chemical bond) It's a common myth though, so anyway...

You have antibonding orbitals because of symmetry. Each 'even' (symmetric) state has a corresponding 'odd' (antisymmetric) state. Now, I don't expect you to get what that means, so I'll demonstrate:

Two hydrogen atoms get close, and their atomic 1s orbitals combine to a _molecular orbital_. (The 1s orbitals are spherical and have a wave function that's like exp(-r), if you neglect constants). We assume for the sake of this example, that they form a linear 'superposition'. The combined wave function is simply the sum of the functions times some constants. 

There are only two possible combinations here: which is 1s_1 + 1s_2 and 1s_1 - 1s_2. This is because the overall phase (sign) of the function doesn't matter. so -1s_1 - 1_s2 is the same thing as 1s_1 + 1s_2. 

In the first one 1s_1 + 1s_2, where they add up, then the electron density is above zero everywhere, since the 1s orbital is exp(-1) and above zero everywhere. So there must be electron density all the way between the two nuclei. It's a _bonding_ molecular orbital.

In the second molecular orbital these two can create, 1s_1 - 1s_2, there is a spot at the exact center between the two nuclei where 1s_1 and 1s_2 are the same (because it's the same 1s orbital and the same distance r from their respective nucleus). So the total wave function there is _zero_. There's a region between the nuclei that lacks electrons! This is an _antibonding_ orbital.

[It's easier to see the thing visualized](http://www.expertsmind.com/CMSImages/2087_bonding1.png)

The antibonding MO has higher energy than the bonding one (fortunately for chemistry). A visual rationalization for this is in there's a higher curvature of the antibonding MO. After all, from one nucleus to the other it has to pass through zero. In quantum mechanics, a higher curvature of the wave function (more tightly located electrons), means higher kinetic energy. So the kinetic energy is higher when you have a 'node' like this (nodes being these areas of zero density, as with where a wave is zero). 

All this holds true whichever orbitals you combine to form your MOs. An antibonding orbital is formed for each bonding one, and the antibonding one has higher energy. 

(Note that the 'formation' here, just as with hybridization, is really just a way describing things. MOs don't suddenly form at a particular distance, it's a seamless transition from AOs to MOs)
",null,0,cdmet34,1recfy,askscience,top_week,7
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,top_week,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,top_week,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,top_week,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,top_week,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,top_week,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,top_week,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,top_week,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,top_week,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,top_week,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,top_week,1
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,top_week,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,top_week,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,top_week,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,top_week,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,top_week,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,top_week,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,top_week,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,top_week,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,top_week,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,top_week,1
HV250,"You seem to be confusing voltage with current. Voltage is just the potential difference required for current to flow. How much current actually flows is what determines whether you have enough for all components. As the current is consumed, the voltage slowly dwindles over time, till a point where the potential difference is simply not sufficient to let the charges move. That's when you need to charge it.",null,0,cdmh1b3,1rebi1,askscience,top_week,10
kizzap,"There are a number of things that could be happening. 

First, it would be most likely be connected in *parallel* not in series, thus the processor will be getting the 3.7V as well. LEDs take such small current too that a single LED will run for quite some time off that battery.

Secondly, it is quite possible that there is a switching power supply in the controller, which changes a lot of things.

Third, not all LEDs are 3 volts... ",null,0,cdmicso,1rebi1,askscience,top_week,3
iorgfeflkd,"Yeah, for example a red dwarf orbiting a much brighter star. When the dwarf is transiting, there will be less total light coming from the system.

[Here](http://arxiv.org/pdf/1109.2055.pdf) is a paper where they tried to measure this loss of light from a red dwarf orbiting another star.",null,1,cdmfm8v,1reb7p,askscience,top_week,7
humanino,"You can access the article here :  
[Thermoelectrically Pumped Light-Emitting Diodes Operating above Unity Efficiency (pdf)](http://dspace.mit.edu/openaccess-disseminate/1721.1/71563)

Please note that they have not broken any thermodynamical law. They have a device which uses electrical power, and converts this power into heat and light. The power emitted in the form of light is larger than than the part of the electrical power directly used to create light. That is because the other part of the electrical power, which created heat, has also been re-converted into more light. That is really neat and clever, and it does have potential applications, but the ""communication"" part might have been misleading. ",null,0,cdnyxu0,1reb7j,askscience,top_week,2
rileymanrr,"Absolutely. This is the most basic idea of materials science. You asked specifically about an alloy of gold and tungsten, and off the bat I wouldn't know what to make of it. There are equations and general rules to help predict what kind of crystal structure, density, conductivity (thermal and electrical), and phases may be present for different materials.


The first place I would start is the gold/tungsten phase diagram and find out what percentage I had of each, so I could figure out the different phases present. Next thing I would do is grind it into a powder and throw it into an X-Ray Diffractometer, and see what it makes of it. This would give me more information on it's crystal structure.


Next up would be mechanical testing. I would use a Charpy impact test a several temperatures to try and determine its glassy transition temperature, if possible. Then a good old fashion tensile test for its elastic modulus and a compression test for its bulk modulus. 




As someone studying materials science I would guess that a gold tungsten alloy would have good thermal and electrical conductivity, but it's mechanical properties are a bit up in the air. The only thing I would feel confident in saying is that it would probably be very dense.",null,0,cdmq26o,1re8v6,askscience,top_week,2
stevenstevenstevenst,"One way it is possible to determine age of a material vs. when a tool of that material was crafted is to compare the age of the material (easily determined by any number of techniques, such a radiocarbon dating and other isotopic methods) and to compare the quantity of atmospheric carbon adsorbed to the surface of the tool.  Quantity of adsorbed surface carbon (also known as adventitious carbon) is proportional to the amount of time the surface has been exposed to atmospheric conditions, and thus a comparison of adventitious carbon quantity of a surface known to have been exposed in the manufacture of the instrument and the isotopically-determined age of the material is informative.  

Other techniques are possible, but various analysis of oxidation, carbon adsorption, or other surface chemical phenomena are generally utilized.",null,0,cdmdtae,1re8mg,askscience,top_week,2
jessickofya,"To date when the tool was used we would look at residue on the tool and date that. So for example, if we found a stone tool with blood we can use dating techniques to get a estimation on when the tool was used. Depending on what material you are dating - you would use one of many different techniques.

There are also ways to break down rock into a gas and estimate the age of formation. Archaeology is all about context too. If we found the tool with a hearth or camp we could look at dating other items and estimate the age of when the artifacts were used based on the dates of surrounding artifacts in the same area. We can even use tree rings, dendrochronology,  to estimate the age of the wood used in the site and assume the age would be similar",null,0,cdmdxqu,1re8mg,askscience,top_week,2
Pachacamac,"Someone else just asked a pretty similar question and I saw theirs first, so I answered it first, and left a pretty detailed response. [You should probably just take a look at it.](http://www.reddit.com/r/askscience/comments/1rectr/how_do_scientistsarchaeologists_carbondate_human/cdmfnvd)

Basically, with most types of stone we can't date the stone at all (so we don't know how old it is, expect by talking to geologists who tell us that it comes from a certain formation of a certain age. But we don't typically care about that). We figure out when the tool was made by assuming that it was made, used, and discarded within a relatively short period of time (a century can be ""short"" to us because of the error ranges that all the different dating methods have, but stone tools wear out and break quickly so anything was probably used and tossed away in the same year that it was made). So because it was discarded at a site, we assume that it is as old as the site itself, and we date the tool by association; i.e. it was found with other things that we can date directly (like charcoal on younger sites, or layer of volcanic ash for sites as old as the one in this article), so we assume that it is as old as those things. So the fact that the rock itself might be 400 million years old doesn't matter; we find a tool at a site that we can date to 280,000 years ago and we assume that the tool and the site are the same age, as long as there is no evidence to suggest otherwise.

Now, I said with most stone. Obsidian is different. There's a method called obsidian hydration dating that we can actually use to date obsidian tools, which are what was found at the site in the article. When you make a stone tool you are always chipping away and breaking the surface, so when the tool is brand new it will have a fresh surface. Obsidian weathers at a known rate so you can look at the surface and determine how old it is by how much weathering is on it. This isn't a perfect method and it can't really tell us exactly how old the tool is (because there's so much variation across regions), but it can tell you that one tool is older than the other. Maybe they can get actual calendar years for Ethiopian obsidian too, I don't know (I'm not familiar with the area).",null,0,cdmfyqt,1re8mg,askscience,top_week,2
StringOfLights,"It is not so much that terrestrial mammals were big back then, it's that they're small now. Mammals [increased in size following the Cretaceous-Paleogene extinction and maintained that large body size](http://www.sciencemag.org/content/330/6008/1216.short) for nearly 30 million years years.  Then there was an [extinction at the end of the Pleistocene](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.1991.tb01149.x/abstract). Most vertebrate taxa made it through this extinction, but a lot of large-bodied animals, and especially large-bodied mammals, were hit particularly hard. Some 150 genera of megafauna (defined as animals &gt;44 kg) existed 50,000 years ago; [97 of those were extinct by 10,000 years ago](http://www.sciencemag.org/content/306/5693/70.full):

Given how geologically recent these extinctions are, it's extremely unlikely that anything would have been able to fill the gaps left by the loss of megafaunal mammals, as there appears to be a [maximum rate](http://pnas.org/content/early/2012/01/26/1120774109.abstract) that mammals can increase in size. In that sense it's completely expected that a recent extinction event would leave a gap in body size. 

Also, in all of this discussion it's worth bearing in mind that we're generally talking about terrestrial mammals. There are plenty of large marine mammals still around (for the time being), including the blue whale!

**Edit:** Forgot something! In terms of dealing with cold weather, having a larger body size actually slows heat loss because it lowers the surface area to volume ratio. So while larger mammals had to eat more overall, they [spend less energy per unit of body mass](http://www.planta.cn/forum/files_planta/511_131.pdf) producing heat. This was the original logic behind [Bergmann's Rule](http://en.wikipedia.org/wiki/Bergmann%27s_rule).

",null,1,cdmdkaq,1re5lq,askscience,top_week,9
KarlOskar12,"That depends what you mean...The major regulators of the cell cycle are [p53 and p27](http://puu.sh/5sB2h.jpg). They both halt the cell cycle, p27 specifically does it by binding to and blocking the action of cyclin and CDK preventing the cell from entering the S phase of the cell cycle (DNA replication phase). Once the cell cycle is halted, the cell is either repaired (let's say for DNA damage). If repair is not possible or too costly, the cell is told to undergo apoptosis (kill itself). This is done by activating [Caspase 3](http://en.wikipedia.org/wiki/Caspase_3) which systematically breaks down the cell by expelling all the water, chopping the DNA up in an orderly manner, degrading the nuclear membrane, degrading the golgi apparatus, blebbing the cytoplasmic membrane, etc.",null,0,cdmen5i,1re5ig,askscience,top_week,2
Platypuskeeper,"The electromagnetic field. It's everywhere.

Somebody is inevitably going to chime in here with virtual particles and whatnot, which are quantum-level descriptions of _how the field works_. But at the end of the day, the 'medium' is the same: Space itself.
",null,1,cdmdzfk,1re5f5,askscience,top_week,11
fishify,"Not every wave needs a medium other than the vacuum in which to travel. Nineteenth century physicists did not recognize this, and thus postulated that the universe was filled with a substance they called *the ether*, which would serve as the medium for light waves.

Einstein in 1905 showed there was no need for an ether. As we understand it today, light travels through space just as an electron does. One way to picture this is to remember that light is made of photons (particle of light), which readily travel through space and which form electromagnetic fields and waves.",null,1,cdmfxew,1re5f5,askscience,top_week,10
killer_alien,"Light is an electromagnetic wave, and therefore does not require a medim to propagate through. On the other hand, waves that need a medium are mechanical waves. Theses include longitidinal, transverse and torsional waves. e.g. sound waves are longitudinal waves",null,0,cdmi1mi,1re5f5,askscience,top_week,2
animationb,"When a field gets enough energy, it ""manifests"" as some fundamental particle. For the electromagnetic wave, energy creates a photon. In sort of the same way matter helps ""create"" (or comes with) a gluon, the fundamental particle for gravity.",null,2,cdmng0x,1re5f5,askscience,top_week,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,top_week,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,top_week,45
null,null,null,13,cdmjqhe,1re305,askscience,top_week,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,top_week,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,top_week,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,top_week,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,top_week,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,top_week,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,top_week,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,top_week,45
null,null,null,13,cdmjqhe,1re305,askscience,top_week,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,top_week,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,top_week,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,top_week,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,top_week,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,top_week,1
eliareyouserious,"A presynaptic (fibre) volley can be observed in extracellular field potential recordings. It is caused by activation of (several) presynaptic fibres (usually using a stimulation electrode), which in turn fire and activate their postsynaptic partner. A brief negative potential preceding EPSPs is indicative of presynaptic action potential(s) and is termed the ""presynaptic volley"". Fig.2C on page 92 in this book indicates the volley in a recording: http://books.google.ch/books?id=y_ucmaDffXsC&amp;dq=presynaptic+volley&amp;hl=de&amp;source=gbs_navlinks_s (The link to the book chapter also serves as reference here). ",null,0,cdnryhk,1re1qb,askscience,top_week,1
fishify,"Depending on your background, this article might be helpful to you:

""The Pumping of a Swing from the Standing Position."" William B. Case, American Journal of Physics, 64, 215 (1996).


",null,1,cdmg0m4,1re182,askscience,top_week,3
Shitler,"As I understand it, motion happens because the swinger shifts their center of gravity, causing gravity to have to recenter the pendulum. However, as is in the nature of pendulums, gravity overdoes it and the swinger ends up on the other side of equilibrium, at which point they shift their center of gravity again. And so on.

Energy is introduced into the pendulum when the swinger shifts their center of gravity by extending or contracting their legs.",null,1,cdmgk9l,1re182,askscience,top_week,3
jofwu,"I'm just going to describe the process...

When swinging forward you lean back, stick your legs out, and pull on the chains. *By leaning your torso back and kicking your legs out you apply torque to your body.* This torque is balanced by pulling on the chains. Imagine trying to perform this action without holding on to or pushing off of something- you can't. Note that the chains bend where you hold them. The line of action of the tension in the chains is *behind* your center of mass. This is where the balance in torque comes from: force (tension in chains) x distance (between force's line of action and your center of mass). On the backswing, everything is the opposite. You pull your torso forward and bend your knees back in, and to balance this out you need a torque in the opposite direction. So you *push* forward on the chains, and the line of action of the chains is *in front* of your center of mass.

*Making these transitions leading up to the peak of your swing is the key.* The movements don't do anything if they aren't timed right. By performing the forward swinging motions, you add some gravitational potential energy at the top of the front of your swing. The back swinging motions add energy at the back end of the swing. *The energy gained is thanks to that little distance you create between you and the chains' line of action- putting you a little bit higher from the ground than if you had just swung freely like a pendulum.* Of course this gravitational potential energy results in more speed/momentum at the bottom. *And I think it's worth mentioning that you don't conjure this extra energy from nowhere. It comes from you body.* The gravitational potential energy you add wouldn't be possible without applying a torque to your body. 

In the end, it's not that much different from swinging on parallel bars. Rather than balancing your torque by pushing/pulling on a chain, you apply a counter torque directly to the bar you hold (with a firm grip).",null,0,cdmmc2l,1re182,askscience,top_week,2
GlowInTheDarkDonkey,"My understanding, as a (uh oh) layman, is that a person on a swing is basically taking advantage of angular momentum in the same way a figure skater tightening their limbs in a spin makes them spin faster.

A shortening of the total length of the swinging body on the upward swing means gravity is being applied to a total body that has less distance to travel (is a shorter swing-arm), and then on the downward swing the thrusting of legs outwards allows gravity to work on a longer swinging body... which again is then shortened on the upward movement.

Some of the angular momentum of the legs themselves also adds to the total forces being shifted around.

When someone is standing on a swing seat you'll notice they put all of their mass to the seat on the down-swing, and then they stand on the upswing.  This, similarly, means gravity is pulling a longer swing-arm (in terms of average mass distribution towards the outermost edge of the arm) on the downward stroke compared to the upward stroke.

I'm curious if someone in a white-coat finds this answer agreeable or not.",null,6,cdmdqxr,1re182,askscience,top_week,5
fishify,"The energy of the initial and final states in beta decay, as in other processes, have the same energy. The W boson that appeas as an intermediate particle in the standard desecription of the process is a so-called virtual particle. In particle physics, our calculational scheme known as perturbation theory tells us that we can calculate what happens using intermediate states known as virtual particles which have the energy, momentum, and other conserved quantities you'd expect; but this also means they have the 'wrong' mass.  We say they are *off mass-shell*.

These virtual particle that appear in calculations are never actually observed. Any W boson you actually detect will have the expected mass of 80.4 GeV/c^(2), or just under 86 proton masses.",null,1,cdmfttr,1re0d7,askscience,top_week,3
cass314,"In both beer and soda, the bubbles are caused by carbon dioxide coming out of solution.  The big difference is what's there to ""catch"" the bubbles and hold them.  In soda, there's not much at all.  In beer, there are proteins.

Soda is mostly water, sugar, salt, and acid.  There's not a lot to give structure, so the bubbles die out quickly, and after a few minutes you can hardly tell there was ever any foam.  Beer, however, has proteins leftover from both the mash (wheat or barley, usually) and the yeasts that did the fermenting, and it's the proteins that give beer such an interesting head. Proteins, especially hydrophobic proteins (they ""like"" oil better than water) and denatured proteins with their inner hydrophobic parts exposed, tend to clump together into structures (many to avoid interacting with water).  These structures can trap air bubbles.  

You can think of it like a less extreme example of whisking sugar water vs. whisking sugar and egg whites.  If you whisk or shake water, you'll get bubbles, but they'll pop very quickly after you stop.  If you whisk egg whites long enough, you'll get meringue.  ",null,0,cdmczfs,1rdy9r,askscience,top_week,3
Truck43,"The lighter works because the butane is a liquid under pressure, opening the valve lets it spray out and be ignited by the flint. When it's very cold, it contracts, reducing the pressure in the fuel vessel, and it's less volatile, this reduces the amount of fuel that is expelled.  ",null,3,cdm9hep,1rduf4,askscience,top_week,8
adlermann,butane's(what bic lighters use for fuel) vapor pressure drops to near zero at atmospheric pressure about 40F not enough gas is released to fuel a flame.  That is why natural gas and propane are used for heating despite butane's higher energy potential,null,2,cdm9ieg,1rduf4,askscience,top_week,4
Platypuskeeper,"&gt; I'm assuming the lighter fluid has less energy therefore it's lazy.

That's one way of putting. A more formal but roughly equivalent way would be to say that the pressure over the liquid butane in it, is lower when at a lower temperature. The equilibrium is shifted towards more liquid and lower pressure at lower temperatures, higher pressure and less liquid at higher temperatures. 
",null,2,cdmae2f,1rduf4,askscience,top_week,4
Platypuskeeper,"Sea salt is from evaporating seawater, table salt either comes from the sea or from salt mines. 

When you say ""table salt"" you're referring to one single compound: Sodium chloride. The vast bulk of the sea salt, and virtually all of what's 'table salt' is sodium chloride. Sea salt has some other salts in it, how much and what depends on where it's from. Table salt is often [iodized](http://en.wikipedia.org/wiki/Iodised_salt), meaning they've added some iodine salts as a dietary supplement. (Lack of iodine causes developmental disorders and thyroid problems) Depending on the salt it might have small amounts of stuff to avoid it caking together too, which aren't usually added to the stuff marketed as 'sea salt'. (I don't believe there's any _requirements_ on this though)

So sea salt has some other minerals in it, but it's such a small part and you use so little salt, that it probably doesn't make a significant impact on your overall mineral intake. The iodization of salt has had a measurable impact on iodine deficiency-related stuff since it was introduced in the 1920s. For the individual any health effects would depend on whether you get enough iodine from other sources. 

The biggest differences are really taste and texture more than anything, though.
",null,2,cdm93o8,1rdu42,askscience,top_week,6
225274,"Sea salt is the salt produced by evaporating sea water. Table salt is the same thing, just crushed into a fine powder, with fewer impurities of other salts like KCl, and is often iodized, i.e. has added iodine salts. 

Table salt is healthier as iodine is not so commonly available in our diet, but is an essential mineral. 

",null,2,cdm8et4,1rdu42,askscience,top_week,4
chuck10470,"The difference between table and sea salt is the iodine. Fresh from the factory it contains 50 ppm iodine. That's all. 50 ppm. As it ages, the iodine evaporates out, losing half every 40 days.

All salt comes from the sea. Or a lake. Mined salt has precipitated out over thousands of years and built up thick beds that became buried through mountain building. Ironically, most of these mines are today quite some distance from the ocean. The Swiss city of Salzburg has several salt mines,  though it is hundreds of miles from the ocean today. 

And some of this mined salt is quite old. But whether it precipitated out at the bottom of the Tethys Sea 65 million years ago or last week in Sardinia, it's still 99.9% NaCl and 0.1% other minerals. Most of the table salt, industrial salt, road salt, animal feed salt, etc, is mined. Sea salt is precipitated out in huge evaporation ponds. It should be noted that sea salt can be iodized and become table salt, and much of it is. ""Sea salt"" is a marketing name given to un-iodized salt produced by evaporation. It supposedly has better taste, but since it's nearly impossible to determine which minerals give it a specific favor profile, maybe it does, maybe it doesn't. It depends on where it's from. The expensive $10/lb culinary salt is usually sea salt, but with additives like smoke or truffles. The various types of salt available at the grocery store differ mostly in the iodine content and the shape of the crystals. That's it. You can use regular iodized table salt for nearly every application you have,  except for canning. The iodine turns some stuff brown.",null,0,cdmf52z,1rdu42,askscience,top_week,2
Smoothened,"The machinery behind X-inactivation specifically targets the X chromosome as opposed to any chromosome. For example, the gene XIST is located on the X chromosome and is required for its inactivation. When this gene is expressed, its transcript, a long noncoding RNA (Xist) coats the respective chromosome, becoming involved in its silencing. A chromosome lacking XIST would not undergo inactivation. If you insert the XIST gene in an autosomal chromosome, that chromosome can then be inactivated. 

A more interesting question is how is inactivation targeted to one of the chromosomes in each cell. That question is not entirely answered, but it is believed that an autosomal gene encodes a blocking factor that prevents one X chromosomes from being inactivated. Interestingly, even when there's more than 2 X-chromosomes present, only one remains active in each cell. ",null,0,cdmbqmg,1rdsv9,askscience,top_week,5
ProfEntropy,"Postmortem fluid and tissue toxicology is able to quantify both the drugs and alcohol present at the time the samples were taken.

Connecting that back to the amounts present at the time of death can sometimes be difficult. For example, many drugs are known to partition into different parts of the body after death. Knowledge of this, and sampling tissues and fluids from the proper place will help get more accurate measurements.

Many other factors must be considered when looking at ethanol concentration. See [this article](http://www.sciencedirect.com/science/article/pii/S0379073806002891) for a good review of postmortem alcohol concentrations and how they relate to BAC at time of death.",null,2,cdm82ym,1rdosg,askscience,top_week,8
fastparticles,The event would melt most of Earth and put the upper mantle into orbit around Earth. At this point the moon is thought to come from Earth because they are so isotopically similar. The compositions of the moon and Earth do differ especially in terms of volatile elements (the moon for example is relatively depleted in potassium). ,null,0,cdm6y13,1rdhkw,askscience,top_week,2
Ejb90,"This revolves around the maximum power transfer theorem. There are two ways to look at it.
Firstly from a circuit-theory point of view, when power energy is transferred from one component to another, the maximum is transferred if they are the same resistance. Impedance is the more generalised, complex form of resistance.  This means that if they are matched in resistance (impedance) then the most power is transferred, which is most efficient.
The second way to look at isn't is from a waves point of view. At the frequencies you get in a transfer cable the currents can be modelled as electromagnetic pulses. When they reach a boundary some are reflected and some are transmitted, just like light is when it passes between air and water. When the two mediums either side of the boundary have the same ""resistance"" to the wave, more of the wave propagates through, as it's almost as if there is no boundary, so the maximum energy is transferred, as expected.",null,0,cdm4hjj,1rdd6o,askscience,top_week,8
SwedishBoatlover,"You should *really* watch this [video](http://youtu.be/DovunOxlY1k) from Bell labs, where the host use a wave machine to visually show how waves work. You can actually get an intuitive feel for what the impedence matching does, it's very interesting!",null,0,cdm9s27,1rdd6o,askscience,top_week,2
rat_poison,"This is the distilled wisdom of my Microwave Networks experience regarding impedance.

The most important defining feature of the transmission line is its characteristic impedance. It is affected by the shape of the transmission line and materials that make it up. Generally, for a TL extending to the z direction, we can divide the whole length in small parts of length z. hose parts can be arbitrarily small: so much so that we can ignore any radiating properties within that z. We can therefore make a lumped-circuit equivalent of that z length of the tramsission line.
Movement along the length of the transmission line will mean some ohmic resistance (R) and some inductance (L). The neighboring of metal surfaces will cause capacitance (C) and the material between them will cause dielectric losses based on a conductance (G).

As the electromagnetic wave travels through the transmission line along direction of propagation J, we can generally define functions I(z) = I+(z) + I-(z) and V(z) = V+(z) + V-(z)

So current and voltage are made up of two constituents: the first (+) representing movement along the direction of propagation and the second (-) representing the part of the current and voltage that are caused by reflection and therefore are moving opposite the direction of propagation.

Characteristic Impedance is the ratio Z_0(z) = V+(z) / I+(z)

for the length of z I have described earlier, it is calculated as Z_0 = sqrt((R+jL)/G+jC)) (j = sqrt(-1))

In most cases, trasmission lines are uniform in the z direction, or they are made up of a cascade of uniform parts. Either way, for every uniform TL, the characteristic impedance is the same no matter which z I choose within it (as long as it's small), so that's why it's such a defining property of a TL.

We can then define another quantity, (z)=V-(z)/V+(z), called the reflection coefficient. This tells us what is the ratio of reflected and forward waves. Its amplitude is 0 if there are no reflections, 1 if the reflections and the forward waves have the same energy therefore leading to standing waves not able to propagate energy, or an-inbetween state for the other values in between.

If z=l, then we have calculated that _Load = (_L-Z_0)/(_L+Z_0). For a lossless transmission line, this will have constant amplitude throughout its length.

Now we want to minimize energy lost in reflections. So  should be 0.

If you look closely at my last equation, you'll see that this can only be true if Z_L = Z_0.

Regarding the part about the return line.

When dealing with high frequency circuits, a return line is not necessary. BUT NOT because of impedance matching. If the outer shell or the inner wire of a coaxial cable didn't exist, it wouldn't posses the geometrical properties that induce the field to operate in the desired way. There wouldn't be two points with different potentials along the direction of propagation around which the EM energy could oscillate back and forth. In fact the concepts I have just described break down. BUT there are transmission lines that don't have a return wire: these are waveguides. The wave DOES oscillate back and forth, but the points are not as strictly defined as in two-wire TL's or coaxial cables. Instead, we have modes: depending on the ratio of the wavelength and the dimensions of the waveguide, there are (possibly several) nulls and peaks at the transversal plane, that are defined by how many half-waves fit into that dimension. These nulls and peaks are now the places around which the energy fluctuates in order to propagate forward. Waveguides are the reason you should be careful when using current and voltage concepts on microwave circuits. Therefore you should just stop thinking about TLs in terms of a phase line and a return line, but a single structure which guides the waves along a direction and (maybe) causes reflections along the opposite direction.
",null,0,cdmbm4q,1rdd6o,askscience,top_week,1
selfification,"http://www.youtube.com/watch?v=DovunOxlY1k

This is a classic that explains all phenomena.  Standing waves, interference, impedance matching, refraction, reflection...  everything.  All in one video.",null,0,cdmcdgz,1rdd6o,askscience,top_week,1
s3c7i0n,"As a basic reply, dogs, like cats, have a reflective coating at the back of their eye, which helps them see in low light situations. The color of the coating is based on the color of the eye, which has some evolutionary benefits having to do with common colors in various environments, but the gist of It is that the colors are caused by the iris colors. 

(edit) the blue eye is red due to a lack of pigment in the reflective layer, so you're actually seeing the reflection of the blood vessels in her retina. ",null,1,cdm3dji,1rdd4v,askscience,top_week,3
owaisofspades,"Your thyroid hormone is responsible for regulating the metabolic rate of most of your body. When you have thyroid insufficiency, your metabolic rate drops and your body no longer functions at full effectiveness. The concentration problems are likely a secondary effect of the lethargy and weakness that are caused by hypothyroidism",null,0,cdmb0lv,1rdckt,askscience,top_week,2
iorgfeflkd,"Beta decay involves a neutron turning into a proton and emitting an electron (beta particle) and an antineutrino. Static electricity involves movement of pre-existing electrons. Nuclear reactions generally involve much higher energies than electronic or atomic. For example, beta particles from potassium decay in bananas have as much energy as if they went through a 1.5 million volt potential, and static discharge is typically in the thousands. However, static discharged generally involves a lot more electrons compared to most radioactive sources.",null,1,cdm28la,1rd953,askscience,top_week,7
zalaesseo,"When Benjamin Franklin said that Charge can only be collected and lost, he really meant it. When you discharge electricity, electrons just moves to the metal object.

Until beta decay. Beta decay literally creates a new proton electron pair and an antineutrino^irrelevant. You're not collecting charges, you're MAKING charges appear from nothing.   ",null,0,cdm2ovh,1rd953,askscience,top_week,3
cosmicosmo4,"When you get a static shock, you're typically experiencing millions-billions of electrons being transferred over thousands or tens of thousands of volts, and they're only doing that because the recipient object (be it you or the metal railing) has a positive charge, meaning there are places for those electrons to settle once they get there.

When a beta particle is emitted, it comes with an energy in the range of millions of volts, and there's no predesignated spot for it to settle, meaning it will fly straight through things until it happens to find a spot to settle, often by displacing some other electron. This is what makes it ionizing radiation.",null,1,cdm357l,1rd953,askscience,top_week,4
iorgfeflkd,"Protons and neutrons are held together by the strong nuclear force (or a residual form of it, sort of the equivalent of van der Waals forces for nucleons), which in stable nuclei is much stronger than the electrostatic repulsion between protons. If a nucleus has too few neutrons then the repulsion will break it up.",null,1,cdm1vxx,1rd8yh,askscience,top_week,5
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,top_week,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,top_week,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,top_week,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,top_week,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,top_week,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,top_week,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,top_week,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,top_week,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,top_week,13
null,null,null,9,cdm64ib,1rd8ip,askscience,top_week,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,top_week,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,top_week,1
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,top_week,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,top_week,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,top_week,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,top_week,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,top_week,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,top_week,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,top_week,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,top_week,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,top_week,13
null,null,null,9,cdm64ib,1rd8ip,askscience,top_week,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,top_week,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,top_week,1
brawnkowsky,"different ethnicities will have different genes that express proteins differently.  For example, [degrees of lactose intolerance vary between regions, from 5% in north europe to 90% in some african and asian countries](http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0100-879X2007001100004&amp;lng=en&amp;nrm=iso&amp;tlng=en).  this is simply because of altered protein expression (lactase in this example), which is a factor in all protein expression in our body.  also, people will have varying levels of gut microorganisms depending on their environment, what they eat, and their own immune strength; this natural flora is important in digestion.

",null,0,cdnfwoy,1rd803,askscience,top_week,2
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,top_week,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,top_week,16
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,top_week,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,top_week,16
patchgrabber,"Well, kelp are basically algae, so they are quite different from land plants in pretty much every way except photosynthesis. Although their holdfasts resemble, and may be a primitive form of, plant roots, kelp are fundamentally different. 

In most land plants, while very limited photosynthesis may occur in the stalk of the plant, most of its photosynthetic activity is in the leaves. Kelp, in contrast, photosynthesize in every part of the organism (although different parts have different levels of photosynthetic ability depending on age), allowing for more and making light less of a limitation than it is in land plants.

The environment the kelp lives in is also a big factor. Since it is under water, light is attenuated differently than above water. Due to the large amount of particulates, blue light is attenuated rapidly in coastal waters, and blue light is much more valuable than red light that can penetrate deeper at a higher intensity.

While there are products out there that purport to use kelp in them to make plants grow faster, I'm thinking this is only because of the nutrients, not any special property that is linked to kelp growth. I cannot think of any way at present to genetically transfer this quality to land plants; their limitations are different, their environments are different, and they are just fundamentally different organisms. Kelp would be much better used as fertilizer, as you suggest, than as a source of genetic information, although in the future that may change.",null,0,cdmahyq,1rd7fs,askscience,top_week,2
MarineLife42,"As /u/patchgrabber said, Algae are very different from plants.  
Here, the main difference is that (higher) plants grow, i.e. create new tissue only at specific regions on their body. Usually this is at the tip of the plant or leaf, the apex. In grasses (grains) it happens at the nodes too.  
Kelp, on the other hand, creates new tissue along the entire length of its thallus (the big leaf) which is why in grows so fast.  
Another big difference is that the thallus doesn't have much, if any speciation; it is composed of more or less the same kind of cell. Higher plants, in contrast, have an internal structure of xylem, phloem, bark etc. that requires many different specialized cell types.  
Both these differences work together to prevent us from simply transplanting this ability into our crop plants. 
BTW - some bamboo species can also grow very fast, up to 10cm a day or so but there is trickery involved. In fact the plant tissue has been created at the usual speed beforehand, but compressed. During the elongation phase, the plant sucks up much water and fills the cells so it telescopes upwards. ",null,0,cdn4dss,1rd7fs,askscience,top_week,1
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,top_week,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,top_week,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,top_week,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,top_week,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,top_week,6
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,top_week,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,top_week,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,top_week,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,top_week,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,top_week,6
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,top_week,43
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,10,cdmh86v,1rd6th,askscience,top_week,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,top_week,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,top_week,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,top_week,1
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,top_week,43
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,10,cdmh86v,1rd6th,askscience,top_week,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,top_week,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,top_week,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,top_week,1
incognegro76,"You can graphically illustrate a line with this equation but it will form an asymptote very rapidly to zero.

y = 2^-x",null,1,cdm6f0c,1rd6ok,askscience,top_week,3
rlee89,"y=-ln(1-t)/ln(2) seem a good place to look.

For a runner running at velocity 1, y is the number of terms you have added to get the runner's distance at time t.  It is rather obvious that no matter how many terms you add, you will never reach the runners distance for any time after t=1.

This is derived from the closed form of the partial sum 1/2 +1/4 + 1/8 ... 1/2^n = 1-2^(-n).",null,0,cdmam7a,1rd6ok,askscience,top_week,2
ultimatety,"The answer to this is actually more complicated than you would think.  It all boils down to the fact that the surface layer of the ice underneath the object is partially melted.  However, the reason for how this top layer melts is somewhat of a scientific controversy.  People used to believe that the pressure exerted causes the ice to melt, however, this appears to be false.  
The two current theories are that: 
1) The friction of the moving object causes the top layer of the ice to melt
or 2) The top layer of water molecules are unable to bind correctly to the layers underneath and thus stay in a quasi water-like state.

TL;DR There is a little bit of liquid water on top of that ice, and liquid on top of something smooth makes it slippery.",null,3,cdm1crb,1rd6cm,askscience,top_week,19
ace425,"Adding on to this, why doesn't waters ability to form hydrogen bonds affect the slipperiness of ice? It seems like since water likes to form hydrogen bonds that ice would not be slippery but instead have a lot of traction, but this obviously isn't the case. Can someone expand on this please?",null,1,cdmjimu,1rd6cm,askscience,top_week,1
sharp12180,"When you step on ice, you apply pressure to the ice directly below you. This pressure decreases the freezing point of ice and so there is a thin layer of liquid water formed between your feet and the ice. Its this difference that causes ice to so slippery.
http://www.youtube.com/watch?v=Stx6kLd9dYI",null,20,cdm153f,1rd6cm,askscience,top_week,6
rupert1920,"Plastics are long polymers, and can undergo [polymer crystallization](http://en.wikipedia.org/wiki/Polymer_crystallization) when stressed. It is the formation of these ordered structures that causes scattering in the material - which is why it looks white.

In some plastics this process can be reversed by heating the plastic (for example, boiling it in water for a few minutes).",null,1,cdm8ncc,1rd67w,askscience,top_week,5
bohr_exciton,"The most probable explanation is that by bending the material you are creating defects, i.e. inhomogeneities in structure, density, etc. These defect sites can then act as scattering centers, which in turn reduces the transparency. This is a similar effect to scratching the surface of ice, for example.",null,1,cdm3rsa,1rd67w,askscience,top_week,4
iorgfeflkd,"tl;dr: If the laws of physics don't depend on location, momentum is conserved.

Noether's theorem says that for every symmetry in a process, there is a conserved quantity. For things that are translationally symmetric, that conserved quantity is momentum. This means that if you consider a collision on a highway, and then the same collision a couple of miles down the highway (translation), if they behave the same (where on the highway it is didn't matter), then momentum is conserved.

That's fairly complicated, another but less rigorous way of looking at it is that momentum changes when a force is applied, and if no force is applied then the change in momentum is zero, so in the absence of external forces the total change in momentum is zero.",null,1,cdm156w,1rd5ys,askscience,top_week,9
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,top_week,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,top_week,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,top_week,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,top_week,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,top_week,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,top_week,1
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,top_week,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,top_week,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,top_week,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,top_week,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,top_week,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,top_week,1
null,null,null,32,cdm23mb,1rd5mf,askscience,top_week,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,21,cdmhzt8,1rd5mf,askscience,top_week,92
null,null,null,8,cdm6gn5,1rd5mf,askscience,top_week,13
null,null,null,30,cdmgdmn,1rd5mf,askscience,top_week,22
null,null,null,32,cdm23mb,1rd5mf,askscience,top_week,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,21,cdmhzt8,1rd5mf,askscience,top_week,92
null,null,null,8,cdm6gn5,1rd5mf,askscience,top_week,13
null,null,null,30,cdmgdmn,1rd5mf,askscience,top_week,22
xtxylophone,"Aside from the comparatively 'busy' time around the Earth's formation, nothing has changed. They are just infrequent and the evidence they leave lasts a long time.

Check out: http://en.wikipedia.org/wiki/Impact_event

Impacts that can change geography are about on the scale of the length of Human civilisation. Don't take the data for one being 'due' though. ",null,2,cdm0yuw,1rd5c8,askscience,top_week,7
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barr Syndrome (paralysis lasting for years, or permanently  sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,top_week,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,top_week,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,top_week,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,top_week,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,top_week,3
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barr Syndrome (paralysis lasting for years, or permanently  sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,top_week,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,top_week,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,top_week,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,top_week,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,top_week,3
null,null,null,31,cdm2wxo,1rd53a,askscience,top_week,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,top_week,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,top_week,19
null,null,null,11,cdm3al3,1rd53a,askscience,top_week,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,top_week,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,top_week,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,top_week,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,top_week,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,top_week,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,top_week,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,top_week,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,top_week,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,top_week,1
null,null,null,22,cdm2cds,1rd53a,askscience,top_week,16
null,null,null,31,cdm2wxo,1rd53a,askscience,top_week,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,top_week,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,top_week,19
null,null,null,11,cdm3al3,1rd53a,askscience,top_week,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,top_week,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,top_week,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,top_week,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,top_week,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,top_week,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,top_week,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,top_week,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,top_week,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,top_week,1
null,null,null,22,cdm2cds,1rd53a,askscience,top_week,16
rossk10,"In my realm (structural engineering), wind tunnels are used to simulate and predict expected wind loading to structures during specified gusts.  Smaller, to-scale models of buildings are built, hooked up with load sensors, and then placed in a wind tunnel that simulates a design storm and provides load data at critical points.

As for your specific question regarding smoke trails with cars, understand that my fluid knowledge comes from two fluid dynamics classes in undergrad.  I think that these trails are used to demonstrate how particles travel over the surface of a car, giving useful information about the aerodynamics and drag coefficient of the car.",null,1,cdm0p63,1rd4yo,askscience,top_week,4
phdpeabody,"There's a lot of different types of wind tunnels, and there's a lot of different tests that can be performed in wind tunnels. The tests with the smoke that you mention are to observe [laminar vs turbulent flow](http://www.youtube.com/watch?v=TqTSyFz6DJc), but these are far more often done under CFD analysis than tunnel testing anymore since the models have become highly accurate. This is done because the more disrupted the airflow, the more ""drag"" that is produced. Most of the data captured is just visualized data captured by cameras that is often interpreted mathematically. I work in supersonic research, and one of the more common aeroelasticity tests is to observe things like [flutter at transonic speeds](http://www.youtube.com/watch?v=Xqbkdy3tBdA), which can often be quite destructive. In other tests, like the vertical tunnel, [spin conditions are provoked in models](http://www.youtube.com/watch?v=M7QkTBKtyw8) to examine the performance limitation of an aircraft or research on how to recover from spin conditions such as the infamous [unrecoverable flat spin of the F-14](http://articles.baltimoresun.com/1993-04-06/news/1993096244_1_f-14-flat-spin-tomcats) made famous in Top Gun. The high-temperature liquid oxygen injected tunnel is another type of tunnel used to experiment with new [hypersonic engine designs](http://www.youtube.com/watch?v=F7b76SPlV2E&amp;t=3m50s) that allows researchers to observe performance characteristics at speeds of up to Mach 7 or more recently to observe the performance of inflatable heat shielding during re-entry conditions, and determining the endurance and tolerance of these new materials. One thing about modern wind-tunnel testing however, is that the tests are normally used to verify and validate what predictions the models have already made, as the newer and more advanced testing facilities can cost a project over a hundred thousand dollars to operate for a week or two.",null,0,cdmcdq3,1rd4yo,askscience,top_week,3
meerkatmreow,"The data from the smoke is a type of qualitative flow visualization.  Based on the behavior of the smoke, conclusions about laminar v. turbulent flow can be drawn.


The data from wind tunnel tests can come in many forms depending on what you're trying to do.  Full field quantitative measurements (using something like Particle Image Velocimetry or Pressure/Temperature Sensitive Paint) can be useful for exploring the entire flowfield.  Point measurements using pressure transducers can provide the needed data if you're interested in a certain area.  Data such as overall forces and moments on the model may be what you're after.  Qualitative measurements such as flow visualization uses smoke lines or laser induced fluorescence can help identify areas where additional investigation would be beneficial (ie, separate flow).

What you want to measure and how you measure it are very tightly coupled.  When you do a wind tunnel test you can often choose how you measure things by what you're interested in rather than using a one size fits all approach.",null,1,cdm59n4,1rd4yo,askscience,top_week,2
AbsolutePwnage,"The smoke shows were the air flow is laminar and where it starts becoming turbulent and therefore, where parasitic drag starts appearing. It also looks cool, which is why they show it very often in ads and other media.",null,0,cdojkoc,1rd4yo,askscience,top_week,1
user2097,"3rd year aerospace engineering student here. Wind tunnels are used largely for models to mimic equivalent flow conditions, and the data from the testing includes qualitative and quantitative data.

Sometimes your test is performed to verify dynamic stability, examine stall characteristics of aeroplanes), examine flow condition (separation, turbulence, mixing...), etc. Other tests will produce data based off sensors attached to the model or tunnel such as force on a wing, dynamic response to an input, measure location of separation with hot wires, etc. ",null,1,cdm4stc,1rd4yo,askscience,top_week,1
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,top_week,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,top_week,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,top_week,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,top_week,18
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,top_week,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,top_week,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,top_week,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,top_week,18
redmeansTGA,"First off, lets look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I dont know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldnt be different) receive whats called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldnt have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally dont have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands dont often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, lets look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (perching) birds didnt evolve to the early cenozoic either. Temperate deciduous forests also didnt exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, its clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,top_week,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,top_week,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,top_week,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,top_week,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,top_week,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,top_week,2
redmeansTGA,"First off, lets look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I dont know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldnt be different) receive whats called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldnt have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally dont have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands dont often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, lets look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (perching) birds didnt evolve to the early cenozoic either. Temperate deciduous forests also didnt exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, its clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,top_week,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,top_week,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,top_week,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,top_week,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,top_week,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,top_week,2
skleats,"Cells receive and respond to survival/apoptotic signals independently, so the senescence or death of one cell does not directly impact those around it. This is key since [controlled apoptosis is a normal part of embryonic development](http://people.ucalgary.ca/~browder/apoptosis.html) in multicellular organisms. However, a multicellular organism relies on coordination of activities between its many cells, so having a large proportion of senescent or apoptotic cells would be likely to impact the ability of those cells to contribute to survival of the organism. [This article](http://www.ncbi.nlm.nih.gov/m/pubmed/15265523/) describes an *in vitro* model which mimicked chronological aging and showed reduced coordination between cells as they aged.",null,0,cdm6ky2,1rd0kh,askscience,top_week,2
miczajkj,"If you talk about two different charged particles, that only interact as a closed system (so no external magnetic or electric fields) the problem is equivalent to the [Hyrdogen atom](http://en.wikipedia.org/wiki/Hydrogen_atom). 

Therefore there are quantized stable orbits and a radiation of photons is not allowed without a time-dependent perturbation.",null,0,cdlym8i,1rcxw0,askscience,top_week,1
Platypuskeeper,"Classically, if you're accelerating a charged particle (and a particle moving in a circular pattern is being accelerated constantly), then you will give off radiation. Obviously if you had one particle orbiting another, you would need some kind of outside energy to sustain this, or the thing would give off all its energy and spiral into the other particle.

[Synchrotron light](http://www.iop.org/publications/iop/2011/page_47511.html), which is up in the X-ray range, is produced by moving electrons around at relativistic velocities.


",null,1,cdlyxjg,1rcxw0,askscience,top_week,2
KerSan,"This is *precisely* the problem that made physicists develop quantum mechanics. The answer to your question is 'no', because otherwise the particles would lose energy and crash into each other. Unless the particles are going to merge or something, this is a violation of the Heisenberg Uncertainty Principle because then you would know too much about both the position and momentum of each particle.",null,0,cdlz5t5,1rcxw0,askscience,top_week,1
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,top_week,47
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,top_week,2
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,top_week,47
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,top_week,2
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,top_week,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,top_week,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,top_week,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,top_week,1
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,top_week,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,top_week,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,top_week,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,top_week,1
PENIS_VAGINA,"90% of the blood flow leaving the glomerulus through the efferent arterioles perfuses the cortex (10% to the medulla) under normal conditions. The main purpose of this is to keep the medulla interstitial fluid hypertonic so that concentrated urine can be produced. I suppose that vasculature changes (i.e. arteriolar constriction) could reroute some of that 90% of blood flow into the medulla to aid in decreasing the hypertonicity of the medulla. 


I'm not 100% sure though. I do know that in normal conditions it is hypertonic to aid in urine concentration (your original question). ",null,0,cdlxyv5,1rcwz4,askscience,top_week,1
Farnswirth,"It's actually very simple.  Pure silver is softer and more malleable than silver alloys.  Just like how pure gold is much more malleable than gold alloys.

http://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness#Hardness_.28Vickers.29",null,0,cdm0s7d,1rcwaq,askscience,top_week,5
bellcrank,Pretty sure you could get away with a plane parallel approximation in this scenario.,null,0,cdmksb0,1rcvv4,askscience,top_week,2
adamhstevens,"If you're talking about long wave radiation from the Earth, I think this is a fairly standard textbook problem. I'll try and look it up when I get home... if I remember!",null,1,cdmildz,1rcvv4,askscience,top_week,2
shiningPate,"The original computer architectures used different circuitry for retrieving bytes  assembling into word sizes matching the register size in the CPU. When you were looking at the memory sequentially, independent from the CPU, you needed to know which way the CPU assembled the data into register values to understand why your calculations where coming out wrong",null,0,cdm27j3,1rcvso,askscience,top_week,2
ofeykk,"I am going to attempt to translate, as best as I can, your problem into a mathematical question. I suspect you are making a bunch of assumptions here which I will try to make more formal.

First, with the view of simplifying as much as possible yet retaining the crux of the original problem, you can make the following assumptions (removing each would yield a slightly different problem to solve):

1. Look for curves not in 3-space but in 2-space.
2. Simplify curvature of earth to be flat  seek planar curves.
3. Simplify sun to be on this flat plane as a point.
4. Simplify yourself to be a point on the sought curve.
5. 180 degrees view is equivalent to dividing the plane by the tangent to the curve at your location (point).
6. Choose an orientation for the curve. This helps fix what it means for a point to be in your field of view or equivalently, to be in the ""correct"" half space of the tangent to the point (you) on the curve.
7. Parametrize looping around the curve to be traveling along the unit interval [0, 1] with the end points {0} and {1} identified  essentially a fancy way of saying that the end points of the interval are glued.
8. Assume that the sun is fixed relative to the time taken to loop around the curve once.
9. Finally is the curve sought smooth or not ? (A circle is a smooth curve whereas a triangle isn't one.) I believe it's easier (at least for me) to imagine smooth curves. Also, will exclude wild curves like space filling curves simply because I am not comfortable dealing with those !!

Now, the question is to find a curve that maximizes the view of the sun when you loop around once.

It appears to me that solution would depend on whether you would wish to make a further assumption about whether your plane is unbounded or not. 

If the plane is unbounded, the answer is simple  any straight line not through the point representing the sun would do.

If the domain on which the curve is sought is compact (or technically if its closure is compact)  think finite if you wish, like a square plot or a circular plot of land  then it depends on where the sun is located relative to this domain. Some examples that come off the top of my mind are as follows:

1. Circular region with the sun in the center: Take any diameter and take a tube around the diameter. This tubular region would have a boundary curve composed of two straight lines (chords of the circle) with a small part of the circular's plot's boundary (two of these actually). You can make this tubular region as small as you please and would provide you with a curve for which the sun is visible for as long as you please. (In other words, give me a number for which you wish the sun to be visible, say 99.99%, and I can give you a *width* of this tubular region for which it would be realized. Make it 99.9999999% and I'll give you a different number for the width and so on.)

2. A circle with the sun not in the center: Use the same idea as in example 1. Drop a radius from the center of the circular plot to the boundary that passes through the sun. Measure the distance, say r, from the sun to the boundary of the circle. Draw a smaller circle within the larger circular plot with radius r and centered at the sun. Repeat example 1.

3. A square with the sun at the intersection of the diagonals: Repeat example 1 with the a circle of length equal to the distance from the sun to any one of the vertices of the square.

4. A square with the sun not at the intersection of the diagonals. Easier to say that one should fall back to example 3 but rather simply draw a circle centered at the sun with a radius equal to the shortest distance from the sun to the boundary of the square.

Can go on but would stop here. I have to say that I did this as a fun Sunday morning exercise, and tried to reason mathematically which may or may not have been what you were looking for ! (I enjoyed it though !) :-)",null,0,cdlztop,1rctma,askscience,top_week,4
Bondator,"Human walking speed is roughly 6 km/h so if you circle around north or south pole at 23km radius, you'll do a full circle in 24 hours, keeping the sun in front of you 100% of the loop.

As for your triangle, you didn't go deep enough. Don't do an equilateral triangle, do an isosceles triangle. Mathematically expressing, if we mark the short side with x, and the equal sides with y, and choose the orientation in such a way that x is the part where you don't face the sun, then uptime of sun in face is lim(x-&gt;0) 2y/(2y+x) = 1.",null,0,cdm9rcs,1rctma,askscience,top_week,2
musubk,"I once drove an 18 hour loop in Alaska in the summer with the Sun shining on the left side of my face for all but about an hour of it.  I suppose if you go above the Arctic Circle at the right time of year and just walk toward the Sun at a constant speed, you'll end up where you started 24 hours later making a complete loop.",null,0,cdnb6l9,1rctma,askscience,top_week,1
Das_Mime,"The expansion of the universe and the speed of light have different units, therefore you can't compare them.

The Hubble Constant is the fraction by which a given parcel of space will grow in a given amount of time. It has units of inverse time, s^(-1). The speed of light, of course, has units of distance/time, m s^(-1).

The Hubble Constant is usually given in units of kilometers per second per megaparsec, but the two distance units just cancel out and you get the result that the universe expands by about 0.00000000000000002% per second.

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

There will eventually come a time when there are no other visible galaxies in our observable universe (except for nearby ones that are gravitationally bound to us).",null,3,cdm01m4,1rcr2g,askscience,top_week,10
IAmMe1,"We in fact know that far-away parts of the universe are receding from us faster than the speed of light. However, this is not a problem. It's better to think about the expansion of the universe as an change in space itself rather than the motion of the things in that space; think of it as extra distance appearing between far-off objects. In this way, nothing is moving faster than light in the sense of any actual motion; instead, the distance between us and such an object increases faster than light can traverse that distance (i.e. more than 1 light-year of distance is added per year).

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

Yes indeed. It will be a dark and dismal universe that day far, far into the future!",null,1,cdlyqtu,1rcr2g,askscience,top_week,7
Luminarie,"Based on what we know about physics right now, we have no reason to believe it won't happen. Lawrence Krauss puts it quite succintly: ""Nothing can move faster than light in empty space, but space itself can to whatever the hell it wants"".

And yes, that's what would happen. At some point, the space between galaxies will be expanding faster than light, and at that point they will disappear from our region of the universe, as light would need to be faster than the expansion to be able to get to us. Therefore we will be causally disconnected from the rest of the universe.

Beyond the point where it accelerates faster than light, extrapolations based on an unchanging acceleration end in a [Big Rip](http://en.wikipedia.org/wiki/Big_Rip). Basically, the increasing speed of expansion overcomes all physical forces, and the universe would seem to end in a singularity (the scalar factor that defines expansion becomes infinite) at the moment when this happens.",null,0,cdlz409,1rcr2g,askscience,top_week,2
Das_Mime,"The expansion of the universe and the speed of light have different units, therefore you can't compare them.

The Hubble Constant is the fraction by which a given parcel of space will grow in a given amount of time. It has units of inverse time, s^(-1). The speed of light, of course, has units of distance/time, m s^(-1).

The Hubble Constant is usually given in units of kilometers per second per megaparsec, but the two distance units just cancel out and you get the result that the universe expands by about 0.00000000000000002% per second.

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

There will eventually come a time when there are no other visible galaxies in our observable universe (except for nearby ones that are gravitationally bound to us).",null,3,cdm01m4,1rcr2g,askscience,top_week,10
IAmMe1,"We in fact know that far-away parts of the universe are receding from us faster than the speed of light. However, this is not a problem. It's better to think about the expansion of the universe as an change in space itself rather than the motion of the things in that space; think of it as extra distance appearing between far-off objects. In this way, nothing is moving faster than light in the sense of any actual motion; instead, the distance between us and such an object increases faster than light can traverse that distance (i.e. more than 1 light-year of distance is added per year).

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

Yes indeed. It will be a dark and dismal universe that day far, far into the future!",null,1,cdlyqtu,1rcr2g,askscience,top_week,7
Luminarie,"Based on what we know about physics right now, we have no reason to believe it won't happen. Lawrence Krauss puts it quite succintly: ""Nothing can move faster than light in empty space, but space itself can to whatever the hell it wants"".

And yes, that's what would happen. At some point, the space between galaxies will be expanding faster than light, and at that point they will disappear from our region of the universe, as light would need to be faster than the expansion to be able to get to us. Therefore we will be causally disconnected from the rest of the universe.

Beyond the point where it accelerates faster than light, extrapolations based on an unchanging acceleration end in a [Big Rip](http://en.wikipedia.org/wiki/Big_Rip). Basically, the increasing speed of expansion overcomes all physical forces, and the universe would seem to end in a singularity (the scalar factor that defines expansion becomes infinite) at the moment when this happens.",null,0,cdlz409,1rcr2g,askscience,top_week,2
Azurity,"If you're up for a bit of fun and history, here's an ancient (1961) paper that originally investigated various mechanisms of polypeptide assembly: [ASSEMBLY OF THE PEPTIDE CHAINS OF HEMOGLOBIN](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC221568/pdf/pnas00219-0005.pdf)

It's actually a fun article to read if you've got an hour or three and you feel like working out a molecular puzzle using 50-year-old methods and logic. Scientists weren't sure if proteins were synthesized from one end to the other, or started at both ends, or if there was actually a giant cellular ""stamping"" machine that knit every amino acid of a protein together at once! Essentially, they used a series of radiolabeling and quenching experiments and froze moments in time as proteins were being made to mathematically derive a mechanism of N-terminus to C-terminus translation. Cool stuff!",null,0,cdm5qng,1rcqnv,askscience,top_week,6
PENIS_VAGINA,"Well read the section about Crick's contribution to molecular biology here: 


http://en.wikipedia.org/wiki/Francis_Crick#1951.E2.80.931953:_DNA_structure


I'm not sure that there was ONE definitive experiment that determined the mechanism. Perhaps there was, but I can't seem to find it.


If you are wondering how you can prove it now, a simple example would be using GFP to follow a DNA sequence to an mRNA transcript and then to a fluorescing protein. There are other experiments that rely so heavily on mRNA as the transcript of DNA that it basically 100% accepted as the mechanism. ",null,3,cdlytmo,1rcqnv,askscience,top_week,4
quantum_lotus,"It seems like you want the actual experiments that led to our understanding of the triplet nature of the genetic code.  I'll offer you two resources that explain the experimental procedures (and the logic behind them) that led to our current understanding.  Both are at a basic undergraduate level, so I doubt you'll have trouble following.

The first is from the Nobel Prize website called[ ""Crack the Code""](http://www.nobelprize.org/educational/medicine/gene-code/history.html).  I'd read the historic background, but the explaination of the experiment starts with the ""A Clever Experiment"" section a little further down the page.

The second is as [PDF](http://basic.shsmu.edu.cn/jpkc/cellbiota/resource/exper/11.pdf) I found with a basic google search (for ""cracking the tRNA code"") that is unattributed, but hosted on a site from the Shanghai Jaio Tong University.  It appears to be from an undergraduate level textbook and gives a more in-depth look at the same history and experiments.",null,0,cdn4cyp,1rcqnv,askscience,top_week,1
ignorant_,"While we're waiting on someone with better credentials, I'll throw in that for every 4 inches in height over 5ft, a person has a 16% increased risk of cancer. So converse to your statement, it is my understanding that more cells, thus more cell divisions, means greater risk of cancer development. ",null,1,cdlztvc,1rcpmh,askscience,top_week,5
Aniridia,"Obesity does increase the risk of several cancers. The inverse, does being ""skinny"" lower the risk of cancers, is less clear, and I'm not sure if it has been directly studied. There's a [Lancet article](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(11\)60814-3/fulltext#article_upsell) that deals with many of the health risks of obesity, including cancers. (The article is free, but you must log in.) Here is a [PDF PowerPoint type presentation](http://www.mhsimulations.co.uk/Documents/WangC.pdf) of the article from the author. ",null,0,cdmr14g,1rcpmh,askscience,top_week,3
therationalpi,"Sound waves definitely *are* affected by the wind. Since sound waves travel through a medium, and wind is a bulk flow of the medium, the sound speed in a windy environment (which is normally the same in all directions) suddenly becomes direction dependent. Specifically, the speed of the wave becomes the speed of the wind in that direction plus the speed of sound at rest. Moreover, since wind tends to move faster the higher you move up from the ground, there is usually an effective sound speed gradient as well. In the presence of a sound speed gradient, sound waves tend to refract towards regions of lower sound speed. As a result, sounds sent out against the wind will tend to refract upwards, and sounds made with the wind will tend to refract downwards. Sounds made cross-wind will tend to refract downwind and up. And since your listeners tend to be near the ground (relatively speaking) the net effect is that sounds carry further with the wind than against it.

As for electromagnetic waves (light/radio), I don't believe there is any notable effect, but I would wait for verification from someone with more experience in the field.",null,0,cdlz4kd,1rcooj,askscience,top_week,7
KerSan,"Energy is a consequence of a symmetry in the laws of physics. This is a deep point, so let me try to explain.

It is first worth explaining probably the most important concept in physics, momentum. Early on in your education, you are told that it is simply mass times velocity. This turns out not to be true generally, because momentum is really what is called the 'generator' of translations.

Consider a point particle that has a definite position in space. That position is actually an arbitrary label, since it refers to your chosen co-ordinate system rather than some actual property of the universe. You could just as easily switch co-ordinate systems by, say, moving the origin of your co-ordinates somewhere else. This action of shifting co-ordinates is called 'translation'.

Now let us suppose that the particle is moving (relative to you, the observer). No matter which co-ordinates you chose, you will notice that the position of the particle is changing over time. If you want to predict where you will see the particle next, you need to translate the last known co-ordinates of the particle by some amount. That translation is determined by the momentum of the particle. Keep in mind that the momentum is not changed when you translate your co-ordinate system. The momentum is, in a technical sense, dual to position.

What does this have to do with energy? Well, energy happens to be the generator of translations in time. This is because *energy is defined to be that quantity which remains invariant under translations in time*, much as momentum is that quantity which is invariant under translations in position. Your co-ordinates for time are just as arbitrary as your co-ordinates for position, because your choice of starting time is your choice and not some property of the universe.

Energy therefore tells you something about how quickly the properties of an object change over time. The more energy, the faster changes can happen. I could write essays explaining this further, but I don't want to lose you.

Work is the addition or subtraction of energy from a system. That energy can be added in a variety of ways, but it is common in early physics education to think in terms of force rather than energy. Energy is actually more fundamental than force, so I'll explain the force times displacement rule by first explaining force in terms of energy.

Force is the derivative of potential energy with respect to position. If you think of the potential energy of a particle as a function of the position of the particle (for instance, gravitational potential varies linearly in the height of the object), then force experience by the particle at a given position is the slope of the potential function at that position. In other words, force is the negative change in potential energy (i.e. the work) divided by displacement (in an infinitesimal sense). Multiply both sides by displacement and you have the relation work = force * displacement.

Hope that helps.",null,2,cdlzluj,1rcojz,askscience,top_week,7
miczajkj,"What is energy?

This is a very interesting question - but we we don't have an answer to this. We formulate physics in equations and stuff and there is this certain quantity that shows up in some of them and seems to be conserved under certain circumstances. 

The first kinds of energy you encounter, when you start to think about physics are the kinetic energy and the potential energy.
Assume a body at the position x that is exposed to a force F(x). Following Newton, you get the differential equation

m*x = F(x) 

The second derivative of the bodies displacement with respect to time times its mass is equal to the force on the particle.  
If it is possible to write the force as the derivative of a potential, namely F(x) = -V'(x) you see, that the following quantity is conserved for every solution of the mentioned equation: 

E = 1/2 mx + V(x) 

If you take it's derivative, you see: 
E = m*x*x + V'(x)*x = x*(mx-F(x))

So if the differential equation is fulfilled, then E' = 0 follows directly. 

You can see: because of the form of the differential equation that describes the movement, there is a certain quantity, that won't change - we call it a conserved quantity. 

If you dive deeper into the mathematical foundament of physics, you find that symmetries are responsible for the most conserved quantities. The energy for example, is only conserved, if the physical laws describing the process are time independent. 

All in all: energy is just a number. A phrase like 'pure energy' is nonsense. Energy is just a quantity, that appears in our equations. 

Work is defined as the change in energy you need to apply to a system, to move it from one state to another. In most cases this movement is a spatial movement from one place to another - then work is just the difference in potential energies, 

W = V_1 - V_2

or because V'(x) = F(x) 

W = integral F(x) dx

For forces that don't depend on position, you get 

W = F*x


I guess that was a whole lot of different concepts - maybe you've never heard of differentation, integration or newton's 2nd law: but I can't come up with an easier way to describe energy, that is not too simplified or wrong. ",null,0,cdlztxi,1rcojz,askscience,top_week,2
dampew,"Work has units of energy -- they're basically the same thing.  For instance, if you want to know how much work you've done, the answer would be in units of energy (joules, calories, whatever).

Why is it force times distance?  Well, you do more work if you have to push something harder through a larger distance.  I'm not really sure how to answer that question -- it's just a name for something, really...",null,0,cdm0kyk,1rcojz,askscience,top_week,1
FeckSakeLads,"work is the amount of energy you must add using a force of a certain strength to move an object with mass a certain distance in a certain direction (the displacement). the equation is:



work = force (joules) x displacement (metres) x cos(theta) (theta being the angle between the direction of the force relative to the direction of displacement).



See [this](http://www.physicsclassroom.com/calcpad/energy/) for more.",null,3,cdlzbtd,1rcojz,askscience,top_week,1
shiningPate,"First, this effect only occurs in a vacuum. In air, the gas molecules rapidly absorb the emitted electrons. Basically the reason xrays are emitted is because of the static electricity generated by separating the tape film from the surface below it. As the tape is pulled away, an electrical field is formed at the point of separation. As the roll rotates away, the film separates from the layer below it, reducing the strength of the field between the point that just separated, and the point at which it was previously touching. Meanwhile a new point has just separated from the roll, and higher strength field is formed at that point. The result is an electrical field at a gradient intensity in the wedge between the roll and film. At the point of separation, the field strength is so high electrons are ripped away from the film material. The film material was also electrically charged as part of the manufacturing process to get it to roll up smoothly. Electrons emitted into the gradient electrical field will begin accelerating along the gradient. Accelerating electrons emit xrays",null,0,cdm2gu7,1rcmj7,askscience,top_week,3
dampew,"http://www.nature.com/news/2008/012345/full/news.2008.1185.html

""The leading explanation posits that when a crystal is crushed or split, the process separates opposite charges. When these charges are neutralized, they release a burst of energy in the form of light.""",null,0,cdm0glp,1rcmj7,askscience,top_week,2
cromonolith,"The only thing stopping this from being intuitive to you is what ""bigger"" means. 

You're used to judging the sizes of things by counting each of them and comparing the numbers. That is, if I gave you two bunches of apples, you might count and see that there are 10 apples in the first bunch and 14 in the second, and conclude that the second bunch is bigger. 

That's fine, but it's not a good way of measuring the size of infinite collections. So here's a better way of comparing the size of two collections: make a pairing between the collections, and the one that has stuff left over is bigger. 

As an example, let's say we had a huge auditorium and a huge crowd of people who want to see a show there. What's the best way to see if we have the same number of seats as people? You can count the seats and the people, but that's dumb. The smart thing to do is to tell everyone to sit down. As long as no one sits down stupidly (two people in the same chair), then you can easily check. If there are empty seats left over there are more seats than people. If there are people left standing there are more people than seats. 

What does this have to do with infinities? Well, this kind of pairing (like pairing people with seats) is a function. If no two people sit in the same seat, the function is called ""injective"" (or ""one-to-one""). If no seats are left unoccupied, the function is called ""surjective"" (or ""onto""). If a function is both injective and surjective (one person to a seat and all the seats are filled) the function is called ""bijective"". 

Bijective functions are what we use to compare the sizes of all sets, including infinite ones. Two sets are *defined* to be the same size if there exists a bijective function between them. So for example, the set of all natural numbers **N** = {1, 2, 3, 4, ....} is the same size as the set of even numbers 2**N** = {2, 4, 6, 8, ...} because, as you can check, multiplication by two is a bijective function from the former to the latter. That is, the function f where f(n) = 2n hits every even number and never sends two numbers to the same even number. 

On the other hand, you might want to compare the set of natural numbers as above to the set **R** of real numbers (the whole number line). This isn't obvious, but there's a [relatively straightforward proof](http://www.mathpages.com/home/kmath371.htm) that no matter how you try, it's impossible to make a bijective function from **N** to **R**, so they can't have the same size. Since **N** is actually a subset of **R**, we say the size of **N** is smaller than the size of **R**. ",null,1,cdlzcjv,1rcjr9,askscience,top_week,18
rlee89,"There are several notions of size when it comes to infinity.  The most common is *cardinality* which in lay terms asks whether one infinity can be fit (or more formally, mapped) into another.

For example, if you have a list of the positive integers, there is a way to place a rational number next to each positive integer in the list in such a way that each rational number is next to some positive integer, and vice versa.  We would formally call this a bijection between the positive integers and the rational numbers, and it would demonstrate that the two are the same cardinality.

[Cantor's diagonal argument](http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument) shows that any attempt to match up the positive integers with the real numbers will necessarily miss at least one real number.  Thus the of real numbers won't fit into the poisitive integers, and thus have a larger cardinality than the positive integers and are a bigger infinity.",null,1,cdlznix,1rcjr9,askscience,top_week,2
protocol_7,"Here's another way of thinking about it: Any natural number can be represented by a finite amount of information. (For example, you can write it down in base 2 as a finite string of 0's and 1's.)

However, a real number has an *infinite* string of digits past the decimal point, so there's no guarantee that any particular real number can be expressed using a finite amount of information. (And, in fact, most real numbers *cannot* be uniquely identified by any finite amount of information.)

So, intuitively, it's the difference between finitely many versus infinitely many ""degrees of freedom"". Since specifying a single real number can involve an infinite number of choices of digit, there are vastly more real numbers than natural numbers.

This is just a vague intuition, though. You can make it precise using [information theory](https://en.wikipedia.org/wiki/Information_theory), but that's rather technical. Instead, here's how to see very quickly that some infinite sets are larger than others  in particular, that given any set (finite or infinite), there's another set that's larger.

By definition, two sets have the same [cardinality](https://en.wikipedia.org/wiki/Cardinality) (basically, size) if they can be put into [one-to-one correspondence](https://en.wikipedia.org/wiki/Bijection) with each other  that is, if you can pair up elements of the sets so that each element of one set is paired up with a unique element of the other set.

**Theorem** ([Cantor](https://en.wikipedia.org/wiki/Cantor%27s_theorem))**.** Let S be any set, and let P(S) be the [set of all subsets of S](https://en.wikipedia.org/wiki/Power_set). Then S and P(S) do not have the same cardinality.

*Proof.* Let f be any function from S to P(S), that is, for each element x in S, we assign a unique element f(x) in P(S). Since each f(x) is a subset of S, we can ask whether x is an element of f(x). In particular, let T be the set of all elements x of S such that x is *not* an element of f(x).

Now we can ask, is there some element y in S such that f(y) = T? Suppose there was: then we can ask whether y is an element of T. If it is, then y is an element of f(y), so by the definition of T, y is not an element of f(y)  a contradiction. If it isn't, then y is not an element of f(y), so by the definition of T, y *is* an element of f(y)  again a contradiction. Therefore, it's impossible for such an element y to exist. So no element of S is paired up with T.

Thus, *any* attempt to pair up elements of S and elements of P(S) fails, which means that S and P(S) don't have the same cardinality. 

But we can embed S inside P(S) by sending each element x in S to the [singleton set](https://en.wikipedia.org/wiki/Singleton_%28mathematics%29) {x}, a subset of P(S). So, in fact, S is strictly smaller than P(S).

Notice that there's no mention of whether S is finite or infinite. (If S is finite with n elements, then P(S) has 2^n elements.) So the proof is valid regardless, meaning that it's true for infinite sets, too.

For instance, if we denote the set of natural numbers by **N**, then P(**N**) is strictly bigger than **N**. (Actually, it turns out to be the same size as the set of real numbers.) And the set P(P(**N**)) is bigger than that, and so on.",null,1,cdlzsre,1rcjr9,askscience,top_week,2
null,null,null,3,cdlz5a1,1rcjr9,askscience,top_week,2
biffym,"It wasn't to make them believe it was real so much as to make it feel more real. They knew it was an experiment and that being arrested was part of it, but it adds to the feeling that they aren't there by choice. If they'd walked in of their own free will the jail would have felt less oppressive.",null,0,cdltsfb,1rcjmy,askscience,top_week,3
DougWC,"What is most interesting about the experiments and others like it is not the obvious.  It's the implications for ""real world"" situations of authority and subordination - that they are no more truly legitimate than are contrived ones.  They are all contrived and all should be seen through.",null,0,cdm1916,1rcjmy,askscience,top_week,1
Platypuskeeper,"There is no motion in a classical sense. Electrons have kinetic energy and that, but they do not follow specific trajectories. The probability of knowing where you might find the electron is all you've got. For specific energy states, these probabilities don't change with time. Electrons have no size of their own, their position-probability distribution in space is basically where the electron is.

The wave function/probability distribution, which for a single particle has [solutions like this](http://chemwiki.ucdavis.edu/@api/deki/files/8855/Single_electron_orbitals.jpg), is analogous to a standing wave in three dimensions. The angle-dependent part of the functions are the spherical harmonics.

Photons and electrons are particles, they both have energy but they have more than that as well. They carry both linear and angular momentum, for instance.
",null,3,cdlufy4,1rcj7e,askscience,top_week,5
The_Serious_Account,"&gt; So do electron move like those diagrams of standing waves? Or do they not wave like that, but whiz around like a particle? If I was reduced in size, is the electron a hard ball or is more of a packet of energy like a photon?


I think the most sensible thing to do is to give up the concept of particles. There's no such thing as a particle. Just drop the concept entirely. We keep it in language, but as a mental picture it's dangerous and misleading. 

The electron doesn't move around within the standing wave. The electron *is* the standing wave. The word electron should refer to the standing wave and nothing else. It's an extremely hard concept to accept, but repeat it to yourself. There is no particle. There is no particle. There's only the wave function. ",null,2,cdlucvm,1rcj7e,askscience,top_week,2
Polyknikes,"Short answer: Stimulation of the vagus nerve (CN-X) which induces bradycardia.

Long answer:

I had not heard the term apneic pause before but from googling it you are referring to a cessation of breathing for at least 10 seconds, commonly referenced in relation to sleep apnea.  I didn't know the term but I do understand cardiac physiology.

In 10 seconds your oxygen saturation levels do not drop by a measurable level.  Try holding your breath while using an oxygen saturation meter and see if you can even get it to go down by 1% - it's really difficult even if you can hold your breath for several minutes.  So I don't believe it is related to lack of oxygen unless we are talking about a much longer duration apneic pause.

During normal inspiration and expiration the heart rate increases and decreases, respectively.  This is thought to be induced by changes in vagal nerve tone (activity) but the mechanism by which breathing influences vagal tone is not understood.  The vagus nerve acts to reduce heart rate by hyperpolarizing the intrinsic pacemaking cells of the heart.  

Stimulating the vagus nerve would decrease heart rate and this can be accomplished by various means including hypoventilation, hypoxemia, respiratory acidosis, or vigorous inspiratory effort against a closed airway known as the Mueller's maneuver.  In the case of obstructive sleep apnea a person may be attempting to inspire but cannot due to a blockage of their airway (usually seen in obesity) which could stimulate the vagus nerve by the Mueller's maneuver mechanism.  But again, the exact mechanism by which the vagus nerve is stimulated by these pressure changes is not known, it has simply been observed indirectly.

I hope this at least partially answers your question!",null,0,cdlwfmx,1rci0v,askscience,top_week,3
meerkatsrgay,"so, I have never been full body immersed in -90. However, I have ineracted with huge standup freezers that cool biological samples to -86C (I have seen -89 once on the readout) Generally when interacting with this environment you wear protective gloves. Grabbing a cooled piece of metal can be dangerous if held for more than a few seconds as it will freeze the moisture on your hand. But, through my interaction with these freezers I would assume that standing in an environment like that naked you would be fine for even up to 15mins provided you only contacted anything solid through your feet (and had shoes) AND there was absolutely no wind. In an environment with no air movement your body is able to build up a very tiny layer of warm air close to the skin. This is the same reason why holding dry ice or sticking your hand into liquid nitrogen is fine, the evaporated gas that is instantly created between you and the material acts as an insulator. 

Either way, I wouldn't recommend going outside naked in -90C weather because you prolly don't look as good as you think and no one wants to see you naked.",null,2,cdm6hb4,1rchgw,askscience,top_week,11
shiningPate,"See the information on the South Pole station [300 Club](http://en.wikipedia.org/wiki/300_Club). To get into the club, you have to run naked from the 200 degree sauna, down the access tunnel, outside 20 feet to the south pole marker, touch it, and return back to the sauna. The dash has to be done when the temperature outside is -100 F. ",null,5,cdm1xw7,1rchgw,askscience,top_week,10
null,null,null,2,cdm2oo5,1rchgw,askscience,top_week,4
Ruiner,"If you were in a planet without atmosphere, then it would, but since we have air friction, the bullet would land with its terminal velocity.

correction.: if its terminal velocity is bigger than its speed when it is fired. Otherwise, it will land with a smaller velocity, given the energy it lost to friction.",null,3,cdlt0kg,1rch1e,askscience,top_week,19
Ruiner,"First, kudos for doing your own experiment and trying to interpret results.

Now, my suggestion is that you should try striking with two coins instead of one. And afterwards, use a coin that's a bit heavier than the others.

After you're done with it, try reading the physics explanation on this [page](http://en.wikipedia.org/wiki/Newton's_cradle).",null,1,cdlsz1r,1rcgby,askscience,top_week,6
Platypuskeeper,"Ice behaves like a normal solid, the [density increases with lower temperature.](http://en.wikipedia.org/wiki/File:Density_of_ice_and_water_%28en%29.svg). 

Nothing particularly interesting happens at the molecular level, unless you have a phase change. The average distance between molecules decreases because they vibrate less at lower temperatures.
",null,1,cdltrht,1rcdt3,askscience,top_week,2
Wrathchilde,"There are many forms of solid water [ice](http://www1.lsbu.ac.uk/water/ice.html), and which form exists is a function of temperature and pressure.  Some forms, as described in the link above, are more dense than liquid water.

However, since you did not include pressure changes in your question, let's assume a constant 1 atm.  This [phase diagram](http://en.wikipedia.org/wiki/File:Phase_diagram_of_water.svg) shows that below about 70k ""normal"" ice will change to ice-XI, a more structured crystal.  However, as the first link describes, the density of ice-XI is pretty much the same.
",null,0,cdlufvc,1rcdt3,askscience,top_week,1
NotAStructrlBiologst,"I hope this gives you a better picture of what is going on at the molecular level.

Even in the crystal/solid state molecules may not move but their atoms continue to vibrate. Bonds contract/expand and angles wobble ever so slightly. Continuing to cool something will also decrease these motions, increasing the order",null,0,cdm0rnu,1rcdt3,askscience,top_week,1
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,207,cdlyfi3,1rccq1,askscience,top_week,1144
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,top_week,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,19,cdlusqz,1rccq1,askscience,top_week,41
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,11,cdlwc5k,1rccq1,askscience,top_week,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,top_week,17
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,12,cdlvk3k,1rccq1,askscience,top_week,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,top_week,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,11,cdlushn,1rccq1,askscience,top_week,17
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,9,cdlz6us,1rccq1,askscience,top_week,13
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,top_week,15
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,12,cdm5uag,1rccq1,askscience,top_week,16
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,10,cdmelu9,1rccq1,askscience,top_week,14
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because _, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,11,cdlyunp,1rccq1,askscience,top_week,15
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,top_week,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,top_week,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,top_week,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,top_week,12
null,null,null,10,cdmg52h,1rccq1,askscience,top_week,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,top_week,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,8,cdm4kp8,1rccq1,askscience,top_week,11
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,207,cdlyfi3,1rccq1,askscience,top_week,1144
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,top_week,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,19,cdlusqz,1rccq1,askscience,top_week,41
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,11,cdlwc5k,1rccq1,askscience,top_week,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,top_week,17
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,12,cdlvk3k,1rccq1,askscience,top_week,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,top_week,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,11,cdlushn,1rccq1,askscience,top_week,17
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,9,cdlz6us,1rccq1,askscience,top_week,13
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,top_week,15
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,12,cdm5uag,1rccq1,askscience,top_week,16
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,10,cdmelu9,1rccq1,askscience,top_week,14
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because _, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,11,cdlyunp,1rccq1,askscience,top_week,15
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,top_week,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,top_week,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,top_week,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,top_week,12
null,null,null,10,cdmg52h,1rccq1,askscience,top_week,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,top_week,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,8,cdm4kp8,1rccq1,askscience,top_week,11
weinerjuicer,"without drawing energy from the environment, it seems like it should in principle be possible to exchange kinetic energy due to translational motion for gravitational potential energy: if they are going slower horizontally after the dive-down-then-pull-up move it may not violate conservation of energy.",null,1,cdlurjr,1rc9xn,askscience,top_week,8
drzowie,"Without some effect to bring energy to the hang-glider, diving and rising will always cause the hang-glider to go down.  That will always happen faster than if the hang-glider pilot just flew along at his best-sink-rate speed and attitude (which should be close to his best-glide-ratio speed and attitude)

That said, energy is not particularly well conserved in the hang-glider's system.  An experienced hang-glider pilot can make use of many counterintuitive effects, mostly involving wind shear or vertical winds, to scrub energy from the environment.  

Hang-gliders near Torrey Pines in San Diego, CA can fly all day without any thermals at all due to the vertical wind break at the Torrey Pines cliffs.
",null,2,cdlw5tk,1rc9xn,askscience,top_week,4
zlatan08,"According to conservation of energy, the total energy, which is the sum of gravitational potential energy, kinetic energy, chemical potential, magnetic potential etc.., must remain the same. In this case, let's consider just kinetic and gravitational potential energy. At any point in time, as long as no outside forces act on the glider (i.e. thermals), their sum must be constant. When the glider dips down, it trades gravitational potential energy for kinetic; when it rises back up, it trades the kinetic for gravitational potential. If it tries to rise to high, the kinetic will come close to zero and the glider will stall. In real life, if the glider keeps trying to dip down, rise, stall and then dip down again, drag forces on the surfaces on the glider will cause it to lose energy and every time the glider stalls, its height will be lower than the previous time. Drag would be considered the outside force in this case and energy would not be conserved.",null,1,cdlv4jy,1rc9xn,askscience,top_week,2
_Jordan,"I have heard that competition gliders, under the rules of the competition get towed to a certain height, and see how long they can stay in the air.

A trick they use, is to carry a tank of water with them when they get towed to the starting altitude. They immediately dive down to the ground (trading altitude for speed), then dump the water low to the ground, and go up (trading speed for altitude). Using this trick, they can rise up above the starting height, and stay in the air for longer.

If you see a hang glider diving off of something high, and then rising higher than they started, look to see if they dropped anything at the bottom of their dive.",null,1,cdm1ln5,1rc9xn,askscience,top_week,1
Platypuskeeper,"&gt; What exactly is an oil? [the requirements to call something oil]

Typically something with a lot of long-chain hydrocarbons in it, but in the broadest sense (e.g. essential oils, vegetable oils) it could really be any liquid composed of non-polar (oily) compounds. It's not a precise (or 'technical') term as far as chemistry is concerned.

&gt; Are there oils which do not have carbon in it?

[Silicone oil](http://en.wikipedia.org/wiki/Silicone_oil), although the term 'oil' there is more because of its use as a lubricant than its chemical composition.

&gt; Is it true that kerosene is not technically an oil? And why?

I don't see what usable definition of 'oil' would exclude kerosene. It's not _crude oil_, it contains a more limited subset of hydrocarbons. But it's still a hydrocarbon mixture.


",null,1,cdm06t8,1rc9wk,askscience,top_week,4
dapwnsauce,"Most oils do not dissolve in water as they have two characterizing features, a polar(hydrophobic) and non polar end(hydrophobic).  An example is a [Micelle](http://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Micelle_scheme-en.svg/532px-Micelle_scheme-en.svg.png).  Kerosene is an really long hydrocarbon chain in comparison to others which are used as **fuels**(although there are some which are longer).  Kerosene does not have a polar end, it is in other words a potential backbone to an oil.  Considering this, it would be classified more as a **fuel** rather than an oil.

Silicone oil does contain carbons and it maintains its ""oil"" status due to the polarity of the Si-O bond.  Non-polar end being the carbon groups that are attached to it.  The design of silicones have multiple chemical uses which sets them apart from conventional oils.  
  
The fluidity of an oil really depends on the hydrocarbon backbone.  The arrangement of the molecules can determine whether they become more rigid and less viscous or more fluid.  When oils are able to tightly pack together, they tend to form more viscous structures and even solid structures (ie butter/shortening).   When they have rigid backbones (ie kinks in the chain) they tend to form more fluid structures such as olive oil.  

Are there any oils that do not have any carbons in them whatsoever?  Theoretically there should be some that exist, though none come across my mind at the moment.

Hope that helps.",null,0,cdmkda9,1rc9wk,askscience,top_week,2
soylentblueissmurfs,"One of the reaons inbreeding can be harmful is you run a much higher risk of recessive genetic disease since your relatives are more likely to carry the same damaged alleles. However, if you inbreed enough those mutations will be weeded out so the answer is basically: they are SO inbred it's a small problem.",null,0,cdlut33,1rc9t0,askscience,top_week,5
mak484,"I'll handle the follow-up question. Mice are an ideal model organism for many reasons- they have a relatively short gestational period, produce many offspring, and reach sexual maturity very quickly. All of these factors lead to incredibly short generation times with a large exponential increase in population size with each generation. This allows scientists to very easily weed out deleterious recessive alleles, leaving breeding stocks with very uniform and well-understood genotypes.Compare mice to humans- it takes 12-16 years to reach sexual maturity, and females can only give birth (naturally) to 1-2 offspring per year. Factor in a high level of allelic heterozygocity, and you can see where aliens would have a difficult time creating a genetically uniform breeding stock.*I got a little morbid below, sorry if this disturbs anyone*Now, if I were the aliens, assuming I had unlimited resources and appropriate technology, I would find brother-sister pairs of paternal twins, and harvest their sperm and eggs. I would then fertilize all of the eggs simultaneously, and begin genetic testing once the fetuses reached several weeks. The offspring with the highest levels of homozygocity would be selected for the F1 generation of breeding, where I would repeat the process. After maybe 5-10 generations complete homozygocity and lack of deleterious genes could be reached. Since I started this process with numerous genetically diverse brother-sister pairs, I could develop multiple lines of humans that carry whatever combination of genes I want. ",null,0,cdlvtyw,1rc9t0,askscience,top_week,4
do_od,"Mountains on Earth will never be much taller than they are now. That is  because at some point the weight of the mountain causes such enormous pressure that the base of the mountain will start to liquify and deform.  

Planets with lower gravity can have higher mountains. A prime example is [Olympus Mons](http://en.wikipedia.org/wiki/Olympus_mons), the tallest mountain on Mars at 22 km. The surface gravity on Earth is about 2.7 times that of Mars. Because pressure scales linearly with height, we can expect the tallest mountains on Mars to be about 2.7 times taller than the tallest on Earth. 2.7 * 8.9 km = 24 km, is in that ballpark.  

Wether or not Mt Everest is higher than the atmosphere depends on your definition. You could say that it is because humans can't survive there for very long.",null,0,cdm3ptj,1rc9ea,askscience,top_week,3
JohnSmith1800,"There's sort of two features going on here, the macro and microscopic, so I'll detail them both.

The light first passes through the pupil and lens, which focus it in particular on a small patch of the retina known as the fovea. This is where the concentration of cone cells (those which detect colour) is highest. You also have a lot of rod cells (just detect light generally, more sensitive in dark settings) here, but they're more common as you move away from the fovea. Collectively rod and cone cells are photoreceptors. These photoreceptors are hooked up to bipolar cells, horizontal cells and ganglion cells, which together do some initial ""processing"" of the image before it passes down the optic nerve to the brain. Interestingly, because the horizontal, bipolar and ganglion cells are actually between the retina and the pupil, the optic nerve has to travel through the retina, which creates a blind spot in each eye, about 15degrees off-centre (which your brain lies to you to fill in).

On the microscopic level, photons travel through the eye to the retina, where some encounter either rod or cone cells. In rod cells there is a stack of ""plates"" which are coated in an enzyme called rhodopsin. When a photon is absorbed by rhodopsin, it changes conformation and can activate another protein known as transducin. Transducin is what is known as a ""G-Protein"", when activated it in turn activates another protein in turn, which then changes cGMP (a small second messenger) into 5'-cGMP. This leads to a closure of Na^+ channels. This hyperpolarises (makes more negative) the cell. Neurons only release neurotransmitter when they depolarise, so this reduces the release of neurotransmitter. I'm not familiar with the exact pathway in cone cells, it is photopsin rather than rhodopsin which absorbs the photon, but otherwise I believe it to be generally similar. This whole process actually takes quite a while, about 200ms from memory between when the photon hits and when your photoreceptor's sodium channels close. This is because of the ""protein cascade"" which occurs, it takes time. However, it does greatly increase your sensitivity to light: A single rhodopsin can activate ~500 transducins, which will in turn do ~500 cGMP's each. This will close ~100 sodium channels, stopping 10^~11 ions, and hyperpolarising the cell by almost a mV. That is to say, your eyes are actually very sensitive to light.

Intriguingly photoreceptors are actually inhibitory neurons, they release a neurotransmitter which hyperpolarises other neurons. As such, when a photon is absorbed their rate of firing decreases, which increases the firing rate of bipolar cells. The rest of the neural pathways are way over my head, but it involves ""receptor fields"" and the other accessory neurons.

Edit: 
Source: I'm a 2nd year physiology student / L. Sherwood ""Human Physiology: From Cells to Systems"" 8th Edn.
Also, I've left out a protein or two in the signalling pathway, but in terms of answering the question I think it's sufficient?",null,0,cdlu2z4,1rc9cl,askscience,top_week,2
rupert1920,"You can read about it in the Wikipedia article for [phototransduction](http://en.wikipedia.org/wiki/Phototransduction).

Basically, it involves the photoisomerization of a molecule of [retinal](http://en.wikipedia.org/wiki/All-trans_retinal) - the incoming photon excites an electron in that molecule, and allows for a rotation of one of the double bonds. This causes a conformational change in the protein that houses it - [rhodopsin](http://en.wikipedia.org/wiki/Rhodopsin) - which then sets of a cascade that leads to the nerve signal.",null,0,cdlu45k,1rc9cl,askscience,top_week,1
zalaesseo,"Stationary electrons generate electric fields. No current

Moving electrons generate magnetic fields. Constant current

Accelerating electrons generate electromagnetic fields. Changing current.

Oscillating circuits generate changing currents, and thus electromagnetic fields. 

Then we can either amplitude or frequency  modulate signals into the carrier wave.",null,0,cdludhm,1rc957,askscience,top_week,3
drzowie,"zalaesseo gave a nice answer.  Another, perhaps even more simple, is:

Shaking electric charge produces electromagnetic waves, just as shaking objects in air produces sound waves (the physics is different but the fundamental waveness is the same).  We make electromagnetic signals by shaking electric charges.  

Every time you switch on or off a circuit, you create electromagnetic waves - so there's a lot of electromagnetic noise all around us.  To punch through that interference, devices that signal each other (like radios, or phones, or wifi units, or whatever) pick a particular frequency and shake electric charges at almost exactly that frequency.  That's like cutting through the noise of a large room full of drunk people, with a particular clear tone (say, from a flute).  You can discern the flute even though it's not any louder than the people, because all the energy is concentrated into a single tone.

Small variations in the strength or frequency of the electromagnetic signal carry the information people want to transmit.  For an old-style AM radio, the strength of the signal indicates where the speaker cone of the receiver should go.  The station gets ""louder"" and ""softer"" very rapidly to move your receiver's speaker cone around, forming sound waves.  FM radios use small changes in the pitch of the radiofrequency tone to control where the speaker cone should go.  Digital radios like wifi or modern cell phones use variants on those two strategies to communicate streams of bits.  Those bits encode the sounds and internet packets that are being transmitted.
",null,0,cdlw28q,1rc957,askscience,top_week,2
chrisbaird,"Causing electric charges to oscillate (bump, jiggle, shake, collide, change energy levels, transition between states, etc.) *always* creates electromagnetic waves, and not just in fancy circuits. The chair you are sitting on is emitting electromagnetic waves right now (mostly in the infrared) because its electrons are slamming together due to thermal motion. There are many ways to oscillate electric charges, and so there are many ways to create EM waves:

- heat them up so they collide more (incandescence)
- shake them up and down a wire using applied voltages (antenna radiation)
- excite electrons into different states and then have them transition back down (lasers, fluorescence, phosphorescence, gas discharge, chemiluminescse)
- send electrons passed a system of magnets that makes them wiggle or circle quickly (free electron laser, cyclotron radiation)
- smash a charged particle at high speed into a material (Brehmstrahlung)

If you want to send a signal on an EM wave, then you need to precisely control the shape of the EM wave. You must therefore precisely control the movement of electric charges. Electric circuits come in handy for that.
",null,0,cdmu1if,1rc957,askscience,top_week,2
PENIS_VAGINA,"Interesting question. I'll try to answer this. 


First off sugar does not neutralize the acidity. However you are correct that sourness is based on H+ ion mediated receptors on the tongue (TRP family receptors). 


I don't believe the sugar changes our tongues ability to taste sour because sweet and sour receptors are distinct from each other so there should not be competitive inhibition of sour receptors when a substance that activates sweet receptors is present. In fact, it is possible that the low pH from the sour substance is enhancing ligand binding to sweet receptors. This is what happens when you ""Taste Trip"" with miraculin. 

My thought is that you are experiencing both tastes simultaneously and therefore your brain in an attempt to process both tastes is not pronouncing the sour taste as strongly as it would if sour was the only taste happening. 

I am looking for a source to confirm this and will update if I can find something. 

Edit: May as well add (because its a common misconception) that the classic ""taste map"" that shows different areas of the tongue to have varying densities of different kinds of taste receptors is FALSE.",null,0,cdlxpl2,1rc6y9,askscience,top_week,1
Ruiner,"I'm having trouble understanding exactly what you mean. But let me try to answer:
(First, keep in mind that if there is a liquid inside, things are a lot more complicated because of convection, which makes thermalization faster than if there was only conduction)

You start heating the bottom of the point. At this point, that surface gains heat through the source and loses heat through conduction, fine. If you leave the source on, it will at some point reach a steady state. This state is not really thermal equilibrium, but it's a state in which you have a constant flux of heat from the bottom to the top, so there is a temperature gradient. You know that you reached the steady state because the temperature of everything inside the pot is no longer increasing, but all the energy coming in is compensated by energy going out. (btw, if you're trying to boil a pot of water, you never get to see the steady state, since the average temperature keeps on increasing).

Once you turn off the source, the steady state will now be approximately thermal equilibrium. It would be thermal equilibrium of the pot was a closed system, but since it interacts, it will still have a temperature gradient - the coldest parts being those that interact the most with the outside. ",null,1,cdlt6gm,1rc3z2,askscience,top_week,2
Quant_Liz_Lemon,"You need to be awake during brain surgery in order to ensure that nothing important is damaged during the procedure. This is especially important if surgery is being conducted near functional areas of the brain. Otherwise, you might risk permanent brain damage. Depending on what area of the brain you're near, a surgeon might ask you to make specific movements, count, say specific phrases, etc, while performing the surgical procedure. 

[source: Mayo Clinic](http://www.mayoclinic.org/awake-brain-surgery/about.html)",null,1,cdluba1,1rc3d0,askscience,top_week,7
U235EU,"I work for a medical device company, one of our products is used to treat movement disorders by deep brain stimulation. The patients are conscious during the implant so that the doctor can insure the proper location of the stimulating leads by direct feedback from the patient, and by neurological monitoring. See this video:

http://www.youtube.com/watch?v=lUG8iFxukig",null,0,cdm1wfl,1rc3d0,askscience,top_week,1
Polyknikes,"Some surgeons still perform operations while the patient is awake but a more modern technique is to use various functional brain imaging techniques prior to the surgery while asking the patient to perform certain tasks, seeing which areas of the brain light up near the tumor, and then avoiding those areas during the surgery.  With this technique the patient can be fully sedated during surgery.",null,2,cdlxqlz,1rc3d0,askscience,top_week,1
brawnkowsky,"not all of it is rinsed, some of it passes into the epithelial cells.  once it passes through the epithelium, it is able to inhibit cyclooxygenase, an enzyme necessary for the creation of prostalgandins.  prostaglandins are involved in pain and vasodilation, two major components of inflammation, which is what a 'breakout' is

source: wikipedia, student of medicine",null,1,cdm050e,1rc303,askscience,top_week,2
owaisofspades,"yup, you have about 10 layers and 5 different types of nerve cells  on your retina, and the photoreceptors are one of them. They have pigments on them (rhodopsin for rods and iodopsin (?) for cones) which respond to light, and transmit the signal through the rest of the layers into your optic nerve",null,0,cdlos9c,1rc0vh,askscience,top_week,4
dakami,"Genes (OPN1SW, OPN1MW, and OPN1LW) express one of three proteins, called opsins.  When an opsin is hit by a photon, it has some chance of isomerizing.  This process causes an electron to be released, and the chance is related to two things:

1) The wavelength of the photon
2) Which opsin is hit

Your ""red"" opsins are more likely to be isomerize in response to ""red"" light.  (This is an extreme oversimplification.)

As you can imagine, nerves are quite good at responding to electrical signals, and creating complex computational cascades.  So vision stops being about light really, really early.

Once isomerized, the spent opsin is recycled.  Your retina is actually among the most (if not the most) metabolically active portions of the body.  Really, your eyes work a lot more like living film than CCD/CMOS silicon.",null,0,cdlqugn,1rc0vh,askscience,top_week,2
EdwardDeathBlack,"You are asking two questions. The first one is what do thermal fluctuations look like in a  DNA molecule. The second is can DNA melt. 

First, the double helix of DNA is indeed held together by Hydrogen bonds. Above a certain temperature, the energy is high enough to overcome those bonds. The double helix melts and the two molecules separate. This has been extensively [studied](http://en.wikipedia.org/wiki/DNA_melting) and is an essential tool of biotechnology (and especially of [PCR](http://en.wikipedia.org/wiki/Polymerase_chain_reaction) ). If you have any background in the thermodynamics of how, say, water, freezes and melts, you will find it very similar to look at DNA melting. 

Now, to address the first point, what does a hot (but not melted) DNA molecule look like. First, a DNA molecule will [""ball up""](http://en.wikipedia.org/wiki/Random_coil), it doesn't stay a nice stretched thing. That little ball jiggles and wiggles along with the solvent, exhibiting [Brownian motion](http://en.wikipedia.org/wiki/Brownian_motion) . It will also have [thermal phonons](http://en.wikipedia.org/wiki/Phonon), of the [1-dimensional kind](http://en.wikipedia.org/wiki/Phonon#One_dimensional_lattice). 

If the temperature is high enough, but not enough to melt completely the DNA, there will also be [""bubbles""](http://www.bu.edu/meller/research_bubbles.html) forming alongside the DNA of partially melted areas.. These will have a limited lifetime, will occur predominantly in areas of weak Hydrogren bonding (AT rich areas), and have a lot of roles to play in living organism. 

Anyway, all that is already a lot. Maybe I'll let you read what I linked to and ask more questions rather than drone on...",null,0,cdloddj,1rbz3z,askscience,top_week,2
SingleMonad,"You don't have everything pinned down in your question.  Namely you need to know how big an ice cube.  Given a 500 ml glass of water, it may well be impossible with what would be considered a conventional ice cube.  

Assuming that no heat is lost to the environment, setting the heat lost from the water equal to the heat gained by the ice, using values in Wikipedia for specific heat and heat of fusion for water, and assuming the final temperature is 0 c, I get that the original ice cube warms by the following amount:

**Delta T = 200 M/m** (degrees c), where M is water mass, and m is ice mass.  Since the final temperature (0 c) is 271 degrees above absolute zero, it had better be a pretty big cube.

Disclaimer:  my arithmetic sucks.  Don't bet anything important on the basis of my answer.

Edit:  also assumed the water was initially at room temp (21 c).",null,0,cdlodsp,1rbyvm,askscience,top_week,11
Farnswirth,"Due to the conflict between /u/just_helping and /u/InexplicableContent results I did my own calculations, which came out to:

167000+2100(Initial water temperature, C) = 576.03(mass of ice) - 2.11(mass of ice)*(temperature of ice, K)

with all masses in grams.

When the initial water temperature is 25C and the temperature of the ice is absolute zero, the required mass of ice needed to freeze the whole cup is: **381g, which agrees with** /u/just_helping 

With an initial water temperature of 21C, and ice temperature of absolute zero, the mass of the ice is: **336g**

For a more realistic scenario, you could assume the initial water temperature is at 1C, and the ice has been cooled with liquid helium (4K).  This gives an ice mass of: **298g**  For liquid nitrogen (77K) the ice mass is: **409g**",null,2,cdlr4kj,1rbyvm,askscience,top_week,4
null,null,null,0,cdlpv6f,1rbyvm,askscience,top_week,2
tysongrey,What temperature is the room?,null,0,cdlnj0m,1rbyvm,askscience,top_week,1
Gradri,"That depends on the initial temperature of the water (probably about 21 C), and the mass of the ice cube.",null,0,cdlnxxe,1rbyvm,askscience,top_week,1
EdwardDeathBlack,"I am not sure you are asking a ""clean"" question. 

First,  let us assume room temperature (293K) and atmospheric pressure (101300Pa or so) for most/all of our discussion. Under different condition, water cohesion can change drastically. 

In the absence of gravity, water will easily for an orb the size of a baseball. SO water cohesion

In the presence of gravity, water reacts like a viscous material. It means the rate of deformation of the water is proportional to the stress applied. Note that a viscous material will always deform in the presence of an arbitrarily small stress.  So if I consider this, you are asking how to make water into an elastic material, instead of a viscous material. Freezing seems the obvious answer. 

There are also polymeric materials that you can add to the water to bind it in place and make it an elastic material, [Jell-O](http://en.wikipedia.org/wiki/Jell-O) being particularly well known. 

No sure I answered your question, but as I said, I am not sure I completely grok'ed your intent...",null,0,cdlo75d,1rbyp8,askscience,top_week,1
SmellyRaghead,"Yes, you can alter the surface tension by using surfactants. As for making a giant ball of it, probably not, unless you were in zero gravity.",null,0,cdlr0fy,1rbyp8,askscience,top_week,1
dreemqueen,"If you dissolve ionic compounds like NaCl into water, the water becomes more polar and cohesive.  If you dissolve sucrose which is an organic covalently bonded molecule in water,  the water becomes less polar and less cohesive.  You can see the difference if you measure the wetting or contact angle.   Salt increases the contact angle, sugar decreases it.  This is the best way to measure relative surface tension.",null,0,cdmqcd4,1rbyp8,askscience,top_week,1
goingforth,"It is likely a combination of both, but your former suggestion likely has the largest effect. Moving clouds tend to maintain a consistent shape over relatively short periods of time, and the distortions that do occur are often just that, and don't involve the addition or subtraction of parts of the clouds (again, this is on a short time frame) Likewise, clouds tend to move faster with higher wind speeds, suggesting a correlation. Your second suggestion is responsible for clouds forming, changing shape, and disappearing, but not as much for the movement of clouds.",null,0,cdlptxo,1rbymp,askscience,top_week,2
neverlupus16,"The question here is phrased incorrectly: when it's a cold morning and you breathe out, you see the water in your breath. But it's not water VAPOR. Water vapor is gaseous water. When it is cooled sufficiently, it will condense into liquid water. 

That condensation is what is actually happening when you see your breath. You see VERY tiny droplets of liquid water suspended in the air. They are so small that the current of air from your lungs will suspend and move them in front of you AS IF they were a gas.",null,1,cdlokzd,1rbyel,askscience,top_week,18
MarkWW,"Supersaturation is the point at which you can no longer stir sugar into your tea, because the water - at that temperature - can no longer dissolve solids into it. Cool it further and more of the sugar comes out as a solid.

The same happens with cold air &amp; foggy breath. Think of other forms of condensation - water appearing on the outside of a cold glass, or on a window. This is water vapor (gas) from the air turning into a liquid, because the surface is so cold that it turns the gas into a liquid.

The same is true for the breath you can see on cold day. The air outside your body is so cold, that the warm, moisture laden air inside your body instantly turns into liquid - albeit, in very tiny droplets.

Something similar happens when you create rock candy &amp; I encourage you to make rock candy with your brother... Because, yum.

Basically warm gas (or liquid) can support lots of gaseous liquid (or solid) in it because it's so chaotic and full of energy that it can keep the liquid (or solid) a gas (or liquid).

As it cools down, it loses energy and more of the matter that's right at the edge of being a gas/liquid reverts to the state you normally associate it with at that temperature. In other words, water isn't always a liquid between 0C and 100C - it's in a constant state of flux, with more water being gaseous the warmer it gets.

For more mind blowing facts - research Swamp Coolers, which cool the air by adding water to it.",null,0,cdlr7a9,1rbyel,askscience,top_week,4
chrisbaird,"Water vapor always comes out of your mouth in gas form (water vapor) when you breath. Water comes out of your mouth in liquid form when the air is cold enough to condense the gaseous water that you always breath out into small drops of liquid water, which we see as a white cloud of steam. ",null,0,cdmu6un,1rbyel,askscience,top_week,1
therationalpi,"Common misconception about sonic booms: You don't just create a boom at the moment when you ""break the sound barrier."" In truth, for the entire duration that an object is traveling faster than the speed of sound, it generates a shock front. This is more apparent when you look at an image of the effect. Here's some [Schlieren photography](http://library.thinkquest.org/12228/Page4.html) of a supersonic jet. The dark colored bow shocks that start in front of the plane are the ""sonic booms"" that it's creating.

So, in answer to your question, nothing particularly special happens when you reach 2 or 3 times the speed of sound. Indeed, you will still be creating sonic booms at those speeds, but you would likewise be continuously generating booms if you were traveling at 1.2x the speed of sound.

Hope that helps!",null,5,cdlnxzg,1rbw8z,askscience,top_week,24
rocketsocks,"You don't create a sonic boom only when you pass the speed of sound.

When you are traveling at or above the speed of sound you trail a cone shaped shock front which travels at the speed of sound. You usually only here the sonic boom once as an observer because the shock front only passes over you once.",null,2,cdlo7lt,1rbw8z,askscience,top_week,5
iorgfeflkd,"The sun's power is distributed evenly over an area. When you are focusing it with a magnifying glass, you are essentially taking all the light that reaches an area the size of the lens and combining it to a region the size of the focus.",null,2,cdlpmaf,1rbw5f,askscience,top_week,3
Mazetron,The sun is a powerful source of light on many wavelengths.  The mafnifying glass just focuses light.  You could burn something with an electric light and a magnifying glass of the light was poweful enough.  I have done it with a laser pointer.,null,0,cdlqqmx,1rbw5f,askscience,top_week,1
chrisbaird,"Light carries energy. Energy causes damage to materials when it is absorbed in a given area faster than it can be dissipated from that area. The ability of energy (and therefore light) to damage materials therefore is a result of a high energy delivered per unit area per unit time, which we call power density. A lens does not create energy, it just focuses the energy so that there is a high power density in one small region and lower power densities in surrounding regions. If the power density is high enough in the focal region, the material heats up faster than it can cool to its surroundings, and its temperature steadily rises. With high enough temperature, materials will melt, burn, ignite, etc.

Light can be focused because it is a wave and waves can be bent (refracted) at the interface between two optically dissimilar materials (such as glass and air). ",null,0,cdmufp9,1rbw5f,askscience,top_week,1
neverlupus16,"It's the infrared light of the sun. The fusion reactions of the sun release electromagnetic radiation across the spectrum. You have infrared photons, which explains why you feel heat. You have visible light, which explains how we can see things using sunlight. And we lastly have ultraviolet light, which is how our skin burns and tans.

The wave nature of light allows it to be refracted by traveling through a medium such as glass. By focusing the wave (and all of it's energy) to a single point, you change the energy from being diffuse and spread out to being concentrated in one small region. It is now easy for the infrared energy to be transferred to another object. If the energy is transferred faster than it can be dissipated, the object will increase in temperature and possibly reach ignition.",null,8,cdlon4k,1rbw5f,askscience,top_week,1
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,top_week,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,top_week,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,top_week,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,top_week,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,top_week,2
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,top_week,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,top_week,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,top_week,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,top_week,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,top_week,2
blacksheep998,"I encountered [this study](http://www.ncbi.nlm.nih.gov/pubmed/12042333) a few years back about whales. They found that the energy demands of accelerating their huge bodies to lunge-feeding speeds to fill their massive mouths with seawater and krill are extremely high.

Massive whales are up against the law of diminishing returns, where each unit they increase in size gives them less and less of a return on that investment.

More info: http://www.americanscientist.org/issues/issue.aspx?id=8779&amp;y=0&amp;no=&amp;content=true&amp;page=2&amp;css=print",null,1,cdlt3h1,1rbu2s,askscience,top_week,6
Izawwlgood,"Some of it has to do with what other organisms are doing. For example, one of the precipitous drops in insect size was due to the development of birds, which are superior fliers. Bison twice as large may be too cumbersome to evade a pack of wolves, for example.",null,1,cdlxbjl,1rbu2s,askscience,top_week,3
promega,"The largest discovered organism on earth is actually a fungus.  

""The discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, believed by most to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. Based on its current growth rate, the fungus is estimated to be 2,400 years old but could be as ancient as 8,650 years, which would earn it a place among the oldest living organisms as well.""

Source: http://www.scientificamerican.com/article.cfm?id=strange-but-true-largest-organism-is-fungus

In theory such an organism could continue to grow until it exhausted one of its resources.",null,0,cdmqwxw,1rbu2s,askscience,top_week,1
null,null,null,3,cdls9rb,1rbu2s,askscience,top_week,1
deruch,"You need to be more careful with your terms.  How are you defining size?  By mass, volume, area, etc.?  Do you really mean ""animal"" instead of ""organism""?  In terms of organisms, I can make the claim that the largest one is an [aspen forest](https://en.wikipedia.org/wiki/Pando_%28tree%29) in Utah, or maybe a [fungus colony](https://en.wikipedia.org/wiki/Largest_organisms#Fungi) in Oregon.",null,17,cdlse7h,1rbu2s,askscience,top_week,11
patchgrabber,"Copper is toxic to algae and bacteria in moderate to high concentrations. A copper plate should have an antimicrobial effect, yes; brass doorknobs are known to have antibiotic properties and sterilize faster than, say, an aluminum doorknob.",null,0,cdm5yoq,1rbtxa,askscience,top_week,2
abstrusey,"""Normal"" is usually defined by sampling lots of apparently healthy individuals and then using statistics to calculate an expected range into which the large majority will fall. This range is often referred to as a ""reference interval."" For aspects of physiology (e.g. heart rate, respiratory rate, temperature, blood pressure, etc.), these ranges are typically set as a standard that you read in a book and/or memorize. For test values (e.g. sodium/potassium/pH value of the blood, red blood cell count, etc.), they are often established individually by the testing facility, and they typically print that range next to the value of the sample they analyzed. 

A reference interval can be established by collecting test results from at least 50 healthy adult animals/humans. In this example, therefore, the range would only represents adults. At least 50 juveniles (and you'd have to define the age range) would have to be collected to have a new ""normal"" range determined. Two statistical analysis techniques are used, based on the distribution of the data. For normal distribution (also called gaussian distribution), there is a ""bell curve"". The data has ""variance,"" which is a representation of how widely scattered most of the animals are when compared to the average of all of them (e.g. they may all be tightly clustered near the average (low variance), or they could be very high and very low away from the average (high variance)). This variance is represented by a number called the ""standard deviation."" In gaussian and non-gaussian distributions, you can use the average  2 standard deviations to select for the ""middle"" 95% of the data. The lowest and highest numbers that you collected are now your range, and putting a dash between them makes it a reference interval. In vet med, we have reference intervals for most parameters for dogs, cats, horses, cattle, goats and alpacas, and many exotic species as well. 

It is very important to remember, though, that if the range only represents 95% of the samples -- 100% of which were apparently healthy -- then you should expect about 5 of 100 individuals to have values outside of the range and *still* be okay.",null,0,cdlm8h3,1rbtd9,askscience,top_week,10
gettingoldernotwiser,"Another way of defining ""abnormal"" is increased risk of death/disease.  People with high blood pressures can have an increased risk of cardiovascular disease, stroke or death compared to those with normal blood pressure.

Similarly, abnormal lab values (glucose, cholesterol) confer a higher risk of disease than normal ones. ",null,0,cdlrvxj,1rbtd9,askscience,top_week,1
Asrat,"In hospitals and other medical facilities, we typically take an individuals blood pressure every 4 to 8 hours and start generating a baseline for the individual. Your primary care physician does this as well to identify any changes. We also ask what you normally run if you are competent enough to answer. Using that information, for example, we can tell if you normally run 95/55 and today your pressure is 125/75, something is wrong and we need to identify that.
",null,0,cdlsaif,1rbtd9,askscience,top_week,1
Jyesss,"Antibiotics target certain essential enzymes and proteins that bacteria have but humans do not, thus giving their specificity. These proteins are coded from genes, so in a round about way, yes, antibiotics are based off of genetic targeting in that they disrupt the proteins that those genes create. Bacteria generally do not become antibiotic resistant by changing the protein the antibiotic targets because this would probably kill the organism in the process. Instead, they evolve new genes that code for enzymes that will break down the antibiotic before it can harm the bacteria. ",null,0,cdme16k,1rbr4k,askscience,top_week,1
leftnuttriedtokillme,"There are a couple of hurdles.  For one thing, you can't target ""just bacterial DNA/RNA"", since the actual structure isn't any different from human DNA/RNA.  You can go after the proteins that form the nucleic acids, because those are slightly different.  And there are some antibiotics that do exactly that.  But it's rather difficult to target bacterial nucleic acids themselves specifically.

There has been some research into using artificial nucleic acids to target specific segments of a genome and basically turn it off, but I don't think they've been able to get it into a practical form that could be used in medicine quite yet.

There has, however, been success in using DNA to determine what antibiotics a particular organism is susceptible to.  Currently one of the normal steps in treating a bacterial infection is to culture and ID the organisms involved, and also to perform a susceptibility test on them, which determines the effectiveness of a number of common antibiotics.  

Traditionally this was done by exposing them to the antibiotic at certain concentrations and seeing whether or not it grows.  The new process allows us to look for the genes responsible for certain types of antibiotic resistance.

The best example of how this is useful would be for MRSA.  If a doctor suspects a MRSA infection, he would traditionally do a culture, which would take 1-3 days to tell him anything.  The new genetic method could tell him if it was *Staph. aureus* in a few hours, and whether or not it was the resistant form at the same time.",null,0,cdmhr6b,1rbr4k,askscience,top_week,1
Farnswirth,"You can absolutely see one atom thick graphene sheets, this is one of the things that makes it such a remarkable material.  Just look at [This picture](http://3.bp.blogspot.com/-2UU-zkkrxm0/UPXIX6at0RI/AAAAAAAACNc/LsODcsw_1Oo/s1600/High-Speed+Graphene+Circuits.jpg), [the bottom of image a in this picture](http://www.nature.com/srep/2012/120921/srep00682/images/srep00682-f2.jpg), and [image c at the bottom left in this one which shows two single-atom graphene sheets layered on top of eachother on a PET film](http://www.nature.com/nnano/journal/v5/n8/images/nnano.2010.132-f2.jpg).  

We can see graphene when it is only one atom thick because it is exceptionally good at absorbing light.  Graphite and graphene are extraordinarily good at absorbing light mainly because the individual sheets of graphene have an extremely low [band gap](http://en.wikipedia.org/wiki/Band_gap) (pretty much 0eV).  This is also one of the reasons it is such a good electrical conductor as well.  Notice diamond has a very high band gap, which makes it transparant to visible light and a poor electrical conductor.  Finally, take a look at the [physicochemical properties of Boron Nitride](http://en.wikipedia.org/wiki/Boron_nitride#Physical).  While its structure is remarkably similar to graphene, it has a high band gap, which makes it appear white or transparent.  ",null,2,cdlranh,1rbqyc,askscience,top_week,7
organiker,"If your application calls for multiple layers, then you grow it as multiple layers.

If you're making electronic devices, you lay it flat on a substrate like glass, silicon or plastic, then add electrodes.

If you're making a coating, you layer it on whatever you want to coat. Probably with a protective layer on top.

If you're making batteries or capacitors, you mix it with your electrode materials.


",null,0,cdlo8z4,1rbqyc,askscience,top_week,1
EdwardDeathBlack,"You incorporate it into another matrix. You layer it in a sandwich of other materials...all that to exploit either its electrical or mechanical properties. 

It is not a first...here is a [TEM](http://large.stanford.edu/courses/2007/ap272/kimej1/images/f2.jpg)  of a gate oxide (a semiconductor term about one of the common layer in modern electronics) that is only a few atoms thick. Or here is  a [STEM](http://origin-ars.els-cdn.com/content/image/1-s2.0-S0038109805008914-gr1.jpg) of a quantum well of the type often use in vcsel and the like. Also atoms thick...

In a world where ""everything"" is made of atoms, all we have is made of ""stacks"" of single atom. The art is to stack them in the right order to make new properties arise that are useful to us. ",null,2,cdlnz09,1rbqyc,askscience,top_week,2
Platypuskeeper,"Alcohol (ethanol) and water are miscible fluids because the -OH group of the ethanol molecules forms [hydrogen bonds](http://en.wikipedia.org/wiki/Hydrogen_bond) with water, just as water does with itself. 

Hydrocarbons, as in oils, only bond negligibly with water molecules, which means it costs energy to separate the water molecules from each other and stick a hydrocarbon molecule between them. The lowest energy situation is if you lump all the hydrocarbon molecules together and minimize the contact area with the water, that is, form an oil phase.
",null,3,cdm0b7z,1rbl1i,askscience,top_week,5
LoyalSol,Density has nothing to do with it unless the two materials don't mix.  In the case of oil and water there is a mismatch in how the molecules interact with each other so they prefer to stay separated.   ,null,1,cdm0l07,1rbl1i,askscience,top_week,2
TehMulbnief,"Couple of reasons. The most direct reason is that alcohol and water are miscible. They mix together very nicely, so much so that you can't tell them apart once they are mixed. The resulting solution is sort of like a metallic alloy. When you look at a stick of bronze, you don't see copper and iron, you just see nice, homogenous bronze.

The other less obvious (and maybe a bit niggly) reason is the Second Law of Thermodynamics. This law introduces the idea of ""entropy"" or randomness of a system. When you add water to alcohol, the tendency is going to be for the two to mix, until the alcohol molecules are perfectly and uniformly distributed amongst the water molecules and vice versa. Once you're at this point, the system is going to stay that way because that's the most stable way for the system to be.",null,0,cdmx3br,1rbl1i,askscience,top_week,1
rupert1920,"Ash is material is leftover material that cannot undergo further combustion - in the case of complete combustion. This is the white ash you often see when something is completely burnt. An example of this would be [wood ash](http://en.wikipedia.org/wiki/Wood_ash), which consists of the trace mineral compounds in wood that don't combust, such as calcium and potassium (which comes from the word [potash](http://en.wikipedia.org/wiki/Potash), with a similar etymology in how it was made historically - from ash).

Since different compounds will have different amounts of these non-combustible elements, they will naturally have different ashes.

This concept also has direct impact in [gravimetric analysis](http://en.wikipedia.org/wiki/Gravimetric_analysis) - which relies on very careful measurements of weight of a compound before and after some process. You can attempt to weigh out your analyte, then completely burn it at high temperatures, and weigh out the ash that is left over in order to find the chemical composition of the analyte (for example, finding the stoichiometric ratio can give you oxidation states). You'll also find filter papers used for this purpose to be ""ashless"" - it produces purely gaseous products under combustion so it won't skew the results of such measurements.",null,1,cdlzjby,1rbk7h,askscience,top_week,3
DeadVirus0,"Earth's solstices and equinoxes are based exclusively on its 23.5 degree axial tilt.

For example, our next perihelion will be on January 4th, 2014 while the upcoming winter solstice will be on December 21st. This means that the northern hemisphere's longest night is 2 weeks away from the Earth's closest orbital position to the Sun. [Source: US Naval Observatory](http://aa.usno.navy.mil/data/docs/EarthSeasons.php)

Additionally, 4 Vesta is, relative to other celestial bodies, small and amorphous.

So, I guess what I'm trying to say is that your assumption that solstice/equinox are related to perihelion/aphelion is flawed.",null,0,cdm19tt,1rbj3h,askscience,top_week,1
bohr_exciton,"Air is not completely transparent, however we perceive it as such for two reasons:

1) The various molecular and atomic species that make up the atmosphere can only absorb light at specific frequencies. If you look at the absorption spectra of the species making up thee majority of the atmosphere, such as nitrogen (N2), you will see that for most of them there will only be negligible absorption in the visible part of the spectrum. 

2) The density of air is so low that we can't perceive the minimal absorption that does take place. The best example of this is water. Water actually has a blue color, which you can see by say looking at the sea. However water only absorbs weakly in the visible, such that to observe this color light needs to pass through meters of water before we can observe its absorption. If you just look at a glass of water, it will just look transparent. Now the density of water vapor in the atmosphere is much much lower than in the glass, and therefore you can't possible see this effect in air with your bare eyes.

Finally, as for why you can see the atmosphere from space, or why the sky looks blue, the answer is that while the atmospheric gases do not significantly absorb visible light, they can scatter the light. Moreover, the major scattering mechanism is so-called Rayleigh scattering, which occurs more strongly for higher energies. Because of this blue light is scattered more than red light, which makes the atmosphere look blue. ",null,1,cdlvjoo,1rbhiu,askscience,top_week,4
chrisbaird,"Air is not perfectly invisible. Look at the sky during the day. That blue color is air. Look at a distant mountain on a clear day and compare it to a very close hill. The distant mountain will seem to be covered with a whitish haze. That haze is air. Clean air is composed of very small molecules that are very far apart. For this reason, you need a lot of air in order to see it with your naked eye.",null,0,cdmul9m,1rbhiu,askscience,top_week,2
Qvanta,"Materia excerts light ""color"" only if it has absorbed and emited the light struck by it. Here is the catch, each materia has a specific amount of energy in form of light it accepts. If the light shining on a materia is below or above the requierd amount of energy it needs to possess, light will just pass by.

The atmospheric light comes from the scattering of the blue spectrum when the light passes through our atmosphere. So you actually dont see any color only the smudging of the suns blue light.  ",null,2,cdluxwu,1rbhiu,askscience,top_week,1
meerkatmreow,"Black both absorbs and radiates better.  The net result can be a lower equilibrium temperature.  For example, the SR-71 was originally not painted black.  However, it turned out that the black paint lowered the temperature enough to allow the structure to be lighter even with the extra weight of paint, resulting in a net savings.",null,0,cdm5fnf,1rbfb3,askscience,top_week,2
miltoniousbastard,If you have ever touched a tinted window you will notice that it is noticeably hotter than non tinted windows on the same vehicle. The tinted window is absorbing most of the light (energy) which keeps it from being transferred to your cars interior. The heat on the window is radiated to the surrounding outside air vs. the heat radiating off your seats/dash/whatever else is in your cars interior.,null,0,cdm0fzy,1rbfb3,askscience,top_week,1
Greyswandir,"I think perhaps this is a good lesson that aesthetic concerns sometimes (often?) trump engineering ones.  I think the real answer to your question is that many people don't want other people peering into their cars, and as you pointed out, other coatings are more expensive.  I imagine that concerns about heat probably weren't too much a part of the design process.

And, as plenty of other people have pointed out, having the light absorbed by your windows which are in contact with the outside and have a high surface area, keeps the heat from instead being trapped inside your car.",null,0,cdmvmx1,1rbfb3,askscience,top_week,1
sharp12180,"Electrons are bound to an atom and his bond has a certain amount of energy. If the is an incident photon with energy greater than or equal to this bond energy, it can cause the electron to become unbound. When enough electrons become unbound, you have a current. In a solar panel, some of the photons from the sun have the right energy to dislodge electrons in the panel which creates usable electricity. ",null,0,cdm0x32,1rberh,askscience,top_week,2
rupert1920,"You should also take note that it is the [photovoltaic effect](http://en.wikipedia.org/wiki/Photovoltaic_effect), not the photoelectric effect, at play here (hence a ""photovoltaic cell"").",null,0,cdm8iuh,1rberh,askscience,top_week,1
owaisofspades,"It's a bit complicated, but I can give you the general idea.

During glucose metabolism you go through glycolyis, which gives you pyruvate. Pyruvate then gets converted to acetyl CoA. Now here's where it gets tricky. If you need energy, your cells are going to send the acetyl CoA through the Citric acid cycle to make loads of ATP. If you don't need energy, the acetyl CoA gets shunted to fatty acid synthesis.

YOu have some enzymes involved that activate the acetyl CoA into malonyl CoA, and this allows you to add an acetyl CoA to it, forming a chain. After a certain number of extensions you get palmitate, which is a freefatty acid, which can then be modified to form other fats or phospholipids, or which can be esterified with a glycerol molecule to form mono-, di-, triglycerides.

This takes place in the liver. Your liver then packages the TAGs into lipid containers (chylomicrons) and then put them into the blood stream. Then lipoprotein lipase on the surface of adipocytes grabs the chylomicron and pulls the TAGs out of them.

Now for the second part of your question. fatty acids don't get converted back into glucose as far as I know, but into acetyl CoA through a process known as beta oxidation (for short and medium chains, I don't remember the long-chain degradation process ATM). The acetyl CoA is for the liver to use (it's not water soluble so can't get transported in blood). For other tissues, the liver converts the acetyl CoA from Beta oxidation into ketone boies, which are water soluble and can be transported to other organs such as the brain through the bloodstream.

Hope this helps, feel free to ask for clarification. As a med student with my biochem final coming up soon i'm trying to keep my knowledge from disappearing haha",null,0,cdmbnot,1rbdz6,askscience,top_week,2
iorgfeflkd,"Freefall doesn't get rid of tidal gravitational fields. The difference in Earth's gravitational field between the front and back of the ship could be detected with precise instruments, and would be absent in intergalactic space.",null,1,cdm16ju,1rbd4j,askscience,top_week,4
AbouBenAdhem,"When the temperature of a gas changes, its density changes. When its density changes, its index of refraction changes. When light passes from one substance to another substance with a different index of refraction, it travels at a different speed; and when it meets the interface between two such substances at an angle, it bends.",null,3,cdlghd7,1rbb96,askscience,top_week,23
Polyknikes,"I don't think anyone will have an exact answer to your question because it would depend on which cell you are talking about, how well stocked they were beginning the fasting period, and how much energy they are being asked to expend over a given amount of time.  As an alternate answer I will discuss what happens during starvation which hopefully will answer your question.

In the normal course of starvation we first burn carbohydrates which basically refers to glucose.  Glucose is stored in many cells, but particularly in the liver hepatocytes, in the form of glycogen.  The breakdown of glycogen is referred to as glycogenolysis which releases glucose into glycolysis for energy production.

After all the glycogen is used up the body begins catabolizing (burning) proteins.  Protein catabolism involves the breakdown of bodily proteins into amino acids for use in synthesizing more glucose in a process called gluconeogenesis (which takes place in the liver).

Sometime during protein catabolism your body will begin the process of lipolysis which is the breakdown of triglycerides into fatty acids which can be oxidized into multiple units of Acetyl-CoA and fed into the TCA cycle for energy production, bypassing glycolysis.  High rates of fatty acid oxidation will lead to ketogenesis, or the creation of ketone bodies.  Ketone bodies are another form of high-energy molecule like glucose which can be metabolized by many tissues and are especially important for the brain during the starvation state as it has high energy demands and cannot directly metabolize fatty acids.

TLDR: During starvation your cells first use glycogen (stored glucose), then catabolize proteins into glucose, then burn fats in the form of ketone bodies.

Hope this answer helps.
",null,0,cdlx6pc,1rb8zj,askscience,top_week,2
owaisofspades,"a-helix and b-sheets are due to hydrogen bonding. Random coil is due to hydrophobic reactions if I remember correctly. There's also di-sulfide bridges, which only cysteine can form. 

Whether a protein will form an a-helix or a b-sheet depends on the sequence (the amount of residues between the two interacting residues determines which one will form)",null,0,cdmbq05,1rb8il,askscience,top_week,2
reddishpanda,"Short answer: the types of interactions needed to produce a secondary structure can be made by a variety of amino acids. 

Helices and sheets are made by the backbone interactions between the amino acids of a proteins, so almost anything goes (except for proline, which would be too geometrically limited to form a helix or sheet and is limited to loops and random coils). You can use many combinations of different amino acids to make one structure or another, but interactions among the side chains of amino acids (where you will find hydrogen bonding and hydrophobic interactions as well as salt bridges between say glutamate and lysine). 


It might be information overload, but try playing around with a protein database like [RCSB](http://www.rcsb.org). If you just come up with an amino acid sequence of your own creation, you can use [Phyre2](http://www.sbg.bio.ic.ac.uk/phyre2/html/page.cgi?id=index) to get a prediction of what your protein might look like and the secondary structures that might form it. ",null,0,cdmedut,1rb8il,askscience,top_week,2
stevenstevenstevenst,"I do not know a lot about what the atmosphere is actually like within the ISS, but I can tell you that in a pure oxygen environment, as was used on some NASA flights, those on board have reported that it is difficult to hear one another.  This is perhaps related to the pressures in these vessels more than atmosphere itself, as a denser atmosphere transmit sound waves more readily.  

Kind of just a curiosity, so I apologize if your question was not answered in its entirety.  The atmosphere being thicker or thinner is related more to pressurization than the content- although content does play a role.",null,0,cdmgh7o,1rb86r,askscience,top_week,2
Proxymace,"At high concentrations oxygen is toxic to organisms, at 100% humans get a ""high"" from breathing it. This tends to make people calmer and is why planes deploy oxygen masks and not just air masks. For the iss I imaging its due to weight limits on supply ships. Carrying a load of nitrogen that you don't need isn't very good.",null,1,cdmjb36,1rb86r,askscience,top_week,3
chrisbaird,"""I saw somewhere that in the ISS and other stations that they have a 100% oxygen environment"" You saw wrong. The composition of air on the ISS is definitely not 100% oxygen, and is in fact intentionally regulated to match the composition of air on earth's surface, with about 21% oxygen:

http://www.nasa.gov/missions/highlights/webcasts/shuttle/sts112/iss-qa_prt.htm

There are a couple of reasons for this:

- Pure oxygen is highly flammable. NASA unfortunately learned this the hard way with the Apollo 1 accident
- High oxygen concentrations are unhealthy to humans as our bodies have evolved to work most efficiently with the oxygen levels common at earth's surface.",null,0,cdmutcf,1rb86r,askscience,top_week,2
Smoothened,"Microorganisms are the main cause of spoilage, but there are other ways in which food can go bad. This varies both with the content of the food and the environment it is exposed to. For example, food that is partially composed of water can dry out. Protein and other molecules in the food would undergo degradation, which will result in changes in both texture and taste. Another thing that can happen is the separation of ingredients in different phases. These changes would be less obvious in foods that are homogeneous and mainly composed of simple molecules such as sugars. All in all, the changes would generally occur more slowly than in the presence of microorganisms. Most likely the resulting food wouldn't make you as sick as if you ate food spoiled by bacteria or fungi. ",null,0,cdmc7d1,1rb7jz,askscience,top_week,2
StarshipEngineer,"There is no such thing as terminal velocity in an airless environment. It doesn't matter what the terminal velocity of an object in air is, if there is no air for the object to interact with through friction, the object will keep accelerating as it falls until it hits a solid surface.",null,1,cdlfu8p,1rb77o,askscience,top_week,12
PepperJack_delicacy,"The ""pins and needles"" feeling is referred to as **paresthesia**, which occurs when a nerve and the arteries supplying the nerve are compressed. This prevents the nerve from carrying electrical impulses that transmit the sense of touch, which you feel as ""pins and needles"". 

It's harmless when it occurs transiently (like after you fall asleep on your hand) because once the pressure is removed, blood supply to the nerve will be restored. However, there are certain chronic cases that are indicative of neurological disease or more traumatic nerve injury, which are more serious.

In the case of a blood clot, though, you are created a plug in an artery that prevents blood flow to a tissue. There is nothing you personally can do (such as switch body positions) that will remove the clot. Furthermore, in the case of a heart attack or stroke, you are preventing blood supply to the heart or brain--two of the most important organs in the body, which absolutely need blood for you to survive. 

So overall, it's true that ""pins and needles"", strokes, and heart attacks are caused by circulation problems. However, in the case of ""pins and needles"", the blockage is readily reversible and the organ that is losing blood supply is no where near as important as the brain or heart. 

Sources:

http://www.ninds.nih.gov/disorders/paresthesia/paresthesia.htm

http://www.urmc.rochester.edu/encyclopedia/content.aspx?ContentTypeID=1&amp;ContentID=58",null,0,cdm065z,1rb75x,askscience,top_week,2
brawnkowsky,"'pins and needles', which is called Paresthesia, is caused by pressure applied to a nerve.  This inhibits its ability to conduct a signal and eventually leads to a limb 'falling asleep' (a dead leg).  a lack of blood circulation does not cause this.

lack of circulation (specifically tissue perfusion) results in a failure to deliver oxygen to systematic cells and to remove metabolic waste.  This lack of oxygen causes cells to create energy through alternative pathways that create acidic products (lactic acid is common), causing acidosis.  Eventually, the cells will die because they fail to maintain the pH needed to function; this is called Ischemia.  Ischemia in organs can lead to organ failure, which will kill a person.  ",null,0,cdm0add,1rb75x,askscience,top_week,1
adoarns,"Pinch a nerve long enough, and it becomes permanently damaged. The nerve fibers will wither back, and you will lose sensation until the nerve grows back (about 1 mm/day). Even then you may expect that not all the withered fibers will find their way back to their proper locations.

Heart muscle and brain tissue are much more metabolically active, and take much less time without proper blood flow to be permanently damaged.

Unlike peripheral nerves, brain tissue and heart muscle does not grow back. You lose it, and it's gone for good.",null,0,cdmc3ds,1rb75x,askscience,top_week,1
_Momotsuki,"If you squeeze just one vessel of your arm, there are many collateral vessels to take up the slack and perfuse the rest of the arm. This principle applies to your heart because there are only a few main vessels that supply the heart (with great variation between individuals, and very simplistically, there's the left and right coronary arteries with the left splitting very early to become the left anterior descending and circumflex artery). Any blockage in one of those 3 vessels will cause ischaemia and lead to death of the tissue areas that are meant to be supplied. Indeed, there are collaterals present in the coronary circulation. However, these are usually functionally non-patent and can't really help with distributing blood because they have such a small lumen. This is especially the case when there is ischaemia due to a sudden thrombo-embolic event. With that said, in *some* cases where there is a slow build up of atherosclerotic plaque within arteries, you can get slow opening of these collateral vessels to help perfuse the heart.",null,0,cdn9eo8,1rb75x,askscience,top_week,1
sharp12180,"The force of gravity due to an object with mass is never zero. It can be very small if the object has very low mass or you are far away from the object. For a galaxy, which is very massive, you can get far enough away where the force of gravity acting on you is negligible but it will never be zero. In fact, the gravitational force is proportional to the inverse-square of the distance between two objects, meaning if you double your distance from an object the force of gravity decreases by a factor of four. Still, this force will never be zero.  ",null,0,cdm0ted,1rb6tc,askscience,top_week,3
Ejb90,"In a way, no - the gravitational field produced by mass is infinite, so the field has an effect throughout the universe.
In another way, yes - there exist points called Lagrangian points where the gravitational fields of objects cancel to produce zero net acceleration. These are quite common, there are several around earth, being utilised for their stability. Though here the rotational effects of the entire galaxy have been ignored and considers the Sun-Earth system as relatively stationary.",null,0,cdm0vay,1rb6tc,askscience,top_week,2
ShwinMan,"Short answer: no

LADEE is a small spacecraft, it's only 2.37m high and if it were visible then so should all the other spacecraft there now as well, including the Apollo descent stages, lunar rovers, debris etc. Even Hubble wouldn't be able to make it out.",null,0,cdmxh0z,1rb6hn,askscience,top_week,1
Ejb90,"I think you're misunderstanding the structure of an atom. The ""shells"" the electrons occupy aren't determined by their distance from the nucleus, but by the relative energies of the electrons in each. Why this happens is explained by quantum mechanics.
In the classical case, the ""radius"" of the first shell would simply expand with the nucleus, though that would never be a real problem - the nucleus is 10^-15m across and the atom 10^-10 - that's 10,000 times bigger, so any atom that big would be inherently unstable.
However in reality the electrons aren't hard point of mass whiz zing around the nucleus, they're a ""cloud"" of delocalised charge with certain characteristics, also described by quantum mechanics. If you want to know more there are loads of online resources about this.",null,0,cdlgd0k,1rb1wp,askscience,top_week,7
Platypuskeeper,"Why would it 'force electrons out of the lowest shell'?
",null,1,cdlg0vx,1rb1wp,askscience,top_week,3
thumbs55,"Basically no.

If you have a very small object (less than the DeBrogle wavelength of the electron ~ 10nm) then the electrons in the outer shell of this object may have descrete energy levels and be treated as a giant atom.

This is refferd to as a [Quantum dot.](http://en.wikipedia.org/wiki/Quantum_dot)

So these quantum dots are much larger than the lowest shell of a Hydrogen atom but still dsiplay quantum properties.

Sure the nucleus is like 15 orders of magnitude smaller than the electron shells but if we had a nutron star and some how made it positevly charged, and placed an electron to see if it would orbit it:

Then we would find that the electron and the proton in the star join together to create a neutron due to all of that pesky gravity.

If the system is changed such that the lowest shell no longer exists then the next lowest shell by definition becomes the lowest. This is just simantics and is a bad argument since the lowest energy is an s shell and the next lowest is a p shell and behave measurable differently. And if you did get rid of the first s shell then the new lowest shell would be an s shell at a higher energy.",null,0,cdlgc3l,1rb1wp,askscience,top_week,1
miczajkj,"In fact this is something, that can happen but not for electrons. 

You may know, that there are three generations of quarks and leptons, while our universe consists mostly from the first generation (up- and down-quarks + electrons and electron-neutrinos). But there are certain natural processes, that produce particles from higher generation, for example the cosmic radiation in the earth's athmosphere. 
The electron equivalent in the second generation is the muon - and if you construct muon-atoms your question gets important!

Like the others already mentioned, 'orbiting' electrons (or muons) don't really orbit the nukleus: their position gets described by a probability density; those densities are the absolute squares of the particles wave function. 
If you calculate those wave functions you find, that the probability density of the ground state has it's maximum at the classical Bohr radius, that can be calculated by using a semi-classical force approach (for muonic hydrogenium): 

Let the Coulomb-Force (c/r^2) equal the centripetal force (p/mr). Also keep in mind the the uncertainty principle: pr ~ . It follows:

c/r^2 = ^2 /mr^3 
r = /mc

Now, because the mass of the muon is 200 times bigger than the mass of the electron, therefore it's Bohr Radius is 200 times smaller and if you also consider the finite circumference of the proton it is possible, that the muon may have finite probability to be found inside of the nukleus, especially if you talk about heavier nuklei. 

The main consequence is a displacement of the energy niveaus. This fact was used, to find the much used formula

r = 1.2 fm * A^(1/3) 

for the radius of the nukleus depending on the Mass number. 

Another consequence is the increased probability of decays like the K-capture, where a muon from the K-shell reacts with a proton:

^- + p -&gt; n + _",null,0,cdlsooe,1rb1wp,askscience,top_week,1
Ruiner,Only by the amount of mass it consumed. ,null,0,cdlt15h,1rb1bz,askscience,top_week,4
Infinite_Ambiguity,"Steven Hawking has shown that black holes also radiate energy because of quantum effects near the event horizon.  Consequently, black holes might increase in mass/energy by the amount of mass/energy consumed, but they are also radiating mass and energy (equivalent by e=mc-squared) and thereby also simultaneously evaporating to some degree. .  ",null,0,cdltx5v,1rb1bz,askscience,top_week,2
snusmumrikan,"[This paper](http://www.academia.edu/372962/Giants_on_the_landscape_modelling_the_abundance_of_megaherbivorous_dinosaurs_of_the_Morrison_Formation_Late_Jurassic_western_USA_) discusses it for herbivorous large dinosaurs and says a few tens of each per sq km. 

[This one](http://earth.geology.yale.edu/~ajs/1993/11.1993.06Farlow.pdf) discusses the limiting factors in population density of large carnivores and the balance between food availability and having enough of each species to ensure a mating fequency high enough to avoid extinction.

It seems your question can't be answered reliably as so much depends upon the preservation of dinosaur remains for that. Looking at the variables and comparing it to modern-day predators might be the best option?",null,3,cdlea2h,1raxgi,askscience,top_week,9
invariance,"No. It is simply inconclusive, because there exist series which diverge and series which converge for which the ratio test gives 1. For the series a_n = 1/n, the sum diverges. For the series a_n = 1/n^2, the sum converges.

The ratios are not 1 in magnitude, except in the limit. Note also that both ratios converge to 1 from below. A refined version of the ratio test will tell you that if |a_{n+1}/a_n| &gt;= 1 for sufficiently large n (so the ratios converge to 1 from above), the series will diverge. The proof for this follows from what you have already said. So in fact, the only inconclusive case is if the limit of the ratios converges to 1 from below.

So the short answer is that if the limit of the ratios converges to 1 from below, the test is inconclusive because there are examples which converge and examples which diverge. ",null,1,cdlud5b,1rawis,askscience,top_week,6
iCookBaconShirtless,"The issue is not which direction that the limit is approached, as you conjecture.  While approaching 1 from above assures divergence, approaching from below does not assure convergence.  A simple example is the harmonic series.

They key to understanding the ratio test is to more precisely understand this statement that you made:  

&gt; I understand what happens when the limit is smaller than 1 (every element of the series is smaller than the previous by a factor L, hence the series tend to stabilize and converge).

The fact that every element of the sequence is smaller than the previous by a factor of L (at least in an asymptotic sense) implies that the sequence converges to zero **exponentially fast**.  Basically, it looks like a geometric series at large n.  This exponential convergence of terms is enough to ensure that the series converges, but it's strictly stronger than what is needed.  Plenty of series have terms that converge more slowly than exponentially, but still converge as a series.  Any p-series with p&gt;1 for example (e.g., 1/n^2 ).

tl;dr Ratio test determines exponential convergence of terms, which is more than is needed for convergence of series.",null,0,cdlv91r,1rawis,askscience,top_week,4
wgunther,"In order to understand the ratio test you have to understand the proof. The proof is if the ratio a_{n+1}/a_n goes to L&lt;1 then eventually the of consecutive terms is less than 1-epsilon for some small but positive epsilon, and therefore, the series is smaller than the geometric series a(1-epsilon)^n for some suitable a. Thus it converges by direct comparison. 

If L&gt;1 then you can do the same thing but with 1+epsilon. 

The problem is this proof doesn't work if L=1. The ratio test will only work when the sequence of the summand of the series converges *faster* than something geometric whose series converges or *slower* than something geometric whose series diverges. In the case when L=1, one can not compare the series to a geometric series that converges or diverges. ",null,0,cdmmc8x,1rawis,askscience,top_week,2
onyablock,"People can become immunocompromised through various ways including pregnancy, viral infection, steroid treatment etc. etc.

The reason it is important when considering vaccinations is that being immunocompromised can increase your risk of obtaining the infection to which you're being immunized against (if the vaccine is 'live' virus) or being vaccinated can be pointless as no immunity will actually be gained from the vaccine.

The effects of an immune deficiency on vaccines varies greatly depending on the vaccine and its application regiment. For example it could be recommended that you receive multiple booster shoots or don't receive the vaccine at all if you are immunocompromised. 

In the case of the flu shot, obtaining the killed-virus vaccine won't allow you to actually contract the flu, however the chances of you generating good and long lasting immunity to the flu is reduced if you are immunocompromised. Obtaining the 'live' attenuated vaccine would not be recommended for immunocompromised patients as there is potential for infection, hence why the box is there.

I hope that makes sense.

Source: 4th year immunology student.",null,3,cdlcf5d,1rawff,askscience,top_week,18
tthershey,"Live vaccines are generally contraindicated in immunosuppressed patients because these patients will not be able to mount a sufficient immune response to the vaccine.  Consequently, the live vaccine could induce an infection.

Inactivated or component vaccines won't put immunosuppressed patients at risk, but the patients might not gain much protection from the vaccine.  This is because immunosuppressed patient's won't be able to generate antibodies to help fight off future infections.

Immunosuppressed patients most certainly need to get vaccinated because they are at greater risk for getting serious complications from infections.  It's important for health care providers to know a patient's immune state in order to deliver the right kind of vaccine.",null,0,cdlk7wj,1rawff,askscience,top_week,3
Chandley54,"There're three reasons why you need to be immunocompetent when vaccinated. 

1. It is rare nowadays for a vaccination to be live &amp; pose a direct threat, some of them may revert to being harmful, for example attenuated vaccines commonly have a harmful gene removed before they are given to the host so they can be dealt with by the immune system without any significant risk. In some cases the process may not be perfect and so some live unattenuated virus may get into the vaccines, so although in theory the virus should not be able to establish itself and replicate, it sometimes can and is therefore much more hazardous to immunocompromised patients. The same is true for killed virus vaccines - in theory all of the virus should be dead, but whatever process they use to kill the virus (e.g. exposure to UV light) may be ineffective, so again, live virus may end up being present in the vaccine and lead to an active viral infection.
2. If a patient is immunocompromised, there is a possibility that the body will not be able to respond effectively and generate the necessary memory lymphocytes for the vaccine to be effective. This would mean that should the person then encounter a live version of the virus, the vaccine would not have provided them with any protection at all as the memory cells were not generated at the initial vaccination. so are not available to respond.
3. Although the vaccine you received was killed, the form you filled out was probably written by some legal team at some point in history when they were using a different type of vaccine, so they're basically just covering their own backs, and if may be expensive for them to change the documentation.

I would imagine it is likely a combination of the above 3 reasons!
Hope this was helpful.

Source: Veterinary Surgeon/Anatomical Pathologist",null,2,cdlcbxz,1rawff,askscience,top_week,3
Urgullibl,"The point of a vaccine is to stimulate your immune system into producing antibodies against whatever it is you're being vaccinated against. If your immune system is suppressed, there is no point in vaccinating, as the reaction would not result in protection from infection.

In case of the flu vaccine, we're talking about a dead vaccine, i.e. there are no attenuated whole viruses in it, hence there is no risk of getting the flu from it. In case of vaccines containing whole attenuated viruses, an immunosuppressed patient might get sick from such a vaccine.",null,1,cdlk4zy,1rawff,askscience,top_week,2
killer_alien,basically there are two types of vaccines: antibodies and dead virus or w/e cells. Anti bodies is a short term vaccine which basically grands you immunity whereas dead cell ones stimulates your body to create antibodies which is a long lasting treatment. (This is the most basic i can put it w/o making to wrong and confusing),null,0,cdmicxd,1rawff,askscience,top_week,1
SingleMonad,"What you're asking has been done.  It called a pulsed laser.  You are imagining making little pulses of red light no more than a few femtoseconds long.  The output is not red.  It is *white*.  The light has a broad spectrum, centered about red, the width is inversely proportional to the pulse duration.

Wiki ulatrafast, supercontinuum, frequency comb, laser.  If you disperse the output, you will see the individual colors in the laser.

http://grad.physics.sunysb.edu/~meardley/fiber/weiss5.jpeg",null,2,cdloqib,1rawb8,askscience,top_week,12
null,null,null,15,cdlfycp,1rawb8,askscience,top_week,8
SingleMonad,"What you're asking has been done.  It called a pulsed laser.  You are imagining making little pulses of red light no more than a few femtoseconds long.  The output is not red.  It is *white*.  The light has a broad spectrum, centered about red, the width is inversely proportional to the pulse duration.

Wiki ulatrafast, supercontinuum, frequency comb, laser.  If you disperse the output, you will see the individual colors in the laser.

http://grad.physics.sunysb.edu/~meardley/fiber/weiss5.jpeg",null,2,cdloqib,1rawb8,askscience,top_week,12
null,null,null,15,cdlfycp,1rawb8,askscience,top_week,8
Sunscorch,"Ok.

For a second, forget that the Earth and Moon are orbiting, and picture the Earth falling straight down towards a stationary Moon. At the very start of our thought-experiment, the Earth is also stationary and its oceans are equally spread out across the entire surface.

The Earth then begins to accelerate towards the Moon, as is begins to fall. The ocean nearest the Moon experiences the greatest acceleration, because it is closer and therefore is exposed to the greatest force. Likewise, the ocean furthest from the Moon experiences the lowest acceleration for opposite reasons. The Earth itself, of course, experiences an average acceleration.

So! The Moon-side ocean begins to move away from the surface of the Earth, as it is accelerating faster than everything else, creating the bulge that is easiest to understand. The ocean on the other side, however, gets ""left behind"" because it is accelerating slower than the Earth. Essentially, the Earth is moving away from it, which creates the opposing bulge on the far side from the Moon.

That is how it works in our thought-experiment, but exactly the same thing happens in orbit because the Earth and Moon are essentially falling towards each other, and are constantly accelerating because of the constant change in direction. That is why there is a tide on each side of the Earth.",null,2,cdlgab2,1ravmk,askscience,top_week,12
thumbs55,"Excellent question:

[Sixty symbols did a video on it.](http://www.youtube.com/watch?v=YO3eDYzFp8Y)

[In this image](http://hendrix2.uoregon.edu/~imamura/123cs/lecture-2/tides.jpg)

Thinking in vectors the first image is of 5 vector forces acting toward the moon. Note that the farther they are the weaker they are and the top and bottom are not parrallel to the other three.

But we dont live in space we live on the earth so if the earth moves we move with it and dont notice so for that reason in the second image the middle vector is subtracted.

Giving a zero vector in the middle since anything minus itself is zero.

The top and bottom vector are pointing in since they were already pointing a little bit in.

The vector nearest to the moon is a lot shorter but still pointing toward the moon.

And most interestingly the vector farthest from the moon is actually pointing away from the moon.",null,2,cdlgink,1ravmk,askscience,top_week,7
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,top_week,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Pikur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Pikur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,top_week,8
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,top_week,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Pikur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Pikur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,top_week,8
ColdWaterEnthusiast,"Good question. It is almost pointless to try and find out whether the Sahara is growing or shrinking, because of the sheer size of the desert (as well as demarcating what exactly constitutes 'desert'). In the late 90s to mid 2000s, the thought was that the Sahara desert was expanding southwards by a certain extent each year. This was of course somewhat exaggerated. On the other hand, so is the perception that the deserts are 'in retreat' as these articles seem to imply.

The thing is, between the late 70s to late 80s, there was a significantly dry period in the Sahel region (the transition zone between the Sahara and the savanna) which exacerbated the effects of desertification, leading to the perception in the 1990s-2000s that the desert was indeed expanding. However, over the last 15-20 years in terms of precipitation, the region has been in a comparatively very wet period. Relative to the significant drought the area previously experienced, it may seem that the deserts are in 'retreat' but arguably that is essentially what is expected in terms of how vegetation has responded (it gets a bit more complicated because some of the previous mesic vegetation has been replaced by xeric vegetation in certain areas so while it is greener, it is not quite the same)

I hope this help. I could go into more detail but this should give you an idea of how complicated it is to understand.

Source - My dissertation research has broadly to do with understanding how vegetation responds to moisture events",null,1,cdlfsdd,1ravcx,askscience,top_week,6
PepperJack_delicacy,"Smoking cigarettes essentially speeds up the aging process of the skin, which leads to wrinkles. The main reason this happens is because **nicotine** is a **vasoconstrictor**, meaning that it narrows the blood vessels that supply the skin. When you impair blood flow, it has a harder time getting oxygen and absorbing nutrients such as **Vitamin A**, which normally keeps the skin hydrated and protects it from oxidative damage. Furthermore, it will have a harder time repairing wounds and synthesizing a protein called **collagen**, which keeps the structure of the skin intact. 

In summary: 

*Smoking cigarettes ==&gt; decreased blood flow to skin ==&gt; skin gets less oxygen and nutrients ==&gt; skin has a harder time protecting itself from damage and repairing wounds.* 

Sources:

http://dermnetnz.org/reactions/smoking.html

http://www.mayoclinic.com/health/smoking/AN00644",null,2,cdll8tr,1rav3s,askscience,top_week,7
iorgfeflkd,"The way it would manifest itself is through a change in density, but because water isn't very compressible, its density only changes by like 3% even at the bottom of the ocean, so the transmission of light from a source at that depth isn't too much different.",null,2,cdlclzj,1rarw1,askscience,top_week,5
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,top_week,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,top_week,2
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,top_week,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,top_week,2
Chandley54,"Yes, in veterinary medicine we can categorise hyperthyroidism based on where the tumour is within the hypothalamic-pituitary-thyroid axis. It slightly alters our treatment options for it, but as far as I know, hypothalamic/pituitary surgery is almost never carried out in animals clinically, so many general practice vets will simply treat the general hyperthyroidism. I imagine in human medicine where surgical removal of the tumour is a possibility they are a lot more rigorous with determining where the cause is in all cases!",null,3,cdlcihm,1raq8w,askscience,top_week,6
mklevitt,"theoretically, yes, but i don't know that one has ever been proven or written up in the human scientific literature. secretory (hormone-producing) tumors of the hypothalamus are rare in general, and to find one that specifically comes from clonal expansion of TRH neurons, and then actually secretes? i couldn't find an example among the human literature. a much more common (albeit still very rare) cause of hyperthyroidism is TSH-secreting pituitary tumors, like you said. most hypothalamic tumors cause dysfunction by damaging/suppressing 'normal' functioning hypothalamic nuclei. thus common hypothalamic tumors like craniopharyngiomas can cause hypopituitarism (from suppression of nuclei that stimulate the pituitary) but not hyperpituitarism.",null,0,cdmppa5,1raq8w,askscience,top_week,1
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,top_week,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,top_week,5
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,top_week,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,top_week,5
rupert1920,"Disclaimer: I'm not an expert in protein folding, and would love to be educated more on the matter.

Check out [Levinthal's paradox](http://en.wikipedia.org/wiki/Levinthal's_paradox), which states that the sheer degrees of freedom a protein has makes it highly unlikely to spontaneously fold into the energetically stable conformation. Which means that there must be other effects - other than thermal sampling - that ""guide"" the protein into the proper conformation. This could be chaperones, or stable intermediates.",null,1,cdl9son,1raob3,askscience,top_week,5
Osymandius,"As /u/rupert1920 has said, Levinthal's paradox states that it would take longer than the age of the universe for a polypeptide of 100 residues to fold into the correct configuration by ""trying"" all phi/psi angles. Anfinsen et al (Anfinsen, CB et al (1961) ProcNatAcadSci 47, 1309-1314) showed that primary structure directly determines tertiary fold, therefore trying all the possible angles is not required - as was evident by proteins folding on a biological time scale and life existing to begin with!

Your question, therefore, is a very good one: if primary structure does determine tertiary structure, why bother with chaperones?

Ken Dill answers this nicely in an excellent review [here](http://www.nature.com/nsmb/journal/v4/n1/abs/nsb0197-10.html). He encourages you not to think of protein folding pathways, rather protein folding tunnels where there exist multiple routes to the most stable configuration (i.e. the lowest energy). Through these multiple routes can exist ""energy traps"" - local energetic minima which require energy input to overcome such that the polypeptide can reach the final fold. This is where chaperones come in. You can sort of think of them as proteins which recognise improper folds - say extensive hydrophobic stretches facing the surface of a protein - pull them apart and say ""try folding again"". This is why we don't get trapped in an infinite loop: who folds the chaperones? The chaperones aren't specific to any one protein, rather they recognise common folding mistakes.

Edit: His review really is rather good - if it's trapped behind a pay wall, reply and I'll get it for you - he explains it much better than I do.",null,1,cdlatkk,1raob3,askscience,top_week,5
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,840,cdlaovd,1raftj,askscience,top_week,2810
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,18,cdl8mfx,1raftj,askscience,top_week,115
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,12,cdlb9od,1raftj,askscience,top_week,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,top_week,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,0,cdl9sks,1raftj,askscience,top_week,20
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,3,cdldi1w,1raftj,askscience,top_week,12
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,1,cdl8ok4,1raftj,askscience,top_week,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,5,cdl8sip,1raftj,askscience,top_week,7
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,top_week,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,top_week,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,top_week,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,top_week,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,top_week,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,top_week,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming V_l &lt;&lt; V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as V -&gt; V_l) [m/V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,top_week,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,6,cdl9v5f,1raftj,askscience,top_week,7
null,null,null,0,cdl8egp,1raftj,askscience,top_week,1
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,840,cdlaovd,1raftj,askscience,top_week,2810
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,18,cdl8mfx,1raftj,askscience,top_week,115
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,12,cdlb9od,1raftj,askscience,top_week,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,top_week,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,0,cdl9sks,1raftj,askscience,top_week,20
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,3,cdldi1w,1raftj,askscience,top_week,12
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,1,cdl8ok4,1raftj,askscience,top_week,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,5,cdl8sip,1raftj,askscience,top_week,7
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,top_week,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,top_week,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,top_week,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,top_week,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,top_week,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,top_week,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming V_l &lt;&lt; V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as V -&gt; V_l) [m/V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,top_week,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,6,cdl9v5f,1raftj,askscience,top_week,7
null,null,null,0,cdl8egp,1raftj,askscience,top_week,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,top_week,20
null,null,null,7,cdl88u6,1rae6m,askscience,top_week,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,top_week,1
null,null,null,3,cdlc4wt,1rae6m,askscience,top_week,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,top_week,20
null,null,null,7,cdl88u6,1rae6m,askscience,top_week,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,top_week,1
null,null,null,3,cdlc4wt,1rae6m,askscience,top_week,1
iorgfeflkd,"Matter doesn't contain gravitons, and if gravitons are used to describe gravity they are neutral particles that are their own anti-particles.

Anti-matter is expected to behave normally in gravitational fields, although it's hard to get enough of it in one place to test this. There is an experiment at CERN working on it, [here is their 1990s looking website](http://aegis.web.cern.ch/aegis/).",null,1,cdl9d7n,1ra79d,askscience,top_week,14
stevenstevenstevenst,"As explained, antimatter will behave classically with respect to gravity.  If, however, you want something with the repulsive properties described, you need negative energy (or, correspondingly negative mass).  While this is a rather abstract idea, negative energy has in fact been observed-notably in the Casimir effect.  

In another example, lasers are reported to produce energy in an oscillation of positive and negative energy.  Assuming this is correct, it is easy to imagine a series of rotating mirror which could then separate the negative and positive energy.  This is the beginning of a discussion on how to expand a singularity in a rotation wormhole blahblahblah time travel etc.  For this process it would be necessary to isolate negative energy.

http://en.wikipedia.org/wiki/Casimir_effect",null,0,cdmfqwz,1ra79d,askscience,top_week,1
Izawwlgood,"To your general question, yes! A number of groups are working on culturing induced pluripotent stem cells into whole organs, by either decellurizing pig equivalents and seeding the collagen matrix with the iPSCs, or causing the tissues to grow onto some other matrix. It's pretty cool and exciting work!

But as to why some transplants need to be grafted to a person; the host body will facilitate the vascularization of the tissue, and 'feed' it, which will allow it to grow and remain healthy. At a later date, when the graft is ready, surgeons will move it. 

Ever seen someone with a crush wound have their hand stitched to their chest or side? It's to promote blood flow into the damaged tissue. Similar idea.",null,0,cdl8bvk,1ra6bi,askscience,top_week,5
KarlOskar12,"The reason they grew the nose/facial skin on the person is because a complex medium is required to grow human tissue. It *can* be done in a lab, but it is much easier to attach it to the person and use their blood supply to get all the required nutrients for growth.

As to your main question: we will absolutely be able to have organ farms. Take this [mouse](http://en.wikipedia.org/wiki/Vacanti_mouse) for example. A live host is - for now - the easiest way to do it but that most definitely does not have to be the case.",null,0,cdl8szw,1ra6bi,askscience,top_week,2
now_you_listen_here,"I'm going to start by explaining it very simply, and then if you want more details I can expand on it!



As far as the part about being non-reactive to CMV (cytomegalovirus), that simply means that you do not have antibodies to the virus.  This means that you have not been exposed to CMV before.  It doesn't mean that your blood ""type"" is non-reactive; we don't speak in terms of blood *types* being reactive or non-reactive to a virus.


As far as your O(-) status, I'm afraid she gave you some misinformation.  That does not mean that your blood is ""good"" for newborns.  It doesn't mean it is ""bad,"" either.  The (+) or (-), as you probably know, is referring to the presence or absence of Rh factor, which is a protein on the outside of red blood cells (you are either Rh+ or Rh-).  You DON'T have it.  Therefore, if your blood was exposed to blood that DID have it, your immune system could form antibodies against it, because it is recognized as something that is ""foreign"" to your body.


The importance of this would be if you would become impregnated by a male who is Rh(+).  In that case, the baby might inherit the gene from the father and also be Rh(+).  Therefore, if your blood (which is Rh-) was exposed to the baby's blood, whether during birth or some event during the pregnancy where bleeding occurred, your immune system could recognize the Rh factor on the baby's red blood cells and form antibodies, which would allow for an immune response at sometime in the future.  Your first baby would be just fine (the immune response takes too long that first time for it to put that baby in danger), but any babies you have after that are also Rh+ would be at risk of your immune system attacking their red blood cells, something we call [hemolytic disease of the newborn](http://en.wikipedia.org/wiki/Erythroblastosis_fetalis). 


There is good news, though!  We have developed something called Rhogam, which can prevent all this!  It's an injection that basically suppresses the mother's immune response to the Rh factor.  We can give it to pregnant women during their pregnancy and prevent the bad stuff that might have happened otherwise.  Science is great, isn't it?
",null,0,cdl8582,1ra6bd,askscience,top_week,6
abbe-normal1,"I'm going to expand on /u/now_you_listen_here since there are a few points I believe they didn't explain as well as could be.  CMV non-reactive as previously stated means you haven't been exposed to CMV.  The reason this is important is because CMV while causing a relatively minor infection in healthy adults is considerably more worrisome for immunocompromised individuals and babies.  See more information [here](http://www.mayoclinic.com/health/cmv/DS00938/DSECTION=symptoms).  The reason your blood is good for these individuals is that you can transmit the virus to them through a blood transfusion.  See [here](http://www.mayomedicallaboratories.com/test-catalog/Clinical+and+Interpretive/62067) again for more information.

As for O-, again as stated previously you lack the Rh factor that can cause an immune response in a recipient of a blood transfusion.  Your blood isn't just 'good for babies' (that's the CMV- part) you are actually known as the universal donor.  Your blood can be given to anyone regardless of blood type because you not only lack the Rh factor, but you also won't react with A or B blood because you being O lack those antigens as well.  Therefore your blood can be given to anyone A, B, AB or O without a life threatening reaction from their body.  In an emergency O- is given to a patient when there isn't time to check their blood type or until cross matched blood is available.  Also, others with your blood type can only receive O- so blood is harder to get because it is rarer.  

TLDR:  GIVE BLOOD!  O- blood is always in need and you would do a great service by regularly donating your blood to help others! ",null,0,cdl8f7o,1ra6bd,askscience,top_week,3
user31415926535,"""CMV non-reactive"" means that your body has never developed antibodies to cytomegalovirus (CMV) - that is, negative for both CMV-[IgM](http://en.wikipedia.org/wiki/Immunoglobulin_M) and CMV-[IgG](http://en.wikipedia.org/wiki/Immunoglobulin_G). A positive CMV-IgM test would mean you have a current CMV infection. A positive CMV-IgG test would mean that you had been infected some time in the past. What's important is that if you have been infected some time in the past, you still have some level of the virus in your body; a negative CMV-IgM test doesn't mean that you are entirely free of the virus. 

[CMV is an extremely common virus](http://www.cdc.gov/CMV/index.html) in humans; worldwide, 40% of adults have antibodies to CMV. [It's not particularly dangerous to healthy adults](http://www.mayoclinic.com/health/cmv/DS00938) (though it is implicated in some cancers and rare syndromes). If you notice an infection at all, it's usually similar to [mono](http://en.wikipedia.org/wiki/Infectious_mononucleosis). But to humans with undeveloped immune systems - newborn infants or immunocompromised people - it can be deadly. [In infants, CMV can cause blindness, deafness, neurological deficits, even death.](http://en.wikipedia.org/wiki/Congenital_cytomegalovirus_infection) 

So we need to be sure that blood used for infants has no trace of CMV in it. Hence, we look to people like you as a source of CMV-negative blood. 

[*Source: I'm not a medical professional, rather I'm an immunocompromised adult who has to know these things to survive.*]",null,0,cdl8jtj,1ra6bd,askscience,top_week,1
Ejb90,"The auroras depend on two main things:
1) Solar activity
2) latitude

Solar activity produces the solar wind - a stream of charged particles from the sun. These interact with the Earth's magnetic field, being drawn up along the field lines to where they intersect the Earth, at the poles. The interaction of these particles produces the lights. There are more particles when the activity is greatest. At the moment we are nearing a sunspot maximum, so the activity should be greater.
The Forster North/south the more field lines, so the more prominent the lights are.

The lights fluctuate daily, but there is a good chance of seeing them. Going in winter doesn't make too much of a difference, though it is darker for longer so they will be clearer for longer.",null,0,cdli1qs,1ra4dm,askscience,top_week,2
bohr_exciton,"That's actually a tricky question. The most direct explanation is that ice is slippery because it behaves as being wet, i.e. that there is liquid water between the bulk of the ice and an object gliding on the surface. However, what causes ice to be wet with respect to objects moving on it is still under debate. There was a Physics Today [article](http://scitation.aip.org/content/aip/magazine/physicstoday/article/58/12/10.1063/1.2169444) that came out a few years ago that neatly describes possible explanations. To briefly summarize them, they are as follows:

1) Pressure melting. Due to the fact that ice (at least the common form) is less dense than water, applying pressure reduces the melting point of the ice. To apply this example to say a skater, the idea is that a skate bay locally increase the pressure to a sufficient extent that the ice can melt, making it slippery locally and the water then refreezes after the skate passes. This is the most common explanation that has been invoked historically, but the problem is that it's not clear whether this additional pressure can be enough to melt the ice.

2) Frictional heating. When objects move across a surface there is (virtually) always some friction which results in local heating. Again, this heat may be enough to melt the ice.  This explanation, however, can't really explain why ice is slippery even for stationary objects immediately before moving. 

3) Ice may be intrinsically wet. In describing solids, we often tend to ignore surfaces to a first approximation, because this simplifies the description, but surfaces can behave very differently from the bulk. In the case of ice, it's been speculated that the different local environment of the first few layers of water molecules may result in this molecules being less strongly bound to the bulk than is the case for molecules within the body of this crystal. Because of this, these surface layers may behave as being liquid, which would then cause the ice to be slippery under all circumstances. ",null,0,cdl7ngd,1ra493,askscience,top_week,8
stillealles,"Because either 
a) a thin amount of water is on top (in the case of ice skating and such)
Or 
B) when you apply pressure to ice, it causes it to melt because of properties of water (this also applies to ice skating and such, but is easier to think about if you have a piece of ice on the floor and step on it and slip) 
Veritasium has a good video on it ",null,0,cdl8vq4,1ra493,askscience,top_week,1
jericho,"This is one of those great questions that results in arguments around the water cooler in the Physics Dept. Short answer; we don't know. [Here is a nytimes article that cover some bases.](https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;ved=0CFwQFjAH&amp;url=http%3A%2F%2Fwww.nytimes.com%2F2006%2F02%2F21%2Fscience%2F21ice.html&amp;ei=nPKQUt78OoXMqgHX2oGwCw&amp;usg=AFQjCNH9Hpt9VbWa5zPA0g36hCt64d71cQ&amp;bvm=bv.56988011,d.aWM)
",null,0,cdlac1j,1ra493,askscience,top_week,1
inmate992,"Self renewal is the ability of cells to continually replicate - preserving genetic information in all subsequent daughter cells. Stem cell reservoirs are usually tightly regulated in the human body to prevent them becoming cancerous.

As for the difference between Autologous and Embryonic, embryonic stem cells are much more pluripotent ie given the right external environment they can assume any cell type (eg cardiac, neurons, immune cells etc).
Autologous stem cells on the other hand are often precursor cells, for example OPC (oligodendrocyte precursor cells) cells are cells that are able to differentiate into immune cells, but their cellular fate is already decided, so I suppose they lose their pluripotency to an extent as their fate is already decided.

I hope this makes sense!",null,0,cdlgfhw,1ra41i,askscience,top_week,2
Osymandius,Could you clarify your question slightly? Partially differentiated haematopoietic stem cells in my hands happily divide lineally for 50 rounds of divisions in culture with no change in genotype or phenotype. eSCs are similarly capable. ,null,0,cdlayhh,1ra41i,askscience,top_week,1
NotFreeAdvice,"hmmm...well, it depends on the reaction that you are interested in.  Multiple things can happen.

1) You can just denature proteins at high/low pH.  It is *very* common to ""cook"" fish in acids (like lemon juice) or bases (such as lye).  These foods would be called ceviche or lutefisk, respectively.  
2) You can hydrolyze bonds that are holding in the flesh together, in which the acid or base acts as a catalyst for this reaction.  

There are probably more reactions of interest, but as a chemist, these are what immediately comes to mind.  ",null,0,cdl81pd,1r9zbg,askscience,top_week,3
Jyesss,"I think the question that you are getting at is ""what happens at the sub-cellular, or protein level, when a strong acid breaks down flesh?"" Proteins are made up of amino acids, and the way that these amino acids fold in 3-dimensional shape largely determines their function and appearance. Their shape depends on the stabilizing interactions between positive and negative charges on the amino acids, on hydrophobic interactions, and on hydrogen bonding. A strong acid increases the concentration of hydrogen ions and thus affects the protonation/deprotonation status of the side chains on the amino acids. The correct folding of the protein will be interrupted if a group that is normally deprotonated at physiological pH is changed to being protonated, thus resulting in the change in appearance. This same thing also happens when the temperature is increased, increasing the vibrational energy in the bonds which can eventually overcome the stabilizing forces at play. This is why an egg looks different when cooked. ",null,0,cdlf7c8,1r9zbg,askscience,top_week,1
iorgfeflkd,"You'd have to sum up the contribution from each height.

If you assume a constant density and cross section ( which isn't exactly true for a person, but you could make it more complicated if you want) would be the integral from r=6380 km to r=6580 km (or whatever) of GMdA/r^2 by dr where G is the gravitational constant, M is the mass of Earth, d is the density, A is the cross sectional area, and r is the radial position.

For heights much less than the diameter of the Earth, just using mg isn't too too wrong.",null,0,cdl94jj,1r9r7m,askscience,top_week,4
goingforth,"If we're going off of your theoretical proposition, the solution would be rather simple. If only half of the person's mass is influenced by gravity, then they would consequently have half their original weight. However, that concrete of a border doesn't exist. The above comments describe the mathematics of the issue, but the conclusion will be that, assuming the person's head is around low earth orbit, their weight will only change by a very small amount, as acceleration due to gravity doesn't change very appreciably from the surface to orbit (about 9.7 rather than 9.8) This effect will, of course, be amplified if the person's head is, say, on the moon, and will thus require definitive calculations in order to come to an accurate conclusion. ",null,0,cdlq4ba,1r9r7m,askscience,top_week,1
glittercheese,"People taking statins (cholesterol lowering medications) are often told to avoid grapefruit and grapefruit juice because consuming grapefruit can cause higher-than-expected levels of the drugs in the blood. Statins are metabolized in the liver by the same type if enzymes that metabolize grapefruit. If these enzymes are busy processing grapefruit, they are unable to metabolize the drugs, leading to more of the drug circulating in the body. This can cause an increase in adverse side effects of the drugs. 

I think the reason other fruits don't cause the same effects is that they are not metabolized by the same liver enzymes. 

This isn't overly scientific but I am a nurse and I wrote a short pamphlet on this subject when I was in nursing school. ",null,2,cdlao57,1r9qsz,askscience,top_week,8
justin3003,"Glittercheese gives a good basic overview of how the process works. However, to add, a substance called bergamottin and the related 6,7-dihydroxybergamottin are thought to be the culprits present within grapefruit specifically. 

These chemicals are potent inhibitors of CYP3A4, which is part of the cytochrome P450 family of enzymes. These enzymes are responsible for liver detoxification of substances in the blood (generally by making them more soluble for excretion in urine or bile). The exact mechanism isn't important, but what is important is that these guys are potent inhibitors of CYP3A4 and thus prevent CYP3A4 from detoxifying a whole host of important things, including a large number of pharmaceutical drugs. Given that many drugs have a fairly narrow therapeutic index (blood concentration range at which they are safe and have an effect), inhibiting their liver metabolism can cause blood levels to rise to toxic or even deadly levels quickly. Also, while they aren't the strongest inhibitors of CYP3A4, they are the most readily available, common, and seemingly innocuous and thus potentially dangerous.  

There is a good list of things CYP3A4 metabolizes at the bottom of this wikipedia article (under substrates): http://en.wikipedia.org/wiki/CYP3A4",null,0,cdli08l,1r9qsz,askscience,top_week,2
the_dan_man,... the same way any cultivated crop survives predation. Human intervention.,null,1,cdl45l4,1r9omj,askscience,top_week,2
the_dan_man,"http://en.wikipedia.org/wiki/Tunica_media

Muscle cells are arranged in a circular fashion around the vessel's interior. When these cells contract, the lumen (inside space) of the vessel grows smaller. It's a radial-ish contraction.",null,1,cdl4a5l,1r9ilc,askscience,top_week,2
frogdude2004,"A balloon is inflated when the pressure inside the balloon is greater than the outside: the air inside the balloon's walls stretches it while it pushes back. This continues until the combined force of the balloon and the air outside the balloon equals the force from the air inside the balloon. If the pressure inside the balloon equals the pressure of the outside of the balloon to start, then the balloon exerts no force: it is 'deflated'. If you were to put a balloon that was filled to a certain pressure into a box with the same pressure and then let the balloon open, the sum of the balloon and outside air force would be greater than the force inside, and it would then contract until deflated. I hope this answers your question. If it wasn't clear, please let me know.",null,0,cdl8ze2,1r9hf4,askscience,top_week,3
Naf623,"No, it cannot stay inflated without something blocking the neck. The rubber skin if the balloon will always be stretched and exerting a force to expel air. 
There is one exception; if the neck on the ballon is open to a source of gas, but the rest of the balloon is placed at a sufficiently lower pressure, then the balloon could be inflated by suction. ",null,0,cdlaggw,1r9hf4,askscience,top_week,2
iorgfeflkd,"Protons and neutrons are made of quarks, held together by gluons. Electrons and quarks, to the best of our knowledge, are fundamental, and so are neutrinos. With better experiments this understanding can change, but right now as far as we know, certain particles are fundamental.",null,2,cdkxbap,1r9g54,askscience,top_week,14
ekohfa,"Solar photovoltaic panels create current due to the [photoelectric effect](http://en.wikipedia.org/wiki/Photoelectric_effect).  Incoming photons cause electrons to jump across the energy ""band gap"" in a semiconductor, typically made of silicon.  The silicon consists of a positively doped layer and a negatively doped layer, just like a diode.  Metal leads are applied to each side to allow electrical current to flow to an external circuit.

Edit: To be clear, the metal leads are not oxidized; they are present only to carry the current created in the semiconductor.",null,2,cdkynzl,1r9es2,askscience,top_week,4
hal2k1,"&gt; Where are these electrons sourced from?

This is a misconception. Electrical current is charge flowing in a [circuit](http://en.wikipedia.org/wiki/Electrical_circuit).

&gt; An electrical circuit is a network consisting of a closed loop, giving a return path for the current.

A circuit is a [**loop** of conductor](http://en.wikipedia.org/wiki/File:Ohm%27s_Law_with_Voltage_source_TeX.svg). 

The carriers of charge are normally electrons (as your question suggests). The electrons are already part of the conductor. A current is merely a flow, or movement, of charge (electrons) around the loop of conductor (the circuit).

The solar panel merely converts photons into a ""push"" for the charge carriers (electrons), like a pump does for a fluid. The solar panel then ""pushes"" the electrons (which are already in the conductors) around the circuit. In electrical circuits, we do not call this pushing force ""pressure"", but rather we use the term ""voltage"".",null,0,cdl4dv3,1r9es2,askscience,top_week,1
jayd42,"My understanding of current is that the 'flow' is actually photons moving between electrons as the electrons change energy levels, gaining photons to raise in energy and losing photons to lower in energy, and not the physical movement of electrons through a material.

From this understanding, solar cells become very easy to understand from a non technical point of view. The cell gets hit with photons raising the energy level of the electrons in the cell and instead of reflecting the photons back as light the photons are redirected into an electric circuit as current.

I'm sure that's not 'exactly' right but close enough is good for me.
",null,0,cdl8979,1r9es2,askscience,top_week,1
iam_sancho2,"A solar cell is made of layers of different materials. The top layer will be some kind of anti-reflection surface. The next region is a thin layer of n-type material followed by a larger region of p-type material. The juxtaposition of these two differing materials creates an internal electric field. 

When a photon with sufficient energy enters the inner material, it generates electron-hole pairs, which are immediately swept to apart from each other by the internal electric field. With the electrons going one way and the holes going another, a photocurrent is generated within the solar cell. 


There is no net loss of electrons in the material. ",null,0,cdlif43,1r9es2,askscience,top_week,1
clever_cuttlefish,"I think you're mistaken about the current.

Electrical current is a flow of electrons, but no electrons are created or lost in the panel. The energy carried from the photons creates a voltage, which causes the electrons to move around, but none are created.

Think of it like this: All the electrons are happily bound to their atoms, but the photon can bump into one and knock it out of place. However, all the atoms and electrons this has happened to would much rather that another electron comes to fill it's place. So electrons move across the panel to fill in the gaps. This flow of electrons is the current, and is what we get the energy from.",null,3,cdl029v,1r9es2,askscience,top_week,3
Tacomelt,"I can give you a short run down.

Light emits photons, the photons are captured and piled up on the plate causing a voltage.  A current is then formed from the resulting voltage producing electricity.  The electricity is stored into a capacitor or battery system.

The electricity is a direct current, most systems go through an alternator changing the dc to ac.  It is now capable to power your home.

",null,4,cdkyc5n,1r9es2,askscience,top_week,2
RelativisticMechanic,"Note that when dealing with roots and complex numbers, you actually get multiple results. For example, -1^(1/2) actually has two values:

1. i
2. -i

Similarly, -1^(1/3) has three values:

1. -1
2. cos(/3) + i\*sin(/3)
3. cos(/3) - i\*sin(/3)

Finally, note that 1/2.5 = 2/5. Thus, (-1)^(1/2.5) = (-1)^(2/5) = 1^(1/5). We therefore need to find the fifth roots of 1. There are five of them, and they are

1. 1
2. cos(2/5) + i\*sin(2/5)
3. cos(2/5) - i\*sin(2/5)
4. cos(4/5) + i\*sin(4/5)
5. cos(4/5) - i\*sin(4/5)

Any of these numbers satisfies x^(5/2) = -1 (and x^(5/2) = 1), since they all satisfy x^5 = 1, and -1 satisfies -1^2 = 1.",null,44,cdkxquq,1r9elk,askscience,top_week,334
Tsien,"To elaborate on what RelativisticMechanic wrote, the roots come from Euler's formula: e^(ix) = cosx + isinx. If you're familiar with Taylor series, this formula can be derived from taking Taylor expansions around x = 0 for e^(ix), cosx, and sinx:  

e^(ix) = 1 + ix - x^2 /2! - ix^3 /3! + x^4 /4! + ix^5 /5! - ...  
cosx   = 1 - x^2 /2! + x^4 /4! - x^6 /6! + ...  
isinx   = ix - ix^3 /3! + ix^5 /5! - ix^7 /7! + ...  

So, using this formula, we know that e^i\*2kpi = 1 where k is an integer. So 1^(1/5) becomes

e^i\*2kpi/5 = cos(2kpi/5) + isin(2kpi/5)

Using k = 0, 1, -1, 2, -2 (in that order) gives RelativisticMechanic's answers. Notice that since cos and sin are 2-pi periodic, if you try to use other values of k, you'll end up getting one of the 5 answers already listed.",null,3,cdkyeo1,1r9elk,askscience,top_week,15
regnirps,"This should help you out: [Graph of (-1)^x in Wolfram Alpha.](http://www.wolframalpha.com/input/?i=%28-1%29%5Ex)  This graph show the real vs. imaginary parts of the powers of (-1).  Notice that (-1)^(1/2.5) is in the part of the graph with nonzero real *and* nonzero imaginary parts!

In general, I think your question has to do with the properties of the function f(x) = a^x, but I'm not exactly sure what explanation you want for ""why"" beyond the graph linked above.",null,5,cdkx2fk,1r9elk,askscience,top_week,17
SidusObscurus,"For this question, you have to compare an inverse function, as defined by convention, with the solutions to an equation involving a function that is not one-to-one (invertible).

In order to define a function, you need exactly one output for each input. For possible multivalued functions, we select one solution completely by convention (sqrt function, inverse trig functions, and many others).

Solving x^2 = -1 has two answers, +i and -i, both imaginary.
Solving x^3 = -1 has three answers, e^(i*pi/3), -1 = e^(i*pi/3 +2*pi/3), and e^(i*pi/3 +4*pi/3).

To define the inverse functions of these equations, we can only pick one of these answers, so we pick one answer entirely based on mathematical convention and general agreement. Typically we pick the real number solutions first if we can (ex. cube root), positive part solutions after this if we can (ex. sqrt), and if it is still multivalued pick the solution set that intersects 0 if we can (ex. inverse trig functions). This defines the inverse functions x^(1/2) and x^(1/3). If we cannot do any of those, it's less clear what we pick, and mathematicians will usually explicitly state the solution branch they are using for their math, so no one is confused.

What about x^(2.5)? Once you have defined all the integer rational roots and powers, the standard convention is to interpret x^(5/2) as either of the two equal expressions,
sqrt(x^5) = sqrt(x)^5.
There is no ambiguity in these expressions. We could also solve
x^(2/5) = -1, which would have multiple solutions. Our solutions would be
e^(i*pi/2.5 + 2*pi*k/2.5) for each integer k.
See [Roots of Unity](http://en.wikipedia.org/wiki/Root_of_unity) and [Euler's Identity](http://en.wikipedia.org/wiki/Euler%27s_Identity) for more information. In this case, we would only have 5 total solutions, due to the 5th root.

For irrational numbers, everything gets a little more complicated, as irrational powers of negative numbers aren't very well defined. But that's another issue entirely.

*Edit: Corrected a really silly mistake of mine.*",null,0,cdl8h2z,1r9elk,askscience,top_week,3
Borlaug,"When the exponent is a fraction, they want you to find the root of the coefficient using the denominator as the radical. 

For the first one, no known number when multipled by itself results to -1. In other words, the square root of -1 is imaginary. This imaginary is represented by the letter i. 

The second one is asking for the cube root of -1, which is -1. i. e. -1*-1 *-1=-1",null,3,cdl6se0,1r9elk,askscience,top_week,5
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,top_week,18
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,top_week,18
nairebis,"That's all in the BIOS (Basic Input/Output System), which is a program stored in a special chip on the motherboard. That provides a standardized interface between the hardware and the operating system, which is why you can load Windows on any Brand-X motherboard. OS/X has the same concept, though of course Apple only officially supports certain motherboards.

How much power it uses would be dependent on the motherboard design, but if it uses electronic starting, then it uses some small amount of power to monitor the button. I believe all modern motherboards do it this way. In the relatively distant past, they used to use a mechanical on-off switch, but there were a lot of advantages to electronic power on/off, so that became standard.

Whether you can use a different key, etc, is whether that feature is programmed into the BIOS. Here is a page that describes typical BIOS settings:

http://www.tomshardware.com/reviews/bios-beginners,1126-8.html

BIOS settings are usually accessed by pressing a special key when you start up the machine, but before it starts loading the operating system. I've typically seen the DEL key, the F2 key and the F8 key. I don't know how Apple does it. If it works like how Apple does other things, then they picked some arbitrary key that no one else uses. :)

Edit: As haikuginger pointed out, the more modern version of BIOS is EFI, which allows changing some settings from within the Operation System (particularly in the case of Macs). But the principle is the same, the controlling program here comes from the motherboard firmware.",null,0,cdlach2,1r9c6s,askscience,top_week,4
Daegs,"no one says it ""must"" be. 

There are 4 forces: strong, weak, electromagnetic and gravity.

The first 3 are all very similar, and in fact the weak / electromagnetic have been joined into a single force, the electro-weak. It *seems* that the strong also fits in very well mathematically, and it is expected in the coming years / decades that we could actually integrate all 3 (strong, weak, electric) into a single force. This is called ""Grand Unification"":

http://en.wikipedia.org/wiki/Grand_Unified_Theory

So the problem is asking why are 3 out of 4 of the forces so similar to the point where they can actually be shown to be the same underlying force, while gravity is such an oddball and so much weaker. 

One *possible* explanation is that all 4 of the forces are equal in strength, and could be unified into a single force, while gravity is special because it ""leaks"" its force to an unseen dimension. This would allow it, in theory, to be unified with the other 3 forces.

There are other explanations, including that gravity is an emergent property of our universe (such as holographic universe theory) rather than a fundamental force.

Or, it could simply be that gravity is fundamentally different from the other 3 forces, which would just be troubling for physicist.

Without an explanation such as extra dimensions or more exotic behavior we haven't seen, there isn't going to be a way to unify gravity with the other 3 forces.",null,4,cdkxnpf,1r9c24,askscience,top_week,17
iorgfeflkd,"I'm not sure I understand your question. Did your teacher say that gravity needs to be as strong as the others, despite not being?

It is thought that at extremely short distances, there is so much energy from interactions of other forces (consider two electrons very close together for example) that the energy itself starts to gravitate. At these scales (typically, Planck scales), gravity would overwhelm other interactions. However, we don't have any way of testing this.",null,0,cdkwv4t,1r9c24,askscience,top_week,2
theoreoman,"Gravity is a very weak force and  it takes an entire planet to counteract an electrical charge on a small object.  This thing is no one knows why exactly why yet, there are theories but one of the holy grails of physics it to figure out how gravity relates mathematically to the other forces ",null,1,cdkwzlp,1r9c24,askscience,top_week,2
OverlordQuasar,"Essentially, it seems out of place. We have 2 forces that we have demonstrated via experimentation to be part of one underlying force (electromagnetism and the weak force becoming the electroweak force), another that has been mathematically shown to be part of that, which is awaiting experimental confirmation which requires a bit higher energy particle accelerators (strong force). All these are part of one thing, but gravity isn't. It is so much weaker, and refuses all efforts to unify it with the others mathematically without resorting to things on the level of higher dimensions.

TL;DR It's out of place to have 3 forces that are easy to unify and one that is much weaker and appears to be completely seperate.",null,0,cdl2y71,1r9c24,askscience,top_week,1
dudley_love,"Great question, putting the finger on the limit of the SF theory. 

One variation of the SF is [Yanagida](http://www.ncbi.nlm.nih.gov/pubmed/2082730)'s, where the actin-myosin crossbridge is not a solid-ish state, but a ""loose"", transient one. 

So instead of a ladder with strong actin ""hands"", you have a ratchet with potential for slippage.",null,0,cdmjgkh,1r9a7m,askscience,top_week,4
KarlOskar12,"[Eccentric contraction](http://medical-dictionary.thefreedictionary.com/eccentric+contraction) happens when you apply force to the muscle through its range of motion while the muscles contractile force is less than the applied for causing the muscle to stretch. The farther you stretch  the [muscle](http://puu.sh/5qXWU.jpg), the less myosin heads will be able to bind to the thick filaments and the amount of force the muscle can produce rapidly decreases as the stretch progresses.

This [article](http://muscle.ucsd.edu/musintro/contractions.shtml) explains it pretty well.",null,2,cdl90c6,1r9a7m,askscience,top_week,2
sporclesam,"If you mean [Sulphate-reducing bacteria](http://en.wikipedia.org/wiki/Sulfate-reducing_bacteria) (which use sulphate and not sulphur as terminal acceptor) then, yes, but not like us but more like plants/algae. These anaerobes use dissimilatory sulfate reduction to obtain sulphide as waste (somewhat similar to plants releasing oxygen from water)

Check out links on [chemosynthesis] (http://en.wikipedia.org/wiki/Chemosynthesis) *vis-a-vis* photosynthesis. ",null,0,cdl3l9h,1r99pm,askscience,top_week,2
Platypuskeeper,"Not really, they reduce SOx to H2S, while we reduce O2 to H2O. H2S doesn't perform any functions beyond that, while we use water for quite a lot of other things. Sulfur-reducing bacteria still use water for all those other things.

It's also a very different enzyme and mechanism. 
",null,1,cdl64y4,1r99pm,askscience,top_week,2
Updatebjarni,"Consider the fact that not only are the electrons in the extension cord running back and forth since the cord carries AC, but it's also running down one conductor while it is running up the other inside the cord. So regardless of exactly how electrons behave with respect to gravity in a conductor, that would seem to answer the question.

Same with the battery; there is one conductor carrying electrons from the negative terminal of the battery to ground, and one carrying them from ground to the positive terminal.
",null,1,cdl15ac,1r933v,askscience,top_week,12
iorgfeflkd,The voltage required to move an electron through a gravitational field is very small but nonzero. It is about 5x10^-11 Volts/meter.,null,2,cdl1mfs,1r933v,askscience,top_week,10
Farnswirth,"Heat engines require a heat differential to function, by definition.  

However, what /u/JimmyGroove said was correct as well, and is merely another way of stating the first law, which is:  The change in total internal energy in a system (in this case an engine) is equal to the heat in, minus the heat out, minus the work done by the engine.  This is one of the most fundamental principles of thermodynamics, [the first law.](http://en.wikipedia.org/wiki/First_law_of_thermodynamics)

For instance, you could have a very energetic system with no heat differential (eg. a cold car with a full tank of gasoline).  In this case, the heat inside the system (the car) would be at thermal equilibrium with the outside air because you haven't started the engine yet.  In this case, there would be no heat differential in the beginning.  The net reaction for the car as a system would be a conversion of internal energy (the chemical energy of the gasoline) into work.  *It's all a question of how you define your system.*  

So to answer your question - a heat engine requires a heat differential, by definition, because that's how a heat engine works.  But depending on how you define your ""engine"" (thermodynamic system) - you don't necessarily need a heat differential- for work to be produced.  But the energy must come from somewhere, and it must have somewhere to go.

Edit: Another good example of a system that does not require a heat differential to do work is a fuel cell, which directly converts chemical energy into electrical energy with no heat differential.  Obviously this isn't a heat engine, but you could definite it as an ""engine"", and it is used as such in some vehicles. ",null,1,cdkwnan,1r90aa,askscience,top_week,8
JimmyGroove,"You only get a net flow of heat from areas of high temperature to low, so a heat engine could only work off ambient heat if the ambient temperature was different from that of the engine (or can be manipulated into being different, the most commonly used way being to change the pressure of a gas).",null,0,cdkt7bu,1r90aa,askscience,top_week,3
theoreoman,"Law of thermodynamics says that heat can only flow from hot to cold, so for work to happen the heat needs to flow from hot to cold, the larger the differential the more work that can be done.  Quantum mechanics might have a different answer that I'm not aware of",null,0,cdkwkpo,1r90aa,askscience,top_week,1
Jonex,"Not really answering your question - as it's already answered - but an interesting addition:
You can extract ambient heat energy without having a heat differential - if you add more energy than you extract. This is used in earth heating systems where energy is taken from the ground at a few degres to add to the indoor heating at around 20 degrees.

To to this you need an heat pump, powered by for instance elecricity. There are physical limitations of efficiancy. But it's a popular way to increase the efficiancy of electricaly powered heating.

I'm a bit tired so not an awesome explanation, someone who has done their thermodynamics more recently can hopefully expand and clear things up a bit.",null,0,cdl4g2a,1r90aa,askscience,top_week,1
oxymoron1629,"The Sickle Cell gene is incompletely dominant with the wild type gene. That means that both will be expressed and will create an intermediate phenotype. One who is heterozygous for the Sickle Cell gene is considered to express the Sickle Cell trait, but is not suffering from Sickle Cell Anemia. A heterozygous person will have some normal, some sickle shapes and some in between. Since a red blood cell had many hemoglobin molecules, when both genes are expressed each red blood cell gets some proportion wild type hemoglobin and some mutant ones. The amount of the mutant ones determines how bent the red blood cell will look. 

Tl;dr Not exactly a 50/50 split, but some completely normal, some completely sickle shaped, and many in between. ",null,2,cdkstzy,1r8w0j,askscience,top_week,6
meaningless_name,"&gt;And how does this help fight against the malaria plasmodium?

[A relevant paper on this topic](http://www.nature.com/news/sickle-cell-mystery-solved-1.9342)

Basically plasmodium, after infecting a red blood cell as part of its life cycle, hijacks RBC actin (a naturally occurring cell-strucure protein), to help it transport a protein of its own (adherin) to the RBC surface to make it stick to surfaces and other RBCs, which helps the plasmodium.

For sickle cell individuals, the mutant RBC actin can polymerize into long, stiff ""rods"" that distort the shape of the RBC and make it ""clog"" in capillaries. 

However, the plasmodium cannot effectively ""hijack"" the mutant RBC actin, which is the source of sickle-cell mediated malaria resistance.

Sickle cell homozygotes have mostly sickle-cell RBCs, which causes the clinical symptoms of sickle cell anemia.

Sickle cell heterozygotes have a mixture of sickle cell and non-sickle cell RBCs (which is not quite 50/50, as oxymoron 1629 explained), meaning they retain the malaria resistance while avoiding the worst of the anemia.",null,2,cdkuzei,1r8w0j,askscience,top_week,5
pravl,"No, they aren't.  The predominant form of hemoglobin in normal individuals, HbA, consists of two alpha-globin chains and two beta-globin chains.  People with sickle cell disease have only mutated beta-globin, which pairs with alpha-globin to form HbS.  The mutated beta-globin chains pair to alpha-globin with less affinity than normal beta-globin, so individuals who have both normal and mutated beta-globin, i.e. people with sickle cell trait, end up having slightly more HbA than HbS.  Usually around 60% HbA, 40% HbS.  And that is the hemoglobin itself, not the percentage of actual sickled red blood cells.  If not otherwise sick/stressed, people with sickle trait usually have no or very few sickled cells on a blood smear.",null,0,cdl1ovy,1r8w0j,askscience,top_week,2
mutatron,"Past performance is no guarantee of future results.

Overall fertility has been declining for a long time, and is expected to continue to decline. The figure of 10 billion is arrived at by looking at this decline in fertility.

Replacement rate fertility is 2.1 children per woman, or 21 children for every 10 women. Many countries, mostly developed ones, are now below replacement rate, some as low as 1.3 children per woman. Mexico has dropped from around 4 children per woman to 2.3 in just a couple of decades, and other less developed countries are expected to follow suit as poverty declines, healthcare improves, and education becomes ubiquitous, especially among women. Studies have shown that the most effective deterrent to fertility is the education of women.",null,22,cdkrdzc,1r8tbm,askscience,top_week,78
4698458973,"Look for a torrent of a video by Hans Rosling called, [Don't panic: the truth about population](http://www.bbc.co.uk/programmes/b03h8r1j). He is a fairly famous statistician, and he's a great speaker.

The short answer is that birth rates have fallen worldwide, in some places like they fell off a cliff. Population growth is still happening because we have at least fifty years left of people getting older: fifty years ago, women were having more babies than women are now, and those babies are going to be around for a while -- and, themselves, have babies. But the next generation will be barely at replacement level for most of the world.",null,3,cdkys9c,1r8tbm,askscience,top_week,18
AshRandom,"It's only predicted to stabilize at 10 billion by the most conservative of estimates. Dr. Michio Kaku gives a lecture where he talks of the coming super abundance of the future. Should the fusion power plant designed by [ITER](http://en.wikipedia.org/wiki/ITER) successfully create a [Tokamak](http://en.wikipedia.org/wiki/Tokamak) reactor capable of converting plasma directly into electricity (without all the fuss of boiling water and turning a big turbine steam engine) the cost of electricity would plummet. Not only would the cost be many thousands of times lower, the amount of energy available would be many thousands of times greater. 

Cheap and abundant electrical energy combined with modern desalination water processing would turn every bay and every inch of oceanic coastline into a fresh water river. Vast amounts of currently unfarmable land would become viable. Additionally, the ability to build, light, and water indoor hydroponics farms would be possible at tremendously reduced costs. This would make skyscraper greenhouse projects highly profitable, where they are currently cost prohibitive. 

Clean energy, fresh water, and a super abundance of food would have obvious consequences on the expansion of future populations. And should the vast uninhabited stretches of the world's surface become utterly filled with cityscapes, moving underground would then further magnify the square footage available for hydroponic farms and human habitation. Pushing Earth's capacity for human population into the trillions would not be unthinkable. 

Partial Source: *Dr. Michio Kaku N.Y.U. Institute for Advanced Study*
",null,6,cdl6b92,1r8tbm,askscience,top_week,9
OctoRock33,"As countries progress through development they go through multiple stages of birth, industry, and death. In the final progression of development Stage 4. The birth rate stays at or below the death rate, which could lead to either a stable population or a slowly decreasing population. 
Source: I took a college course on Human Geography
",null,0,cdl6pyd,1r8tbm,askscience,top_week,2
Zedred,"Predictions calculate there is a tipping point at which insufficient arible land and water exists to sustain the population, factoring in the amount of land and water required produce enough food to sustain each human and the livestock they require.  That number stands at about 24 or 25 acres per human given today's technology and weather models.

Theoretically at a population of somewhere between 4 and 16 billion ( average 10 billion) every single acre would have to be in use for farming  or housing given today's technology and water availability and assumes air quality is sufficient.  Before that limit is reached, population grosth would start to decline due to econonimic factors.  UN calculations no doubt take all these factors into account.

http://en.wikipedia.org/wiki/Human_overpopulation

 Global warming, nuclear disaster, war, or unforseen causes of crop failure could result in less food due to less farmable land, more sea water, and less fresh water, further constraining population growth.  Now facter in the potential for a plague disaster along with antibiotic resistence in a large population, and natural changes in fertility rates due to pollution or declining economic conditions that prevent access to birth control. 

It is not hard to understand that a planet with finite limits on the resources required to sustain human life will sustain only a finite maximum of people.  Technology  improvements and wiser land/water/air use could raise that maximum number, but it will stabilize again at the higher number unless additional resources are introduced into the food and energy production  system from other planets. 

This upper limit on population on earth is why space exploration is so critical to to the future of humanity.   Lets also hope China is smart enough not to launch a population  boom by lifting their one child policy amidst their current economic expansion, lest we reach that upper limit faster than technology can solve the problems that would cause.",null,1,cdl73zt,1r8tbm,askscience,top_week,2
TheMuslinCrow,"Each species has a population threshold for the environment it's in, known as the carrying capacity (K). This is determined by species requirements, as well as the environmental constraints on these requirements (food, space, etc). When a species population goes above this level, there's usually a setback in the form of disease or famine, and the resultant deaths bring the population back near the carrying capacity.

In the case of humans, we are able to artificially increase the size of our K through technology, medicine, urban planning and such. However, this planet has finite resources and space. So what is the carrying capacity for humans on Earth? We really won't know until we reach it. Some places such, as Japan and parts of Europe, seem to be nearing their K for their respective environments.

Source: I'm a zoologist.

EDIT: Am I being downvoted for providing a biology based answer about population, because this is an earth sciences subreddit? There's too much segregation in science, and that holds us back.",null,6,cdl76cg,1r8tbm,askscience,top_week,6
Filipinolurve,"Hans Rosling: Religions and babies

https://www.google.com/search?q=ted+talks+hans+rosling+religion+and+babies&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;client=safari

OP I tried to copy and paste the link above (I'm on my phone) anyways this stats guy on TED talks explains how the population trend will go and why, the video starts off talking about if religion and does it affect the population growth but it'll def answer your question.",null,2,cdl6cjs,1r8tbm,askscience,top_week,2
mutatron,"Past performance is no guarantee of future results.

Overall fertility has been declining for a long time, and is expected to continue to decline. The figure of 10 billion is arrived at by looking at this decline in fertility.

Replacement rate fertility is 2.1 children per woman, or 21 children for every 10 women. Many countries, mostly developed ones, are now below replacement rate, some as low as 1.3 children per woman. Mexico has dropped from around 4 children per woman to 2.3 in just a couple of decades, and other less developed countries are expected to follow suit as poverty declines, healthcare improves, and education becomes ubiquitous, especially among women. Studies have shown that the most effective deterrent to fertility is the education of women.",null,22,cdkrdzc,1r8tbm,askscience,top_week,78
4698458973,"Look for a torrent of a video by Hans Rosling called, [Don't panic: the truth about population](http://www.bbc.co.uk/programmes/b03h8r1j). He is a fairly famous statistician, and he's a great speaker.

The short answer is that birth rates have fallen worldwide, in some places like they fell off a cliff. Population growth is still happening because we have at least fifty years left of people getting older: fifty years ago, women were having more babies than women are now, and those babies are going to be around for a while -- and, themselves, have babies. But the next generation will be barely at replacement level for most of the world.",null,3,cdkys9c,1r8tbm,askscience,top_week,18
AshRandom,"It's only predicted to stabilize at 10 billion by the most conservative of estimates. Dr. Michio Kaku gives a lecture where he talks of the coming super abundance of the future. Should the fusion power plant designed by [ITER](http://en.wikipedia.org/wiki/ITER) successfully create a [Tokamak](http://en.wikipedia.org/wiki/Tokamak) reactor capable of converting plasma directly into electricity (without all the fuss of boiling water and turning a big turbine steam engine) the cost of electricity would plummet. Not only would the cost be many thousands of times lower, the amount of energy available would be many thousands of times greater. 

Cheap and abundant electrical energy combined with modern desalination water processing would turn every bay and every inch of oceanic coastline into a fresh water river. Vast amounts of currently unfarmable land would become viable. Additionally, the ability to build, light, and water indoor hydroponics farms would be possible at tremendously reduced costs. This would make skyscraper greenhouse projects highly profitable, where they are currently cost prohibitive. 

Clean energy, fresh water, and a super abundance of food would have obvious consequences on the expansion of future populations. And should the vast uninhabited stretches of the world's surface become utterly filled with cityscapes, moving underground would then further magnify the square footage available for hydroponic farms and human habitation. Pushing Earth's capacity for human population into the trillions would not be unthinkable. 

Partial Source: *Dr. Michio Kaku N.Y.U. Institute for Advanced Study*
",null,6,cdl6b92,1r8tbm,askscience,top_week,9
OctoRock33,"As countries progress through development they go through multiple stages of birth, industry, and death. In the final progression of development Stage 4. The birth rate stays at or below the death rate, which could lead to either a stable population or a slowly decreasing population. 
Source: I took a college course on Human Geography
",null,0,cdl6pyd,1r8tbm,askscience,top_week,2
Zedred,"Predictions calculate there is a tipping point at which insufficient arible land and water exists to sustain the population, factoring in the amount of land and water required produce enough food to sustain each human and the livestock they require.  That number stands at about 24 or 25 acres per human given today's technology and weather models.

Theoretically at a population of somewhere between 4 and 16 billion ( average 10 billion) every single acre would have to be in use for farming  or housing given today's technology and water availability and assumes air quality is sufficient.  Before that limit is reached, population grosth would start to decline due to econonimic factors.  UN calculations no doubt take all these factors into account.

http://en.wikipedia.org/wiki/Human_overpopulation

 Global warming, nuclear disaster, war, or unforseen causes of crop failure could result in less food due to less farmable land, more sea water, and less fresh water, further constraining population growth.  Now facter in the potential for a plague disaster along with antibiotic resistence in a large population, and natural changes in fertility rates due to pollution or declining economic conditions that prevent access to birth control. 

It is not hard to understand that a planet with finite limits on the resources required to sustain human life will sustain only a finite maximum of people.  Technology  improvements and wiser land/water/air use could raise that maximum number, but it will stabilize again at the higher number unless additional resources are introduced into the food and energy production  system from other planets. 

This upper limit on population on earth is why space exploration is so critical to to the future of humanity.   Lets also hope China is smart enough not to launch a population  boom by lifting their one child policy amidst their current economic expansion, lest we reach that upper limit faster than technology can solve the problems that would cause.",null,1,cdl73zt,1r8tbm,askscience,top_week,2
TheMuslinCrow,"Each species has a population threshold for the environment it's in, known as the carrying capacity (K). This is determined by species requirements, as well as the environmental constraints on these requirements (food, space, etc). When a species population goes above this level, there's usually a setback in the form of disease or famine, and the resultant deaths bring the population back near the carrying capacity.

In the case of humans, we are able to artificially increase the size of our K through technology, medicine, urban planning and such. However, this planet has finite resources and space. So what is the carrying capacity for humans on Earth? We really won't know until we reach it. Some places such, as Japan and parts of Europe, seem to be nearing their K for their respective environments.

Source: I'm a zoologist.

EDIT: Am I being downvoted for providing a biology based answer about population, because this is an earth sciences subreddit? There's too much segregation in science, and that holds us back.",null,6,cdl76cg,1r8tbm,askscience,top_week,6
Filipinolurve,"Hans Rosling: Religions and babies

https://www.google.com/search?q=ted+talks+hans+rosling+religion+and+babies&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;client=safari

OP I tried to copy and paste the link above (I'm on my phone) anyways this stats guy on TED talks explains how the population trend will go and why, the video starts off talking about if religion and does it affect the population growth but it'll def answer your question.",null,2,cdl6cjs,1r8tbm,askscience,top_week,2
arumbar,"Maternal and fetal blood do not normally mix.  Maternal blood is pumped through the [maternal blood vessels in the placenta](http://www.biog1445.org/media/placenta.jpg), where nutrients and oxygen are allowed to diffuse into the fetal bloodstream due to the close proximity of fetal vessels.  There are a number of [fetal anatomic features](http://img.docstoccdn.com/thumb/orig/107478990.png) that work together to make this system work, as the fetus will be obtaining oxygen from the mother rather than its lungs.  The expansion in maternal blood volume and cardiac output is simply a consequence of having to perfuse not only her normal organs but also divert a significant amount of bloodflow to perfuse the uterus and placenta, which then feed into fetal circulation - but they do remain separate.

Typically having a baby with a different blood type is not an issue.  However, a Rh negative mother (eg O-, AB-, etc) can become sensitized to the Rh antigen if her fetus is Rh positive, resulting in complications for future pregnancies.  This usually occurs as tiny volumes of fetal blood (&lt;0.1mL) enter maternal circulation and trigger formation of anti-Rh IgG antibiodies, which can then cross the placental barrier in subsequent pregnancies and cause hemolysis (destruction of fetal red blood cells).  ABO alloimmunization is less commonly an issue - for a variety of reasons the antibodies associated with Rh factor are more prone to cross the placenta and cause disease (they are IgG rather than IgM, and fetal rbcs express more of the Rh antigen).  There are a few other high-risk antibodies associated with fetal hemolysis (eg anti-Kell).",null,3,cdkqj3h,1r8r2g,askscience,top_week,29
cryptorchidism,"There is a [maternal-fetal barrier](https://en.wikipedia.org/wiki/Placenta#Placental_circulation) in the placenta, similar in function to the [blood-brain barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier) (or for that matter the [blood-testis barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93testis_barrier)/blood follicle barrier). It prevents large molecules from passing, like pathogens, immune cells, and blood antigens, the last of which determine blood type.

^(Thanks, now ""blood"" looks funny.)",null,3,cdkpyc2,1r8r2g,askscience,top_week,14
NassT,"No, mothers and babies have separate circulatory systems, but the mother's blood does have to carry all of the oxygen, nutrients, etc. for the baby as well.  Pregnant women also tend to put on weight in addition to that of the baby, which also stresses the heart.",null,2,cdkq3u4,1r8r2g,askscience,top_week,5
Naf623,"No; the mother's heart pumps blood through the placenta, which makes for a small increase in where it needs to be pumped. The baby has it's own completely separate blood circulatory system which also goes through the placenta. 
In the placenta the blood vessels get very small, thin walled &amp; close so that nutrients can be transferred between them. 
I'm very surprised by the 30-50% increase figure; ",null,0,cdkq3hb,1r8r2g,askscience,top_week,3
arumbar,"Maternal and fetal blood do not normally mix.  Maternal blood is pumped through the [maternal blood vessels in the placenta](http://www.biog1445.org/media/placenta.jpg), where nutrients and oxygen are allowed to diffuse into the fetal bloodstream due to the close proximity of fetal vessels.  There are a number of [fetal anatomic features](http://img.docstoccdn.com/thumb/orig/107478990.png) that work together to make this system work, as the fetus will be obtaining oxygen from the mother rather than its lungs.  The expansion in maternal blood volume and cardiac output is simply a consequence of having to perfuse not only her normal organs but also divert a significant amount of bloodflow to perfuse the uterus and placenta, which then feed into fetal circulation - but they do remain separate.

Typically having a baby with a different blood type is not an issue.  However, a Rh negative mother (eg O-, AB-, etc) can become sensitized to the Rh antigen if her fetus is Rh positive, resulting in complications for future pregnancies.  This usually occurs as tiny volumes of fetal blood (&lt;0.1mL) enter maternal circulation and trigger formation of anti-Rh IgG antibiodies, which can then cross the placental barrier in subsequent pregnancies and cause hemolysis (destruction of fetal red blood cells).  ABO alloimmunization is less commonly an issue - for a variety of reasons the antibodies associated with Rh factor are more prone to cross the placenta and cause disease (they are IgG rather than IgM, and fetal rbcs express more of the Rh antigen).  There are a few other high-risk antibodies associated with fetal hemolysis (eg anti-Kell).",null,3,cdkqj3h,1r8r2g,askscience,top_week,29
cryptorchidism,"There is a [maternal-fetal barrier](https://en.wikipedia.org/wiki/Placenta#Placental_circulation) in the placenta, similar in function to the [blood-brain barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier) (or for that matter the [blood-testis barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93testis_barrier)/blood follicle barrier). It prevents large molecules from passing, like pathogens, immune cells, and blood antigens, the last of which determine blood type.

^(Thanks, now ""blood"" looks funny.)",null,3,cdkpyc2,1r8r2g,askscience,top_week,14
NassT,"No, mothers and babies have separate circulatory systems, but the mother's blood does have to carry all of the oxygen, nutrients, etc. for the baby as well.  Pregnant women also tend to put on weight in addition to that of the baby, which also stresses the heart.",null,2,cdkq3u4,1r8r2g,askscience,top_week,5
Naf623,"No; the mother's heart pumps blood through the placenta, which makes for a small increase in where it needs to be pumped. The baby has it's own completely separate blood circulatory system which also goes through the placenta. 
In the placenta the blood vessels get very small, thin walled &amp; close so that nutrients can be transferred between them. 
I'm very surprised by the 30-50% increase figure; ",null,0,cdkq3hb,1r8r2g,askscience,top_week,3
TangentialThreat,"Yes. Well, [sort of](http://www.the-scientist.com/?articles.view/articleNo/32997/title/Electrical-Bacteria/).

*Desulfobulbaceae* are forming living wires that connect electron-rich upper sediment with the electron-poor deeper sediment. It is taking advantage of a natural electrochemical potential - in other words, a living wire that's feeding off a battery. Tell me that's not cool. [Study](http://www.nature.com/nature/journal/v491/n7423/full/nature11586.html)

Injecting synthetic ATP into your arm is probably a bad idea. There is never much free ATP in the body except after a major injury. This may cause several body systems to freak out, [including your heart](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3265710/).

",null,1,cdkw2ds,1r8phn,askscience,top_week,10
JimmyGroove,"I can think of no theoretical reason why such a bacteria couldn't exist.  The practical reason for why one doesn't exist already is that free charge differentials aren't very common in the natural world, don't tend to last terribly long, and don't occur over distances that would allow a bacteria to travel from one to another when their first electrical source evens out.",null,0,cdktc9s,1r8phn,askscience,top_week,2
JeremyJBarr,"Actually there is a whole scientific field called ""Microbial Electrochemisty"" utilizing this phenomenon! I spent a little part of my PhD working on a microbial fuel cell (MFC) where the basic premise is that you feed a microbial community a waste product in an enclosed reactor operating as an anode, that is connected to a separate reactor which operates as the  cathode. [Google images picture](http://www.technologyreview.com/sites/default/files/legacy/hydrogen_x600.jpg)

In these systems waste products are used as the food source for mixed microbial communities in the anode reactor. These bacterial communities degrade the organic components present in the waste product, while utilizing the anode as an electron sink, and dump protons into the surrounding media. The electrons are then sent via a conductor across to a separate cathode reactor generating current! The protons generate in the anode then flow across to the cathode via a specific membrane that permits their transport, while keeping the microbial communities separate. 

Microbial fuel cells (MFC) were once suggested as a renewable source of electricity from waste products (typically wastewater). However, current MFC designs do not produce sufficient electrical current to make them sustainable. But a recent suggested use for them has been to pump electrons into the MFC, essentially feeding the microbial communities electrons, and force them to produce bioproducts of extreme value. There is lots of research going on in this field investigating novel MFC designs to generate more current, and the formation of high-value bioproducts from wastewater.

Some sources [1](http://www.sciencemag.org/content/337/6095/686.full) [2](http://link.springer.com/article/10.1023/A:1025484009367) [3](http://www.nature.com/ismej/journal/v1/n1/full/ismej20074a.html)",null,1,cdlc8gu,1r8phn,askscience,top_week,3
TangentialThreat,"Yes. Well, [sort of](http://www.the-scientist.com/?articles.view/articleNo/32997/title/Electrical-Bacteria/).

*Desulfobulbaceae* are forming living wires that connect electron-rich upper sediment with the electron-poor deeper sediment. It is taking advantage of a natural electrochemical potential - in other words, a living wire that's feeding off a battery. Tell me that's not cool. [Study](http://www.nature.com/nature/journal/v491/n7423/full/nature11586.html)

Injecting synthetic ATP into your arm is probably a bad idea. There is never much free ATP in the body except after a major injury. This may cause several body systems to freak out, [including your heart](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3265710/).

",null,1,cdkw2ds,1r8phn,askscience,top_week,10
JimmyGroove,"I can think of no theoretical reason why such a bacteria couldn't exist.  The practical reason for why one doesn't exist already is that free charge differentials aren't very common in the natural world, don't tend to last terribly long, and don't occur over distances that would allow a bacteria to travel from one to another when their first electrical source evens out.",null,0,cdktc9s,1r8phn,askscience,top_week,2
JeremyJBarr,"Actually there is a whole scientific field called ""Microbial Electrochemisty"" utilizing this phenomenon! I spent a little part of my PhD working on a microbial fuel cell (MFC) where the basic premise is that you feed a microbial community a waste product in an enclosed reactor operating as an anode, that is connected to a separate reactor which operates as the  cathode. [Google images picture](http://www.technologyreview.com/sites/default/files/legacy/hydrogen_x600.jpg)

In these systems waste products are used as the food source for mixed microbial communities in the anode reactor. These bacterial communities degrade the organic components present in the waste product, while utilizing the anode as an electron sink, and dump protons into the surrounding media. The electrons are then sent via a conductor across to a separate cathode reactor generating current! The protons generate in the anode then flow across to the cathode via a specific membrane that permits their transport, while keeping the microbial communities separate. 

Microbial fuel cells (MFC) were once suggested as a renewable source of electricity from waste products (typically wastewater). However, current MFC designs do not produce sufficient electrical current to make them sustainable. But a recent suggested use for them has been to pump electrons into the MFC, essentially feeding the microbial communities electrons, and force them to produce bioproducts of extreme value. There is lots of research going on in this field investigating novel MFC designs to generate more current, and the formation of high-value bioproducts from wastewater.

Some sources [1](http://www.sciencemag.org/content/337/6095/686.full) [2](http://link.springer.com/article/10.1023/A:1025484009367) [3](http://www.nature.com/ismej/journal/v1/n1/full/ismej20074a.html)",null,1,cdlc8gu,1r8phn,askscience,top_week,3
temuchan,"Nucleotides can be synthesized ""de novo"" from precursor molecules (obtained from the breakdown of food, for example).  The major organ involved in this process is the liver.  However, nucleotides can also be [recycled](http://en.wikipedia.org/wiki/Nucleotide_salvage) through a process that synthesizes nucleotides from the components of degraded nucleotides.",null,1,cdkqro4,1r8p24,askscience,top_week,6
sphenopalatine,"The new nucleotides are synthesized from a large number of other precursors, such as folic acid, glutamine, glycine, etc. The method of synthesis differs between purines (A and G) and pyrimidines (T and C). 

The purine synthesis pathway is [quite long](http://gallus.reactome.org/figures/denovo_IMP_synthesis.jpg), but can be summed up as resulting in the end product inosine monophosphate (IMP). This can be interconverted to GMP or AMP. Two more phosphate groups are added on to give the triphosphate tail of a nucleotide. These ribonucleotides (NTPs) are then converted to deoxyribonucleotides (dNTPs) using [Ribonucleotide Reductase](https://en.wikipedia.org/wiki/Ribonucleotide_reductase) and a dNTP is born.

The pyrimidine synthesis pathway is of [similar length](http://gallus.reactome.org/figures/denovo_UMP_synthesis.jpg) and gives uridine monophosphate (UMP), which is then converted to UTP (used in RNA synthesis). UTP can be interconverted with CTP and TTP. Ribonucleotide Reductase once again converts the NTPs into dNTPs.",null,0,cdkriwp,1r8p24,askscience,top_week,3
oxymoron1629,"The same mechanism that creates energy from your food has a built in arm that takes the energy in food and instead of creating energy for later use, it uses the energy from food to create the bases needed for DNA replication. But it only does this when the cell decides to replicate so most of the time, it just stores energy from food. ",null,0,cdksxgh,1r8p24,askscience,top_week,2
Pallidium,"Yes. This would not be innate though, and would result from the person learning to identify themselves. The ""higher"" brain regions, such as the prefrontal and parietal cortices, would register it as ""me"" or ""my face,"" and in turn use this to alter activity in ""lower"" regions. There probably would not inherently be any any mechanism in the visual cortex or fusiform gyrus (a brain region heavily implicated in facial recognition) for a person identifying their own faces, but these regions activity would be modulated by prefrontal and parietal input, which could lead to different activity viewing oneself as compared to others faces. [Here is a study](http://www.ncbi.nlm.nih.gov/pubmed/18656465) which shows difference between self-recognition and other-recognition, and shows fusiform involvement. I'd like to restate that this is NOT an innate ability of the fusiform and probably results from modulation by higher brain centers. The related citations section of that pubmed abstract (right hand side) has some other abstracts about the neural correlates of self-recognition.",null,2,cdkydec,1r8hem,askscience,top_week,11
Pallidium,"Yes. This would not be innate though, and would result from the person learning to identify themselves. The ""higher"" brain regions, such as the prefrontal and parietal cortices, would register it as ""me"" or ""my face,"" and in turn use this to alter activity in ""lower"" regions. There probably would not inherently be any any mechanism in the visual cortex or fusiform gyrus (a brain region heavily implicated in facial recognition) for a person identifying their own faces, but these regions activity would be modulated by prefrontal and parietal input, which could lead to different activity viewing oneself as compared to others faces. [Here is a study](http://www.ncbi.nlm.nih.gov/pubmed/18656465) which shows difference between self-recognition and other-recognition, and shows fusiform involvement. I'd like to restate that this is NOT an innate ability of the fusiform and probably results from modulation by higher brain centers. The related citations section of that pubmed abstract (right hand side) has some other abstracts about the neural correlates of self-recognition.",null,2,cdkydec,1r8hem,askscience,top_week,11
iorgfeflkd,"More energetic photons are actually more likely to interact with materials (above a certain threshold), because it increases the likelihood of pair production.

[This is essentially a graph of how likely a photon is to interact, vs energy](http://en.wikipedia.org/wiki/File:Attenuation_Coefficient_Iron.svg).",null,4,cdkoadv,1r8gmf,askscience,top_week,11
StarshipEngineer,"Conservation of energy prohibits a photon from interacting with vacuum itself, and so pair production is restricted to the vicinity of relatively heavy atoms. Therefore, interaction with vacuum and an electron-positron cascade is probably not something that can occur; if it could, such an event would likely have spread across the observable universe long ago. (Unless, of course, photons of such energy are exceedingly rare. However, even one photon would be enough to trigger such a cascade, so it is unlikely that it is possible at all.)

It is true that, from the perspective of particle physics, even a pure vacuum is filled with ""virtual"" particles born of quantum fluctuations. What makes them virtual however is that they appear and disappear in such a short time that their brief existence does not violate the uncertainty principle, and under ordinary circumstances, they are not directly detectable. If a photon were to interact with such a particle, it would have to reduce the energy of the photon at least enough to account for the particle's mass-energy, in order to obey conservation of energy. This would prohibit such a runaway cascade.",null,0,cdkq03p,1r8gmf,askscience,top_week,2
D0ct0rJ,"I think a photon of sufficient energy will interact with free space strongly enough to prefer to spew out electrons and positrons, which will radiate as they experience forces in material, leading to more high energy photons, and so on: an electron-positron cascade. Gamma rays interact with heavy metals due to pair production, or so the current theory goes.",null,3,cdknpx7,1r8gmf,askscience,top_week,2
TheCrazyOrange,"Because sound is just compression waves traveling through the air, and our ears are highly sensitive to the frequencies in the range of human voices.

But when you blow air, your attempting to physically move the air with enough velocity that it will reach the person. Contrary to what it seems, air has mass, and thus the air separating you can diffuse and block the air you blow our.",null,2,cdkn173,1r8gcu,askscience,top_week,9
selfification,You can't throw a handfull of water that far without a lot of effort but you can create a small splash on a pond and have the ripples travel for a long long distance.,null,3,cdkoctf,1r8gcu,askscience,top_week,5
therationalpi,"I feel like you haven't gotten a satisfying answer yet, so I'm gonna try my hand at this.

To start, let's imagine what the air is like at rest. All of the air particles are bouncing around randomly, but *for the most part* they are all bouncing around at equilibrium. All of the air particles are where they want to be.

Now, let's say I blow on the air. I'm introducing a slug of fast moving air, with a good deal of kinetic energy. That air has not found it's equilibrium point, so it's going to carry forward. In the process, the air is slamming against the air around it that was formerly at equilibrium, imparting kinetic energy, and carrying it forward. A moment after the air first comes in, you now have a larger slug of slower moving air all trying to find an equilibrium position. The process then repeats, with more and more air going slower and slower. Eventually, the air stops and finds its place.

Now let's look at sound. For sound, you have the same transfer of kinetic energy when the molecules slam into eachother. The big difference is that after they slam into the next group, they *bounce back* and end up right where they started.

As a result of going back to the old equilibrium position, none (or at least very little) of the kinetic energy ends up permanently stored in potential energy. When you blow, part of the energy goes towards moving the next cluster of air molecules, and part of it goes towards choosing a new equilibrium position. For a sound wave, pretty much all of the energy is able to keep moving forward. Look at the first figure on [this page](http://www.acs.psu.edu/drussell/Demos/waves/wavemotion.html), paying careful attention to how the red dot isn't really going anywhere.

Hopefully that answers your questions, but if you would like any clarification please don't hesitate to ask!",null,0,cdkus3i,1r8gcu,askscience,top_week,2
Bbrhuft,"Tsunamis are caused when the sea floor is displaced up or down within just a few seconds or minutes, this is typically caused by a megathrust earthquake, where oceanic plate is forced under continental plate, creating a violent earthquake and sea floor displacement. 

In the largest megathrust earthquakes, 10,000 to 100,000 km2 or more of sea floor can be displaced upwards several metres in only a few minutes, displacing vast quantities of ocean water. This displaced water radiates away and eventually hits land as a tsunami.

Mud volcanoes, on the other hand, are much smaller and happen over a much longer time frame, days to even years. When the mud is erupted to the surface, the surrounding land surface or ocean bottom may subside slightly. But none of these changes are ever on a large enough scale or fast enough to create a tsunami.",null,0,cdkpc7x,1r8g1s,askscience,top_week,6
GreenAdept,"Landslides and impacts can actually cause what is know as a Megatsunami, with potentially greater impact than the typical earthquake tsunami. I'm not very familiar  with mud volcanoes (last I knew they were fairly poorly understood) but it's reasonable to surmise if one were to ""erupt"" in an area that could cause a massive landslide it could be responsible. http://en.wikipedia.org/wiki/Megatsunami",null,0,cdkxyky,1r8g1s,askscience,top_week,5
Daegs,"Sharpening is basically scraping off material until it resembles the shape you want (in this case, a thin edge).

To start sharpening, a large grit is used, which translates to very deep ""trenches"" and irregularities in the resulting material, these are refined with finer grits to even these trenches (scratches) out. Doing this progressively leads to a smoother finish which approximates the shape you want (again, a low angle straight edge).

",null,1,cdkp9xn,1r8fth,askscience,top_week,6
robged,"Before I answer the question, I need to describe a speakers construction: an electromagnetic coil produces magnetic fields when electricity is applied. A rare earth magnet attached to a flexible cone moves in response to this magnetic field. The flexible cone compresses air generating sound. Bass is low frequency sound which is generally between 32 Hz at 512 Hz. The overall range of human hearing is between 20 Hz and 20,000 Hz. 

Speakers are less efficient at producing low frequency sounds than high frequency sounds, as during the low frequency cycle, air has time to equalibriate. To get around this inefficiency, it is possible to build the speaker larger and having it travel farther. Fortunately, it only needs to vibrate at lower frequency than high frequency speakers, so this is possible. Also, note that not only can high frequency speakers be smaller, but they also *must* be smaller and have less travel, because if they were as big as bass speakers the high amplitude combined with high travel would rip them apart.

Now, knowing that, it becomes clear why there is such a thing as a crossover circuit in good quality speakers. A crossover circuit is a low-pass filter before the bass speaker, and a high-pass filter before the treble, such that the small high frequency speaker doesn't get the large amplitude bass signals which will cause it to rip, and the low frequency speaker doesn't get the high frequency signals which will make a distorted sound as it is too large to respond to the high frequency signals.

Sound quality is *defined* as the ability of a system to produce an even response across all frequencies, at high volume, without distortion. If you like more bass, on a good system, you can always use an equalizer to add more bass without distortion.

EDIT: The energy of sound waves is independent of frequency (I get confused with light) so I had to delete a sentence.

TL;DR? The big speakers are for bass, medium speakers for mid range, the small speakers are for treble.",null,1,cdkm3ga,1r8c16,askscience,top_week,7
JoolNoret,"Sea level is the average level of the water in the ocean between tides.

The Netherlands is below sea level (hence the name). So is New Orleans, which resulted in flooding when the levees broke during Katrina.

Just because an area is below sea level doesn't mean the water will climb over a hill to fill it up.",null,2,cdklqri,1r8bxo,askscience,top_week,7
Astrokiwi,"That's exactly it: you can't! This is actually a big part of the theory of relativity: there is no universal concept of ""not moving"". You can say things are moving or not moving relative to something (as you mentioned), but to simply say that something is ""not moving"" does not actually make sense.",null,1,cdklc3q,1r8bbn,askscience,top_week,7
Naf623,"We can't tell if something is absolutely moving; only if it's position is changing relative to us. So we still wouldn't know for sure if we're both moving or just one, or which. 
One method is red shift if light based on the Doppler effect. Similar to how sounds are distorted as an object moves (sirens coming closer then further away) so is the light from/reflected off it. If the light is more red then the object is getting further away; more blue ; it's approaching. ",null,0,cdklyi0,1r8bbn,askscience,top_week,1
StringOfLights,"Crocodylians, which include alligators and crocodiles, are not dinosaurs. They are the closest living relatives of dinosaurs, however (because birds *are* theropod dinosaurs).

Dinosauria is a group that was originally defined by anatomist [Richard Owen](http://www.nhm.ac.uk/nature-online/science-of-natural-history/biographies/richard-owen/) based on a few described taxa, including [*Iguanodon*](http://en.wikipedia.org/wiki/Iguanodon) and [*Megalosaurus*](http://en.wikipedia.org/wiki/Megalosaurus). There are a few more technical ways to define the group, but no matter what it falls out being comprised of two smaller groups: [Ornithischia](http://en.wikipedia.org/wiki/Ornithischia) and [Saurischia](http://en.wikipedia.org/wiki/Saurischia), although these groups were not recognized at the time. Ornithischia includes dinosaurs like *Triceratops*, *Iguanodon*, and ankylosaurs. Saurischia includes sauropods and theropods.

Crocodylians, dinosaurs, and a couple other groups (including pterosaurs) are [archosaurs](http://archosaurmusings.wordpress.com/what-are-archosaurs/) (side note: people often refer to pterosaurs as dinosaurs, but they're actually not). 

To get more at the heart of your question: Crocodylians are widely perceived as these unchanging, prehistoric animals. They're really not. Crown-group crocodylians (that is, the group consisting of the common ancestor of all living species and all of the descendents of that ancestor) first show up in the Late Cretaceous, around 84 million years ago. This actually isn't a terribly long time ago, and it overlaps with the non-avian dinosaurs for about 20 million years. For reference, the [oldest known placental mammal](http://www.livescience.com/15734-oldest-placental-mammal.html) is 160 million years old. 

It is true that crocodylians do have relatives that extended back much further, because archosaurs started to diversify in the Triassic some 250 million years ago, but the crocs you see today are highly derived, not long-forgotten vestiges of the Mesozoic. It's true that some have had a fairly stable body plan, but it's also a body plan that has cropped up multiple times in vertebrate evolution, including in [temnospondyl amphibians](http://en.wikipedia.org/wiki/Prionosuchus) some 270 million years ago. In a lot of these cases it has evolved independently.

The major radiation of archosaurs that includes modern crocodylians is known as Pseudosuchia, and it [first shows up about 250 million years ago](http://openi.nlm.nih.gov/detailedresult.php?img=3194824_pone.0025693.g012&amp;req=4). These early [relatives of crocs](http://web.missouri.edu/~hollidayca/Croc_epipterygoid/Fig%202.jpg) looked more like [this](http://en.wikipedia.org/wiki/Hesperosuchus) (in that cladogram Crurotarsi = Pseudosuchia). Nothing like a modern croc. 

Even as we move up the tree towards Crocodylia, early crocodyliforms looked like [this](http://en.wikipedia.org/wiki/Protosuchus). These were fairly gracile, terrestrial animals. Again, a similar croc body plan pops up in a few lineages, like in the [phytosaurs](http://en.wikipedia.org/wiki/Phytosaur), which are likely a basal pseudosuchian but not closely related to crocodylians.

[Mesoeucrocodylians](http://en.wikipedia.org/wiki/Mesoeucrocodylia), a grade of crocodyliforms that isn't a valid taxon but useful for referring to groups outside the crown group, often look more like the body plan associated with typical crocodylians, but they also show significantly more morphological diversity than that. Pholidosaurs (like [*Sarcosuchus*](http://en.wikipedia.org/wiki/Sarcosuchus)) and dyrosaurs have a similar body plan. Metriorhynchids like [*Metriorhynchus*](http://en.wikipedia.org/wiki/Metriorhynchus_superciliosus) were marine and had flippers. Notosuchians like [*Simosuchus*](http://en.wikipedia.org/wiki/Simosuchus) are very different. *Simosuchus* probably wasn't even carnivorous. It was also pretty adorable. 

The oldest members of crown-group Crocodylia are more morphologically similar to extant crocodylians. However, you still have morphological variation within Crocodylia, such as the [pristichampsids](http://en.wikipedia.org/wiki/Pristichampsidae), which were terrestrial. Terrestriality shows up again even in the family Crocodylidae (with the Mekosuchinae, including *Quinkana*).

The oldest members definitely attributable the genus *Crocodylus* [date to the Late Miocene](http://www.bioone.org/doi/abs/10.1643/0045-8511%282000%29000%5B0657:PRADTO%5D2.0.CO%3B2) (paywalled, sorry), and the genus probably diverged in the last 10 million years or so. That's pretty recent in the grand scheme of things, and some 55 million years after the non-avian dinosaurs went extinct.

",null,0,cdkrh86,1r8b8y,askscience,top_week,15
null,null,null,2,cdkow4g,1r8b8y,askscience,top_week,2
NAG3LT,"Well, almost all 3D stuff that is made today is Stereoscopic 3D. It means that two images are created with a slight offset in camera position. When one image is shown to one eye, while the other eye sees the slightly different picture we perceive it as a single 3D image. So to display such 3D content, your TV must be capable to display two different images to the two eyes at the same time.


Your standard TV is likely an LCD panel with 50 or 60 Hz refresh rate. Your both eyes see the same image on TV, which is slightly shifted between eyes and you perceive it at the same distance as the TV is. To allow your eyes to see different images you need some additional tricks. The simplest solution is to use special glasses with filters capable of filtering different stuff. 


There is one type of 3D your TV can show with no issues - those 2 colour [Anaglyph images](http://en.wikipedia.org/wiki/Anaglyph_3D). You then use glasses with red and cyan filters (or other colour combo) and one eye sees only red and other only the cyan image. The main issue of this method is colour reproduction, which is awful, so it works best with black and white content. Some colour choices are less bad, but there is no perfect choice for this method. Another type of 3D that normal TV can display is [active shutter 3D] (http://en.wikipedia.org/wiki/Active_shutter_3D_system). You use a glasses with LCD shutters which show 1 frame to right eye while covering the left, second to the left and so on. TV meanwhile displays frames in pairs: R-L-R-L-... , which you also perceive as a 3D view. The refresh rate you see is effectively halved and the smaller it is the more perceptible the flickering is. Using it on a 60 Hz display is possible, but far from comfortable, so TV's using it have 100 Hz or higher refresh rates to get 50 Hz of more perceived 3D refresh rate. 


Another interesting property of light used for 3D displays, but mostly with projectors is the polarisation. The light can come in two different polarisations, like linear vertical and linear horizontal or CW circular and CCW circular. These are just some of possible combinations, but what is important that they are exclusive. You can use a special polariser to pass only CW polarisation while completely blocking CCW. Polarisation is also independent from colour and can be filtered by a passive filter, making glasses with polarised filters relatively cheap. So by showing image for one eye in CW polarisation and other image in CCW, glasses with corresponding filters can allow you to see a 3D film. This technique simply requires the special hardware to show different polarisations and wont work on a normal TV. BTW, all LCD displays emit a linearly polarised light, but only in one polarisation. 


If you don't want to use glasses there are some solutions, but they are less reliable so far and less used. Some TVs use micro lenses to show some pixels only from a specific angle of view. This allows them to show different images to your eyes, which look at TV with a slight difference in angle. Their main problem is the fact that you can only view them from specific spots, otherwise the 3D effect breaks down. 


There are other methods as well, but all of them still require you to have additional hardware to show separate images to each eye. The methods with the highest quality require hardware solutions that normal non-3D TVs almost always lack.",null,0,cdkn5i6,1r8ayj,askscience,top_week,5
threegigs,"3D TVs have alternating pixels which are polarized horizontally and vertically, allowing it to display one vertically polarized image and one horizontally polarized image, which are then selectively filtered out by 3D glasses (one lens horizontal, one vertical).

Alternately, active shutter glasses can be used to make any TV a 3D TV, by presenting alternating images for the left/right eyes, and by using special glasses which alternate turning each lens opaque, thus presenting different images to each eye.",null,1,cdkn8xx,1r8ayj,askscience,top_week,3
wazoheat,"**tl;dr: Wind gusts are usually very shallow, only within a kilometer (0.6 miles) or so of the ground ([simulation video](http://www.youtube.com/watch?v=G7aOwKigyTk)), and are due to turbulence.**

Sudden wind gusts are caused by a few different phenomenon, but the most common is just plain old turbulence. Through most of the depth of the atmosphere, wind speeds are relatively constant over short periods of time. Wind speed and direction above the ground changes all the time, but over the course of hours, unlike the ""gusts"" we experience which only last a few seconds. Friction in the [planetary boundary layer](https://en.wikipedia.org/wiki/Planetary_boundary_layer) (the layer of air closest to the ground) means that the wind above Earth's surface is almost always going to be stronger than the wind near the surface, where people spend the majority of their time. This means that there is a region of high wind speed flowing over a region of relatively low wind speed near the ground. The boundary between these two regions is inherently unstable, which results in turbulence. This turbulence has the consequence of sending some areas of high-velocity wind down towards the ground

[Here is a simulation that probably gives the best visualization of this phenomenon](http://www.youtube.com/watch?v=G7aOwKigyTk). In that video, high winds are marked in red, and calm winds in green. You can see that as time goes on, areas of high winds are brought down to the surface due to the turbulent motions of the boundary layer, causing what we know as a ""gust"" of wind. So to answer your initial question, a cross-section of a gust would look like one of those green areas in the video; it does not extend throughout the atmosphere. The depth of the boundary layer is usually [around 1 km (0.6 mi or 3300 feet)](http://www.met.rdg.ac.uk/~swrhgnrj/teaching/MT36E/MT36E_BL_lecture_notes.pdf), so this is about how ""tall"" a gust would be.",null,0,cdkni9i,1r8apk,askscience,top_week,15
wazoheat,"**tl;dr: Wind gusts are usually very shallow, only within a kilometer (0.6 miles) or so of the ground ([simulation video](http://www.youtube.com/watch?v=G7aOwKigyTk)), and are due to turbulence.**

Sudden wind gusts are caused by a few different phenomenon, but the most common is just plain old turbulence. Through most of the depth of the atmosphere, wind speeds are relatively constant over short periods of time. Wind speed and direction above the ground changes all the time, but over the course of hours, unlike the ""gusts"" we experience which only last a few seconds. Friction in the [planetary boundary layer](https://en.wikipedia.org/wiki/Planetary_boundary_layer) (the layer of air closest to the ground) means that the wind above Earth's surface is almost always going to be stronger than the wind near the surface, where people spend the majority of their time. This means that there is a region of high wind speed flowing over a region of relatively low wind speed near the ground. The boundary between these two regions is inherently unstable, which results in turbulence. This turbulence has the consequence of sending some areas of high-velocity wind down towards the ground

[Here is a simulation that probably gives the best visualization of this phenomenon](http://www.youtube.com/watch?v=G7aOwKigyTk). In that video, high winds are marked in red, and calm winds in green. You can see that as time goes on, areas of high winds are brought down to the surface due to the turbulent motions of the boundary layer, causing what we know as a ""gust"" of wind. So to answer your initial question, a cross-section of a gust would look like one of those green areas in the video; it does not extend throughout the atmosphere. The depth of the boundary layer is usually [around 1 km (0.6 mi or 3300 feet)](http://www.met.rdg.ac.uk/~swrhgnrj/teaching/MT36E/MT36E_BL_lecture_notes.pdf), so this is about how ""tall"" a gust would be.",null,0,cdkni9i,1r8apk,askscience,top_week,15
strummingmusic,"Even though there are no ""crows"" in South America, there are plenty of other corvids. Different types of jays and such in the genuses Calocitta, Cyanocorax, Cyanolycra, etc. Here's a photo I took of a Green Jay in Venezuela some years ago: http://imgur.com/aVoI4Lp

Maybe over time the range of crows will extend down into there, who knows - we're looking at such a short timespan when you really think about things on an evolutional level. ",null,39,cdkorbl,1r8ake,askscience,top_week,209
carolnuts,"But we have corvids here in Brazil! We call them ""gralhas"" down here.  
The most common is the azure ray ( gralha-azul) , who lives in southern  Brazil. [Link](http://farm5.staticflickr.com/4087/5176353885_f94fbabfc1_z.jpg)


But we also have the white-naped jay ( gralha canc ) , who lives in the Northeast semi arid region.  [Link](http://www.criadourosonhomeu.com.br/sonhomeu/images/gralhacanca.jpg)",null,13,cdksjqx,1r8ake,askscience,top_week,73
AshRandom,"This might seem overly simplistic, but the short answer is that there are a vast number of bird species in south america which out-compete the north american crow. 

Also, it's possible that it is a question of the motivation for continental movement which triggers the spread of species. Crows in the southern parts of their range appear to be resident and not migratory. This tendency might also contribute to the explanation for why they have so far failed to take up residency in the southern continent. 

Partial Source: *Dr. Kevin J. McGowan, Cornell Lab of Ornithology.*",null,9,cdkp7e8,1r8ake,askscience,top_week,65
AcaAwkward,The migration pattern observed in crows excludes both New Zealand and southern parts of South America. There is no concensus on the reason behind this,null,0,cdknamr,1r8ake,askscience,top_week,4
null,null,null,1,cdkn03v,1r8ake,askscience,top_week,1
strummingmusic,"Even though there are no ""crows"" in South America, there are plenty of other corvids. Different types of jays and such in the genuses Calocitta, Cyanocorax, Cyanolycra, etc. Here's a photo I took of a Green Jay in Venezuela some years ago: http://imgur.com/aVoI4Lp

Maybe over time the range of crows will extend down into there, who knows - we're looking at such a short timespan when you really think about things on an evolutional level. ",null,39,cdkorbl,1r8ake,askscience,top_week,209
carolnuts,"But we have corvids here in Brazil! We call them ""gralhas"" down here.  
The most common is the azure ray ( gralha-azul) , who lives in southern  Brazil. [Link](http://farm5.staticflickr.com/4087/5176353885_f94fbabfc1_z.jpg)


But we also have the white-naped jay ( gralha canc ) , who lives in the Northeast semi arid region.  [Link](http://www.criadourosonhomeu.com.br/sonhomeu/images/gralhacanca.jpg)",null,13,cdksjqx,1r8ake,askscience,top_week,73
AshRandom,"This might seem overly simplistic, but the short answer is that there are a vast number of bird species in south america which out-compete the north american crow. 

Also, it's possible that it is a question of the motivation for continental movement which triggers the spread of species. Crows in the southern parts of their range appear to be resident and not migratory. This tendency might also contribute to the explanation for why they have so far failed to take up residency in the southern continent. 

Partial Source: *Dr. Kevin J. McGowan, Cornell Lab of Ornithology.*",null,9,cdkp7e8,1r8ake,askscience,top_week,65
AcaAwkward,The migration pattern observed in crows excludes both New Zealand and southern parts of South America. There is no concensus on the reason behind this,null,0,cdknamr,1r8ake,askscience,top_week,4
null,null,null,1,cdkn03v,1r8ake,askscience,top_week,1
NightmareOfLagrange,"Stimulation of epithelial cells in the upper airway from tobacco smoke (both first and secondhand) has been shown to result in both an immediate inflammatory response (SEVERELY exacerbated by any concurrent allergic reactions, I might add) as well as long-term remodeling of the tissue.  Without getting too technical, constant damage and stimulation of the airway tissue causes the tissue to remodel in an attempt to repair and acclimate to the smoke.",null,0,cdkl5ck,1r8ahf,askscience,top_week,1
iorgfeflkd,"Yes, that's why you get two-slit interference patterns when you do the experiment with electrons.",null,3,cdkkuxr,1r89ry,askscience,top_week,9
nanopoop,Another example is neutron scattering.,null,1,cdkl3y0,1r89ry,askscience,top_week,4
selfification,Also consider bonding and anti-bonding orbitals.  That's literally electron wave functions overlapping in phase and out of phase.,null,1,cdkopgo,1r89ry,askscience,top_week,4
Trill-Nye,"The two most prominent examples, diffraction of electrons and neutrons, have been mentioned. To expand on this, all particles can be made to interfere if suitable conditions are imposed. This is easiest to do with subatomic particles. By shooting energetic electrons or neutrons at some kind of grating for which the spacing of the grates are on the order of the wavelength of the particles used, one can produce a sort of ""many-slit"" experiment. 

Crystalline materials are good for this, because they have regular atomic spacings, such that the atoms scatter incoming particles, generating quasi-point sources of scattered particles similar to the slits in a double slit experiment. If you then have detectors set up to measure where these particles end up, you will see an interference pattern where certain places get many hits (where constructive interference of the wave-like particles occurs) and other places get few or none (where destructive interference occurs).

Interestingly, diffraction using [x-rays](http://en.wikipedia.org/wiki/X-ray_diffraction#Overview_of_single-crystal_X-ray_diffraction) (photons) gives similar results to [neutron](http://en.wikipedia.org/wiki/Neutron_diffraction) and [electron](http://en.wikipedia.org/wiki/Electron_diffraction) diffraction, because all three behave as waves in a scattering experiment, even though the latter two are particles.",null,0,cdklu16,1r89ry,askscience,top_week,3
bertrussell,"Destructive interference doesn't mean that the matter disappears or cancels itself out.

Destructive interference means that the there is an interference pattern in the position/momentum pattern for the objects.

When light undergoes destructive interference in one location, there is necessarily constructive interference in another location. This means that the light is more likely to interact in the constructive interference location than in the destructive interference location. The same is true for matter that interferes.",null,0,cdkrgwe,1r89ry,askscience,top_week,2
Brodken,"I think your question have been answered already, but I will contribute with an example I personally like a lot. 

You can macroscopically see matter interference with Bose-Einstein condensates. Around 1995-1996 there started to be a lot of experiments of interference with this condensates (formed by millions of particles). All this macroscopic system *behaves* like one giant quantum particle, and as such, it behaves as a huge wave. This is what we call a giant matter wave. 

I find this so incredibly awesome, the fact that we can actually see in a experiment, in a direct way, the inteference of two macroscopic clouds of atoms.",null,0,cdkuce8,1r89ry,askscience,top_week,2
clade_nade,"One not-yet-posted example is [Anderson localization](http://en.wikipedia.org/wiki/Anderson_localization), which is basically the result of electron wavefunctions canceling themselves out in disordered solids.*

*Actually I'm not sure if this has been experimentally observed for electrons, but the theory is quite solid and it's worked for photons, so...",null,0,cdl8gli,1r89ry,askscience,top_week,2
polandpower,"Yes, it's not a ""strong law"" in that you could, theoretically, observe the reverse happening. However, the chance of heat flowing from cold to hot on human observable time scales is so *astronomically* low that in practice the law is never violated.

If you look for instance at a gas, you can compute the microstates and see just how gigantically more likely heat is to flow from hot to cold. 

If you're interested in reading more about the topic, I definitely recommend going to your (University) library or Amazon to read [""Thermal Physics"" by Daniel V Schroeder](http://www.amazon.com/Introduction-Thermal-Physics-Daniel-Schroeder/dp/0201380277). It's very well written and pretty readable if you've had high school physics. 

",null,0,cdkp8za,1r88ue,askscience,top_week,6
Mooslletoe,"Everything goes from high to low, unless you do work. That is the second law of thermodynamics. The hot and cold try to equalize and reach equilibrium unless you do work on the system. This also applies to grades in school (assuming you start with an A), your grade will drop unless you do work. ",null,10,cdklifh,1r88ue,askscience,top_week,3
afranius,"Bitcoin and peer-to-peer data sharing are about equally anonymous. When people say bitcoin is anonymous, they don't mean that it's impossible to find out who paid for what, they mean that you don't have to use a bank account or credit card number (which usually requires supplying your real name, DOB, etc and stores a history of your transactions). You still need to use a computer to transmit the information, which exposes an IP address, which can in principle be traced back to you. The same is true for peer-to-peer data sharing. You can mitigate this by not using a connection registered to you (internet cafe, public wifi) and taking other steps to anonymize your traffic (proxies, etc.), and that applies equally to both bitcoin and peer-to-peer traffic.

To summarize, if the FBI wants to prove that you paid for something with bitcoin, they can show that the payment data came from your computer, just as they can show that your computer was involved in peer-to-peer file sharing. Methods to obfuscate your identity are equally applicable to both.",null,0,cdknq1u,1r87g0,askscience,top_week,8
Daegs,"While other posters are correct that bitcoin isn't anonymous, often transactions are made through the Tor network, which makes it anonymous.

The reason for this working for bitcoin and not P2P, is that the Tor network has a lot of overhead, leading to high latencies and very small bandwidth. 

For transmitting a bitcoin transaction, only a few ID's and keys need to be passed around, so even on extremely slow connection, it will be very fast. For P2P, this might make a 5 min download take a day or more. ",null,0,cdkp7yv,1r87g0,askscience,top_week,3
null,null,null,1,cdknxni,1r87g0,askscience,top_week,1
KarlOskar12,"Mythbusters actually demonstrate how much a stomach can stretch before bursting in this [clip](http://www.youtube.com/watch?v=93vjY9RY4-k). However, one technique competitive eaters use is to use abdominal muscle contractions to force food from the stomach into the small intestines creating more room for them to continue eating. Normally this would likely cause one to vomit, but through practice they have been able to bypass the reflex. Their bowel movements must be massive after a hot dog eating competition, although it would probably feel better if they just vomit a lot of it up. And they drink plenty of fluids so they don't get dehydrated.",null,0,cdkyz6w,1r86rk,askscience,top_week,2
proule,"Simply put: Genes come in different versions referred to as ""alleles"". Different alleles show different degrees of dominance over each other, and on top of that, some alleles may show incomplete dominance when expressed along with a certain other allele. 

In the dominant-recessive gene interaction, one gene's visible end effect dominates and is all you see. In incomplete dominance, two alleles are expressed where neither fully dominates the other.

In reference to the eye colour question: Eye colour tends to be more of a dominant-recessive relationship. The allele for brown eyes dominates that for blue, so we don't get any sort of brown-blue intermediate. This works similarly for green and blue.

Skin pigmentation is the result of several (I believe around 6?) separate genes relating to the amount of pigment produced, the ability for pigment to be shipped to the proper place in the body, stuff like that. In this case, the myriad of different genes contributing to skin pigmentation result in expression that mirrors incomplete dominance (but is actually slightly more complicated than the normal single-gene dominance question).

There are some concrete reasons that some traits mix and others don't. People with blue eyes have significantly reduced melanin in their irises, so any allele for eye colour that isn't blue will just overwrite blue eyes, since the trait is due to little pigmentation (so it can just be overwritten essentially, by the presence of said pigment). The reasons for other allelic interactions vary by case, and there's likely no satisfactory answer that's all encompassing.",null,0,cdklnkc,1r847u,askscience,top_week,5
ToThink,"I'm only in my 3rd year studying genetics, so I hope I can get this through properly :)

First we have to establish the fact that within a gene, there are alleles of that trait. Alleles are just different forms of a gene, so for eye colour there is a ""green eye"" allele and there is a ""blue eye"" allele. I put the quotations there because eye colour isn't due to a specialised pigment which reflects that colour of light (e.g., blue eyes aren't due to a pigment which reflects blue lights), rather eye colour is dependent on the amount of melanin in the eye. So blue eyes = very low melanin
Similarly with skin colour, the amount of melanin in your skin determines how dark you are.
Currently, one of the explanations behind why a parent with blue eyes and a parent with green eyes don't make a turquoise eye baby is because of dominance of alleles. The child will most likely have green eyes because the green eye allele is ""dominant"" to the blue eye allele. This isn't completely true but eye colour works in a ""dominance-recessiveness"" sort of manner.
There are new studies which reveal this may not be the case, rather some new studies reveal eye colour is dependent on many different genes. (I tried looking for them, couldn't find them).",null,1,cdkkf68,1r847u,askscience,top_week,6
Fignot,"Codominance can be a funny thing. There's a type of codominance that is unique to genes located on the X-chromosome, because women develop as genetic chimera's of themselves.

Another example is in blood where the A and B are codominate. This is because most genes you have are being expressed, even the recessive ones. In the case of the blood genes though any amount of antibody production is enough to make certain types of blood incompatible with you. Since people with the AB blood type are producing A and B antibodies, they will react with blood that contains the relevant antigens.

On the other hand though you have traits like hair colour. Say you have the blond hair allele, and the brown hair allele. You'll make the pigments of both colours, but the brown will overpower the blonde. So you'll still have brown hair, but maybe you'll have a slightly lighter shade of brown, or some blond highlights.

*source: I went to school for Bioinformatics.",null,1,cdkrzvy,1r847u,askscience,top_week,2
shavera,"well it does always travel at the same speed. And if the space between two points is expanding, you could imagine that it might be that light would appear *as if* the light was being slowed, since it wasn't covering the right amount of distance in time. 

What **actually** happens is that the light is stretched out while it travels, so that the speed stays the same, even though distance is added between things. 

The other thing maybe you're getting confused about (I'm not sure) is that both wavelength is elongated and frequency is reduced. If you work out the wave equations, they still work for speed of light = c",null,3,cdkjmnc,1r845k,askscience,top_week,6
bohr_exciton,"The red shift of light is a consequence of the fact that energy is not Lorentz invariant. In simpler terms, an object will be perceived as having different energies depending on your relative motion with respect to it. For instance, take a basketball on a bus. In the stationary frame of reference of an observer on the sidewalk, the ball has a kinetic energy due to its motion with the bus, but in the frame of reference of someone on the bus, the ball would appear stationary and have no kinetic energy.

Now think of light (or photons) as the basketball. The energy of the light is given by h*f, where h is Plank's constant and f is the frequency. Two observers moving relative to one another will observe the photon to be moving with the same speed. However, at the same time, they will measure different energies for the light, which due to the relation above means that they will observe different frequencies/wavelengths. 

Since there is a known relationship between the change in apparent frequency and the relative motion between two reference frames, we can use this information to calculate the relative motion of say an object emitting that light towards us assuming we know the energy of the light in the frame of reference of the body emitting it. ",null,1,cdklf0z,1r845k,askscience,top_week,3
jimustanguitar,"Does the wavelength shift along with time and space, and I shouldn't be thinking of speed and wavelength as being locked together?",null,0,cdkiz30,1r845k,askscience,top_week,1
LaLife,"**EDIT:** *It turns out that it's both: there is both Doppler (related to the motion of the point of light emission) and Cosmological (related to space expansion during travel) redshifts, and these both play into the redshift component. The funny thing is that redshift at relativistic velocities can happen even when the objects are not moving away from each other, due to time-dilation effects (Ives-Stilwell experiment).*

To put it simply: the wavelength of the light is a different quantity than the velocity. Any given photon travels at constant velocity c. It's the wavelength of the photon that is 'stretched' at the point and at the time of emission of the photon, due to the relative speed of the light source to the observer.

The redshift is not a measure of the expansion of the universe, it's a measure of the velocity of the light source. We know the universe is expanding by virtue of the fact that the furthest a galaxy is, the faster it's moving away from us.",null,0,cdkred8,1r845k,askscience,top_week,1
Spiralofourdiv,"I see a lot of kinda complex answers here, so let me take a crack a a simple one:

Light is indeed a fixed speed for all reverence frames, etc. but the color of light has nothing to do with speed, it only has to do with frequency.

Dopler shift **doesn't** ""speed up"" or ""slow down"" light, it's actually entirely dependent on the fact that neither can happen! Since light moves at a fixed speed for all observers, if the source and/or target are moving, then the wavelength needs to adjust correspondingly so that there can be n/2 wavelengths between the objects (that's the ""quantum"" part of quantum mechanics, no ""23.1495 wavelengths"" allowed, only 1/2, 1, 3/2, 3, ...). When the wavelength is ""pulled out"" because the targets are moving away from each other, the light is red. It's not slower light, it's simply lower wavelength/frequency light. 

This actually makes some intuitive sense if you consider that frequency is a measure of energy. The following is a bit naive, but I think it's fair. It makes sense that light has to travel farther to reach targets that are moving away, and traveling farther takes more energy, so the light we perceive is lower frequency/energy. Similarly, objects moving towards each other ""give"" energy to the light since it's reaching it's target in shorter distance. The energy can't disappear, so the frequency is higher.",null,0,cdkwrch,1r845k,askscience,top_week,1
-Rookery-,While the speed of light is still the same certain aspects of the light change. As the wavelength of the light increases it red shifts. To ensure that the speed of light is the same (c=wavelength * frequency) the frequency of the photon needs to decrease.,null,0,cdlpoek,1r845k,askscience,top_week,1
shavera,"Okay so imagine you have the simplest circuit. Current flows around a simple loop of wire. Now, you cut the wire. The electrons are still in motion, so one end of the cut gets a little bit depleted of electrons, and the other one gets an electron pile-up. But this won't last too long, since the depleted end now has a positive charge, and the pile-up end is negatively charged, the electrons will be pulled in the opposite direction somewhat.

A simple analogy is like a circular pipe of water that's got water flowing around through it. Imagine you were to cut the pipe *and* seal the ends simultaneously. There would be a back pressure at one end, and a low pressure at the other end, causing the flow to slow down and stop. (neglecting some other effects we'll get to in a bit like ""ringing"")

So what if you wanted the effect to last longer, to take a bit longer before the circuit slowed to a halt, what could you do? Well you could take the ends of each wire and add more and more metal to them, giving the electrons more space to spread out on the pile-up end, and more electrons to draw from on the depletion end. (we'll call the pile-up end the negative plate, and the depletion end the positive plate from now on). And if those plates were closer together, the electrons would ""see"" the opposite plate more easily and so not feel the effect as strongly (ie, electrons travelling toward the negative plate and being repelled would also be attracted to the positive plate just slightly beyond it, and so the overall repulsion would be relatively less). 

So that describes our basic ""parallel plate capacitor:"" The amount of charge you can ""store"" in the capacitor, its 'capacitance' is proportional to the area and inversely proportional to the distance between the plates. C = k A / d (where k is just a constant to show proportionality).

---

Now let's look at the next step. We have these plates of charge, one positive and one negative. The current has stopped. well since we have a potential difference, that's going to drive a current in the opposite direction, no? The electrons on the negative plate are going to be pushed off of it and pulled onto the positive plate. But since they can't cross the gap between the plates (deal with this a bit later) they have to go the opposite way of the current that charged the plates up; they are 'discharging.'

---

So now let's put a little more into it. Clearly if you just pushed the electrons harder, you could push more electrons onto the plate, so we can also make a new equation Q=CV, where the charge Q is given by the structure of the capacitor itself (how it distributes charges) and the Voltage (how strongly charges are being pushed onto the plate).

Furthermore, we'd like to see how these plates change over time, right? Well, the amount of charge being added or removed from the plates is going to relate to how much charge is already on the plates, right? If they're really charged up, there are going to be very few new charges added to them (if we're charging) or a *lot* of charges leaving them (if we're discharging). And if we want something where the rate of change is proportional to the value at that moment, we want an exponential. Some kind of e^t function. 

Well let's consider discharging full plates. They start with some charge Q0 and get rid of a lot quickly. But as time goes on, there's less pressure to push new charges out, so they lose less charge. So if we look at e^t, we see that it goes from 0 at t=-infinity, passes through ""1"" at t=0 and then climbs high after that. What we'd like to do is flip that graph around the t=0 axis, right? So that it starts with some value and trends toward 0 after very long times. Flipping about that axis takes t to -t. So we can say that Q(t) = Q0 * e^-t . 

Now let's think of charging. Again we know that to start there's no charge, so we want rapid change. But we also want it to level off to some constant value, where the ""back-pressure"" from the charge on the plates is equal to the ""forward-pressure"" we're using to charge it with. So we look at our e^t again. Well that won't work since it goes up to infinity, which isn't reality. We try flipping it about the t=0 axis again (e^-t ), since that at least levels off. But this graph is decreasing charge, and we want increasing. So we multiply the whole thing by a -1, giving us -e^-t . Well the charge is increasing from a negative 1 to zero... that's not right either, since it starts at 0 and goes to some steady value. So let's add 1 to it, and we get 1 - e^-t . It starts at 0 and increases to 1 in a way that's proportional to the amount that's on the plate. That's nearly what we're looking for. If we just multiply the whole thing by its steady constant charge (Q0) then we'll have it. Q(t) = Q0 * (1 - e^-t ). ",null,2,cdkkfxd,1r841l,askscience,top_week,46
njaard,"Water metaphor:

Imagine a sphere in which right in the middle is a membrane made of rubber, and tubes go out on either end:

           ___
    ======( | )======
           ---

If you push water on one side, the rubber membrane starts stretching in the other direction. If you release that pressure, the membrane pushes water back.

Now replace ""water"" with ""current"" and ""push"" with ""apply voltage"".",null,2,cdkjvwd,1r841l,askscience,top_week,7
PorchPhysics,"A lot of the examples given here are great, but capacitors have another use in AC circuits.

Capacitors in AC circuits have decreased impedance at higher frequencies.  Impedance is effectively like resistance, but is more dependent on other factors in the circuit. 

One place where this comes in handy is for tweeters in speakers, they want only the high frequency sounds to come out so that the lower sounds are not overpowering.  Hook up a capacitor in series and it will filter out lower frequencies by resisting them more than the higher ones.  Since lower frequencies are already overpowering when coming out of a subwoofer, they don't need a similiar filter (but you could in theory use an inductor to the same effect on higher frequencies).",null,2,cdklbkp,1r841l,askscience,top_week,5
flawless-contempt,"The capacitor for a hydraulic system would be your accumulator and it basically does the same things.  Stores ""presure/voltage"" until a need arises, and creates a shock absorber for the system in case any irregularities occur as not to damage the system.  It has a very fast discharge rate and both are equally dangerous when not handled with proper care and due respect. 

Edit: Also a capacitor will only block direct current (dc)  it allows alternating current (ac)  to pass through. 
An accumulator does this by tying into a single direction flow. Meaning it only has one line in and will push its charge directly into a single line of the system there by absorbing impacts to the system or similarly ultimately supplying power.   Hydraulic systems and basic electrical systems are very identical. ",null,1,cdkl7qd,1r841l,askscience,top_week,2
Lost_Afropick,"I was using some at work yesterday for power factor correction.

You can read about it [here](http://www.kwsaving.co.uk/Business/pfc/pfc-simple.htm)

But I had to supply an inductive load, a big transformer with a certain amount of amps and because of it's impedence that would take a certain amount of volts to do.

Only my supply doesn't have that kind of V/A power and that current is slightly higher than what my supply regulator will allow (it's windings may burn).   So I used capacitors to get me a few extra amps.  

Use the link i put above to see how.  But In a purely resistive circuit your current will be in phase (rise and fall at the same point of a cycle) as your volts.  With an inductive load like my transformer, the current lags by 90degrees.  With a capacitive circuit the current leads the volts by 90degrees.  So adjusting the capacitance lets you play around with the angle of the current in this RLC circuit.  The current the transformer sees is still it's high rated current but I'm drawing less amps from my supply.  

[Here's more](http://en.wikipedia.org/wiki/Power_factor#Power_factor_correction_of_linear_loads)",null,1,cdklepb,1r841l,askscience,top_week,2
Mathness,"A simple system that will show the effect of charge and discharge is an astable multivibrator with two bulbs/LEDs, the time each bulb is on and off can be set by the capacitors (and resistors). Also fairly simple to calculate the times and show that the theory fits reality.

Another is an AM radio, which can be build with very few components.

Capacitors have a lot different uses, to list some basic uses. Block AC and let DC pass (and vice versa), change the frequency response of a system (filtering sound/noise for instance) and generating a specific frequency (for example in a radio).",null,1,cdkma0z,1r841l,askscience,top_week,2
SkyDolphin99,"Thanks for your replies everyone. I appreciate your effort to explain it to me, but I really can't understand I'm afraid. I would just like you to go slower in terms of explaining. :)",null,0,cdkrrep,1r841l,askscience,top_week,1
kajarago,"Other folks here have discussed the physical theory of the capacitor.  This explanation outlines some of the uses of capacitors in a circuit.

The function of the capacitor will depend on the type of input voltage (alternating or direct) as well as the placement of the component relative to the circuit.

The capacitor is very useful in a circuit because its impedance (electrical ""resistance"") is a function of the capacitance and the frequency of the signal as Z = 1/(2*pi*f*C).  This means that a system can be tuned to a certain band of frequencies or can be used as a filter depending on the application.  Take this simple circuit as an example:

http://upload.wikimedia.org/wikipedia/commons/e/e0/1st_Order_Lowpass_Filter_RC.svg

The capacitor will ""hold"" the lower-frequency components of the signal so you will see DC signals have the full* amplitude of the input signal, and the amplitude will roll off to zero as the signal frequency component increases to infinity.

In other more extreme cases, the amount of charge stored in a capacitor is very high and can be released to power devices like a camera's flash or an electric car.",null,0,cdksko5,1r841l,askscience,top_week,1
fourpenguins,[Related question](http://www.reddit.com/r/AskElectronics/comments/1pru9j/what_does_a_capacitor_do_what_are_they_used_for/) from three weeks ago in /r/AskElectronics,null,0,cdky9te,1r841l,askscience,top_week,1
knflrpn,"One of the useful properties of capacitors that I don't think anyone has [directly] mentioned yet is that the current through them can change instantaneously, while the voltage across them can not.


This is why they're used to ""smooth out"" voltage.  If the current in some device needs to change very quickly (as in, for example, a computer's CPU), but the power supply is relatively far away (e.g. somewhere else on the motherboard) then capacitors nearby will prevent the voltage from changing at the device (or at least mitigate it).


The reason that being ""far away"" matters is the corollary to the capacitor: the inductor.  For an inductor, the voltage across it can change instantaneously but the current can not.  Wires have inductance, so if the power supply is somewhere else, the inductance of the wires can cause problems with fast changes in current.",null,1,cdkzi8u,1r841l,askscience,top_week,1
whatzefuk,"best to test and check it out on a oscilloscope and you will understand it fully.
it pretty much packs electrons and releases them when its full , and you can influence curbs and yeah theres plenty of uses like Mathness said , you can pulse dc , use it as a filter , to turn from ac to dc its a diode bridge you will use , you dont have to use a cap but its highly recommnended to filter out.

same goes for neon lights , balasts you need a jolt top turn on the chemical reaction in the neon light but after that it can run on very low power , to create that jolt your gonna use caps.

also since caps hold on electrons after putting something on power be advised that it might hold a charge , small farad caps you can worry too much but once you get to the big boys it can be lethal , turning off the power and time will have it decay its charge.

Some capacitors have poles also , in my course it was always funny to hear a gunshot somewhere in the class you knew someone plugged his cap on the wrong pole.
",null,4,cdknkys,1r841l,askscience,top_week,2
shavera,"Okay so imagine you have the simplest circuit. Current flows around a simple loop of wire. Now, you cut the wire. The electrons are still in motion, so one end of the cut gets a little bit depleted of electrons, and the other one gets an electron pile-up. But this won't last too long, since the depleted end now has a positive charge, and the pile-up end is negatively charged, the electrons will be pulled in the opposite direction somewhat.

A simple analogy is like a circular pipe of water that's got water flowing around through it. Imagine you were to cut the pipe *and* seal the ends simultaneously. There would be a back pressure at one end, and a low pressure at the other end, causing the flow to slow down and stop. (neglecting some other effects we'll get to in a bit like ""ringing"")

So what if you wanted the effect to last longer, to take a bit longer before the circuit slowed to a halt, what could you do? Well you could take the ends of each wire and add more and more metal to them, giving the electrons more space to spread out on the pile-up end, and more electrons to draw from on the depletion end. (we'll call the pile-up end the negative plate, and the depletion end the positive plate from now on). And if those plates were closer together, the electrons would ""see"" the opposite plate more easily and so not feel the effect as strongly (ie, electrons travelling toward the negative plate and being repelled would also be attracted to the positive plate just slightly beyond it, and so the overall repulsion would be relatively less). 

So that describes our basic ""parallel plate capacitor:"" The amount of charge you can ""store"" in the capacitor, its 'capacitance' is proportional to the area and inversely proportional to the distance between the plates. C = k A / d (where k is just a constant to show proportionality).

---

Now let's look at the next step. We have these plates of charge, one positive and one negative. The current has stopped. well since we have a potential difference, that's going to drive a current in the opposite direction, no? The electrons on the negative plate are going to be pushed off of it and pulled onto the positive plate. But since they can't cross the gap between the plates (deal with this a bit later) they have to go the opposite way of the current that charged the plates up; they are 'discharging.'

---

So now let's put a little more into it. Clearly if you just pushed the electrons harder, you could push more electrons onto the plate, so we can also make a new equation Q=CV, where the charge Q is given by the structure of the capacitor itself (how it distributes charges) and the Voltage (how strongly charges are being pushed onto the plate).

Furthermore, we'd like to see how these plates change over time, right? Well, the amount of charge being added or removed from the plates is going to relate to how much charge is already on the plates, right? If they're really charged up, there are going to be very few new charges added to them (if we're charging) or a *lot* of charges leaving them (if we're discharging). And if we want something where the rate of change is proportional to the value at that moment, we want an exponential. Some kind of e^t function. 

Well let's consider discharging full plates. They start with some charge Q0 and get rid of a lot quickly. But as time goes on, there's less pressure to push new charges out, so they lose less charge. So if we look at e^t, we see that it goes from 0 at t=-infinity, passes through ""1"" at t=0 and then climbs high after that. What we'd like to do is flip that graph around the t=0 axis, right? So that it starts with some value and trends toward 0 after very long times. Flipping about that axis takes t to -t. So we can say that Q(t) = Q0 * e^-t . 

Now let's think of charging. Again we know that to start there's no charge, so we want rapid change. But we also want it to level off to some constant value, where the ""back-pressure"" from the charge on the plates is equal to the ""forward-pressure"" we're using to charge it with. So we look at our e^t again. Well that won't work since it goes up to infinity, which isn't reality. We try flipping it about the t=0 axis again (e^-t ), since that at least levels off. But this graph is decreasing charge, and we want increasing. So we multiply the whole thing by a -1, giving us -e^-t . Well the charge is increasing from a negative 1 to zero... that's not right either, since it starts at 0 and goes to some steady value. So let's add 1 to it, and we get 1 - e^-t . It starts at 0 and increases to 1 in a way that's proportional to the amount that's on the plate. That's nearly what we're looking for. If we just multiply the whole thing by its steady constant charge (Q0) then we'll have it. Q(t) = Q0 * (1 - e^-t ). ",null,2,cdkkfxd,1r841l,askscience,top_week,46
njaard,"Water metaphor:

Imagine a sphere in which right in the middle is a membrane made of rubber, and tubes go out on either end:

           ___
    ======( | )======
           ---

If you push water on one side, the rubber membrane starts stretching in the other direction. If you release that pressure, the membrane pushes water back.

Now replace ""water"" with ""current"" and ""push"" with ""apply voltage"".",null,2,cdkjvwd,1r841l,askscience,top_week,7
PorchPhysics,"A lot of the examples given here are great, but capacitors have another use in AC circuits.

Capacitors in AC circuits have decreased impedance at higher frequencies.  Impedance is effectively like resistance, but is more dependent on other factors in the circuit. 

One place where this comes in handy is for tweeters in speakers, they want only the high frequency sounds to come out so that the lower sounds are not overpowering.  Hook up a capacitor in series and it will filter out lower frequencies by resisting them more than the higher ones.  Since lower frequencies are already overpowering when coming out of a subwoofer, they don't need a similiar filter (but you could in theory use an inductor to the same effect on higher frequencies).",null,2,cdklbkp,1r841l,askscience,top_week,5
flawless-contempt,"The capacitor for a hydraulic system would be your accumulator and it basically does the same things.  Stores ""presure/voltage"" until a need arises, and creates a shock absorber for the system in case any irregularities occur as not to damage the system.  It has a very fast discharge rate and both are equally dangerous when not handled with proper care and due respect. 

Edit: Also a capacitor will only block direct current (dc)  it allows alternating current (ac)  to pass through. 
An accumulator does this by tying into a single direction flow. Meaning it only has one line in and will push its charge directly into a single line of the system there by absorbing impacts to the system or similarly ultimately supplying power.   Hydraulic systems and basic electrical systems are very identical. ",null,1,cdkl7qd,1r841l,askscience,top_week,2
Lost_Afropick,"I was using some at work yesterday for power factor correction.

You can read about it [here](http://www.kwsaving.co.uk/Business/pfc/pfc-simple.htm)

But I had to supply an inductive load, a big transformer with a certain amount of amps and because of it's impedence that would take a certain amount of volts to do.

Only my supply doesn't have that kind of V/A power and that current is slightly higher than what my supply regulator will allow (it's windings may burn).   So I used capacitors to get me a few extra amps.  

Use the link i put above to see how.  But In a purely resistive circuit your current will be in phase (rise and fall at the same point of a cycle) as your volts.  With an inductive load like my transformer, the current lags by 90degrees.  With a capacitive circuit the current leads the volts by 90degrees.  So adjusting the capacitance lets you play around with the angle of the current in this RLC circuit.  The current the transformer sees is still it's high rated current but I'm drawing less amps from my supply.  

[Here's more](http://en.wikipedia.org/wiki/Power_factor#Power_factor_correction_of_linear_loads)",null,1,cdklepb,1r841l,askscience,top_week,2
Mathness,"A simple system that will show the effect of charge and discharge is an astable multivibrator with two bulbs/LEDs, the time each bulb is on and off can be set by the capacitors (and resistors). Also fairly simple to calculate the times and show that the theory fits reality.

Another is an AM radio, which can be build with very few components.

Capacitors have a lot different uses, to list some basic uses. Block AC and let DC pass (and vice versa), change the frequency response of a system (filtering sound/noise for instance) and generating a specific frequency (for example in a radio).",null,1,cdkma0z,1r841l,askscience,top_week,2
SkyDolphin99,"Thanks for your replies everyone. I appreciate your effort to explain it to me, but I really can't understand I'm afraid. I would just like you to go slower in terms of explaining. :)",null,0,cdkrrep,1r841l,askscience,top_week,1
kajarago,"Other folks here have discussed the physical theory of the capacitor.  This explanation outlines some of the uses of capacitors in a circuit.

The function of the capacitor will depend on the type of input voltage (alternating or direct) as well as the placement of the component relative to the circuit.

The capacitor is very useful in a circuit because its impedance (electrical ""resistance"") is a function of the capacitance and the frequency of the signal as Z = 1/(2*pi*f*C).  This means that a system can be tuned to a certain band of frequencies or can be used as a filter depending on the application.  Take this simple circuit as an example:

http://upload.wikimedia.org/wikipedia/commons/e/e0/1st_Order_Lowpass_Filter_RC.svg

The capacitor will ""hold"" the lower-frequency components of the signal so you will see DC signals have the full* amplitude of the input signal, and the amplitude will roll off to zero as the signal frequency component increases to infinity.

In other more extreme cases, the amount of charge stored in a capacitor is very high and can be released to power devices like a camera's flash or an electric car.",null,0,cdksko5,1r841l,askscience,top_week,1
fourpenguins,[Related question](http://www.reddit.com/r/AskElectronics/comments/1pru9j/what_does_a_capacitor_do_what_are_they_used_for/) from three weeks ago in /r/AskElectronics,null,0,cdky9te,1r841l,askscience,top_week,1
knflrpn,"One of the useful properties of capacitors that I don't think anyone has [directly] mentioned yet is that the current through them can change instantaneously, while the voltage across them can not.


This is why they're used to ""smooth out"" voltage.  If the current in some device needs to change very quickly (as in, for example, a computer's CPU), but the power supply is relatively far away (e.g. somewhere else on the motherboard) then capacitors nearby will prevent the voltage from changing at the device (or at least mitigate it).


The reason that being ""far away"" matters is the corollary to the capacitor: the inductor.  For an inductor, the voltage across it can change instantaneously but the current can not.  Wires have inductance, so if the power supply is somewhere else, the inductance of the wires can cause problems with fast changes in current.",null,1,cdkzi8u,1r841l,askscience,top_week,1
whatzefuk,"best to test and check it out on a oscilloscope and you will understand it fully.
it pretty much packs electrons and releases them when its full , and you can influence curbs and yeah theres plenty of uses like Mathness said , you can pulse dc , use it as a filter , to turn from ac to dc its a diode bridge you will use , you dont have to use a cap but its highly recommnended to filter out.

same goes for neon lights , balasts you need a jolt top turn on the chemical reaction in the neon light but after that it can run on very low power , to create that jolt your gonna use caps.

also since caps hold on electrons after putting something on power be advised that it might hold a charge , small farad caps you can worry too much but once you get to the big boys it can be lethal , turning off the power and time will have it decay its charge.

Some capacitors have poles also , in my course it was always funny to hear a gunshot somewhere in the class you knew someone plugged his cap on the wrong pole.
",null,4,cdknkys,1r841l,askscience,top_week,2
800gpm,"Considering the radius of a bottle opening, the additional thickness is not a minuscule factor. I can't measure right now, but estimating the radius at 2cm and the thickness of a towel under pressure at 2mm, the moment of force would increase by 10% (assuming the same force). 

Another factor may be that the towel makes it less painful and thus lets you apply more force before reaching your pain limit.",null,0,cdkkmii,1r833i,askscience,top_week,3
ozone_one,"Cell towers generally have a relatively low power signal (a few miles to tens of miles of range), and are meant to cover a specific area. When you get close to the border of a cell you are handed off to the next cell.

You would not be likely to receive a cell signal in a plane at 35,000 feet for a couple of reasons...  1) the vast majority of the country doesn't really have much cell service.  Look at a coverage map - all of those white areas have no coverage.  and 2) You are moving so fast in the plane that your phone is not able to lock on to a particular tower, and if it does lock on it will be unable to hand you off to the next tower.
",null,0,cdkk8fo,1r7xrc,askscience,top_week,7
BizQuit,"3d printers that use DLP projectors are more expensive because they require a powerful projector to initiate polymerization of photosensitive resin. 

The more common hobbyist 3d printers are simply melting plastic and drawing lines with it requiring little expense as they are only slightly more than a hot glue gun. These common printers are very limited in just how fine a line they can draw, and they must create their work very slowly drawing out the entire parts layer.

With that understanding, It is easiest to think of a Dlp based 3d printer as a photographic process. Each layer is ""drawn"" not line by line but entire layer at once. An image of each layer is projected for a few seconds. Areas where pixels are dark remain fluid and uncured. Pixels that are illuminated cause a chemical reaction which causes solidification of the resin. The printer must then advance in only one direction UP or down depending on the design of the printer, allowing new resin to flow, and another layer can be ""exposed"". 

The tradeoff here comes in resolution and maximum part size. In the melt and deposit printers where you are limited by the fineness of the drawn line (minimum features of 200-400 microns at its best) you can, within the boundaries of the mechanical construction of your printer print parts of any size. The fineness of motor movement can make these parts seem quite smooth, but the ""line thickness"" can significantly limit small features.

Whereas the DLP printer has a fixed resolution in general no greater than 1920X1080.  If you fix your projector at a focus in a similar size to the line of an extrusion printer you only get to print (@200microns 15.1X8.5 inches) and you do not get the smoothing effect of a motor gliding along. BUT unlike these melt and draw printers, a DLP printer can attain VERY fine focus creating individual pixels 50 microns, 20 microns, a few have successfully managed as fine as 5 microns with off the shelf projectors. But each step down shrinks the total field printed. @20 microns you only get 1.5X.85 inches but the resulting part is amazingly detailed as a result. 

All that said, Photopolymer is more expensive than filament. The projector adds expense. And for many hobbyists the resolution is not worth the tradeoff in build size. If you are looking to make large objects it is not the right technology. If you are looking to make fine jewelry, intricate small scale models, dental work (yep labs are making dental crowns with these printers) right now the speed and resolution cannot be beat by any other hobbyist attainable technology. ",null,0,cdl6044,1r7wig,askscience,top_week,1
Marsdreamer,"We breath oxygen today primarily because of organisms called cyanobacteria which created organic compounds from H2O and CO2; turning it into O2. Before this time there was no or very little atmospheric oxygen and it was actually toxic to most other organisms. This catapulted the Oxygen Revolution and is largely responsible for shaping the world as we know it today. 

Fast forward to your more pertinent question, which the answer is The Electron Transport Chain. You see, in order for our bodies to generate chemical energy in the form of ATP we metabolize organic compounds. Through some very lengthy and complex chemistry electrons are stripped from atoms and other molecules to generate an electrochemical gradient responsible for powering the generation and packaging of dense energy molecules known as ATP. 

During this process of ripping negative charges (Electrons) from organic compounds a build up of electrons takes place and they *must* go somewhere. 

Enter Oxygen. Oxygen loves electrons and it is *highly* electronegative. Because of this our bodies have adapted to harvest oxygen for the use of dumping the excess electrons generated from the Electron Transport Chain. Oxygen takes the electrons, binds with Carbon, and then is exhaled. 

*Gen Bio was a long time for me, but this is what I remember. I'm going to double check my sources now and would also appreciate other biologists to check my explanation as well.*",null,0,cdkjwie,1r7w97,askscience,top_week,17
Jetamors,"Marsdreams does a great job of answering the Why Oxygen question. You also asked Why Not Nitrogen: the answer is because most nitrogenases (enzymes that break down N2) are inhibited by oxygen; if there's any oxygen around, they won't work. If you're a bacterium or a plant with roots in the soil, you have areas where you can use those enzymes, but if you're an animal living in the air, there's no way to avoid oxygen, so nitrogenases aren't going to be useful for you.",null,0,cdklcvn,1r7w97,askscience,top_week,6
proule,"Many organisms actually use nitrogen as a terminal electron acceptor (essentially ""breathing"" it in the same way we breath oxygen). The difference is that these organisms are very small and can live on significantly less energy than higher multicellular organisms.

Oxygen is significantly more electronegative than nitrogen (especially considering nitrogen in the atmospheric form N2 is already satisfied with its electron configuration and won't accept more electrons). This difference in electronegativity results in a significant increase in energy liberated from giving oxygen electrons. 

You can think of giving oxygen electrons as a starting point and an end point (your electron donor, and your electron acceptor, which is oxygen). If you simply give oxygen the electron right away then you're losing a bunch of energy to heat. So, between the oxygen and the electron you essentially put up walls for the electron to push (walls being an analogy for cellular work). Oxygen is so powerfully attracting the electron that it pulls it in despite these walls, with the movement of the walls performing work in the cell.

Nitrogen is less electronegative than oxygen, meaning that nitrogen can't pull electrons through as many walls. Giving an electron to nitrogen just doesn't result in as much energy because it's not pulling on the electron as hard.

Higher organisms, mammals especially, have tremendous energy consumption. From a single glucose molecule, a fermenting organism like yeast will produce two molecules of ATP (the energy currency of any cell). The only way for complex multicellular organisms to evolve was to get more energy out of a single molecule of glucose. With oxygen and the electron transport chain we get more like 38 molecules of ATP. If multicellular organisms extracted energy from glucose as inefficiently as fermenting organisms, we'd have to (very simplified) basically consume 19 times more food to get the same energy.

Animals are all nitrogen dependent, but luckily, consuming other organisms yields plenty of nitrogen, because it's the base of amino acids that make up proteins. Trees fix nitrogen in their roots (with the help of symbiotic relationships to bacteria; they don't do this themselves) because they get their energy from the sun, and light obviously can't provide the tree with nitrogen.",null,1,cdkm17n,1r7w97,askscience,top_week,5
Son_of_Thomas,"The different blood types refer to the different types of antigens on the surface of our red blood cells. There are 3 main ones: A, B, and D. A and B determine the type of blood you have (A, B, AB, or O type) and D determines whether your blood is positive or negative- the presence of the D antigen (also known as the Rh factor mentioned in another comment) means you are positive. For example, having the antigens A and D make your blood A+. If you have A and B but no D, your blood is AB-. Having only the D antigen without A or B means you have O+ blood, and a lack of all 3 antigens means you have O- blood. 

The reason this is a problem during blood transfusions is because if you have a certain antigen, it means you have an antibody for the other antigens you dont have. For example, if you have type A+ blood, meaning you have the A and D antigens, you have a B antibody. If your A+ blood were to come into contact with a B type blood, the B antibodies in your blood would react with the B antigens in the B blood, causing the B blood to coagulate and virtually be unusable. 

This is why type O- is the ""universal donor"", because it has no antigens for anybody else's antibodies to react with. This is also why AB+ is the ""universal acceptor"" because it has no antibodies to coagulate any type of donor blood. 

also, I do not know *why* we have different types. 

EDIT: clarification

EDIT 2: gold?! success! thank you, kind stranger. ",null,543,cdkkjeh,1r7v5g,askscience,top_week,2027
Yes_That_Guy,"Have had this saved after answering this question a few times.  Much better summary than I could write up with the time I have this morning.  Edited for content.  Credit to /u/Its_the_bees_knees

**If you have any other specific questions after reading this please let me know.  I feel this should satisfy most if not all of your questions**

**Antigen**- The tag that identifies something. These antigens exist, to help our body distinguish molecules as ""self"" or ""non-self."" Our immune system will recognize these antigens and react, or wont react. When there is an antigen-antibody reaction that is what causes the destruction of these cells and a massive clot forms.

*Edited expansion*- In my analogy. The antigens are the Locks. And the Antibodies are the keys. When your body encounters a Lock that doesnt belong to you, it tries to destroy it (open it) Once your body encounters this lock, it has now seen it and has now seen its design. After encountering a foreign lock, your body will now make keys to open and destroy these locks. Some of these locks are small (ABO blood group) and the keys can be made immediately and the reaction occurs immediately. However the locks of Blood type of +/- (explained more detailed below) are relatively big and can take a while for the appropriate keys to be made. So you constantly have these keys floating around in your blood constantly looking for a compatible lock. When a key matches a lock, that's what activates your immune response, destroys the lock and in turn causes all the negative effects of a transfusion reaction (explained more below)

**Why are there different types of antigens?** This boils down to their molecular structure. All blood types have the same ""core"" structure, but what differentiates the types is a Polysaccharide molecule that sticks out from the molecule. This polysaccharide (or Sugar in simpler terms) is what is the antigen and helps our body identify them as different blood types.


**Antibodies**- The main part of our immune system. Antigen-antibody interactions follow a lock (antigen) and key (antibody) When the key fits the lock that's what causes and immune reaction which ends up destroying the Red Blood Cell (RBC)

The locks are pre-determined due to the sugar ""tag."" Now one thing people dont realize is that certain blood types will have the specific antigen, but WILL HAVE THE OPPOSITE ANTIBODY.


**What does that mean?**

*Blood Type A*: Has A Antigen, and B Antibody

*Blood Type B*: Has B Antigen, and A Antibody

*Blood Type AB*: Has A and B antigens, No antibodies

*Blood Type O*: Doesn't have A and B, Has both A and B antibodies 

*edit*- (as pointed out by nearquincy below, Blood Type O actually contains H antigen, which is the precursor antigen to both antigen A and antigen B.
There are other antigens that is present on the surface of the blood type O itself although they are not significant in ABO system.""






**What does that translate into the real world?**

*Blood Type A*: They only have 1 antibody-Type B, so if that antibody finds its partner antigen (found in Type B or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type B*: They only have 1 antibody-Type A, so if that antibody finds its partner antigen (found in Type A or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type AB*: Has both antigens, but no Antibodies. (Because if it had any antibodies to either A or B, the person wouldn't exist in the first place, because the Antibodies would keep finding their Locks, and self-destructing the RBC's) Because blood type AB has no Antibodies, they are known as **Universal Recipients** Meaning that because there are no keys, there will never be keys to correspond with the locks.

*Blood type O*: Has no antigens, but does have both types of antibodies. Because of the lack of antigens, they are known as **universal donors** meaning their blood can be transfused/mixed with any other blood type and there will not be a reaction.





**Below an even simpler version of what this means**

*Type A blood*- Can Receive type A or Type O Blood. Will react with type B or type AB.

*Type B blood*- Can receive Type B or Type O Blood. Will react with type A or type AB.

*Type AB blood*- Can receive all types of blood- Type A, B, AB, or O. Will not react with any.

*Type O blood*- Can only receive type O blood. But can be donated to any blood type person. Will react with types A, AB, and B (when type O is the type of the receipient)

*edit*- cheat sheet for blood types http://i.imgur.com/fTw8AIj.png




**What is Positive and Negative Blood?**

Well this refers to the presence or the lack of another type of antigen, known as the Rh Factor or also known as the D antigen.



**Why is being positive or negative important?**

Well, this is most important for pregnancies. Specifically a mother's **second pregnancy**. It occurs in a Rh(-) Mother and an Rh(+)Baby. What does being Rh (-) or (+) mean? Well like I said it depends on whether or not you have the Antigen or you don't. During the 1st pregnancy the fetal blood will mix with the mothers blood. Because the mother has no Rh factor, but the fetal blood does- the immune system will recognize these particles as foreign and will in turn start making antibodies to it. Anything your immune system recognizes as foreign, it assumes its an enemy and will start to attack it.



**Why did you mention 2nd pregnancy and not first?**

Well these antibodies belong to a different class than the ones that react with blood typing. These antibodies take MUCH longer to form. Hence it wont affect the first pregnancy, but can affect the second pregnancy.


**Also, can a A- receive A+ blood?**

A+ can receive blood from a A- Negative donor, or A+ donor.

A- can only receive blood from A-.

Its the antigens that make up the difference. Negative means you have no antigen, so even if - donor blood present inside of a + person's body, the body doesn't recognize it as foreign, so its okay.

The problem exists when - person receives + blood. + Blood does contain the antigen, so when - Persons body sees the + blood, it sees the antigen, recognizes it as foreign and attacks it. So this blood transfusion is incompatible


**What happens when you have an incompatible blood type?** 

You get-

Fever

Jaundice

Decreased blood pressure

Increased heart rate

Increased breathing rate

Acute kidney failure (if severe enough)

Blood in urine (after it reaches the kidney failure stage)

Shock ( if severe, and untreated)

Death ( if severe and untreated due to the sudden loss of blood pressure and the shock)

Basically I described it as self destruct, because the host antibodies attack the foreign antigens and cause these red blood cells to be destroyed. When your red blood cells are destroyed they release a molecule know as hemoglobin. Hemoglobin is normally a good molecule which is responsible for oxygen transport, but that is only of its attached to/inside of the Red blood cell. When there is too much free floating hemoglobin in the blood- that is what causes all of the above symptoms.


**Can transfusion reactions be treated? And is it symptomatic treatment or does it require another full transfusion?**
Yeah it's basically sympomatic treatment.

But remember, that symptoms that appear are really variable and dependent on how severe the reaction is. 
But for a complete overall treatment of the above symptoms you would require-

Treating and anticipating the Shock (Which would also treat the low blood pressure and fast heart rate

Treating the kidney failure (Dialysis to filter out the hemoglobin) (If the reaction is severe enough, it may require a complete re-transfusion)

Treat the subsequent clot that will form. (When there is a transfusion reaction there will also be a massive blood clot that forms due to all of the now destroyed Red Blood cells)

**Certain blood types have shown greater incidence with certain diseases/infections. But there is no increased susceptibility**

Blood Type A: Hepatitis, Small Pox

Blood Type O: Black Plague, other digestive system infections, Autoimmune disorders(when your body attacks itself)

Blood Types A, B, AB: Clots in your veins


**Also some other interesting things to note is that-**

ABO (+/-) Blood typing will fit and categorize about 99.97% of the worlds population. But there is still that 0.03% who's blood has neither A/B Antigens nor the A/B Antibodies.

For these special people we have to to EXTREMELY careful when transfusing any type of blood, also we have to use other systems to type/categorize their blood

When a Blood Type O patient, receives blood from a blood type A patient. This causes the most severe of the transfusion reactions.

About 40% of the population is Type O. Type AB is the rarest.

About 80% of the population is +. So being AB- is the rarest of blood types


**Are there any benefits to having either A, B, AB, or O blood types?**

Type O can donate to all types of blood.

Type AB can receive all types of blood.

Types- A, AB, B dont have a distinct advantage over the other. *edit*- as pointed out by /u/hammurarbisan "" both A and B types routinely have elevated platelet counts versus the other blood types - most especially female A types - and thus more frequently tested for platelet counts and targeted for apheresis collection. Platelets are commonly used for transfusion during surgery.""

Being + however is an advantage over being negative. + Can receive from + or - people. Negative people however can only receive from Negative people.

*edit*- Type AB can donate PLASMA to all people, since they dont have any antibodies (the donation rules for plasma and whole blood are opposite in terms of antigens and antibodies)
",null,23,cdkmrvv,1r7v5g,askscience,top_week,143
swamp14,"A lot of people have been asking about the ""why"" and rightly so because it was asked in the OP but not yet answered. 

WHY humans have different blood types is akin to why some humans have more or less of a specific protein that helps break down alcohol, why some humans tend to be taller or shorter than others, why some humans have different proportions in arm/leg length... etc. It's simply genetic variation. Different genes being expressed because of genetic diversity in the human species, amplified by sexual reproduction.

To all the people asking what evolutionary advantage different blood types offer - just because a species has a particular trait doesn't mean that trait helps the species survive and reproduce more than if the species lacked that trait. Traits can be passed on simply because of luck or a number of other factors. Evolution works because traits that tend to be favorable for survival and reproduction tend to get passed on and thus become more prevalent in the species. But that doesn't mean disadvantageous traits don't get passed on. They're just less likely.

""Isn't it more evolutionarily advantageous to have a single blood type?""
Perhaps. I can't give a straight answer because I don't know. But this question indicates a misunderstanding of evolution. It's like asking ""Isn't it more more evolutionarily advantageous for humans to be stronger, faster, and smarter?"" So, even if the answer is yes, it doesn't mean it should be true. 

What this tells us is that evolution is not perfect. It is the process that results from random mutations and natural selection over long periods of time. We have different types of blood because of random mutations and those blood types got passed on. Whether or not those different blood types contributed to our survival and reproduction, I cannot tell you.

But what I can tell you is: 1) traits are not perfect, and asking why they are not ""better"" usually demonstrates a misunderstanding of evolution; and 2) traits do not necessarily need to contribute to survival and reproduction in order to be passed on, and assuming that they MUST be is not reasonable. Yes, traits usually are advantageous because if they weren't advantageous, they'd be less likely to be passed on. But again, this does not mean they absolutely need to be.

Edit - Thanks so much for the gold~

",null,14,cdkkr1g,1r7v5g,askscience,top_week,83
cracked_chemist,"In terms of evolution:

Reading a bit the article ""ABO blood group glycans modulate sialic acid recognition on erythrocytes"" (Cohen, M, et al. Blood, 2009), they suggest that the different blood type ""antigens found on human erythrocytes modulate the specific interactions of 3 sialic acid-recognizing proteins...  with sialylated glycans on the same cell surface."" These antigens ""modulate sialic acid-mediated interaction of pathogens such as Plasmodium falciparum malarial parasite."" Thus the different blood types may affect the host-pathogen interaction. To be fair hematology is not my field, but their model seems plausible. ",null,18,cdkmgly,1r7v5g,askscience,top_week,79
Andrenator,"And [an interesting thing](http://andrenator.tumblr.com/post/66202105636/detenebrate-0xymoronic-shitarianasays) about the B antigen (and a little of the A antigen),

is that during the black plague, the bacteria mimicked the B antigen, so if people had B type blood, they would have virtually no defense against it.  A type was a little better, people could recover from it.  Type O blood developed in small villages where accidental marrying of distant cousins happened, and O blood reacted violently to the plague.

So that's why B blood is so rare now, A is a little more common, and O type blood is so common.",null,5,cdknftz,1r7v5g,askscience,top_week,30
mobilehypo,"Interestingly, some blood types do give resistance to disease. The lack of the Duffy antigen in the majority of Black Africans has been shown to give resistance to two species of malarial parasites. This blood group system has been studied closely and we are finding that specific combinations of this antigen on the surface of red blood cells might have impacts on other diseases. [This part of the Duffy article](http://en.wikipedia.org/wiki/Duffy_antigen_system#Clinical_significance) on Wikipedia gives a rundown of what we have found so far.

This question has come up before, here's a set of search results that might help further:

* http://www.reddit.com/r/askscience/search?q=blood+type&amp;restrict_sr=on&amp;sort=top&amp;t=all

Here are some posts that address some whys:

* http://www.reddit.com/r/askscience/comments/iepiv/why_did_we_evolve_with_different_blood_types/

* http://www.reddit.com/r/askscience/comments/f19p4/how_or_why_did_blood_types_evolve_is_there_any/

",null,1,cdkjtyc,1r7v5g,askscience,top_week,27
Cabin_Sandwich,"And what connection, if any, does this have to ""eating for your blood type""?  I have a hippy friend who is always telling me I need to eat according to my blood type, how the blood types arouse from different groups of people who were eating different things and therefore I should eat what my ancestors ate.  I think it's nonsense.",null,7,cdkkrn4,1r7v5g,askscience,top_week,16
Chl0eeeeeee,"Differences in blood types arise from different protein expression on the red blood cell. So, first with the Rh factor (named so because it was first discovered in Rhesus monkeys). You can be Rh + or -, which means that your red blood cells have or don't have this certain protein on the surface. Problems can arise when a mother is Rh - and is carrying a child who is Rh+, as an immune reaction can occur (because the mothers body sees the Rh+ proteins as being foreign) if there is blood exchange between the two.  Usually the first child is fine, but when there's blood exchange in child birth, the mother builds up anti-Rh antibodies. To help with this, all Rh- mothers are treated with Rhogam to suppress that response. 

Now, on to the A/B/O blood type. Again, this is code for the type of proteins that are on the surface of the blood cell. The differences arise from genetic inheritance with one gene determining ABO inheritance. So, without making it too complicated... The presence of at least one ""A allele"" will cause the body to make specific glycoproteins, while the absence causes anti-A antibodies. The same goes for the B allele. If a person has one A and one B, then they are type AB. Because they are AB, they don't produce either antibodies. If they have two As, they are type A and likewise for B. In this gene, both the A and B genes are co-dominant. Type O arises when the person has neither an A allele nor a B allele, and instead has two recessive alleles. This person would not have any glycoproteins on the cell, and would produce anti-A and anti-B antibodies. This is why they are the universal donor (transfusion of blood won't set off an immune reaction, especially if they are O-). AB blood types are considered the universal receiver because they don't have the antibodies for any of the blood types, and thus can receive any blood. 

Hope this helps!",null,4,cdklo01,1r7v5g,askscience,top_week,9
Fazaman,"There are several great explainations in this thread as to what the different blood types are and how they interact with each other, but the question is *why* do we have these different types (just evolutionary/genetic differences?), are there any effects to having different types of blood (Say, A is more prone to getting disease X, or some such).",null,1,cdkq47v,1r7v5g,askscience,top_week,9
billyvnilly,"A,B,AB,O are designations for cells that express antigens.  D is also an antigen, and its significance is that it's reactive with antibodies as much as A and B.

We have evolved to develop these blood groups.  When Red Blood Cells (RBCs) are made they express a carbohydrate chain (millions) which is called H.  We have evolved over time to have different enzymes that modify this carbohydrate chain.  Most people believe B was the original enzyme.  A is speculated to happen afterwards.  So enzyme A or B act on the carbohydrate chain H, and modify it.  The modified chain is thus termed A or B.  If neither enzyme acted on the H chain, its termed O.  The enzyme A and B behave totally differently (enzyme activity and thus amount of H chains they convert), so if you have enzymes A and B, its not an equal split (thats a minor point, doesn't matter for this).  So these are just two examples of antigens on RBCs.  D is a third antigen that is clinically significant and very common, that is why people know about it.  What they don't know is D is a combo of many antigens.  D,C,c,E, and e.  but we always say if you're D+ then your blood is positive.  there is no d antigen.  
What most people don't know is that there are tons of antigens (100s) on red blood cells.  Some are there from the development of the RBCs' nucleus, and some are adsorbed from secreted antigens circulating in the blood stream.  
Why are all these important?  Well as your body makes antigens, it is also recognizing those antigens as ""self"" ... our bodies have the ability to then recognize ""not-self"".  So if you only have enzyme A, and not make any RBCs with B on their surface, you make antibodies to B, but not to A.  People with type A blood cant receive type B or AB blood because they have antibodies to B.  
Why do we have different blood types?  Well a good example is a blood group called Duffy.  The blood group has two antigens: Fya and Fyb.  And again these antigens come from two enzymes.  If you lack both enzymes, you lack the antigens.  And if you lack both antigens you're termed Fy(a-b-).  Why is this evolutionary?  Well because Plasmodium vivax (malaria) uses the antigen site to enter cells.  So if you lack the antigen, malaria cannot enter the cell (evolutionary!  This is why the majority of African-Americans are Fy(a-b-).",null,2,cdkm1ad,1r7v5g,askscience,top_week,8
kroxldyphivian,"Everyone is answering *what* the different blood types are. And while that's really informative and helpful, the original question in the post title (and the much more interesting topic imo) is *why* we have different blood types. Can anyone shed some light on this?",null,0,cdkobtd,1r7v5g,askscience,top_week,5
arumbar,"Blood antigen variation is thought to be a response to pathogens like viruses spreading from person to person and carrying some component of membrane protein with them, such that individuals who created an antibody response to foreign ABO markers had more success fighting off infections.  [wiki explanation here](http://en.wikipedia.org/wiki/ABO_blood_group_system#Origin_theories), [journal article here](http://www.ncbi.nlm.nih.gov/pubmed/15293861)",null,0,cdl3nvf,1r7v5g,askscience,top_week,7
Mumma_Sooz,"I've been lurking for a while but this has had me step out of the shadows.. I run a blood donor centre in Western Australia.  I encourage and applaud anyone who donates blood, be it whole blood, plasma or platelets.  People do it for so many reasons and in oz they get nothing tangible in return.  1 in 3 people will need a blood transfusion but only 1 in 30 donates.  If you have ever considered it but are hesitant, please don't be.  I can tell you from experience that it's nothing to worry about and the feeling of having contributed to saving lives is amazing.  ",null,1,cdl40ig,1r7v5g,askscience,top_week,4
pleasantliving,"I am in the Rare Donor Program and I never really understood what it means. All I know is I get a letter every few months making sure my address hasn't changed. On my donor card, my blood type says things like K:-1 and Jk(b-). Can anyone explain this to me? Why do some people need this rare blood? Does this mean I need rare blood? ",null,1,cdkzjdh,1r7v5g,askscience,top_week,5
OldMarmalade,"http://www.dnalc.org/view/15404-Chromosome-9-gene-for-blood-group-Matt-Ridley.html

AB - Very resistant to cholera

O - Susceptible to cholera

Among other things this may be one of the evolutionary pressures that explains variation in bloodtype. If you want to create the maximum number of ABs you'd have a population that was ~41% A and ~41% B to randomly generate ~18% AB supermen (in the time of cholera). This is indeed what you see in many populations http://www.blood.co.uk/images/content/pie-charts.jpg ",null,0,cdklqsz,1r7v5g,askscience,top_week,3
WasIsMitDenKohlen,"I haven't seen an answer to the first two questions. WHY are there different blood types. Why was there a need to have those distinct blood types. 

Would it make more sense evolutionary to have one blood type only, why bother preserving those distinct mutations, and everything that goes along with maintaining those? Or if not, why are there 3 distinct ones, and not 1000? What is the driving force to create 3 blood types? ",null,2,cdkpw6a,1r7v5g,askscience,top_week,4
redplate12,A clarification of the discussion of 'antigens' that represent the various groups. An O type does have an antigen structure but nobody has antibodies to it. The O antigen is a chain of 4 six carbon sugars. The A antigen has an added six carbon sugar (N-acetyl-galactosamine bound to second sugar in a 'branch) while the B antigen has an additional six carbon sugar (galactose) that is found bound to the fourth six carbon sugar core. An AB has both the A and B  five sugar chains present. Thus the difference is the presence of a single sugar moiety added to the core 4 sugar chain.,null,0,cdkswmc,1r7v5g,askscience,top_week,2
slakist,"I have no knowledge on the subject but have always been interested so this thread was great. I see some people have answered as to why we have different blood types, as well as explaining the difference between them. 

I would like to know if there is any disadvantage to having a specific blood type. For instance, I am O-; am I more susceptible to different kinds of diseases and disorders based on my blood type alone, more than a person with say, blood A? ",null,3,cdktzqz,1r7v5g,askscience,top_week,4
beliefinphilosophy,"Bonus factoid:

The different blood group antigens (or surface markers) are
Different kinds of sugars, (glucos, carbohydrate) and the +/- is a protein.





For example, the antigens of the ABO blood group are sugars. They are produced by a series of reactions in which enzymes catalyze the transfer of sugar units. A person's DNA determines the type of enzymes they have, and, therefore, the type of sugar antigens that end up on their red blood cells.
In contrast, the antigens of the Rh blood group are proteins. A person's DNA holds the information for producing the protein antigens. The RhD gene encodes the D antigen, which is a large protein on the red blood cell membrane. Some people have a version of the gene that does not produce D antigen, and therefore the RhD protein is absent from their red blood cells.

More [here](http://www.ncbi.nlm.nih.gov/books/NBK2264/)",null,0,cdkubis,1r7v5g,askscience,top_week,2
12and32,"I don't think the question of ""why we have different surface antigens"" has been answered. 

So why do we have differing surface and blood borne antigens? Are the genes that code for them related in any way, say by a few SNPs, or even just a single point mutation? I would think that they're related if we can predict heredity through simple Mendelian genetics, and because they do seem to have a high degree of compatibility (hemolytic newborn disease aside). There doesn't seem to be any particular reason for the Rh factor, nor does there seem to be a reason for antibodies that can only attack the A and B antigens.",null,0,cdkw1vb,1r7v5g,askscience,top_week,2
Jrj84105,"The different blood groups reflect differences in antigens present on the red blood cell surface. Because some of these antigenic variants confer resistance to infections by organisms such as malaria, certain blood groups are more common in different regions of the world. For the most part though, prior to the advent of blood transfusions, these differences were largely of no impact, essentially benign polymorphisms.

These kind of benign variations aren't just limited to our red cells but are present throughout our bodies. As we continue to challenge our bodies with new medical therapies, there turn out to be lots of previously irrelevant benign differences between people that now are seen as increasingly important factors in response to therapy and risk of side effects. ""Personalized medicine"" is largely an attempt to tailor therapy not just to a person's disease but also to the way in which any individual's response to therapies is a product of their unique set of variations.",null,0,cdl12gq,1r7v5g,askscience,top_week,2
MrGrow,"Our red cells consist of many different antigens (something that can trigger an immune response). Think of them like land mines. What people have can vary a ton (Kidd antigens, Duffy, Rh, Lewis, and so on). You simply make what you don't have. If I am A+ (very common) I am going to make the antibody to B. If you were to give me B blood, because I don't have that antigen on my own cells, my body will recognize that as foreign. What will more than likely happen is antibodies will coat the B blood cells, and when they are taken to the spleen, they will be ripped up and destroyed. This is called a transfusion reaction. There are different kinds of these and they each have there own symptoms. Why do humans have different types? Good question.     ",null,0,cdl68l5,1r7v5g,askscience,top_week,2
bigdaddymatty,"Whenever I donated blood the first time I became really interested in blood types and wondered this same question. I gave blood and didn't know my blood type before, but found out after that it was O- by looking on the donation companies website and logging in to see my donation session.   
I expect to receive a phone call each quarter now and every time I donate blood that ask me to do double red blood cells since I'm the universal donor! Giving blood is definitely worth it though",null,0,cdl6fqd,1r7v5g,askscience,top_week,2
laika84,"I checked comments and did not see this brought up as far as what the blood types ""actually mean.""  People have mentioned antigens and that these antigens are glycoproteins, which is correct.  However, I think it's also interesting/important to go a little further into how these different antigens are made and their significance, in terms of the ABO portion.

ABO bloods types are distinguished by the individual alleles that encode an enzyme.  The different enzymes have long names and essentially add different sugars via O-linked glycosylation to what could be a ""base"" sugar chain that is added first.  This ""base"" sugar chain exists on our RBCs and receives a fucose residue by fucosyl-transferase and having this enzyme alone constitutes the ""O"" phenotype/antigen.  The ""A"" and ""B"" enyzmes each add a different sugar to this base carb chain with the fucose and that creates the different antigens that we see.  You can think of it as the ""A"" enzyme adding the ""A"" sugar/antigen (N-acetylgalactosamine) and the ""B"" enzyme adding the ""B"" sugar/antigen (galactose).  It may seem trivial, but I thought it was interesting when I learned it and enhanced my understanding of what these antigens ""looked like"" on a biochemical level.

For a really cool tangent, do a search for the ""Bombay"" phenotype!",null,0,cdl02x8,1r7v5g,askscience,top_week,2
Shekho,"I don't remember where I heard this but i'll try to be nice, and simple....

Lets say a disease came along and wiped out all people with (x) blood type because they don't have a certain antibody that protects them from disease then other people with (y) blood type would live because they naturally can protect themselves..

If we all had the same blood type, wouldn't that mean we could easily be wiped out as a race from diseases? 

Please correct me if I'm wrong.",null,1,cdl33v2,1r7v5g,askscience,top_week,2
quiktom,"I was told that the blood types evolved over time. The O being the original and oldest when we ate anything we could find. The A type evolved when we got into agriculture and the B type when we started living in concentrated cities. Apparently AB is only 600 or so years old. AB blood can accept any blood before it (since those tpes were around when it evolved and essentially are a part of it) but no other type can accept AB, whereas at the opposite end O can be accepted by any but can accept none. All the carbons, sugars and whatnot are the details of how the evolution took place.

Apparently this affects what we should eat according to our blood types but I'm an O type and as much as I'd like to eat steak and nuts and raw veg it'd get really expensive without bread or pasta to fill me up.",null,3,cdl4ckm,1r7v5g,askscience,top_week,4
SparklePonyBoy,"I'm surprised that no one mentioned CMV, cytomegalovirus.  When blood is tested, spun, etc., they also test for this because it can kill immunocompromised individuals, also babies.  I know this because I used to donate blood every 8 weeks and they told me I am O- CMV- and that each donation can save up to 5 babies' lives.  Supposedly up to half of the general healthy population may be walking around with CMV and not even know.

http://www.cdc.gov/cmv/overview.html",null,0,cdl4r5d,1r7v5g,askscience,top_week,1
A_Soggy_Sheep,"This is real pseudoscience, but i would hazard a guess that it is beneficial for the human race to have different blood types for survival, incase some kind of disease was incredibly deadly to a specific blood type...

A bit like [diversify your bonds](http://www.youtube.com/watch?v=FTsNEUZx8v8) .

As i said earlier i may be totally wrong, so feel free to patronise me into oblivion. ",null,1,cdkkwwv,1r7v5g,askscience,top_week,2
Son_of_Thomas,"The different blood types refer to the different types of antigens on the surface of our red blood cells. There are 3 main ones: A, B, and D. A and B determine the type of blood you have (A, B, AB, or O type) and D determines whether your blood is positive or negative- the presence of the D antigen (also known as the Rh factor mentioned in another comment) means you are positive. For example, having the antigens A and D make your blood A+. If you have A and B but no D, your blood is AB-. Having only the D antigen without A or B means you have O+ blood, and a lack of all 3 antigens means you have O- blood. 

The reason this is a problem during blood transfusions is because if you have a certain antigen, it means you have an antibody for the other antigens you dont have. For example, if you have type A+ blood, meaning you have the A and D antigens, you have a B antibody. If your A+ blood were to come into contact with a B type blood, the B antibodies in your blood would react with the B antigens in the B blood, causing the B blood to coagulate and virtually be unusable. 

This is why type O- is the ""universal donor"", because it has no antigens for anybody else's antibodies to react with. This is also why AB+ is the ""universal acceptor"" because it has no antibodies to coagulate any type of donor blood. 

also, I do not know *why* we have different types. 

EDIT: clarification

EDIT 2: gold?! success! thank you, kind stranger. ",null,543,cdkkjeh,1r7v5g,askscience,top_week,2027
Yes_That_Guy,"Have had this saved after answering this question a few times.  Much better summary than I could write up with the time I have this morning.  Edited for content.  Credit to /u/Its_the_bees_knees

**If you have any other specific questions after reading this please let me know.  I feel this should satisfy most if not all of your questions**

**Antigen**- The tag that identifies something. These antigens exist, to help our body distinguish molecules as ""self"" or ""non-self."" Our immune system will recognize these antigens and react, or wont react. When there is an antigen-antibody reaction that is what causes the destruction of these cells and a massive clot forms.

*Edited expansion*- In my analogy. The antigens are the Locks. And the Antibodies are the keys. When your body encounters a Lock that doesnt belong to you, it tries to destroy it (open it) Once your body encounters this lock, it has now seen it and has now seen its design. After encountering a foreign lock, your body will now make keys to open and destroy these locks. Some of these locks are small (ABO blood group) and the keys can be made immediately and the reaction occurs immediately. However the locks of Blood type of +/- (explained more detailed below) are relatively big and can take a while for the appropriate keys to be made. So you constantly have these keys floating around in your blood constantly looking for a compatible lock. When a key matches a lock, that's what activates your immune response, destroys the lock and in turn causes all the negative effects of a transfusion reaction (explained more below)

**Why are there different types of antigens?** This boils down to their molecular structure. All blood types have the same ""core"" structure, but what differentiates the types is a Polysaccharide molecule that sticks out from the molecule. This polysaccharide (or Sugar in simpler terms) is what is the antigen and helps our body identify them as different blood types.


**Antibodies**- The main part of our immune system. Antigen-antibody interactions follow a lock (antigen) and key (antibody) When the key fits the lock that's what causes and immune reaction which ends up destroying the Red Blood Cell (RBC)

The locks are pre-determined due to the sugar ""tag."" Now one thing people dont realize is that certain blood types will have the specific antigen, but WILL HAVE THE OPPOSITE ANTIBODY.


**What does that mean?**

*Blood Type A*: Has A Antigen, and B Antibody

*Blood Type B*: Has B Antigen, and A Antibody

*Blood Type AB*: Has A and B antigens, No antibodies

*Blood Type O*: Doesn't have A and B, Has both A and B antibodies 

*edit*- (as pointed out by nearquincy below, Blood Type O actually contains H antigen, which is the precursor antigen to both antigen A and antigen B.
There are other antigens that is present on the surface of the blood type O itself although they are not significant in ABO system.""






**What does that translate into the real world?**

*Blood Type A*: They only have 1 antibody-Type B, so if that antibody finds its partner antigen (found in Type B or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type B*: They only have 1 antibody-Type A, so if that antibody finds its partner antigen (found in Type A or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type AB*: Has both antigens, but no Antibodies. (Because if it had any antibodies to either A or B, the person wouldn't exist in the first place, because the Antibodies would keep finding their Locks, and self-destructing the RBC's) Because blood type AB has no Antibodies, they are known as **Universal Recipients** Meaning that because there are no keys, there will never be keys to correspond with the locks.

*Blood type O*: Has no antigens, but does have both types of antibodies. Because of the lack of antigens, they are known as **universal donors** meaning their blood can be transfused/mixed with any other blood type and there will not be a reaction.





**Below an even simpler version of what this means**

*Type A blood*- Can Receive type A or Type O Blood. Will react with type B or type AB.

*Type B blood*- Can receive Type B or Type O Blood. Will react with type A or type AB.

*Type AB blood*- Can receive all types of blood- Type A, B, AB, or O. Will not react with any.

*Type O blood*- Can only receive type O blood. But can be donated to any blood type person. Will react with types A, AB, and B (when type O is the type of the receipient)

*edit*- cheat sheet for blood types http://i.imgur.com/fTw8AIj.png




**What is Positive and Negative Blood?**

Well this refers to the presence or the lack of another type of antigen, known as the Rh Factor or also known as the D antigen.



**Why is being positive or negative important?**

Well, this is most important for pregnancies. Specifically a mother's **second pregnancy**. It occurs in a Rh(-) Mother and an Rh(+)Baby. What does being Rh (-) or (+) mean? Well like I said it depends on whether or not you have the Antigen or you don't. During the 1st pregnancy the fetal blood will mix with the mothers blood. Because the mother has no Rh factor, but the fetal blood does- the immune system will recognize these particles as foreign and will in turn start making antibodies to it. Anything your immune system recognizes as foreign, it assumes its an enemy and will start to attack it.



**Why did you mention 2nd pregnancy and not first?**

Well these antibodies belong to a different class than the ones that react with blood typing. These antibodies take MUCH longer to form. Hence it wont affect the first pregnancy, but can affect the second pregnancy.


**Also, can a A- receive A+ blood?**

A+ can receive blood from a A- Negative donor, or A+ donor.

A- can only receive blood from A-.

Its the antigens that make up the difference. Negative means you have no antigen, so even if - donor blood present inside of a + person's body, the body doesn't recognize it as foreign, so its okay.

The problem exists when - person receives + blood. + Blood does contain the antigen, so when - Persons body sees the + blood, it sees the antigen, recognizes it as foreign and attacks it. So this blood transfusion is incompatible


**What happens when you have an incompatible blood type?** 

You get-

Fever

Jaundice

Decreased blood pressure

Increased heart rate

Increased breathing rate

Acute kidney failure (if severe enough)

Blood in urine (after it reaches the kidney failure stage)

Shock ( if severe, and untreated)

Death ( if severe and untreated due to the sudden loss of blood pressure and the shock)

Basically I described it as self destruct, because the host antibodies attack the foreign antigens and cause these red blood cells to be destroyed. When your red blood cells are destroyed they release a molecule know as hemoglobin. Hemoglobin is normally a good molecule which is responsible for oxygen transport, but that is only of its attached to/inside of the Red blood cell. When there is too much free floating hemoglobin in the blood- that is what causes all of the above symptoms.


**Can transfusion reactions be treated? And is it symptomatic treatment or does it require another full transfusion?**
Yeah it's basically sympomatic treatment.

But remember, that symptoms that appear are really variable and dependent on how severe the reaction is. 
But for a complete overall treatment of the above symptoms you would require-

Treating and anticipating the Shock (Which would also treat the low blood pressure and fast heart rate

Treating the kidney failure (Dialysis to filter out the hemoglobin) (If the reaction is severe enough, it may require a complete re-transfusion)

Treat the subsequent clot that will form. (When there is a transfusion reaction there will also be a massive blood clot that forms due to all of the now destroyed Red Blood cells)

**Certain blood types have shown greater incidence with certain diseases/infections. But there is no increased susceptibility**

Blood Type A: Hepatitis, Small Pox

Blood Type O: Black Plague, other digestive system infections, Autoimmune disorders(when your body attacks itself)

Blood Types A, B, AB: Clots in your veins


**Also some other interesting things to note is that-**

ABO (+/-) Blood typing will fit and categorize about 99.97% of the worlds population. But there is still that 0.03% who's blood has neither A/B Antigens nor the A/B Antibodies.

For these special people we have to to EXTREMELY careful when transfusing any type of blood, also we have to use other systems to type/categorize their blood

When a Blood Type O patient, receives blood from a blood type A patient. This causes the most severe of the transfusion reactions.

About 40% of the population is Type O. Type AB is the rarest.

About 80% of the population is +. So being AB- is the rarest of blood types


**Are there any benefits to having either A, B, AB, or O blood types?**

Type O can donate to all types of blood.

Type AB can receive all types of blood.

Types- A, AB, B dont have a distinct advantage over the other. *edit*- as pointed out by /u/hammurarbisan "" both A and B types routinely have elevated platelet counts versus the other blood types - most especially female A types - and thus more frequently tested for platelet counts and targeted for apheresis collection. Platelets are commonly used for transfusion during surgery.""

Being + however is an advantage over being negative. + Can receive from + or - people. Negative people however can only receive from Negative people.

*edit*- Type AB can donate PLASMA to all people, since they dont have any antibodies (the donation rules for plasma and whole blood are opposite in terms of antigens and antibodies)
",null,23,cdkmrvv,1r7v5g,askscience,top_week,143
swamp14,"A lot of people have been asking about the ""why"" and rightly so because it was asked in the OP but not yet answered. 

WHY humans have different blood types is akin to why some humans have more or less of a specific protein that helps break down alcohol, why some humans tend to be taller or shorter than others, why some humans have different proportions in arm/leg length... etc. It's simply genetic variation. Different genes being expressed because of genetic diversity in the human species, amplified by sexual reproduction.

To all the people asking what evolutionary advantage different blood types offer - just because a species has a particular trait doesn't mean that trait helps the species survive and reproduce more than if the species lacked that trait. Traits can be passed on simply because of luck or a number of other factors. Evolution works because traits that tend to be favorable for survival and reproduction tend to get passed on and thus become more prevalent in the species. But that doesn't mean disadvantageous traits don't get passed on. They're just less likely.

""Isn't it more evolutionarily advantageous to have a single blood type?""
Perhaps. I can't give a straight answer because I don't know. But this question indicates a misunderstanding of evolution. It's like asking ""Isn't it more more evolutionarily advantageous for humans to be stronger, faster, and smarter?"" So, even if the answer is yes, it doesn't mean it should be true. 

What this tells us is that evolution is not perfect. It is the process that results from random mutations and natural selection over long periods of time. We have different types of blood because of random mutations and those blood types got passed on. Whether or not those different blood types contributed to our survival and reproduction, I cannot tell you.

But what I can tell you is: 1) traits are not perfect, and asking why they are not ""better"" usually demonstrates a misunderstanding of evolution; and 2) traits do not necessarily need to contribute to survival and reproduction in order to be passed on, and assuming that they MUST be is not reasonable. Yes, traits usually are advantageous because if they weren't advantageous, they'd be less likely to be passed on. But again, this does not mean they absolutely need to be.

Edit - Thanks so much for the gold~

",null,14,cdkkr1g,1r7v5g,askscience,top_week,83
cracked_chemist,"In terms of evolution:

Reading a bit the article ""ABO blood group glycans modulate sialic acid recognition on erythrocytes"" (Cohen, M, et al. Blood, 2009), they suggest that the different blood type ""antigens found on human erythrocytes modulate the specific interactions of 3 sialic acid-recognizing proteins...  with sialylated glycans on the same cell surface."" These antigens ""modulate sialic acid-mediated interaction of pathogens such as Plasmodium falciparum malarial parasite."" Thus the different blood types may affect the host-pathogen interaction. To be fair hematology is not my field, but their model seems plausible. ",null,18,cdkmgly,1r7v5g,askscience,top_week,79
Andrenator,"And [an interesting thing](http://andrenator.tumblr.com/post/66202105636/detenebrate-0xymoronic-shitarianasays) about the B antigen (and a little of the A antigen),

is that during the black plague, the bacteria mimicked the B antigen, so if people had B type blood, they would have virtually no defense against it.  A type was a little better, people could recover from it.  Type O blood developed in small villages where accidental marrying of distant cousins happened, and O blood reacted violently to the plague.

So that's why B blood is so rare now, A is a little more common, and O type blood is so common.",null,5,cdknftz,1r7v5g,askscience,top_week,30
mobilehypo,"Interestingly, some blood types do give resistance to disease. The lack of the Duffy antigen in the majority of Black Africans has been shown to give resistance to two species of malarial parasites. This blood group system has been studied closely and we are finding that specific combinations of this antigen on the surface of red blood cells might have impacts on other diseases. [This part of the Duffy article](http://en.wikipedia.org/wiki/Duffy_antigen_system#Clinical_significance) on Wikipedia gives a rundown of what we have found so far.

This question has come up before, here's a set of search results that might help further:

* http://www.reddit.com/r/askscience/search?q=blood+type&amp;restrict_sr=on&amp;sort=top&amp;t=all

Here are some posts that address some whys:

* http://www.reddit.com/r/askscience/comments/iepiv/why_did_we_evolve_with_different_blood_types/

* http://www.reddit.com/r/askscience/comments/f19p4/how_or_why_did_blood_types_evolve_is_there_any/

",null,1,cdkjtyc,1r7v5g,askscience,top_week,27
Cabin_Sandwich,"And what connection, if any, does this have to ""eating for your blood type""?  I have a hippy friend who is always telling me I need to eat according to my blood type, how the blood types arouse from different groups of people who were eating different things and therefore I should eat what my ancestors ate.  I think it's nonsense.",null,7,cdkkrn4,1r7v5g,askscience,top_week,16
Chl0eeeeeee,"Differences in blood types arise from different protein expression on the red blood cell. So, first with the Rh factor (named so because it was first discovered in Rhesus monkeys). You can be Rh + or -, which means that your red blood cells have or don't have this certain protein on the surface. Problems can arise when a mother is Rh - and is carrying a child who is Rh+, as an immune reaction can occur (because the mothers body sees the Rh+ proteins as being foreign) if there is blood exchange between the two.  Usually the first child is fine, but when there's blood exchange in child birth, the mother builds up anti-Rh antibodies. To help with this, all Rh- mothers are treated with Rhogam to suppress that response. 

Now, on to the A/B/O blood type. Again, this is code for the type of proteins that are on the surface of the blood cell. The differences arise from genetic inheritance with one gene determining ABO inheritance. So, without making it too complicated... The presence of at least one ""A allele"" will cause the body to make specific glycoproteins, while the absence causes anti-A antibodies. The same goes for the B allele. If a person has one A and one B, then they are type AB. Because they are AB, they don't produce either antibodies. If they have two As, they are type A and likewise for B. In this gene, both the A and B genes are co-dominant. Type O arises when the person has neither an A allele nor a B allele, and instead has two recessive alleles. This person would not have any glycoproteins on the cell, and would produce anti-A and anti-B antibodies. This is why they are the universal donor (transfusion of blood won't set off an immune reaction, especially if they are O-). AB blood types are considered the universal receiver because they don't have the antibodies for any of the blood types, and thus can receive any blood. 

Hope this helps!",null,4,cdklo01,1r7v5g,askscience,top_week,9
Fazaman,"There are several great explainations in this thread as to what the different blood types are and how they interact with each other, but the question is *why* do we have these different types (just evolutionary/genetic differences?), are there any effects to having different types of blood (Say, A is more prone to getting disease X, or some such).",null,1,cdkq47v,1r7v5g,askscience,top_week,9
billyvnilly,"A,B,AB,O are designations for cells that express antigens.  D is also an antigen, and its significance is that it's reactive with antibodies as much as A and B.

We have evolved to develop these blood groups.  When Red Blood Cells (RBCs) are made they express a carbohydrate chain (millions) which is called H.  We have evolved over time to have different enzymes that modify this carbohydrate chain.  Most people believe B was the original enzyme.  A is speculated to happen afterwards.  So enzyme A or B act on the carbohydrate chain H, and modify it.  The modified chain is thus termed A or B.  If neither enzyme acted on the H chain, its termed O.  The enzyme A and B behave totally differently (enzyme activity and thus amount of H chains they convert), so if you have enzymes A and B, its not an equal split (thats a minor point, doesn't matter for this).  So these are just two examples of antigens on RBCs.  D is a third antigen that is clinically significant and very common, that is why people know about it.  What they don't know is D is a combo of many antigens.  D,C,c,E, and e.  but we always say if you're D+ then your blood is positive.  there is no d antigen.  
What most people don't know is that there are tons of antigens (100s) on red blood cells.  Some are there from the development of the RBCs' nucleus, and some are adsorbed from secreted antigens circulating in the blood stream.  
Why are all these important?  Well as your body makes antigens, it is also recognizing those antigens as ""self"" ... our bodies have the ability to then recognize ""not-self"".  So if you only have enzyme A, and not make any RBCs with B on their surface, you make antibodies to B, but not to A.  People with type A blood cant receive type B or AB blood because they have antibodies to B.  
Why do we have different blood types?  Well a good example is a blood group called Duffy.  The blood group has two antigens: Fya and Fyb.  And again these antigens come from two enzymes.  If you lack both enzymes, you lack the antigens.  And if you lack both antigens you're termed Fy(a-b-).  Why is this evolutionary?  Well because Plasmodium vivax (malaria) uses the antigen site to enter cells.  So if you lack the antigen, malaria cannot enter the cell (evolutionary!  This is why the majority of African-Americans are Fy(a-b-).",null,2,cdkm1ad,1r7v5g,askscience,top_week,8
kroxldyphivian,"Everyone is answering *what* the different blood types are. And while that's really informative and helpful, the original question in the post title (and the much more interesting topic imo) is *why* we have different blood types. Can anyone shed some light on this?",null,0,cdkobtd,1r7v5g,askscience,top_week,5
arumbar,"Blood antigen variation is thought to be a response to pathogens like viruses spreading from person to person and carrying some component of membrane protein with them, such that individuals who created an antibody response to foreign ABO markers had more success fighting off infections.  [wiki explanation here](http://en.wikipedia.org/wiki/ABO_blood_group_system#Origin_theories), [journal article here](http://www.ncbi.nlm.nih.gov/pubmed/15293861)",null,0,cdl3nvf,1r7v5g,askscience,top_week,7
Mumma_Sooz,"I've been lurking for a while but this has had me step out of the shadows.. I run a blood donor centre in Western Australia.  I encourage and applaud anyone who donates blood, be it whole blood, plasma or platelets.  People do it for so many reasons and in oz they get nothing tangible in return.  1 in 3 people will need a blood transfusion but only 1 in 30 donates.  If you have ever considered it but are hesitant, please don't be.  I can tell you from experience that it's nothing to worry about and the feeling of having contributed to saving lives is amazing.  ",null,1,cdl40ig,1r7v5g,askscience,top_week,4
pleasantliving,"I am in the Rare Donor Program and I never really understood what it means. All I know is I get a letter every few months making sure my address hasn't changed. On my donor card, my blood type says things like K:-1 and Jk(b-). Can anyone explain this to me? Why do some people need this rare blood? Does this mean I need rare blood? ",null,1,cdkzjdh,1r7v5g,askscience,top_week,5
OldMarmalade,"http://www.dnalc.org/view/15404-Chromosome-9-gene-for-blood-group-Matt-Ridley.html

AB - Very resistant to cholera

O - Susceptible to cholera

Among other things this may be one of the evolutionary pressures that explains variation in bloodtype. If you want to create the maximum number of ABs you'd have a population that was ~41% A and ~41% B to randomly generate ~18% AB supermen (in the time of cholera). This is indeed what you see in many populations http://www.blood.co.uk/images/content/pie-charts.jpg ",null,0,cdklqsz,1r7v5g,askscience,top_week,3
WasIsMitDenKohlen,"I haven't seen an answer to the first two questions. WHY are there different blood types. Why was there a need to have those distinct blood types. 

Would it make more sense evolutionary to have one blood type only, why bother preserving those distinct mutations, and everything that goes along with maintaining those? Or if not, why are there 3 distinct ones, and not 1000? What is the driving force to create 3 blood types? ",null,2,cdkpw6a,1r7v5g,askscience,top_week,4
redplate12,A clarification of the discussion of 'antigens' that represent the various groups. An O type does have an antigen structure but nobody has antibodies to it. The O antigen is a chain of 4 six carbon sugars. The A antigen has an added six carbon sugar (N-acetyl-galactosamine bound to second sugar in a 'branch) while the B antigen has an additional six carbon sugar (galactose) that is found bound to the fourth six carbon sugar core. An AB has both the A and B  five sugar chains present. Thus the difference is the presence of a single sugar moiety added to the core 4 sugar chain.,null,0,cdkswmc,1r7v5g,askscience,top_week,2
slakist,"I have no knowledge on the subject but have always been interested so this thread was great. I see some people have answered as to why we have different blood types, as well as explaining the difference between them. 

I would like to know if there is any disadvantage to having a specific blood type. For instance, I am O-; am I more susceptible to different kinds of diseases and disorders based on my blood type alone, more than a person with say, blood A? ",null,3,cdktzqz,1r7v5g,askscience,top_week,4
beliefinphilosophy,"Bonus factoid:

The different blood group antigens (or surface markers) are
Different kinds of sugars, (glucos, carbohydrate) and the +/- is a protein.





For example, the antigens of the ABO blood group are sugars. They are produced by a series of reactions in which enzymes catalyze the transfer of sugar units. A person's DNA determines the type of enzymes they have, and, therefore, the type of sugar antigens that end up on their red blood cells.
In contrast, the antigens of the Rh blood group are proteins. A person's DNA holds the information for producing the protein antigens. The RhD gene encodes the D antigen, which is a large protein on the red blood cell membrane. Some people have a version of the gene that does not produce D antigen, and therefore the RhD protein is absent from their red blood cells.

More [here](http://www.ncbi.nlm.nih.gov/books/NBK2264/)",null,0,cdkubis,1r7v5g,askscience,top_week,2
12and32,"I don't think the question of ""why we have different surface antigens"" has been answered. 

So why do we have differing surface and blood borne antigens? Are the genes that code for them related in any way, say by a few SNPs, or even just a single point mutation? I would think that they're related if we can predict heredity through simple Mendelian genetics, and because they do seem to have a high degree of compatibility (hemolytic newborn disease aside). There doesn't seem to be any particular reason for the Rh factor, nor does there seem to be a reason for antibodies that can only attack the A and B antigens.",null,0,cdkw1vb,1r7v5g,askscience,top_week,2
Jrj84105,"The different blood groups reflect differences in antigens present on the red blood cell surface. Because some of these antigenic variants confer resistance to infections by organisms such as malaria, certain blood groups are more common in different regions of the world. For the most part though, prior to the advent of blood transfusions, these differences were largely of no impact, essentially benign polymorphisms.

These kind of benign variations aren't just limited to our red cells but are present throughout our bodies. As we continue to challenge our bodies with new medical therapies, there turn out to be lots of previously irrelevant benign differences between people that now are seen as increasingly important factors in response to therapy and risk of side effects. ""Personalized medicine"" is largely an attempt to tailor therapy not just to a person's disease but also to the way in which any individual's response to therapies is a product of their unique set of variations.",null,0,cdl12gq,1r7v5g,askscience,top_week,2
MrGrow,"Our red cells consist of many different antigens (something that can trigger an immune response). Think of them like land mines. What people have can vary a ton (Kidd antigens, Duffy, Rh, Lewis, and so on). You simply make what you don't have. If I am A+ (very common) I am going to make the antibody to B. If you were to give me B blood, because I don't have that antigen on my own cells, my body will recognize that as foreign. What will more than likely happen is antibodies will coat the B blood cells, and when they are taken to the spleen, they will be ripped up and destroyed. This is called a transfusion reaction. There are different kinds of these and they each have there own symptoms. Why do humans have different types? Good question.     ",null,0,cdl68l5,1r7v5g,askscience,top_week,2
bigdaddymatty,"Whenever I donated blood the first time I became really interested in blood types and wondered this same question. I gave blood and didn't know my blood type before, but found out after that it was O- by looking on the donation companies website and logging in to see my donation session.   
I expect to receive a phone call each quarter now and every time I donate blood that ask me to do double red blood cells since I'm the universal donor! Giving blood is definitely worth it though",null,0,cdl6fqd,1r7v5g,askscience,top_week,2
laika84,"I checked comments and did not see this brought up as far as what the blood types ""actually mean.""  People have mentioned antigens and that these antigens are glycoproteins, which is correct.  However, I think it's also interesting/important to go a little further into how these different antigens are made and their significance, in terms of the ABO portion.

ABO bloods types are distinguished by the individual alleles that encode an enzyme.  The different enzymes have long names and essentially add different sugars via O-linked glycosylation to what could be a ""base"" sugar chain that is added first.  This ""base"" sugar chain exists on our RBCs and receives a fucose residue by fucosyl-transferase and having this enzyme alone constitutes the ""O"" phenotype/antigen.  The ""A"" and ""B"" enyzmes each add a different sugar to this base carb chain with the fucose and that creates the different antigens that we see.  You can think of it as the ""A"" enzyme adding the ""A"" sugar/antigen (N-acetylgalactosamine) and the ""B"" enzyme adding the ""B"" sugar/antigen (galactose).  It may seem trivial, but I thought it was interesting when I learned it and enhanced my understanding of what these antigens ""looked like"" on a biochemical level.

For a really cool tangent, do a search for the ""Bombay"" phenotype!",null,0,cdl02x8,1r7v5g,askscience,top_week,2
Shekho,"I don't remember where I heard this but i'll try to be nice, and simple....

Lets say a disease came along and wiped out all people with (x) blood type because they don't have a certain antibody that protects them from disease then other people with (y) blood type would live because they naturally can protect themselves..

If we all had the same blood type, wouldn't that mean we could easily be wiped out as a race from diseases? 

Please correct me if I'm wrong.",null,1,cdl33v2,1r7v5g,askscience,top_week,2
quiktom,"I was told that the blood types evolved over time. The O being the original and oldest when we ate anything we could find. The A type evolved when we got into agriculture and the B type when we started living in concentrated cities. Apparently AB is only 600 or so years old. AB blood can accept any blood before it (since those tpes were around when it evolved and essentially are a part of it) but no other type can accept AB, whereas at the opposite end O can be accepted by any but can accept none. All the carbons, sugars and whatnot are the details of how the evolution took place.

Apparently this affects what we should eat according to our blood types but I'm an O type and as much as I'd like to eat steak and nuts and raw veg it'd get really expensive without bread or pasta to fill me up.",null,3,cdl4ckm,1r7v5g,askscience,top_week,4
SparklePonyBoy,"I'm surprised that no one mentioned CMV, cytomegalovirus.  When blood is tested, spun, etc., they also test for this because it can kill immunocompromised individuals, also babies.  I know this because I used to donate blood every 8 weeks and they told me I am O- CMV- and that each donation can save up to 5 babies' lives.  Supposedly up to half of the general healthy population may be walking around with CMV and not even know.

http://www.cdc.gov/cmv/overview.html",null,0,cdl4r5d,1r7v5g,askscience,top_week,1
A_Soggy_Sheep,"This is real pseudoscience, but i would hazard a guess that it is beneficial for the human race to have different blood types for survival, incase some kind of disease was incredibly deadly to a specific blood type...

A bit like [diversify your bonds](http://www.youtube.com/watch?v=FTsNEUZx8v8) .

As i said earlier i may be totally wrong, so feel free to patronise me into oblivion. ",null,1,cdkkwwv,1r7v5g,askscience,top_week,2
Whisket,"Scientist discovered the [chemical reaction series](http://www.google.com/imgres?imgurl=http://www.theozonehole.com/images/ozoned43.jpg&amp;imgrefurl=http://www.theozonehole.com/ozonedestruction.htm&amp;h=438&amp;w=580&amp;sz=29&amp;tbnid=2B5I5wnJEI6IvM:&amp;tbnh=90&amp;tbnw=119&amp;zoom=1&amp;usg=__VSGqa4LvGkOQioh7a5z697w7CNA=&amp;docid=n7L0JaEnpXluyM&amp;sa=X&amp;ei=5pCPUsSTKKW42wXn0IA4&amp;sqi=2&amp;ved=0CDQQ9QEwBA) where chlorine acted as a catalyst for ozone depletion.

CFC stands for chloro-fluoro carbons. For those without O-chem knowledge,  carbon atoms can bond with up to 4 other atoms. In CFC's, there are at least one chlorine and one flourine atoms bonded to the central carbon atom, with the rest being hydrogen atoms (there are multiple variations based on numbers and positions of these atoms). When these CFC's get into the atmosphere, they start to break down due to UV radiation, and chlorine atoms are released. These chlorine atoms then proceed to catalyse the chemical reaction series I linked earlier.

[Here's a link from the US EPA with more detail](http://www.epa.gov/ozone/science/process.html)

A quote from the article: ""It is estimated that one chlorine atom can destroy over 100,000 ozone molecules before it is removed from the stratosphere """,null,0,cdkk6rk,1r7s9w,askscience,top_week,3
richard_woodhouse,"It's a bit of a long process but there were 4 major breakthroughs associated with the discovery of the Ozone Hole:

1.)  **The Fate of CFCs:**  Mario Molina was a postdoc working with Professor Sherwood Rowland at UC Riverside in the 70's.  His project was to determine the fate of CFCs in the atmosphere.  CFCs were used as refrigerants for a lot of industrial processes because they were incredibly stable.  They happen to be so stable that nothing in the troposphere can break them down, so no one really knew what happened to them when they were released.  Molina eventually discovered that the CFCs would finally break down high in the stratosphere (~20-40km) when they absorb high-energy UV light (~175-220nm).  This releases Cl radicals into the stratosphere.  Their famous Science paper [here](http://www.nature.com/nature/journal/v249/n5460/abs/249810a0.html) (with a great typo in the title, see the [pdf](http://www.nature.com/nature/journal/v249/n5460/pdf/249810a0.pdf)) presented this initial warning that CFCs could deplete ozone.  However, this reaction needed observational evidence now.

2.) **Observations of enhanced ClO:**  In 1976 James Anderson of Harvard lead a field campaign to the south pole and made in situ observations of enhanced ClO.  This confirmed that chlorine was present in the stratosphere and verified Molina's proposed CFC fate.

3.) **Observations of the Ozone Hole:**  This one's kinda funny in hindsight, in 1985 Joe Farman published a paper in Nature ([here](http://www.nature.com/nature/journal/v315/n6016/abs/315207a0.html)) showing dramatic ozone loss in the austral spring.  This was in direct contrast to satellite observations over the south pole at the time.  It turns out that the satellite data was being filtered out because concentrations were so low.  Essentially, the scientists had assumed the satellite measurements must have been noisy and were throwing out data that was ""unrealistic""!

4.) **Clouds and Crazy Chemistry:**  The original chemical reaction chain that Molina proposed was not the cause of the ozone hole (the source of chlorine was right but the catalytic reaction mechanism was not).  The ozone depletion that Farman noticed occurred in the spring at first light in the south pole.  There's three reasons for this depletion:  (1) the polar vortex keeps the south pole somewhat isolated from the surrounding regions, (2) typically the chlorine can be ""locked up"" in reservoir species such as chlorine-nitrate, however Susan Soloman discovered that [polar stratospheric clouds](http://en.wikipedia.org/wiki/Polar_stratospheric_cloud) can form due to the extremely cold temperatures and provide a surface for heterogeneous chemistry to occur.  The chlorine nitrate can react with HCl to yield Cl_2 + HNO_3 and the HNO_3 will dry deposit over the winter.  So in the spring there's a lot of Cl_2 that will photolyze and no nitrogen to lock it up.  (3)  In 1986 [Luisa and Mario Molina discovered](http://pubs.acs.org/doi/abs/10.1021/j100286a035) that ClO can self react: ClO+ClO -&gt; ClOOCl and that the dimer will break at the Cl-O bond, not the weaker O-O bond.  This means that ClO self reactions can produce radical chlorine and deplete ozone without radical oxygen atoms!  

In summary, Mario Molina proposed that CFCs could deplete ozone in the stratosphere.  His original paper correctly pointed out the source of chlorine in the stratosphere but didn't identify the actual reaction that would cause the ozone hole.  2 years later Jim Anderson then verified that enhanced chlorine was present in the stratosphere.  10 years later Joe Farman noticed the ozone hole.  3 years later Susan Soloman, Luisa Molina, and Mario Molina discovered the crazy set of chemical reactions that lead to the ozone hole.

Edit:  Added a note about the typo in the Science paper title.",null,0,cdm3m9e,1r7s9w,askscience,top_week,3
Surf_Science,"So an antidote can vary widely but I'm not aware of any 'traditional' antidotes that might think of from the movies. 

In the case of venoms we often use antibodies generated by exposing animals to the venom. These antibodies bind the venom, may prevent it from acting and may help our body remove it (antibody opsonization). 

In the same way an antibody will bind a dangerous substance we will sometimes use chelators to bind other molecules that may be problems (heavy metals, iron). 

In some other cases the antidote may be using something that competes with the dangerous substance for a receptor. In the case of ethylene glycol poisoning for example (and this is what happens when a dog for example drinks antifreeze) ethanol can be used as a treatment. Ethanol will compete for the target receptor with the ethylene glycol preventing the action of ethylene glycol (ethanol has a higher affinity for the receptor).  ",null,0,cdkjlta,1r7rep,askscience,top_week,3
alex199119,"I guess there are two main ways that drugs (which could be an antidote) are discovered. 

It's possible to test a large number of chemicals in a number of different systems and see what effect they have and if any of those effects are positive or beneficial. So upon testing a certain chemical, you may find that the chemical in question works as an antidote for a certain chemical poisoning. 

Or it's possible to look biologically at the effect a poision has on a body, and then try to 'engineer' a solution to combat that poison. So I suppose in someways you could say the latter method derives the antidote from the posion, but not from the actual presence of the poison but from the way the poison has an effect on the body and an understanding of this.",null,0,cdktj6s,1r7rep,askscience,top_week,1
snusmumrikan,"Realistically there is no difference, which the anti-GMO lobby refuses to admit. The development of hardier and more productive crop strains has been done for thousands of years through crossing plants and hybridisation, along with the development of larger load-bearing horses and farm animals.

Our ability to do it from an informed position and alter specific genes through 'genetic engineering' only makes it more precise and targeted and less likely to have the Brassicoraphanus problems that Karpechenko had in the 1920s; where he tried to cross radishes and cabbages to get to root of a raddish and the head of a cabbage. Unfortunately he got the useless root of a cabbage and the useless head of a raddish in one plant.

For you paper are you considering talking about Armand Pusztai and his flawed release of information regarding GMO potatoes poisoning rats? - refuted endlessly by reliable peer-reviewed studies and yet still one of the major reasons GMOs have a bad name. Also Prince Charles and his irrational vehement fight against GMO without any expertise in the area itself?",null,0,cdkks93,1r7qtk,askscience,top_week,2
ToThink,What Borlaug essentially did was he maximized the growing efficiency of wheat as well as increasing the number of disease resistance genes in wheat to increase overall crop efficiency of wheat. He did this on the macro scale by breeding several varieties of wheat together and selected for the best phenotype. An example would be how he bred wheat which were more likely to resist cold temperatures together for many generations. He kept doing this with other environmental conditions until the wheat became very resistant to any extreme environmental conditions altogether. He did genetic engineering on the macro scale by actively selecting favourable traits (selecting for the best trait by sexual selection) over several generations of wheat.,null,0,cdkko4q,1r7qtk,askscience,top_week,1
Cherrysquid,"The ball will land back in the boat if the boat is moving at a constant velocity. When you throw the ball straight up the ball not only has your vertical velocity that you gave it with your hand, but also the same horizontal velocity the boat has. The only acceleration acting on the ball is vertical, gravitational acceleration. With no horizontal acceleration the ball will maintain the same horizontal velocity (velocity of the boat) until it lands.",null,0,cdkg2oq,1r7qcs,askscience,top_week,13
AleccMG,"Start by considering that you, the ball, and the boat are all moving at the same velocity (speed and direction).  In the frame of reference of the boat, your velocity is zero (as is the ball before you throw it).

When you throw the ball straight into the air, you give it an initial vertical velocity.  With respect to the boat, it has no horizontal velocity.  The ball's motion is now entirely determined by the initial conditions (how you threw it), and the forces acting on the ball.  Since we are neglecting air resistance, the only force remaining is gravity which acts towards the boat.

Since we have no force in the horizontal direction, and your throw imparted no horizontal velocity, the ball will not move horizontally from the perspective of you on the boat.  If you don't catch the ball, you'll likely end up with a good knot on your head!",null,0,cdkhifr,1r7qcs,askscience,top_week,3
ArmyOfFluoride,"This is a really big question you're asking, but I'll try to give a bit of a foundation for you.   If you've ever taken a class in the life sciences you've likely heard the phrase ""structure determines function"".  I first learned this in the context of proteins: the amino acid structure of a protein determines its biochemical function.  This is also true for tissues however.  The way cells are organized in your heart is what allows it to work as the core of your circulatory system.  Your brain is no different.  The structure of the tissue in your brain is (as far as we know) what determines its functioning.    You also likely know that the structure of your body is determined by both your environment and your genetics.  ""Instinct"" then, can be thought of as the behavior that arises from the brain structure that can be attributed to genetics, as opposed to the environment.  This is where things get messy, as ascribing any trait as complex as behavior to entirely a genetic or environmental cause is impossible, because we all have both environmental and genetic histories.  Does that help?",null,3,cdklzjv,1r7q41,askscience,top_week,18
bags_of_geckos,"
I think its easier to use the term innate behavior rather than instincts because the latter has a lot of more metaphysical connotations in humans, at least. I can describe an innate behavior in a simpler organism, like a fly. 

Fly mating behavior is what we call stereotyped- given certain stimuli (like a young female fly or something that smells like one) a male fly will initiate a series of behaviors that are pretty much the same every time: a little wing dance, an attempt to mount her, etc. The neurons that sense the odor of the female fly connect to a few parts of the fly brain, including an area called the lateral horn. Its a little oversimplified, but neurons in that region connect with downstream neurons that control the flys muscle movements that cause them to do the dance, and more complex series of movements that include mounting. Its a circuit of neurons that starts with a sensory stimulus and ends in a behavior, with a little processing in the middle. 

So where to genes come in? Genes are what tell the brain how to structure itself in the first place. In each neuron there are a series of genes transcribed that tell the neuron how to develop, what kind of neuron to be, where to migrate to in the brain during development. The neurons in the lateral horn that bridge the gap between the smell of a female (sensory stimulus) and the mating (behavior) are where they are, and fire when they fire, because of the history of genetic switches that got them there. 

I should add, there is no one gene for this behavior or that, every single cell is the product of thousands of genes turned on or off throughout the organisms development. Cells (such as neurons) are the sum of their genetic history, and behaviors are the sum of the neural activity that was elicited by a stimulus. 

You can check out the work of [Vanessa Ruta](http://www.rockefeller.edu/research/faculty/labheads/VanessaRuta/), for a more in-depth explanation. I am probably butchering my description her work but you might find more of what you are looking for there.

Most of human behavior is learned, though we can suck as infants and grasp things, most of what we do during the day we had to learn at some point. We dont have a specific brain structure devoted to drinking coffee that is the same for all humans. Less complex organisms tend to have a much greater percentage of their brains devoted to innate behaviors than we do.




",null,1,cdknup8,1r7q41,askscience,top_week,15
zzerrp,"Yeah this is a pretty huge field of research, involving people working on a variety of brain circuits and the genetics that underlie their function.  A couple examples: Hopi Hoekstra studies how genetics affect the shapes of the burrows that mice decide to build (simplification; see her [lab website](http://www.oeb.harvard.edu/faculty/hoekstra/projects_behavioral_genetics.html) ).  And all the people who study [FOXP2](http://en.wikipedia.org/wiki/FOXP2) and its associated gene regulatory networks, which are involved in innate ability to acquire language and vocalize.  It's a very cool field of research, but it's tricky.  One big reason is that there is a lot more to the story than just the sequence of bases in the DNA.  There are [some 250,000 exons and a similar number of introns](http://www.ncbi.nlm.nih.gov/pubmed/15217358) known in the human genome. That gives a rough idea of how many distinct genes our bodies have to work with -- half a million.  Compare that with the over 100 BILLION (prob closer to a trillion) neurons that are in the human brain.  There isn't anywhere even close to enough genes to specify the properties of each neuron individually.  So it's all about how the body uses the products of these genes in combinations, and the program of development that is set up by those combinations, such that a functional network of neurons grows.  Innate behaviors are driven largely by the particular forms of the connections that have a tendency to keep arising out of that biological program of development.  ",null,0,cdkwlqa,1r7q41,askscience,top_week,2
null,null,null,9,cdkmeh4,1r7q41,askscience,top_week,4
ArmyOfFluoride,"This is a really big question you're asking, but I'll try to give a bit of a foundation for you.   If you've ever taken a class in the life sciences you've likely heard the phrase ""structure determines function"".  I first learned this in the context of proteins: the amino acid structure of a protein determines its biochemical function.  This is also true for tissues however.  The way cells are organized in your heart is what allows it to work as the core of your circulatory system.  Your brain is no different.  The structure of the tissue in your brain is (as far as we know) what determines its functioning.    You also likely know that the structure of your body is determined by both your environment and your genetics.  ""Instinct"" then, can be thought of as the behavior that arises from the brain structure that can be attributed to genetics, as opposed to the environment.  This is where things get messy, as ascribing any trait as complex as behavior to entirely a genetic or environmental cause is impossible, because we all have both environmental and genetic histories.  Does that help?",null,3,cdklzjv,1r7q41,askscience,top_week,18
bags_of_geckos,"
I think its easier to use the term innate behavior rather than instincts because the latter has a lot of more metaphysical connotations in humans, at least. I can describe an innate behavior in a simpler organism, like a fly. 

Fly mating behavior is what we call stereotyped- given certain stimuli (like a young female fly or something that smells like one) a male fly will initiate a series of behaviors that are pretty much the same every time: a little wing dance, an attempt to mount her, etc. The neurons that sense the odor of the female fly connect to a few parts of the fly brain, including an area called the lateral horn. Its a little oversimplified, but neurons in that region connect with downstream neurons that control the flys muscle movements that cause them to do the dance, and more complex series of movements that include mounting. Its a circuit of neurons that starts with a sensory stimulus and ends in a behavior, with a little processing in the middle. 

So where to genes come in? Genes are what tell the brain how to structure itself in the first place. In each neuron there are a series of genes transcribed that tell the neuron how to develop, what kind of neuron to be, where to migrate to in the brain during development. The neurons in the lateral horn that bridge the gap between the smell of a female (sensory stimulus) and the mating (behavior) are where they are, and fire when they fire, because of the history of genetic switches that got them there. 

I should add, there is no one gene for this behavior or that, every single cell is the product of thousands of genes turned on or off throughout the organisms development. Cells (such as neurons) are the sum of their genetic history, and behaviors are the sum of the neural activity that was elicited by a stimulus. 

You can check out the work of [Vanessa Ruta](http://www.rockefeller.edu/research/faculty/labheads/VanessaRuta/), for a more in-depth explanation. I am probably butchering my description her work but you might find more of what you are looking for there.

Most of human behavior is learned, though we can suck as infants and grasp things, most of what we do during the day we had to learn at some point. We dont have a specific brain structure devoted to drinking coffee that is the same for all humans. Less complex organisms tend to have a much greater percentage of their brains devoted to innate behaviors than we do.




",null,1,cdknup8,1r7q41,askscience,top_week,15
zzerrp,"Yeah this is a pretty huge field of research, involving people working on a variety of brain circuits and the genetics that underlie their function.  A couple examples: Hopi Hoekstra studies how genetics affect the shapes of the burrows that mice decide to build (simplification; see her [lab website](http://www.oeb.harvard.edu/faculty/hoekstra/projects_behavioral_genetics.html) ).  And all the people who study [FOXP2](http://en.wikipedia.org/wiki/FOXP2) and its associated gene regulatory networks, which are involved in innate ability to acquire language and vocalize.  It's a very cool field of research, but it's tricky.  One big reason is that there is a lot more to the story than just the sequence of bases in the DNA.  There are [some 250,000 exons and a similar number of introns](http://www.ncbi.nlm.nih.gov/pubmed/15217358) known in the human genome. That gives a rough idea of how many distinct genes our bodies have to work with -- half a million.  Compare that with the over 100 BILLION (prob closer to a trillion) neurons that are in the human brain.  There isn't anywhere even close to enough genes to specify the properties of each neuron individually.  So it's all about how the body uses the products of these genes in combinations, and the program of development that is set up by those combinations, such that a functional network of neurons grows.  Innate behaviors are driven largely by the particular forms of the connections that have a tendency to keep arising out of that biological program of development.  ",null,0,cdkwlqa,1r7q41,askscience,top_week,2
null,null,null,9,cdkmeh4,1r7q41,askscience,top_week,4
TehMulbnief,"Right up my alley! I worked at a food science company for a few months earlier this year and wouldn't you know it, we were developing caramel colorings!

I'm not too sure why ammonia would be added, or I should say, we never used ammonia in any of our colorings, but sulfites are used as a preservative. In fact, if you punch sodium sulfite into [wikipedia](http://en.wikipedia.org/wiki/Sodium_sulfite), one of the uses listed is a preservative.

Since sodium sulfite isn't exactly tasteless, there is a lot of pressure from consumers to eliminate its use in certain foodstuffs. Also, with the ever growing presence of chemophobia regarding food, consumers want less stuff added. However, sodium sulfite doesn't just limit bacteria growth.

The caramel coloring I was developing was actually based on a particular strain of corn starch. To extract the starch completely from the raw kernels, the corn was soaked in a dilute solution of sodium sulfite overnight. This helped loosen the intermolecular connections between the starch and cellulose present in each kernel. Higher yields of starch means more coloring which means lower prices for our buyers and their customers.

Let me know if you have any followup questions!",null,0,cdmxea3,1r7o3s,askscience,top_week,2
rupert1920,"Check out [pair production](http://en.wikipedia.org/wiki/Pair_production), where a particle and its antiparticle can be produced from a photon. This is essentially the reverse of [annihilation](http://en.wikipedia.org/wiki/Annihilation).

I'll also add that ""energy"" used to create ""mass"" is a bit of a an awkward wording, as both of these are properties of a system. It's akin to saying can ""height"" be used to create ""width"". In the example above, it's a case of particles with no rest mass creating particles with rest mass.",null,27,cdkftg9,1r7mfl,askscience,top_week,144
50bmg,"Absolutely. If you add enough energy to a particle, and collide it with another, you can create new particles. This happens in particle accelerators all the time, and it is the method by which we create and study antimatter (usually by bouncing protons off iridium and creating antiprotons). However, it does take a LOT of energy to do, which is why antimatter is the most expensive material (per gram) every created. ",null,8,cdkg0ub,1r7mfl,askscience,top_week,35
technogeeky,"I'm surprised to not see a direct and relevant answer.

**Yes** and this is exactly what high energy accelerators do. Really. All of them. Stanford's Linear Collider. The LEP. The LHC. The Tevatron (before it was closed).

You could ask yourself: If the [Large Electron-Positron Collider](http://en.wikipedia.org/wiki/Large_Electron%E2%80%93Positron_Collider) collided electrons and positrons (which weigh 1/1836 as much as a proton), how could that machine have possibly seen the W and Z bosons (which weigh ~80 and ~90 times more than the proton)? Combining the two, where did this extra ~150,000 electron masses come from? There isn't anywhere near enough mass to create these particles in the collision!

The point of particle colliders is to provide the particles with enough extra energy that they can produce interesting particles as outputs. Extra might be understating it a bit: the vast majority of the energy of colliding particles in these machines is the kinetic (motion) energy of the particles, and not the rest mass of the particles.

In other words, if a lot of energy **could not** be used to create mass, then particle physics would not exist at all.",null,3,cdkm8h5,1r7mfl,askscience,top_week,14
yinz_n-at,"Yep they're equivalent. But in the nuclear world, energy is very big and mass is very small (from our point of view). This is why nuclear power works (convert a small amount of mass into equivalent energy). So going in reverse will take a tremendous amount of energy but is still possible (particle accelerators). 

Look at Binding Energy per nucleon for the ranges of fission and fusion. About iron is the most stable nucleus 

Source: I'm a Nuclear Eng. ",null,2,cdkkfnm,1r7mfl,askscience,top_week,7
QCD-uctdsb,"In any experiment used to determine mass, pure energy (photons/gluons) is actually indistinguishable from matter. Imagine you have a mirrored box full of photons, and let's say that the total energy of all these photons is 9x10^17 joules. If you measure the mass of this box (say you put it on a scale) then from your mass-energy equation the box will have a mass of 10 kg. 

As another example, up and down quarks have a mass of roughly 3 MeV. You have 3 of these in a proton, so you'd guess that the proton would have a mass of 9 MeV. But nope!.. the measured mass is actually 938 MeV. This is because protons have extra ""binding"" energy (mostly in the form of gluons) which contribute to the total proton mass.

To answer your question then... yes, it happens all the time. If you wanted to create *matter*, well that's a different question. You would need some mechanism (pair production) and experimental setup (particle accelerators) for creating quarks and leptons from photons and gluons.",null,0,cdkli31,1r7mfl,askscience,top_week,1
JonathanFeinberg,"Indeed yes. A fast thing has more mass than a slow thing. A compressed spring has more mass than a relaxed spring. Of course, under ordinary circumstances, this differences are vanishingly small. But, as other answers have pointed out, you can actually create new particles from the kinetic energy of a collision of existing particles.",null,0,cdkms66,1r7mfl,askscience,top_week,1
_ridden,"To put it shortly, yes. The addition of energy into a system can be used to create mass, this is why in the Large Hadron Collider (LHC @ CERN) particles are sped up (thus giving them energy) and colliding two particles together, creating new particles which may overall have a greater mass then before.",null,0,cdkpi9s,1r7mfl,askscience,top_week,1
roh8880,"You're talking about matter to energy/energy to matter transference. 

The issue that has arisen during this research is in order to construct any material object is that you have to build each molecule and have them stabilize. It's the stabilization that is the problem, if I recall the article I read correctly.",null,0,cdkydl1,1r7mfl,askscience,top_week,1
jakkes12,This is what happens in CERN. As the particle that's being accelerated it gains energy.The energy is transformed into kinetic energy until it reaches a speed close to the speed of light. Nothing can go faster than light therefore the energy added cannot be transformed into kinetic energy at this point. Instead the energy is transformed into mass. The particle being accelerated will start growing.,null,0,cdl5a12,1r7mfl,askscience,top_week,1
null,null,null,16,cdkhys9,1r7mfl,askscience,top_week,6
rupert1920,"Check out [pair production](http://en.wikipedia.org/wiki/Pair_production), where a particle and its antiparticle can be produced from a photon. This is essentially the reverse of [annihilation](http://en.wikipedia.org/wiki/Annihilation).

I'll also add that ""energy"" used to create ""mass"" is a bit of a an awkward wording, as both of these are properties of a system. It's akin to saying can ""height"" be used to create ""width"". In the example above, it's a case of particles with no rest mass creating particles with rest mass.",null,27,cdkftg9,1r7mfl,askscience,top_week,144
50bmg,"Absolutely. If you add enough energy to a particle, and collide it with another, you can create new particles. This happens in particle accelerators all the time, and it is the method by which we create and study antimatter (usually by bouncing protons off iridium and creating antiprotons). However, it does take a LOT of energy to do, which is why antimatter is the most expensive material (per gram) every created. ",null,8,cdkg0ub,1r7mfl,askscience,top_week,35
technogeeky,"I'm surprised to not see a direct and relevant answer.

**Yes** and this is exactly what high energy accelerators do. Really. All of them. Stanford's Linear Collider. The LEP. The LHC. The Tevatron (before it was closed).

You could ask yourself: If the [Large Electron-Positron Collider](http://en.wikipedia.org/wiki/Large_Electron%E2%80%93Positron_Collider) collided electrons and positrons (which weigh 1/1836 as much as a proton), how could that machine have possibly seen the W and Z bosons (which weigh ~80 and ~90 times more than the proton)? Combining the two, where did this extra ~150,000 electron masses come from? There isn't anywhere near enough mass to create these particles in the collision!

The point of particle colliders is to provide the particles with enough extra energy that they can produce interesting particles as outputs. Extra might be understating it a bit: the vast majority of the energy of colliding particles in these machines is the kinetic (motion) energy of the particles, and not the rest mass of the particles.

In other words, if a lot of energy **could not** be used to create mass, then particle physics would not exist at all.",null,3,cdkm8h5,1r7mfl,askscience,top_week,14
yinz_n-at,"Yep they're equivalent. But in the nuclear world, energy is very big and mass is very small (from our point of view). This is why nuclear power works (convert a small amount of mass into equivalent energy). So going in reverse will take a tremendous amount of energy but is still possible (particle accelerators). 

Look at Binding Energy per nucleon for the ranges of fission and fusion. About iron is the most stable nucleus 

Source: I'm a Nuclear Eng. ",null,2,cdkkfnm,1r7mfl,askscience,top_week,7
QCD-uctdsb,"In any experiment used to determine mass, pure energy (photons/gluons) is actually indistinguishable from matter. Imagine you have a mirrored box full of photons, and let's say that the total energy of all these photons is 9x10^17 joules. If you measure the mass of this box (say you put it on a scale) then from your mass-energy equation the box will have a mass of 10 kg. 

As another example, up and down quarks have a mass of roughly 3 MeV. You have 3 of these in a proton, so you'd guess that the proton would have a mass of 9 MeV. But nope!.. the measured mass is actually 938 MeV. This is because protons have extra ""binding"" energy (mostly in the form of gluons) which contribute to the total proton mass.

To answer your question then... yes, it happens all the time. If you wanted to create *matter*, well that's a different question. You would need some mechanism (pair production) and experimental setup (particle accelerators) for creating quarks and leptons from photons and gluons.",null,0,cdkli31,1r7mfl,askscience,top_week,1
JonathanFeinberg,"Indeed yes. A fast thing has more mass than a slow thing. A compressed spring has more mass than a relaxed spring. Of course, under ordinary circumstances, this differences are vanishingly small. But, as other answers have pointed out, you can actually create new particles from the kinetic energy of a collision of existing particles.",null,0,cdkms66,1r7mfl,askscience,top_week,1
_ridden,"To put it shortly, yes. The addition of energy into a system can be used to create mass, this is why in the Large Hadron Collider (LHC @ CERN) particles are sped up (thus giving them energy) and colliding two particles together, creating new particles which may overall have a greater mass then before.",null,0,cdkpi9s,1r7mfl,askscience,top_week,1
roh8880,"You're talking about matter to energy/energy to matter transference. 

The issue that has arisen during this research is in order to construct any material object is that you have to build each molecule and have them stabilize. It's the stabilization that is the problem, if I recall the article I read correctly.",null,0,cdkydl1,1r7mfl,askscience,top_week,1
jakkes12,This is what happens in CERN. As the particle that's being accelerated it gains energy.The energy is transformed into kinetic energy until it reaches a speed close to the speed of light. Nothing can go faster than light therefore the energy added cannot be transformed into kinetic energy at this point. Instead the energy is transformed into mass. The particle being accelerated will start growing.,null,0,cdl5a12,1r7mfl,askscience,top_week,1
null,null,null,16,cdkhys9,1r7mfl,askscience,top_week,6
MayContainNugat,"It feels cold precisely *because* it conducts heat so well. If the metal is at 20C, and your body temperature is 37C, then the metal will very efficiently conduct heat away from your body, and that feels cold. It does this much more efficiently than wood or plastic. ",null,9,cdkejkr,1r7l11,askscience,top_week,92
ww-shen,"Metals generally are good conductors, but they are good capacitors aswell. So they can 'take' many heat 'fast' from your hand. The cooling process will last as long as the temperature of the  metal is equal with your hand's. The metals inner temperature will be almost equal (becouse of the good coductivity) and will last 'long' to warm it (becouse of the capacitivy). Ceramics for example are good capacitors and bad conductors. It takes longer time to warm up, but they will radiate the heat longer. (coclkle stove). Plastics and air (and vacuum) are bad in both, so they can be used in heat insulations.",null,12,cdket3p,1r7l11,askscience,top_week,2
Das_Mime,"Tidal locking means that the same face of the Moon always points toward Earth.

This occurs because tidal forces--the difference in gravitational tug from the Earth between different parts of the Moon-- cause small distortions and stretches in the shape of the Moon. These distortions dissipate the moon's rotational energy, slowing its rotation until eventually ~~it stops.~~ its rotational period is equal to its orbital period.",null,5,cdkfbfz,1r7ivu,askscience,top_week,16
Das_Mime,"Tidal locking means that the same face of the Moon always points toward Earth.

This occurs because tidal forces--the difference in gravitational tug from the Earth between different parts of the Moon-- cause small distortions and stretches in the shape of the Moon. These distortions dissipate the moon's rotational energy, slowing its rotation until eventually ~~it stops.~~ its rotational period is equal to its orbital period.",null,5,cdkfbfz,1r7ivu,askscience,top_week,16
EdwardDeathBlack,"The positive/negative temperature scales of Celsius and Fahrenheit are arbitrary. The more common scientific scale is the [Kelvin](http://en.wikipedia.org/wiki/Kelvin). The Kelvin never goes negative. It goes towards zero, the famous ""absolute zero"", the temperature at which ""jiggling"" is at its absolute minimum (! But not zero jiggling , see [""zero point energy""](http://en.wikipedia.org/wiki/Zero-point_energy)). Absolute zero is -273.15 Celsisus, -459.67 Fahrenheit and of course, 0 Kelvin. 

As for why life isn't very fond of negative temperature, one place to look at is of course the freezing of water. We are mostly made of water, so freezing isn't very desirable. At atmospheric pressure, water freezes at 0C, 32F or 273.15K. Below those, things get trickier (but not impossible, plenty of life forms have evolved to live in freezing weather). 

There are other places to look to for reasons why life on earth doesn't strive in very cold weather , chemistry kinetics for exemple. But we can get to that if you have follow up questions. ",null,1,cdkjjky,1r7i1w,askscience,top_week,9
spookyjeff,"Besides what others have mentioned here, at low temperatures proteins can undergo ""cold denaturaiton."" Hydrogen bonding and the hydrophobic effect are dependent on temperature, they are also responsible for the shapes of macromolecules in the body. At low (and high) temperatures these molecules can lose their shape and function.",null,0,cdkrgt2,1r7i1w,askscience,top_week,2
ramk13,"Freezing is a major (but not the only) problem. Life depends on the diffusion or active transport of chemicals across concentration gradient. For example, if there is more sugar outside a cell than inside, some can diffuse in. When you freeze the cell, mass transport grinds to a halt. Chemical reactions also slow down as temperature decreases, but not all at the same rate. Also, freezing water expands and can rip apart cellular structure. The combination of all those things disrupts most of the basic processes in a cell (or organism).",null,0,cdkogih,1r7i1w,askscience,top_week,1
GruntingButtNugget,"The Rings are Saturn are much more dense than the asteroid belt. The rings themselves are actually seven sets of concentric circles that circle the planet. There are gaps between each set of rings big enough that Cassini was able to fly between two sets of rings on its way closer to the planet. Flying THROUGH a ring would be similar to how you would probably have originally pictured the asteroid belt

Edit: spelling is hard on a phone",null,0,cdkjzau,1r7hch,askscience,top_week,3
DarylHannahMontana,"One thing to note here is that the *definition* of an integral is ""the area between the function and the x-axis"" (ok, the real definition is a little more technical than that, but the main idea is the same; if you remember Riemann sums from your calculus class, *that* is the real definition of the integral^\*).

It is a *fortunate coincidence* (i.e. a theorem) that the integral of a function is related to its antiderivative; this is the fundamental theorem of calculus (and more generally, there is something called [Stokes' theorem](http://en.wikipedia.org/wiki/Stokes%27_theorem), of which FToC is a specific example).

So, if you can find an antiderivative for a function, that gives you an easy way to calculate an integral ""by hand"" (and, as others have said, there always *is* an antiderivative, but it may not be ""elementary"" (see JoshuaZ1's post)).

But if you can't find an antiderivative (and usually, you can't), you can just integrate using the definition (adding up the area of very narrow rectangles or [trapezoids](http://en.wikipedia.org/wiki/Trapezoidal_rule)) and get a numerical value that is as precise as needed.

That is, is some sense, using an antiderivative is the exceptional way of computing an integral, and numerical integration is the ""standard"" method.

\*: there's also a Lebesgue integral which is defined differently, but this is not the time to discuss it fully.

EDIT: misspelled 'Lebesgue', and added wikipedia link to Stokes' thm",null,1,cdki2kc,1r7hb8,askscience,top_week,11
JoshuaZ1,"The issue simply put is that there are functions without an elementary anti-derivative. For our purposes, a function is [elementary](https://en.wikipedia.org/wiki/Elementary_function) if it a combination of x, log, constants, exponentials, closed under addition, subtraction, division, multiplication, and composition. Note that we get trig functions from using exponentials with complex numbers, but if you want to stick to the reals you can instead throw in the basic trig functions and inverse trig functions. You'll get essentially the same set for the totally real functions but it turns out this is a harder definition to  work with.

[Liouville's Theorem](https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra\)) severely restricts what anti-derivatives can look like, and can be used to prove that there are elementary functions that don't have elementary anti-derivatives. Now, one can extend what one is working with to a larger set of functions. For example, one can also throw in the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function). But these functions only help so much, and they are about as hard to approximate as simply doing the numeric approximation of the integral you care about. But at another level, this isn't so bad, you are in practice going to need approximations even for elementary functions for practical purposes, so this is a comparatively minor inconvenience for engineers. Generally,  it is mildly inconvenient for physicists, and is substantially more inconvenient for mathematicians who may care about the exact behavior of a function, and may have to get that way from analyzing its derivative rather than the function directly. 
",null,2,cdkgcbp,1r7hb8,askscience,top_week,11
ohsohigh,"The approach we take to solving integrals is based on the fact that integrals are antiderivatives. To oversimplify quite a bit, we basically look at a function and say this looks like the derivative of this other function, so that must  be the integral. If a function doesn't look like the derivative of some other function then we don't know how to integrate it analytically and have to use a computer to approximate the answer.",null,2,cdkf9ma,1r7hb8,askscience,top_week,5
Sirkkus,"What exactly constitutes an analytical solution or closed-form solution is subjective and arbitrary. Take for example the differential equation dy/dx = y. Most people will say that this equation has an analytical solution, the exponential function. But what exactly *is* the exponential function? It can be defined in a number of ways, but ultimately if you want to calculate it's value you have to use some numerical definition. The only reason that the exponential gets to be considered an analytic solution is that it's very common and most people are very comfortable with it's properties.

Some integrals may not have solutions *in terms of other functions that we're familiar with*, and that's all we mean by not having an analytic solution. If a particular integral comes up a lot you could give it's solution a name and include it in your new definition of elementary functions. This could be useful later if you find that some other complicated integral can be written in terms of this one, meaning you may not need to write a new numerical algorithm to compute it.",null,2,cdki7sj,1r7hb8,askscience,top_week,6
doctorbong,"In addition to the good answers already here, I'd like to point out [this thread](http://mathoverflow.net/questions/66377/why-is-differentiating-mechanics-and-integration-art) on MathOverflow. In particular, I like Terry Tao's answer. In summary, integration is a global operation: In fact, when one defines a Lebesgue integral, a pointwise definition of a function isn't even needed. On the other hand, the derivative is an *extremely* local property of a function - it only deals with limits over arbitrarily small neighbourhoods of a point.",null,1,cdl0l9k,1r7hb8,askscience,top_week,5
DarylHannahMontana,"One thing to note here is that the *definition* of an integral is ""the area between the function and the x-axis"" (ok, the real definition is a little more technical than that, but the main idea is the same; if you remember Riemann sums from your calculus class, *that* is the real definition of the integral^\*).

It is a *fortunate coincidence* (i.e. a theorem) that the integral of a function is related to its antiderivative; this is the fundamental theorem of calculus (and more generally, there is something called [Stokes' theorem](http://en.wikipedia.org/wiki/Stokes%27_theorem), of which FToC is a specific example).

So, if you can find an antiderivative for a function, that gives you an easy way to calculate an integral ""by hand"" (and, as others have said, there always *is* an antiderivative, but it may not be ""elementary"" (see JoshuaZ1's post)).

But if you can't find an antiderivative (and usually, you can't), you can just integrate using the definition (adding up the area of very narrow rectangles or [trapezoids](http://en.wikipedia.org/wiki/Trapezoidal_rule)) and get a numerical value that is as precise as needed.

That is, is some sense, using an antiderivative is the exceptional way of computing an integral, and numerical integration is the ""standard"" method.

\*: there's also a Lebesgue integral which is defined differently, but this is not the time to discuss it fully.

EDIT: misspelled 'Lebesgue', and added wikipedia link to Stokes' thm",null,1,cdki2kc,1r7hb8,askscience,top_week,11
JoshuaZ1,"The issue simply put is that there are functions without an elementary anti-derivative. For our purposes, a function is [elementary](https://en.wikipedia.org/wiki/Elementary_function) if it a combination of x, log, constants, exponentials, closed under addition, subtraction, division, multiplication, and composition. Note that we get trig functions from using exponentials with complex numbers, but if you want to stick to the reals you can instead throw in the basic trig functions and inverse trig functions. You'll get essentially the same set for the totally real functions but it turns out this is a harder definition to  work with.

[Liouville's Theorem](https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra\)) severely restricts what anti-derivatives can look like, and can be used to prove that there are elementary functions that don't have elementary anti-derivatives. Now, one can extend what one is working with to a larger set of functions. For example, one can also throw in the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function). But these functions only help so much, and they are about as hard to approximate as simply doing the numeric approximation of the integral you care about. But at another level, this isn't so bad, you are in practice going to need approximations even for elementary functions for practical purposes, so this is a comparatively minor inconvenience for engineers. Generally,  it is mildly inconvenient for physicists, and is substantially more inconvenient for mathematicians who may care about the exact behavior of a function, and may have to get that way from analyzing its derivative rather than the function directly. 
",null,2,cdkgcbp,1r7hb8,askscience,top_week,11
ohsohigh,"The approach we take to solving integrals is based on the fact that integrals are antiderivatives. To oversimplify quite a bit, we basically look at a function and say this looks like the derivative of this other function, so that must  be the integral. If a function doesn't look like the derivative of some other function then we don't know how to integrate it analytically and have to use a computer to approximate the answer.",null,2,cdkf9ma,1r7hb8,askscience,top_week,5
Sirkkus,"What exactly constitutes an analytical solution or closed-form solution is subjective and arbitrary. Take for example the differential equation dy/dx = y. Most people will say that this equation has an analytical solution, the exponential function. But what exactly *is* the exponential function? It can be defined in a number of ways, but ultimately if you want to calculate it's value you have to use some numerical definition. The only reason that the exponential gets to be considered an analytic solution is that it's very common and most people are very comfortable with it's properties.

Some integrals may not have solutions *in terms of other functions that we're familiar with*, and that's all we mean by not having an analytic solution. If a particular integral comes up a lot you could give it's solution a name and include it in your new definition of elementary functions. This could be useful later if you find that some other complicated integral can be written in terms of this one, meaning you may not need to write a new numerical algorithm to compute it.",null,2,cdki7sj,1r7hb8,askscience,top_week,6
doctorbong,"In addition to the good answers already here, I'd like to point out [this thread](http://mathoverflow.net/questions/66377/why-is-differentiating-mechanics-and-integration-art) on MathOverflow. In particular, I like Terry Tao's answer. In summary, integration is a global operation: In fact, when one defines a Lebesgue integral, a pointwise definition of a function isn't even needed. On the other hand, the derivative is an *extremely* local property of a function - it only deals with limits over arbitrarily small neighbourhoods of a point.",null,1,cdl0l9k,1r7hb8,askscience,top_week,5
hideous-bike,"A differential allows for the driving wheels to spin at different rotations per minute therefore allowing a car to turn corners. This is because the inside wheel in a turn has less distance to cover than the oudside wheel on the vehicle in the same turn. 

Think plastic coffe cup on it's side. It will roll in a circle / turn because the bottom is narrower than the dinking end of the cup. What a differential essentially does is give power to the wheel with the least resistance. This is a desirable trait in wheels on a road surface, because otherwise the driving wheels start to squirm and bounce. 

In mud however this is not, because when a drive wheel looses traction and starts to spin the power of the engine will just be sent to the spinning wheel and therefore not adding to the traction of the vehicle. To overcome this some cars can lock their differentials to ensure that both wheels will start churning at the same speed giving a lot more traction to the vehicle.

On a side note: don't lock your diffs on a road surface (if your car has the option). That is, unless you really like your mechanic, and like to give him, or her, fist fulls of money.     

",null,3,cdkdzw6,1r7e4c,askscience,top_week,24
stairwaytoheaven57,"Where did you hear this info?  Locking differentials are almost always paired with four wheel drive.  It certainly does get more traction in addition to 4x4, but a locking differential without 4x4 wouldn't be as effective.  With four wheel drive you have power going to every wheel unless you lose traction in one wheel from the front and one wheel from the back, but until then you're pulling with all the tread so it's less likely that you'll get in a situation where you lose traction.  With just a locking differential you would have to lose traction to both back wheels which is more likely.  First, the back generally has less weight on it, second, you can steer the front wheels back and forth to try to grab onto the sides of the hole, and lastly, without 4x4 the front wheels would be introducing drag instead of pulling. ",null,3,cdkeg17,1r7e4c,askscience,top_week,23
MrPickleCoppter,"Locking differentials are best suited for off-road or low traction environments. With a locked differential both drive wheels will turn at the same speed making turning more difficult if you have full traction.  This will give more wear and tear on your tires on asphalt and your tires will chirp as you go around a turn.

I have just installed an auto locker in to the front differential of my Jeep.  Its is only active if I put it in 4 wheel drive and if I apply gas.  If 4 wheel drive is not engaged it will still preform like an open differential but with a ratcheting mechanism.

[install process of the locker](http://imgur.com/a/HlDGq) 
 ",null,0,cdkfdva,1r7e4c,askscience,top_week,8
LibertyBill,"A locking diff isn't always necessarily better than 4x4. 
Not only that but 4x4 accompanied with a limited slip rear diff is much better suited to everyday driving due to the ""flexibility"" of a limited slip diff vs a locked diff.

A locking diff would have very limited use in a primarily road vehicle. This is mostly due to the fact that the tires will spin at the same speed even when taking turning into consideration. On pavement this will cause ""wheel hop"" and a lot of strain is put at the diff wearing it down/damaging it. Wheel hop will occur in vehicles with 4x4 with open or limited slip diffs as well but these options are more ""giving"" in terms of stress on the diffs.

Also, it's a myth that 4x4 means all four tires have ""power"" going to them.  The only case this would apply to is if both the front and rear diff are locked. I believe this option exists in some Hummer models. In many ""All-wheel"" and 4x4 designs there is usually one or two ""open"" diffs involved. An example would be my Tacoma. It has 4x4 but both the front and rear diff are open. This means (unfortunately) the energy will be routed to the tires that give the least resistance. I say unfortunate as these tires will spin in place. To clarify this point even more I actually got my truck stuck in 4x4. Both my front left and rear left tire were on slippery ground.  I was however able to get out by cycling the steering wheel back and forth.

So to answer your question: Given all the possible scenarios of ""4x4"" I'd still take the 4x4, even with both front and rear open diffs, over just a locked rear dif any day.",null,0,cdkkbbr,1r7e4c,askscience,top_week,2
pbae,"Most cars have ""Open Differentials"" which already has been explained, let's the outside wheel turn faster than the inside wheel when a car is making a turn.

A MAJOR disadvantage of an ""Open Diff"" is that the wheel with the LEAST amount of traction gets the MOST power provided by the engine.  For example, if you take a car with an open diff and jack it up on either the left or right side of the car and then rev the engine, the wheel that's in the air is the one that is going to spin, not the wheel that is touching the ground.

Now imagine being off-road.  If one of your driven wheels gets stuck in the air, you would be stuck because your car or truck isn't going anywhere because the only wheel getting power is the one in the air.  You would be able to get out of this situation if you had a locking diff since you could lock the diffs and send power to both wheels at the same time.

And to answer the OP's question, a rear-locking diff is better than four wheel drive (4WD) for the reason explained above because some 4WDs have two open diffs.  The best would be 4WD with a locking diff.

",null,0,cdkq885,1r7e4c,askscience,top_week,1
hideous-bike,"A differential allows for the driving wheels to spin at different rotations per minute therefore allowing a car to turn corners. This is because the inside wheel in a turn has less distance to cover than the oudside wheel on the vehicle in the same turn. 

Think plastic coffe cup on it's side. It will roll in a circle / turn because the bottom is narrower than the dinking end of the cup. What a differential essentially does is give power to the wheel with the least resistance. This is a desirable trait in wheels on a road surface, because otherwise the driving wheels start to squirm and bounce. 

In mud however this is not, because when a drive wheel looses traction and starts to spin the power of the engine will just be sent to the spinning wheel and therefore not adding to the traction of the vehicle. To overcome this some cars can lock their differentials to ensure that both wheels will start churning at the same speed giving a lot more traction to the vehicle.

On a side note: don't lock your diffs on a road surface (if your car has the option). That is, unless you really like your mechanic, and like to give him, or her, fist fulls of money.     

",null,3,cdkdzw6,1r7e4c,askscience,top_week,24
stairwaytoheaven57,"Where did you hear this info?  Locking differentials are almost always paired with four wheel drive.  It certainly does get more traction in addition to 4x4, but a locking differential without 4x4 wouldn't be as effective.  With four wheel drive you have power going to every wheel unless you lose traction in one wheel from the front and one wheel from the back, but until then you're pulling with all the tread so it's less likely that you'll get in a situation where you lose traction.  With just a locking differential you would have to lose traction to both back wheels which is more likely.  First, the back generally has less weight on it, second, you can steer the front wheels back and forth to try to grab onto the sides of the hole, and lastly, without 4x4 the front wheels would be introducing drag instead of pulling. ",null,3,cdkeg17,1r7e4c,askscience,top_week,23
MrPickleCoppter,"Locking differentials are best suited for off-road or low traction environments. With a locked differential both drive wheels will turn at the same speed making turning more difficult if you have full traction.  This will give more wear and tear on your tires on asphalt and your tires will chirp as you go around a turn.

I have just installed an auto locker in to the front differential of my Jeep.  Its is only active if I put it in 4 wheel drive and if I apply gas.  If 4 wheel drive is not engaged it will still preform like an open differential but with a ratcheting mechanism.

[install process of the locker](http://imgur.com/a/HlDGq) 
 ",null,0,cdkfdva,1r7e4c,askscience,top_week,8
LibertyBill,"A locking diff isn't always necessarily better than 4x4. 
Not only that but 4x4 accompanied with a limited slip rear diff is much better suited to everyday driving due to the ""flexibility"" of a limited slip diff vs a locked diff.

A locking diff would have very limited use in a primarily road vehicle. This is mostly due to the fact that the tires will spin at the same speed even when taking turning into consideration. On pavement this will cause ""wheel hop"" and a lot of strain is put at the diff wearing it down/damaging it. Wheel hop will occur in vehicles with 4x4 with open or limited slip diffs as well but these options are more ""giving"" in terms of stress on the diffs.

Also, it's a myth that 4x4 means all four tires have ""power"" going to them.  The only case this would apply to is if both the front and rear diff are locked. I believe this option exists in some Hummer models. In many ""All-wheel"" and 4x4 designs there is usually one or two ""open"" diffs involved. An example would be my Tacoma. It has 4x4 but both the front and rear diff are open. This means (unfortunately) the energy will be routed to the tires that give the least resistance. I say unfortunate as these tires will spin in place. To clarify this point even more I actually got my truck stuck in 4x4. Both my front left and rear left tire were on slippery ground.  I was however able to get out by cycling the steering wheel back and forth.

So to answer your question: Given all the possible scenarios of ""4x4"" I'd still take the 4x4, even with both front and rear open diffs, over just a locked rear dif any day.",null,0,cdkkbbr,1r7e4c,askscience,top_week,2
pbae,"Most cars have ""Open Differentials"" which already has been explained, let's the outside wheel turn faster than the inside wheel when a car is making a turn.

A MAJOR disadvantage of an ""Open Diff"" is that the wheel with the LEAST amount of traction gets the MOST power provided by the engine.  For example, if you take a car with an open diff and jack it up on either the left or right side of the car and then rev the engine, the wheel that's in the air is the one that is going to spin, not the wheel that is touching the ground.

Now imagine being off-road.  If one of your driven wheels gets stuck in the air, you would be stuck because your car or truck isn't going anywhere because the only wheel getting power is the one in the air.  You would be able to get out of this situation if you had a locking diff since you could lock the diffs and send power to both wheels at the same time.

And to answer the OP's question, a rear-locking diff is better than four wheel drive (4WD) for the reason explained above because some 4WDs have two open diffs.  The best would be 4WD with a locking diff.

",null,0,cdkq885,1r7e4c,askscience,top_week,1
patchgrabber,"Cats are [obligate carnivores](http://en.wikipedia.org/wiki/Carnivore#Obligate_carnivores), meaning that they need to get all their nutrition from animal sources, and they also lack the digestive enzymes to break down plant material. Kibble can be used as a food source because it contains some animal meat and likely an extraneous source of taurine, which is very important for cats.

Dogs, however, are also carnivores. Their short intestines and colon are not well-suited to digesting plant matter either, as most digestion for dogs is in the stomach and they lack the long intestines required for proper fermentation of plant matter by intestinal microbes, although they do have higher levels of amylase as [this](http://news.sciencemag.org/plants-animals/2013/01/diet-shaped-dog-domestication) study found. This means that over time having been domesticated and fed starchy foods, they have somewhat adapted to be able to process these foods a little better. This is similar to people; those from Europe and Asia have more amylase production than people from Africa. It is not advisable to only feed your dog vegetables, as they are still carnivores like their wolf counterparts. Dogs' stomachs are also different than a feline's. Dogs have strong muscles that can push large and confusing things through their digestive system. An old veterinarian friend of mine would tell me stories of socks and other assorted weird things being passed through a dog's system.",null,3,cdkeiwk,1r7cxp,askscience,top_week,20
mrsix,"Cats are [Obligate carnivores](http://en.wikipedia.org/wiki/Obligate_carnivore#Obligate_carnivores) - their metabolisms are unable to produce a few essential nutrients out of vegetable matter, and therefore take it in directly from animal protein. A very important one specifically for cats is [Taurine](http://www.vcahospitals.com/main/pet-health-information/article/animal-health/taurine-in-cats/3857), which [some of the first 'cat food' in the 50s-70s lacked](http://www.onlynaturalpet.com/knowledgebase/knowledgebasedetail.aspx?articleid=113) which unfortunately lead to many cat's premature deaths until about the 80s. Most animals are able to produce it from methionine which is commonly found in plant material, whereas cats simply ingest it from other animals.",null,3,cdkehqp,1r7cxp,askscience,top_week,7
arumbar,"There is a 'minimum' lung volume - this is known as the [residual volume](http://www.admit-online.info/fileadmin/materials/images/cd_rom/1221_lung_volume.gif).  This is the volume of air that remains in the lungs after maximal expiration.  You can see some of the other typically referenced volumes in that figure there - the narrow band in the middle (tidal volume) represents air movement during regular breathing, while the peak (inspiratory reserve) represents how much additional air intake occurs during maximal inspiration.  Similarly, expiratory reserve tells you how much additional air you can blow out to reach residual volume.  These values are often clinically very significant in describing pathologies of various lung diseases (for example, someone with restrictive lung disease such as pulmonary fibrosis typically has reduced volumes due to inability to fully expand the lungs, while someone with obstructive disease like COPD/asthma may have larger volumes due to air trapping).  We also use [spirometery](http://en.wikipedia.org/wiki/Spirometry) to further characterize how air moves in and out of lungs.

Outside of intentionally breathing out to collapse a lung, this can also happen with a pneumothorax, where the pressure gradient normally created by the pleural space is lost (eg due to a puncture wound to the chest).  This can result in collapse of the [lung on one side of the chest](http://www.daviddarling.info/images/pneumothorax_radiograph.gif) - note the loss of pulmonary markings on the right side of the chest (left side of the picture).",null,0,cdkejtb,1r77hw,askscience,top_week,7
Astromike23,"First off, MAVEN does *not* have the ability to detect methane. There were plans to include an instrument that could do so, but it was removed due to budget cuts. Such an  instrument will likely be included on the 2016 ""Trace Gas Orbiter"", though.

With that said, it's still an important measurement to make with a spacecraft. Over the past few years, we think we may have detected methane on Mars using Earth-based telescopes - but this is a very hot topic, and very controversial. There are quite a lot of people in the planetary science community that are not convinced by the evidence, and yet also quite a lot who have based their research on it being true.

The biggest problem here is that it's an exceptionally difficult measurement to make from Earth, because our own atmosphere's methane gets in the way. When looking at a spectrum, it's hard to tell exactly what's coming from our planet, and what's coming from Mars. By measuring this from outside Earth's atmosphere, we can instantly remove that source of confusion from the data. 

The current data taken from Earth is pretty ragged and down in the noise. It's equally suspicious that the methane seems to come and go in both location and time, with only some groups detecting it, and others not able to do so (the Curiosity rover, so far, has not). Maybe it's an exciting dynamic process that suggests Mars is not a dead planet...or maybe it's just imprecise measurements. 

Either way, it's an important enough result that it bears checking with greater precision by a spectrometer outside the Earth's atmosphere. The Hubble would have a difficult time doing this - the easiest place to measure methane is in the mid-infrared (methane has strong spectral lines at 3.3 and 7.8 micron wavelengths), which is outside the range of Hubble's spectrographs.

**TL;DR**: MAVEN cannot sense methane due to budget cuts, which is a shame. From Earth, methane is very difficult to detect on Mars because our own methane gets in the way of the observation.",null,3,cdkdp3s,1r75ya,askscience,top_week,6
adamhstevens,"MAVEN isn't going to measure methane at all. Its entire science goal is to measure water loss in the upper atmosphere. I'm pretty sure it couldn't measure methane if it tried. However, the recently launched [Indian mission](http://en.wikipedia.org/wiki/Mars_Orbiter_Mission) and the European [Trace Gas Orbiter](http://en.wikipedia.org/wiki/ExoMars_Trace_Gas_Orbiter) both have instruments essentially devoted to detecting methane.

As to your actual question, we can do it from Earth and it has been done, and actually terrestrial results are the most convincing. (e.g. [Mike Mumma's results](http://images.spaceref.com/news/2009/Mumma_et_al_Methane_Mars_wSOM_accepted2.pdf)). However, some people (quite rightly) question terrestrial spectra. This is because there's an awful lot of methane in the Earth's atmosphere (roughly a part per million - ppm) compared to Mars' atmosphere (if there is any - it's around a part per billion - ppb) so you have to 'correct' for the terrestrial atmosphere. This can be done, but only to a certain level of accuracy (since the atmosphere gets stirred up a lot), which turns out to be about accuracy you need to be able to detect methane on Mars at all. However, there's a bit of leeway given by the fact that spectra taken from Earth of Mars are either slightly red-shifted or blue-shifted depending on the planets' relative motion. This means you should be able to distinguish martian methane by a slight shift in the methane absorbance. Unfortunately there are other issues that mean this isn't quite as straightforward, either.

So basically this all means that we need to send instruments to Mars so they can image the atmosphere without interference. Curiosity's in-situ experiments set the level of methane as having to be very low (they didn't detect any, so it must either not have been there or be there at a level below the instrument's precision) but that was only measured at one place on the planet at one time, and the measurements we do have suggest that any methane is highly variable in the atmosphere, so really we need long term observations on a planet-wide basis, which is what orbital spectroscopy allows us to do!

Anyway, this is actually what I'm doing my PhD on so if you have any more questions, feel free to ask.",null,1,cdkdppm,1r75ya,askscience,top_week,6
neha_is_sitting_down,"No idea, but if we haven't found it yet, odds are that we can't detect it from here using regular techniques (maybe it is very scarce. Or maybe we don't have the right conditions to use spectrography) and so the only way to be sure is to go there and measure directly.

Just thinking about this, the only way to do spectrography at that distance would be to have a light source behind the planet (correct me if I'm wrong about this). Because mars is outside our orbit, it will never be between us and the sun, so we will never have that light source.",null,4,cdkcp54,1r75ya,askscience,top_week,2
Astromike23,"First off, MAVEN does *not* have the ability to detect methane. There were plans to include an instrument that could do so, but it was removed due to budget cuts. Such an  instrument will likely be included on the 2016 ""Trace Gas Orbiter"", though.

With that said, it's still an important measurement to make with a spacecraft. Over the past few years, we think we may have detected methane on Mars using Earth-based telescopes - but this is a very hot topic, and very controversial. There are quite a lot of people in the planetary science community that are not convinced by the evidence, and yet also quite a lot who have based their research on it being true.

The biggest problem here is that it's an exceptionally difficult measurement to make from Earth, because our own atmosphere's methane gets in the way. When looking at a spectrum, it's hard to tell exactly what's coming from our planet, and what's coming from Mars. By measuring this from outside Earth's atmosphere, we can instantly remove that source of confusion from the data. 

The current data taken from Earth is pretty ragged and down in the noise. It's equally suspicious that the methane seems to come and go in both location and time, with only some groups detecting it, and others not able to do so (the Curiosity rover, so far, has not). Maybe it's an exciting dynamic process that suggests Mars is not a dead planet...or maybe it's just imprecise measurements. 

Either way, it's an important enough result that it bears checking with greater precision by a spectrometer outside the Earth's atmosphere. The Hubble would have a difficult time doing this - the easiest place to measure methane is in the mid-infrared (methane has strong spectral lines at 3.3 and 7.8 micron wavelengths), which is outside the range of Hubble's spectrographs.

**TL;DR**: MAVEN cannot sense methane due to budget cuts, which is a shame. From Earth, methane is very difficult to detect on Mars because our own methane gets in the way of the observation.",null,3,cdkdp3s,1r75ya,askscience,top_week,6
adamhstevens,"MAVEN isn't going to measure methane at all. Its entire science goal is to measure water loss in the upper atmosphere. I'm pretty sure it couldn't measure methane if it tried. However, the recently launched [Indian mission](http://en.wikipedia.org/wiki/Mars_Orbiter_Mission) and the European [Trace Gas Orbiter](http://en.wikipedia.org/wiki/ExoMars_Trace_Gas_Orbiter) both have instruments essentially devoted to detecting methane.

As to your actual question, we can do it from Earth and it has been done, and actually terrestrial results are the most convincing. (e.g. [Mike Mumma's results](http://images.spaceref.com/news/2009/Mumma_et_al_Methane_Mars_wSOM_accepted2.pdf)). However, some people (quite rightly) question terrestrial spectra. This is because there's an awful lot of methane in the Earth's atmosphere (roughly a part per million - ppm) compared to Mars' atmosphere (if there is any - it's around a part per billion - ppb) so you have to 'correct' for the terrestrial atmosphere. This can be done, but only to a certain level of accuracy (since the atmosphere gets stirred up a lot), which turns out to be about accuracy you need to be able to detect methane on Mars at all. However, there's a bit of leeway given by the fact that spectra taken from Earth of Mars are either slightly red-shifted or blue-shifted depending on the planets' relative motion. This means you should be able to distinguish martian methane by a slight shift in the methane absorbance. Unfortunately there are other issues that mean this isn't quite as straightforward, either.

So basically this all means that we need to send instruments to Mars so they can image the atmosphere without interference. Curiosity's in-situ experiments set the level of methane as having to be very low (they didn't detect any, so it must either not have been there or be there at a level below the instrument's precision) but that was only measured at one place on the planet at one time, and the measurements we do have suggest that any methane is highly variable in the atmosphere, so really we need long term observations on a planet-wide basis, which is what orbital spectroscopy allows us to do!

Anyway, this is actually what I'm doing my PhD on so if you have any more questions, feel free to ask.",null,1,cdkdppm,1r75ya,askscience,top_week,6
neha_is_sitting_down,"No idea, but if we haven't found it yet, odds are that we can't detect it from here using regular techniques (maybe it is very scarce. Or maybe we don't have the right conditions to use spectrography) and so the only way to be sure is to go there and measure directly.

Just thinking about this, the only way to do spectrography at that distance would be to have a light source behind the planet (correct me if I'm wrong about this). Because mars is outside our orbit, it will never be between us and the sun, so we will never have that light source.",null,4,cdkcp54,1r75ya,askscience,top_week,2
Lillelyse,"Yes it does. The time it takes for signals to reach our brain depends on the length of the axons involved. The speed of an action potential, depending on the nerve, is generally very fast, so delays are in the ms range. 


Taller people, who need longer axons to connect one part of the body to another, will take longer to perform for example a knee jerk reflex as the signals will have to travel a longer distance. When it comes to things like vision and hearing, I don't think there will be much difference between pathway lengths from person to person (unless you have a massive or abnormally small head), so any differences will be miniscule. This does of course not apply to people with diseases that affect neurone ability to propagate action potentials, such as MS. ",null,0,cdketnw,1r7531,askscience,top_week,3
clockwerx,"You may also be interested in learning about blindsight (http://en.wikipedia.org/wiki/Blindsight), which suggests ""comprehension"" of an image takes place very differently from many of the earlier stages; or why we have certain frame rates (http://en.wikipedia.org/wiki/Frame_rate#Background); which imply our brains are stitching together inputs into a seamless experience.

Interestingly, that stitching together of information that our brains do is far from perfect - Optical illusions (http://en.wikipedia.org/wiki/Optical_illusion) demonstrate we've got a few helpful rendering optimisations that work for most, but not all scenarios.

When you think about it, it suggests we live in a slightly laggy simulation of reality just kind of guessing our way through it all. 

It works well enough to avoid being eaten by large animals or falling off cliffs, so after that the pressure is effectively off for our bodies to get any better at this sort of thing.
",null,2,cdkf3gv,1r7531,askscience,top_week,6
batmantis25,"Our brains actively compensate for this ""lag"". Effectively, we generate images before we actually ""see"" them. I've seen sources which suggest a lapse range amongst individuals but I'm having trouble finding that at the moment.
http://www.livescience.com/4950-key-optical-illusions-discovered.html",null,0,cdkhcqq,1r7531,askscience,top_week,3
SimpleBen,"OK, some numbers. 
  
Sense of touch: Light touch  
Finger to brain: 20-30 msec  
Toe to brain: 50-60 msec  
  
Sense of burning pain  
Finger to brain: 1 second  
Toe to brain: 2 seconds  
  
Itch:
Finger to brain: 2 seconds  
Toe to brain: 4 seconds (ever wonder why that mosquito seems to get such a good head start??)  
  
Sense of vision to brain: 30 msec or so  
  
Sense of hearing: Ear to brain
About 10 msec
  
These time lags will vary from person to person, but not that much. Light touch, for example, may have a 5 msec range across people.",null,2,cdkinxn,1r7531,askscience,top_week,4
afcagroo,"In addition to the large sensory lag, there's a small lag that ensures that we are always living in the past.  If you have a CPU that executes instructions at a rate of 600 MHz, that means that it completes one instruction every 1.67 nanoseconds. Which is how much time it takes light to get to you from a computer monitor 1/2 meter away. ",null,0,cdki8c8,1r7531,askscience,top_week,2
Jyesss,"Yes, the action potential must travel from the nerve through the spinal cord to the brain, the brain must process the information, and then send an effector response back. The rate that the signal can travel along the nerve can be increased by increasing the diameter of the nerve. This is important for larger organisms, and a lot of what we know about nervous transmission was learned by studying octopus' nerve. They are huge and can be studied under just a light microscope. Mammals developed another way to expedite nerve transmission via myelination. This helps to insulate the nerve signal and increase rate. There is a theory proposed that posits that athletes with fast response times is due to the body further myelinating the appropriate nerves to convey the action potential faster. ",null,0,cdme9lo,1r7531,askscience,top_week,1
DEATH-OF_RATS,"What others have said is more or less correct. There are a few stipulations, though they're tangential.

The knee jerk reflex only changes with length of the nerves between muscles around the knee and where they they hit the spinal cord. It's a locally generated spinal reflex, so the brain has no part in it (this is a general definition of a reflex - that it's generated in the spinal cord).

In addition to some variance of axon size across people, different sensory inputs have different axon conduction speeds. Proprioception is a very fast sense, where vision is relatively slow. /u/SimpleBen summarized delay times nicely. As /u/batmantis25 pointed out, the brain also compensates for the lag by extrapolating current vision to what we should expect to see next.

The same thing happens in the sensory system with tactile and proprioceptive inputs. The brain predicts what future tactile and proprioceptive input it expects to receive based on the current tactile and proprioceptive inputs in combination with the current motor output.

This sharing of motor output is called ""efference copy"" - the motor cortex sends commands directly to sensory cortex, as well as to muscles. The sensory cortex can use that information to predict what it should next feel, allowing much better control than systems that are purely reactive. This is one problem you run into with rudimentary robotics - they adjust activity to compensate for sensed (past) inputs, instead of having a good generative model of what should come next.

What this all means at a higher level is that, in very trained movements (like an olympic wrestler executing a take-down during a match), your brain knows how to predict what will come next based on current input, because it has a very good model of how the system acts. Here, ""the system"" includes the player's own body, the floor, the opponent... everything that interacts. As a result, the athlete knows quite well what move their opponent is doing once the opponent starts the move, which means they know what the future state of the opponent will be, and can respond to that future instead of the present, which is really the past. A novice wrestler does not yet have a good model built and relies only on reaction time (usually in the 200-400ms range), which is usually not enough to compete against someone with training.

The idea of ""compiled"" actions also comes into play, and are the reason for repetitive drills. Instead of having to process each step in a sequence of moves, a trained person can initiate the sequence and the body automatically completes it; this is the idea behind muscle memory. This is why you don't have to think much about a practiced movement (like tying your shoes).",null,0,cdmjahz,1r7531,askscience,top_week,1
Lillelyse,"Yes it does. The time it takes for signals to reach our brain depends on the length of the axons involved. The speed of an action potential, depending on the nerve, is generally very fast, so delays are in the ms range. 


Taller people, who need longer axons to connect one part of the body to another, will take longer to perform for example a knee jerk reflex as the signals will have to travel a longer distance. When it comes to things like vision and hearing, I don't think there will be much difference between pathway lengths from person to person (unless you have a massive or abnormally small head), so any differences will be miniscule. This does of course not apply to people with diseases that affect neurone ability to propagate action potentials, such as MS. ",null,0,cdketnw,1r7531,askscience,top_week,3
clockwerx,"You may also be interested in learning about blindsight (http://en.wikipedia.org/wiki/Blindsight), which suggests ""comprehension"" of an image takes place very differently from many of the earlier stages; or why we have certain frame rates (http://en.wikipedia.org/wiki/Frame_rate#Background); which imply our brains are stitching together inputs into a seamless experience.

Interestingly, that stitching together of information that our brains do is far from perfect - Optical illusions (http://en.wikipedia.org/wiki/Optical_illusion) demonstrate we've got a few helpful rendering optimisations that work for most, but not all scenarios.

When you think about it, it suggests we live in a slightly laggy simulation of reality just kind of guessing our way through it all. 

It works well enough to avoid being eaten by large animals or falling off cliffs, so after that the pressure is effectively off for our bodies to get any better at this sort of thing.
",null,2,cdkf3gv,1r7531,askscience,top_week,6
batmantis25,"Our brains actively compensate for this ""lag"". Effectively, we generate images before we actually ""see"" them. I've seen sources which suggest a lapse range amongst individuals but I'm having trouble finding that at the moment.
http://www.livescience.com/4950-key-optical-illusions-discovered.html",null,0,cdkhcqq,1r7531,askscience,top_week,3
SimpleBen,"OK, some numbers. 
  
Sense of touch: Light touch  
Finger to brain: 20-30 msec  
Toe to brain: 50-60 msec  
  
Sense of burning pain  
Finger to brain: 1 second  
Toe to brain: 2 seconds  
  
Itch:
Finger to brain: 2 seconds  
Toe to brain: 4 seconds (ever wonder why that mosquito seems to get such a good head start??)  
  
Sense of vision to brain: 30 msec or so  
  
Sense of hearing: Ear to brain
About 10 msec
  
These time lags will vary from person to person, but not that much. Light touch, for example, may have a 5 msec range across people.",null,2,cdkinxn,1r7531,askscience,top_week,4
afcagroo,"In addition to the large sensory lag, there's a small lag that ensures that we are always living in the past.  If you have a CPU that executes instructions at a rate of 600 MHz, that means that it completes one instruction every 1.67 nanoseconds. Which is how much time it takes light to get to you from a computer monitor 1/2 meter away. ",null,0,cdki8c8,1r7531,askscience,top_week,2
Jyesss,"Yes, the action potential must travel from the nerve through the spinal cord to the brain, the brain must process the information, and then send an effector response back. The rate that the signal can travel along the nerve can be increased by increasing the diameter of the nerve. This is important for larger organisms, and a lot of what we know about nervous transmission was learned by studying octopus' nerve. They are huge and can be studied under just a light microscope. Mammals developed another way to expedite nerve transmission via myelination. This helps to insulate the nerve signal and increase rate. There is a theory proposed that posits that athletes with fast response times is due to the body further myelinating the appropriate nerves to convey the action potential faster. ",null,0,cdme9lo,1r7531,askscience,top_week,1
DEATH-OF_RATS,"What others have said is more or less correct. There are a few stipulations, though they're tangential.

The knee jerk reflex only changes with length of the nerves between muscles around the knee and where they they hit the spinal cord. It's a locally generated spinal reflex, so the brain has no part in it (this is a general definition of a reflex - that it's generated in the spinal cord).

In addition to some variance of axon size across people, different sensory inputs have different axon conduction speeds. Proprioception is a very fast sense, where vision is relatively slow. /u/SimpleBen summarized delay times nicely. As /u/batmantis25 pointed out, the brain also compensates for the lag by extrapolating current vision to what we should expect to see next.

The same thing happens in the sensory system with tactile and proprioceptive inputs. The brain predicts what future tactile and proprioceptive input it expects to receive based on the current tactile and proprioceptive inputs in combination with the current motor output.

This sharing of motor output is called ""efference copy"" - the motor cortex sends commands directly to sensory cortex, as well as to muscles. The sensory cortex can use that information to predict what it should next feel, allowing much better control than systems that are purely reactive. This is one problem you run into with rudimentary robotics - they adjust activity to compensate for sensed (past) inputs, instead of having a good generative model of what should come next.

What this all means at a higher level is that, in very trained movements (like an olympic wrestler executing a take-down during a match), your brain knows how to predict what will come next based on current input, because it has a very good model of how the system acts. Here, ""the system"" includes the player's own body, the floor, the opponent... everything that interacts. As a result, the athlete knows quite well what move their opponent is doing once the opponent starts the move, which means they know what the future state of the opponent will be, and can respond to that future instead of the present, which is really the past. A novice wrestler does not yet have a good model built and relies only on reaction time (usually in the 200-400ms range), which is usually not enough to compete against someone with training.

The idea of ""compiled"" actions also comes into play, and are the reason for repetitive drills. Instead of having to process each step in a sequence of moves, a trained person can initiate the sequence and the body automatically completes it; this is the idea behind muscle memory. This is why you don't have to think much about a practiced movement (like tying your shoes).",null,0,cdmjahz,1r7531,askscience,top_week,1
Platypuskeeper,"There's a closed-form solution for a Dirac potential, with a bound state. (and for the double-Dirac potential as well)

I don't really see the connection. Our knowing about a closed form solution doesn't make those stable, bound states any more special than the stable, bound states of potentials that can't be solved analytically. Bertrand's theorem is inherently linked to trajectories - which quantum particles don't have (or at least not in that sense), and conservation of angular momentum - but the ground state solution to the Schrdinger equation for a central potential is always l=0. 

",null,2,cdk9tp8,1r74bj,askscience,top_week,4
proule,"Animals like cats raise their fur with the help of arrector pili muscles in each hair follicle. This serves a myriad of purposes, from thermal insulation (less movement of air close to the animal's skin) to intimidation (hair stands up, animal looks bigger).

Humans actually have this arrector pili muscle, but it's strongly reduced in function. Humans do actually raise their hair, but the main thing you'd notice is goosebumps. You'll get goosebumps in response to cold (again, to reduce air flow around the skin), but since our hair isn't very thick in most places, this largely isn't effective anymore.

The arrector pili muscle in humans is thus largely vestigial (a remnant of a structure that served a purpose that isn't necessary anymore). 

This weakness in our arrector pili means that the hair on your head just largely will not move, even with these muscles engaged. They are, however, still present in the scalp, and if you get goosebumps you can feel the action of your arrector pili on your scalp. You know the sensation, it's just not as readily obvious as when a cat's hair stands up.",null,2,cdkmc4v,1r737x,askscience,top_week,5
iorgfeflkd,"The quantity that is often cited as being the best measured is the magnetic moment of an electron (related to how an electron responds to magnetic fields). It is very close to two, and half the deviation from 2 has been measured as 0.00115965218073(28), where the brackets indicate uncertainty on the final digits. This means the magnetic moment has been measured to a part per trillion.

The theoretical value, based on quantum electrodynamics, is .001159652180{85(76)}, where I put curly brackets where it deviates from the experiment (within the uncertainties). This is the most accurate verified prediction in science.",null,3,cdk98m9,1r734s,askscience,top_week,11
Trill-Nye,"A very [recent experiment](http://arxiv.org/pdf/0709.2996.pdf) managed to make a measurement with an accuracy approaching the Heisenberg uncertainty limit. This means that the uncertainty associated with this measurement is roughly as small as is physically possible, because quantum mechanical considerations preclude precise knowledge of the state of a system at higher accuracies. 

This measurement was of the interference of two photon that were only slightly out of phase with one another.",null,0,cdkifpr,1r734s,askscience,top_week,1
Local_Motion,"Interesting question. Antibiotics can disrupt the normal gut flora. This can be seen with penicillin-based antibiotics and cephalosporins, among others. 

What's more interesting to me is the end result. You are treating one infection, but in the end can cause another ""infection"". These antibiotics, when they disrupt certain gut flora, allow the excessive proliferation of other gut flora, specifically Clostridium difficile. C. diff can overgrow since the competition has been reduced, and with it's toxins, it can produce pseudomembranous colitis and some bad diarrhea. 

So what do you do? Treat it with another antibiotic, namely, metronidazole. Makes the differences in the bacteria structure/virulence and antibiotic mechanism of action an interesting study.

Source: A lowly med student",null,5,cdkgzr3,1r72qt,askscience,top_week,22
null,null,null,22,cdkad4i,1r72qt,askscience,top_week,32
HushaiTheArchite,"One of the proposed functions of the [appendix](http://en.wikipedia.org/wiki/Vermiform_appendix#Maintaining_gut_flora) is preventing this kind of gut flora-pocalypse from having a long term impact. That said, gut bacteria can get very far out of whack and have been thought to contribute to some illnesses of the colon. You're interested, look up stool transplants. They're simultaneously disgusting and a really fascinating example of the interaction between humans and their bacteria.",null,0,cdki6j5,1r72qt,askscience,top_week,4
ModernTarantula,Most digestion does not involve bacteria. So you will still get calories and proteins. But diarrhea is a common complication of antibiotics. Most absorption of antibiotics takes place in the mostly sterile small intestine.  The colon mainly absorbs water and is the locale of bacteria.  A serious compliction is altering the gut flora to have more serious and infectious bacteria (*Clostridia dificile*) ,null,1,cdkc31q,1r72qt,askscience,top_week,4
null,null,moderator,2,cdkjwbi,1r72qt,askscience,top_week,2
codyish,"It can and does.  Not enough to kill it all off but enough to inhibit digestion of some food that rely heavily on gut fauna. AFAIK, this is one reason why it's recommended to not consume dairy while on a antibiotics. ",null,10,cdkabfh,1r72qt,askscience,top_week,10
Local_Motion,"Interesting question. Antibiotics can disrupt the normal gut flora. This can be seen with penicillin-based antibiotics and cephalosporins, among others. 

What's more interesting to me is the end result. You are treating one infection, but in the end can cause another ""infection"". These antibiotics, when they disrupt certain gut flora, allow the excessive proliferation of other gut flora, specifically Clostridium difficile. C. diff can overgrow since the competition has been reduced, and with it's toxins, it can produce pseudomembranous colitis and some bad diarrhea. 

So what do you do? Treat it with another antibiotic, namely, metronidazole. Makes the differences in the bacteria structure/virulence and antibiotic mechanism of action an interesting study.

Source: A lowly med student",null,5,cdkgzr3,1r72qt,askscience,top_week,22
null,null,null,22,cdkad4i,1r72qt,askscience,top_week,32
HushaiTheArchite,"One of the proposed functions of the [appendix](http://en.wikipedia.org/wiki/Vermiform_appendix#Maintaining_gut_flora) is preventing this kind of gut flora-pocalypse from having a long term impact. That said, gut bacteria can get very far out of whack and have been thought to contribute to some illnesses of the colon. You're interested, look up stool transplants. They're simultaneously disgusting and a really fascinating example of the interaction between humans and their bacteria.",null,0,cdki6j5,1r72qt,askscience,top_week,4
ModernTarantula,Most digestion does not involve bacteria. So you will still get calories and proteins. But diarrhea is a common complication of antibiotics. Most absorption of antibiotics takes place in the mostly sterile small intestine.  The colon mainly absorbs water and is the locale of bacteria.  A serious compliction is altering the gut flora to have more serious and infectious bacteria (*Clostridia dificile*) ,null,1,cdkc31q,1r72qt,askscience,top_week,4
null,null,moderator,2,cdkjwbi,1r72qt,askscience,top_week,2
codyish,"It can and does.  Not enough to kill it all off but enough to inhibit digestion of some food that rely heavily on gut fauna. AFAIK, this is one reason why it's recommended to not consume dairy while on a antibiotics. ",null,10,cdkabfh,1r72qt,askscience,top_week,10
tehgreatblade,Why does any body part make cracking noises? Is this a sign of injury or is it benign?,null,0,cdkue60,1r72q9,askscience,top_week,2
Zangetsux20a,"Between the joints in your body are little pockets of synovial fluid. These pockets contain gases like nitrogen and CO2 bubbles.They're there to prevent your bones from rubbing up against each other when your move around. The neck has these joints as well, and the same synovial fluid pockets. When you ""crack"" a joint, you deform the shape of the pocket. This increases the pressure on the bubbles in the synovial fluid, with make a popping sound. ",null,0,cdlxta9,1r72q9,askscience,top_week,1
expertunderachiever,"You can see forever.

What you mean to ask his how finitely of an arc can I resolve at a given distance.  

The definition of 20/20 vision for instance [see http://en.wikipedia.org/wiki/Visual_acuity] means that at 20 ft you can tell two lines that are 1 arc minute [about 1.75mm] apart.  So you could see something 100ft away but it would have to be separated by at least 89mm.  At 1km it would be 290mm, etc and so on.

So given enough brightness you could see two lights from 100,000km away if they were separated by about 30km.
",null,2,cdkfhfg,1r72q2,askscience,top_week,15
RetraRoyale,You'd be able to see to the edge of the universe. All it takes for you to see something is for light to enter your eye. The only reason you don't see that far in some directions is because there are objects in the way.,null,1,cdk9y4b,1r72q2,askscience,top_week,2
erlegreer,"Absent of haze (atmosphere) and other objects, you would be able to see objects at least up to millions of light years away.
source: Normal healthy naked human eyes can see the Andromeda Galaxy, which is about 2.54 million light years from Earth.",null,0,cdka0uz,1r72q2,askscience,top_week,1
I30T,"It depends on what you want to observe.
Technically you can observe as far as an object can give light to. Stars can be observe from light years away. However if you want to observe another person then (remembering from 7 years ago) you can only see about 300 miles.

VSauce has done a video on this.",null,0,cdkdxly,1r72q2,askscience,top_week,1
Dominus_,"Theoretically, to the end of the observable universe (14 Gigaparsec, or 45 billion lightyears), but in reality, your eye cannot see anything that takes up less than 0.07 of your field of vision (average). 

This is equivalent to about 1.2 mm at a meters distance. So at one and a half kilometer, you would just barely be able to see an ~average person. ",null,0,cdkewbt,1r72q2,askscience,top_week,1
temuchan,"The book ""Why we get sick"" by Nesse and Williams discusses this in one of the chapters.  Natural selection selects for genes that increase an organism's ability to pass on their own genes (reproduce and make sure your offspring reproduce), regardless of whether or not the individual desires the trait associated with that gene.  In humans, miscarriage occurs in a fairly high percentage of pregnancies, often without the mother ever knowing she was pregnant (This is why IVF clinics implant fairly large numbers (~7) of fertilized eggs).  Nesse and Williams discuss that myopia (near sightedness) may decrease the percentage of pregnancies that result in miscarriages.  So even though someone with myopia may not want myopia, the trait will be selected for if it means that person will have more offspring than a non-myopic person.  Edit: Also, having myopia today is not as life threatening as it may have been for cavemen.  If you live in New York you are unlikely to be eaten by a tiger regardless of how good your vision is!

Another example of this is Huntington's disease.  ~~The fact that it has not disappeared yet means there is some selective pressure for keeping those genes.~~ (Edit: See Darkaardvark's comment)  Huntington's symptoms tend to begin around age 40.  Since many people reproduce before age 40, the debilitating effects of the disease do not decrease the ability to reproduce and the gene is not selected against.

Edit: More food for thought, if a hypothetical mutation appeared that caused exceptional vision, but also made the person sterile, it would never be passed on, no matter how desirable eagle eyes are.",null,3,cdkcm8o,1r72ls,askscience,top_week,19
Klinefelter,"optometrist here: one of the theories for the increase in myopia is that an increased amount of near-work increases myopia. so for instance, studying in school have been shown to make you more nearsighted. There has also been shown to be a genetic component for refractive error. Since glasses can correct for refractive error, poor uncorrected eye sight does not make you less likely to be able to reproduce so poor eye sight is still prominent in our society. ",null,0,cdkfzcw,1r72ls,askscience,top_week,5
atomfullerene,"Myopia is a disease of modern life, like diabetes, asthma, and obesity.  In premodern societies, it is very rare.  Even in most modern societies, it has vastly increased in prevalence due to environmental condition in the recent past.

It is _not_ the case that selection doesn't select against nearsightedness.  Nearsightedness has historically been a huge fitness disadvantage, and only within the last few generations (since the spread of eyeglasses) has this decreased.  That's not enough time to cause eyesight to degenerate, since when selective pressure is removed from a trait, it tends to remain the same, not degenerate (except over very long timespans)
",null,0,cdkmq7q,1r72ls,askscience,top_week,1
bdlxb3,"I'm willing to bet you start to lose your eyesight around the time you lose the ability to reproduce. Evolutionarily speaking you are a waste of space after you turn 30 or 40. Yes I know there are advantages to having grandparents alive to care for their young, but I don't think Cro-Magnon cared too much about his 401K. People with truly horrible eyesight are able to survive thanks to modern science, just like diabetics, pussies allergic to gluten, and fat-asses with bad thyroids. Still a human with sub-par sight has better sight than other animals. 
",null,5,cdkza88,1r72ls,askscience,top_week,4
temuchan,"The book ""Why we get sick"" by Nesse and Williams discusses this in one of the chapters.  Natural selection selects for genes that increase an organism's ability to pass on their own genes (reproduce and make sure your offspring reproduce), regardless of whether or not the individual desires the trait associated with that gene.  In humans, miscarriage occurs in a fairly high percentage of pregnancies, often without the mother ever knowing she was pregnant (This is why IVF clinics implant fairly large numbers (~7) of fertilized eggs).  Nesse and Williams discuss that myopia (near sightedness) may decrease the percentage of pregnancies that result in miscarriages.  So even though someone with myopia may not want myopia, the trait will be selected for if it means that person will have more offspring than a non-myopic person.  Edit: Also, having myopia today is not as life threatening as it may have been for cavemen.  If you live in New York you are unlikely to be eaten by a tiger regardless of how good your vision is!

Another example of this is Huntington's disease.  ~~The fact that it has not disappeared yet means there is some selective pressure for keeping those genes.~~ (Edit: See Darkaardvark's comment)  Huntington's symptoms tend to begin around age 40.  Since many people reproduce before age 40, the debilitating effects of the disease do not decrease the ability to reproduce and the gene is not selected against.

Edit: More food for thought, if a hypothetical mutation appeared that caused exceptional vision, but also made the person sterile, it would never be passed on, no matter how desirable eagle eyes are.",null,3,cdkcm8o,1r72ls,askscience,top_week,19
Klinefelter,"optometrist here: one of the theories for the increase in myopia is that an increased amount of near-work increases myopia. so for instance, studying in school have been shown to make you more nearsighted. There has also been shown to be a genetic component for refractive error. Since glasses can correct for refractive error, poor uncorrected eye sight does not make you less likely to be able to reproduce so poor eye sight is still prominent in our society. ",null,0,cdkfzcw,1r72ls,askscience,top_week,5
atomfullerene,"Myopia is a disease of modern life, like diabetes, asthma, and obesity.  In premodern societies, it is very rare.  Even in most modern societies, it has vastly increased in prevalence due to environmental condition in the recent past.

It is _not_ the case that selection doesn't select against nearsightedness.  Nearsightedness has historically been a huge fitness disadvantage, and only within the last few generations (since the spread of eyeglasses) has this decreased.  That's not enough time to cause eyesight to degenerate, since when selective pressure is removed from a trait, it tends to remain the same, not degenerate (except over very long timespans)
",null,0,cdkmq7q,1r72ls,askscience,top_week,1
bdlxb3,"I'm willing to bet you start to lose your eyesight around the time you lose the ability to reproduce. Evolutionarily speaking you are a waste of space after you turn 30 or 40. Yes I know there are advantages to having grandparents alive to care for their young, but I don't think Cro-Magnon cared too much about his 401K. People with truly horrible eyesight are able to survive thanks to modern science, just like diabetics, pussies allergic to gluten, and fat-asses with bad thyroids. Still a human with sub-par sight has better sight than other animals. 
",null,5,cdkza88,1r72ls,askscience,top_week,4
PepperJack_delicacy,"To understand prolapses, you'll need to know just a little bit about the anatomy of the pelvis. Basically, all the pelvic organs (bladder, intestines, uterus, vagina, etc.) are held in place by a sheet of muscles and ligaments referred to as the **pelvic floor**. Here is a very short clip that gives you a 3D view of what's going on down there: http://www.youtube.com/watch?v=cWZdRebxsdM
If you take a look, you'll see that it pretty much acts like a ""net"" holding the organs in place and has passageways that let the anus (and in women, the vagina) pass through. 

The immediate cause of a rectal prolapse is a weakening of these muscles and ligaments. **Once the weight bearing down exceeds the strength of the supports, you'll get a prolapse**. What exactly causes the weakness though? The important thing to keep in mind is that we don't know the **exact** causes but there are several conditions that can increase the chances of you getting a prolapse. I'll try to outline some of the biggest factors.

* **Straining**: If you have a long history of straining while pooping because of chronic constipation, you're more likely to wear out the muscles.
* **Old age**: As you get older, your muscles get weaker in general. The same applies to the pelvic floor muscles.
* **Nerve damage**: If the nerves that supply the pelvic floor muscles don't work properly, the muscles will have a harder time supporting the organs.

Again, since we don't know the exact causes of a prolapse, it's a bit hard to say for certain why women experience it more often. The most likely explanation is that women experience pregnancy and child birth, which absolutely do weaken the pelvic muscles. 

If you're worried about getting a prolapse, remember to have enough fiber in your diet and to drink adequate amounts of water. If you notice that you're constipated a lot and having to strain, have it checked out by a doctor. 

Let me know if you want me to clarify anything.

Sources: 
http://www.emedicinehealth.com/rectal_prolapse/page12_em.htm#prevention
http://my.clevelandclinic.org/disorders/rectal_prolapse/hic-rectal-prolapse.aspx",null,0,cdkmnbq,1r72is,askscience,top_week,3
GoThirdParty,"There is a canvas of muscles called the levator ani. Think of it as a basket on the bottom of your pelvis.

The other thing that holds them in is that your guts are on a mesentery. This is basically a membrane that the colon hangs on and is anchored to the posterior intraabdominal space.

EDIT: [Here is a video.](http://www.youtube.com/watch?feature=player_detailpage&amp;v=Qw-97RU2NFs#t=182) The mesentery is the flappy looking thing the colon is on.",null,0,cdkmhiq,1r72is,askscience,top_week,1
henriquetk,"bubbles are a delicate equilibrium of forces. It problaby bursts when other force is applied to it( example the contact between the bubble and other surface).The bubble can also burst when it's surface dehydrate and can't no longer sustain it self.
I'm not sure, any scientist can correct me.
Sorry for the poor english, I hope I've helped you. ",null,0,cdkdwvm,1r7118,askscience,top_week,2
Trill-Nye,"Bubbles are usually characterized by a force balance between the pressure of the gas inside the bubble and the surface tension of the material comprising the bubble film. The pressure pushes outwards on this surface, which would make the bubble expand. However, the surface itself resists this expansion, because it would require stretching of the film. This is similar to a balloon, the more air you blow into it (increasing the internal pressure) the more the surface stretches. 

Anything that affects the mechanical properties of the bubble surface, such as heat or contact with another surface, can cause the bubble to pop. The real source of the popping is usually either a decrease in the strength of the surface material or an event that causes the escape of the gas inside, similar to the popping of a balloon.",null,0,cdki6x8,1r7118,askscience,top_week,2
blakemerkes,"Not too sure, and not my area of expertise. But I do know that many cows are being fed Corn in farms to fatten them up. And one of the side effects is the buildup of pathogens in their alimentary canal. But I believe that is because corn contains much less fibre than grass.",null,3,cdkagdg,1r70u0,askscience,top_week,2
iorgfeflkd,"The speed of sound in any real material can't exceed the speed of light.

The speed of sound in the early universe, confirmed through observations of the large scale structure of universe, is believed to be the speed of light divided by the square root of three. (1.7x10^8 m/s).",null,0,cdk8flo,1r70oa,askscience,top_week,12
Daegs,"iorgfeflkd is of course right, but the question itself shows a misunderstanding of relativity.

The speed of light is not simply a ""speed"", it is a property of our universe that literally changes reality around us to remain true. Both space and time change (length contraction / time dilation) in order to ensure the speed of light remains both constant and unattainable for anything to travel faster.

",null,0,cdku9h9,1r70oa,askscience,top_week,1
Das_Mime,"There's no evidence that the fields or radio emissions from alternating current have any negative health effects, attempts to detect things like claimed [electromagnetic sensitivity syndrome](http://en.wikipedia.org/wiki/Electromagnetic_hypersensitivity) (not a recognized medical condition) have all turned up the result that there is no detectable effect on people.

Any conductor will stop a radio wave. Some electronics are shielded inside what are known as [Faraday cages](http://en.wikipedia.org/wiki/Faraday_cage), which is basically where you surround it with a mesh of conducting wire. This is commonly used near radio telescopes, where the radio waves from electronics might otherwise interfere with the telescope.",null,0,cdka5rl,1r6ymz,askscience,top_week,6
Mazetron,"Electromagnetic radiation can harm you, it can be harmless.  For example: 
-UV radiation in sunlight causes sunburn
-gamma radiation from space can cause cancer
-X-rays can cause problems if the dose is too high

But visible light, radio waves, and lower energy waves are harmless.

Also, electromagnetic fields (emf) are completely harmless.

And both DC currents and AC currents produce magnetic fields, the difference is an AC current will have a changing field while a DC current will have an unchanging field

Things stop electromagnetic radiation all the time: whenever light hits an opaque object.  For blocking high energy radiation that typically passes through objects that visible light cannot, dense materials like lead are used.  Lead pads can block x rays and gamma rays.

Electric blankets and power lines are harmless.

EDIT: Alright, technically you could kill someone with a magnet or burn their face off with visible light.  You can also die from drinking too much water.  The dosage of visible light and magnetism you find in everyday objects is way too small to harm you.",null,1,cdk7wsr,1r6ymz,askscience,top_week,6
ohsohigh,"Electromagnetic radiation can be dangerous. If it is a high enough frequency it can ionize molecules in your body which can adversely affect DNA and cause cancer. This requires high frequency radiation in the UV or X-ray region of the spectrum. Lower frequency radiation can transfer heat to your body, which can be bad for you if you get a sufficiently powerful dose all at once; this is how a microwave oven works. AC running through power lines isn't going to do either of these things. It is not going to produce high frequency ionizing radiation and if it was putting out radiation at sufficient power to burn you it would render the transmission of electricity incredibly inefficient not to mention the fact that that would be impossible for anyone to miss as you would be able to feel the heat. I am not aware of any other mechanism by which electromagnetic radiation can have detrimental effects on human health.",null,0,cdk7y5q,1r6ymz,askscience,top_week,1
LegateDamar,"For your example, no. Increasing temperature will not reduce the pressure needed to form diamond. Here is a phase diagram for carbon. Notice that diamond will only be formed at extremely high pressures and increasing temperature will just melt or sublimate the graphite. In fact, higher temperature increase the pressure needed to form diamond. 

http://www2.chemistry.msu.edu/courses/cem152/snl_cem152_SS12/_images/carbonphase.png

In the general case however, yes there are minerals whose crystal structures can be changed through heat. An example would be quartz. Here is a phase diagram for quartz. 

http://www.geo.arizona.edu/xtal/geos306/silica_phase_diagram_large.gif",null,1,cdkajxr,1r6xvg,askscience,top_week,3
EdwardDeathBlack,"Here is my understanding. Right right after the big bang, the universe was overall an electrically conductive material. EM waves can not propagate very far in a conductive material, so there is no remaining radiation from that immediate time. It couldn't ""propagate""

A little while later, matter starts forming hydrogen, and the universe becomes transparent, allowing radiation to propagate. We can see the radiation that existed at the point of time in the universe by looking ""far enough"". It is known as the [cosmic microwave background](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation) . 

My understanding is the universe was [~400,000 years old](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation#Features) when it happened. So out of the 13.7980.037 billion years of the universe, we can pretty much see all the way up to 13.798 10^9 - 400,000 ~ 13.797 10^9 years...so pretty far back. ",null,4,cdk7pk0,1r6xdz,askscience,top_week,11
MaskedEngineer,"Not only is it possible, it's easy. Or used to be. If you turn on an old-school analog TV, connect an antenna, and tune to an unused channel, much of the static you see is from the [Cosmic Microwave Background radiation](http://en.wikipedia.org/wiki/Cosmic_background_radiation). These are photons that are red-shifted so far that they've become radio signals. They're essentially coming from the edge of the discernable universe.",null,1,cdkl46l,1r6xdz,askscience,top_week,1
Blacklightzero,"As said before, we can see nearly as far back as the big bang.  They are working on a map of the universe using the Hubble right now.

And, if you look at the map so far, you can see that we are right smack dab in the center of the universe.  We can see equally distant in all directions all the way back to when things started emitting light.  And also that the oldest objects we can see look just like the ones that we can see right next door.  Curious, isn' it?",null,1,cdklhth,1r6xdz,askscience,top_week,1
EdwardDeathBlack,"Here is my understanding. Right right after the big bang, the universe was overall an electrically conductive material. EM waves can not propagate very far in a conductive material, so there is no remaining radiation from that immediate time. It couldn't ""propagate""

A little while later, matter starts forming hydrogen, and the universe becomes transparent, allowing radiation to propagate. We can see the radiation that existed at the point of time in the universe by looking ""far enough"". It is known as the [cosmic microwave background](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation) . 

My understanding is the universe was [~400,000 years old](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation#Features) when it happened. So out of the 13.7980.037 billion years of the universe, we can pretty much see all the way up to 13.798 10^9 - 400,000 ~ 13.797 10^9 years...so pretty far back. ",null,4,cdk7pk0,1r6xdz,askscience,top_week,11
MaskedEngineer,"Not only is it possible, it's easy. Or used to be. If you turn on an old-school analog TV, connect an antenna, and tune to an unused channel, much of the static you see is from the [Cosmic Microwave Background radiation](http://en.wikipedia.org/wiki/Cosmic_background_radiation). These are photons that are red-shifted so far that they've become radio signals. They're essentially coming from the edge of the discernable universe.",null,1,cdkl46l,1r6xdz,askscience,top_week,1
Blacklightzero,"As said before, we can see nearly as far back as the big bang.  They are working on a map of the universe using the Hubble right now.

And, if you look at the map so far, you can see that we are right smack dab in the center of the universe.  We can see equally distant in all directions all the way back to when things started emitting light.  And also that the oldest objects we can see look just like the ones that we can see right next door.  Curious, isn' it?",null,1,cdklhth,1r6xdz,askscience,top_week,1
joshhinz,"Hello. I'm a mechanical engineering student, I noticed you have not had any responses yet, so although this is not strictly my area of expertise, I will share my thoughts on your question with no pretense of absolute certainty...If you were to model an arm or leg for example, as a set of mechanical elements, it might be reasonable to model the bone as a cylindrical beam, and the muscles and tendons as a set of parallel springs and dampers on different sides of the beam. Additionally, it might be reasonably to say that: at rest these springs are likely in a small amount of tension(putting the bones in a small amount of compression). Also, the human body is only able to actuate these springs to either relax them or to put them into greater tension. Therefore my conclusion is this: Yes! muscles and tendons provide strain relief especially in tension because the muscles are able to ""share the load"" in tension and only a minimal amount (if any) in compression because the muscles are not likely to be brought past their relaxed (small tension) state into a state of much compression, the body is simply not able to physically actuate muscles in this way. ",null,0,cdkadb9,1r6vj0,askscience,top_week,3
NightmareOfLagrange,"Interesting question.  Muscles and tendons actually apply stress to the bones they are attached to.  Bone structure can be thought of as a composite material composed of collagen and mineral.  Structure of this composite is what gives bone its mechanical properties, and it's been shown that stresses acting on bone at a stage of incomplete secondary remodeling can affect the resulting structure.  In a way, you could say that yes, the stresses applied to the bone will help it develop into a design better suited to bear those loads, and physiologically other types of tissue do help in impact absorption to relieve stress on the skeletal system.",null,0,cdkldls,1r6vj0,askscience,top_week,1
Lost_Wandering,"The strain to failure will not increase due to the tendons/muscles since it is a property of the material not the biomechanical system. They will however increase the amount if load that can be applied to the system since some of the stress will be transferred to the soft tissue. Increasing the strain to failure would happen if composite composition changed, the collagen content is increased  (and consequently decreasing the brittle hydroxyapatite phase) creating a more viscoelastic material response, but decreasing the ultimate strength.

Source: I do computational material modeling of bone-like biocomposites",null,0,cdl9vnq,1r6vj0,askscience,top_week,1
galinstan,Some iron **is** oxidized to form iron(II) oxide in the slag and fume. Table III in the link provided by the OP mentions that 3.2 % of the charge is lost to slag and fume formation.,null,12,cdk8pey,1r6vha,askscience,top_week,49
SmellyRaghead,"Iron oxide will be destroyed at temperatures that high. The oxygen would be free until it encountered silicon or other elements with high melting point oxides.

When you weld (say) mild steel, you are relying on the fact that the surface coating of oxide melts at a lower temprature than the bulk material.

I can't remember the exact temp but certain iron oxides are quite easy to destroy with heat. ",null,11,cdkafnj,1r6vha,askscience,top_week,31
dmd53,"In short, this occurs because of the relative free energies of the various oxidation reactions at such elevated temperatures. The creation of SiO2 is far more energetically favorable than the creation of, say, Fe2O3, and as such the oxygen will selectively bind with the Si and prevent rust formation. Elements like Si are intentionally added to the steel melt to scavenge oxygen and prevent rust formation (as well as to form ceramic inclusions which increase the strength/hardness of the steel); these elements are known as [deoxidizers](http://en.wikipedia.org/wiki/Deoxidizer).

",null,1,cdkh6c9,1r6vha,askscience,top_week,9
ShoutyCrackers,"At the heat level used (3000F +), most impurities are destroyed. Medieval blacksmiths referred to this as ""crucible steel"" and for a long time the only place to get it was the Middle East until the Vikings found a route to Iran from the Volta. There's a great NOVA (PBS) piece about it called ""Secrets of the Viking Sword"" that tells the whole story. Its on Netflix.",null,16,cdkatrq,1r6vha,askscience,top_week,20
372xpg,"If you look at a chart of the oxidation potentials of these metals and the temperature you can see that most of these metals/elements oxidize more readily than iron especially carbon, the big difference between pig iron and steel. Its a handy property, however some impurities do not like to oxidize this way.

An example of a similar process where iron is oxidized(with air) before another metal is in nickel converting.",null,0,cdkkyn4,1r6vha,askscience,top_week,2
mjwaters,"I think the answer you are looking for is that there is a lot of carbon in the molten iron at this point (http://en.wikipedia.org/wiki/Pig_iron). When they blow the oxygen in, it will preferentially react with carbon forming carbon monoxide and releasing a bunch of heat. With good mixing from the oxygen lance and good exhaust pull from above, the CO is stirred and pulled out. Silicon also reacts with oxygen before iron, but the primary reaction is combustion of carbon. If there was no carbon in the iron, you would see very fast oxidation into iron oxide. Source: I am a materials scientist and I have read this for fun http://www.amazon.com/Making-Shaping-Treating-Steel-Refining/dp/0930767020 ",null,1,cdklt4m,1r6vha,askscience,top_week,2
ImpossiblePossom,"As with all things in chemistry and thermodynamics: The reactions and resulting mixture allow for a lower Gibbs Free Energy (GFE) at the conditions of reaction. 

At room temperature and in the right environment, iron reacts with oxygen to form a few different iron oxides we commonly consider rust. One could assume that this would also occur in the furnace during the production of steel from pig iron.  However iron oxides that make up rust do not end up forming at high levels because other reactions occur instead. These reaction(s) are known as decarburization reactions.  In the case of steel making the exact reactions are O + C -&gt; CO and CO + O -&gt; CO2 (per OPs link). Some iron oxide are formed in the BOS process (FEO). However these are not the same oxides present in rust (Iron III Oxide and Iron III Hydroxide). The FeO that formed is either blown out the top of the furnaces effluent gas stream or forms a separate phase in the form the slag that can be removed.

I think the crux of the question is: Why do some reactions (decarburization) occur preferentially and others do not (Iron Oxide formation). This is where Gibbs Free Energy comes in.  At the conditions in the furnace the decarburization reactions and resulting products require an overall lower Gibbs Free Energy.  Since Gibbs Free Energy is always minimized at equilibrium, the reactions and resulting mixtures that require a lower GFE will occur preferentially. This is why the majority of the carbon in the pig iron reacts to form CO, and a much smaller fraction or carbon forms CO2, and an even smaller fraction of Iron Oxide (FeO) is formed. 

So why doesnt this happen at room temperature too? Well at room temperature Iron-O2-Environment system changes dramatically such that rust formation gives a lower GFE and the Iron Oxides in rust are preferentially formed.

Thanks for the question. It really challenged me to answer it in an accurate, concise, and understandable way.

Source: Im a chemical engineer who specializes in the production process of advanced materials.

Link to wiki on decarburization:
http://en.wikipedia.org/wiki/Decarburization

Link to wiki on BOS Process (IMO its a bit less technical than the link OP posted)
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to the wiki on the Bessemer process (because why not! its a fun read but not really necessary to answer this question):
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to wiki on GFE (in case you need help sleeping tonight):
http://en.wikipedia.org/wiki/Gibbs_free_energy
",null,0,cdkma57,1r6vha,askscience,top_week,1
ramk13,"It's true iron and oxygen can combine to form rust (Fe2O3/FeOOH), but that doesn't mean that the two will fully react in that combination. The practical extent (how much forms) of the reaction depends on the thermodynamics of the reaction, the environmental conditions (temperature, pressure) and the kinetics (speed) of the reaction.

In this case, as a couple other people have said. Rust is less stable at high temperatures (thermodynamic) and the oxygen prefers to react with other compounds first (both thermodynamic and kinetic).",null,0,cdknkde,1r6vha,askscience,top_week,1
NightmareOfLagrange,"Although the original question regarding formation of impurities has already been answered, I'd just like to add that formation of rust specifically is a reaction where iron is oxidized, but rust is not just iron oxide.  Rust is either the hydrated oxide, or a hydroxide where water and oxygen are reduced to form hydroxide ions with electrons from what is assumed to be the iron oxidation.  
",null,1,cdkkvdq,1r6vha,askscience,top_week,1
galinstan,Some iron **is** oxidized to form iron(II) oxide in the slag and fume. Table III in the link provided by the OP mentions that 3.2 % of the charge is lost to slag and fume formation.,null,12,cdk8pey,1r6vha,askscience,top_week,49
SmellyRaghead,"Iron oxide will be destroyed at temperatures that high. The oxygen would be free until it encountered silicon or other elements with high melting point oxides.

When you weld (say) mild steel, you are relying on the fact that the surface coating of oxide melts at a lower temprature than the bulk material.

I can't remember the exact temp but certain iron oxides are quite easy to destroy with heat. ",null,11,cdkafnj,1r6vha,askscience,top_week,31
dmd53,"In short, this occurs because of the relative free energies of the various oxidation reactions at such elevated temperatures. The creation of SiO2 is far more energetically favorable than the creation of, say, Fe2O3, and as such the oxygen will selectively bind with the Si and prevent rust formation. Elements like Si are intentionally added to the steel melt to scavenge oxygen and prevent rust formation (as well as to form ceramic inclusions which increase the strength/hardness of the steel); these elements are known as [deoxidizers](http://en.wikipedia.org/wiki/Deoxidizer).

",null,1,cdkh6c9,1r6vha,askscience,top_week,9
ShoutyCrackers,"At the heat level used (3000F +), most impurities are destroyed. Medieval blacksmiths referred to this as ""crucible steel"" and for a long time the only place to get it was the Middle East until the Vikings found a route to Iran from the Volta. There's a great NOVA (PBS) piece about it called ""Secrets of the Viking Sword"" that tells the whole story. Its on Netflix.",null,16,cdkatrq,1r6vha,askscience,top_week,20
372xpg,"If you look at a chart of the oxidation potentials of these metals and the temperature you can see that most of these metals/elements oxidize more readily than iron especially carbon, the big difference between pig iron and steel. Its a handy property, however some impurities do not like to oxidize this way.

An example of a similar process where iron is oxidized(with air) before another metal is in nickel converting.",null,0,cdkkyn4,1r6vha,askscience,top_week,2
mjwaters,"I think the answer you are looking for is that there is a lot of carbon in the molten iron at this point (http://en.wikipedia.org/wiki/Pig_iron). When they blow the oxygen in, it will preferentially react with carbon forming carbon monoxide and releasing a bunch of heat. With good mixing from the oxygen lance and good exhaust pull from above, the CO is stirred and pulled out. Silicon also reacts with oxygen before iron, but the primary reaction is combustion of carbon. If there was no carbon in the iron, you would see very fast oxidation into iron oxide. Source: I am a materials scientist and I have read this for fun http://www.amazon.com/Making-Shaping-Treating-Steel-Refining/dp/0930767020 ",null,1,cdklt4m,1r6vha,askscience,top_week,2
ImpossiblePossom,"As with all things in chemistry and thermodynamics: The reactions and resulting mixture allow for a lower Gibbs Free Energy (GFE) at the conditions of reaction. 

At room temperature and in the right environment, iron reacts with oxygen to form a few different iron oxides we commonly consider rust. One could assume that this would also occur in the furnace during the production of steel from pig iron.  However iron oxides that make up rust do not end up forming at high levels because other reactions occur instead. These reaction(s) are known as decarburization reactions.  In the case of steel making the exact reactions are O + C -&gt; CO and CO + O -&gt; CO2 (per OPs link). Some iron oxide are formed in the BOS process (FEO). However these are not the same oxides present in rust (Iron III Oxide and Iron III Hydroxide). The FeO that formed is either blown out the top of the furnaces effluent gas stream or forms a separate phase in the form the slag that can be removed.

I think the crux of the question is: Why do some reactions (decarburization) occur preferentially and others do not (Iron Oxide formation). This is where Gibbs Free Energy comes in.  At the conditions in the furnace the decarburization reactions and resulting products require an overall lower Gibbs Free Energy.  Since Gibbs Free Energy is always minimized at equilibrium, the reactions and resulting mixtures that require a lower GFE will occur preferentially. This is why the majority of the carbon in the pig iron reacts to form CO, and a much smaller fraction or carbon forms CO2, and an even smaller fraction of Iron Oxide (FeO) is formed. 

So why doesnt this happen at room temperature too? Well at room temperature Iron-O2-Environment system changes dramatically such that rust formation gives a lower GFE and the Iron Oxides in rust are preferentially formed.

Thanks for the question. It really challenged me to answer it in an accurate, concise, and understandable way.

Source: Im a chemical engineer who specializes in the production process of advanced materials.

Link to wiki on decarburization:
http://en.wikipedia.org/wiki/Decarburization

Link to wiki on BOS Process (IMO its a bit less technical than the link OP posted)
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to the wiki on the Bessemer process (because why not! its a fun read but not really necessary to answer this question):
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to wiki on GFE (in case you need help sleeping tonight):
http://en.wikipedia.org/wiki/Gibbs_free_energy
",null,0,cdkma57,1r6vha,askscience,top_week,1
ramk13,"It's true iron and oxygen can combine to form rust (Fe2O3/FeOOH), but that doesn't mean that the two will fully react in that combination. The practical extent (how much forms) of the reaction depends on the thermodynamics of the reaction, the environmental conditions (temperature, pressure) and the kinetics (speed) of the reaction.

In this case, as a couple other people have said. Rust is less stable at high temperatures (thermodynamic) and the oxygen prefers to react with other compounds first (both thermodynamic and kinetic).",null,0,cdknkde,1r6vha,askscience,top_week,1
NightmareOfLagrange,"Although the original question regarding formation of impurities has already been answered, I'd just like to add that formation of rust specifically is a reaction where iron is oxidized, but rust is not just iron oxide.  Rust is either the hydrated oxide, or a hydroxide where water and oxygen are reduced to form hydroxide ions with electrons from what is assumed to be the iron oxidation.  
",null,1,cdkkvdq,1r6vha,askscience,top_week,1
OrbitalPete,"There are several problems with forecasting reversals. Firstly, they do not occur at regular intervals. 

I was going to post a big explanation with nice figures, but in searching for nice figures I found this, which does the job brilliantly. http://all-geo.org/highlyallochthonous/2009/02/is-the-earths-magnetic-field-about-to-flip/

The long and short is we can go for millions of years without reversals, and a reversal is not an instantaneous thing but can take decades to millenia to occur.  

The causes are due to changes in convection direction in the outer core - convecting a fluid around a rotating sphere leads to instabilities. Then the bulk fluid direction changes, so does the field polarity.

It's worth noting that earth's field is currently not a true dipole either, there are significant positive and negative magnetic anomalies (particularly one over the South Atlantic), and if you look at the magnetic field at depth we are currently in a fairly multipolar condition, with several North and South magnetic poles. This may or may not be an indication that the poles are in the process of reversing. ",null,0,cdkd57i,1r6vce,askscience,top_week,3
McBented,"Some background: 

1) Reversals do not occur on regular intervals. It could happen after 100k years, and then not happen for a million years after that. 

2) Sometimes, the dynamo also tries to reverse, but end up the same polarity after the ""reversal"". These are called ""excursions"".

3) The reversal process itself usually takes about ~20,000 years, so it's very gradual to the time-scale we live in.

There are some people in the field who believe that we are at the beginning of a reversal right now. In my opinion it's pretty much impossible to know what the dynamo is trying to do until we're well into the reversal, so I find it pointless to speculate.

Dynamo reversals occur due to nonlinear behavior of the convecting fluids in Earth's iron-core, and its interactions with the magnetic field it generates. We can now simulate these fluids by solving the appropriate equations with a computer. In our models, we do get spontaneous reversals for geodynamo models. Since these equations are non-linear, they are not predictive for very far into the future (just like how weather forecast models are only good for the next few hours/days or so).

Interestingly, we can also model a cyclic dynamo with predictable reversals with the right parameters, like the Sun's dynamo.",null,0,cdkjlp1,1r6vce,askscience,top_week,1
UncertainHeisenberg,"[This USGS FAQ](http://www.usgs.gov/faq/?q=categories/9830/3353) does a good job of explaining why larger ruptures can't be prevented using smaller, harmless, ""controlled"" earthquakes. 

The main reason is that you need thousands of smaller quakes to release the equivalent energy. For example, 32000 M3 earthquakes release equivalent energy to one M6 event. This scales to around *one million* M3 events if you want the equivalent energy of an M7 earthquake!

Secondly, how do you guarantee that the earthquakes you trigger will be minor enough to cause no damage? Now guarantee that for every one of the many thousands of quakes you will need to trigger.",null,83,cdkcb4a,1r6u2f,askscience,top_week,448
null,null,null,2,cdkaw9q,1r6u2f,askscience,top_week,28
MrsWerf,"Earthquakes also don't universally decrease stress in the surrounding area. Seismologists are now able to model the changes in stress ([Coulomb failure stress - the bottom of the page](http://www.geology.um.maine.edu/geodynamics/AnalogWebsite/UndergradProjects2010/PeterStrand/html/Introduction.html)) and show that some areas, particularly the tips of the ruptured fault, are areas with increased stress after an earthquake. ",null,4,cdkczko,1r6u2f,askscience,top_week,26
Anomander82,It's not really viable method of reducing stress at tectonic boundaries if one simply considers the scale of energy involved. The energy dispersed from the well during hydraulic fracturing is typically confined to a radius of a few metres to a few tens of metres from the wellbore. The commonly cited incident in the UK in 2011 is really an isolated occurrence. The amount of energy required to relieve large-scale tectonic stress is orders of magnitude greater than what would typically be applied to an unconventional hydrocarbon play. Also consider that there are numerous risks associated with the drilling process in a tectonic province with a high level of activity. Overall it's simply not feasible.,null,0,cdkgr5m,1r6u2f,askscience,top_week,5
ripitupandstartagain,"You can't state that hydraulic fracturing only creates small earthquakes, just that it seems to be responsible for increased seismic activity and that the events attributed to this cause have so far been small. 

The frequency of different magnitudes of earthquakes in a set area obeys the principles of chaos theory (ie the frequency of the event is inversely proportional to the energy released). There will be a stress point reached that will trigger a quake but the size of the quake produced is pretty random based on the odds of a certain amount of energy being released. For example a magnitude 6 earthquake releases approximately 32 times the energy of a magnitude 5 so over a set area you would expect magnitude 5 earthquake to be about 32 times more frequent than a magnitude 6 quake. 

Rather than causing small quakes, fracing can be said to be loading an area with stress past its trigger point. The earthquakes recorded at hydraulic fracturing sites tend to be on the order of about 3. So one could reasonably expect a magnitude 6 quake to occur within the time taken to create 16000 magnitude 3 quakes (32x32x32/2 for average). 
Now if a mag 3 quake is happening once a week (which seems to be very unlikely) that would mean a mag 6 would be expected after around 2300 years (obviously it could happen at any tipping point but the cumulative odd suggest about then) or about 70 years for a mag 5.

This is based on fracing being the major cause of the earthquakes which would be the case for the majority of sites as they are away from active faults (such as with the Blackpool quakes which halted UK fracing the other year). ",null,1,cdkhvgu,1r6u2f,askscience,top_week,5
Male_Rikku,"This idea is actually similar to a concept in plasma physics whereby you steadily bleed off free energy at the plasma-wall boundary by inducing magnetic oscillations. It's used to keep large events that could damage the wall from happening. In *practice* we may not have a good way to do this with geophysical problems, but in *principle* a way could exist if we could model everything well enough and had a lot of control over the ""shape"" of our perturbation.",null,3,cdkgjjj,1r6u2f,askscience,top_week,5
classycactus,"Also, induced seismicity is more closely linked to fluid injection. Particularly salt water/waste water disposal wells (class II), the induced earthquake that are being the cause of controversy are those causing earthquakes in parts of the crust that are fairly tectonically dead (like the mid west US). Also the earthquakes that cause real damage(~magnitude 6.5+) are much much much larger then anything you might get from fluid injection(IIRC the usually the earthquakes are from ~2.0-4.5, which is a fraction of 6.5, as the Richter scale is a log scale)",null,0,cdkkjej,1r6u2f,askscience,top_week,2
astazangasta,"At last, a good use for pie charts!

[Here](http://feww.files.wordpress.com/2009/07/total-seismic-moment-released-by-earthquakes-1906-2005.jpg?w=420&amp;h=361) is a graph showing total seismic release divided by magnitude of events. As you can see, a single event like the SF 1906 earthquake is a small fraction of the total, even though it had a huge impact. Meanwhile, there are single events that dominate this chart, implying it would take MANY SF-scale events to relieve the stress of one of the big movements. [Here](http://scienceblogs.com/greengabbro/wp-content/blogs.dir/265/files/2012/04/i-51c95bfa0685b39b350ab449ef2bd73e-2004-2005-seismic-energy.png) is another image of 2004-2005 events, showing how total energy release from a single event is much larger than many many small-scale events put together.",null,0,cdkkbue,1r6u2f,askscience,top_week,1
UncertainHeisenberg,"[This USGS FAQ](http://www.usgs.gov/faq/?q=categories/9830/3353) does a good job of explaining why larger ruptures can't be prevented using smaller, harmless, ""controlled"" earthquakes. 

The main reason is that you need thousands of smaller quakes to release the equivalent energy. For example, 32000 M3 earthquakes release equivalent energy to one M6 event. This scales to around *one million* M3 events if you want the equivalent energy of an M7 earthquake!

Secondly, how do you guarantee that the earthquakes you trigger will be minor enough to cause no damage? Now guarantee that for every one of the many thousands of quakes you will need to trigger.",null,83,cdkcb4a,1r6u2f,askscience,top_week,448
null,null,null,2,cdkaw9q,1r6u2f,askscience,top_week,28
MrsWerf,"Earthquakes also don't universally decrease stress in the surrounding area. Seismologists are now able to model the changes in stress ([Coulomb failure stress - the bottom of the page](http://www.geology.um.maine.edu/geodynamics/AnalogWebsite/UndergradProjects2010/PeterStrand/html/Introduction.html)) and show that some areas, particularly the tips of the ruptured fault, are areas with increased stress after an earthquake. ",null,4,cdkczko,1r6u2f,askscience,top_week,26
Anomander82,It's not really viable method of reducing stress at tectonic boundaries if one simply considers the scale of energy involved. The energy dispersed from the well during hydraulic fracturing is typically confined to a radius of a few metres to a few tens of metres from the wellbore. The commonly cited incident in the UK in 2011 is really an isolated occurrence. The amount of energy required to relieve large-scale tectonic stress is orders of magnitude greater than what would typically be applied to an unconventional hydrocarbon play. Also consider that there are numerous risks associated with the drilling process in a tectonic province with a high level of activity. Overall it's simply not feasible.,null,0,cdkgr5m,1r6u2f,askscience,top_week,5
ripitupandstartagain,"You can't state that hydraulic fracturing only creates small earthquakes, just that it seems to be responsible for increased seismic activity and that the events attributed to this cause have so far been small. 

The frequency of different magnitudes of earthquakes in a set area obeys the principles of chaos theory (ie the frequency of the event is inversely proportional to the energy released). There will be a stress point reached that will trigger a quake but the size of the quake produced is pretty random based on the odds of a certain amount of energy being released. For example a magnitude 6 earthquake releases approximately 32 times the energy of a magnitude 5 so over a set area you would expect magnitude 5 earthquake to be about 32 times more frequent than a magnitude 6 quake. 

Rather than causing small quakes, fracing can be said to be loading an area with stress past its trigger point. The earthquakes recorded at hydraulic fracturing sites tend to be on the order of about 3. So one could reasonably expect a magnitude 6 quake to occur within the time taken to create 16000 magnitude 3 quakes (32x32x32/2 for average). 
Now if a mag 3 quake is happening once a week (which seems to be very unlikely) that would mean a mag 6 would be expected after around 2300 years (obviously it could happen at any tipping point but the cumulative odd suggest about then) or about 70 years for a mag 5.

This is based on fracing being the major cause of the earthquakes which would be the case for the majority of sites as they are away from active faults (such as with the Blackpool quakes which halted UK fracing the other year). ",null,1,cdkhvgu,1r6u2f,askscience,top_week,5
Male_Rikku,"This idea is actually similar to a concept in plasma physics whereby you steadily bleed off free energy at the plasma-wall boundary by inducing magnetic oscillations. It's used to keep large events that could damage the wall from happening. In *practice* we may not have a good way to do this with geophysical problems, but in *principle* a way could exist if we could model everything well enough and had a lot of control over the ""shape"" of our perturbation.",null,3,cdkgjjj,1r6u2f,askscience,top_week,5
classycactus,"Also, induced seismicity is more closely linked to fluid injection. Particularly salt water/waste water disposal wells (class II), the induced earthquake that are being the cause of controversy are those causing earthquakes in parts of the crust that are fairly tectonically dead (like the mid west US). Also the earthquakes that cause real damage(~magnitude 6.5+) are much much much larger then anything you might get from fluid injection(IIRC the usually the earthquakes are from ~2.0-4.5, which is a fraction of 6.5, as the Richter scale is a log scale)",null,0,cdkkjej,1r6u2f,askscience,top_week,2
astazangasta,"At last, a good use for pie charts!

[Here](http://feww.files.wordpress.com/2009/07/total-seismic-moment-released-by-earthquakes-1906-2005.jpg?w=420&amp;h=361) is a graph showing total seismic release divided by magnitude of events. As you can see, a single event like the SF 1906 earthquake is a small fraction of the total, even though it had a huge impact. Meanwhile, there are single events that dominate this chart, implying it would take MANY SF-scale events to relieve the stress of one of the big movements. [Here](http://scienceblogs.com/greengabbro/wp-content/blogs.dir/265/files/2012/04/i-51c95bfa0685b39b350ab449ef2bd73e-2004-2005-seismic-energy.png) is another image of 2004-2005 events, showing how total energy release from a single event is much larger than many many small-scale events put together.",null,0,cdkkbue,1r6u2f,askscience,top_week,1
KarlOskar12,"Well all we can really do is look at what has worked in the past for organisms and what success they have had. As it currently stands the most successful organisms are single celled prokaryotes. They have been around for billions of years and their DNA replication has many errors in it. They mutate very quickly as a result and as a result they have been able to adapt to literally every environment on this planet. They have been around far longer than anyone else, and they will probably be the last surviving organisms on this planet. So you could easily make an argument that a higher rate of error is optimal in a DNA replication system.",null,0,cdk7wao,1r6tl1,askscience,top_week,1
snusmumrikan,"Agh the paper I'm trying to find is eluding me! It's from this month though.

The potential for variation is selected for evolutionarily. Without the potential of random mutation, a species will not be able to evolve and respond to varying or novel selection pressures in their environment (which is always changing, and on an evolutionary timescale the environment can change significantly quite quickly).

If you are 'stuck' without any change in the genome then a species will be at a significant disadvantage as there will be no way for the gene pool to adapt. As such it is actually preferable for the DNA machinery to have a background level of error, otherwise the species would die out. 

If anyone can find and link the article I mean, I think it is Nature this month, something like 'The capacity for variation is selected for...' I would be very grateful. It was also on many normal news sites as a side note article",null,0,cdkl6hf,1r6tl1,askscience,top_week,1
cmuadamson,"It's an interesting question.  I can think of a number of conditions that would have to be met, but none of them rule out the possibility completely. Perhaps someone else can think of an impossible requirement.

*  The planet would have to be slow rotating, so that the synchronous orbital distance is far enough away that the two bodies don't tidally rip each other apart.

* The planet would have to be extremely more massive than the moon, so that their center of mass (the barycenter), about which they both are truly orbiting, is very close to the center of mass of the planet.

* The moon would have to have been captured, not formed by normal gravitational collapse of a gaseous field. Otherwise, the clumps falling together to form the moon would not have had a chance to ""sweep"" through the area of its orbit. Someone may be able to argue down this point.

* The moon must be orbitting directly in the sidereal plane of the planet, i.e. directly above the equator, which is unlikely to occur. If its orbit has any inclination, it is no longer a ""stationary"" orbit, but could be a ""synchronous"" orbit.

* Similarly, the moon's orbit must be circular. Any elongation means the moon's speed changes throughout its orbit, also changing it from a ""stationary"" orbit to a ""synchronous"" orbit. This is also very unlikely to happen coincidentally.
",null,0,cdk7lmj,1r6sqm,askscience,top_week,3
Gargatua13013,"Nope

Spider and scorpion are arachnids (and indeed 8 limbed)

Crabs are crustaceans, together with lobsters and barnacles (and actually 10 limbed)

The closest marine relatives of spiders are  Pycnogonids (see: http://en.wikipedia.org/wiki/Sea_spider). Although pycnogonids are *not* spiders, or even arachnids, they are a somewhat related class.",null,0,cdkl2xw,1r6shx,askscience,top_week,2
thenotoriousFIG,"Yes, they are both Arthropods. http://en.wikipedia.org/wiki/Arthropoda",null,0,cdk76p7,1r6shx,askscience,top_week,1
tigerhobs,"No, human cells do not, but surprisingly your gut microbiota can produce it if a yeast gets into and persists in your intestines.  Your intestines are largely anaerobic, and these yeast will ferment sugars to make alcohol, and you can get drunk.  It doesn't sound like a very pleasant condition, but it should be easily curable with anti-fungal agents.  [I first heard about it on NPR, click me!](http://www.npr.org/blogs/thesalt/2013/09/17/223345977/auto-brewery-syndrome-apparently-you-can-make-beer-in-your-gut) .  The literature does not have much on this relatively rare condition, but it seems to be possible.

To reiterate- your own cells cannot, but your gut, supplemented with alcohol producing yeasts, might.",null,4,cdkb7by,1r6pmw,askscience,top_week,15
null,null,null,6,cdka124,1r6pmw,askscience,top_week,6
tigerhobs,"No, human cells do not, but surprisingly your gut microbiota can produce it if a yeast gets into and persists in your intestines.  Your intestines are largely anaerobic, and these yeast will ferment sugars to make alcohol, and you can get drunk.  It doesn't sound like a very pleasant condition, but it should be easily curable with anti-fungal agents.  [I first heard about it on NPR, click me!](http://www.npr.org/blogs/thesalt/2013/09/17/223345977/auto-brewery-syndrome-apparently-you-can-make-beer-in-your-gut) .  The literature does not have much on this relatively rare condition, but it seems to be possible.

To reiterate- your own cells cannot, but your gut, supplemented with alcohol producing yeasts, might.",null,4,cdkb7by,1r6pmw,askscience,top_week,15
null,null,null,6,cdka124,1r6pmw,askscience,top_week,6
NightmareOfLagrange,"I'm gonna go out on a limb here with this answer, but hopefully my simplifications won't be too offensive:

Soda is mostly water, sugar, and carbonation.  Based on their molecular structures, although water has decent surface tension (and can form bubbles), based on interaction with the rest of the water molecules and mutual attraction, neither of those are very good at holding a structure like a bubble.

Ice cream introduces lipids into the solution of your float, which, based on their structure, are more likely to retain bubbles (like how you can make bubbles with soap but ice cream tastes a lot better).",null,0,cdklmbz,1r6pjl,askscience,top_week,1
thetango,"When a bottle says aged 12 year they mean 12 year in a barrel not the bottle.   Liquor, for example whiskey, picks up flavor from the cask/keg it is barreled in.   The cask/keg can impart very subtle textures and flavors to a whiskey.

Wine, on the other hand, ages differently than liquor.  As wine ages, a chemical called 'tannins' breaks down and the result is a ""smoother"" tasting wine.",null,1,cdki31j,1r6pfu,askscience,top_week,4
Proxymace,"Potentially yes. But any orbit through the solar system undergoes changes from all of the planets gravities. These are likely to have a much greater effect than a solar flare which does not have much impact mass, even if it does contact directly with solar material",null,0,cdmjdr7,1r6p3q,askscience,top_week,2
Drunk-Scientist,"Adding to Proxymace's answer; another thing that can alter a comet or asteroids trajectory is simply the sunlight itself. [Radiation pressure](http://en.wikipedia.org/wiki/Radiation_pressure) from the photons colliding with an object can lead to minute changes in orbits that add up over time. In fact, one suggested technique to shift an asteroid headed for Earth would be to paint one half of it white and let the radiation pressure from the Sun do it's business.

However it should be noted that both this effect and that of Solar Flares is orders of magnitude less than those due to gravity, for example by close interactions with Jupiter. So unless a comet remains in a totally undisturbed orbit close to the Sun for many millennia (an unlikely state of affairs for eccentric comets), the effects of the solar wind and radiation pressure will be negligible.",null,0,cdn1ib2,1r6p3q,askscience,top_week,2
GProteins,"Okay-- so I'm going to assume that you're talking about things like cuts/abrasions/etc. 

The first thing that happens is your blood vessels release a variety of cells, some to eat bacteria, some to help close the hole by a) plugging it and b) secreting chemicals that help plug it in a more stable manner (that thing we call a 'scab'). 

Then, your body rushes to fill the gap with really fibrous tissue-- mostly collagen and some elastic fibers. This is your initial scar. It's red and leaky and not terribly strong, but it's better than nothing. Your body then goes through and slowly replaces that fast-placed fibrous tissue with the normal cells that belong in the area. Eventually, the fibrous tissue gets replaced (sometimes it doesn't entirely go away), but the tissue that's there is a LITTLE smaller than the tissue that was originally in the wound area, which is why scars can feel ""tight"". 

Basically, when you're talking more severe wounds, you're just seeing more fibrous tissue, meaning a longer time for the body to replace it with normal cells. And sometimes when you have such fast cell proliferation (even though it's slow to you, it's fast for your body), the cells ""forget"" things and you get little mutations here or there-- darker or lighter skin sometimes. And you can lose architecture that was there already (hair cells, moles, etc).",null,0,cdn9ep2,1r6o58,askscience,top_week,2
codyish,"I'd be interested to see a source confirming your idea. My understanding is that capsaicin is fat soluble, so milk washes it away more effectively than most drinks, and that olive oil would do an even better job. ",null,0,cdkai3o,1r6lk9,askscience,top_week,2
rupert1920,"The existence of a permanent dipole moment is independent of whether you put it in the microwave or not.

Some functional groups in olive will have dipole moments along their bonds - for example, the C=O double bond - but there are also large extents of the molecule that doesn't have a strong dipole moment - such as the hydrocarbon chain. The end result is that the molecule as a whole has a small dipole moment.

This, combined with the size and weight of the molecule (i.e., high moment of inertia), makes it much less responsive to microwaves than, say, a water molecule.",null,0,cdk6p7b,1r6liv,askscience,top_week,5
walluwe,"No. The oil is made up of several fatty (hydrocarbon) chains connected to carboxylic acids. While the carboxylic acids themselves have a dipole moment, the molecule on a whole is not really affected by that, just due to the very long hydrocarbon chains (the carboxylic acids are so much smaller than the fatty side chains). The water molecules themselves will vibrate, but the fatty acid will not be affected by this radiation. ",null,3,cdk5m1p,1r6liv,askscience,top_week,2
rupert1920,Check out the Wikipedia article on [Roche limit](http://en.wikipedia.org/wiki/Roche_limit).,null,1,cdk78nz,1r6kn2,askscience,top_week,4
Drunk-Scientist,"Interesting question. The above answer on Roche Limits is correct; within a certain radius (governed by how strongly held together each planet is), the gravity from each would rip them both apart. This is why [Saturn has rings](http://www.astro.washington.edu/users/smith/Astro150/Tutorials/Roche/) and why [Comet ISON might not make it through](http://www.asterism.org/tutorials/tut25-1.htm) it's close interaction with the Sun.

Much of the material from this interaction would collide and form a [disc of material](http://news.bbcimg.co.uk/media/images/63566000/jpg/_63566727_r3400571-artwork_showing_the_moon_s_formation-spl.jpg) around the point where the planets came nearest. Some material would also likely be thrown off into the solar system. This disc would then re-accrete back into a (much larger) planet. In fact that outcome is similar to [what happened to Earth 4.5 billion years ago](http://en.wikipedia.org/wiki/Giant_impact_hypothesis) when a Mars-sized body collided with us and the resulting debris disc coallesced into the Moon.

HOWEVER, if you ignore the 'planets' and 'dense atmospheres' part of your question, then such an occurrence can and does happen in the universe. [Contact Binary Stars](http://en.wikipedia.org/wiki/Contact_binary) are stars that orbit so tightly that their atmospheres are connected. Mass and energy are transferred between the stars and eventually the smaller companion may even become swallowed entirely by the larger, but they may last happily together for hundreds of millions of years.

Going back to the question in hand though: What would actually happen if these planets could touch atmospheres without being ripped to shreds (eg, assume they are made of adamantium). Well, as the gases mix some of the molecules would become gravitationally bound by the planet above and cross the gap to the other, effectively. It would also cause huge atmospheric disruption on both planets, possibly with supersonic winds created that might blow around the planet for days.

However, such a scenario would be unlikely to last for more than a couple of orbits: the atmospheric drag on both would reel in their aphelion until, rather than just kissing atmospheres, they were slamming into each other head-first. But I guess if these really are indestructable planets they would just form a strange bow-tie shaped world; a bit like a [Contact Binary Asteroid](http://en.wikipedia.org/wiki/Contact_binary_(asteroid).",null,0,cdn0yzy,1r6kn2,askscience,top_week,1
Hiddencamper,"Nuclear engineer here.

The RBMK is graphite moderated. This means that the cooling medium and the moderation medium are separated. 

Under normal operation, water enters the bottom of the reactor, and flows upward. The water heats up on its way through the core, and the amount of steam voids by volume increases on your way up through the core. 

Steam is drastically less dense than liquid water, and is virtually transparent to neutrons when compared to liquid water. As such, an increase in voids means that my water is absorbing less neutrons. It also means that my neutrons will have a longer mean free path length, and ultimately means more neutrons will be able to get to my moderator. tl;dr, Increase in steam voids = increase in moderation = increase in power.

Inherently this has stability issues. As I increase my heat output, I'm going to increase the amount of voiding I have. This will then increase power, which further increases voiding. Active control systems which adjust control rods can come down to compensate for this. The control rods for this reactor design drop in from the top, which makes sense as the top is where the highest neutron flux is likely to be seen. Active control rod motion suppresses any power excursions and maintains the reactor in a stable operating state.

At high power levels, you are producing a lot of steam flow, and as a result, you have a high flow of water through the reactor. With high flow rates through a reactor, your boiling boundary remains fairly constant, and it takes quite a bit to have a runaway excursion, as the forced flow of water into your reactor tends to push voids out quickly and ensure cooling water gets to where it needs to be. Additionally at high flow high power conditions, the voids do not have a dominiant contribution to reactivity, meaning small changes in voids have small changes in reactivity.

When you are at low power, you are in a situation where you have low flow. Your boiling boundary is higher. Your voids have a much larger contribution to reactivity in the reactor. At low flow conditions with low control rod density conditions in boiling reactors, we observe the boiling boundary is somewhat unstable. Steam voids take longer to get out of the reactor, and they begin to have a stronger impact on reactivity. As such, anything that changes your boiling boundary even a little bit is going to have an amplified effect on your neutorn flux and power output. Little things like random noise in your pump flow can start these oscillations, causing the boiling boundary to move, which starts causing power oscillations. In the RBMK, to respond to the power oscillations, the control rods will start moving in and out to try and stabilize this. Having graphite tipped rods combined with having a control system with a response time constant in seconds (which is similar to the fuel's thermal time constant) just means that the control rods are going to be trying to catch a power change, but will have trouble keping up because it will be causing some of its own problems. 

All of these things together will drive power oscillations in the core. Under a worst case condition it can drive a runaway condition requiring a reactor scram. When operated appropriately, the reactor scram (even with the graphite tips) will have sufficient margin to preclude a steam explosion. When not operated appropriately (with nearly all control rods out), a power excursion can occur which leads to a steam explosion and loss of the unit.

The reason this is an issue at low power is due to the way the boiling boundary behaves at low flow conditions with low control rod density. The voids have too much contribution to the core's reactivity under these conditions, and small changes in voids drive large changes in flux. Because the voids contribute so much to reactivity under these conditions and the possibility of an instability can occur, RBMKs have a safety limit which requires a minimum number of control rods to be inserted at all times, to ensure that the voids do not carry enough reactivity to drive a core damaging power excursion. At Chernobyl they removed these rods past the safety limit because they were in the xenon pit, and trying to reach a specific power level on their reactor.

I'm more familiar with BWRs for instabilities. Standard BWRs like those in the US have stability issues as well at low flow low control rod density conditions, however the because the moderator and coolant are the same, they tend to be self limiting and are not capable of undergoing a power excursion/steam explosion. Most BWRs also have a system (called OPRM) that detects core stability issues and initiates an automatic reactor scram. What we see in BWRs is an oscillation with a time constant that is usually 1-2 seconds. We will start seeing small oscillations, then the oscillations will start growing. Oscillations in a BWR grow slowly. If the plant has an OPRM, it will scram the reactor prior to it increasing past certain limits. If the plant does not have an OPRM, it will grow over several minutes until the reactor hits either the high or low flux scram setpoints. The main danger in BWR type reactors is that you can cause localized thermal stress and plastic strain on the fuel (localized fuel cladding damage), but no power excursion.

For BWR light water reactors that have it equipped (all US BWRs have this) the OPRM (Oscillation Power Range Monitor) looks for counts (how many oscillations am I getting in a row), period (are the oscillations in the right time constant that is indicative of thermalhydraulic instabililty or is it just random noise), and growth (is my oscillation diverging with a &gt;1.0 decay ratio). There is also a confirmation density algorithm which uses a factor that looks at the above factors across the core to anticipate these factors before they start. If the OPRM gets enough counts, on the right period, with growth, it will initiate a scram immediately. US BWRs are forbidden from entering the region where core thermalhydraulic oscillations exist, and are required to insert an immediate reactor scram if they enter the region.

Generally, the only time a BWR enters the instability region is if they have an inadvertent loss of their reactor cooling pumps. 

Another poster mentions xenon. Xenon can cause long term issues which force you into a low rod density low flow situation, however xenon will not cause the instabilities that created the chernobyl event. Xenon does not respond fast enough (hours), while the boiling boundary response is in seconds. He's not entirely wrong, xenon transients can force you into an instable core operating region, but they do not cause the instability. Proper reactivity management can ensure that you pass around the instability region without going into it. RBMK reactors have a very tough time with the xenon pit and are more likely to put themselves into an instability, while other BWR type reactors can deal with it just fine.

I hope this helps. If you have any other questions please let me know.",null,0,cdks0mi,1r6hig,askscience,top_week,3
grillkohle,"Are you referring to the Chernobyl incident? I will try to explain the problems without looking at that incident.  
Controlling nuclear reactors is all about controlling (slow) neutrons. The slow neutrons basically are the chain reaction.  
A big problem with reactors in low power mode is Xenon 135. It is known as a neutron poison, because it has a high probability for absorbing neutrons (and thereby reacting to the stable Xenon 136). It is formed by decay of Iodine with a half life of 6.6 hours. When the reactor is running in normal operation mode, there is a balance between the ""production"" and the ""reduction"" of Xenon 135.  
If you lower the power output, there is more production of Xenon 135 because of the delay (decay from Iodine). Hence, it is more difficult to control the reactor, you can't increase the reactivity, because the Xenon will absorb your neutrons. Usually you have to shut it down and wait until most of the Xenon is decayed itself (half life 10 hours).  
But if you try to increase the reactivity (by pulling out control rods for example), you increase the reduction of Xenon (obviously the production stays at a lower level for some time because it takes some time for the ""production"" of the Iodine and its decay), which further increases your reactivity because you have more slow neutrons because less neutrons are absorbed by the Xenon. This happens to all nuclear reactors.   
The next problem is the positive void coefficient, which basically means: more thermal activity (more reactivity) =&gt; less cooling water (because more steam (much lower density!), more bubbles) =&gt; more thermal activity because of the void coefficient [because of the graphite still moderating at higher temperatures].  
These 2 effects support each other and make controlling the reactor in a low power state quite difficult.  
In a water moderated reactor (negative Void coefficient) for example the chain reaction stops or slows down if the water vaporizes too quickly, because there is less moderator to slow neutrons down. It is sort of ""self stabilizing"".  
I don't know how much you know about reactors, but I hope I could explain it to you. I know that stuff because I took a course at the university about nuclear reactors.
There are some other factors regarding the Chernobyl incident which lead to the explosion.
",null,0,cdkehwb,1r6hig,askscience,top_week,2
zopamine,"Forget everything you've ever heard about right brain/left brain. For the most part, it's a myth.

However, functions can be localized to certain hemispheres of the brain. For example, when we see human faces or pictures of faces, that process has been said to take place mostly in the right occipitotemporal (fusiform) gyrus, which is above your right ear and a little further back. 

Basically, there are cognitive functions that are localized to each hemisphere of the brain, but the whole being more left-brained (colloquially logical) or right-brained (colloquially creative) just isn't true. We use both hemispheres equally. 

Source: Nielsen, Zielinski, Ferguson, Lainhart, &amp; Anderson (2013) 

It's on PLOS One, so it's widely accessible if you're interested. I'd link it but I'm on my phone. Hope that was helpful!",null,1,cdk98lr,1r6hfl,askscience,top_week,4
3asternJam,Most famous example of left/right differences in brains is speech - Wernicke's area (speech/writing comprehension) and Broca's area (speech production) are only found in the left hemisphere (generally speaking; there are exceptions). The corresponding areas in the other hemispheres are more general sensory/premotor areas respectively.,null,1,cdkg8e2,1r6hfl,askscience,top_week,2
Platypuskeeper,"Without experiment, a guess from homology is the best you can do. ",null,1,cdk7kj3,1r6g8q,askscience,top_week,4
Siny_AML,I think the next question to ask would be the possible nature of the enzyme. You would have to test the Kd values of the reaction with and without the protein of interest to see whether the rate of the reaction either stays constant or changes. I may be wrong but I don't think that this is something that you would be able to test without using some kind of in-vitro system. Relying on computational methods would only give you a theoretical system with no biological basis.,null,0,cdk7ltk,1r6g8q,askscience,top_week,2
edge000,"I think this question is usually approached from the opposite direction. Biologists tend to be interested in processes, so I would think people wouldn't typically investigate a random protein unless there was some interesting reason to be looking at it. A scientists is more likely to be investigating some phenomenon and investigate what causes it. 

Genetic knockout experiments are carried out to elucidate a metabolic or signalling pathway. Basically, someone will identify that a suite of genes controls some pathway and will start [silencing genes](https://en.wikipedia.org/wiki/Gene_silencing) for enzymes along the pathway and see what substrate builds up in the pathway. 


After doing these types of experiments enough times we can build the database that people can search like you mentioned. ",null,0,cdkipub,1r6g8q,askscience,top_week,2
Baloroth,"t'=t/sqrt(1-(v^2 /c^2 )). So for a v of 370,000 m/s, that gives a rate of 1.000000760556423223785 seconds in our frame for every second in the ""stationary"" frame. So, time in our frame is passing at 99.9999239% the rate of the CMB stationary frame.

Note: that is with respect to an observer in the CMB frame. In our frame, it's the CMB frame that is moving through time slower (hence, relativity), by the exact same amount.",null,1,cdk7px4,1r6e8s,askscience,top_week,5
adamsolomon,"You phrased your question in two subtly different ways. The correct way, at the end of your post, ""how slowly are we experiencing time, relative to an observer who is stationary to [the] CMB?"". That's a valid question and has been answered already.

But in the title of your post you asked how slowly we're experiencing time relative to the CMB radiation itself. And that's actually *not* a valid question, since radiation doesn't experience time!",null,0,cdkdxq2,1r6e8s,askscience,top_week,2
redit_,"It produces lightning in basically the same way that thunderstorms do. It's a release of the accumulated static charge. Volcanic ""ash"" is actually a large cloud of rock dust. These particles bump against each other until they build up enough charge to release a spark. It's the same as when you rub your feet on the carpet except for a much larger spark.

And snowstorms can produce lightning, it's just very rare. They actually have a name for it; thundersnow. It's really, really cool to experience. It almost sounds like an avalanche because the reverberations are muffled by the snow. You don't hear a sharp crack and the echos. It's more of a deep rumble from out of nowhere.",null,1,cdk5nyf,1r6ag9,askscience,top_week,7
Sannish,"If there is lightning there will be thunder, but we might not hear it over the sound of an erupting volcano.

In typical thunderstorms it is the collision of ice and water that creates the charge separation that results in lightning.  We do know that there is charge separation in some volcanic plumes that produces lightning but it is currently unknown what is causing the charge separation.

It could be ash particles, it could be water in the plume, or it could be something else entirely.  Part of the problem is that it is very hard to take measurements inside of a volcanic ash plume.",null,0,cdkiq6w,1r6ag9,askscience,top_week,2
antpuncher,"In the first generation of stars, the only elements are hydrogen and helium, and they suck at cooling.  So when stuff collapses, it stays very hot.  That means there's a lot of available pressure, so for a blob of hydrogen to collapse by its own gravity, it has to be huge.  The first estimates are hundreds or thousands of solar masses, more recent estimates permit stars as small as 10-20 solar masses.  That's still _way_ bigger than a gas giant, which 10^-3 solar masses.

(The alternate scenario is that the rocky core forms first, but that won't work because rocks are silicon, carbon, and iron, and you need to make stars first, before you can make S,C, and FE.)",null,0,cdk0nqx,1r68w8,askscience,top_week,5
baloo_the_bear,"A person has 2 types of sweat glands, apocrine and eccrine. Eccrine glands are present all over your body and act to aid cooling. Apocrine sweat glands are present only certain areas like in the axilla (armpits) and groin. They develop during puberty and though they also contribute to cooling the body, they also excrete a small about of cytoplasm when they do so. This leads to the body smell. ",null,1,cdjxdqo,1r5y1m,askscience,top_week,22
lemons47,"Body odor generally starts becoming noticeable on a person when they reach puberty. Sweat glands in the groin and armpits become surrounded by dense, thick hairs around this time which provides a better environment for the growth of bacteria. Part of the smell is a combination of the bacteria producing odious chemicals from their own biological processes as well as the breakdown of molecules in sweat into more volatile chemical constituents. Because of this, your diet can sometimes affect your body odor because the chemical composition of secreted sweat can change which changes the volatile breakdown products produced by your skin microbiota.",null,0,cdkzlkd,1r5y1m,askscience,top_week,1
HexagonalClosePacked,"Not an aerospace guy, but it likely has to do with fault/failure tolerance.  Each propeller on a helocopter represents a single point of failure, meaning that it is a single part or component that causes the entire vehicle to become inoperative if it fails.  A helicopter cannot fly properly/safely if either one of its propellors is disabled.  Contrast this with a plane, which due to the lift generated by air moving over the wings, can keep flying if it loses one or more engines (even if it loses all of them it can glide for a while).

Say you have a helicopter with two props.  That means there are two points on the vehicle where a malfunction will cause a major accident/crash.  If we increase the number of propellors to 6, as is common in some RC helicopters, we have now tripled the number of points of failure, since if any one of these 6 props are disabled the aircraft will become unstable.  Someone might be able to correct me, but I'm assuming it would be much more difficult to design a 6 prop helicopter that could keep in the air after losing one propeller than it is to design a 6 engine plane that can do the same.",null,4,cdjxbul,1r5tcm,askscience,top_week,12
jvs_nz,"There are a few reasons why multicopters have not been brought to life size yet. I can see it being feasible one day, but improbable.

* Most multicopters you see (4,6 and 8 rotors) control their pitch, altitude, and yaw by varying the speed of the blades. They can do this because the inertia of the blades is quite small, so the motors don't have a big job of changing the speed. This wouldn't be the case for full size blades needed to lift a multi-ton craft. 
This can however be offset by using variable pitch blades, much the same as on a normal helicopter - this obviously increases complexity however and also the number of components that can fail.
* 4 rotor crafts will crash if a prop fails. All 4 are fundemental to having controlled lift. 6 and 8 rotor crafts can survive quite easily if 1 prop or motor fails (provided the failure doesn't take any of the other motors out, like a blade flying through neighbouring motors). 
* The electronics and systems needed to keep multicopters afloat are still very new - only in the past 5-8 years has the technology matured enough to make such a thing feasible. 
* Cost. A single engine, single rotor heli is expensive enough. Now times that by 6 - and for what? There are very few scenarios where a traditional helicopter can't do a job a multi could. Now times the running costs by 6 as well. (I say 6 because you wouldn't build a 4 rotor due to above mentioned safety/redundancy reasons).

The reason multi rotors have taken off in the hobby and small scale world is because traditional single rotor model helicopters have been very difficult to control and learn to use - as well as horribly dangerous. Because multi rotors are controlled by software - it's easy enough to make that software user friendly, so it doesn't take much to pick up and learn. They are also cheaper in a lot of the cases - you can build a multi rotor for a couple of hundred $. 
They can also be quieter. 

I wouldn't rule out full size multi's in the future, I just don't think it'll be very common for a long time, if not ever.",null,1,cdk2y6y,1r5tcm,askscience,top_week,9
Farnswirth,"A lot of people here are arguing that more rotors = more complexity.  While this is true, it does not mean that more rotors = worse design.  Take the B-52 for example, it has 8 jet engines.  Or the V-22, it has two engines built on rotating pivots.  Complexity is not always bad, sometimes you are merely trading simplicity for functionality.  In the case of the B-52 you are gaining redundancy and power.  In the case of the V-22 you are gaining speed and VTOL capability.  

There is no reason you couldn't build a full sized multi-copter.  I am sure there are companies working on one right now.  The problem with multicopters is not necessarily the complexity of the *design* but the complexity of the software and control systems needed to build a full scale one.  For example, if you loose one engine on a B-52 and the flight computer fails, a pilot can still fly the plane manually with no problem.  If the flight computer fails on a multicopter - you are screwed, especially if a rotor fails as well.  A pilot simply can't react fast enough.  We've seen examples of unstable aircraft with extensive flight control systems: the F-16, the [F-22](http://www.youtube.com/watch?v=faB5bIdksi8), [V-22](http://www.youtube.com/watch?v=n3lbKqStvHI), [B-2](http://www.youtube.com/watch?v=_ZCp5h1gK2Q), F-117, etc.  Consequently, most of these planes have had horrific crashes where the flight computer fails and the pilot can't react fast enough - that's just the nature of the plane.  An unstable helicopter is just an added level of danger and complexity, it's just asking for trouble.  That's not to say it's not possible, it's just very challenging.  The technology is new.  It's maturing.  

[People have done it](http://www.youtube.com/watch?v=L75ESD9PBOw).  [The military has played around with it.](http://en.wikipedia.org/wiki/Curtiss-Wright_VZ-7) Commercial and modern military versions are likely on their way.  It's just a matter of time.  ",null,0,cdkggl3,1r5tcm,askscience,top_week,2
SitnaltaPhix,"Because single-rotor RC helicopters are **extremely difficult** to learn how to fly. Not only do aerodynamics work differently on small aircraft (and therefor make them very unstable), but you are not operating in the same reference frame as a regular pilot would be. Even experienced RC pilots crash all the time. That's rather disconcerting to someone who just wants to take aerial shots with their camera.

The advent of Lithium polymer batteries, more efficient motor controllers, and better computing power have allowed multi-rotor RC helicopters to be possible. Allowing not particularly experienced pilots to fly.

Larger aircraft are just inherently more stable. Their rotors have more air molecules to push against and more inertia to keep everything in line. They don't really need extra stability, making the engineering feat of designing gas-powered engines driving multiple propellers not economically viable.",null,2,cdkakpm,1r5tcm,askscience,top_week,2
good_n_plenty,"So... not an expert of this but my understanding is that one of the advantages of having two rotors rather than one is that the rotors spin in opposite directions, so their torques cancel each other out. With only one main rotor, you need a tail rotor to control the angle of the helicopter and keep it from spinning in the opposite direction as the rotors are moving. That's why helicopters with two main rotors don't have a tail rotor. Having 3 (or 5, or 7) main rotors would defeat the purpose of having multiple rotors. I don't know about 4. You should ask r/helicopters",null,3,cdk1dic,1r5tcm,askscience,top_week,2
PepperJack_delicacy,"The other poster is completely right but since you're teaching a science class I thought maybe your students would appreciate a slightly more detailed answer.


Alcohol is metabolized predominantly in the liver and there are **3** important enzymes that you should be aware of. First, alcohol is converted to acetaldehyde by **alcohol dehydrogenase** or **cytochrome P-450**. In turn, acetaldehyde is converted to acetate by **acetaldehyde dehydrogenase**.

Both alcohol dehydrogenase and acetaldehyde dehydrogenase create molecules of **NADH** during these reactions and NADH will inhibit gluconeogenesis and fatty acid oxidation. The important thing to take away from this is that **this leads to a very fatty liver when you chronically abuse alcohol**.

Also, an alcoholic will have an increased amount of cytochrome P-450 (this is because the body tries to adapt to the increased intake of alcohol). Cytochrome P-450 handles many other reactions and will cause the body to create an excess amount of **reactive oxygen species**. Having a lot of reactive oxygen species around creates unnecessary inflammation, cell death, and fibrosis. 

*So overall, an alcoholic will have a fatty liver and an excess of reactive oxygen species. Together, this creates a lot of damage in the liver tissue, which over a long period of time will lead to cirrhosis.* 

I hope this answers your question. Let me know if you want me to clarify anything. 


Source: http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hepatology/alcoholic-liver-disease/",null,2,cdjxr3r,1r5rxu,askscience,top_week,14
then_and_again,"the body processes ethanol by oxidizing it to acetylaldehyde, and then to acetyl-CoA. a lot of the enzymes needed are in the liver, and the liver takes the brunt of alcohol metabolism. Why some people get cirrhosis is still unclear, we know that the liver undergoes oxidative stress, which causes apoptosis and eventually fibrosis. What we don't know is why this only happens to some people, most people get hepatitis instead.  ",null,2,cdjv9o2,1r5rxu,askscience,top_week,7
wishfulthinkin,"We have adapted to live in this pressure.  Reduced pressure is not equivalent to increased health, as evidenced by the death of many deep sea fish and crustaceans when they are brought too close to the surface.  Their bodies have evolved for life in the high pressure of the sea floor, and upon losing the pressure, their bodies lose structural integrity and they simultaneously increase in size and decrease in stiffness.  The blobfish picture everyone has seen is a good example.  In its natural habitat, the blobfish looks similar to most other fish, but when it is brought up to the surface, a low pressure area for it, its body collapses and it looks like a gelatinous blob.",null,6,cdjw6hn,1r5qol,askscience,top_week,16
goldistastey,"The atmospheric pressure is balanced by our internal pressure which is just due to us being a bunch of compressed stuff, and our cells and tissues have evolved this pressure to match the atmospheric pressure. That's why in space, with no atmospheric pressure, our internal pressure would kill us.",null,2,cdjz0eb,1r5qol,askscience,top_week,6
adamhstevens,"Mainly because we are mostly composed of incompressible materials (i.e. water). In fact, the airspaces in the body (i.e. lungs, stomach, sinuses, ears) are generally in equilibrium with the atmosphere, so air moves freely in and out, preventing a pressure difference.

Problems actually occur in certain situations (e.g. diving) where these airspaces are blocked from equilibrating and the pressure difference can cause tissues to rupture, but most of your body is unaffected.",null,1,cdkgsba,1r5qol,askscience,top_week,3
Javi2639,"Soap is the salt of a carboxylic acid with a long hydrocarbon tail. The ion head will interact strongly with polar water, while the hydrocarbon tails will interact strongly with each other. This forms a bilayer with the tails pointing inward towards each other and the heads pointed outwards towards the water, forming a sphere. It is the same principle that creates the cell membrane of all living creatures. ",null,1,cdjw2rs,1r5o2u,askscience,top_week,3
slartibartfastfive,"Presence of surfactants; water is a racist.

Water is very cohesive because it's small and can form networks of hydrogen bonds. This property leads to surface tension, which is the force that opposes the formation of bubbles--the water doesn't want to be spread out in a thin surface touching the air, it wants to be huddled up in a ball with as many water molecules as possible touching each other.

Surfactants, like soaps or other amphiphilic molecules, have a structure where one part of the molecule is quite polar (water-loving, hydrophilic) and one part is less polar (""oil""-like, hydrophobic, hates water). The surfactant molecules arrange themselves at the air-water interface with the nonpolar parts touching the air. The nonpolar parts of the surfactant are much happier in contact with air than water would be, and the polar parts are pretty happy touching water, so this arrangement lowers the interfacial tension at the bubble surface, allowing it to grow beyond a certain size.

Bubbles will form in soapy water or milk because of this. Mayonnaise and salad dressing are ""emulsions"" of oil and water stabilized by similar surfactants.",null,0,cdl2lc5,1r5o2u,askscience,top_week,1
Overunderrated,"Good question!

Intuitively, if you have a rigid container which contains a tube at equilibrium, if you then create a vacuum within that tube, you'll be increasing the pressure of the rest of the container.

But earth's atmosphere isn't a rigid box; it exponentially decays towards zero density far from the surface. The hydrostatic pressure you feel at the surface of the earth is due to gravity acting on all the air above you. Now, gravitational attraction decreases with height... and if you were to create a vacuum chamber at the surface, this would have the effect of pushing air up, towards higher altitude, where gravity is weaker. So I could see that creating a vacuum chamber at the surface of the earth might actually *lower* the ambient atmospheric pressure. Analogous to the notion that if Earth's radius doubled, but the total mass of air stayed the same, the ambient pressure would be much lower.


I'm just spitballing here, so corrections/refutations/alternatives welcome.",null,1,cdjv1la,1r5n2t,askscience,top_week,13
Javi2639,"Differing intermolecular forces. Milk is mainly water, which is polar and can form strong hydrogen bonds with other water molecules. Chocolate is mostly fat, which only have van der Waals interactions. This will cause the two to interact with each other and exclude the other, separating them. ",null,0,cdjvy8k,1r5lx8,askscience,top_week,5
wishfulthinkin,"Improvements in dental care have significantly extended the lifespan of humans.  The wealthy in pre-modern-medicine times very frequently died from cavities, as only the wealthy were able to afford sugar.  In fact even nowadays, there are instances where people get cavities in a molar that eventually extends back through the mandible and into the brain, causing infection and later, death.",null,2,cdjw217,1r5lfe,askscience,top_week,10
TrainerGary,"The concept is called [proprioception](http://en.wikipedia.org/wiki/Proprioception). 

I'm sure someone will give a more in depth answer, but essentially there are multiple body maps in your brain, and these maps are used in conjunction with your peripheral nervous system in order to determine body position. 

For example, proprioceptors are found in skeletal muscles. The relative stretching/compression of these proprioreceptors gives information about the position of the limb.",null,4,cdju34l,1r5l7s,askscience,top_week,26
Lillelyse,"As already mentioned this is called proprioception. To expand a bit on how sensors in the muscles tell you about limb position: The body's sense of its position in space is determined by stretch receptors in the muscles, joints and tendons. Muscle stretch receptors, or muscle spindles, are specialised muscle fibres in a fibrous capsule. These detect changes in muscle length.

Joint receptors are mechanosensitive axons in the connective tissue of joints. They are good at detecting the movement of the joint, but not so useful when the joint is kept still. 

Finally, at the junction of a muscle and a tendon we find Golgi tendon organs. These register muscle tension, or force of contraction. The information from all of these sensors come together to tell us where our limbs are. ",null,1,cdjxdl4,1r5l7s,askscience,top_week,9
SpacemanSpiff1222,"Without getting too in depth, your body will use proprioceptive sense organs to obtain this information. In the muscle itself, your muscle spindles will pick up how fast and how intense a contraction. Muscle spindles exist all throughout the muscles and in different numbers in different muscles. For example, the sub occipital muscles have a much higher amount of muscle spindles than, say, the biceps femoris. Inside the tendonous attachments to the bones you have Golgi Tendon Organs which will pick up information regarding tension. The information will then travel to the spinal cord through peripheral sensory nerves, in to the spinal cord in the medial lemniscal system. From here it will travel up through the medulla and decussate (cross) at the nuclei gracilis and cuneatus as internal arcuate fibers. The tracts will then head on up to the thalamus (VPM and VPL) and out to the post central gyrus in the cortex of the cerebrum. Skipping a lot of stuff, but that is the basics of it. 

ninja edit: This is purely proprioceptive. Your body will also use the vestibular system to track the position of the head. Also, as simple as it sounds, your body will keep track of your segments using your vision as well. ",null,0,cdk15wc,1r5l7s,askscience,top_week,2
lengendscrary,"Like it was mentioned before visual stimuli seems to be important in creating your body map. If you're interested in getting a better understanding of proprioception, you can read a short story called the Disembodied Lady by neurologist Oliver Sacks. It is sort of a case study of what happens when you lose your sense of proprioception. It's pretty cool because the lady would not know what her limbs were doing and where they were unless she was looking directly at them.",null,0,cdkj53i,1r5l7s,askscience,top_week,1
ModernTarantula,The [cerebellum](http://neuroscience.uth.tmc.edu/s3/chapter05.html) may be responsible for most of the body map. It is not part of consciousness. Such that you don't have to think exactly about the movement necessary to touch your knee. I heard tell that if you hold a stick it will become part of the cerebellar map.,null,2,cdkc572,1r5l7s,askscience,top_week,1
darksingularity1,"I'm assuming you mean the relative positions of your limbs and not just the relative position of your entire body(well just your head really) in space. The latter deals with the vestibular system near your ears. The way I like to think of it is that it covers the three planes of space (x,y,z) with its three orthogonal ""pipes"". 

Now to the issue of knowing where your hand is at any given moment. There are two ways that we can generally figure it out. Or at least there are two theories. So theoretically, in order to find the arms position, the brain can just ""ask"" it or if the arm just moved somewhere it can calculate the new position from the old position and the force/energy/etc that was used to get it there. 

First of all, the general sense of position is called proprioception. We have different sensors on our body to figure out our positions. Most importantly, we have things called muscle spindles within out muscles. These give the brain an idea of how contracted/relaxed the muscle is.
Also, when messages are sent to muscles to do something, they sometimes send something back, called an efference copy, which kinda gives the brain more info on position and stuff.
The brain also uses the traditional senses for this as well. Vision plays a huge part in this. Seeing where your arm is gives you a great idea of where it is (amazing right?!). ",null,8,cdjxkkv,1r5l7s,askscience,top_week,3
TrainerGary,"The concept is called [proprioception](http://en.wikipedia.org/wiki/Proprioception). 

I'm sure someone will give a more in depth answer, but essentially there are multiple body maps in your brain, and these maps are used in conjunction with your peripheral nervous system in order to determine body position. 

For example, proprioceptors are found in skeletal muscles. The relative stretching/compression of these proprioreceptors gives information about the position of the limb.",null,4,cdju34l,1r5l7s,askscience,top_week,26
Lillelyse,"As already mentioned this is called proprioception. To expand a bit on how sensors in the muscles tell you about limb position: The body's sense of its position in space is determined by stretch receptors in the muscles, joints and tendons. Muscle stretch receptors, or muscle spindles, are specialised muscle fibres in a fibrous capsule. These detect changes in muscle length.

Joint receptors are mechanosensitive axons in the connective tissue of joints. They are good at detecting the movement of the joint, but not so useful when the joint is kept still. 

Finally, at the junction of a muscle and a tendon we find Golgi tendon organs. These register muscle tension, or force of contraction. The information from all of these sensors come together to tell us where our limbs are. ",null,1,cdjxdl4,1r5l7s,askscience,top_week,9
SpacemanSpiff1222,"Without getting too in depth, your body will use proprioceptive sense organs to obtain this information. In the muscle itself, your muscle spindles will pick up how fast and how intense a contraction. Muscle spindles exist all throughout the muscles and in different numbers in different muscles. For example, the sub occipital muscles have a much higher amount of muscle spindles than, say, the biceps femoris. Inside the tendonous attachments to the bones you have Golgi Tendon Organs which will pick up information regarding tension. The information will then travel to the spinal cord through peripheral sensory nerves, in to the spinal cord in the medial lemniscal system. From here it will travel up through the medulla and decussate (cross) at the nuclei gracilis and cuneatus as internal arcuate fibers. The tracts will then head on up to the thalamus (VPM and VPL) and out to the post central gyrus in the cortex of the cerebrum. Skipping a lot of stuff, but that is the basics of it. 

ninja edit: This is purely proprioceptive. Your body will also use the vestibular system to track the position of the head. Also, as simple as it sounds, your body will keep track of your segments using your vision as well. ",null,0,cdk15wc,1r5l7s,askscience,top_week,2
lengendscrary,"Like it was mentioned before visual stimuli seems to be important in creating your body map. If you're interested in getting a better understanding of proprioception, you can read a short story called the Disembodied Lady by neurologist Oliver Sacks. It is sort of a case study of what happens when you lose your sense of proprioception. It's pretty cool because the lady would not know what her limbs were doing and where they were unless she was looking directly at them.",null,0,cdkj53i,1r5l7s,askscience,top_week,1
ModernTarantula,The [cerebellum](http://neuroscience.uth.tmc.edu/s3/chapter05.html) may be responsible for most of the body map. It is not part of consciousness. Such that you don't have to think exactly about the movement necessary to touch your knee. I heard tell that if you hold a stick it will become part of the cerebellar map.,null,2,cdkc572,1r5l7s,askscience,top_week,1
darksingularity1,"I'm assuming you mean the relative positions of your limbs and not just the relative position of your entire body(well just your head really) in space. The latter deals with the vestibular system near your ears. The way I like to think of it is that it covers the three planes of space (x,y,z) with its three orthogonal ""pipes"". 

Now to the issue of knowing where your hand is at any given moment. There are two ways that we can generally figure it out. Or at least there are two theories. So theoretically, in order to find the arms position, the brain can just ""ask"" it or if the arm just moved somewhere it can calculate the new position from the old position and the force/energy/etc that was used to get it there. 

First of all, the general sense of position is called proprioception. We have different sensors on our body to figure out our positions. Most importantly, we have things called muscle spindles within out muscles. These give the brain an idea of how contracted/relaxed the muscle is.
Also, when messages are sent to muscles to do something, they sometimes send something back, called an efference copy, which kinda gives the brain more info on position and stuff.
The brain also uses the traditional senses for this as well. Vision plays a huge part in this. Seeing where your arm is gives you a great idea of where it is (amazing right?!). ",null,8,cdjxkkv,1r5l7s,askscience,top_week,3
goldistastey,"They're made of generally the same stuff, but in different amounts. More or less fast-action muscles (aka white or dark meat), more or less fat in the tissues, more or less fat around the tissues, more or less fibers between the tissues, more or less gelatin in the bones, etc. Cooked very precisely, you could probably make many parts of a turkey or duck taste like many parts of a chicken.",null,1,cdjz6nm,1r5fge,askscience,top_week,3
then_and_again,"It does refract light, the refraction index of the vitrous humor is 1.336, water's around 1.333 if i remember correctly. We don't have fish eye vision because your brain is the organ that 'sees' not your eyes. What you see is just your brain's interpretation of certain light signals, so yes the light is refracted, and that refraction causes your brain to see it as 'normal'. if you look through an lens that corrects this refraction, your brain will interpret it as a different image and your vision will be distorted. Basically, your brain is used to light being refracted, so that's the basal image",null,0,cdjvfca,1r5d2l,askscience,top_week,3
wishfulthinkin,"The vitreous humor does refract light, but the brain is able to counter this by processing the image before you ""see"" it.  Likewise, the image of the world that reaches your brain via the optic nerve is upside down, and the brain flips it right side up.  Similarly, your brain hides the small blind spot that is created by the complete lack of rods and cones on your optic nerve.  When you see anything, it's actually your brain's interpretation of the real image of the world.",null,0,cdjvw0i,1r5d2l,askscience,top_week,2
FatSquirrels,"It definitely falls into the definition of a molecule, but we tend not to think of it that way when talking about polymers.

When working with [macromolecules](http://en.wikipedia.org/wiki/Macromolecule) we will instead break things up into repeat units or subgroups.  For polymers we look at the repeat units, for copolymers we look at the individual pieces/blocks and for proteins we look at the amino acid sequence.  When looking at something small, a single picture of the molecule is all we need to define, but for things where one molecule is sometimes arbitrarily large we determine the properties of the substance by analyzing smaller pieces and we don't even think about the term ""molecule.""

However, the fact that a piece of vulcanized rubber might be one big molecule is very important to its material properties.  You could theoretically melt it or dissolve the whole thing but it would remain completely connected to itself, just one big connected molecule that has very strange properties when compared to ""normal"" sized molecules.",null,0,cdjrxb3,1r5ckr,askscience,top_week,6
Brewe,"It is one molecule, but it's usually not treated as such. It's like with crystal structures that are also one big molecule (at least as long as we're talking about a perfect crystal), here we look at a unit cell, the smallest part of the crystal that explains the whole structure. The same is done with polymers, where f.x. a polymer could be shown as [-CH2-CH2-]n, where n is a large number.

Polymers, crossslinked polymers, crystals etc. are technically one very large molecule, but they are not treated as such.",null,2,cdjr831,1r5ckr,askscience,top_week,4
owaisofspades,"yup, eventually the receptors for it on your tongue will get down regulated with continued exposure. 

Also, it's slightly toxic, so it might even lead to tolerance by damaging the cells that contain the receptors themselves, but I'm not entirely sure ",null,1,cdjx7uz,1r5c17,askscience,top_week,2
PorchPhysics,"I can answer this from a physics standpoint, but I might be uninformed on a particular topic:

No.

Nuclei are held together by the strong nuclear force, if it were not for this force, then all the protons would repel one another greatly due to the electric force between two charges.  Protons are all positive and would want to repel away.  

If the two nuclei you propose are close enough for the strong nuclear force to keep them from repelling significantly, they would be merged, as the drop-off in strength of the strong nuclear force is extremely rapid over all but the smallest of distances.",null,1,cdjrjz7,1r5bzl,askscience,top_week,5
Platypuskeeper,"Yes, [halo nuclei](http://en.wikipedia.org/wiki/Halo_nucleus).",null,3,cdjx255,1r5bzl,askscience,top_week,4
hotshot_sawyer,"Check this out: [antiprotonic helium](http://en.wikipedia.org/wiki/Antiprotonic_helium). It's an electron and an antiproton orbiting a helium nucleus. They make it in 3% yield by simply shooting antiprotons into helium. It lasts for microseconds.

The antiproton is bound in an orbital with n around 38, which is different from normal atoms where you fill up orbitals from n=1 and up with very few exceptions. This seems interesting but I'm pretty helpless with quantum stuff and it would be great if someone else could unpack this aspect a little.

There's also [positronium](http://en.wikipedia.org/wiki/Positronium), which is a bound electron-positron pair that behaves a lot like hydrogen but doesn't last very long.",null,0,cdlzl7r,1r5bzl,askscience,top_week,1
TheSkyPirate,"ELY5: 

1. Opiates make you feel good, so you want to keep taking them. 

2. Your body uses natural opiates to regulate feelings if discomfort. You are constantly experiences a ton of slightly painful stimuli, and your body uses natural opiates to block these and keep you feeling ""normal.""  Taking artificial opiates causes down-regulation of these natural opiates and the receptors that they bind to, so when you're off them your body is less able to compensate for pain. You end up feeling nauseous and achy because your body can't properly regulate discomfort.  

This is different than normal addiction. You don't just crave opiates, you're consciously aware the you need them to not be in pain, and you want to take them in order to feel happy. ",null,1,cdjz371,1r5a61,askscience,top_week,5
Javi2639,"Narcotics are opiates, which are chemicals that imitate natural endogenous painkillers like endorphins, enkephalins, and dynorphins. The body releases these in life or death situations, such as when you are running from a predator. If you break your ankle doingso, you will collapse in pain and die without these painkillers. The brain focuses on survival first and repair later in these situations. However, if there is a chemical that can bind to the same opioid receptors as these endogenous painkillers, the receptors will become overstimulated. The brain adapts to the constant stimulus, but upon stopping the opiates, the brain thinks it is in pain, so you take more. This is where the addiction potential comes in. ",null,3,cdjwd3x,1r5a61,askscience,top_week,4
Takagi,"Percocet is an opioid which increases dopamine levels in synapses (Online edition of Harrison's Principles of Internal Medicine, 18e), and dopamine is a chemical associated with rewards and ""feeling good"". That's the neuro part of why it makes us feel good. There's withdrawal that comes with people who use percocet which can develop between 6-8 weeks of chronic use (ibid).",null,1,cdjswxf,1r5a61,askscience,top_week,1
Surf_Science,"It varies, even with Salmonella for example a few bacteria (10-20) can cause illness in the right circumstances where in other cases it will take millions. 1 Bacterium could cause illness but it is unlikely. ",null,2,cdjrf7z,1r57f2,askscience,top_week,10
wishfulthinkin,"As stated before in this thread, it varies depending on the strain and species of the bacteria.  It is theoretically possible for an individual to become ill from a single bacterium, but depending on the strength of the individual's immune system relative to the aggression of the bacterial strain.  In the vast majority of cases, a single bacterium would be destroyed by the immune system before it got a chance to reproduce and further infect the body.",null,0,cdjvsro,1r57f2,askscience,top_week,2
endocytosis,"As others mentioned, yes.  With [TB](http://iai.asm.org/content/73/10/6467.full) it is widely hypothesized that a single bacterium can successfully infect someone.  This does not necessarily mean that infection will happen every time, but the that it *can* happen.  The journal article is from a scientific group that studied *Mycobacterium bovis*, a related strain of bacteria that was also used to make the BCG [vaccine](http://en.wikipedia.org/wiki/BCG_vaccine).",null,0,cdjznhe,1r57f2,askscience,top_week,1
Davecasa,"Bone mass decreases, but the bones don't actually get much smaller, just less dense. The height increase is mostly due to lengthening of the spinal column due to not fighting against gravity... the same happens every night when you sleep, to a lesser extent.

Some more info:  
http://www.nasa.gov/mission_pages/station/research/news/spinal_ultrasound.html#.Uo4-hMR9te5  
http://science1.nasa.gov/science-news/science-at-nasa/2001/ast01oct_1/",null,0,cdjqu8k,1r57d7,askscience,top_week,10
dkreat,"There is a constant gravitational force on your entire body; this is something that your bones, ligaments, cartilage, muscles and joints all have to oppose and overcome to produce the common movements and actions you perform daily. This competition creates strain, and the cartilage and bones over time compress and shrink due to the act of holding your body up.

As /u/Davecasa said, your spinal column becomes decompressed in space and your bones decrease in density. This can compensate for a good portion of your height since the spinal column counts for nearly 25% of the average person's height. The cartilaginous discs that intercalate with our vertebrae can change shape and compress to allow for weight bearing to shift from the bones themselves to the cartilage in certain positions. 

In space, and when you aren't walking bipedally, the weight bearing role of your spinal column decreases, so the cartilage can return to its resting shape and density, and your height increases, even if by a little bit.

Source: I'm in medical school",null,0,cdk5ak7,1r57d7,askscience,top_week,1
__Pers,"One of the hypotheses consistent with the data (at least back in the 1990s, when I last followed this field) is that gases like N2 and O2 dissociate during collapse, form different chemicals (e.g., peroxide), and thus are lost to the water during implosion. Noble gases don't undergo such chemical changes, meaning then when one has trace amounts of such gases there's a surfeit of argon or xenon left behind to compress and radiate. ",null,0,cdjxnxa,1r55qg,askscience,top_week,2
zmil,"Think of the human genome like a really long set of beads on a string. About 3 billion beads, give or take. The beads come in four colors. We'll call them bases. When we sequence a genome, we're finding out the sequence of those bases on that string. 

Now, in any given person, the sequence of bases will in fact be unique, but unique doesn't mean completely different. In fact, if you lined up the sequences from any two people on the planet, something like 99% of the bases would be the same. You would see long stretches of identical bases, but every once in a while you'd see a mismatch, where one person has one color and one person has another. In some spots you might see bigger regions that don't match at all, sometimes hundreds or thousands of bases long, but in a 3 billion base sequence they don't add up to much. 

edit 2: I was wrong, it ain't a consensus, it's a mosaic! I had always assumed that when they said the reference genome was a combination of sequences from multiple people, that they made a consensus sequence, but in fact, any given stretch of DNA sequence in the reference comes from a single person. They combined stretches form different people to make the whole genome. TIL the reference genome is even crappier than I thought. They are planning to change it to something closer to a real consensus in the very near future. My explanation of consensus sequences below was just ahead of its time! But it's definitely not how they produced the original genome sequence.

If you line up a bunch of different people's genome sequences, you can compare them all to each other. You'll find that the vast majority of beads in each sequence will be the same in everybody, but, as when we just compared two sequences, we'll see differences. Some of those differences will be unique to a single person- everybody else has one color of bead at a certain position, but this guy has a different color. Some of the differences will be more widespread, sometimes half the people will have a bead of one color, and the other half will have a bead of another color. What we can do with this set of lined up sequences is create a *consensus* sequence, which is just the most frequent base at every position in that 3 billion base sequence alignment. And that is basically what they did in the initial mapping of the human genome. That consensus sequence is known as the reference genome. When other people's genomes are sequenced, we line them up to the reference genome to see all the differences, in the hope that those differences will tell us something interesting. 

As you can see, however, the reference genome is just an average genome*; it doesn't tell us anything about all the differences between people. That's the job of a lot of other projects, many of them ongoing, to sequence lots and lots of people so we can know more about what differences are present in people, and how frequent those differences are. One of those studies is the 1000 Genomes Project, which, as you might guess, is sequencing the genomes of a thousand (well, more like two thousand now I think) people of diverse ethnic backgrounds. 

*It's not even a very good average, honestly. They only used 8 people (edit: 7, originally, and the current reference uses 13.), and there are spots where the reference genome sequence doesn't actually have the most common base in a given position. Also, there are spots in the genome that are extra hard to sequence, long stretches where the sequence repeats itself over and over; many of those stretches have not yet been fully mapped, and possibly never will be.

edit 1: I should also add that, once they made the reference sequence, there was still work to be done- a lot of analysis was performed on that sequence to figure out where genes are, and what those genes do. We already knew the sequence of many human genes, and often had a rough idea of their position on the genome, but sequencing the entire thing allowed us to see exactly where each gene was on each chromosome, what's nearby, and so on. In addition to confirming known sequences, it allowed scientists to predict the presence of many previously unknown genes, which could then be studied in more detail. Of course, 98% of the genome *isn't* genes, and they sequenced that as well -some scientists thought this was a waste of time, but I'm grateful the genome folks ignored them, because that 98% is what I study, and there's all sorts of cool stuff in there, like ancient viral sequences and whatnot.

edit 3: Thanks for the gold! Funny, this is the second time I've gotten gold, and both times it's been for a post that turned out to be wrong, or partly wrong anyway...oh well.",null,198,cdjoort,1r54d1,askscience,top_week,1053
Chl0eeeeeee,"Even though everyone has unique DNA, genes still would occur in the same location in the genome (exclusive of any mutations that would add/delete a nucleotide). Basically what genome mapping does is look at multiple samples of DNA from different people. It aims to understand what regions are coding versus non-coding, and to annotate the genome (see what the coding genes control). This has been done for other species. ",null,17,cdjopps,1r54d1,askscience,top_week,77
Tass237,"Complete mapping of what sections apply to what.  A redhead and a blonde both have a gene for hair color, and the location of that hair color gene can be mapped.  The fact that they have *different alleles* doesn't mean it's a different gene or in a different location.",null,7,cdjq7af,1r54d1,askscience,top_week,29
zedrdave,"In addition to other answers in this thread, one important clarification: when one says that a person's DNA is unique, that's still no more than somewhere around a 0.01% difference, out of the entire sequence, between two individuals.

Most nucleotides (the small bricks that make the DNA sequence) are the same for all individual of the same species (humans, for instance), with a very few single nucleotides changing here and there (these changes are called [SNPs](http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism)). Just the same way that moving a single cog in a complex mechanism, or modifying a single byte in a computer program, will give out a completely different result, that single nucleotide modification can have huge consequences on the person's appearance, health etc.

Mapping the first genome, meant mapping *a* genome (with its specific SNPs), with the implicit idea that we were first interested in the parts that were common to everybody. Now that sequencing is a lot cheaper and more widespread, there are a number of efforts to map genomes for a number of individuals, in order to figure out more specifically which positions in the sequence can occasionally differ (see ""1000 genome project"").

**Edit:** I should have also mentioned that, while some SNP variations have huge effects on the resulting organism, other SNP mutations are completely silent (""synonymous mutations""), thanks to the redundancy of the DNA-Amino Acid transcription code (i.e. different triplets of DNA can end up coding for the same AA). Because such silent mutations do not affect fitness (and therefore are more likely to be passed down), they are a lot more common than you would expect from pure chance.",null,1,cdjud86,1r54d1,askscience,top_week,9
tdcarlo,"Each person's DNA is unique, that is true.  But the difference between you an me is incredibly small.  

DNA is made up of nucleotides.  There are four kinds of nucleotides.  Think of nucleotides as legos each kind being a different color....let's say Aqua, Green, Cyan, and Teal.  A gene is composed of nucleotides in particular order.  Imagine stacking legos.  Using the first letter of the colors from the legos, the insulin gene is 450 nucleotides long and looks like this.

Aqua Green Cyan Cyan Cyan Teal Cyan Aqua GGACAGGCTGCATCAGAAGAGGCCATCAAGCAGATCACTGTCC
TTCTGCCATGGCCCTGTGGATGCGCCTCCTGCCCCTGCTGGCGCTGCTGGCCCTCTGGGGACCTGACCCAGCCGCAGCCTTTGTGAACCAACACCTGTGCGGCTCACACCTGGTGGAAGCTCTCTACCTAGTGTGCGGGGAACGAGGCTTCTTCTACACACCCAAGACCCGCCGGGAGGCAGAGGACCTGCAGGTGGGGCAGGTGGAGCTGGGCGGGGGCCCTGGTGCAGGCAGCCTGCAGCCCTTGGCCCTGGAGGGGTCCCTGCAGAAGCGTGGCATTGTGGAACAATGCTGTACCAGCATCTGCTCCCTCTACCAGCTGGAGAACTACTGCAACTAGACGCAGCCCGCAGGCAGCCCCACACCCGCCGCCTCCTGCACCGAGAGAGATGGAATAAAGCCCTTGAACCAGCAAAA

So we know what a gene is...the next thing to understand is a chromosome.  A chromosome is a long stack of DNA that contains numerous genes.  There are 23 chromosomes in the human genome.  The longest human chromosome is about 250 million nucleotides long the shortest is around 50 million nucleotides.  Each chromosome contains hundreds of genes along with some other ""accessory"" DNA that is beyond the scope of this explanation.    The entire size of the human genome is around 3 billion nucleotides.

Human being the clever types have been able to determine the precise order of all of the nucleotides in each human chromosome and have identified most if not all of the genes on it.  So each chromosome has the location of each gene mapped. Pretty amazing.

Your DNA is unique but the percentage of the 3 billion nucleotides that are different than mine is less than 0.0001% and most of the differences will be in the so called ""accessory"" DNA.  ",null,0,cdjtoip,1r54d1,askscience,top_week,8
nanoakron,"I feel the need to write this because whilst all the previous commenters have gone into great depths to explain the science behind genes and genomes, they have failed to address a fundamental misunderstanding the OP has:

Your DNA is **NOT** unique. Only about 0.1% of it is. You are somewhere around 99.5-99.9% **genetically identical to every other human on the planet**.

You're also 98.8% **identical** to every chimpanzee, 98.4% **identical** to every gorilla, 88% to every mouse, 65% to each chicken and 47% genetically **identical** to a fruit fly.

This means you have the exact same codes (give or take a letter) for the most essential 'housekeeping' functions - the ones that process energy in your cells, allow your cells to reproduce, build cell walls, cell skeletons and the other basic stuff all multicellular life needs to do. As a side note, this is very strong evidence that these abilities evolved only once in a distant ancestor, and then because they were so successful compared to all species around at *their* time, they outcompeted them and all their descendants now share those genes.

The closer you get to a human in genetic relatedness, the similarities extend beyond simple housekeeping genes to those which allow us to be 4-limbed, air-breathing, visually-dominant omnivores. Cows are 4 limbed - we share the same genes which switch on in embryonic development which cause 4 limbs to develop. We *also* share these with fish - after all, these are the genes which were first used to make fins, they were just 'repurposed' to make limbs through mutation and natural selection.

And so on with all 30,000 genes that make us human. We're not even genetically the best at doing many things in the animal kingdom - plants 'eat' sunshine, some bacteria detoxify alcohol better than we can, and as for our radiation susceptibility, we're pathetic. We just so happen to carry the baggage of every creature that came before us that was able to reproduce.",null,3,cdk283p,1r54d1,askscience,top_week,10
knobtwiddler,"I work in genetic informatics and we sequence and analyze human genomes.  ""complete mapping,"" rather optimistically, means is that we have assembled a reference genome of a number of pooled humans' gene sequences, so we know where a typical human's sequences fall in the chromosomes from beginning to end (around 50 billion base pairs).  This assembly is used as a reference to compare against.  Currently we are using a reference genome sequence called HG19. HG20 (human genome v20) is coming
out soon.   It's an ongoing process.


From this reference genome we can align pieces of sequenced dna from samples in an effort to to say where those pieces of dna came from in the genome.


This is far from an exact science, and there are large portions of the genome for which we have no clue about their function.  However we have identified around 56,000 protein-coding genes (the exome) and a large number of ""intronic"" non-protein-coding regions which do code for RNA (lncRNA), some of which are functional, most of which we don't know anything about (previously referred to as ""junk dna""). 


believe me though, as far as understanding the function of all these genes, let alone the non-coding regions, the process is far from complete.",null,0,cdjqpyk,1r54d1,askscience,top_week,7
BillieHayez,"How interesting that you ask this question today. Fred Sanger, a pioneer in the mapping of the human genome, aged 95, and winner of two Nobel prizes has just passed. Maybe you were tuned in this morning, as well.",null,0,cdjqzu4,1r54d1,askscience,top_week,6
jams2014,"Think of the Genome like the spec sheet for a car, except it's been broken up into 46 text files and compressed so that the data is all mashed together into 46 strings, and somewhat difficult to parse out. Somebody didn't comment their code. If we were just trying to read the strings, and infer what they mean, we would fail. But luckily! there's also an automatic, computer-controlled factory that reads the strings and builds stuff! (Cells in the body.)

In the simplest sense, genome mapping is about making the factory build from *parts* of these strings, so that we can see what they do.  Imagine that you run your fictional automatic car factory like normal - it builds you a hot little red Corvette. Now imagine that you take part of the instruction string and copy/paste/copy/paste that part until you've made that section repeat a bunch of times. When you run the factory again, the car comes out a *deep, vivid red* instead of the ordinary red from before. 

You've found a gene for the paintjob, but you don't know for sure whether you've found the gene for red paint only, or for the whole thing. Now, that section might be a little bit different in someone else - like, maybe it's a different color. If you enhanced that section in someone else's instruction sheet, maybe you'd go from blue to a more vivid blue (if all of the color selection is in that part). Or maybe you would just add red, so that someone's purple paint would approach pink.

Anyway, what you've found is the meaning of a section of the instruction sheet, but it can be difficult to determine exactly which of the machines are activated by each string. Sometimes the instructions trigger other instructions, and wind up causing lots of parts to move. Sometimes they trigger something very tiny - like spinning a part of one machine. And sometimes they don't do anything at all (like bits of commented-out code). And sometimes they do something, but don't appear to unless certain conditions are met - imagine instructions to turn on or off some safety feature on the factory floor.

- EDIT - 

To perfect the analogy - we're not talking here about running the whole apparatus to create new cars. That would be like making changes to an embryo's genes and letting them grow up, which is unethical.

It's more like flipping switches in the factory while the assembly line is down, just to see which machines start to spin, or spray paint. ",null,1,cdjw201,1r54d1,askscience,top_week,4
tsacian,"The best way to understand what scientists are doing with the human genome, it is best to look at a much smaller and simpler genome (such as the **Japanese Rice Genome Project**).  It is simpler because the rice being mapped only has 9 chromosomes, whereas humans have much more.

http://rgp.dna.affrc.go.jp/E/GenomeSeq.html

Here you can click on a chromosome and literally see the sequences which have been directly mapped.  The difference is the wealth of knowledge already learned from this project due to its ""simplicity"", such as finding genes responsible for specific proteins and tracing them all the way back to the base pair patterns.  You can search through the big discoveries, and even look for specific proteins.

Click on chromosome 1 and then click the link for the first accession.  This first set has 31,687 base pairs (bp) (think ATCG).  You can then click on a gene and see the sequence that scientists believe is responsible for a gene.  The reason it is a ""gene"" is because it has the correct properties for coding of a gene, including a start sequence (a pattern they look for that is typical for the beginning of a gene), and a stop sequence (called codons).

Additionally, you can click and see a specific pattern of base pairs responsible for coding an mRNA and even specific proteins.  Using these ""Maps"", scientists can study each chromosome and find which genes are responsible for specific attributes of the organism.  We can find which sections of DNA are responsible for specific proteins, and use that to find mutations that result in the absense or mutation of a protein that causes harm in an organism.  There is really a wealth of information.",null,0,cdjwez6,1r54d1,askscience,top_week,3
XSlayerALE,"Mapping the Human Genome is like identifying the parts of a car. Sure, a wheel can be Pirelli, Firestone, Goodyear or whot not but we know its a wheel and its not the axle or the brakes or that funny triangle sign on your dashboard that no one really knows what it does....
",null,0,cdk6q2y,1r54d1,askscience,top_week,3
futuregp,"simply speaking, think that all humans have the same genes that have specific functions (and every human being needs these to be considered human)

but each gene can have different traits (blue eyes, brown eyes etc)

complete mapping of the human genome is to identify all those functional parts of our DNA (most of our DNA is technically not 'functional' and doesn't play a part in protein synthesis)

Each functional part ('functional gene') would have different traits, and every human being is composed of permutations/combinations of these millions of gene traits combined (e.g. let's say we only have 2 genes, A/B. Gene A has 2 traits - male (m) or female (f). Gene B has 2 traits - tall (t) or short (s).

I'm a short male. I would have A(m), and B(s) genes. You are tall and female. You would have A(f), and B(t) genes.
We're both unique, but that doesn't mean you have to map both of us to realize that there are 2 genes.

By mapping a single human being, you can map all the genes of the human genome. The uniqueness comes not from which 'gene' you have but which 'trait' of the gene you have.",null,0,cdjot9c,1r54d1,askscience,top_week,3
Drfilthymcnasty,"I may be wrong, but I think a complete ""mapping"" means a complete understanding of all the functional genes in our DNA. So while we may know the general sequence of nucleotides, our understanding of how/why certain segments get translated into proteins is not yet complete. Also we still have a long way to go understanding epigenetic changes and controls.",null,3,cdjouzf,1r54d1,askscience,top_week,6
liteerl,"Each person's DNA is unique, but their DNA differs from others at certain genes. You could record the common variants of each gene (these variants are called alleles), although certainly you would be very like to miss some of the variation. Individual's DNA sequences are not all completely random, but differ from each other in predictable ways.",null,0,cdjrxb1,1r54d1,askscience,top_week,2
shanebonanno,"Everyone's DNA is unique, however, nearly all of it is shared with every human on the planet. Only a very small part is unique. When scientists talk about the genome of any given species, they basically mean a list of the genes in the DNA of the species and eventually what they do.",null,0,cdjuwzx,1r54d1,askscience,top_week,2
dreamhunters,"Or think about it this way: it is not some much about the content but about the placement. The genes are somewhere in the genome, their position is much more fixed that the genes themselves. That is why we use mapping, because as with a map it is about location. ",null,0,cdk9y90,1r54d1,askscience,top_week,2
Hillsbottom,"I am a biology teacher and I use the following analogy. 

Think of the genome as a recipe to make bread. A recipe is basically a list of instructions that need to be followed in a particular order to get the desired result. These instructions are analogous to genes. 

Bread is not all the same; you get white, brown, wholemeal granary, bananana, pumpkin etc. These differences are due to slight changes in the instructions to the recipe eg putting white flour in instead of brown. The instructions are basically the same they are just different versions of it (in genectics these are called alleles; different versions of the same gene).

What scientists have done is got lots recipes (genomes) for many differents type of bread (people, including Ozzie Osbourne!) and worked out the order the instructions (genes) go in. They have created a map of how to make a bready human.

The instructions you have as a human are almost indentical to all other humans however the the combinition of which type of instructions you have is unquie to you (with a few exceptions).

So now we have this massive recipe of how to make a human that we can compare with indivdual humans and look for difference and similarities. 

",null,4,cdkdx9l,1r54d1,askscience,top_week,4
Sherm1,"I don't think that a true complete map could ever exist, because it would have to know how each sequence of code would react to every given environmental stimulus, including what its surrounding genes might be. How do we know that some of these stretches of ""junk dna"" don't become active when bombarded by some special radioactive waves that aliens can emit from their space ships? 

Point is, we need to not only know what the sequences are, but also what they do, and what they do is always determined by their interaction with the environment.",null,1,cdjoxib,1r54d1,askscience,top_week,3
the_sex_kitten,"Although each sequence is unique, there are still common gene codes that exist in each of us. By mapping the genome, they are able to locate these codes. For example, the gene for cystic fibrosis is located [here], and since we know that we are able to specifically look [here] for that gene. CF is way more complicated than that because there are a number of different genes that can be mutated, but that's just one example. Basically it allows us to determine the relative location of where potential mutations can occur. Apologies for the lack of sources and simplicity in my response. And please anyone feel free to correct me if I'm wrong!",null,1,cdjrcly,1r54d1,askscience,top_week,2
smfdeivis,"Only around 0.1% of the DNA between humans is different! So 99.9% genomic human DNA is the same. That 0.1% accounts for observable characteristics (phenotypes) like hair,eye, skin colors, and many others. Complete mapping of the human genome is basically mapping these conserved 99.9% of the DNA which codes for various essential peptides that make up proteins that give rise to tissues. There is a new project on the way called, ""the real human genome project"" Prof. Erick Lander gave a great summary of it on youtube!",null,0,cdjsquj,1r54d1,askscience,top_week,1
civilizedanimal,"This really depends on what definition you are using. Strictly speaking, mapping a genome is marking out where genes are located on the chromosome. Again, we are talking genes, or chunks of DNA that code for something. Most frequently, when people talk about mapping the human genome what they are actually referring to is sequencing the human genome. Sequencing the human genome is simply recording the sequence of nucleotides in a complete set of human DNA. They do this by sequencing more than one person's DNA and then averaging it. In order to map the genes, they would need to do a lot more research. When we finally get all the genes mapped, we will know what portions of human chromosome code for something. Even after mapping out all the genes it still takes a long time before you can determine what genes code for what.",null,0,cdjtir5,1r54d1,askscience,top_week,1
DLove82,"Mapping tells us the relative location of stretches of DNA that actually encode something (genes). This arrangement is very very similar between individuals (rarely, duplication, deletion, or transposition events can add, move, or delete a region of DNA, but that is uncommon), even if the genes themselves differ slightly on occasion. The genes are arranged in a group of 23 different unique chromosomes, or HUUUUGE stretches of DNA that are wrapped up really tight.

Mapping tell us the location of one gene relative to another in one dimension (along a line). (EDIT: 3-dimensional genome sequence is all the rage now - it actually looks in 3D at which stretches of DNA are in contact or close to which others - this is very important because those local interactions between genes REALLY far away have turned out to really impact gene function) Each of these genes is composed of a sequence of building blocks, or nucleotides, of which there are four - A, T, C, G (each is a slightly different molecule). The sequence of these nucleotides in a gene determine almost everything about its function - when it turns on and off, what it makes, what cells it's active in. Between individuals, the sequence of these genes is nearly identical, because the products of most genes (proteins) only function if they are composed of precisely the correct sequence of molecules (amino acids). Some, however, can work to varying degrees when the sequences are slightly different. If these occur in more than 1% of the population, they're called ""polymorphisms."" If they occur in less than 1% of the population, they're regarded as ""mutant"" forms of a ""wild-type"" (or normal) gene. 

So, in fact, mapping a bunch of individuals genomes actually allows researchers to come up with a heat map of the building block changes that occur in individuals. Genomic mapping is actually what tells us specifically what areas of the genome are unique between individuals. This can be immensely helpful in disease research where large regions of chromosomes are duplicated, lost, or moved. By mapping genomes, we can say which genes specifically are lost in a certain disease, narrowing down the number of genes which might cause the disease. For example, Down syndrome is caused by an entire extra copy of a chromosome (I think it's 21). That means these individuals have an extra copy of ALL the genes on that chromosome. And since we've mapped where all the genes in the genome are, we can identify which genes might be involved in Down syndrome (this is just an example, it's not really all that practical since the chromosome encodes THOUSANDS of genes).

tl;dr: The unique components of a person's genome are very few relative to the HUGE size and homogeneity (""sameness"") of the genome as a whole between individuals. For the most part, we all have the same number of chromosomes, each with the same number of genes in the same orientation. Complete mapping of the human genome allows us to build up a heat map of the few little areas where genes actually are unique, and see how common those changes are; if they're associated with disease, etc.",null,0,cdjtw4g,1r54d1,askscience,top_week,1
SMURGwastaken,"It means we've sequenced all of a person's DNA and worked out what each part codes for - whether it be amelase for digesting simple carbohydrates or amelogenins for producing tooth enamel, or the homeobox genes for deciding which organs and body sections go where. Since all humans are essentially identical in terms of how they work, all humans will have the genes for these things. Only about 0.1% of your genes are different to another human, and you'd be surprised at how little the difference between you and any other vertebrate (or even any other eukaryotic organism) is.",null,0,cdjuvtu,1r54d1,askscience,top_week,1
EvOllj,"There are differences on individual DNA that get completely ignored/lost when they are read, because the reading mechanism is very error tolerant. And there ate a LOT of differences that never get read.

And the differences in appearances are so small compared to the whole genome, that the genome of all humans is basically the same, all genes do the same thing, some are just more active and rarely a few barely important genes are disabled or damaged.",null,0,cdk5fgr,1r54d1,askscience,top_week,1
zmil,"Think of the human genome like a really long set of beads on a string. About 3 billion beads, give or take. The beads come in four colors. We'll call them bases. When we sequence a genome, we're finding out the sequence of those bases on that string. 

Now, in any given person, the sequence of bases will in fact be unique, but unique doesn't mean completely different. In fact, if you lined up the sequences from any two people on the planet, something like 99% of the bases would be the same. You would see long stretches of identical bases, but every once in a while you'd see a mismatch, where one person has one color and one person has another. In some spots you might see bigger regions that don't match at all, sometimes hundreds or thousands of bases long, but in a 3 billion base sequence they don't add up to much. 

edit 2: I was wrong, it ain't a consensus, it's a mosaic! I had always assumed that when they said the reference genome was a combination of sequences from multiple people, that they made a consensus sequence, but in fact, any given stretch of DNA sequence in the reference comes from a single person. They combined stretches form different people to make the whole genome. TIL the reference genome is even crappier than I thought. They are planning to change it to something closer to a real consensus in the very near future. My explanation of consensus sequences below was just ahead of its time! But it's definitely not how they produced the original genome sequence.

If you line up a bunch of different people's genome sequences, you can compare them all to each other. You'll find that the vast majority of beads in each sequence will be the same in everybody, but, as when we just compared two sequences, we'll see differences. Some of those differences will be unique to a single person- everybody else has one color of bead at a certain position, but this guy has a different color. Some of the differences will be more widespread, sometimes half the people will have a bead of one color, and the other half will have a bead of another color. What we can do with this set of lined up sequences is create a *consensus* sequence, which is just the most frequent base at every position in that 3 billion base sequence alignment. And that is basically what they did in the initial mapping of the human genome. That consensus sequence is known as the reference genome. When other people's genomes are sequenced, we line them up to the reference genome to see all the differences, in the hope that those differences will tell us something interesting. 

As you can see, however, the reference genome is just an average genome*; it doesn't tell us anything about all the differences between people. That's the job of a lot of other projects, many of them ongoing, to sequence lots and lots of people so we can know more about what differences are present in people, and how frequent those differences are. One of those studies is the 1000 Genomes Project, which, as you might guess, is sequencing the genomes of a thousand (well, more like two thousand now I think) people of diverse ethnic backgrounds. 

*It's not even a very good average, honestly. They only used 8 people (edit: 7, originally, and the current reference uses 13.), and there are spots where the reference genome sequence doesn't actually have the most common base in a given position. Also, there are spots in the genome that are extra hard to sequence, long stretches where the sequence repeats itself over and over; many of those stretches have not yet been fully mapped, and possibly never will be.

edit 1: I should also add that, once they made the reference sequence, there was still work to be done- a lot of analysis was performed on that sequence to figure out where genes are, and what those genes do. We already knew the sequence of many human genes, and often had a rough idea of their position on the genome, but sequencing the entire thing allowed us to see exactly where each gene was on each chromosome, what's nearby, and so on. In addition to confirming known sequences, it allowed scientists to predict the presence of many previously unknown genes, which could then be studied in more detail. Of course, 98% of the genome *isn't* genes, and they sequenced that as well -some scientists thought this was a waste of time, but I'm grateful the genome folks ignored them, because that 98% is what I study, and there's all sorts of cool stuff in there, like ancient viral sequences and whatnot.

edit 3: Thanks for the gold! Funny, this is the second time I've gotten gold, and both times it's been for a post that turned out to be wrong, or partly wrong anyway...oh well.",null,198,cdjoort,1r54d1,askscience,top_week,1053
Chl0eeeeeee,"Even though everyone has unique DNA, genes still would occur in the same location in the genome (exclusive of any mutations that would add/delete a nucleotide). Basically what genome mapping does is look at multiple samples of DNA from different people. It aims to understand what regions are coding versus non-coding, and to annotate the genome (see what the coding genes control). This has been done for other species. ",null,17,cdjopps,1r54d1,askscience,top_week,77
Tass237,"Complete mapping of what sections apply to what.  A redhead and a blonde both have a gene for hair color, and the location of that hair color gene can be mapped.  The fact that they have *different alleles* doesn't mean it's a different gene or in a different location.",null,7,cdjq7af,1r54d1,askscience,top_week,29
zedrdave,"In addition to other answers in this thread, one important clarification: when one says that a person's DNA is unique, that's still no more than somewhere around a 0.01% difference, out of the entire sequence, between two individuals.

Most nucleotides (the small bricks that make the DNA sequence) are the same for all individual of the same species (humans, for instance), with a very few single nucleotides changing here and there (these changes are called [SNPs](http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism)). Just the same way that moving a single cog in a complex mechanism, or modifying a single byte in a computer program, will give out a completely different result, that single nucleotide modification can have huge consequences on the person's appearance, health etc.

Mapping the first genome, meant mapping *a* genome (with its specific SNPs), with the implicit idea that we were first interested in the parts that were common to everybody. Now that sequencing is a lot cheaper and more widespread, there are a number of efforts to map genomes for a number of individuals, in order to figure out more specifically which positions in the sequence can occasionally differ (see ""1000 genome project"").

**Edit:** I should have also mentioned that, while some SNP variations have huge effects on the resulting organism, other SNP mutations are completely silent (""synonymous mutations""), thanks to the redundancy of the DNA-Amino Acid transcription code (i.e. different triplets of DNA can end up coding for the same AA). Because such silent mutations do not affect fitness (and therefore are more likely to be passed down), they are a lot more common than you would expect from pure chance.",null,1,cdjud86,1r54d1,askscience,top_week,9
tdcarlo,"Each person's DNA is unique, that is true.  But the difference between you an me is incredibly small.  

DNA is made up of nucleotides.  There are four kinds of nucleotides.  Think of nucleotides as legos each kind being a different color....let's say Aqua, Green, Cyan, and Teal.  A gene is composed of nucleotides in particular order.  Imagine stacking legos.  Using the first letter of the colors from the legos, the insulin gene is 450 nucleotides long and looks like this.

Aqua Green Cyan Cyan Cyan Teal Cyan Aqua GGACAGGCTGCATCAGAAGAGGCCATCAAGCAGATCACTGTCC
TTCTGCCATGGCCCTGTGGATGCGCCTCCTGCCCCTGCTGGCGCTGCTGGCCCTCTGGGGACCTGACCCAGCCGCAGCCTTTGTGAACCAACACCTGTGCGGCTCACACCTGGTGGAAGCTCTCTACCTAGTGTGCGGGGAACGAGGCTTCTTCTACACACCCAAGACCCGCCGGGAGGCAGAGGACCTGCAGGTGGGGCAGGTGGAGCTGGGCGGGGGCCCTGGTGCAGGCAGCCTGCAGCCCTTGGCCCTGGAGGGGTCCCTGCAGAAGCGTGGCATTGTGGAACAATGCTGTACCAGCATCTGCTCCCTCTACCAGCTGGAGAACTACTGCAACTAGACGCAGCCCGCAGGCAGCCCCACACCCGCCGCCTCCTGCACCGAGAGAGATGGAATAAAGCCCTTGAACCAGCAAAA

So we know what a gene is...the next thing to understand is a chromosome.  A chromosome is a long stack of DNA that contains numerous genes.  There are 23 chromosomes in the human genome.  The longest human chromosome is about 250 million nucleotides long the shortest is around 50 million nucleotides.  Each chromosome contains hundreds of genes along with some other ""accessory"" DNA that is beyond the scope of this explanation.    The entire size of the human genome is around 3 billion nucleotides.

Human being the clever types have been able to determine the precise order of all of the nucleotides in each human chromosome and have identified most if not all of the genes on it.  So each chromosome has the location of each gene mapped. Pretty amazing.

Your DNA is unique but the percentage of the 3 billion nucleotides that are different than mine is less than 0.0001% and most of the differences will be in the so called ""accessory"" DNA.  ",null,0,cdjtoip,1r54d1,askscience,top_week,8
nanoakron,"I feel the need to write this because whilst all the previous commenters have gone into great depths to explain the science behind genes and genomes, they have failed to address a fundamental misunderstanding the OP has:

Your DNA is **NOT** unique. Only about 0.1% of it is. You are somewhere around 99.5-99.9% **genetically identical to every other human on the planet**.

You're also 98.8% **identical** to every chimpanzee, 98.4% **identical** to every gorilla, 88% to every mouse, 65% to each chicken and 47% genetically **identical** to a fruit fly.

This means you have the exact same codes (give or take a letter) for the most essential 'housekeeping' functions - the ones that process energy in your cells, allow your cells to reproduce, build cell walls, cell skeletons and the other basic stuff all multicellular life needs to do. As a side note, this is very strong evidence that these abilities evolved only once in a distant ancestor, and then because they were so successful compared to all species around at *their* time, they outcompeted them and all their descendants now share those genes.

The closer you get to a human in genetic relatedness, the similarities extend beyond simple housekeeping genes to those which allow us to be 4-limbed, air-breathing, visually-dominant omnivores. Cows are 4 limbed - we share the same genes which switch on in embryonic development which cause 4 limbs to develop. We *also* share these with fish - after all, these are the genes which were first used to make fins, they were just 'repurposed' to make limbs through mutation and natural selection.

And so on with all 30,000 genes that make us human. We're not even genetically the best at doing many things in the animal kingdom - plants 'eat' sunshine, some bacteria detoxify alcohol better than we can, and as for our radiation susceptibility, we're pathetic. We just so happen to carry the baggage of every creature that came before us that was able to reproduce.",null,3,cdk283p,1r54d1,askscience,top_week,10
knobtwiddler,"I work in genetic informatics and we sequence and analyze human genomes.  ""complete mapping,"" rather optimistically, means is that we have assembled a reference genome of a number of pooled humans' gene sequences, so we know where a typical human's sequences fall in the chromosomes from beginning to end (around 50 billion base pairs).  This assembly is used as a reference to compare against.  Currently we are using a reference genome sequence called HG19. HG20 (human genome v20) is coming
out soon.   It's an ongoing process.


From this reference genome we can align pieces of sequenced dna from samples in an effort to to say where those pieces of dna came from in the genome.


This is far from an exact science, and there are large portions of the genome for which we have no clue about their function.  However we have identified around 56,000 protein-coding genes (the exome) and a large number of ""intronic"" non-protein-coding regions which do code for RNA (lncRNA), some of which are functional, most of which we don't know anything about (previously referred to as ""junk dna""). 


believe me though, as far as understanding the function of all these genes, let alone the non-coding regions, the process is far from complete.",null,0,cdjqpyk,1r54d1,askscience,top_week,7
BillieHayez,"How interesting that you ask this question today. Fred Sanger, a pioneer in the mapping of the human genome, aged 95, and winner of two Nobel prizes has just passed. Maybe you were tuned in this morning, as well.",null,0,cdjqzu4,1r54d1,askscience,top_week,6
jams2014,"Think of the Genome like the spec sheet for a car, except it's been broken up into 46 text files and compressed so that the data is all mashed together into 46 strings, and somewhat difficult to parse out. Somebody didn't comment their code. If we were just trying to read the strings, and infer what they mean, we would fail. But luckily! there's also an automatic, computer-controlled factory that reads the strings and builds stuff! (Cells in the body.)

In the simplest sense, genome mapping is about making the factory build from *parts* of these strings, so that we can see what they do.  Imagine that you run your fictional automatic car factory like normal - it builds you a hot little red Corvette. Now imagine that you take part of the instruction string and copy/paste/copy/paste that part until you've made that section repeat a bunch of times. When you run the factory again, the car comes out a *deep, vivid red* instead of the ordinary red from before. 

You've found a gene for the paintjob, but you don't know for sure whether you've found the gene for red paint only, or for the whole thing. Now, that section might be a little bit different in someone else - like, maybe it's a different color. If you enhanced that section in someone else's instruction sheet, maybe you'd go from blue to a more vivid blue (if all of the color selection is in that part). Or maybe you would just add red, so that someone's purple paint would approach pink.

Anyway, what you've found is the meaning of a section of the instruction sheet, but it can be difficult to determine exactly which of the machines are activated by each string. Sometimes the instructions trigger other instructions, and wind up causing lots of parts to move. Sometimes they trigger something very tiny - like spinning a part of one machine. And sometimes they don't do anything at all (like bits of commented-out code). And sometimes they do something, but don't appear to unless certain conditions are met - imagine instructions to turn on or off some safety feature on the factory floor.

- EDIT - 

To perfect the analogy - we're not talking here about running the whole apparatus to create new cars. That would be like making changes to an embryo's genes and letting them grow up, which is unethical.

It's more like flipping switches in the factory while the assembly line is down, just to see which machines start to spin, or spray paint. ",null,1,cdjw201,1r54d1,askscience,top_week,4
tsacian,"The best way to understand what scientists are doing with the human genome, it is best to look at a much smaller and simpler genome (such as the **Japanese Rice Genome Project**).  It is simpler because the rice being mapped only has 9 chromosomes, whereas humans have much more.

http://rgp.dna.affrc.go.jp/E/GenomeSeq.html

Here you can click on a chromosome and literally see the sequences which have been directly mapped.  The difference is the wealth of knowledge already learned from this project due to its ""simplicity"", such as finding genes responsible for specific proteins and tracing them all the way back to the base pair patterns.  You can search through the big discoveries, and even look for specific proteins.

Click on chromosome 1 and then click the link for the first accession.  This first set has 31,687 base pairs (bp) (think ATCG).  You can then click on a gene and see the sequence that scientists believe is responsible for a gene.  The reason it is a ""gene"" is because it has the correct properties for coding of a gene, including a start sequence (a pattern they look for that is typical for the beginning of a gene), and a stop sequence (called codons).

Additionally, you can click and see a specific pattern of base pairs responsible for coding an mRNA and even specific proteins.  Using these ""Maps"", scientists can study each chromosome and find which genes are responsible for specific attributes of the organism.  We can find which sections of DNA are responsible for specific proteins, and use that to find mutations that result in the absense or mutation of a protein that causes harm in an organism.  There is really a wealth of information.",null,0,cdjwez6,1r54d1,askscience,top_week,3
XSlayerALE,"Mapping the Human Genome is like identifying the parts of a car. Sure, a wheel can be Pirelli, Firestone, Goodyear or whot not but we know its a wheel and its not the axle or the brakes or that funny triangle sign on your dashboard that no one really knows what it does....
",null,0,cdk6q2y,1r54d1,askscience,top_week,3
futuregp,"simply speaking, think that all humans have the same genes that have specific functions (and every human being needs these to be considered human)

but each gene can have different traits (blue eyes, brown eyes etc)

complete mapping of the human genome is to identify all those functional parts of our DNA (most of our DNA is technically not 'functional' and doesn't play a part in protein synthesis)

Each functional part ('functional gene') would have different traits, and every human being is composed of permutations/combinations of these millions of gene traits combined (e.g. let's say we only have 2 genes, A/B. Gene A has 2 traits - male (m) or female (f). Gene B has 2 traits - tall (t) or short (s).

I'm a short male. I would have A(m), and B(s) genes. You are tall and female. You would have A(f), and B(t) genes.
We're both unique, but that doesn't mean you have to map both of us to realize that there are 2 genes.

By mapping a single human being, you can map all the genes of the human genome. The uniqueness comes not from which 'gene' you have but which 'trait' of the gene you have.",null,0,cdjot9c,1r54d1,askscience,top_week,3
Drfilthymcnasty,"I may be wrong, but I think a complete ""mapping"" means a complete understanding of all the functional genes in our DNA. So while we may know the general sequence of nucleotides, our understanding of how/why certain segments get translated into proteins is not yet complete. Also we still have a long way to go understanding epigenetic changes and controls.",null,3,cdjouzf,1r54d1,askscience,top_week,6
liteerl,"Each person's DNA is unique, but their DNA differs from others at certain genes. You could record the common variants of each gene (these variants are called alleles), although certainly you would be very like to miss some of the variation. Individual's DNA sequences are not all completely random, but differ from each other in predictable ways.",null,0,cdjrxb1,1r54d1,askscience,top_week,2
shanebonanno,"Everyone's DNA is unique, however, nearly all of it is shared with every human on the planet. Only a very small part is unique. When scientists talk about the genome of any given species, they basically mean a list of the genes in the DNA of the species and eventually what they do.",null,0,cdjuwzx,1r54d1,askscience,top_week,2
dreamhunters,"Or think about it this way: it is not some much about the content but about the placement. The genes are somewhere in the genome, their position is much more fixed that the genes themselves. That is why we use mapping, because as with a map it is about location. ",null,0,cdk9y90,1r54d1,askscience,top_week,2
Hillsbottom,"I am a biology teacher and I use the following analogy. 

Think of the genome as a recipe to make bread. A recipe is basically a list of instructions that need to be followed in a particular order to get the desired result. These instructions are analogous to genes. 

Bread is not all the same; you get white, brown, wholemeal granary, bananana, pumpkin etc. These differences are due to slight changes in the instructions to the recipe eg putting white flour in instead of brown. The instructions are basically the same they are just different versions of it (in genectics these are called alleles; different versions of the same gene).

What scientists have done is got lots recipes (genomes) for many differents type of bread (people, including Ozzie Osbourne!) and worked out the order the instructions (genes) go in. They have created a map of how to make a bready human.

The instructions you have as a human are almost indentical to all other humans however the the combinition of which type of instructions you have is unquie to you (with a few exceptions).

So now we have this massive recipe of how to make a human that we can compare with indivdual humans and look for difference and similarities. 

",null,4,cdkdx9l,1r54d1,askscience,top_week,4
Sherm1,"I don't think that a true complete map could ever exist, because it would have to know how each sequence of code would react to every given environmental stimulus, including what its surrounding genes might be. How do we know that some of these stretches of ""junk dna"" don't become active when bombarded by some special radioactive waves that aliens can emit from their space ships? 

Point is, we need to not only know what the sequences are, but also what they do, and what they do is always determined by their interaction with the environment.",null,1,cdjoxib,1r54d1,askscience,top_week,3
the_sex_kitten,"Although each sequence is unique, there are still common gene codes that exist in each of us. By mapping the genome, they are able to locate these codes. For example, the gene for cystic fibrosis is located [here], and since we know that we are able to specifically look [here] for that gene. CF is way more complicated than that because there are a number of different genes that can be mutated, but that's just one example. Basically it allows us to determine the relative location of where potential mutations can occur. Apologies for the lack of sources and simplicity in my response. And please anyone feel free to correct me if I'm wrong!",null,1,cdjrcly,1r54d1,askscience,top_week,2
smfdeivis,"Only around 0.1% of the DNA between humans is different! So 99.9% genomic human DNA is the same. That 0.1% accounts for observable characteristics (phenotypes) like hair,eye, skin colors, and many others. Complete mapping of the human genome is basically mapping these conserved 99.9% of the DNA which codes for various essential peptides that make up proteins that give rise to tissues. There is a new project on the way called, ""the real human genome project"" Prof. Erick Lander gave a great summary of it on youtube!",null,0,cdjsquj,1r54d1,askscience,top_week,1
civilizedanimal,"This really depends on what definition you are using. Strictly speaking, mapping a genome is marking out where genes are located on the chromosome. Again, we are talking genes, or chunks of DNA that code for something. Most frequently, when people talk about mapping the human genome what they are actually referring to is sequencing the human genome. Sequencing the human genome is simply recording the sequence of nucleotides in a complete set of human DNA. They do this by sequencing more than one person's DNA and then averaging it. In order to map the genes, they would need to do a lot more research. When we finally get all the genes mapped, we will know what portions of human chromosome code for something. Even after mapping out all the genes it still takes a long time before you can determine what genes code for what.",null,0,cdjtir5,1r54d1,askscience,top_week,1
DLove82,"Mapping tells us the relative location of stretches of DNA that actually encode something (genes). This arrangement is very very similar between individuals (rarely, duplication, deletion, or transposition events can add, move, or delete a region of DNA, but that is uncommon), even if the genes themselves differ slightly on occasion. The genes are arranged in a group of 23 different unique chromosomes, or HUUUUGE stretches of DNA that are wrapped up really tight.

Mapping tell us the location of one gene relative to another in one dimension (along a line). (EDIT: 3-dimensional genome sequence is all the rage now - it actually looks in 3D at which stretches of DNA are in contact or close to which others - this is very important because those local interactions between genes REALLY far away have turned out to really impact gene function) Each of these genes is composed of a sequence of building blocks, or nucleotides, of which there are four - A, T, C, G (each is a slightly different molecule). The sequence of these nucleotides in a gene determine almost everything about its function - when it turns on and off, what it makes, what cells it's active in. Between individuals, the sequence of these genes is nearly identical, because the products of most genes (proteins) only function if they are composed of precisely the correct sequence of molecules (amino acids). Some, however, can work to varying degrees when the sequences are slightly different. If these occur in more than 1% of the population, they're called ""polymorphisms."" If they occur in less than 1% of the population, they're regarded as ""mutant"" forms of a ""wild-type"" (or normal) gene. 

So, in fact, mapping a bunch of individuals genomes actually allows researchers to come up with a heat map of the building block changes that occur in individuals. Genomic mapping is actually what tells us specifically what areas of the genome are unique between individuals. This can be immensely helpful in disease research where large regions of chromosomes are duplicated, lost, or moved. By mapping genomes, we can say which genes specifically are lost in a certain disease, narrowing down the number of genes which might cause the disease. For example, Down syndrome is caused by an entire extra copy of a chromosome (I think it's 21). That means these individuals have an extra copy of ALL the genes on that chromosome. And since we've mapped where all the genes in the genome are, we can identify which genes might be involved in Down syndrome (this is just an example, it's not really all that practical since the chromosome encodes THOUSANDS of genes).

tl;dr: The unique components of a person's genome are very few relative to the HUGE size and homogeneity (""sameness"") of the genome as a whole between individuals. For the most part, we all have the same number of chromosomes, each with the same number of genes in the same orientation. Complete mapping of the human genome allows us to build up a heat map of the few little areas where genes actually are unique, and see how common those changes are; if they're associated with disease, etc.",null,0,cdjtw4g,1r54d1,askscience,top_week,1
SMURGwastaken,"It means we've sequenced all of a person's DNA and worked out what each part codes for - whether it be amelase for digesting simple carbohydrates or amelogenins for producing tooth enamel, or the homeobox genes for deciding which organs and body sections go where. Since all humans are essentially identical in terms of how they work, all humans will have the genes for these things. Only about 0.1% of your genes are different to another human, and you'd be surprised at how little the difference between you and any other vertebrate (or even any other eukaryotic organism) is.",null,0,cdjuvtu,1r54d1,askscience,top_week,1
EvOllj,"There are differences on individual DNA that get completely ignored/lost when they are read, because the reading mechanism is very error tolerant. And there ate a LOT of differences that never get read.

And the differences in appearances are so small compared to the whole genome, that the genome of all humans is basically the same, all genes do the same thing, some are just more active and rarely a few barely important genes are disabled or damaged.",null,0,cdk5fgr,1r54d1,askscience,top_week,1
S7R4nG3,"You should check out [Newton's Law of Cooling](http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html). Its a fairly simple differential equation that has solutions as an exponential increase or decay that, in your case, would be dependent on the thermal properties of the cups.

You end up with an equation like:
T (t) = Ta + (To - Ta) e ^ -kt. 

-Where Ta is the surrounding temp. of the room.

-To is the initial temp. of the water in the cup.

-k is a unitless constant based on the thermal properties of the cup. The negative value is due to the solution of the original differential equation.

-t is the amount of time the cups are left in the room.

-T(t) is the temp. the cup falls to after time t, has elapsed.

Obviously to find one value, you would need to know the other three.

Edit: I can't read the link I posted.
",null,1,cdjrze0,1r53ot,askscience,top_week,4
SonOfOnett,"Since it's equally valid to model the situation as heat dispersing or cold dispersing with the diffusion equation, in an ideal situation they will both reach room temperature at the same time.

In actuality the thermal conductivity of the water and cup will actually vary with temperature, meaning that it might be easier to transfer heat into or out of the cup at higher/lower temperature. Therefore the answer actually depends on the equilibrium temperature you are trying to reach.

Another complicating factor is phase transitions, but I think you want both the cups to be full of liquid water at and the room to be at standard temperature and pressure.
",null,4,cdjt7hb,1r53ot,askscience,top_week,5
cmuadamson,"The amount of energy required to change the temp of a liquid by 1 degree does depend on the starting temperature.  Imagine you have a pipe from a steam plant that it used to heat buildings (This is a real scenario, there are underground steam pipes all over Pittsburgh, to the hospitals, universities, etc)  Suppose the steam starts at 500 degrees and goes into building 1, comes out at 400 degrees, then goes into building 2, and comes out at 300 degrees.

Both customers seemed to take 100 degrees out of the steam. If you bill customer 1 the same as customer 2, you're cheating customer 2, because 1 got more energy out of the steam. This is the ""enthalpy"" of the steam. No, I did not *mithpell* that, it is [enthalpy](http://en.wikipedia.org/wiki/Enthalpy).

Similarly, if you look at the energy difference between room temp and your cold cup and hot cup, the hot cup has a bigger enthalpy difference, and has to radiate more energy to get to room temp  than the cold cup has to absorb to get to room temp. So I would say the hot cup will take longer.
",null,0,cdjx5gp,1r53ot,askscience,top_week,1
null,null,null,2,cdjqppn,1r53ot,askscience,top_week,1
wishfulthinkin,"Clots are created by platelets and fibrinogen, which form fibrin, a web of solids that clots consist of.  Normally, this happens when blood pools at the location of a cut, forming a scab to prevent infection and allow for the formation of new skin or scar tissue.  The fibrin cannot be formed when blood is circulating at normal speeds, as any beginnings of a web will be destroyed by the moving fluids around it.


This has many benefits but some disastrous consequences.  If our blood did not behave in this way, every time we got a cut (on our skin, or internally, anywhere) we would slowly bleed out until we died, if we didn't die from infection first.  However, sometimes this will result in fatal blood clots, aneurysms, etc.",null,2,cdjwdjx,1r52wz,askscience,top_week,5
abstrusey,"Veterinaran here: I know of no evidence to support that idea.

I was unable to find the source, but there have been some instances of different species that were so phenotypically similar (I believe the distinction was visual patterns in birds or moths) that they confused other non-same species as a member of their own species. It resulted in sufficient unsuccessful matings that one of the species was ultimately wiped out (or could no longer be found). 

Ultimately, it begs the question of defining ""mating preferences"" more clearly. Not many species mate for pleasure. Also, it appears that they are biologically driven to reproduce because they are wired that way, but it's not evident that they have any conscious ""thought"" in the decision of whether to mate or not to mate with another animal.",null,0,cdl10a3,1r52dn,askscience,top_week,3
abstrusey,"Veterinaran here: I know of no evidence to support that idea.

I was unable to find the source, but there have been some instances of different species that were so phenotypically similar (I believe the distinction was visual patterns in birds or moths) that they confused other non-same species as a member of their own species. It resulted in sufficient unsuccessful matings that one of the species was ultimately wiped out (or could no longer be found). 

Ultimately, it begs the question of defining ""mating preferences"" more clearly. Not many species mate for pleasure. Also, it appears that they are biologically driven to reproduce because they are wired that way, but it's not evident that they have any conscious ""thought"" in the decision of whether to mate or not to mate with another animal.",null,0,cdl10a3,1r52dn,askscience,top_week,3
chrisbaird,"Spacetime itself contracts and not just the objects in spacetime. The size of objects, atoms, etc. *relative to their own spacetime* stays constant. In other words, if a spaceship contracts in half due to high velocity, a ruler in the rest frame of the spaceship will also contract in half, so measuring the length of the spaceship using that ruler will give you the same number, no matter what frame you are in. This is important, because the laws of physics must be the same in all frames even despite relativistic effects. ",null,1,cdjt5fg,1r513d,askscience,top_week,9
NGC2467,"A Planck length is just a unit of measurement. It has no special significance. (Quantum field theory is often said to ""break down"" at the Planck length, but you could just as easily say it breaks down at two times the Planck length, or pi times the Planck length, or 0.12345 times the Planck length) What is a Planck length-long stick in a comoving frame will not be a Planck length-long stick in another, the same way that a meter-long stick will not be a meter-long stick in a different reference frame.",null,3,cdjtr82,1r513d,askscience,top_week,6
KerSan,"First, I want to be clear about a misconception you share with many professionals. Objects do not appear to shrink at relativistic speeds so much as they rotate in space-time. The ""Terrell rotation"" is pretty well explained by [this YouTube video](http://www.youtube.com/watch?v=JQnHTKZBTI4) (bad quality, sorry). Other replies mention this.

As for your question about the effect on various particles, you should remember that elementary particles are point-like -- they don't really have an extent in space beyond the spread of their position-basis wavefunction. The position-basis wavefunction can indeed undergo relativistic effects, but the more important consideration is the energy of the particles.

Finally, your question about the Planck length. The Planck length is just a unit of distance, like a meter or an inch. If you run really fast, do you think that the inch changes? No, it's a definition. The number three also doesn't change if you run really fast, either. The Planck length is an abstract quantity, not a physical thing.",null,1,cdk2gae,1r513d,askscience,top_week,2
null,null,null,4,cdjqxe2,1r513d,askscience,top_week,3
then_and_again,"It's most likely due to the timed expression of different genes. Not all genes are expressed at once; at certain points your body will activate genes that code for the expression of, say, a mustache or increased bone growth, etc. Most likely the hazel and dark brown genes were not being expressed until you hit a certain age. Since brown hair and hazel are caused by increased melatonin expression you wouldnt even need to 'turn off' the blond/blue gene, just upregulate the melatonin gene. Hope that made sense...",null,0,cdjuk2g,1r5110,askscience,top_week,1
then_and_again,"bad breath is caused by bacteria in your mouth. One of the mouth's best ways of cleaning itself is saliva flushing, where saliva carries the bacteria down your throat and into the stomach. When you sleep, saliva production generally slows down. No saliva, no cleaning, and therefore, bad breath.",null,0,cdjub41,1r509m,askscience,top_week,12
SigmaStigma,"The basis of electrogenesis is derived from a salt pump. These pumps generate ion gradients with an electrical potential. It uses sodium (Na^+) and potassium (K^+) ions, which are actively pumped in or out, to generate en electrical potential. Remove K^+ and you'll get a negative charge inside of a membrane. [This article](https://en.wikipedia.org/wiki/Resting_potential) goes into depth on that topic.

Some animals have evolved mechanisms that use these ion gradients in pretty interesting ways in what are called electric organs. You can think of them as essentially batteries, which they are constantly charging. These battery-like muscles store a charge, and the animal discharges them for whatever purpose they use them for. Some use it to paralyze prey, others as defense, and others as communication and navigation.

Electric eels aren't the only ones with electric organs. [Atlantic stargazers](https://en.wikipedia.org/wiki/Atlantic_stargazer) have electric organs derived from the sonic muscles. In other fishes the sonic muscles produce vibrations on the gas bladder. The croaker, or drum, is a good example of a fish that uses sonic muscles. Another stargazer species has its electric organ derived from eye muscles.

The [elephant fish](https://en.wikipedia.org/wiki/Mormyridae) use them in very turbid, or cloudy, waters to navigate.

And on a related note, some animals have electro-recepting organs. Sharks, for example, have what are called [ampullae of Lorenzini](https://en.wikipedia.org/wiki/Ampullae_of_Lorenzini), with which they can detect the muscle movements of animals. This is helpful for burrowing in the sand or mud for food that may be hiding.",null,0,cdk3zgw,1r4zeu,askscience,top_week,5
Davecasa,"A 200 meter telescope operating at the diffraction limit would be able to resolve objects about 1 meter apart on the surface of the moon using visible light. One meter resolution makes a lunar lander about 4 effective pixels wide. Probably not quite good enough to actually confirm what you're seeing, but I'll call 200 meters a lower bound. The largest optical telescope on Earth has a 10.4 meter primary mirror.

The Lunar Reconnaissance Orbiter has a 0.195 meter primary mirror, and photographed Apollo landing sites from an altitude of 31 km. This should give it a resolution of about 10 cm, but the stated resolution at this altitude is actually 50 cm, so diffraction on the primary mirror is not directly a limiting factor on resolution. [Here's an example image of the Apollo 11 landing site](http://featured-sites.lroc.asu.edu/view_site/14).",null,1,cdjtjo8,1r4z3k,askscience,top_week,5
PorchPhysics,"I don't know about being able to see them from Earth, and I actually doubt it.  HOWEVER, i do know that the LRO (Lunar Reconnaissance Orbiter) did manage to photograph the landing site of Apollo 11 in enough detail to make out the small paths left over of the footprints surrounding the remains of the LEM as well as a small excursion that Armstrong took to a nearby crater.",null,0,cdjrmu4,1r4z3k,askscience,top_week,2
Dannei,"I would argue that globular clusters *are* part of the galaxies they're orbiting - galaxies have a lot of structure below their large-scale size! There's no real reason for the clusters to break up - like the Solar System, they are bound much more strongly to themselves than to the galaxy they are part of. They also spend much of their time very far away from anything else that is very massive, such as other clusters or molecular clouds, and so don't get disrupted. However, we do see evidence of globulars being disrupted as they pass through the disc of the MW on their orbits - these tend to form tidal streams of stars that precede/follow the cluster in its orbit, as well as ""stealing"" some.

Clusters (both globular and open) are generally thought to be made by the collapse of massive clouds of gas. These form large amounts of stars in a small volume of space, and so the stars remain gravitationally bound to each other. Specifically talking about globulars, they are suspected to have been made during the formation of the galaxy, collapsing to form stars before the rest of the Milky Way had fully formed - hence why their orbits do not resemble those of stars in the disc, which were formed at a later stage.

For reference, open clusters are those that have formed, and are still forming, in the disc - these do tend to break up quite quickly, firstly as they become unstable once the gas cloud they are forming from dissipates, and also due to gravitational interactions with other structures in the disc.",null,1,cdjmj5f,1r4ym5,askscience,top_week,5
wishfulthinkin,"You feel with your sensory neurons, and the vast majority of your sensory neurons are concentrated on your skin and especially at your extremities (finger tips, toes, face).  As such, there are very few sensory neurons in your internal organs, so it's very difficult to localize feeling in your intestines.  However, there is an abundance of nociceptors (pain receptors) in your internal organs as well as your skin, in case of injury.


That is because the purpose of pain is to alert the brain about bodily damage, and to condition you to avoid pain causing activities in the future.  Damage can be caused everywhere, but light touch and pressure sensing is only necessary on your skin and extremities.",null,0,cdjxo4r,1r4yaq,askscience,top_week,2
assay,"Heya mate, you can feel pain in your intestines, it just doesn't localize well.  Your spleen and liver pain localizes a bit better, but what you're feeling as sharp pinpointable pain is due to irritation of the peritoneum (a lining in your abdominal cavity that has somatic (and not visceral) pain receptors.  Take another example: appendicitis.  It often starts out as pain in the mid epigastric area above the belly button, but once the infected appendix touches the peritoneum lining the abdominal cavity, the pain becomes sharp and very localized to the lower right quadrant.",null,1,cdjsp6m,1r4yaq,askscience,top_week,2
The_Serious_Account,"Let's take the Riemann hypothesis. It's statement about *all* primes. The naive approach is to let a computer run through all primes, one by one, and check if the hypothesis is correct for that prime.

You know how many primes there are? It's a lot. Like a lot a lot. Infinitely many primes, in fact. Such a computer would need to do infinitely many calculations. Neither normal or quantum computers are capable of such a thing. 

So unless we discover more exotic forms of computation, this approach will never work. ",null,3,cdjqhva,1r4x1p,askscience,top_week,14
Vietoris,"How do you think a computer work ?

Computers are not magical creatures. They can do only what we teach them to do. They cannot solve problems that humans cannot solve.


The only thing computers can do better than us is doing very large computations in a very short amount of time. But it is important to understand that any computations that can be done by a computer could be done by a human using paper and pen. It will just take the human much more time.

Now the last piece of thing you need to understand is that almost all interesting unsolved maths problems are not computational problems, but theoretical ones. It's not the computational power that we lack, but the method to solve the problem.",null,4,cdjrs1v,1r4x1p,askscience,top_week,17
farhaven,"Because these math problems are not about calculating some large number. They are about fundamental properties of numbers and their relations. It takes a human mind to analyze these relations, form theories and finally proove them. Computers can aid in this, for example by doing prime factorization of very large numbers, but they are only a tool, never the solution.",null,1,cdjqfii,1r4x1p,askscience,top_week,5
selfification,"All proofs come down to starting with a fixed set of rules, a set of start points and a goal and then figuring out a way to use the set of rules you have to somehow transform the start points into the goal.  It's like being given a maze, a starting position and set of simple rules like ""you can go forward by 2 spaces"" or ""you can turn left, but only if you've already visited the square or your right"" or stuff like that.  If the maze is fairly limited, then a computer can just exhaustively search it faster than a human can intuit his way through it.  If the maze is extremely large, then exhaustive searches are out.  A computer stills needs to be taught heuristics - how to search through a large search space without doing unnecessary work.  These heuristics are human heuristics and are limited by how clever we are.  We do have machine learning algorithms and expert learning systems that try to emulate human pattern matching and inference skills, but these are still programmed to emulate some fixed, limited subset of human intuitions and skills.

That said, if we do have a certain intuitive ""inkling"" as to how a proof needs to proceed, we can easily program computers to exhaustively verify the parts that are impractical for a human to do.  See http://en.wikipedia.org/wiki/Four_color_theorem .  The first proof of this was based on a *massive* computer generated proof that generated 2000 or so possible cases and then solved every one of them mechanically using one set of techniques.  No human could have walked through all of those quickly or correctly.  The output proof was virtually uncheckable by humans.  One could verify that the computer code was correct, but the proof itself was too voluminous to verify by hand.  That's one example where computer step in to *assist* humans in tasks that we're not good at.",null,0,cdk367m,1r4x1p,askscience,top_week,2
Glimt,"Actually, we can, but it will take very long time. We can define formally when a string of letters is a proof of a given sentence with a given set of axioms (first-order logic). It is then easy to write a computer program to check if we have a proof.

Now all we need is for a computer to check all strings, in some order, and check if any of them is a proof of a given conjecture, or of its negation. If the conjecture is not undecidable, the program will surely halt.

This method is, of course, completely non-realistic, due to the number of strings that need to be checked.",null,0,cdkeomw,1r4x1p,askscience,top_week,2
jjandre,"From a layman's perspective, computers are programmed with math, the math to make those promblems machine solvable has to be written. You'd have to either already have the solution, or be pretty confident you were already headed pretty deep in the right direction of a solution just to write the code. Sounds like more of a job for AI. ",null,0,cdjzuof,1r4x1p,askscience,top_week,1
winduken,"A typical use of computers to solve an interesting math problem takes two steps. First, analyze the problem to boil down to some possible large but finite number of configurations that can be checked for correctness. Then, either check each configuration by hand or write a computer program to do the checking instead. The 4-color theorem for planar graph (http://en.wikipedia.org/wiki/Four_color_theorem) was solved this way. Unfortunately, it isn't obvious how to do the first step for most interesting math problems.

Incidentally, the P = NP problem (http://en.wikipedia.org/wiki/P_versus_NP_problem) in computer science is one the Millenium problems. It asks a simple question: Given a combinatorial problem, if every guessed solution can be checked for correctness by a computer in a number of steps bounded by some polynomial, is there an actual algorithm to come up with a solution in a number of steps bounded by a (possibly different) polynomial?",null,0,cdk83hh,1r4x1p,askscience,top_week,1
rapist1,"I am surprised to find that all the comments this far do not answer your question correctly, so I will. 
You are right to think that a computer can try to brute force search for a proof of any statement in your axiomatic system, however if your axiomatic system can express 0,1,2,3... etc, then you will never stop searching for true statements. That is sort of redundant however, but it gets worse; even if you pick a specific sentence you want to search for a proof or disproof of, the search may never halt because some sentences are ""independent"", and cannot be proven true or false in those axioms. It is stated in the second paragraph of [Godel's incompleteness theorems](http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems)",null,0,cdn9tdl,1r4x1p,askscience,top_week,1
PorchPhysics,"All atoms have small motion from the kinetic energy at an atomic level.  When you put matter under pressure, you increase the number of collisions between neighboring atoms which leads to more chaotic motion of atoms and constant transferring of this energy.  Planets are constantly applying pressure to their cores due to the outer layers pushing inward, which adds more energy and so on.

it can get much more detailed than this, but that is the general idea.",null,1,cdjqyq1,1r4r9g,askscience,top_week,7
chrisbaird,Gravitational potential energy is converted to heat when the gravity of an astronomical body crushes it to a higher pressure.,null,0,cdjsju8,1r4r9g,askscience,top_week,5
goldistastey,"The original theory was that the heat comes from the leftover formation energy of the planet. After all, for heat to diffuse through the entirety of the earth into space would take billions of years. The calculations, however, found that this does not account for all the heat, so now scientists think that it's a mix of the contributions of radioactive decay and formation energy. Consensus is that there is no fusion in the Earth I believe, btw.",null,0,cdjza2l,1r4r9g,askscience,top_week,1
haikuginger,"I can talk about two things that were factors in the long development process of color TV: backwards compatibility and display technology.

These two topics are heavily linked together. For example, during the early days of color TV development, one system, CBS Color, was a filter that spun in front of the screen, sequentially showing red, green, and blue. With that system, each frame was transmitted three times sequentially; one time each as red, green, and blue. The TV would time the spinning of the disk to its reception of each colored frame; the red frame would be drawn on the screen while the red filter was in front, and so on.

However, to see why that wouldn't work very well, we need to take a step backwards to see how black and white TV worked. Reduced to a basic level, a stream of electrons was fired at the inside of a glass screen coated in phosphorus. When electrons struck the phosphorus, it lit up. By using electromagnets, that single beam of electrons could be moved to point at any part of the screen. 

So, to form a picture on the screen, the electron beam would sweep across the screen, line by line, with the intensity of the beam determined by the amplitude of the TV signal arriving in the set at any given moment.

So, if you fed a CBS color signal into a black-and-white TV, it simply wouldn't work. TV at that point relied on the signals coming down exactly as the set expected them to, and on being able to ""line up"" those signals so that the first bit of a frame arrived at the TV when the electron beam was pointing at the top-left corner of the screen. CBS color just couldn't do that.

So, an alternate route needed to be taken. Color TV couldn't use an additive color space (composed of separate red, green, and blue signals), it needed a subtractive color space- one that started out as a complete monochrome picture (with equal amounts of red, green, and blue) that any black-and-white TV could read, but that had bits of extra information carried alongside it that could be used to turn that monochrome picture into color.

This introduced a new challenge, though. TVs at that point in time didn't have any ""memory""- they were sending the signal from an antenna or cable directly to the picture tube. If what was coming in didn't get drawn on the screen immediately, it wasn't getting drawn at all. And, with a subtractive color space, all the color was coming in at once. Unlike with CBS color, a TV couldn't use a mechanical trick like a spinning wheel to alternate between colors being drawn by a single electron beam- it needed to draw all the colors at once.

So, the logical answer to that problem was to go from using one vacuum tube producing an electron beam to three; one for each of red, green, and blue. Each tube would fire only when aimed at a bit of phosophor designated specially for it; each bit of phosphor would have a tiny filter in front of it so that, from the front, it would appear to glow red, green or blue.

As you might guess, this arrangement introduced a new problem. Before, it didn't matter if your electron beam was aligned exactly right or if it was particularly fine; the entire tube of your TV was coated in phosphor, and the picture would show up no matter what, even if the electron beam was angled over to the left a bit.

With the new arrangement, the problem was that the electron beams weren't fine enough and couldn't be aligned exactly right 100% of the time. This meant that ""blue"" phosphor dots were being lit up by the ""red"" electron beam- and the same for every other color. A reliable color picture wasn't being produced.

That's where Werner Flechsig's shadow mask comes in. The concept is simple, but brilliant. Take a metal sheet, and poke thousands of tiny holes in it. Then, put it directly behind the screen, which has your pattern of differently-colored phosphor dots on it. Now, because you're using three different electron beams, they're each coming from a different location- and each will enter any particular hole in the shadow mask, as the metal sheet is called, at a different angle. This allows you to restrict where the electrons from each beam land on the screen.

You can try it yourself. Take a sheet of paper and poke a hole in it. Then, holding the paper still, look through that hole from different angles. You'll see different things through it. Similarly, even if the electron beams were too big to focus and align down to the sizes they needed to be, shooting them through a tiny hole from slightly different places resulted in precise control over which points on the screen they struck.

Those are the things that were necessary for color TV to take off. We needed a way to send a color TV signal that could be viewed by people with black-and-white TVs. We needed a way to put each component color of that color signal on the screen simultaneously. And we needed a way to  keep each color at the point on the screen it needed to be.",null,1,cdjv089,1r4qud,askscience,top_week,12
cheeseynacho42,"Well, a huge obstacle towards broadcasting colour television was the massive amount of bandwidth it took. This went away slowly with time, as we got better and better at transmitting radio signals. 

As far as the actual television goes, there were a lot of ideas as to how you could make a TV colour. There was combining three monochrome images (red, green, and blue ones) to make a colour one, and shooting electrons at a phosphor plate (knows as ""Telechrome""), but probably the best-known one is Technicolour.

Technicolour evolved a lot over its lifespan, and while itw as eventually replaed, throughout most of the late 40s and 50s it was the dominant way of filming in colour and broadcasting in colour. Early Looney Toons used Technicolour, and the Wizard of Oz was filmed using it. 

Also, the way you talk about black-and-white TV makes it sound like we could have filmed in colour, but chose not to. That's not the case at all - it was very hard to figure out how to properly capture colour on film, much harder than filming black and white images. They weren't stripping the colour, or changing it, that was just what the film could capture. It's like asking the fish that live in the Abyssal zone why, when there's so much in the world to see, they only have light-detecting discs as eyes. It's the best they can do. ",null,0,cdjs5gp,1r4qud,askscience,top_week,1
Dannei,"Yes, GPS jamming is possible - as with any radio communications, you just need to fill the relevant frequencies with enough ""junk"" to drown out the actual signal.

It's also possible to spoof GPS signals, by creating a much more powerful signal that overrides the normal one. This can be used to convince a receiver that it is somewhere else than it actually is - examples of possible uses include misdirecting planes/ships to another destination, by carefully changing the signals you send it over time.

In both cases, yes, the area you cover would be limited by your transmission power.",null,2,cdjmmtx,1r4orq,askscience,top_week,11
zerbey,"You didn't ask, but it's important from a historical point of view if you're interested in GPS.  When GPS was first deployed, there was a function known as Selective Availability, which means that the US Government could intentionally block the GPS signal to all users except those who had authorised receivers.  The basic idea was that it would cause all ""civilian"" receivers to be inaccurate, so it'd be useless for tracking.  This functionality was turned off permanently during the Clinton administration and new Satellites are incapable of using it.
",null,0,cdjsx0o,1r4orq,askscience,top_week,3
Bradm77,Dannei gave a good answer.  I'm an engineer and I used to work on anti-jamming devices for the military.  We built devices that would prevent enemy jamming equipment from effecting our military's GPS devices.  ,null,1,cdjpp0x,1r4orq,askscience,top_week,3
LoyalSol,"Opening the window at best will make your apartment as humid as the outside, but Canada in general is a pretty dry area on the relative humidity scale so not really sure how much more opening the window will do.  ",null,1,cdjnz18,1r4geg,askscience,top_week,2
FatSquirrels,"That somewhat depends on what humidity you are talking about.

Absolute humidity is how much water is in the air.  Opening the window will equalize the absolute humidity between inside and outside.  Hot air can hold more moisture, but that does not mean that it necessarily has more water in it.  If it is dry outside then likely your enclosed space inside with showering, respirating humans/animals, cooking and such is actually more humid.

Relative humidity is the amount of water in the air relative to how much it can hold.  I think this is more relevant to humans as it determines the perceived temperature and how close the atmosphere is to precipitation.  Warm air can hold more water, so if you increase the temperature inside by opening a window the relative humidity will go down (assuming equal absolute humidity inside and out).  Opposite if it is colder outside.

Really what this means is that you need to know both the temperature and the relative or absolute humidity both in and outside to make this call.  Likely, your best bet is to keep your windows closed and buy a humidifier.  They aren't very expensive and they could help you out if the dry air is bothering you.",null,1,cdjs7zk,1r4geg,askscience,top_week,2
Problem119V-0800,"More force, yes. The light should exert twice as much pressure (aka transfer of momentum) on the reflecting surface as the black one, assuming other things are equal.

Looks like the first experimental measurement of this was in the early 1900s.

(Note that Crookes radiometersthose light powered spinning things in glass tubes that people sometimes have as toys / science demonstratorsactually work by thermal expansion of the air near the heated vane, not by photon pressure. Photon pressure is a much smaller effect. and would turn the vanes the opposite direction anyway.)

[a mildly interesting article on the subject (pdf)](http://dujs.dartmouth.edu/2002S/pressureoflight.pdf)",null,17,cdjixs9,1r4g5u,askscience,top_week,120
do_od,"A [Nichols radiometer](http://en.wikipedia.org/wiki/Nichols_radiometer) is essentially a small scale that measure the inertial force of light, or radiation pressure. It uses a pair of mirrors.

A perfectly reflecting scale would register a pressure twice as high as a perfectly absorbing, pitch black one. This is because of conservation of momentum. In total reflection the momentum of the light is reversed, while in absorption the momentum is transferred into the surface.",null,6,cdjj4sd,1r4g5u,askscience,top_week,20
SproutsCrayons,"Yes it would. The absorbed light, would be reemitted in a random direction or absorbed as heat. In both cases the net force is bigger for the mirror. 
If you are thinking about these things the gravity wave detectors, e.g. GEO600 and LISA, might interest you as they deal with these kind of problems. It might also be interesting to look in to [optical tweezers] (http://en.wikipedia.org/wiki/Optical_tweezers) , [laser cooling] (http://en.wikipedia.org/wiki/Laser_cooling) and [cavity optomechanics](http://en.wikipedia.org/wiki/Cavity_optomechanics). 

",null,1,cdjk3l8,1r4g5u,askscience,top_week,6
null,null,null,27,cdjo3q8,1r4g5u,askscience,top_week,6
Problem119V-0800,"More force, yes. The light should exert twice as much pressure (aka transfer of momentum) on the reflecting surface as the black one, assuming other things are equal.

Looks like the first experimental measurement of this was in the early 1900s.

(Note that Crookes radiometersthose light powered spinning things in glass tubes that people sometimes have as toys / science demonstratorsactually work by thermal expansion of the air near the heated vane, not by photon pressure. Photon pressure is a much smaller effect. and would turn the vanes the opposite direction anyway.)

[a mildly interesting article on the subject (pdf)](http://dujs.dartmouth.edu/2002S/pressureoflight.pdf)",null,17,cdjixs9,1r4g5u,askscience,top_week,120
do_od,"A [Nichols radiometer](http://en.wikipedia.org/wiki/Nichols_radiometer) is essentially a small scale that measure the inertial force of light, or radiation pressure. It uses a pair of mirrors.

A perfectly reflecting scale would register a pressure twice as high as a perfectly absorbing, pitch black one. This is because of conservation of momentum. In total reflection the momentum of the light is reversed, while in absorption the momentum is transferred into the surface.",null,6,cdjj4sd,1r4g5u,askscience,top_week,20
SproutsCrayons,"Yes it would. The absorbed light, would be reemitted in a random direction or absorbed as heat. In both cases the net force is bigger for the mirror. 
If you are thinking about these things the gravity wave detectors, e.g. GEO600 and LISA, might interest you as they deal with these kind of problems. It might also be interesting to look in to [optical tweezers] (http://en.wikipedia.org/wiki/Optical_tweezers) , [laser cooling] (http://en.wikipedia.org/wiki/Laser_cooling) and [cavity optomechanics](http://en.wikipedia.org/wiki/Cavity_optomechanics). 

",null,1,cdjk3l8,1r4g5u,askscience,top_week,6
null,null,null,27,cdjo3q8,1r4g5u,askscience,top_week,6
TangentialThreat,"They're called [fulgurites](http://en.wikipedia.org/wiki/Fulgurite).

Cool, huh?",null,1,cdjhga1,1r4em9,askscience,top_week,17
slumberprojekt,"You get a piece of fulgurite if you're lucky.

http://en.wikipedia.org/wiki/Fulgurite",null,1,cdjhow8,1r4em9,askscience,top_week,8
Criticalist,"In 2009 a group of researchers published a paper in the New England Journal of Medicine in which they reported on direct measurements of blood oxygen levels as they ascended to the summit of Mount Everest. At this altitude, 8800m, the partial pressure of oxygen is thought to be at the lower limit of what acclimatised humans can tolerate- according to the article, under 4% of people who climb Everest do so without supplemental oxygen.

The researchers took samples from their femoral arteries (located in the groin) at various altitudes as they ascended to the summit; the samples were transported down to the base camp for analysis. Above 7300m, the researchers used supplemental oxygen, but took it off for the sampling process.

The mean pO2 value, which is the partial pressure of oxygen dissolved in blood, was around 20mmHg at 8000m - about one fifth of normal.
However, the arterial oxygen content, which reflects the amount of oxygen bound to haemoglobin, was not nearly as reduced; it was 150ml/l, compared to a normal value of 200ml/l. This reflects the degree of acclimitisation the researchers had - non acclimaitised subjects exposed to that degree of low oxygen rapidly lose consciousness, whereas the climbers were functioning reasonably normally.

The pO2 levels recorded in this study are amongst the lowest recorded in healthy humans  and probably represent the lowest limit of tolerance.

[Link to the paper](http://www.nejm.org/doi/pdf/10.1056/NEJMoa0801581)",null,5,cdjk2e3,1r4egz,askscience,top_week,6
SmellyRaghead,"The amount you absorb is proportional to the partial pressure. At 1 atm., ppO2 is about o.21 atm. At 2 at., 0.42 atm, etc.

With a lower partial pressure, the less mass transfer of oxygen occurrs, obviously causing a lack of oxygen in the blood.

The opposite is also true - when diving beyond certain pressure thresholds, the ppO2 becomes far too high and can cause serious damage, as oxgen in too great a concentration is quite toxic.

In short, we are most 'comfortable' at sea level and 1 atm of total pressure.",null,0,cdjjkmo,1r4egz,askscience,top_week,2
irreligiosity,"It's complicated.. Your body adjusts to it's climate's oxygen levels to assure the appropriate intake of oxygen is met for cellular respiration. Hemoglobin supplies oxygen to your body. People who live at high elevation with little oxygen can have twice as much hemoglobin as those at lower elevations. This happens to ensure that there is a higher uptake of oxygen because there is so little available in the air. This is also why people have to wait at base-camp for 3 months before climbing Everest - so their body adjusts to the appropriate hemoglobin levels. If they do not wait, and your body does not get enough oxygen then the electron transport chain that is used to form ATP (energy for your body) is disrupted. This is hard on your body, and there are other effects of lack of oxygen also. 

With too much oxygen in the air your body receives too much oxygen. This does various things that are unwanted to the body. Oxygen can bind to things in your body, and it is used in important cell regulating. It is possible for the oxygen molecules to bind to protons forming a hydroxyl group which is undesirable. Too much oxygen it stressful for your bodies normal cellular functioning. ",null,3,cdjjs85,1r4egz,askscience,top_week,4
Criticalist,"In 2009 a group of researchers published a paper in the New England Journal of Medicine in which they reported on direct measurements of blood oxygen levels as they ascended to the summit of Mount Everest. At this altitude, 8800m, the partial pressure of oxygen is thought to be at the lower limit of what acclimatised humans can tolerate- according to the article, under 4% of people who climb Everest do so without supplemental oxygen.

The researchers took samples from their femoral arteries (located in the groin) at various altitudes as they ascended to the summit; the samples were transported down to the base camp for analysis. Above 7300m, the researchers used supplemental oxygen, but took it off for the sampling process.

The mean pO2 value, which is the partial pressure of oxygen dissolved in blood, was around 20mmHg at 8000m - about one fifth of normal.
However, the arterial oxygen content, which reflects the amount of oxygen bound to haemoglobin, was not nearly as reduced; it was 150ml/l, compared to a normal value of 200ml/l. This reflects the degree of acclimitisation the researchers had - non acclimaitised subjects exposed to that degree of low oxygen rapidly lose consciousness, whereas the climbers were functioning reasonably normally.

The pO2 levels recorded in this study are amongst the lowest recorded in healthy humans  and probably represent the lowest limit of tolerance.

[Link to the paper](http://www.nejm.org/doi/pdf/10.1056/NEJMoa0801581)",null,5,cdjk2e3,1r4egz,askscience,top_week,6
SmellyRaghead,"The amount you absorb is proportional to the partial pressure. At 1 atm., ppO2 is about o.21 atm. At 2 at., 0.42 atm, etc.

With a lower partial pressure, the less mass transfer of oxygen occurrs, obviously causing a lack of oxygen in the blood.

The opposite is also true - when diving beyond certain pressure thresholds, the ppO2 becomes far too high and can cause serious damage, as oxgen in too great a concentration is quite toxic.

In short, we are most 'comfortable' at sea level and 1 atm of total pressure.",null,0,cdjjkmo,1r4egz,askscience,top_week,2
irreligiosity,"It's complicated.. Your body adjusts to it's climate's oxygen levels to assure the appropriate intake of oxygen is met for cellular respiration. Hemoglobin supplies oxygen to your body. People who live at high elevation with little oxygen can have twice as much hemoglobin as those at lower elevations. This happens to ensure that there is a higher uptake of oxygen because there is so little available in the air. This is also why people have to wait at base-camp for 3 months before climbing Everest - so their body adjusts to the appropriate hemoglobin levels. If they do not wait, and your body does not get enough oxygen then the electron transport chain that is used to form ATP (energy for your body) is disrupted. This is hard on your body, and there are other effects of lack of oxygen also. 

With too much oxygen in the air your body receives too much oxygen. This does various things that are unwanted to the body. Oxygen can bind to things in your body, and it is used in important cell regulating. It is possible for the oxygen molecules to bind to protons forming a hydroxyl group which is undesirable. Too much oxygen it stressful for your bodies normal cellular functioning. ",null,3,cdjjs85,1r4egz,askscience,top_week,4
Minifig81,"It's something they call a stroboscopic effect and it's something that you commonly see in Western movies on television, so they also call it the wagon wheel effect. It's actually just an optical illusion in which the spoke wheel just appears to rotate differently than the true rotation. Actually it may look like it's rotating faster or not at all or even in the opposite direction.

It's illusion that's very similar to what we see with camera strobing that we might see in dance club or a party, when you have a flashing light that kind of freezes dances in a series of images.

If we think of the wagon wheel and we imagine that one of the spokes is painted white and it's pointed straight up - so like at like 12:00 on a clock - if we rotate that wheel at once per second and then we take images once every second, in our minds, it's going to look like that spoke never moved, because it's always going to be pointing at the 12.

If we were to suddenly now start capturing those images faster than once a second, so faster than it's rotating, that white spoke that's pointing up at 12, the next time we see an image, it's not going to make it all the way around, so it's going to be pointing at 11. And then the next image that we take, it's going to be pointing at 10. And so we're going to imagine that it's going to be moving in this counter-clockwise direction instead of the clockwise direction. ",null,1,cdjhxsk,1r4dw4,askscience,top_week,12
Mossman11,"I can't comment on why a cheaper high octane fuel isn't available, but considering the relatively small market and the fact that 93 octane for cars is $3.50, $6 for 100LL doesn't seem ridiculous to me.  Check out race fuels for cars, they're upwards of $10/gallon.

I do know a thing or two about motors, and octane, and compression so I can clear some stuff up there.  The 100 in 100LL is the motor octane rating.  This is a measure of how resistant to knock the fuel is, and how slowly it burns.  Especially in high compression engines, if a fuel is too volatile and its knock resistance is not sufficient, the fuel can pre-ignite due to compression alone.  When combustion happens while the piston is still on the way up, the piston now has to be fight through the high combustion pressures generated, as opposed to using them to propel it down again (the power stroke).  This is called knock, or ping, and is really bad for engine internals.  I know on some older aircraft the pilot had to control timing advance.  This would be equivalent to running too much timing advance all the time, except you can't push a lever to rectify it.  I've never flown a Cessna but I imagine if your motor grenades mid-air, that's not a good thing.  

As for why aircraft use high compression engines I would imagine it's to make the most power from the smallest/lightest engine.  Also the fact that you're flying in thin air means that you're sucking in less oxygen,nitrogen,etc. at altitude and in order to have sufficient power at altitude you'd need a high compression, knock prone engine at sea level.  

Also, I found this on wikipedia which seems like a nice feature of 100LL that possibly the proposed replacements don't have: 
""Avgas has a lower and more uniform vapor pressure than automotive gasoline so it remains in the liquid state despite the reduced atmospheric pressure at high altitude, thus preventing vapor lock""
",null,1,cdjo282,1r4coe,askscience,top_week,4
king_of_the_universe,"Yes, more heat will be trapped, but this heat is irrelevant in comparison to the heat that your hands are able to feel when they touch the screen: That heat is *not* caused by the light itself but by the mechanism that creates it / by other components of the screen.",null,0,cdjj1v5,1r49bc,askscience,top_week,1
rupert1920,"Would this not depend on how much water there is in a bucket, or the size of the bucket?

What scientific principle are you trying to elucidate here?",null,1,cdjduam,1r43z8,askscience,top_week,4
endocytosis,"Depends on how angry you arejust kidding.
I doubt you'd get hypothermia, but it wouldn't be pleasant.  Your head will transfer some thermal energy to the ice water, raising its temperature slightly.  Your head will lose the thermal energy, causing among other things, blood vessels to contract, cellular metabolism to slow, and you to get a splitting headache (long story but tried it once on a dare-wouldn't recommend it).  You may feel the initial effects of hypothermia, uncontrollable shivering, confusion, disorientation, etc., mainly because your brain is having difficulty getting enough oxygen, nutrients, etc. and is chilled, but your heart is still pumping warmer blood to your brain to keep it going, assuming it's a reasonably warm (70 deg F, 25 deg C) day.  Edit: grammar",null,0,cdjm5il,1r43z8,askscience,top_week,1
fishify,"When a particle interacts with its environment, the state of the two-particle system can change, disentangling the previous entangled state.

Entanglement simply means that a two- (or more) particle state cannot be written as a product of separate one-particle states.  If one of the particles interacts with its environment so its state becomes definite, then it is no longer entangled with its one-time partner.",null,0,cdjd7fq,1r41ug,askscience,top_week,7
KerSan,"I think /u/fishify's answer is incomplete. If systems A and B are entangled, why should interaction with system C diminish the entanglement between A and B?

It turns out that [entanglement is monogamous](http://arxiv.org/abs/quant-ph/9907047). If systems A and B are perfectly entangled, it turns out that they cannot be entangled (or even correllated) with any other system C. More generally, the amount of entanglement between A and B places a limit on the amount of entanglement that either system, or both considered together, can share with C. The right way to count entanglement is still an open question, but this odd behaviour is the reason why particles become un-entangled.

The most important thing to realize about entanglement is that it *cannot arise without interaction*. That is to say, systems that do not directly interact cannot become entangled. Therefore, a particle P that is entangled with a system S generally becomes less entangled with S if P interacts with another system R.

This of course begs an interesting question: is there such a thing as an un-entangled particle? All particles might be entangled with *something*. This comes down to an interpretation of quantum theory, which I don't think is worth trying to discuss on reddit. Those kinds of discussions are mostly fruitless anyway. Emphasis on 'mostly'.",null,1,cdk2z6l,1r41ug,askscience,top_week,1
baloo_the_bear,"Electronegativity is how strongly a particular atom attracts electrons. 

To understand what that means, you need to know what attracts electrons, and what factors can affect that.

Electron orbitals have very specific shapes in 3 dimensions, and can be thought of as a waveform. These waveforms are most stable when filled with an appropriate number of electrons. The first valence shell is *s* and the first level of the *s* orbital prefers 2 electrons, this makes it most stable. Hydrogen is nothing more than a proton and an electron, but it is very stable when it can fill its valence shell with 2 electrons, hence a high electronegativity and why it is so weird. Helium has 2 protons and 2 electrons (and 2 neutrons, but they don't matter for the moment), so its valence shell is full at 2 electrons, and it does not attract electrons to fill and sort of void, and therefore has a low electronegativity. 

Now, we talk about ionic forces.

Electrons are negatively charged, and as such they are attracted to positive charges, such as the nucleus of an atom. The force of ionic attraction is proportional to the charges of the two objects, and inverse of the square of the distance between them. The higher the positive charge of an atomic nucleus, the stronger the force of attraction, and the smaller the distance between those charges, the stronger the force of attraction.

As you move from the left to the right of the periodic table, you have more and more protons, which means an increasing positive charge in the nucleus. This increasing positive charge exerts an increasing force of attraction on the electron orbitals. This causes the **size** of the orbital to **decrease** as you move across the periodic table. This is why the **electronegativity increases going from left to right.** 

Now to add another concept.

Valence shells exist in orbitals that have different levels of energy. The fact that energy is discrete (dividable down to quanta) means that the orbitals have discrete levels, or layers. Not only do more layers increase the distance between the nucleus and the electron it is attracting, but those layers are all negatively charged and will act to repel another negatively charged electron. 
This is why as you **move down** the periodic table (increasing levels of valence shells), the **electronegativity reduces.**

Now remember that we talked about valence shells being most stable with certain numbers of electrons? The next 'magic number' of valence electrons that make the orbitals stable is 8. Now count over from left to right on the table. Florine has....**7** electrons in its valance shell, just one short. It strongly attracts that last electron not only because it is small and has a large charge in its nucleus, but because gaining another electron makes it have a **more stable** valence structure. 

OK, so we've talked about what electronegativity means, and what factors have an affect on it, but why do we care?

We stated that a stable valence shell has 8 electrons. This is why Carbon, with 4 electrons in its valence shell, will make 4 covalent bonds. Covalent bonds, as their name suggests, are when 2 atoms **share** an electron so that **both** atoms can have a stable valence shell. in the case of a covalent bond between atoms that are the same, the electron is shared equally, because the **electronegativity** of each atom is the same. However, if one atom in the covalent bond has a higher electronegativity, the electron is attracted more to that side of the bond. What happens when you're attracted to something? You want to spend more time there. Because of this unequal sharing, the bond becomes **polar**, in that one side of the bond has a slightly negative charge, and one side has a slightly positive charge. 

This matters in incredibly significant ways. Water, for example, is a polar molecule. Because of this water is liquid at room temperature, held together by hydrogen bonds (a consequence of polar molecules). DNA is also held together to their complementary strands by hydrogen bonds. 

edit: the stability of 8 valence shell electrons are also why the noble gases are very unreactive. They do not need to share electrons to be stable. The low reactivity of this group of elements is why they are called 'noble', as nobility kept to themselves.
",null,27,cdjc2e8,1r3wa9,askscience,top_week,126
MJ81,"I'm not sure there's a ""rigorous"" explanation of electronegativity - there are quantitative ways to calculate electronegativity (see [the Wiki article](http://en.wikipedia.org/wiki/Electronegativity)), after a fashion, but it's primarily a way to formalize available experimental data &amp; observed trends.",null,3,cdjeduy,1r3wa9,askscience,top_week,6
M4rkusD,"In short. Hydrogen is so weird because, in contrary to the other elements, its nucleus is just a proton. That means its properties (and especially those of its H+ ion, essentially a naked proton) are partly different from those of other molecules.

The next element, helium, which has 2 protons already needs 2 neutrons to stabilise its nucleus.",null,6,cdjl4ss,1r3wa9,askscience,top_week,4
ThePizar,"Atoms want to fill their valance shell (s &amp; p sub-shells) and to do that they need an electron. As you go across a period atoms get closer to having a full valance shell and thus they want that next electron. Going down a group atoms get bigger and the valance shell is farther from the nucleus and thus they have a weaker pull in each other. This leads the atom to not want the electron as much. 

You may notice a drop after the Cu/Ag/Au group. That is because the next group has all its existing outer sub-shells (s &amp; d) filled. 

Edit: Lewis Dot structure is a simple way of showing how molecules interact. Actually molecules are in 3d space. There are two main ways of showing/describing interactions between molecules. [Valance bond theory](http://en.wikipedia.org/wiki/Valence_Bond_Theory) and [Molecular orbital theory](http://en.wikipedia.org/wiki/Molecular_orbital_theory). 

Source: AP Chemistry class",null,9,cdjblqa,1r3wa9,askscience,top_week,4
baloo_the_bear,"Electronegativity is how strongly a particular atom attracts electrons. 

To understand what that means, you need to know what attracts electrons, and what factors can affect that.

Electron orbitals have very specific shapes in 3 dimensions, and can be thought of as a waveform. These waveforms are most stable when filled with an appropriate number of electrons. The first valence shell is *s* and the first level of the *s* orbital prefers 2 electrons, this makes it most stable. Hydrogen is nothing more than a proton and an electron, but it is very stable when it can fill its valence shell with 2 electrons, hence a high electronegativity and why it is so weird. Helium has 2 protons and 2 electrons (and 2 neutrons, but they don't matter for the moment), so its valence shell is full at 2 electrons, and it does not attract electrons to fill and sort of void, and therefore has a low electronegativity. 

Now, we talk about ionic forces.

Electrons are negatively charged, and as such they are attracted to positive charges, such as the nucleus of an atom. The force of ionic attraction is proportional to the charges of the two objects, and inverse of the square of the distance between them. The higher the positive charge of an atomic nucleus, the stronger the force of attraction, and the smaller the distance between those charges, the stronger the force of attraction.

As you move from the left to the right of the periodic table, you have more and more protons, which means an increasing positive charge in the nucleus. This increasing positive charge exerts an increasing force of attraction on the electron orbitals. This causes the **size** of the orbital to **decrease** as you move across the periodic table. This is why the **electronegativity increases going from left to right.** 

Now to add another concept.

Valence shells exist in orbitals that have different levels of energy. The fact that energy is discrete (dividable down to quanta) means that the orbitals have discrete levels, or layers. Not only do more layers increase the distance between the nucleus and the electron it is attracting, but those layers are all negatively charged and will act to repel another negatively charged electron. 
This is why as you **move down** the periodic table (increasing levels of valence shells), the **electronegativity reduces.**

Now remember that we talked about valence shells being most stable with certain numbers of electrons? The next 'magic number' of valence electrons that make the orbitals stable is 8. Now count over from left to right on the table. Florine has....**7** electrons in its valance shell, just one short. It strongly attracts that last electron not only because it is small and has a large charge in its nucleus, but because gaining another electron makes it have a **more stable** valence structure. 

OK, so we've talked about what electronegativity means, and what factors have an affect on it, but why do we care?

We stated that a stable valence shell has 8 electrons. This is why Carbon, with 4 electrons in its valence shell, will make 4 covalent bonds. Covalent bonds, as their name suggests, are when 2 atoms **share** an electron so that **both** atoms can have a stable valence shell. in the case of a covalent bond between atoms that are the same, the electron is shared equally, because the **electronegativity** of each atom is the same. However, if one atom in the covalent bond has a higher electronegativity, the electron is attracted more to that side of the bond. What happens when you're attracted to something? You want to spend more time there. Because of this unequal sharing, the bond becomes **polar**, in that one side of the bond has a slightly negative charge, and one side has a slightly positive charge. 

This matters in incredibly significant ways. Water, for example, is a polar molecule. Because of this water is liquid at room temperature, held together by hydrogen bonds (a consequence of polar molecules). DNA is also held together to their complementary strands by hydrogen bonds. 

edit: the stability of 8 valence shell electrons are also why the noble gases are very unreactive. They do not need to share electrons to be stable. The low reactivity of this group of elements is why they are called 'noble', as nobility kept to themselves.
",null,27,cdjc2e8,1r3wa9,askscience,top_week,126
MJ81,"I'm not sure there's a ""rigorous"" explanation of electronegativity - there are quantitative ways to calculate electronegativity (see [the Wiki article](http://en.wikipedia.org/wiki/Electronegativity)), after a fashion, but it's primarily a way to formalize available experimental data &amp; observed trends.",null,3,cdjeduy,1r3wa9,askscience,top_week,6
M4rkusD,"In short. Hydrogen is so weird because, in contrary to the other elements, its nucleus is just a proton. That means its properties (and especially those of its H+ ion, essentially a naked proton) are partly different from those of other molecules.

The next element, helium, which has 2 protons already needs 2 neutrons to stabilise its nucleus.",null,6,cdjl4ss,1r3wa9,askscience,top_week,4
ThePizar,"Atoms want to fill their valance shell (s &amp; p sub-shells) and to do that they need an electron. As you go across a period atoms get closer to having a full valance shell and thus they want that next electron. Going down a group atoms get bigger and the valance shell is farther from the nucleus and thus they have a weaker pull in each other. This leads the atom to not want the electron as much. 

You may notice a drop after the Cu/Ag/Au group. That is because the next group has all its existing outer sub-shells (s &amp; d) filled. 

Edit: Lewis Dot structure is a simple way of showing how molecules interact. Actually molecules are in 3d space. There are two main ways of showing/describing interactions between molecules. [Valance bond theory](http://en.wikipedia.org/wiki/Valence_Bond_Theory) and [Molecular orbital theory](http://en.wikipedia.org/wiki/Molecular_orbital_theory). 

Source: AP Chemistry class",null,9,cdjblqa,1r3wa9,askscience,top_week,4
CatalyticDragon,"Yes. All the objects you have listed contain the same elements but in different total amounts and ratios. And they all behave according to the same universal physics.

Here is a quick page on the makeup of the earth, stars, and even interstellar dust clouds. You'll see all of this ""stuff"" is made up of the same ""stuff"" because it all comes from the same place. Exploding stars;

- http://spiff.rit.edu/classes/phys240/lectures/elements/elements.html",null,0,cdjbbg6,1r3v70,askscience,top_week,3
iorgfeflkd,"There are only 90 or so elements that can be found in nature. Asteroids and planets are all made from them, in different abundances.",null,0,cdjczor,1r3v70,askscience,top_week,2
Bbrhuft,"There would be no difference. Riffling causes the bullet to spin, it acts like a gyroscope, giving it extra stability that keeps it from tumbling randomly and deviating from its intended target due to variable wind drag.

But there's no air in space, no wind drag, so whether a bullet tumbles or not, it will still follow the same path. 

And indeed when calculating the orbits of asteroids there's no need to take into account their rotation period.",null,9,cdjdg2i,1r3sde,askscience,top_week,44
TangentialThreat,"It depends on whether you need the bullet to hit pointy-end first.

Many bullets have either a hollow point so it fragments on impact and does more damage, or a hardened armor-piercing tip. Rotation helps keep this end towards the enemy. A bullet that hits sideways or backwards is still pretty lethal on Earth, but you are presumably shooting at either your fellow kevlar-suited cosmonauts or battling an unknown alien menace. The terminal ballistics will be much less predictable and this could be a problem.

It should be noted that firing any gun in space is probably a rather delicate skill due to conservation of momentum. Rifling will make this even more complicated. Best of luck out there and try to make Earth look good.

[Loose wads of nitrocellulose](http://www.youtube.com/watch?v=QnDZ_cO5Ln4) refuse to burn in vacuum.

[Double-action revolvers](http://www.youtube.com/watch?v=hUdkIn7C9fA) do appear to work, which I would not have expected.",null,3,cdjhdea,1r3sde,askscience,top_week,9
Evomon,Wouldnt it be impossible to shoot a gun in space as the conventional  bullet wont have o2 to ignite ? ,null,10,cdjdkgf,1r3sde,askscience,top_week,9
Bbrhuft,"There would be no difference. Riffling causes the bullet to spin, it acts like a gyroscope, giving it extra stability that keeps it from tumbling randomly and deviating from its intended target due to variable wind drag.

But there's no air in space, no wind drag, so whether a bullet tumbles or not, it will still follow the same path. 

And indeed when calculating the orbits of asteroids there's no need to take into account their rotation period.",null,9,cdjdg2i,1r3sde,askscience,top_week,44
TangentialThreat,"It depends on whether you need the bullet to hit pointy-end first.

Many bullets have either a hollow point so it fragments on impact and does more damage, or a hardened armor-piercing tip. Rotation helps keep this end towards the enemy. A bullet that hits sideways or backwards is still pretty lethal on Earth, but you are presumably shooting at either your fellow kevlar-suited cosmonauts or battling an unknown alien menace. The terminal ballistics will be much less predictable and this could be a problem.

It should be noted that firing any gun in space is probably a rather delicate skill due to conservation of momentum. Rifling will make this even more complicated. Best of luck out there and try to make Earth look good.

[Loose wads of nitrocellulose](http://www.youtube.com/watch?v=QnDZ_cO5Ln4) refuse to burn in vacuum.

[Double-action revolvers](http://www.youtube.com/watch?v=hUdkIn7C9fA) do appear to work, which I would not have expected.",null,3,cdjhdea,1r3sde,askscience,top_week,9
Evomon,Wouldnt it be impossible to shoot a gun in space as the conventional  bullet wont have o2 to ignite ? ,null,10,cdjdkgf,1r3sde,askscience,top_week,9
Claclink,"ive tested this in the lab with a spectrometer and it depends on the color and person, but in optimal conditions you can detect a change of about 2nm.",null,1,cdjet36,1r3qnw,askscience,top_week,5
Greyswandir,Human eyes are surprisingly sensitive to changes in wavelength.  My research used to involve a colorimetric assay (one where the main output is a color changed) which involved me both taking pictures of the samples and measuring their spectrum.  A relatively small change in the shape of the spectrum (~20nm of peak broadening) could change the perceived color of the sample from green to orange.,null,0,cdjqi6j,1r3qnw,askscience,top_week,1
Das_Mime,"Particles certainly exist, although on a quantum level everything exhibits particle/wave duality. Particles, instead of being the hard pellets that we sometimes envision, are actually described by a probability function which tells you how likely the particle is to be in any given location. So in that sense the particle can be thought of as a probability field.

In the Standard Model (which is the more or less unanimously accepted model of particle physics), mass is caused by particles interacting with the Higgs field, whose existence is supported by the discovery over the last few years of the Higgs Boson.",null,1,cdjk1ri,1r3pfa,askscience,top_week,4
null,null,null,0,cdja5vr,1r3pfa,askscience,top_week,1
datums,"Taking a photograph on film is a chemical reaction. When light hits film, it causes chemical changes in the film itself, and the pattern of these changes makes up the image.   
  
Like any chemical reaction, it takes a particular amount of time. If I expose a particular film to light, it might take 1/24 of a second for the chemical reaction to occur, as is the case for standard movie film.   
  
If I want to increase the frame rate with that film to 1/48 of a second, I am going to have a problem. The same chemical reaction will have to happen in half of the time. A potential solution? Double the amount of light to compensate.    
  
Another way to analogize this is that is takes a particular number of photons to record an image. If I want to capture images more quickly, I can simply increase the number of photons per second, ie. the brightness, to compensate. ",null,1,cdjajxm,1r3nxd,askscience,top_week,7
johnwilkesbandwith,"Running at 24 FPS the shutter, a physical spinning disk, exposes the film at 1/48 rotations per second with a 180 degree shutter opening. This means every second half of the oval blocks the light and half exposes the film as it runs through the gate. 

The gate is a plate system that holds the physical medium in place. At this speed, with the proper F/stop, the film is exposed properly.

When you increase the frame rate, you need more light because the speed that the film is going through the gate is getting the proper exposure.

You can compensate for a higher frame-rate by either switching to a higher shutter speed, which can effect the motion blur of the image or you can open your lens up to a lower F/Stop. 

If you don't have the available light, you can compensate with the shutter speed but it does effect the visual aesthetic of the image.

The image gets dimmer because the film loses about a stop/multiplication of speed that you run the film. 

This concept applies to digital filmmaking, but its effects on 3D photography are more complex.",null,0,cdji3t2,1r3nxd,askscience,top_week,1
brawnkowsky,"depends specifically on what you're allergic too and whether that allergy exists on that particular animal.  but most cat allergies are caused by a specific protein (Fel d 1) that is produced by cats and released via sebaceous glands on skin.  this protein interacts with IgE and IgG4 to create an immune response.

research shows that this protein is present on some big cats, so if this is the protein you are allergic too, then chances are you will have a reaction when coming in contact with any cats.  but there is a chance you are not allergic to this protein.

http://www.ncbi.nlm.nih.gov/pubmed/1695231

Edit: research shows that a Fel d 1-like molecule is present.  same results, different cause",null,0,cdjg5om,1r3lwr,askscience,top_week,4
justin3003,"I hate to keep doing this as a means of answering questions here, but I wrote a blog post on this exact topic a few months ago. The short answer, based on my research, is ""kind of."" Big cats are genetically related but distinct from house cats (as well as distinct from each other) and have different forms of the allergenic protein Fel d 1. The big cat version of the protein can cause an acute allergic reaction in humans but not all to the same extent as Fel d 1 does. I cite the same paper as brawnkowsky does; in it they have a chart of histamine release vs. concentration of proteins as well as relative release of the relevant immunoglobulins from big cats compared with Fel d 1. The lion, for example, has a significantly lower reactivity in a cat allergic patient than both a tiger and Fel d 1 cause (and fel d 1 &gt; tiger). The chart also shows dramatic (at least 50%) lower immunoglobulin release during exposure for big cat dander vs. Fel d 1/house cats. While the conclusions presented in the abstract are technically correct, since they can/do elicit an immune response, the actual data within the paper much more clearly demonstrates the nuances of their conclusion: while many are still allergenic to some extent, Fel d 1 has a much higher activity than any protein present in the big cats by a fairly wide margin for both IgE and IgG4. 
Link to my blog post: http://theweeklypaperblog.com/2013/02/13/lions-and-tigers-and-bears-oh-my-allergies-and-cross-species-issues/
Link to the ScienceDirect site (if you have institutional access to papers, sorry I can't post it directly for you): http://www.sciencedirect.com.libproxy.usc.edu/science/article/pii/S0091674905801307#",null,0,cdksxlm,1r3lwr,askscience,top_week,1
ModernTarantula," you have to imagine yourself smaller that what you are building and building it vertical not horizontal. Next watch a spider make it's web. You will see how it happens. Next is speculation from that. They need to have a central hub to make the spokes from . They follow the old line to the middle and then can hold up a string out with a leg to make the next radial. The contehcntric circles are made as they walk around the radials like a racetrack. at each pint connecting to the radial. 
so spokes and a tight spiral is simpler than measuring out a grid in the air.",null,0,cdjh1ch,1r3lv4,askscience,top_week,1
Platypuskeeper,"It's a common misconception, from the [Bohr model of the atom](http://images.tutorvista.com/cms/images/38/bohr-atom-model.JPG), that there'd be any 'empty space' between the atom and nucleus. Electrons are quantum-mechanical particles, which means they don't normally occupy a precise location, nor radius from the nucleus, they're sort of 'smeared out' in space. Atoms actually 'look' more [like this](http://kaffee.50webs.com/Science/images/orbitals/1s.gif), with a diffuse 'cloud' of electrons around them, where the density represents the probability of finding an electron in the vicinity of a point. The density is actually _highest_ near the nucleus, and drops off exponentially as you move away from it.

So there's no real 'empty space' there, but some parts of space are emptier than others. (That said, there's still a concept of the vacuum in physics). 

One thing that could be said to be true of the Bohr model is that it shows that the 'size' of an atom is the 'size' of the volume which its electrons occupy. Although that's a diffuse 'cloud', it's still 4-5 orders of magnitude larger than the 'cloud' of the nucleus. 

Electrons and protons are not very 'solid'. An electron has no radius of its own, and the protons (which are composite particles) are also spread out. Due to this quantum smearing-out, electrons (if they have opposite 'spins') can occupy the same location, and the electron and proton can be in the same location, which wouldn't be possible for classical point-charges, as it would lead to infinite energies. 

The classical concepts of things 'touching' and having surfaces and volumes don't apply here. You have to flip it around: _Because_ the electron density drops off so rapidly as you move away from the nucleus, the electrical forces between two adjacent atoms get enormous within a short distance, once their electron clouds start pushing into each other. Things push back with more force than you can exert over an imperceptibly small distance, which is why things appear 'solid' to you at the large-scale, macroscopic, level. At the atomic level, nothing is solid though.

",null,5,cdjbguz,1r3ju6,askscience,top_week,13
r2k,"We can't ""see""  atoms or subatomic particles. To properly resolve atoms,  we need to use electrons or other particles which have a smaller wavelength on the same order as the atoms. Electrons are considered point particles, and the nucleus of an atom is composed of protons and neutrons. These subatomic particles are themselves composed of things called quarks. 

You can't probe a proton/neutron to see how hard it is -  because you would have to use something much smaller than it in the first place. In reality,  it is not a sphere,  and scientists are still trying to determine what the fundamental building block of matter is. String theory proposes that ultimately,  everything is composed of bundles of energy. 

The space between a nucleus and the orbiting Electrons is the same space that exists between the moon and Earth. It really is nothing. ",null,10,cdjarhu,1r3ju6,askscience,top_week,4
s8nlovesme,"""Gray hair, is simply hair with less melanin, and white hair has no melanin at all. Genes control this lack of deposition of melanin, too. In some families, many members' hair turns white in their 20s. Generally speaking, among Caucasians 50 percent are 50 percent gray by age 50. There is, however, wide variation. This number differs for other ethnic groups, again demonstrating the effect of genetic control.""",null,0,cdjazwp,1r3fe1,askscience,top_week,1
iorgfeflkd,"We don't need particularly fancy telescopes to detect the cosmic microwave background, and recent missions like WMAP have given us pictures of it with extraordinary resolution.

Too see farther back than the CMB (about 400,000 years after the big bang), we can not longer use light (or any EM radiation) because the universe was opaque. Neutrino or gravitatioonal wave observatories could bypass this, but our capability in those areas is much much worse than with light telescopes.",null,1,cdj6d9b,1r3fak,askscience,top_week,15
quality_is_god,"This would depend on the nature of the restriction. 

Only if the restriction were isentropic would the pressure at C not be affected by the upstream restriction. 

Isentropic restrictions are very specific devices (like a nozzle and diffuser) but under the vast majority of restrictions, there will be unrecoverable pressure losses that propagate downstream.",null,0,cdje3hg,1r3f2v,askscience,top_week,2
202024,"Yes it will, the restriction will have a pressure drop, depending on the type some of it will be recovered, but there will still be a permanent pressure drop caused by the restriction.

http://en.wikipedia.org/wiki/Orifice_plate is a good place to start if you want some more technical information.",null,0,cdjn8dk,1r3f2v,askscience,top_week,1
Randomaway,"Your pump will adjust to provide a constant flow rate.  Let's call point C some kind of spray nozzle.  That spray nozzle will require a certain pressure to produce the desired flow rate.  Introducing a restriction (B)  upstream will not affect the required pressure at C.  Point B  does introduce extra pressure drop, so the pressure at point A will have to be higher to produce the required pressure at point C.",null,0,cdkmxmc,1r3f2v,askscience,top_week,1
whatsup4,"Point C should not see any difference theoretically. I say theoretically because if for some reason the blockage at B caused some kind of turbulence at point C it could alter what is happening downstream. Something like a vortex upstream forces your fluid to vortex downstream. But if that is not the case or C is far enough away from B then no there shouldn't be a difference.

Here's one way to look at it if you have a pipe with a certain diameter and a certain back pressure from the piping after it then there is only one pressure it can be for a certain flow rate. If it's pressure increased its flow would need to increase if its pressure decreased its flow would need to decrease. Since its flow must remain constant so must its pressure.",null,0,cdlj3wc,1r3f2v,askscience,top_week,1
evertjm,"Yes, pressure is caused by a resistance to flow. As you have stated that point B will restrict flow, if the diameter of A and C are equal then C will have less flow and pressure.

Simple test is to kink your garden hose while the tap is on fully. If you kink the hose at the halfway point, and the end of the hose is unrestricted, then you will notice a considerable drop in flow and pressure.",null,1,cdjb2g4,1r3f2v,askscience,top_week,1
__Jay,"One very important aspect that has gone unmentioned is your Reynolds number.  Loosely speaking, an indication of smooth (laminar) vs turbulent flow.  Your Reynolds number is a function of distance, i.e., as distance increases your flow will become less stable and inevitable losses are incured which call for pressure increases to maintain flow. 

 In technical terms, you have boundary layer separation with distance.  A mechanical energy balance will show, any losses with kinetic energy (fluid velocity) lead to a neccessary pressure increase to maintain fluid velocity at constant cross section (pipe diameter).",null,1,cdjdbut,1r3f2v,askscience,top_week,1
iorgfeflkd,"If the planet passes in front of the star periodically, its radius can be determined by how much the star dims as the planet passes in front, while the mass can be determined by the planet's gravitational effect on the star's velocity, which is based on the stars chemical spectrum changing with the Doppler shift. Combining these, a density can be calculated.",null,1,cdj6ius,1r3ezh,askscience,top_week,4
iorgfeflkd,"If you want to look at it that way, you can find the average kinetic energy per molecule (sort of, it's a bit more complicated) by multiplying the temperature by Boltzmann's constant (which is equal to the ideal gas constant divided by Avogardro's number). However, the average molecular energy isn't the best definition of temperature; it actually relates to how energy changes with entropy.",null,1,cdj6gu5,1r3ez4,askscience,top_week,12
__Pers,"In some subfields of physics such as plasma physics, temperature is indeed quoted in units of energy, typically electron Volts (eV) or kilo-electron Volts (keV), referring to the average kinetic energy a particle from the ensemble would have.",null,1,cdj77de,1r3ez4,askscience,top_week,4
natty_dread,"Temperature and Energy are connected by the so called Boltzmann constant.

That means, that there is a fixed ""exchange rate"" between temperature and energy. You could choose your system of units in a way that will result in the constant being equal to 1. This will render temperature and energy equal in terms of value.",null,2,cdj880a,1r3ez4,askscience,top_week,2
arumbar,"Enzymes are proteins - what happens when you ingest proteins?  They are digested into their substituent components and lose any structure/function they once had.  This is why replenishing insulin in diabetic patients orally does not work - they need injections instead.  The one scenario that I know of where we give people oral enzymes is for pancreatic insufficiency.  In this case, they are missing digestive enzymes that would normally be secreted into the gut, so artificially adding some in works well.",null,0,cdj6qmf,1r3em7,askscience,top_week,7
KarlOskar12,"The enzyme isn't missing from the blood, it is missing from lysosomes. Tay-Sachs is a *lysosomal storage disorder*. The problem with giving supplemental enzyme to someone with Tay-Sachs disease is that [HEXA](http://www.news-medical.net/health/Tay-Sachs-Disease-Research.aspx) is too large to pass the blood brain barrier where it is needed to break down GM2 gangliosides.",null,1,cdj9tei,1r3em7,askscience,top_week,5
gilgoomesh,"I don't know what measure of signal strength you're looking at or what WiFi hardware you're using so there could be 100 different answers. Three possibilities stand out in my mind:

a) When idle, your connection's sync rate will sit on a ""best guess"". When you're actually downloading, it can use the data sent and the error rate on that data to measure the *real* signal strength and it turns out that it's much lower than the guess so it needs to lower the sync rate.

b) The router antenna increases its power when sending data. Maybe this causes the power supply to create more interference.

c) You're getting a lot of multi path interference and your own WiFi signal bouncing off walls in the room is interfering with itself.",null,0,cdj90vh,1r3eju,askscience,top_week,3
superAL1394,"For a more complete answer, input from an EE or a physicist is necessary, but in general wifi strength is a 'best guess' estimate by the operating system based on its reception of packets from the router. When transmitting, you might be able to hear the router just fine, but your packets aren't being heard by the router. This is often the case if the power on your router has been increased, or you are using a lower power device (like an iPhone).

When you receive data, you will transmit back acknowledgements as defined by the IP protocol. This helps the system determine how fast it can send packets based on the number of collisions and dropped packets. If you are hearing the router just fine at idle, it will show a high strength, but as soon as you start receiving and the acknowledgements aren't being received by the router, the operating system will revise its guess on the signal strength to reflect your inability to effectively communicate back. The operating system will know the router isn't receiving acknowledgements because it will be receiving packets multiple times as the router will assume transmission failed and resend the packet.",null,0,cdjxq13,1r3eju,askscience,top_week,3
snusmumrikan,"[Yes it appears so](http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291096-8644%28199808%29106:4%3C483::AID-AJPA4%3E3.0.CO;2-K/pdf)

If you don't have access through an academic institution to that article this is the reference: American Journal of Physical Anthropology, 106: 483503.

It's all on Wikipedia.

(Edit spelling)",null,0,cdj67pa,1r390m,askscience,top_week,3
ooburai,"It's not definitive, but The Straight Dope has this article on the topic.

http://www.straightdope.com/columns/read/619/why-do-we-nod-our-heads-for-yes-and-shake-them-for-no

Also this: http://en.wikipedia.org/wiki/Nod_(gesture)

It seems that most people nod for yes, but there are some specific exceptions.  Bulgaria and Albania are cited.  That would seem to imply that it's not completely innate, or possibly that if it is that it can be overridden by culturally learned behaviour.",null,0,cdjb1d3,1r38ut,askscience,top_week,5
iorgfeflkd,It is indeed held together by gravity. The collective gravity of the sun is a lot stronger than that of Earth.,null,1,cdj8lbv,1r38kq,askscience,top_week,6
__Pers,"For the most part, gravity keeps the sun together. That said, the solar atmosphere (the chromosphere and corona) is composed of hot--hundreds of thousands of Kelvin to millions of Kelvin plasma--that indeed floats off into space. This hot, expanding plasma is called the solar wind.

Edit: typo",null,1,cdj8onh,1r38kq,askscience,top_week,4
Professor_Snuggles,"A simple thing that no one's mentioned is to put the piece of paper on top of and underneath the book when you drop them, rather than side by side. If they actually fall at different speeds, they would separate in one case. They'll fall together in both though, because the paper is shielded from air resistance effects by the book. If air resistance wasn't what was causing the difference, then this wouldn't matter.",null,0,cdjem5e,1r37tg,askscience,top_week,16
IAmMe1,"The key difference here is between force and acceleration. You won't really be able to get away with avoiding inertia to explain this.

See, the gravitational *force* is indeed stronger for an object with more mass. However, that object also has more mass and thus more inertia. This means that it takes a larger force to reach the same *acceleration* for that object. It just so happens that the dependence on mass cancels out for gravity; every object experiences the same *acceleration* due to gravity.

However, the *force* of air resistance does not depend on mass; it depends on the shape of the object (and its speed). The result is that, while all objects of the same shape (moving at the same speed) experience the same *force* of air resistance, due to different amounts of inertia (mass) they experience different *accelerations* due to air resistance. In particular, since the heavy object has more inertia, the same force produces less acceleration than on a light object.

To summarize: gravity produces more force on a heavier object than a light one but the same acceleration, while air resistance produces the same force on each but less acceleration for the heavy one.",null,1,cdj7jf3,1r37tg,askscience,top_week,7
Weed_O_Whirler,"So, to answer this you have to understand the difference between ""force"" and ""acceleration"" (I know, you're thinking- of course I do! But, your explanation shows a misunderstanding between them). Gravity attempts to *accelerate* everything at the same rate- and it does this by pulling on heavier things with more *force.* 

A nine year old might not be able to fully understand Newton's second law (F = m\*a if you need a refresher) but you can probably explain it to him pretty well. Tell him to imagine a rocket hooked up to a car, and how that rocket can make the car go fast. Now imagine that instead of a car, it is a big truck. The rocket, which puts out the same force regardless, will push the truck slower than the car. And now hook that rocket up to a train, and the train might not move at all. Or if it does, it will move slowly. This is the basics of Newton's second law- if you apply the same force to objects, the heavier ones will move slower than the light ones. You can do this experiment by trying to push a book across the table, or a stack of books- you'll have to push harder for the stack. 

OK, so gravity isn't a rocket. Gravity pulls harder on things which are heavier. In fact, if you double the mass of the object, gravity will pull twice as hard. So that is like if you made a car twice as heavy, but also attached two rockets- the acceleration would be the same regardless. So, gravity provides *twice the force* on an object twice as heavy, but due to Newton's second law, that is *the same acceleration.*

So now, add in air. Air resistance comes from the object having to move air molecules out of the way as it falls. So, it makes sense that the force of air resistance would be dependent on two things- the ""surface area"" or shape of the object in the direction it is falling and the speed at which it falls. The larger the surface area (again, only in the direction of falling, a book turned up on its spine would have less air resistance than a book lying flat), the more air molecules it has to move out of the way. Also, the faster it is falling, it will hit more air molecules it has to move. So, the larger those things are, the larger the force of air resistance. 

Thus, as you might expect, a book and a single sheet of paper should have the same air resistance (at least, when it first starts to fall- eventually the book will have more because it is moving faster). So, they have the same force pushing up on them- but that *force* causes more *acceleration* on the lighter object (the sheet of paper) than the heavy one (the book). 

Playing with some numbers (we'll choose easy ones). Imagine you have a 1 kg book and a 2 kg book, and we'll say the acceleration due to gravity is 10 m/s^(2). The, using Newton's 2nd law we can see that the force due to gravity on the first book is 10 Newtons (A Newton is the SI equivalent to a pound, it has units of force) and the second book at a force of 20 N. But now imagine each of them have 5 Newton's of air resistance acting on them. So, the total force acting on book 1 is 5 N, and the total force acting on book 2 in 15 N (Forces add- and since they are in opposite directions you are getting 10-5 and 20-5). So now, we can use Newton's second law to calculate their acceleration:

&gt; a = F/m (just re-arranged) 

&gt; a1 = 5N/1kg = 5 m/s^2

&gt; a2 = 15N/2kg = 7.5 m/s^2

So, the book that weighs more (but has the same air resistance) accelerates faster. ",null,0,cdj932w,1r37tg,askscience,top_week,4
DanielSank,"Please, please please show your son this:

Hold the paper and book up in the air shoulder width apart and drop them at the same time. The book hits the floor first.

Now place the paper on top of the book. Make sure it's pretty flat and hugs the top of the book as best you can get it (edges of the paper must not extend past edges of book). Now drop the book. They fall together.

If nothing else this will astound him enough that he won't forget it and will continue to seek answers.",null,0,cdjh47y,1r37tg,askscience,top_week,4
dampew,"Smart kid.  Tells it like it is.

Wind resistance is proportional to the area of the object.  But heavier objects are heavier and push down harder on the air.  So if two objects of the same size are falling through the air, the heavier object will fall faster.",null,0,cdjgo0j,1r37tg,askscience,top_week,3
ww-shen,"Little OFF: If you explain something to your kid (or any kids) it is a common wish for them to understand it. Sometimes explain the matter more advanced way (without the simplification). This way the kid will learn that there are things he cannot understand, and in time, that many things you don't understand either. It will open his perspective about the nature of knowledge.",null,0,cdji8zp,1r37tg,askscience,top_week,3
Surf_Science,"Dropping things may not be the way to go. You should try rolling them down a hill. I think this was the way Galileo went about some of his experiments (though not a hill and more precise). I think the only time you'd get into a problem would be with weird shapes so you should be able to do a bit better experiment with different weighted balls. 

With respect to the paper book issue. Isn't the paper loosing part of its downward speed from gravity by moving in a lateral direction? That may be easier for a kid to understand.  ",null,0,cdj7jdk,1r37tg,askscience,top_week,2
jayman419,"For an simple unscientific demonstration, you can [build a parachute out of a plastic bag, some tape, and some yarn](http://www.wikihow.com/Build-a-Plastic-Parachute) and have him experiment with different weights, to show that wind resistance is a limited thing that only slows objects by a certain rate.

Like hook a toy action figure up to it, and drop it from the balcony (if you have one) and then hook up something heavier, and he can see that the same parachute and the same air affect objects differently. You could even hook up the book to the parachute and race it against the sheet of paper.

That way you don't have to get into the maths of wind resistance, and you can show him the difference in practical terms.

EDIT: There's also the Apollo 15 video (one example of it is here: http://www.youtube.com/watch?v=5C5_dOEyAfk) where they dropped a hammer and a feather on the Moon, you can let him see what happens when there's no air.",null,0,cdj7oyv,1r37tg,askscience,top_week,2
natty_dread,"The thing is, that gravity **does** pull with different force on different things.

The actual constant value is *acceleration* not *force*. The sheet of paper and the book are being accelerated equally towards the ground. Since their shape is the same, the force of air resistance should be roughly the same too. However, since the book has more mass, the earth pulls harder on the book, thus making it fall faster.


The mathematical description is as follows: (This is meant to give **you** additional insight in order to give you well funded knowledge to share with your son)

 
Newton's law of gravity states that F_Gravity=G * m * M/r^2 (G being the gravitational constant, M&amp;m the masses and r the distance between them)

Now, Newtons Axiom states that the movement of a mass under the influence of a force is given by F=m*a

If we substitute F with F_Gravity we get G * m * M/r^2 = m * a.

As you can see, m can be canceled out of the equation.

This leaves us with a = g = G*M/r^2 .
This equation shows, that the acceleration of all masses in a gravitational field is, indeed, equal.

Then why are some things heavier than others?

To answer this question, we have to ask ourselves, what weight is. Weight is the force with which an object is pulled towards the earth.
Since the force, is given by F=m * a, and the mass of different object differs, the force which objects are pulled towards the ground is does not necessarily have to be constant.

",null,2,cdj81fn,1r37tg,askscience,top_week,4
joshhinz,"I always found the dropping two objects experiment misleading and unintuitive, because of the very problem you encountered! It simply isn't true in many cases on earth because there is air resistance on earth and even buoyancy forces (most evident with helium balloons). Maybe try to explain to him this fact to some degree then show him the [Apollo 15 Hammer and Feather Drop](http://www.youtube.com/watch?v=-4_rceVPVSY) ",null,0,cdj8gkl,1r37tg,askscience,top_week,2
tagaragawa,"I think one of the first mind-blowing things one learns in life is when you are taught that *everything* attracts *everything else*. Please include this in your discussions. It is not obvious at all, and only nowadays do we have experiments sensitive enough to measure the influence of, say, a solid sphere upon another test object. You can however easily convey this by pointing out that the earth moves around the sun, and the moon around the earth. The simplest explanation is that both the sun and the earth 'exert gravity', and from there that everything 'exerts gravity'.

Related: does anyone know when this idea was conceived? Did the early inventors of heliocentric models immediately make this implication, or was it not until Hooke and Newton that is was fully realized?",null,0,cdj95vn,1r37tg,askscience,top_week,2
therationalpi,"Great questions, I'll try my best to answer this.

Your questions are about a [vibrating string](http://en.wikipedia.org/wiki/Vibrating_string). The equation for the frequency of a vibrating string is given by

f=(1/2L) \* (T/)

Where f is frequency (pitch), L is length, T is tension, and  is linear density (mass per unit length). The answers to your first two equations should be apparent from that equation, but I'll explain each by itself.

1. You increased the length while holding T and  constant, thus f decreased.

2. You increased L while also increasing T and lowering  ( decreased because the mass of the rubber band is conserved, just spread over a longer band now).  ought to change as =_0\*L_0/L, where _0 and L_0 are the initial linear density and length respectively. Thus, the new equation becomes: f=(1/2) \* (T/_0\*L_0\*L). Tension will also increase as you increase the length, but if it's increasing more than just linearly, you'll find the frequency increasing.

As for three, I'm not certain, but I would assume that temporal resolution is to blame.",null,1,cdj7r5w,1r36co,askscience,top_week,5
IAmMe1,"Let's talk quickly about what determines pitch. The pitch of the sound wave is determined by its frequency, which is set by the number of times per second that the object producing it (the rubber band) wiggles up and down. Now, the wiggling of the rubber band itself is set by two things: the wavelength of the wiggles and the wave speed in the rubber band.

The wavelength is basically how long a wave has to be to ""fit perfectly"" into the amount of rubber band available to wiggle. This means that the wavelength is basically the length of the wiggling part of the rubber band (over 2 for the fundamental frequency, but that's not important).

The speed of waves in the rubber band is set by the band's density (a property of the material it's made out of) and the tension in the band. Higher tension makes the wave travel faster.

Also, you should know that pitch goes up if wavelength goes down and pitch also goes up if the wave speed goes up.

1) You have kept the tension in the band the same while increasing the length. This means you haven't changed the wave speed, but increased the wavelength, so the pitch goes down.

2) You have increased both the tension (EDIT: and decreased the density) and the length. Increasing the tension (EDIT: and decreasing the density) should increase the pitch, and increasing the length should decrease the pitch, so it's a question of which effect is bigger. It turns out that for your rubber band, the effect of tension was bigger. This may not be true for all rubber bands at all points in their stretch.

3) Not my field, so I'm going to have to leave this one for someone else.",null,0,cdj7spc,1r36co,askscience,top_week,3
KarlOskar12,"3) Our eyes can see about 30 FPS. When the rubber band *blurs* as you say, it just means that it is cycling at a faster rate. Side note, if you get a strobe light where you can alter the rate of light flashing and you set it to flash at *the same rate as the rubber band is vibrating* the rubber band will appear to be stationary (as long as you shut all the other lights off).",null,0,cdjabwu,1r36co,askscience,top_week,2
kooksies,"I believe fibrocartilage is stronger than hyaline cartilage. 
This is because it contains type I collagen which form thicker fibres and are packed more densely together than collagen found in hyaline cartilage. Meaning it has good tensile strength, flexibility and general rigidity.

Fibrocartilage can be found in tendons and ligaments, but they are mainly found in the tissue that separates your vertebrae.   

It shouldn't break as easily as hyaline cartilage, but i have no idea how it would affect you during working out. You should probably seek a  doctor's opinion !",null,0,cdjarwg,1r369o,askscience,top_week,2
KarlOskar12,"Cartilage is poorly vascularized tissue and as a result of low blood supply (amongst other things) it has poor regeneration capabilities. When damaged it is not replaced by another type of cartilage. However, cartilage that is repeatedly injured becomes highly vascularized and consequently calcified and it starts to turn into bone. This process causes arthritis.",null,1,cdj9cjp,1r369o,askscience,top_week,1
mc2222,"when shooting with a round aperture in a camera, every point will be imaged as a tiny round point.  When imaging something like a star (a very good point source), the image will be a point, but the out of focus image will be nothing more than a big round disk.  It gets a bit more interesting when we change the shape of the aperture.  Let's put a disk in the center of the aperture to block some light - [like the design of some telescopes](http://nimax-img.de/Produktbilder/normal/10215_2/Meade-Schmidt-Cassegrain-telescope-SC-203-2034-8-UHTC-LX90-GoTo.jpg).  The aperture for such systems is a doughnut shape.  When you try an image a star, it will be a pinpoint at the focal plane, but will be doughnut shaped either inside or outside of focus, like [this](http://legault.perso.sfr.fr/airy_collim_2.gif).

Bookeh is simply exploiting the shape of the aperture to make out of focus point sources take on that shape.  Want heart shaped bokeh? Simply cut out a heart shaped pinhole and put it in front of the camera.  The camera will still focus on what you want it to, but everything that's out of focus will look like a heart.",null,0,cdjdc4h,1r35z3,askscience,top_week,1
quality_is_god,"Engineer here.
Essentially the answer is never. 

Unless the force exceeds the yield stress of the steel in the spring causing the steel to fail, or if the spring is heated up and cooled down (re-crystallizing) in the deformed shape, or it completely rusts, it will spring back when the force is released.

Metals can fail under repeated compression/release cycles if close to the yield stress, but a constant load like a weight will not cause steel to permanently compress (with the  above noted exceptions).

Many of the engineered structures you take for granted rely on the permanence of the elasticity of steel below the yield stress.",null,0,cdjdjkq,1r35c8,askscience,top_week,13
nosignificanceatall,"It is common to describe the creep rate of a material by an empirical power law of the applied stress, i.e.  
d/dt = k\*^n where k is a constant determined by the creep mechanism, temperature, etc.  If we substitute =/E, then we have a simple differential equation for the time-dependence of the stress.  It can be solved to yield  
(t) = ( (0)^(n-1) - E\*k\*t\*(n-1) )^1/(n-1)

This model predicts that the stress never reaches zero, so there will always be some expansion when the load is removed, but that the expansion becomes arbitrarily small as time increases.  If you run experiments/look up tabulated values to determine E, k, and n, then you can use the above equation to predict how much your material will expand when you remove the applied load after some time t.

Adjustments can be made to the differential equation to better suit the particular material that you are working with.  For example, when the dominant mechanism of deformation is dislocation creep (as is probably the case for your example of a spring), it is common to replace ^n with (-\*)^n - that is, the material ceases to creep once the stress drops below some threshold \*.  In this situation, the material will still expand by \*/E even if you leave it under stress for an indefinite amount of time.",null,2,cdje1vg,1r35c8,askscience,top_week,5
Trill-Nye,"Materials scientist here. Technically, all materials will, over a long enough time scale, relax at the atomic level such that any stresses are negated.

Stress, like almost everything else, really happens at the atomic level. When you compress a string, the atoms in that metal must rearrange in some way to accommodate this deformation. This can means stretching of atomic bonds or simply the grinding together of microscopic grains within the metal, for example. These structural rearrangements will leave the atoms in a high energy state.

Thermodynamically, atomic systems tend to relax to their lowest energy accessible state. Thus, when the atoms are rearranged during deformation, they will then move around until they adopt an arrangement that minimizes the material's internal stress, one similar to that exhibited prior to deformation. The problem is that, at room temperature, this process is extremely slow. The motion of atoms, called diffusion, is, for most materials, only appreciable at high temperatures.

So yes, the relaxation time of an applied stress can be calculated if a lot of material specific values related to atomic vibrations, diffusion, grain boundary slide, structure defects, etc. are known. For most engineering materials, at room temperature, this will yield very long times (sometimes geologic timescales). Plastics, on the other hand, often relax very quickly, such that it can be observed in laboratory experiments. A good place to start if you want to learn more would be to read about the highly industrial-relevant process of [annealing](http://en.wikipedia.org/wiki/Annealing_(metallurgy)). ",null,0,cdjey0z,1r35c8,askscience,top_week,4
FlyingSagittarius,"Springs are usually made of metal, which has already been discussed.  Ceramics act similarly, within the elastic limit at least.  Polymers, on the other hand, can have both a strain and a strain rate, so if you apply a constant strain to most plastics they will eventually flow to relieve the stress.

The basic equation for this behavior is e = (S / E) + (T / V)*t.  e is the normal strain, S is the normal stress in the material, E is the tensile modulus, T is the shear stress in the material (which always exists, even if only normal force or stress is applied), and V is the viscosity.  If you set a value for e and you know everything else, you can solve for the time.  E and V are material properties, T can be calculated from S, and S can be calculated from e.  The resulting expression is a differential equation that shows the stress in the material asymptotically approaches zero, and the strain rate asymptotically reaches zero.  The time constant depends on the material properties.

This is for a constant strain, though.  What about for a constant stress?  This would be like suspending a weight with a polymer rope.  As the polymer rope supports the weight, the polymer flows to relieve the stress.  But the weight responds by dropping down, and ""reapplying"" the stress.  So the stress stays constant, and the rope would respond with a constant strain, plus a constant strain rate.  So it'll stretch initially as the weight is applied, and will stretch even more as time goes on.  (Eventually it'll stretch too much and snap, though.)",null,0,cdjij1v,1r35c8,askscience,top_week,3
OrbitalPete,"Erosion is overwhelmingly responsible.

Once the plate has moved sufficiently for an island to lose its source magma, there is no replacement material being supplied. AGes along the chain look like this: http://www.uhh.hawaii.edu/~kenhon/GEOL205/Chain/chnmap.jpg

It's basically impossible to say exactly how big the other islands and seamounts were at their peak, but there's no reason they couldn't be similar sizes to Hawaii. There may also have been smaller islands grouped clser together at times. Once you're looking at the old seamounts it's likely that the material preserved now is simply the highly agglutinated material surrounding the vent itself, with large amounts of the surrounding pillow lavas and hyaloclastite (which form the bulk of submarine volcanics) having been strongly eroded.",null,0,cdjiimo,1r329d,askscience,top_week,4
RageousT,"Well, the hotspot is, somewhat unsurprisingly, hot, and thus the crust above it is hot, and thus buoyant. This contributes to Hawai'i being as high as it currently is. As the hotspot moves away from the old islands, they cool, and thus sink. Not entirely sure how much this compares to the contribution from erosion, however.",null,3,cdj8ng3,1r329d,askscience,top_week,2
ScootMaBoot,"A penny would sink.

[The density of water at the bottom of the Mariana Trench is only ~5% greater than at the top.](http://en.wikipedia.org/wiki/Mariana_Trench)

Liquid water (and liquids in general) are [not very compressible](http://en.wikipedia.org/wiki/Compressed_fluid), especially when compared to gases.",null,2,cdjc7my,1r2z86,askscience,top_week,7
varodrig,"Since liquids as virtually incompressible, water has virtually the same density even at the bottom of the ocean. The density of copper is 8960 kg/m3. The density of water is 1000 kg/m3. Therefore a penny would sink all the way to the bottom without much noticeable slowing.",null,0,cdjk3iu,1r2z86,askscience,top_week,3
jofwu,"Sounds to me like you're mixing up pressure and density. While a gas's density is directly related to pressure, this is not typically the case for liquids (including water).

Buoyancy is the force responsible for making things float, and it is a function of the relative densities of the fluid and the object. Water pressure is not involved.",null,0,cdjz5t6,1r2z86,askscience,top_week,1
datums,"This is a politically loaded question, but in general, the answer is no. Most of the significant gains have come as a result of regulation by governments. Two examples of this are CFCs (chlorofluorocarbons), and PCBs (polychlorinated biphenyls). Though they were both highly damaging, and demonized in the press, they were both widely utilized in markets that had not banned them.      
  
A potential reason for this is that their use is not always easily tied to particular consumer products. For example, PCBs were used mainly in the operation of heavy machinery. It would be hard to say that a product was 'PCB free', given the number of mining, refining, and manufacturing steps involved in making most consumer products. One might be able to say that a pencil is PCB free, but it would be difficult to say that none of the machinery involved in any of the steps required to make a pencil were PCB free. 
  
",null,0,cdja8ra,1r2yus,askscience,top_week,1
adamsolomon,"The Milky Way is quite a standard spiral galaxy. Nothing especially out of the ordinary. The Milky Way, by the way, is filled to the brim with stars with planets, and it's quite likely that some of those have life. Already we're beginning to find planets almost like Earth around other stars, and those are generally hard to find - not to mention we can only look in our solar neighborhood. The same goes for any other decently-sized galaxy. So it's not as if life on Earth is necessarily a rare feature in itself, either.",null,0,cdj5orq,1r2xuh,askscience,top_week,6
wwarnout,"For stars whose axis of rotation is not pointing toward us, we can measure the light (spectra) of the left and right sides of the stars.  Since one side is moving away from us, and the other is moving toward us, there is a shift in the spectra (similar to the doppler shift that causes a car coming toward us to sound different than when it goes away from us).

By observing this difference, astronomers can calculate the rotational speed.",null,0,cdj6zlq,1r2xu4,askscience,top_week,4
selfification,"So you know how we write decimal numbers?  What exactly are we doing?  We're trying to represent numbers as digits that we ascribe a meaning to based on their place in the sequence.  If our number is in base b and our digits are a0,a1,a2...  Then the number we are trying to represent is a0 + a1*b + a2*b^2 + ...  Power series generalize this concept.  In a power series, your coefficients can take on any value and you can use an arbitrary base.",null,1,cdjg3jo,1r2u62,askscience,top_week,4
DarylHannahMontana,"Conceptually, I think of power series (and Fourier series) like I think of a chemical compound having a ""recipe"" in terms of its component elements. For instance, the chemical aluminum silicate is 

   Al_2 O_7 Si_2

i.e. to make an aluminum silicate molecule, you need 2 aluminum, 7 oxygen and 2 silicon. 

In a similar way, we can decompose a function in terms of simpler functions (polynomials or sine and cosine), though there are some obvious differences, mainly that you can have any amount of each component (unlike compounds/elements, where only integer multiples happen), and there are an infinite (but [countable](http://en.wikipedia.org/wiki/Countable_set)) number of ""elements"" to consider. For instance, the function e^x and its Taylor series

   e^x = 1 + 1 x + 1/2 x^2 + 1/6 x^3 + 1/24 x^4 + ...

give you a ""recipe"" for e^(x): you need a 1, an x, half of an x^(2), one-sixth of an x^(3), etc.

Alternatively, if you already know some linear algebra, a more directly parallel idea is that of a vector space, and the idea of a basis. Recall that a basis for an n-dimensional vector space is a collection of n vectors {v_1, v_2, ... v_n} that ""span"" the entire vector space. That is, every other vector can be written

   w = c_1 v_1 + c_2 v_2 + ... + c_n v_n

with *some* choice of numbers c_1, ..., c_n.

This analogy is especially apt for Fourier series, where there is a notion of orthogonality; i.e. you can rigorously define what it means for two functions to be ""perpendicular"" and if you know some linear algebra, you know that this can make bases much easier to work with.

There are also ""series expansions"" of functions that are much more general than power series or Fourier series; depending on the setting and application, there are reasons that make some of these more ""exotic"" series better suited to the task at hand. If you're interested, you might look up [wavelets](http://en.wikipedia.org/wiki/Wavelet).",null,2,cdjop6k,1r2u62,askscience,top_week,4
aczelthrow,"Power series are used in two major ways.

Often in analysis (calculus and such) power series are simply types of functions that behave like polynomials of infinite degree. Sometimes they only converge (or make sense) for particular values of the variable. Sometimes we write other functions in terms of power series, such as exp(x) = sum(n = 0 to infinity) x^(n)/(n!). There are methods to figure out how to write a given function as a power series. In that sense, a power series is way to represent a function that is often more convenient. For instance, if you had paper and pencil only, how would you compute exp(2)? The best way is to use the power series. (Many calculators and mathematics software internally use power series, or something related, to compute exp, sin, cos, etc.)

In algebra, by contrast, we sometimes use ""formal"" power series. A formal power series looks just like a usual power series but it's regarded as its own object divorced from any notion of values of x for which it converges or not. The coefficients of the formal power series may be chosen from a given ring or field (generalizations of real numbers). We can add, multiply, even divide formal series using the usual rules, and doing so we define a ring or field of power series that are of interest to algebraists. This kind of power series is less encountered in applications.",null,1,cdjhknw,1r2u62,askscience,top_week,2
Spiralofourdiv,"Which part is confusing, the ""power"", or the ""series"" part?

It's important you understand what a series representation of a function is fairly well before you can understand what makes a power series special. A power series is just a clever way of rewriting a function as a, perhaps infinite, polynomial using its derivatives and a center. How it works takes a bit of calculus background. If you have a calculus background and are still mystified, then your hang up probably will be answered sometime throughout a real analysis course. 

However, I have a feeling exploring series' in general as representations of functions might answer part of your question. A great example is a Fourier Series, which is a way to describe **any** periodic function as a sum of sines and cosines. First, consider and understand [this](http://en.wikipedia.org/wiki/File:Sine_curve_drawing_animation.gif) illustration of sine. Now look at what a [Fourier Series](http://i.imgur.com/w1IuPKJ.gif) looks like in the same fashion. If you were to add an infinite number of circles, you'd have a square wave, even though it's simply a sum of the very-non-square trig functions. Each additional circle is just another sine or cosine term in the series, and this makes sense in the picture! A little circle is added on that behaves the same way. The specific radii and angular frequency will depend on the series, but study this gif and it will make sense how the series approaches a square wave.

Most series work in a really similar way: they start out as a broad approximation and then, as more terms are added, get closer and closer to the function. Sometimes you can describe the function in a finite number of terms, and sometimes you need an infinite amount. A power series is no different: each term in the sequence of partial sums is a more accurate approximation of the function than the proceeding element. With an infinite amount of elements, we get infinite accuracy and we're done.

If you're asking WHY an infinite series is easier to deal with than the original, nice looking function, all I can say is there are too many reasons to count, or at least applications to count. Most of the time it has to do with certain operations being significantly easier to do on the series representation.",null,0,cdkx38e,1r2u62,askscience,top_week,1
iorgfeflkd,"There really isn't much antimatter in the known universe. Why, is a mystery. Positrons are created fairly frequently and they often annihilate with electrons. If you look for positron-electron annihilation in the sky you see it [coming from the galactic center](http://www.cesr.fr/~jurgen/spi/spi511map-pr-hres.jpeg).",null,2,cdj006a,1r2pq1,askscience,top_week,16
miczajkj,"It mostly does. When for example an positron is produced in the ^+ Radiation (what happens rather often), it ionizes the medium it is moving in and slows down until it finally annihilates with an electron.

The biggest part of antimatter in our universe consists of anti-neutrinos: just like neutrinos they just don't interact with other matter/anti-matter very often and are therefore very likely to have a long lifespan. They are for example produced in the ^- radiation. 

As we know so far, there is no particular difference between matter and anti-matter that can explain, why there is much more matter, so much more, that anti-matter can't really enjoy it's existence until it annihilates with regular matter. This is one of the unsolved mysteries of particle physics. ",null,0,cdj09t7,1r2pq1,askscience,top_week,3
atomfullerene,"The fundamental continental arrangements which lead to an ice age (namely the formation of the Isthumus of Panama, and, for Antarctic ice, the Drake Passage)  have not changed in the past few thousand years, so barring anthropogenic climate change, we'd be heading into another glacial period as soon as we swing back around the proper point in the [Milankovitch cycle](http://en.wikipedia.org/wiki/Milankovitch_cycles).",null,0,cdjpewm,1r2ppd,askscience,top_week,3
ModernTarantula,"Just saw on TV. The Earth wobbles on its axis. At a certain wobble winters would be shorter The orbit also changes from more elliptical to less. Less elliptical the winters are warmer. Both those conditions are current.  However we are closer to the next glacial period, due to be in 1500 years.",null,2,cdjh7qp,1r2ppd,askscience,top_week,2
blacksheep998,"It has more calories in the sense that hot things have more energy than cold ones. But it wouldn't have more usable calories. Not directly at least since the body cannot directly convert heat into a usable calorie source for metabolic action.

Indirectly though it might give us a little more energy. When you eat something it enters your stomach and rapidly changes to body temperature. In the case of something cold this means it absorbs heat from your body. This heat has to be replaced and the body does so by burning calories.

If the food you ate was warm then the heat energy would be absorbed by your body which would not need to burn as much to keep warm, for a few minutes anyway.

I'm on my phone so I can't do any calculations right now to see what exactly the difference is, but its not a lot. Not what you'd consider a 'significant amount.'",null,10,cdiym89,1r2n1j,askscience,top_week,56
Gibonius,"Most foods have specific heats around 0.6 to 0.8 kcal/kg*C.  A steak has around 2000 kcals/kg in chemical energy.  If we look at a +40C change in temperature (reasonable), we're looking at 26 kcal/kg (for steak).

That's 1.3% more energy, which I'd say counts as ""not significant."" 

That's neglecting any consideration of how the body uses thermal energy.  I'd be reasonably comfortable assuming it's far less than 100% efficient.  ",null,4,cdiz4o9,1r2n1j,askscience,top_week,24
Oznog99,"Only in one specific case of hypothermia.

The body produces about 100W of heat, and in normal operation regulates internal temperature to ~98.6F by regulating blood flow to the skin and perspiration.

If you drink 1L (1KG) of icewater at 0C (hopefully this takes awhile), this will steal 37,000 ""small calories"" (the proper chemistry term) , which would be 37 ""food calories""/""large Calories"", to warm it up to body temp.  This is confusing, but the calories for food you know are actually ""kilocalories"".

But the body doesn't work that way.  As you drink icewater, the body senses a small temperature drop and reduces blood flow to the skin.  If you were sweating before, you'll stop sweating now. The skin is not as warm and stops losing as much heat to the environment.  

Soon the same generation of heat in the body, with a *reduction* in heat loss through the skin, restores temp to normal and everything goes back to how it was before.

In general the body does NOT increase metabolism to restore heat, so the cold water (or food) does not count against your dietary calories.  

There is an exception, and that's ""shivering"", an automatic emergency defense.  Shivering can consume 252 kcal/hr!  IF you were shivering, and drank hot unsweetened tea with no dietary calories, the added heat will reduce the shivering condition and thus decrease the amount of calories you burn during that day.  

It is an unusual case, though.  And a terrible concept for a diet plan.  As noted, simply turning down the AC to 58F will not cause your metabolism to increase at all.  You must be *shivering*, which is very uncomfortable, and wreaks havoc on body processes because a hypothermia is an emergency condition the body is responding to.  You can't think clearly, digestion may slow down or speed up, your muscles may cramp.  Greater calorie consumption can be had simply with moderate exercise, and going for a walk is far more enjoyable- and effective- than trying to take a bath in icewater.

",null,1,cdjd0b7,1r2n1j,askscience,top_week,6
CanadianSnow,"There are two things to consider, when both dishes have the same amount of potential calories
1) the amount of body heat required/expended to heat the cold food as it enters your body so it can be digested. Our bodies do not digest cold food, even ice cream is first heated to body temperature before it can be digested fully.
2) The thermogenic effect of food relates largely to spiciness of food, however a very hot dish will cause the body to expend calories via sweating to cool the body down from the heat/spiciness of the food.",null,0,cdj4g7x,1r2n1j,askscience,top_week,2
rightwaydown,"Actually food cooling can undergo chemical reactions changing it's exact extractable energy. For instance potatoes left to go cold have a higher percentage of resistant starch. 

I'm can't think of many examples though. In general I think they would be very similar. Of course there are many examples of food gaining calories from being cooked, but that's not your question.",null,0,cdjnarv,1r2n1j,askscience,top_week,2
ModernTarantula,The Calories of food is that which is generated by burning it completely in a closed stove. Our metabolism is the breakdown  and rebuilding of our body. The calories needed for that is only chmical not thermal. If we wanted to save energy used to maintain body temperature we should use the stove to heat the air not the soup.,null,0,cdjgo5l,1r2n1j,askscience,top_week,1
blacksheep998,"It has more calories in the sense that hot things have more energy than cold ones. But it wouldn't have more usable calories. Not directly at least since the body cannot directly convert heat into a usable calorie source for metabolic action.

Indirectly though it might give us a little more energy. When you eat something it enters your stomach and rapidly changes to body temperature. In the case of something cold this means it absorbs heat from your body. This heat has to be replaced and the body does so by burning calories.

If the food you ate was warm then the heat energy would be absorbed by your body which would not need to burn as much to keep warm, for a few minutes anyway.

I'm on my phone so I can't do any calculations right now to see what exactly the difference is, but its not a lot. Not what you'd consider a 'significant amount.'",null,10,cdiym89,1r2n1j,askscience,top_week,56
Gibonius,"Most foods have specific heats around 0.6 to 0.8 kcal/kg*C.  A steak has around 2000 kcals/kg in chemical energy.  If we look at a +40C change in temperature (reasonable), we're looking at 26 kcal/kg (for steak).

That's 1.3% more energy, which I'd say counts as ""not significant."" 

That's neglecting any consideration of how the body uses thermal energy.  I'd be reasonably comfortable assuming it's far less than 100% efficient.  ",null,4,cdiz4o9,1r2n1j,askscience,top_week,24
Oznog99,"Only in one specific case of hypothermia.

The body produces about 100W of heat, and in normal operation regulates internal temperature to ~98.6F by regulating blood flow to the skin and perspiration.

If you drink 1L (1KG) of icewater at 0C (hopefully this takes awhile), this will steal 37,000 ""small calories"" (the proper chemistry term) , which would be 37 ""food calories""/""large Calories"", to warm it up to body temp.  This is confusing, but the calories for food you know are actually ""kilocalories"".

But the body doesn't work that way.  As you drink icewater, the body senses a small temperature drop and reduces blood flow to the skin.  If you were sweating before, you'll stop sweating now. The skin is not as warm and stops losing as much heat to the environment.  

Soon the same generation of heat in the body, with a *reduction* in heat loss through the skin, restores temp to normal and everything goes back to how it was before.

In general the body does NOT increase metabolism to restore heat, so the cold water (or food) does not count against your dietary calories.  

There is an exception, and that's ""shivering"", an automatic emergency defense.  Shivering can consume 252 kcal/hr!  IF you were shivering, and drank hot unsweetened tea with no dietary calories, the added heat will reduce the shivering condition and thus decrease the amount of calories you burn during that day.  

It is an unusual case, though.  And a terrible concept for a diet plan.  As noted, simply turning down the AC to 58F will not cause your metabolism to increase at all.  You must be *shivering*, which is very uncomfortable, and wreaks havoc on body processes because a hypothermia is an emergency condition the body is responding to.  You can't think clearly, digestion may slow down or speed up, your muscles may cramp.  Greater calorie consumption can be had simply with moderate exercise, and going for a walk is far more enjoyable- and effective- than trying to take a bath in icewater.

",null,1,cdjd0b7,1r2n1j,askscience,top_week,6
CanadianSnow,"There are two things to consider, when both dishes have the same amount of potential calories
1) the amount of body heat required/expended to heat the cold food as it enters your body so it can be digested. Our bodies do not digest cold food, even ice cream is first heated to body temperature before it can be digested fully.
2) The thermogenic effect of food relates largely to spiciness of food, however a very hot dish will cause the body to expend calories via sweating to cool the body down from the heat/spiciness of the food.",null,0,cdj4g7x,1r2n1j,askscience,top_week,2
rightwaydown,"Actually food cooling can undergo chemical reactions changing it's exact extractable energy. For instance potatoes left to go cold have a higher percentage of resistant starch. 

I'm can't think of many examples though. In general I think they would be very similar. Of course there are many examples of food gaining calories from being cooked, but that's not your question.",null,0,cdjnarv,1r2n1j,askscience,top_week,2
ModernTarantula,The Calories of food is that which is generated by burning it completely in a closed stove. Our metabolism is the breakdown  and rebuilding of our body. The calories needed for that is only chmical not thermal. If we wanted to save energy used to maintain body temperature we should use the stove to heat the air not the soup.,null,0,cdjgo5l,1r2n1j,askscience,top_week,1
puma721,"The idea of a fan isn't that it makes your room cooler (although ceiling fans can move cold air from the ground upward, or hot air off the ceiling)
The benefit a fan gives you is that you feel cooler because the air is more efficient at pulling moisture off your skin.",null,0,cdj2jp7,1r2n13,askscience,top_week,14
LogicForDummies,"Depends on three factors mainly. 1) The insulating value of the ""walls"" 2) the temperature difference between the ""inside"" and the ""outside"" of the sealed room and 3) The amount of heat produced by the fan motor.

If the outside temp is lower AND the insulating value allows enough heat to pass through the walls at a rate higher than the heat added from the motor AND the rate of thermal transfer is higher with the air moving over the ""walls"" (think heatsink) versus static air after factoring the added motor heat, then it will cool the room. Otherwise it will warm the room. But if you were in the room (which also adds heat) you would still ""feel"" cooler with the fan on even though you could be heating the room, up to a point.",null,0,cdiygfh,1r2n13,askscience,top_week,10
S7R4nG3,"This question is highly dependent on what you have inside the room ""measuring"" the temperature?

If you are taking a person inside and using their sense of temperature as a ""measurement"" then yes, they would feel cooler as a result of normal sweat evaporation on the skin.

If you are taking a thermometer in the sealed room, then it will always read the ambient temperature of the room as it is uninhibited by the effects of [wind chill](http://en.wikipedia.org/wiki/Wind_chill#Explanation).

So, with a dry thermometer at the stable room temperature, the effect of adding a ceiling fan to the room would not change its reading. 

However, because you state that the room would be warmer than the ambient temperature around the room, the effect would be just as you specify, it would increase the rate by which it cooled to the ambient temperature.",null,0,cdiyfx5,1r2n13,askscience,top_week,4
InternalEnergy,"Chemical engineer here--lots of training in thermodynamic analysis.

Let's make some assumptions. First, the room is completely sealed off to flux of matter. Not a single atom passes through the doors, walls, etc.

Second: the room is well-insulated, so an adiabatic assumption is valid. No heat is transferred through the boundaries of the room.

Third: the boundaries/walls are completely rigid. They will not move at all even if the pressure inside the room increases.

Ok. Now let's do some thermodynamics. First law of thermo states that matter and energy is conserved: neither created nor destroyed. With no heat flux (adiabatic), matter-material flux, or pressure-volume work (rigid boundary), the only input to the room is the electrical energy required to power the fan. Moving stuff takes energy--air is no exception, and certainly the blades of the fan have mass too. So we have energy coming in, in the form of electricity.

Ever feel the heat coming off of your computer or smart phone during use? Electrical energy converts easily to heat.

But we insulated the room, remember? And we sealed it too, so no hot air leaves, to take that energy away. So the end result is the room gets warmer. 

In fact, the only reason why a fan might make you perceive the room as cooler is because you sweat. The moving air evaporates that sweat, which requires heat energy, which your body supplies. 

Yay, thermodynamics!",null,0,cdj4a4m,1r2n13,askscience,top_week,3
puma721,"The idea of a fan isn't that it makes your room cooler (although ceiling fans can move cold air from the ground upward, or hot air off the ceiling)
The benefit a fan gives you is that you feel cooler because the air is more efficient at pulling moisture off your skin.",null,0,cdj2jp7,1r2n13,askscience,top_week,14
LogicForDummies,"Depends on three factors mainly. 1) The insulating value of the ""walls"" 2) the temperature difference between the ""inside"" and the ""outside"" of the sealed room and 3) The amount of heat produced by the fan motor.

If the outside temp is lower AND the insulating value allows enough heat to pass through the walls at a rate higher than the heat added from the motor AND the rate of thermal transfer is higher with the air moving over the ""walls"" (think heatsink) versus static air after factoring the added motor heat, then it will cool the room. Otherwise it will warm the room. But if you were in the room (which also adds heat) you would still ""feel"" cooler with the fan on even though you could be heating the room, up to a point.",null,0,cdiygfh,1r2n13,askscience,top_week,10
S7R4nG3,"This question is highly dependent on what you have inside the room ""measuring"" the temperature?

If you are taking a person inside and using their sense of temperature as a ""measurement"" then yes, they would feel cooler as a result of normal sweat evaporation on the skin.

If you are taking a thermometer in the sealed room, then it will always read the ambient temperature of the room as it is uninhibited by the effects of [wind chill](http://en.wikipedia.org/wiki/Wind_chill#Explanation).

So, with a dry thermometer at the stable room temperature, the effect of adding a ceiling fan to the room would not change its reading. 

However, because you state that the room would be warmer than the ambient temperature around the room, the effect would be just as you specify, it would increase the rate by which it cooled to the ambient temperature.",null,0,cdiyfx5,1r2n13,askscience,top_week,4
InternalEnergy,"Chemical engineer here--lots of training in thermodynamic analysis.

Let's make some assumptions. First, the room is completely sealed off to flux of matter. Not a single atom passes through the doors, walls, etc.

Second: the room is well-insulated, so an adiabatic assumption is valid. No heat is transferred through the boundaries of the room.

Third: the boundaries/walls are completely rigid. They will not move at all even if the pressure inside the room increases.

Ok. Now let's do some thermodynamics. First law of thermo states that matter and energy is conserved: neither created nor destroyed. With no heat flux (adiabatic), matter-material flux, or pressure-volume work (rigid boundary), the only input to the room is the electrical energy required to power the fan. Moving stuff takes energy--air is no exception, and certainly the blades of the fan have mass too. So we have energy coming in, in the form of electricity.

Ever feel the heat coming off of your computer or smart phone during use? Electrical energy converts easily to heat.

But we insulated the room, remember? And we sealed it too, so no hot air leaves, to take that energy away. So the end result is the room gets warmer. 

In fact, the only reason why a fan might make you perceive the room as cooler is because you sweat. The moving air evaporates that sweat, which requires heat energy, which your body supplies. 

Yay, thermodynamics!",null,0,cdj4a4m,1r2n13,askscience,top_week,3
do_od,"A full water bottle would not be crushed because water is very incompressible. [This experiment](http://earth.geology.yale.edu/~ajs/1969/ajs_267A_11.pdf/70.pdf) shows that in order to press a liter of water into a 9 deciliter bottle you have to subject it to a pressure equivalent to that on the bottom of a 33 km ocean, three times deeper than the Mariana trench which is the deepest known on Earth. Salt water and oil is similarily incompressible. ",null,1,cdizgkk,1r2lsg,askscience,top_week,18
blackhawk0093,"Deep-sea ecologist here. When we send down our collection chambers we ALWAYS fill them with surface water. As mentioned earlier on this thread, water isn't very compressible so going down 1,000 meters or so isn't going to cause a huge effect. On the other hand, there's been a few times when people forgot and sent them down with air, and they literally imploded on the side of the sub. This is (partly) also why all of the electronics on the sub are encased in oil- it won't short everything out like water or implode like air, and you can actually increase the pressure as the sub descends so it is always under a slightly higher pressure than the surrounding seawater (so small leaks will leak out, not in - better to lose some oil than gain some seawater). ",null,0,cdjbeer,1r2lsg,askscience,top_week,8
topher-dot-com,"The bottle would shrink but it wouldn't be crushed, because liquids aren't very compressible. [Here] (http://docs.engineeringtoolbox.com/documents/309/water-density-temperature-pressure_2.png) is a graph of the density of water as a function of temperature and pressure. even at 200 times ambient pressure the density only increases by about 1%. 

It doesn't really matter what is in the bottle as long at it is completely filled with liquid then it won't be crushed because liquids aren't very compressible.

Bathyscaphes, use gasoline to trim buoyancy because it is a liquid (so it won't compress at those depths) but it is less dense than water. So it is used to help it return to the surface since the ballast tanks that have air in them on the surface now have water, which can't be pushed out because the pressure is too great.",null,2,cdizvs2,1r2lsg,askscience,top_week,6
Thew_Nell,"This is probably a bit off topic, but you might be interested in the movie ""The Abyss.""  One of the characters is able to withstand incredible deep oceanic pressure because of this idea.  He 'inhales' a liquid that replaces the gasses in his body to reduce compression.  However,  this isn't entirely accurate, because it is a movie, but I thought I would bring it up regardless.
",null,1,cdj52ec,1r2lsg,askscience,top_week,3
S7R4nG3,"Ok, from what I'm remembering when going through dive training, if you take a full perfectly water-tight waterbottle that allows no water to escape regardless of pressure and that imparts no bouyancy, then yes it would crush as you moved it further down the water column. 

When you fill the water bottle at the surface, you are sealing in the water at the ambient sea-level pressure, thus as you moved it down the water column, the pressure around it increases crushing the bottle to compress the contents to ambient pressure. 

I specify contents here because water at sea-level generally has more dissolved gases that would allow the bottle to compress somewhat.

When considering similar effects of different liquids, its quite interesting to look at older [bathyscaphes](http://en.wikipedia.org/wiki/Bathyscaphe) that used gasoline to trim their bouyancy due to its higher density than water. You can also read up on [Henry's Law](http://en.wikipedia.org/wiki/Henry's_law) that defines the solubility of gases in liquids at pressure.",null,10,cdiz9wf,1r2lsg,askscience,top_week,3
do_od,"A full water bottle would not be crushed because water is very incompressible. [This experiment](http://earth.geology.yale.edu/~ajs/1969/ajs_267A_11.pdf/70.pdf) shows that in order to press a liter of water into a 9 deciliter bottle you have to subject it to a pressure equivalent to that on the bottom of a 33 km ocean, three times deeper than the Mariana trench which is the deepest known on Earth. Salt water and oil is similarily incompressible. ",null,1,cdizgkk,1r2lsg,askscience,top_week,18
blackhawk0093,"Deep-sea ecologist here. When we send down our collection chambers we ALWAYS fill them with surface water. As mentioned earlier on this thread, water isn't very compressible so going down 1,000 meters or so isn't going to cause a huge effect. On the other hand, there's been a few times when people forgot and sent them down with air, and they literally imploded on the side of the sub. This is (partly) also why all of the electronics on the sub are encased in oil- it won't short everything out like water or implode like air, and you can actually increase the pressure as the sub descends so it is always under a slightly higher pressure than the surrounding seawater (so small leaks will leak out, not in - better to lose some oil than gain some seawater). ",null,0,cdjbeer,1r2lsg,askscience,top_week,8
topher-dot-com,"The bottle would shrink but it wouldn't be crushed, because liquids aren't very compressible. [Here] (http://docs.engineeringtoolbox.com/documents/309/water-density-temperature-pressure_2.png) is a graph of the density of water as a function of temperature and pressure. even at 200 times ambient pressure the density only increases by about 1%. 

It doesn't really matter what is in the bottle as long at it is completely filled with liquid then it won't be crushed because liquids aren't very compressible.

Bathyscaphes, use gasoline to trim buoyancy because it is a liquid (so it won't compress at those depths) but it is less dense than water. So it is used to help it return to the surface since the ballast tanks that have air in them on the surface now have water, which can't be pushed out because the pressure is too great.",null,2,cdizvs2,1r2lsg,askscience,top_week,6
Thew_Nell,"This is probably a bit off topic, but you might be interested in the movie ""The Abyss.""  One of the characters is able to withstand incredible deep oceanic pressure because of this idea.  He 'inhales' a liquid that replaces the gasses in his body to reduce compression.  However,  this isn't entirely accurate, because it is a movie, but I thought I would bring it up regardless.
",null,1,cdj52ec,1r2lsg,askscience,top_week,3
S7R4nG3,"Ok, from what I'm remembering when going through dive training, if you take a full perfectly water-tight waterbottle that allows no water to escape regardless of pressure and that imparts no bouyancy, then yes it would crush as you moved it further down the water column. 

When you fill the water bottle at the surface, you are sealing in the water at the ambient sea-level pressure, thus as you moved it down the water column, the pressure around it increases crushing the bottle to compress the contents to ambient pressure. 

I specify contents here because water at sea-level generally has more dissolved gases that would allow the bottle to compress somewhat.

When considering similar effects of different liquids, its quite interesting to look at older [bathyscaphes](http://en.wikipedia.org/wiki/Bathyscaphe) that used gasoline to trim their bouyancy due to its higher density than water. You can also read up on [Henry's Law](http://en.wikipedia.org/wiki/Henry's_law) that defines the solubility of gases in liquids at pressure.",null,10,cdiz9wf,1r2lsg,askscience,top_week,3
fishify,"In the first several minutes after the Big Bang, the first nuclei formed.  At this time, this matter consisted of about 75% hydrogen and 25% helium; there was a little bit of lithium and beryllium formed, too.  (Hydrogen nuclei are the easiest to form -- quarks bind to form nucleons, free neutrons are unstable, and protons are single particles, whereas nuclear fusion processes are needed to generate even the slightly heavier nuclei.) To this day, we see that the matter in the universe is around 3/4 hydrogen and nearly 1/4 helium.

Atoms, rather than nuclei, did not become stable till the universe was about 370,000 years old; prior to this, the universe was so hot that electrons and nuclei did not reliably bind.

The nuclei heavier than the ones made right after the Big Bang are primarily formed via the nuclear fusion processes that go on in stars.  The first stars formed when the universe was around 200 million years old.  Over time, stars convert some of their matter to heavier nuclei: first helium, and then heavier nuclei (especially carbon, nitrogen, oxygen, neon, magnesium, silicon, sulfur, and iron, but other nuclei up to iron as well), and then nuclei beyond iron arising during supernova explosions.

So the short answer answer is that a lot of hydrogen was formed in the early universe, and all the subsequent nuclear processes have only converted a small fraction of that.",null,0,cdiy1qz,1r2kqn,askscience,top_week,8
f4hy,"Hydrogen is the simplest atom. It is just an electron orbiting a proton. In the early universe, the aftermath of the big bang, there were processes which could create protons, but protons repel each other unless you can get them really close together. So in the aftermath of the big bang, we were left with a bunch of protons, and electrons were able to find them, but it was not until stars formed and nuclear fusion at their cores to fuse the protons together into heavier atoms.",null,0,cdixjmh,1r2kqn,askscience,top_week,4
meaningless_name,"Does coarsely ground coffee make a stronger cup? I don't think it does. The higher surface area of finely ground coffee maximizes surface area exposed to water, as you pointed out. This definitely means fine grinds will produce stronger coffee in a given amount of time.",null,3,cdiz3ra,1r2jdi,askscience,top_week,24
2dwgs,"Coarsely ground coffee is usually exposed to water for more time (french press, for example) which leads to a bold cup of coffee. But finer ground coffee has more surface area exposed, which leads to a stronger flavor when comparing coarse vs. fine *given everything else is the same* (brew time, water amount, origin, roast, etc.)",null,0,cdizqhy,1r2jdi,askscience,top_week,8
endocytosis,"Not a barista, but increasing the grinding time on something has an effect on surface area.  Here's an example:

4 coffee beans, 16 x 16 x 16, total volume = 16384 units (4096 units each, assume for simplicity the coffee is cubic in shape)
SA = 6*L^2 = 6*256 = 1536 x 4 = 6144 units total surface area

The coffee goes through a special grinder and the grinds are perfectly halved in size to be Coarsely ground coffee:
8 grounds of coffee, 16 x 16 x 8 = 16384 total volume (2048 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*16) + 2(16*8) + 2(16*8) = 8*(512 + 
256 + 256) = 8192 units total surface area

This makes sense.  You don't put coffee beans into a coffee maker, you grind them up.

The coffee again goes through the special grinder and the grinds are perfectly halved in size to be Finely ground coffee:
16 grounds of coffee, 16 x 8 x 8 = 16384 total volume (1024 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*8) + 2(8*8) + 2(8*8) = 16*(256 + 
128 + 128) = 8192 units total surface area

No Change.  So we keep on grinding...

The coffee once again goes through the special grinder and the grinds are perfectly halved in size to be Super Finely ground coffee:
32 grounds of coffee, 8 x 8 x 8 = 16384 total volume (512 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(8*8) + 2(8*8) + 2(8*8) = 32*(128 + 
128 + 128) = 12288 units total surface area

Now the surface area is greater, but it is worth noting that the relationship between surface area and grind time isn't exactly linear.  However, the more finely ground something is, *usually* the slower it is that the coffee passes through.  More coarse grinds are bigger and don't fit together as well, so there's more room for water to pass around the bits and pieces.  Finer grinds will pack together more tightly and slow the passage of water, probably increasing flavor and caffeine extraction.

TL;DR: Grinding coffee more increases surface area, but you need to fine-tune your grinding to get the maximal effectiveness.  Also the finer the grind, the longer the water will sit.",null,2,cdj21cy,1r2jdi,askscience,top_week,3
null,null,null,13,cdize84,1r2jdi,askscience,top_week,9
PraecorLoth970,"I attended a lecture that addressed exactly this issue. It was by Dr. Gerald H. Pollack who has a lot of studies published on water. I've found a video lecture which looks similar to the one I watched, so I recommend you watch it in its entirety. It was fascinating. So many things that I took for granted and he, with compelling evidence, showed how wrong many of my assumptions were.

The video lecture is [here](http://www.youtube.com/watch?v=XVBEwn6iWOo). The part about clouds is at about [48:30](http://youtu.be/XVBEwn6iWOo?t=47m22s) but in order to understand the explanation, you need to watch the entire thing.

If I may try a TL;DW: It's because on many interfaces, water molecules aggregate on a liquid crystalline structure which he proposed as being similar to that of ice, which is negatively charged (this is called the exclusion zone). This would give water droplets's surface a negative charge, and the atmosphere is positively charged (ground is negative, atmosphere is positive). This results in the negatively charged water droplets to attract each other (like likes like. Weird, huh?), because between two negatively charged surfaces of droplets there would be a bigger concentration of positive atmospheric charges, and the droplets would be attracted to this, and would move towards each other, and thus coalesce into a cloud. There is a [TEDxTalk](http://www.youtube.com/watch?v=i-T7tCMUDXU) which is shorter. Youtube has lots of other videos with him.

Edit: Made my TL;DW a bit more clearer and more accurate.

Edit 2: Some images from his articles and videos.

* [Abstract 1](http://i.imgur.com/XLXVqHT.png)
* [Abstract 2](http://i.imgur.com/c2PrPxg.png)
* [Abstract 3](http://i.imgur.com/XkpMnkv.png)
* [Abstract 4](http://i.imgur.com/nncKR1w.png)
* [Exclusion zone](http://i.imgur.com/tUUF8nq.png)
* [Charge in the exclusion zone](http://i.imgur.com/aOlwvlT.png)
* [Tube with EZ; Produces flow without pressure difference. Energy source is ambient radiation, infrared](http://i.imgur.com/euZ0PvO.png)
* [Positive region created due to negative spheres.](http://i.imgur.com/ckAQYVq.png)
* [Negative region created due to positive spheres](http://i.imgur.com/vzz3DZH.png)
* [Like attracts like](http://faculty.washington.edu/ghp/images/stories/likelike.png)
* [Formation of the exclusion zone (Video)](http://vimeo.com/7294988)
* [Water droplets on water surface (Video)](http://vimeo.com/7279153)
* [Pollack laboratory webpage](http://faculty.washington.edu/ghp/)
",null,8,cdj5gbr,1r2j3i,askscience,top_week,17
SpicyBuffaloFeather,"Based on my understanding of meteorology: air with differing humidity levels and temperature do not readily mix, especially in fairly calm weather. Instead, a parcel (bubble) of warm air will rise if surrounded by cooler air until it cools adiabaticaly due to expansion. When this happens, the relative humidity of said parcel approaches 100% (relative humidity is a measure of how much water vapor a given unit of air is holding vs. how much water vapor it can hold before it is totally saturated. When the parcel cools sufficiently, it reaches the saturation point and water vapor begins to condense on condensation nuclei (fancy science terminology for bits of dust floating around in the air). If enough water vapor condenses on said nuclei, it will become heavy enough to fall. When it falls a process known as ""collision and coalescence"" occurs which basically means the water droplet hits other water droplets and combines to make a bigger droplet, thus increasing the speed at which it falls. However, a raindrop can only get so big before friction with the air causes it to break apart and the process starts anew. 

Fun fact: rain drops do not actually look like like [this](http://www.psdgraphics.com/file/water-droplet-icon.jpg) but infact look like more like [this](http://www.sailingissues.com/raindrop-shape-evolution.png)

Edit: I realize I kind of digressed from your question there. To answer your question, differential heating of the earth's surface is why water vapor does not evenly distribute itself throughout the atmosphere. Warm air can hold more water vapor than cold air and in general, warm air at the equator moves toward the poles while cold air at the poles moves to the equator in a futile attempt to achieve equilibrium. This overly simplistic model is based on the [hadley cell](http://serc.carleton.edu/images/eslabs/hurricanes/3d_hadley_md.v3.jpg). ",null,1,cdjeyox,1r2j3i,askscience,top_week,6
__Pers,[This article](http://www.scientificamerican.com/article.cfm?id=why-do-clouds-always-appe) in *Scientific American* addresses your question in some detail. ,null,2,cdiz70k,1r2j3i,askscience,top_week,9
rocketsocks,"Water vapor is transparent, you literally cannot see it.

When you have a pot of water on the stove boiling and you see the ""water vapor"" rising above it's not the vapor you see it's tiny water droplets recondensing as it cools off in the air. Those tiny droplets are still hot (nearly 100 deg. C) and they are easily blown around by air currents, which is why they continue to rise.

Clouds are visible because they are composed of water droplets, not just vapor. ",null,1,cdjbhmp,1r2j3i,askscience,top_week,2
PraecorLoth970,"I attended a lecture that addressed exactly this issue. It was by Dr. Gerald H. Pollack who has a lot of studies published on water. I've found a video lecture which looks similar to the one I watched, so I recommend you watch it in its entirety. It was fascinating. So many things that I took for granted and he, with compelling evidence, showed how wrong many of my assumptions were.

The video lecture is [here](http://www.youtube.com/watch?v=XVBEwn6iWOo). The part about clouds is at about [48:30](http://youtu.be/XVBEwn6iWOo?t=47m22s) but in order to understand the explanation, you need to watch the entire thing.

If I may try a TL;DW: It's because on many interfaces, water molecules aggregate on a liquid crystalline structure which he proposed as being similar to that of ice, which is negatively charged (this is called the exclusion zone). This would give water droplets's surface a negative charge, and the atmosphere is positively charged (ground is negative, atmosphere is positive). This results in the negatively charged water droplets to attract each other (like likes like. Weird, huh?), because between two negatively charged surfaces of droplets there would be a bigger concentration of positive atmospheric charges, and the droplets would be attracted to this, and would move towards each other, and thus coalesce into a cloud. There is a [TEDxTalk](http://www.youtube.com/watch?v=i-T7tCMUDXU) which is shorter. Youtube has lots of other videos with him.

Edit: Made my TL;DW a bit more clearer and more accurate.

Edit 2: Some images from his articles and videos.

* [Abstract 1](http://i.imgur.com/XLXVqHT.png)
* [Abstract 2](http://i.imgur.com/c2PrPxg.png)
* [Abstract 3](http://i.imgur.com/XkpMnkv.png)
* [Abstract 4](http://i.imgur.com/nncKR1w.png)
* [Exclusion zone](http://i.imgur.com/tUUF8nq.png)
* [Charge in the exclusion zone](http://i.imgur.com/aOlwvlT.png)
* [Tube with EZ; Produces flow without pressure difference. Energy source is ambient radiation, infrared](http://i.imgur.com/euZ0PvO.png)
* [Positive region created due to negative spheres.](http://i.imgur.com/ckAQYVq.png)
* [Negative region created due to positive spheres](http://i.imgur.com/vzz3DZH.png)
* [Like attracts like](http://faculty.washington.edu/ghp/images/stories/likelike.png)
* [Formation of the exclusion zone (Video)](http://vimeo.com/7294988)
* [Water droplets on water surface (Video)](http://vimeo.com/7279153)
* [Pollack laboratory webpage](http://faculty.washington.edu/ghp/)
",null,8,cdj5gbr,1r2j3i,askscience,top_week,17
SpicyBuffaloFeather,"Based on my understanding of meteorology: air with differing humidity levels and temperature do not readily mix, especially in fairly calm weather. Instead, a parcel (bubble) of warm air will rise if surrounded by cooler air until it cools adiabaticaly due to expansion. When this happens, the relative humidity of said parcel approaches 100% (relative humidity is a measure of how much water vapor a given unit of air is holding vs. how much water vapor it can hold before it is totally saturated. When the parcel cools sufficiently, it reaches the saturation point and water vapor begins to condense on condensation nuclei (fancy science terminology for bits of dust floating around in the air). If enough water vapor condenses on said nuclei, it will become heavy enough to fall. When it falls a process known as ""collision and coalescence"" occurs which basically means the water droplet hits other water droplets and combines to make a bigger droplet, thus increasing the speed at which it falls. However, a raindrop can only get so big before friction with the air causes it to break apart and the process starts anew. 

Fun fact: rain drops do not actually look like like [this](http://www.psdgraphics.com/file/water-droplet-icon.jpg) but infact look like more like [this](http://www.sailingissues.com/raindrop-shape-evolution.png)

Edit: I realize I kind of digressed from your question there. To answer your question, differential heating of the earth's surface is why water vapor does not evenly distribute itself throughout the atmosphere. Warm air can hold more water vapor than cold air and in general, warm air at the equator moves toward the poles while cold air at the poles moves to the equator in a futile attempt to achieve equilibrium. This overly simplistic model is based on the [hadley cell](http://serc.carleton.edu/images/eslabs/hurricanes/3d_hadley_md.v3.jpg). ",null,1,cdjeyox,1r2j3i,askscience,top_week,6
__Pers,[This article](http://www.scientificamerican.com/article.cfm?id=why-do-clouds-always-appe) in *Scientific American* addresses your question in some detail. ,null,2,cdiz70k,1r2j3i,askscience,top_week,9
rocketsocks,"Water vapor is transparent, you literally cannot see it.

When you have a pot of water on the stove boiling and you see the ""water vapor"" rising above it's not the vapor you see it's tiny water droplets recondensing as it cools off in the air. Those tiny droplets are still hot (nearly 100 deg. C) and they are easily blown around by air currents, which is why they continue to rise.

Clouds are visible because they are composed of water droplets, not just vapor. ",null,1,cdjbhmp,1r2j3i,askscience,top_week,2
Surf_Science,"A tentative contig should be just what it sounds, a draft contig. 

For those who don't know what a contig is. 

DNA is sequenced in very small bits, usually 75 to ~800 base pairs. This bits are lined up to make a longer sequence. A contig is one of those alignments and contigs are usually millions and millions of base pairs. A contig of the mouse genome for example could be more than 50 million base pairs. ",null,0,cdizwc9,1r2isv,askscience,top_week,5
PricaCells,"These are known as sun-sensitizing drugs. This is because they cause certain side effects only when in direct sunlight, such as photoallergy, or phototoxicity. The latter is more common, and it occurs when the skin is exposed to sunlight after certain medications are injected, taken orally, or applied to the skin. The drug absorbs the UV light, then releases it into the skin, causing cell death. So, that's what I can tell you. ",null,1,cdixufz,1r2id5,askscience,top_week,18
null,null,null,0,cdjhvqy,1r2id5,askscience,top_week,1
PricaCells,"These are known as sun-sensitizing drugs. This is because they cause certain side effects only when in direct sunlight, such as photoallergy, or phototoxicity. The latter is more common, and it occurs when the skin is exposed to sunlight after certain medications are injected, taken orally, or applied to the skin. The drug absorbs the UV light, then releases it into the skin, causing cell death. So, that's what I can tell you. ",null,1,cdixufz,1r2id5,askscience,top_week,18
null,null,null,0,cdjhvqy,1r2id5,askscience,top_week,1
iorgfeflkd,"The extreme example is [Anatoly Bugorski](http://en.wikipedia.org/wiki/Anatoli_Bugorski) who had a high energy proton beam go through his head. He was injured, but survived.",null,96,cdiwr26,1r2gwb,askscience,top_week,489
DAlder_HardlyKnowHer,"Alpha particles are just that, they are equivalent to a He2+ atom. Which consists of 2 protons, 2 neutrons, and no electrons. Very few things are smaller than alpha particles including isotopes of hydrogen ( H ). Alpha particles are already too large to pass through your body. However, if the source which emits the alpha particle is already inside of you ( ingested or inhaled ) it will cause extreme amounts of damage, pending the amount.",null,21,cdizx62,1r2gwb,askscience,top_week,113
50bmg,"1 atom won't do much unless it contains extraordinary amounts of energy (see: http://en.wikipedia.org/wiki/Oh-My-God_particle). 


A stream of protons (essentially hydrogen nuclei) would do exactly this if carefully controlled:
http://en.wikipedia.org/wiki/Proton_therapy


A more energetic beam would do this:
http://en.wikipedia.org/wiki/Anatoli_Bugorski


Essentially what happens is the proton will likely collide with another atom (although many will make it all the way through without hitting anything!). Depending on the energy of the proton, and the type of atom it collides with - several types of atomic reactions could occur, most of them altering the molecule it collided with quite dramatically, or releasing heat and various types of radiation. In the worst case - it would break or alter a DNA strand and potentially cause cancer, however in most cases the cell with the damaged DNA would just be unable to replicate and therefore die. ",null,7,cdj40hn,1r2gwb,askscience,top_week,42
fillterfood,"Astronauts in space report seeing flashes in their vision when they close their eyes. It is thought that these are caused by cosmic rays (nuclei of atoms) shooting through them and hitting their retinas. While I imagine some level of damage is happening, it doesn't seem to be too dangerous.

The most obvious danger is an increased risk of cancer. If the atoms collide with DNA, they can break it apart. While a single killed cell here or there isn't much of a problem in our bodies. It's when the cell gets reprogrammed to start growing endlessly that it becomes a real threat to our health.

So I think the answer to your question is a bit of a yes and no. Physically, there seems to be no risk, biologically however, combined with a bit of bad luck, it can kill.",null,11,cdizsy1,1r2gwb,askscience,top_week,40
EdPeggJr,"The [Oh My God particle](http://en.wikipedia.org/wiki/Oh-My-God_particle) was a single particle, likely a proton, which hit with a force ""equal to that of 50 Joules, or a 5-ounce (142 g) baseball traveling at about 100 kilometers per hour (60 mph)."" Detectors have confirmed 15 similar events.",null,4,cdj41fd,1r2gwb,askscience,top_week,15
bertrussell,"This happens to everyone, all the time. There are cosmic radiation sources and also terrestrial radiation sources. Sure, radiation has its issues, but the body can actually handle a fair amount of radiation throughout a person's lifetime.

http://video.mit.edu/watch/cloud-chamber-4058/",null,2,cdj7wu3,1r2gwb,askscience,top_week,12
dddm,"A single atom or particle (or photon) passing through a human body would never be able to cause much damage, although a beam containing many individual particles certainly could.  There is a range of energies that are most damaging and it's not accurate to say that higher energy radiation sources always produce greater radiation damage.

If the energy is too low, below about 5-100 eV depending on which [definition](http://en.wikipedia.org/wiki/Ionizing_radiation#Ionization_and_the_definition_problem) is used, the radiation is not energetic enough to ionize the target material.  This type of non-ionizing radiation is generally not much of a concern (sitting near a normal light bulb probably won't cause much long term damage), but there still are some [damaging effects](http://en.wikipedia.org/wiki/Non-ionizing_radiation#Health_risks) mostly due to heating in the target material.

Above the ionizing threshold, radiation damage generally increases with energy up to a certain point.  This is because radiation damage is generally proportional to the total energy deposited.  If the energy of the incident particle isn't too high, it will stop within the target material, indicating that all of its kinetic energy was deposited into the target.  In this regime, higher energy particles will always lead to a larger radiation dose.

However, the situation isn't as clear when the incident particle has enough initial energy to pass through the target.  Even if a particle passes through the target, much of its initial energy may be deposited in the target material along its path.  The rate of energy deposition (called the [stopping power](http://pdg.lbl.gov/2006/reviews/passagerpp.pdf)) as a function of depth within the target is described by the [Bragg curve](http://en.wikipedia.org/wiki/Bragg_peak).  As an approximation, the shape of the Bragg curve depends mostly on the species and initial energy of the incident particle, and on the density of the target.

The rate of energy deposition by a particle generally decreases for shallow depths as the particle initial energy increases.  Thunderf00t has an excellent video describing this effect (the pertinent discussion is towards the end of the video, but the whole video is relevant):

https://www.youtube.com/watch?v=oj6v8MtuVdU

For very high energy, the thickness of a human body may be a very small portion of the maximum radiation depth, and the stopping power in this depth range would approach zero as the particle energy increases.  So there exists a threshold energy above which the particle actually does less radiation damage compared to (comparatively) lower energies.

Estimating this threshold energy for simple geometries, such as a human hand in a large vacuum, can be done by integrating the stopping power up to the target thickness and comparing the result for different energies.  For more complicated geometries, for example those involving non-vaccum materials near the target that may produce secondary particles when exposed to the initial radiation, the system generally needs to be simulated using particle tracking toolkits such as Geant4, FLUKA, or MCNP.

In any case, a single particle cannot impart an infinite amount of energy into a target material.  The maximum deposited energy would be on the order of 100's of MeV for protons, yielding a single proton maximum effective dose of maybe 100 nSv.  This is an negligible dose compared to [other common exposures](http://en.wikipedia.org/wiki/Orders_of_magnitude_(radiation\)), such as the daily background dose of about 10,000 nSv.

A string of particles, which would be a model for a particle beam, can certainly lead to intense radiation damage, since a typical particle beam may contain on the order of 10^10 individual particles.",null,2,cdjaj1n,1r2gwb,askscience,top_week,7
madscientistuk,"Sixty Symbols (a video series from the University of Nottingham about Physics, Astronomy and Maths) answered a similar question which I think is relevant.

As part of the video series they have a number of videos where they asked physicists, astronomers and maths lecturers questions submitted by the sixty symbols viewers. The first question of the 2nd video about viewers questions was:

""If I put my hand in front of the beam at the Large Hadron Collider, what would happen to my hand.""

The responses are great in my opinion :) [YouTube video - Putting your hand in the Large Hadron Collider...](http://www.youtube.com/watch?v=_NMqPT6oKJ8) question answered 0.00-3.54.

This was so popular they then had a second video where they asked scientists working at the Large Hadron Collider the same question. Again I think the answers are great.

[YouTube video - Hand back into the Large Hadron Collider - Sixty Symbols](http://www.youtube.com/watch?v=lVefgfmFg9o)

Edit - hopefully fixed the links",null,1,cdjemqa,1r2gwb,askscience,top_week,5
yinz_n-at,"Depends. Lost of Probability involved. The higher the energy the more damaging but also the more likely it'll just go straight through you (gamma ray) . Low energy particles are less damaging but more likely to collide with a nucleus in your body (atoms are vastly empty).

A neutron from radioactive decay is pretty damaging because it has a high probability of colliding with a human nucleus. 

And one nuclei isn't all that damaging because the probability of one particle causing damage is very small. Now a flux or beam of particles will do some serious damage. 

Side Note: Ingesting a radioactive material is pretty dangerous because every decay product will be absorbed by your body. Alpha particles are pretty easy to shield against but if theyre not shielded then they'll definitely knock some nuclei around (worst case is knocking a DNA nuclei out causing mutations.) 

Hope that helps!

Source: Nuclear Engineer",null,1,cdjawm3,1r2gwb,askscience,top_week,5
KillerInYourCloset,"Sorry, this might sound *very* unscientific- but wasn't there an example of someone who put their hand in front a particle collider? I believe they had really no adverse affects. Sorry, I can't look into it much I'm on a sev A call, but not really listening unless I hear my name :)",null,0,cdjb6yn,1r2gwb,askscience,top_week,3
OnlyOneWithThisName,"I believe this question is a very important obstacle to consider in regard to interplanetary space travel within the immediate solar system, and most likely long distance interstellar travel as well. This article about the [health threat of cosmic rays](http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays ""en.wikipedia.org"") will only partially answer your question, but I believe it is a worthwhile read.",null,0,cdj9o6m,1r2gwb,askscience,top_week,2
antpuncher,"You have trillions of neutrinos passing through you every second, and you're doing just grand.

The important piece is how much energy the particle can deposit in your body.  This depends on how much energy the particle has, and what the ""interaction cross section"" is.  Like it sounds, the cross section is kind of like how big it is, but for quantum mechanical things it's not really a property of the _size_ of the object, and has to do with things like the electrical force.   

Then it also depends on where it goes.  You could probably take a packet from the LHC through your ear lobe and only lose some skin, but a few wrongly placed alpha particles and you have brain cancer.",null,6,cdj9rbr,1r2gwb,askscience,top_week,8
websterandy42,"What you are talking about is a very rare scientific concept known as radiation. Haha,  one of the most common types of radiation (and the first to decompose after a nuclear reaction) is literally a helium nuclei. While your cells are tough enough not to be hurt by small amounts. What it really affects is DNA, DNA does not heal so damaged DNA can will work but not properly, the cells will begin reproducing at an increased rate. This is how tumors form.",null,0,cdjmbsd,1r2gwb,askscience,top_week,2
recycled_ideas,"It depends if it hits anything,  and if so what. At the subatomic scale there's a lot empty space and any given particle could fly right through you doing no damage quite easily. Of course it could also hit something and damage it.  Enough particles and enough hits could kill you. A few hits could cause cancer and kill you over time. Or your body could repair the damage and nothing happens. 

You're actually being hit by particles all day. ",null,0,cdjotgd,1r2gwb,askscience,top_week,1
denchpotench,"Charged particles would cause a lot more damage than uncharged ones. For example beta radiation is just fast moving electrons or positrons. These have charge and can ionize atoms, if these are ionised at certain points in strand of DNA it can cause the parent cell to die or become cancerous. If an uncharged atom flew through you it might take a few atoms out on the way but would affect far fewer than a positron stripping electrons from many atoms.",null,0,cdk0oc3,1r2gwb,askscience,top_week,1
iorgfeflkd,"The extreme example is [Anatoly Bugorski](http://en.wikipedia.org/wiki/Anatoli_Bugorski) who had a high energy proton beam go through his head. He was injured, but survived.",null,96,cdiwr26,1r2gwb,askscience,top_week,489
DAlder_HardlyKnowHer,"Alpha particles are just that, they are equivalent to a He2+ atom. Which consists of 2 protons, 2 neutrons, and no electrons. Very few things are smaller than alpha particles including isotopes of hydrogen ( H ). Alpha particles are already too large to pass through your body. However, if the source which emits the alpha particle is already inside of you ( ingested or inhaled ) it will cause extreme amounts of damage, pending the amount.",null,21,cdizx62,1r2gwb,askscience,top_week,113
50bmg,"1 atom won't do much unless it contains extraordinary amounts of energy (see: http://en.wikipedia.org/wiki/Oh-My-God_particle). 


A stream of protons (essentially hydrogen nuclei) would do exactly this if carefully controlled:
http://en.wikipedia.org/wiki/Proton_therapy


A more energetic beam would do this:
http://en.wikipedia.org/wiki/Anatoli_Bugorski


Essentially what happens is the proton will likely collide with another atom (although many will make it all the way through without hitting anything!). Depending on the energy of the proton, and the type of atom it collides with - several types of atomic reactions could occur, most of them altering the molecule it collided with quite dramatically, or releasing heat and various types of radiation. In the worst case - it would break or alter a DNA strand and potentially cause cancer, however in most cases the cell with the damaged DNA would just be unable to replicate and therefore die. ",null,7,cdj40hn,1r2gwb,askscience,top_week,42
fillterfood,"Astronauts in space report seeing flashes in their vision when they close their eyes. It is thought that these are caused by cosmic rays (nuclei of atoms) shooting through them and hitting their retinas. While I imagine some level of damage is happening, it doesn't seem to be too dangerous.

The most obvious danger is an increased risk of cancer. If the atoms collide with DNA, they can break it apart. While a single killed cell here or there isn't much of a problem in our bodies. It's when the cell gets reprogrammed to start growing endlessly that it becomes a real threat to our health.

So I think the answer to your question is a bit of a yes and no. Physically, there seems to be no risk, biologically however, combined with a bit of bad luck, it can kill.",null,11,cdizsy1,1r2gwb,askscience,top_week,40
EdPeggJr,"The [Oh My God particle](http://en.wikipedia.org/wiki/Oh-My-God_particle) was a single particle, likely a proton, which hit with a force ""equal to that of 50 Joules, or a 5-ounce (142 g) baseball traveling at about 100 kilometers per hour (60 mph)."" Detectors have confirmed 15 similar events.",null,4,cdj41fd,1r2gwb,askscience,top_week,15
bertrussell,"This happens to everyone, all the time. There are cosmic radiation sources and also terrestrial radiation sources. Sure, radiation has its issues, but the body can actually handle a fair amount of radiation throughout a person's lifetime.

http://video.mit.edu/watch/cloud-chamber-4058/",null,2,cdj7wu3,1r2gwb,askscience,top_week,12
dddm,"A single atom or particle (or photon) passing through a human body would never be able to cause much damage, although a beam containing many individual particles certainly could.  There is a range of energies that are most damaging and it's not accurate to say that higher energy radiation sources always produce greater radiation damage.

If the energy is too low, below about 5-100 eV depending on which [definition](http://en.wikipedia.org/wiki/Ionizing_radiation#Ionization_and_the_definition_problem) is used, the radiation is not energetic enough to ionize the target material.  This type of non-ionizing radiation is generally not much of a concern (sitting near a normal light bulb probably won't cause much long term damage), but there still are some [damaging effects](http://en.wikipedia.org/wiki/Non-ionizing_radiation#Health_risks) mostly due to heating in the target material.

Above the ionizing threshold, radiation damage generally increases with energy up to a certain point.  This is because radiation damage is generally proportional to the total energy deposited.  If the energy of the incident particle isn't too high, it will stop within the target material, indicating that all of its kinetic energy was deposited into the target.  In this regime, higher energy particles will always lead to a larger radiation dose.

However, the situation isn't as clear when the incident particle has enough initial energy to pass through the target.  Even if a particle passes through the target, much of its initial energy may be deposited in the target material along its path.  The rate of energy deposition (called the [stopping power](http://pdg.lbl.gov/2006/reviews/passagerpp.pdf)) as a function of depth within the target is described by the [Bragg curve](http://en.wikipedia.org/wiki/Bragg_peak).  As an approximation, the shape of the Bragg curve depends mostly on the species and initial energy of the incident particle, and on the density of the target.

The rate of energy deposition by a particle generally decreases for shallow depths as the particle initial energy increases.  Thunderf00t has an excellent video describing this effect (the pertinent discussion is towards the end of the video, but the whole video is relevant):

https://www.youtube.com/watch?v=oj6v8MtuVdU

For very high energy, the thickness of a human body may be a very small portion of the maximum radiation depth, and the stopping power in this depth range would approach zero as the particle energy increases.  So there exists a threshold energy above which the particle actually does less radiation damage compared to (comparatively) lower energies.

Estimating this threshold energy for simple geometries, such as a human hand in a large vacuum, can be done by integrating the stopping power up to the target thickness and comparing the result for different energies.  For more complicated geometries, for example those involving non-vaccum materials near the target that may produce secondary particles when exposed to the initial radiation, the system generally needs to be simulated using particle tracking toolkits such as Geant4, FLUKA, or MCNP.

In any case, a single particle cannot impart an infinite amount of energy into a target material.  The maximum deposited energy would be on the order of 100's of MeV for protons, yielding a single proton maximum effective dose of maybe 100 nSv.  This is an negligible dose compared to [other common exposures](http://en.wikipedia.org/wiki/Orders_of_magnitude_(radiation\)), such as the daily background dose of about 10,000 nSv.

A string of particles, which would be a model for a particle beam, can certainly lead to intense radiation damage, since a typical particle beam may contain on the order of 10^10 individual particles.",null,2,cdjaj1n,1r2gwb,askscience,top_week,7
madscientistuk,"Sixty Symbols (a video series from the University of Nottingham about Physics, Astronomy and Maths) answered a similar question which I think is relevant.

As part of the video series they have a number of videos where they asked physicists, astronomers and maths lecturers questions submitted by the sixty symbols viewers. The first question of the 2nd video about viewers questions was:

""If I put my hand in front of the beam at the Large Hadron Collider, what would happen to my hand.""

The responses are great in my opinion :) [YouTube video - Putting your hand in the Large Hadron Collider...](http://www.youtube.com/watch?v=_NMqPT6oKJ8) question answered 0.00-3.54.

This was so popular they then had a second video where they asked scientists working at the Large Hadron Collider the same question. Again I think the answers are great.

[YouTube video - Hand back into the Large Hadron Collider - Sixty Symbols](http://www.youtube.com/watch?v=lVefgfmFg9o)

Edit - hopefully fixed the links",null,1,cdjemqa,1r2gwb,askscience,top_week,5
yinz_n-at,"Depends. Lost of Probability involved. The higher the energy the more damaging but also the more likely it'll just go straight through you (gamma ray) . Low energy particles are less damaging but more likely to collide with a nucleus in your body (atoms are vastly empty).

A neutron from radioactive decay is pretty damaging because it has a high probability of colliding with a human nucleus. 

And one nuclei isn't all that damaging because the probability of one particle causing damage is very small. Now a flux or beam of particles will do some serious damage. 

Side Note: Ingesting a radioactive material is pretty dangerous because every decay product will be absorbed by your body. Alpha particles are pretty easy to shield against but if theyre not shielded then they'll definitely knock some nuclei around (worst case is knocking a DNA nuclei out causing mutations.) 

Hope that helps!

Source: Nuclear Engineer",null,1,cdjawm3,1r2gwb,askscience,top_week,5
KillerInYourCloset,"Sorry, this might sound *very* unscientific- but wasn't there an example of someone who put their hand in front a particle collider? I believe they had really no adverse affects. Sorry, I can't look into it much I'm on a sev A call, but not really listening unless I hear my name :)",null,0,cdjb6yn,1r2gwb,askscience,top_week,3
OnlyOneWithThisName,"I believe this question is a very important obstacle to consider in regard to interplanetary space travel within the immediate solar system, and most likely long distance interstellar travel as well. This article about the [health threat of cosmic rays](http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays ""en.wikipedia.org"") will only partially answer your question, but I believe it is a worthwhile read.",null,0,cdj9o6m,1r2gwb,askscience,top_week,2
antpuncher,"You have trillions of neutrinos passing through you every second, and you're doing just grand.

The important piece is how much energy the particle can deposit in your body.  This depends on how much energy the particle has, and what the ""interaction cross section"" is.  Like it sounds, the cross section is kind of like how big it is, but for quantum mechanical things it's not really a property of the _size_ of the object, and has to do with things like the electrical force.   

Then it also depends on where it goes.  You could probably take a packet from the LHC through your ear lobe and only lose some skin, but a few wrongly placed alpha particles and you have brain cancer.",null,6,cdj9rbr,1r2gwb,askscience,top_week,8
websterandy42,"What you are talking about is a very rare scientific concept known as radiation. Haha,  one of the most common types of radiation (and the first to decompose after a nuclear reaction) is literally a helium nuclei. While your cells are tough enough not to be hurt by small amounts. What it really affects is DNA, DNA does not heal so damaged DNA can will work but not properly, the cells will begin reproducing at an increased rate. This is how tumors form.",null,0,cdjmbsd,1r2gwb,askscience,top_week,2
recycled_ideas,"It depends if it hits anything,  and if so what. At the subatomic scale there's a lot empty space and any given particle could fly right through you doing no damage quite easily. Of course it could also hit something and damage it.  Enough particles and enough hits could kill you. A few hits could cause cancer and kill you over time. Or your body could repair the damage and nothing happens. 

You're actually being hit by particles all day. ",null,0,cdjotgd,1r2gwb,askscience,top_week,1
denchpotench,"Charged particles would cause a lot more damage than uncharged ones. For example beta radiation is just fast moving electrons or positrons. These have charge and can ionize atoms, if these are ionised at certain points in strand of DNA it can cause the parent cell to die or become cancerous. If an uncharged atom flew through you it might take a few atoms out on the way but would affect far fewer than a positron stripping electrons from many atoms.",null,0,cdk0oc3,1r2gwb,askscience,top_week,1
Needless-To-Say,"Typically the signals are separated within the common medium by frequency shifting. Each individual endpoint would have a dedicated frequency range within the common medium. Copper cables have a much lower threshold of channels than Cable which in turn have a much lower threshold than Fiber.

When the signal is digital the data is typically encrypted to prevent unathorized access. Anyone trying to Sniff the data would first need to decrypt the data. With older, analog technology, it can be a much simpler matter to ""sniff other peoples signals""

When you get into wireless tech, things get far more interesting with multiple simultaneous security measures in play to prevent eavesdropping but nothing is completely secure.",null,2,cdj234i,1r2esb,askscience,top_week,5
chrisbaird,"There are two basic ways to stabilize a projectile: 1) passive control, such as fixed fins or spinning the object, and 2) active control, such as tilting the exhaust cones in the appropriate direction, having small position-correcting thrusters, or moving wing flaps. A lot of rockets have both because using passive controls means that your active controls have an easier job to do. But you can get by with no passive controls such as fins if your active controls are advanced enough. 

It depends a lot on the application. Many rockets, such as missiles, spend a large portion of their trajectory in ballistic motion (i.e. rocket engines turned off). In such a case, you can't use gimballing the rocket nozzles to stabilize flight, so fins become more important.",null,1,cdix4y7,1r2e54,askscience,top_week,20
DUFFYSTE98,"It's all about the exhausts. They can change the direction of the exhausts which changes the path of the rocket.

Hope this answers your question, if you want to read more into it http://inventors.about.com/library/inventors/blrocketcontrol.htm",null,1,cdiw9st,1r2e54,askscience,top_week,4
rupert1920,"They have a very good idea on what it's _supposed_ to do. This data is available from early research _in vitro_.

In terms of side effects, this is gleamed in toxicological studies and [clinical trials](http://en.wikipedia.org/wiki/Clinical_trials) - specifically Phase I, where the drug is administered to healthy volunteers. To move onto later phases of the trials, safety and efficacy must be demonstrated to the relevant regulating agencies in the country.

Especially now in the era of [rational drug design](http://en.wikipedia.org/wiki/Rational_drug_design), a drug is synthesized with the explicit purpose of interfering with some known biochemical pathway. So it's really not like drug companies are just randomly administering chemicals just to see what they do.",null,1,cdivgvd,1r2dfy,askscience,top_week,10
snusmumrikan,"Starting a new comment because I'm directly answering your question but that is not to take away from some valid points made by Rupert and botanist. 

I have experience in early R&amp;D at the UKs largest pharma (devilishly difficult clue there). 

For a new drug to have even the slimmest chance of getting to first-time in human (FTIH) it has to have gone through extensive and rigorous tests. Initially almost all drug design begins with the development of assays (repeatable standardised test experiments) which are used to screen a company's vast library (millions and millions) of compounds. These assays are becoming more extensive and phenotypicaly relevant over time as the biggest problem in drug development is attrition, and for every step further down the chain you take a compound before finding out it is not fit for use as a medicine it costs exponentially more - you want to be almost completely sure you're compound is viable before progressing it. A shift away from standard model cell lines and towards disease-relevant tissue samples derived from patients is currently underway as this gives significantly more confidence that the 'hits' are actually good, whilst doing away with the confidence, experience and reliability of those extensively studied model cell lines. If you're interested I can send you sections of my Masters research project which was in this area and has the appropriate references.

Once you have your hits from your initial screen these are then run against standard assays for things like cardiac liability (will it mess up ion channels in your heart?) or general toxicity to know whether it shouldn't be allowed in your body at all, you'd be surprised just how often certain molecules come up as great hits (elevating the protein which is deficient in a disease, for example) but turn out to be the old red herrings like HDAc inhibitors.

After this the compound will be carried forwards into animal trials, and this is where there is too much variation to try and cover it all but it will usually start in something small with a shorter lifespan like mice, potentially recombinant mice which have had a disease phenotype replicated within them, along with healthy controls and so on. After this other animals may be used, for example for respiratory and cardiac models, dogs are used as they represent the most human-like model.

Once the animal testing has been done extensively and for a long period to identify any and all side effects, if there are none which would indicate toxicity or severe side effects in humans the company will spend several months compiling the data, checking it, analysing it extensively before submitting it for Phase 0 clinical trials. These are before phase 1 trials and involve extremely low doses in relatively very few people in order to assess at an early stage the pharmacokinetic (where it goes inside you) and pharmacodynamic (what it does) implications of the compound. 

As you can see it takes a lot to get there and companies want to be incredibly sure of their compound before going near a human because A: it costs an armada full of money and B: no one wants another thalidomide. 

This is obviously a huge subject and i have left out many of the initial stages and assumed the compound came from a small molecule library (as opposed to chemical design, structural rational design or biologics such as antibodies) but the general principles remain true. 

Hope this helps, happy to answer anything else as long as I feel comfortable with my knowledge. 
",null,0,cdj1lcx,1r2dfy,askscience,top_week,8
czyivn,"All the rules of pharma are written in blood.  Past experiences with bad drugs have shaped our current practice to prevent patient deaths.  Here are the major things that prevent human deaths during drug trials:

1. Test it before it gets to animals.  Lots of general mechanisms where drugs kill people are already known.  Inhibition of hERG channels causes cardiac deaths, so they have an assay where all drugs are tested for hERG inhibition.  Liver damage is another mechanism, so all drugs are tested for toxicity to hepatocytes. 

2. Test it extensively in animals.  Before it ever sees a human, the drug has usually been tested in at least rats and at least one larger animal (dogs, monkeys, pigs).  We aren't smart enough to predict animal toxicity in advance (and anyone who says otherwise is a liar), so the only way to be safe in humans is to test it in other animals first.

2. Dose escalation.  They start out in humans at doses that are usually much less than where they expect serious side effects.  They gradually increase the dosage until side effects start to appear, like elevated liver enzymes.  If any of these symptoms are troubling, they stop the dose escalation immediately, and typically remove that patient from the trial.",null,0,cdizkvg,1r2dfy,askscience,top_week,3
chrisbaird,"I think you meant to ask ""Is a gravitational pull fundamental"". Of course gravitational pulls exist, they are just not fundamental. Fundamentally, gravitational effects are caused by the warping of spacetime by mass and energy, and not by a Newton-style force field stretching out from massive bodies. But just because a concept is not fundamental, does not make it wrong, imaginary, or useless. Most of the things we talk about in science (e.g. friction, centrifugal force, Van der Walls force, buoyancy, etc.) are not fundamental. The only fundamental things in the universe are the ones in this chart: 

http://en.wikipedia.org/wiki/File:Standard_Model_of_Elementary_Particles.svg 

But describing the motion of a child on a swing strictly in terms of quarks, electrons, and photons would be unnecessary and difficult.

 ",null,1,cdiylvn,1r2dfo,askscience,top_week,8
iorgfeflkd,"If you drop something, it falls. That is a manifestation of the existence of gravitational pull.",null,2,cdiwi9z,1r2dfo,askscience,top_week,4
fishify,"Via Einstein, we learned that we can understand gravity as a manifestation of space and time being curved.

Alternatively, we can not take into account this curvature, and then we must define a force to account for gravity.  Said force is attractive and pulls on objects.",null,0,cdiy566,1r2dfo,askscience,top_week,3
pucklermuskau,"its a trade off of investment. Some species invest little in their offspring, in terms of the amount of energy contained in individual eggs, and in terms of the amount of parental investment afterwards. That means individual offspring have little chance of survival. Other species reproduce less, and later, but invest more in the individual, so that particular individual has greater chance of survival... reproduction per se isnt the goal: successful reproduction is the goal: your offspring need to themselves reproduce!",null,1,cdiu7cw,1r2b9n,askscience,top_week,10
Unidan,"I think this question isn't so much asking about the trade-offs between being a long-lived organism versus a short-lived one, but rather *why* long-lived organisms would evolve in the first place, correct?

If that's the question, the current idea is that within very *stable* environments, being able to exploit scarce resources can be advantageous.  With this, longer-lived organisms with more experience or learning may be selected for!  

If you're a very short-lived organism in a stable environment, but not necessarily very competitive, you most likely *won't* be favored if you're going up against an organism that has experience in exploiting a particular resource.  So, eventually, evolution has produced organisms that may require large amounts of parental investment, long periods of growth to maturity and more, but these will give an advantage over those that are only equipped with that with which they are born.

Does that make more sense?",null,5,cdj14hm,1r2b9n,askscience,top_week,9
polistes,"In addition to the answers already given, I'd like to also illustrate this issue with the difference between short-lived plants and long-lived plants (I like using plants because they cannot run away from their problems). Some plants live for many years, while others only live one season. There are advantages for both strategies. Short lived plants use resources quickly and don't have to invest too much in adaptations for surviving hard conditions, like a dry period or winter. They simply make lots of seeds, which can survive these difficult periods. On the other hand, if there is a year in which the growing season is very tough as well (prolonged drought, insect pests like locusts), the population of single-season plants gets a large blow, because many plants won't make it to seed production and you end up with less seeds being deposited.

Now if you are a long-lived plant, you invest a lot in these survival skills and grow slower. If you endure a particulary hard year, you may end up with not having enough energy to make flowers and seeds. However, this is not that bad, because you'll have a new chance next year! For example, there are a lot of plants in remote areas that struggle to attract pollinators for their flowers, so they end up not having seeds in many consecutive years. However, because they are long-lived, they still have many opportunities to pass their genes. If you were a short-lived plant, your population would die of pretty soon.  

So, there are both advantages and disadvantages of these strategies, and neither is particulary better.",null,0,cdjkefq,1r2b9n,askscience,top_week,1
Truck43,"It may cause a conventional explosion and release of nuclear material, but modern nuclear weapons (older designs are less safe) include safeties to prevent nuclear detonation in event of fire or accident. To have a nuclear detonation requires precise timing of the detonation of the explosive shell, unlikely in a lightning strike. ",null,3,cdiwbg3,1r2adb,askscience,top_week,11
Sannish,"The fuel tanks of the nuclear warhead would explode, creating a radioactive plume (e.g. a dirty bomb).  I would guess that the currents from the lightning strike would not trigger the nuclear fission process as I assume the warhead design has some basic safety/grounding features in the circuitry.",null,3,cdiwer1,1r2adb,askscience,top_week,5
redditor5690,"I've seen 60's and 70's nuclear warheads up close many times.

The casings will not allow electrical flow from outside.  Think of what happens when a car or plane is struck my lightning, then imagine how much better it would be if the engineers actually chose that capability as a design requirement.

I would be more concerned with the heat that might be generated in the case if sufficient current flowed through it on the way to electrical earth ground. That heat might be enough to trigger an explosion of the conventional explosives which act as the detonator for the weapon, but those explosive charges, arranged around the U-235/P-239 core, must explode with extremely precise timing to produce a nuclear explosion. But, setting one off would be enough to cause a ""dirty bomb"" explosion which would contaminate a large area.

But, there's always Murphy to consider.",null,0,cdjhktw,1r2adb,askscience,top_week,2
SimpleBen,"Graying is dependent on the balance of two opposing processes. The first is the loss of pigment, which is dependent on inflammatory factors, natural catalase levels, and H2O2 levels in the epithelium. The second is the restoration of pigment, which is UV exposure dependent.  
  
The scalp above the ears doesn't get a whole lot of UV exposure. ",null,0,cdiyfc4,1r275u,askscience,top_week,6
bearsnchairs,Without knowing more specifics it could be due to the mixing in your container. Even when doing a simple acid base titration with an indicator you can get transient color change whee you have a local high concentration of titrant. If you increase the stirring rate it becomes harder to form these areas of high concentration so you don't get patches of color change.,null,0,cdiuncm,1r265d,askscience,top_week,4
PricaCells,"Social grooming is not exclusive to just apes. In fact, its quite common in animal behaviour - and is also present in Humans. However, this may not be in the same context as chimpanzees. Keep in mind that different primates have different patterns of grooming as well.

Amongst people, we see this explained in studies by Nelson, H. (2006), 'Human mutual grooming: an ethological perspective on its form and function' and Nelson, Holly and Geher, Glenn. (2007-09-15) 'Mutual Grooming in Human Dyadic Relationships: An Ethological Perspective'. 

To summarize, there is grooming, but mostly in romantic couples. These are studies that have been done on western societies, however, and from an ethnographic perspective I believe that there are many more examples of grooming in different relationships as well. Social grooming as a form of touch communication is exceptionally important in many mammalian species, and I'd answer your question with a yes. The same is happening with humans, but in different was, and through different means., ",null,2,cdixqbb,1r23q6,askscience,top_week,7
snickeringshadow,"Tentatively yes, but it's complicated. I'm sure if you look hard enough you could find examples of humans grooming each other as part of social bonding. It's a little clich but think about your 'typical' teenage girl slumber party where they paint each other's nails. That certainly qualifies. The problem is it's rather difficult to come up with universal explanations of how we do this because human behavior is patterned by culture as well as biology. Humans like to create formalized, idiosyncratic rituals that govern social interactions. All humans are social, but *how* we are social varies between populations and changes over time. This makes it really difficult to get at the biological basis for behavior - as they are phenotypically inseparable from the environmental or 'cultural' components. ",null,1,cdiy215,1r23q6,askscience,top_week,7
LXZY,It would depend on what you set as zero.  ,null,0,cdizk4y,1r21m7,askscience,top_week,10
MayContainNugat,"The value of the potential energy isn't observable. Only changes of energy matter. So the value itself is arbitrary, up to a constant. ",null,1,cdj0cth,1r21m7,askscience,top_week,6
rlee89,"Gravitational potential energies are usually expressed as negative values.  Under such a representation, 0 is the energy when you are infinitely far from the object and at the top of its gravity well, and the potential energy becomes more negative the further down you are in the gravity well.

There are a few reasons why one might do this.  Picking zero energy at infinite distance natural leads to all potentials being negative and it is the least arbitrary point for zero to be chosen, since the potential energy goes to minus infinity at the object and any given distance would be an arbitrary choice.",null,0,cdj2sav,1r21m7,askscience,top_week,5
MCMXCII,"As others have said, potential energy scales are arbitrary. However it's common to represent bound states with negative potential energies.",null,0,cdj13cv,1r21m7,askscience,top_week,2
LoyalSol,"As other's have mentioned,  it depends on what you define your reference energy as.   A positive value means you are above that reference energy and a negative value means you are below that.

",null,0,cdj68ch,1r21m7,askscience,top_week,2
euneirophrenia,You can make the surface of an object as complex as you'd like to make the surface area huge and the volume tiny. At the extreme you can have  fractal curve like the [Menger sponge](http://en.wikipedia.org/wiki/Menger_sponge) which has infinite surface area and zero volume (at infinite iterations),null,0,cdjripe,1r215u,askscience,top_week,2
patchgrabber,"Molecular Clock Hypothesis tries to estimate how far apart organisms are evolutionarily by means of using specific proteins. Some proteins, such as cytochrome c (present in almost all organisms) seem to have a fairly consistent time between neutral mutations, meaning that if most mutations are neutral (have no effect on fitness), and if they occur at more or less regular intervals, you can estimate how many new mutations you should see in a generation. 

Thus, by measuring the number of mutations in that protein from the time when two now distinct species had the same or very similar versions of these proteins, one can theoretically estimate the time these species diverged. There are several limitations of this process, like fossil prevalence, generation time and metabolic rate, among others. So while it may not be a perfect process, it's not without its uses.",null,73,cdiruch,1r1z4w,askscience,top_week,382
oliverisyourdaddy,"I'm an evolutionary anthropologist!

They compared the genomes of humans and chimps, estimating the total number of divergences (changes).  Then they calculated the average number of mutations (changes) in one generation (by comparing the genes of parents and children).  

Then they performed the following calculation: 
[(Number of total divergences)/2]/(mutations per generation)
to determine how many generations have passed since the divergence of humans and chimps.  (They divide the total number by two because the divergences represent changes accumulated in both the chimp genome AND the human genome, whereas you want the number of generations for just one species, since they're happening simultaneously.)

Now that they have the number of generations, they convert that to a time by multiplying that number by the average generation time - that is, the age at which a parent has a child (the average child, not first or last). 

So basically, find out how different the genomes are, find out how many mutations happen per generation, and calculate how many generations have passed.  Then multiply by the number of years a generation is.

Finally, they corroborate it with fossil evidence.  We can date fossils using isotope dating, so if we have fossils for all the ""intermediate"" species dating back to a common ancestor for two species, we can get a good timeframe for their divergence.  The problem with fossil evidence is that it's actually very limited for non-human apes.  We have a good fossil record for the human lineage, but not for the chimp, gorilla, or orangutan lineage.  The next closest primate that has a really good fossil record is actually macaques (a type of monkey), so calculations are often checked against the macaque record.  For a long time, our ape calculations actually didn't jive so well with the macaque record.

Something interesting happened in 2012 (I could be misremembering the year).  Scholars named Scally and Durbin proposed that the calculations had all been incorrect because they had used generation time for *current apes*.  Larger animals tend to have larger generation times (bigger animals have kids later, take longer to mature), and extant modern apes are generally larger than their ancestors.  Therefore the ""generation time"" variable was decreased a little, and these guys' new calculations fit better with the macaque evidence. 

Edit: wording",null,14,cdiub9d,1r1z4w,askscience,top_week,82
skadefryd,"I'm gonna stick out like a sore thumb in this thread, because I have a very different picture of how these estimates are obtained. I look forward to being proven wrong, though, because there are some people in this thread who know a lot more about the subject than I do.

My understanding has always been that divergence time estimates are generally obtained based on fossil calibration. These estimates are then compared to the number of (purportedly neutral) substitutions to obtain a neutral substitution rate and hence mutation rate, *not the other way around*. Measuring mutation rates in vivo is really hard, and we've only just recently been able to do it with any degree of precision, and a variety of factors can cause it not to agree precisely with mutation rates estimated phylogenetically (though they typically agree to within fifty per cent or so).

Any of the above might be completely wrong. Maybe /u/patchgrabber or /u/jjberg2 can set me right.",null,3,cdj46ol,1r1z4w,askscience,top_week,12
null,null,null,0,cdiyvny,1r1z4w,askscience,top_week,3
nedved777,"How do we figure out the ""number of substitutions per base pair per generation for a given piece of DNA?""  Is this something we can find using, for example, only two or three generations of chimpanzee DNA, or is it something requiring us to count the number of substitutions over thousands of years in a fossil sample whose age was determined by another method (e.g. radioactive dating)?

Since we are talking about specific proteins, some of which (cytochrome c) are present in almost all organisms, is there any reason we can't monitor rate of mutations per generation in some species of bacteria or something to find the rate?",null,1,cdiu46g,1r1z4w,askscience,top_week,2
null,null,null,1,cdiytjz,1r1z4w,askscience,top_week,2
null,null,null,13,cdirtxz,1r1z4w,askscience,top_week,4
patchgrabber,"Molecular Clock Hypothesis tries to estimate how far apart organisms are evolutionarily by means of using specific proteins. Some proteins, such as cytochrome c (present in almost all organisms) seem to have a fairly consistent time between neutral mutations, meaning that if most mutations are neutral (have no effect on fitness), and if they occur at more or less regular intervals, you can estimate how many new mutations you should see in a generation. 

Thus, by measuring the number of mutations in that protein from the time when two now distinct species had the same or very similar versions of these proteins, one can theoretically estimate the time these species diverged. There are several limitations of this process, like fossil prevalence, generation time and metabolic rate, among others. So while it may not be a perfect process, it's not without its uses.",null,73,cdiruch,1r1z4w,askscience,top_week,382
oliverisyourdaddy,"I'm an evolutionary anthropologist!

They compared the genomes of humans and chimps, estimating the total number of divergences (changes).  Then they calculated the average number of mutations (changes) in one generation (by comparing the genes of parents and children).  

Then they performed the following calculation: 
[(Number of total divergences)/2]/(mutations per generation)
to determine how many generations have passed since the divergence of humans and chimps.  (They divide the total number by two because the divergences represent changes accumulated in both the chimp genome AND the human genome, whereas you want the number of generations for just one species, since they're happening simultaneously.)

Now that they have the number of generations, they convert that to a time by multiplying that number by the average generation time - that is, the age at which a parent has a child (the average child, not first or last). 

So basically, find out how different the genomes are, find out how many mutations happen per generation, and calculate how many generations have passed.  Then multiply by the number of years a generation is.

Finally, they corroborate it with fossil evidence.  We can date fossils using isotope dating, so if we have fossils for all the ""intermediate"" species dating back to a common ancestor for two species, we can get a good timeframe for their divergence.  The problem with fossil evidence is that it's actually very limited for non-human apes.  We have a good fossil record for the human lineage, but not for the chimp, gorilla, or orangutan lineage.  The next closest primate that has a really good fossil record is actually macaques (a type of monkey), so calculations are often checked against the macaque record.  For a long time, our ape calculations actually didn't jive so well with the macaque record.

Something interesting happened in 2012 (I could be misremembering the year).  Scholars named Scally and Durbin proposed that the calculations had all been incorrect because they had used generation time for *current apes*.  Larger animals tend to have larger generation times (bigger animals have kids later, take longer to mature), and extant modern apes are generally larger than their ancestors.  Therefore the ""generation time"" variable was decreased a little, and these guys' new calculations fit better with the macaque evidence. 

Edit: wording",null,14,cdiub9d,1r1z4w,askscience,top_week,82
skadefryd,"I'm gonna stick out like a sore thumb in this thread, because I have a very different picture of how these estimates are obtained. I look forward to being proven wrong, though, because there are some people in this thread who know a lot more about the subject than I do.

My understanding has always been that divergence time estimates are generally obtained based on fossil calibration. These estimates are then compared to the number of (purportedly neutral) substitutions to obtain a neutral substitution rate and hence mutation rate, *not the other way around*. Measuring mutation rates in vivo is really hard, and we've only just recently been able to do it with any degree of precision, and a variety of factors can cause it not to agree precisely with mutation rates estimated phylogenetically (though they typically agree to within fifty per cent or so).

Any of the above might be completely wrong. Maybe /u/patchgrabber or /u/jjberg2 can set me right.",null,3,cdj46ol,1r1z4w,askscience,top_week,12
null,null,null,0,cdiyvny,1r1z4w,askscience,top_week,3
nedved777,"How do we figure out the ""number of substitutions per base pair per generation for a given piece of DNA?""  Is this something we can find using, for example, only two or three generations of chimpanzee DNA, or is it something requiring us to count the number of substitutions over thousands of years in a fossil sample whose age was determined by another method (e.g. radioactive dating)?

Since we are talking about specific proteins, some of which (cytochrome c) are present in almost all organisms, is there any reason we can't monitor rate of mutations per generation in some species of bacteria or something to find the rate?",null,1,cdiu46g,1r1z4w,askscience,top_week,2
null,null,null,1,cdiytjz,1r1z4w,askscience,top_week,2
null,null,null,13,cdirtxz,1r1z4w,askscience,top_week,4
YoYoDingDongYo,"Here's one: http://link.springer.com/article/10.1007/BF01055264#page-1

Here's another: http://www.ncbi.nlm.nih.gov/pubmed/1175391",null,0,cdir0dx,1r1xxt,askscience,top_week,8
My_Nipples_AreOnFire,"Your question actually touches on a great deal of pharmacological principles. Classical pharmacology assumes that a drug's therapeutic/toxic effects are directly related to concentration achieved at the site of action (ie. the tissue/cells that the drugs are effecting). This concentration is directly correlated to the concentration achieved in the blood, since this is essentially how much drug will be distributed to the site of action. This assumption leads to the conclusion that taking more of a drug will ellicit a greater effect:

* Greater drug concentration administered -&gt; greater blood concentration achieved -&gt; greater distribution to site of action -&gt; stronger elicited effect

This principle has essentially dictated pharmacology for a long time and has had good results.

However, I think this is what others are getting at, this does not mean that greater dose necessarily means more **therapeutic** effect. This simply means greater dose produces greater effect. After a point, this effect leads to a **toxic** effect rather than a therapeutic one. As you increase concentration you will reach a point where 100% of respondents are experiencing the drug effect, but you enter a concentration where toxic effects are produced. So to answer your question, classical pharmacology (actually pharmacokinetics, to be specific) would say, in terms of effect, no. Greater concentration of drug will elicit greater effect up to a point, but this does not necessarily equal the desired therapeutic effect. Just the drug's chemical effect.

This is the distinction that must be made. Because some drugs (such as those listed by /u/YoYoDingDongYo) will have some idea therapeutic dose, but prescribing a dose greater than that could lead to a greater *drug* effect that inhibits it's *therapeutic* effect.",null,2,cdj4v6k,1r1xxt,askscience,top_week,4
mrdeath5493,"So basically I think you would want to look into dose-response curves.  [This one](http://www.softchalk.com/lessonchallenge09/lesson/Pharmacology/dose_response.png), for morphine, shows that at some point increasing the dose  no longer increases its analgesic effects (pain relief).  But if you keep going, you start getting way way too much respiratory depression.  So if you were giving a dose that was too high for a particular patient, then you could decrease the dose and it would be just as effective at pain relief but with very little of the undesirable effect.  In a way, a decrease in this manner is making better use of the drug.  even though I don't think this is what you were looking for, it applies to most drugs and is a very good example to start with.

From here it is important to note that it is all relative.  Drugs have many effects, but are mostly used for a single effect.  I'll try to focus on the single therapeutic uses instead of the side effects.  In trying to answer your question, I found myself at a theoretical conundrum.  The smallest dose of a drug would be 1 individual particle.  Is there a drug out there that has its greatest effect at 1 particle and then decreases in effect as you increase the dose.  I wasn't sure that was possible, [however /u/YoYoDingDongYo provided an excellent example](http://www.reddit.com/r/askscience/comments/1r1xxt/chemistrypharmacology_are_there_any_drugs_that/cdir0dx) where a drug was more effective at *1 part per billion* than at higher doses which actually kinda blew my mind.

Why is this?...The following is educated guessing:  Well, the drug in that study exerts an effect a 1ppb, and at some point past that it must breech a threshold where it activates another biological process that works against the desired effect.  Either a detection mechanism is triggered or as it becomes more concentrated the cell might start to actively eliminate it.  I would like to see studies to see if there was an induction of elimination and if it was permanent.  Also, it might be helpful to look at 1,2,3,4, etc. ppb to see if there might actually be a threshold and perhaps an increasing effect on its way to 1,000 ppb.",null,2,cdiwwtj,1r1xxt,askscience,top_week,3
ModernTarantula,"Here are a couple of concepts: In immunology prophylaxis is the action of vaccine wherein the first exposure has a small immune response but subsequent exposure has a magnified immune response. Anaphylaxis wherein prior exposure leads to a magnified detrimental immune effect. The last of these is not from immunology--tachyphylaxis, repeated exposure depletes the response until absent. This is seen amongst tweekers who use meth until they deplete all neurotransmitters in their brains.",null,0,cdjhidk,1r1xxt,askscience,top_week,1
eclarep,"One example of a drug that has the effect you mention is Trazdone, an antidepressant that is much more commonly prescribed for sleep problems.
At a lower dose, much too low for antidepressant effects, trazdone has sedative effect but it takes a much higher dose to have antidepressant properties.
So, as a prescription sleep aid, the effect is dependent on the dose in a somewhat inverse manner. Above a certain threshold and up to a limit, it causes sedation, outside that range it can help with depression.
This is not medical advice, just a case that is an example of a drug of the kind you asked about.
[source](http://www.medscape.org/viewarticle/508820)",null,0,cdkulfh,1r1xxt,askscience,top_week,1
ringboard,"If something causes a increase in one effect, then it is by default causing a decrease in another effect. For example the Non-steroidal anti-inflammatory drug, COX-2 inhibitor, will decrease the activity of Cox-2, but by doing this it is increasing anti-inflammation effects. So higher doses means less Cox-2 activity but more anti-inflammatory activity.",null,4,cdiu26c,1r1xxt,askscience,top_week,2
Rangi42,"First of all, inventing a way to describe how stuff works in terms of atoms, gravity, spacetime, or other such models requires the same kind of creativity as inventing a way to depict your senses using [geometric shapes](https://en.wikipedia.org/wiki/Nude_Descending_a_Staircase,_No._2) or [colored points](https://en.wikipedia.org/wiki/Pointillism), if not more. To quote Voltaire, ""There is far more imagination in the head of [Archimedes](https://en.wikipedia.org/wiki/Archimedes%27_principle) than in that of [Homer](https://en.wikipedia.org/wiki/Iliad)."" The difference is, artists use their intuitive inspiration to fuel their works, whereas scientists have to define [their theories](https://en.wikipedia.org/wiki/Atomic_theory) in concrete enough terms that they can be understood and tested by others.

You bring up the example of atomic theory. Democritus somehow had the inspiration of matter being made up of atoms 2400 years ago, but at the time his idea remained just an interesting concept. [John Dalton's theory](https://en.wikipedia.org/wiki/Dalton%27s_atomic_theory#Atomic_theory) in 1800 was more than just a neat idea: he took the time to deduce what consequences would follow from the initial idea, such as different types of atoms having particular weights, and used his theory to [explain his observations](https://en.wikipedia.org/wiki/Law_of_multiple_proportions).

The thing is, 100% certainty is impossible. Alternative theories can always account for the observed evidence if you twist them enough. The geocentric model of the solar system was bolstered by adding more and more [epicycles](https://en.wikipedia.org/wiki/Epicycle) to fit the planets' observed orbits, and even though the heliocentric model is much simpler, the geocentric one isn't actually ruled out.

A rule of thumb to decide between two models is [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), which basically says that you should prefer the simpler theory as long as it explains the same data. Before Einstein came up with his theory of relativity, Newton's law of universal gravitation seemed like a good enough model for how mass moves through space. The [precession of Mercury's orbit](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury) could be dismissed as observational error, or a result of some unknown celestial body influencing it, or other explanations. When relativity's predictions not only matched Mercury's observed orbit more precisely, but also [explained other phenomena](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Deflection_of_light_by_the_Sun), Occam's razor would suggest adopting relativity over Newtonian gravitation.

As for how scientists gather data to test their theories and invent new ones: it's hard. Measuring [tiny](https://en.wikipedia.org/wiki/Quark), or [huge](https://en.wikipedia.org/wiki/Supercluster), or [distant](https://en.wikipedia.org/wiki/Pulsar), or other extreme things requires technological breakthroughs, which rely on the same sort of insight as artistic or scientific breakthroughs. There are [theories](https://en.wikipedia.org/wiki/Insight#Theories) about how people come up with new ideas, and attempts to [program computers](https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms) with some of our abilities, but we're nowhere near understanding or reproducing it yet. One technique is to predict the side-effects something will have and look for those instead. Black holes, for instance, can't be observed directly, but [the way they bend starlight](https://en.wikipedia.org/wiki/Gravitational_lensing) can be predicted, and telescopes check if the actual sky matches the predictions.",null,27,cdir7gf,1r1xxj,askscience,top_week,161
__Pers,"Atomic theory is one of the most well established and experimentally validated theories in science. Are we 100% sure of anything in science? No, but in this case, we're very confident we have the essential theory correct. 

Like the rest of science, it came about through application of the scientific method: proposing hypotheses, testing said hypotheses, and refining one's models as needed.

As astonishing as it may seem, while atoms are indeed small, [we've even been able to trap and study individual atoms in isolation](http://www.opticsinfobase.org/ol/abstract.cfm?uri=ol-35-13-2164). ",null,7,cdir6fh,1r1xxj,askscience,top_week,41
yeast_problem,"I think the easiest way is to learn the history of the discoveries. The greeks believed that elements could be divided into smaller and smaller pieces until an ""atom"" remained, but they didnt even know about electrons or the periodic table.

Try looking at the [Thomson Model](http://en.wikipedia.org/wiki/Plum_pudding_model) to get a feel for some of the intermediate stages of discovery.

You can calculate the mass of charged particles by firing them through a magnetic field and working out the force the magnetic field applied from the radius of the curve they make.",null,0,cdirad3,1r1xxj,askscience,top_week,5
RainmakerUK,"It is the best model we have -right now- to describe our observations of matter. 

In the world of Science, we are never sure. There is no ""sure"" in Science. We only have best guesses, which are replaced by better guesses over time, often as a result of observations which can't be explained by our current model. 

Always remember, ""the map is not the territory"". Our equations and models are simply our best descriptors of reality. They are a functional tool. They are not the roadmarkers of reality itself. ",null,1,cdiyswc,1r1xxj,askscience,top_week,6
HoopyHobo,"CERN is 99.999999999% percent sure that they have found the Higgs Boson, and that absolutely pales in comparison to how certain we are about protons, electrons and neutrons. Scientists will probably always tell you (correctly) that they can never be 100% certain about anything, but the infinitesimal percentage that isn't accounted for shouldn't really matter to anyone, especially those of us who aren't physicists.",null,1,cdj6vr7,1r1xxj,askscience,top_week,5
TurboCommander,"From Albert Einstein and Leopold Infeld, in their co-written book The Evolution of Physics. 

In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears the ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations.

He will never be able to compare his picture with the real mechanism and he cannot even imagine the possibility or meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of sensuous impressions.",null,2,cdiyefe,1r1xxj,askscience,top_week,4
TheGoodMachine,"There is no such thing as 100% sure. All we know is that until now all our observations agreed on that, and not a single observation disagreed, and hence it is ***useful*** (in predicting the world), which is exactly all that matters. And we did a shitload of observation (usually as a result of experiments).

So while this is a valid layman question, its an invalid type of question if you really think about it. A very common one though, which we all asked at least once in our lives.

Hence there will be no answer that will make you happy. You have to change your question. :)",null,0,cdj00dm,1r1xxj,askscience,top_week,2
Hypothesis_Null,"Yes we are 100% certain.

We are not 100% certain what makes up protons, neutrons, and electrons, and even less certain of what makes up what we think makes them up... but we are not wrong that things we call protons neutrons and electrons exist and act approximately as we've modeled them.

There is too much evidence and consistency in the model we have.  The model cannot be proven 'wrong' at this point.  It can be proven to be a constrained version of a more general model.  We could discover them doing something we didn't notice before if we put them under extreme conditions we havn't observed them under before.  But that will only result in a deeper understanding of them, not a different one.  Any new rules we learn will have the old rules as emergent approximations when put under the more common conditions we're used to. 

The common analogy is Newtonian Physics.  Newton wasn't wrong.  It's just when you things get extremely massive and/or fast, or are massless, the equations no longer accurately describes their behavior.  But if you use the more general equations Einstein and others have developed since, the equations reduce back to the Newtonian approximations when used on everything from large molecules to stars going slower than 1% lightspeed.

**TL;DR** We are certain, because they have been observed behaving as we believe them to behave too consistently and too much.  Anything at this point will not disprove what we know, but simply shrink the error bars on what we do not.",null,0,cdj5r5l,1r1xxj,askscience,top_week,2
dracho,"On almost any level, yes, we are sure.  

On the other hand, nothing is ever 100%.  Nothing.  Sorry to get philosophical, but there's always an exception to the rule.  Different universe?  Inside a black hole?  Dimension X?  Who knows?  Nobody, with 100% certainty.",null,3,cdiwu3h,1r1xxj,askscience,top_week,4
CoolStoryJohn,"The atomic model can be viewed as just that: a model.  We've had scientific models in the past that have been outdated/scrapped, and  inherently, any legitimate scientific model will be open to the same treatment given sufficient evidence.  That being said, the atomic model has been incredibly successful in both explaining behavior and predicting it.  At its base, the atomic model is based on induction (i.e. experimental evidence and generalization of specific results).  We've used the atomic model to explain the results of various experiments, and used those results/explanations to predict future cases (which have largely been accurate).  So then, up until this point: so far so good.  ",null,2,cdixr0j,1r1xxj,askscience,top_week,3
coniform,"&gt; Are we 100 % sure that the matter is made of atoms , the atoms made of protons, electrons and neutrons ect... ?

Yes. There is overwhelming, irrefutable evidence supporting the existence of atoms, and the particular structure of atoms.

&gt; how did we even deduce such things?

Here is a very clever and historically groundbreaking approach: [brownian motion](http://en.wikipedia.org/wiki/Brownian_motion#Einstein.27s_theory).

You can see brownian motion yourself...get a cup of water and watch very closely that the particles will move very erratically.

&gt; And how could we calculate the number of electrons of each element? And most of all , how could we calculate each of these particles' mass if they are so small ?

This has to do with how we figured out how the elements were [ordered in the periodic table](http://www.rsc.org/education/teachers/resources/periodictable/pre16/order/atomicnumber.htm).

All of these problems were studied just over a century ago. Isn't it amazing? This was barely after the discovery of the electron, and before the development of quantum mechanics. All of these very basic ideas foreshadowed an entire century of amazing scientific ideas.",null,1,cdjbl8k,1r1xxj,askscience,top_week,2
logical,"The atomic theory of matter is a proven theory.  We are certain of it.  We did not deduce it, but we induced it.  

This theory, like all proven scientific theories, start with observations of reality and arrive at generalizations by integrating them with other observations.  Contradictions of observations in controlled experiments rule out possible explanations and confirmations lead to further validation.  

It is important to note that additional knowledge can further refine a theory, but that in the context of what was known the original theory is not incorrect, but limited.  

If the atomic theory of matter was false, atom bombs and nuclear reactors wouldn't work, the periodic table of elements would not be such an accurate predictor of what elements exists and what their properties were, molecular science wouldn't work (which is what is behind most of the chemicals in your life, from cleaning agents to medicines) and so much of what you depend on from science would simply have to obey different laws and possess different properties.  

You can find a history of the timeline of the discovery of the atomic theory of matter [here](http://atomictimeline.net/index.php).

There are some good courses available that summarize the history of this theory if you look for them.",null,0,cdjcfij,1r1xxj,askscience,top_week,1
HighPriestofShiloh,"No.  Only 99.999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999%",null,2,cdjdtst,1r1xxj,askscience,top_week,3
dopsi,"Actually we can't be sure that these elements even exist, this is a model (which means an approximation) of the universe, build from our knowledge. For now this is the most accurate model we have, but it has its limits, like every model.

If you have a look back, the Greek astronomers believed the Earth was in the middle of the universe and the Sun and planets turned around it in circle. Their model gave them accurate enough results for their purpose and they didn't need another model. We know it isn't the case, but this was discovered when scientists got new and more accurate methods of measuring the world, that's why the current model could be seen as senseless is several centuries.

That's why we can't say we're 100% sure, because it's a model.",null,9,cditw52,1r1xxj,askscience,top_week,8
ChocoThornton,"I recently graduated with a degree in the History and Philosophy of Science, and this is a popular [realism/anti-realism debate](http://plato.stanford.edu/entries/realism/). Are scientific theories what actually happens, or are they just tools used to understand the Universe? It's a vast philosophical concept with no answer I'm afraid!",null,6,cdiw8bc,1r1xxj,askscience,top_week,6
Rangi42,"First of all, inventing a way to describe how stuff works in terms of atoms, gravity, spacetime, or other such models requires the same kind of creativity as inventing a way to depict your senses using [geometric shapes](https://en.wikipedia.org/wiki/Nude_Descending_a_Staircase,_No._2) or [colored points](https://en.wikipedia.org/wiki/Pointillism), if not more. To quote Voltaire, ""There is far more imagination in the head of [Archimedes](https://en.wikipedia.org/wiki/Archimedes%27_principle) than in that of [Homer](https://en.wikipedia.org/wiki/Iliad)."" The difference is, artists use their intuitive inspiration to fuel their works, whereas scientists have to define [their theories](https://en.wikipedia.org/wiki/Atomic_theory) in concrete enough terms that they can be understood and tested by others.

You bring up the example of atomic theory. Democritus somehow had the inspiration of matter being made up of atoms 2400 years ago, but at the time his idea remained just an interesting concept. [John Dalton's theory](https://en.wikipedia.org/wiki/Dalton%27s_atomic_theory#Atomic_theory) in 1800 was more than just a neat idea: he took the time to deduce what consequences would follow from the initial idea, such as different types of atoms having particular weights, and used his theory to [explain his observations](https://en.wikipedia.org/wiki/Law_of_multiple_proportions).

The thing is, 100% certainty is impossible. Alternative theories can always account for the observed evidence if you twist them enough. The geocentric model of the solar system was bolstered by adding more and more [epicycles](https://en.wikipedia.org/wiki/Epicycle) to fit the planets' observed orbits, and even though the heliocentric model is much simpler, the geocentric one isn't actually ruled out.

A rule of thumb to decide between two models is [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), which basically says that you should prefer the simpler theory as long as it explains the same data. Before Einstein came up with his theory of relativity, Newton's law of universal gravitation seemed like a good enough model for how mass moves through space. The [precession of Mercury's orbit](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury) could be dismissed as observational error, or a result of some unknown celestial body influencing it, or other explanations. When relativity's predictions not only matched Mercury's observed orbit more precisely, but also [explained other phenomena](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Deflection_of_light_by_the_Sun), Occam's razor would suggest adopting relativity over Newtonian gravitation.

As for how scientists gather data to test their theories and invent new ones: it's hard. Measuring [tiny](https://en.wikipedia.org/wiki/Quark), or [huge](https://en.wikipedia.org/wiki/Supercluster), or [distant](https://en.wikipedia.org/wiki/Pulsar), or other extreme things requires technological breakthroughs, which rely on the same sort of insight as artistic or scientific breakthroughs. There are [theories](https://en.wikipedia.org/wiki/Insight#Theories) about how people come up with new ideas, and attempts to [program computers](https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms) with some of our abilities, but we're nowhere near understanding or reproducing it yet. One technique is to predict the side-effects something will have and look for those instead. Black holes, for instance, can't be observed directly, but [the way they bend starlight](https://en.wikipedia.org/wiki/Gravitational_lensing) can be predicted, and telescopes check if the actual sky matches the predictions.",null,27,cdir7gf,1r1xxj,askscience,top_week,161
__Pers,"Atomic theory is one of the most well established and experimentally validated theories in science. Are we 100% sure of anything in science? No, but in this case, we're very confident we have the essential theory correct. 

Like the rest of science, it came about through application of the scientific method: proposing hypotheses, testing said hypotheses, and refining one's models as needed.

As astonishing as it may seem, while atoms are indeed small, [we've even been able to trap and study individual atoms in isolation](http://www.opticsinfobase.org/ol/abstract.cfm?uri=ol-35-13-2164). ",null,7,cdir6fh,1r1xxj,askscience,top_week,41
yeast_problem,"I think the easiest way is to learn the history of the discoveries. The greeks believed that elements could be divided into smaller and smaller pieces until an ""atom"" remained, but they didnt even know about electrons or the periodic table.

Try looking at the [Thomson Model](http://en.wikipedia.org/wiki/Plum_pudding_model) to get a feel for some of the intermediate stages of discovery.

You can calculate the mass of charged particles by firing them through a magnetic field and working out the force the magnetic field applied from the radius of the curve they make.",null,0,cdirad3,1r1xxj,askscience,top_week,5
RainmakerUK,"It is the best model we have -right now- to describe our observations of matter. 

In the world of Science, we are never sure. There is no ""sure"" in Science. We only have best guesses, which are replaced by better guesses over time, often as a result of observations which can't be explained by our current model. 

Always remember, ""the map is not the territory"". Our equations and models are simply our best descriptors of reality. They are a functional tool. They are not the roadmarkers of reality itself. ",null,1,cdiyswc,1r1xxj,askscience,top_week,6
HoopyHobo,"CERN is 99.999999999% percent sure that they have found the Higgs Boson, and that absolutely pales in comparison to how certain we are about protons, electrons and neutrons. Scientists will probably always tell you (correctly) that they can never be 100% certain about anything, but the infinitesimal percentage that isn't accounted for shouldn't really matter to anyone, especially those of us who aren't physicists.",null,1,cdj6vr7,1r1xxj,askscience,top_week,5
TurboCommander,"From Albert Einstein and Leopold Infeld, in their co-written book The Evolution of Physics. 

In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears the ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations.

He will never be able to compare his picture with the real mechanism and he cannot even imagine the possibility or meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of sensuous impressions.",null,2,cdiyefe,1r1xxj,askscience,top_week,4
TheGoodMachine,"There is no such thing as 100% sure. All we know is that until now all our observations agreed on that, and not a single observation disagreed, and hence it is ***useful*** (in predicting the world), which is exactly all that matters. And we did a shitload of observation (usually as a result of experiments).

So while this is a valid layman question, its an invalid type of question if you really think about it. A very common one though, which we all asked at least once in our lives.

Hence there will be no answer that will make you happy. You have to change your question. :)",null,0,cdj00dm,1r1xxj,askscience,top_week,2
Hypothesis_Null,"Yes we are 100% certain.

We are not 100% certain what makes up protons, neutrons, and electrons, and even less certain of what makes up what we think makes them up... but we are not wrong that things we call protons neutrons and electrons exist and act approximately as we've modeled them.

There is too much evidence and consistency in the model we have.  The model cannot be proven 'wrong' at this point.  It can be proven to be a constrained version of a more general model.  We could discover them doing something we didn't notice before if we put them under extreme conditions we havn't observed them under before.  But that will only result in a deeper understanding of them, not a different one.  Any new rules we learn will have the old rules as emergent approximations when put under the more common conditions we're used to. 

The common analogy is Newtonian Physics.  Newton wasn't wrong.  It's just when you things get extremely massive and/or fast, or are massless, the equations no longer accurately describes their behavior.  But if you use the more general equations Einstein and others have developed since, the equations reduce back to the Newtonian approximations when used on everything from large molecules to stars going slower than 1% lightspeed.

**TL;DR** We are certain, because they have been observed behaving as we believe them to behave too consistently and too much.  Anything at this point will not disprove what we know, but simply shrink the error bars on what we do not.",null,0,cdj5r5l,1r1xxj,askscience,top_week,2
dracho,"On almost any level, yes, we are sure.  

On the other hand, nothing is ever 100%.  Nothing.  Sorry to get philosophical, but there's always an exception to the rule.  Different universe?  Inside a black hole?  Dimension X?  Who knows?  Nobody, with 100% certainty.",null,3,cdiwu3h,1r1xxj,askscience,top_week,4
CoolStoryJohn,"The atomic model can be viewed as just that: a model.  We've had scientific models in the past that have been outdated/scrapped, and  inherently, any legitimate scientific model will be open to the same treatment given sufficient evidence.  That being said, the atomic model has been incredibly successful in both explaining behavior and predicting it.  At its base, the atomic model is based on induction (i.e. experimental evidence and generalization of specific results).  We've used the atomic model to explain the results of various experiments, and used those results/explanations to predict future cases (which have largely been accurate).  So then, up until this point: so far so good.  ",null,2,cdixr0j,1r1xxj,askscience,top_week,3
coniform,"&gt; Are we 100 % sure that the matter is made of atoms , the atoms made of protons, electrons and neutrons ect... ?

Yes. There is overwhelming, irrefutable evidence supporting the existence of atoms, and the particular structure of atoms.

&gt; how did we even deduce such things?

Here is a very clever and historically groundbreaking approach: [brownian motion](http://en.wikipedia.org/wiki/Brownian_motion#Einstein.27s_theory).

You can see brownian motion yourself...get a cup of water and watch very closely that the particles will move very erratically.

&gt; And how could we calculate the number of electrons of each element? And most of all , how could we calculate each of these particles' mass if they are so small ?

This has to do with how we figured out how the elements were [ordered in the periodic table](http://www.rsc.org/education/teachers/resources/periodictable/pre16/order/atomicnumber.htm).

All of these problems were studied just over a century ago. Isn't it amazing? This was barely after the discovery of the electron, and before the development of quantum mechanics. All of these very basic ideas foreshadowed an entire century of amazing scientific ideas.",null,1,cdjbl8k,1r1xxj,askscience,top_week,2
logical,"The atomic theory of matter is a proven theory.  We are certain of it.  We did not deduce it, but we induced it.  

This theory, like all proven scientific theories, start with observations of reality and arrive at generalizations by integrating them with other observations.  Contradictions of observations in controlled experiments rule out possible explanations and confirmations lead to further validation.  

It is important to note that additional knowledge can further refine a theory, but that in the context of what was known the original theory is not incorrect, but limited.  

If the atomic theory of matter was false, atom bombs and nuclear reactors wouldn't work, the periodic table of elements would not be such an accurate predictor of what elements exists and what their properties were, molecular science wouldn't work (which is what is behind most of the chemicals in your life, from cleaning agents to medicines) and so much of what you depend on from science would simply have to obey different laws and possess different properties.  

You can find a history of the timeline of the discovery of the atomic theory of matter [here](http://atomictimeline.net/index.php).

There are some good courses available that summarize the history of this theory if you look for them.",null,0,cdjcfij,1r1xxj,askscience,top_week,1
HighPriestofShiloh,"No.  Only 99.999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999%",null,2,cdjdtst,1r1xxj,askscience,top_week,3
dopsi,"Actually we can't be sure that these elements even exist, this is a model (which means an approximation) of the universe, build from our knowledge. For now this is the most accurate model we have, but it has its limits, like every model.

If you have a look back, the Greek astronomers believed the Earth was in the middle of the universe and the Sun and planets turned around it in circle. Their model gave them accurate enough results for their purpose and they didn't need another model. We know it isn't the case, but this was discovered when scientists got new and more accurate methods of measuring the world, that's why the current model could be seen as senseless is several centuries.

That's why we can't say we're 100% sure, because it's a model.",null,9,cditw52,1r1xxj,askscience,top_week,8
ChocoThornton,"I recently graduated with a degree in the History and Philosophy of Science, and this is a popular [realism/anti-realism debate](http://plato.stanford.edu/entries/realism/). Are scientific theories what actually happens, or are they just tools used to understand the Universe? It's a vast philosophical concept with no answer I'm afraid!",null,6,cdiw8bc,1r1xxj,askscience,top_week,6
miczajkj,"I can choose an coordinate system, where the Origin and the trajectory of the object lie in the same plane and make this plane the xy-plane. 
The origin is now (0,0). Now if the particle is moving with constant velocity v (constant velocity also means always the same direction) I can choose t=0 in a way, that d(0) = (d_0, 0) and v(0) = (0, v). Than the position of the object at an arbitrary t is d(t) = (d_0, vt). 

This is only an coordinate displacement, so the formula is completely non-relativistic. The relativistic effects only arise in the objects reference frame.

Okay, now let's have a look on what we think about the particles position. At the time t we see the particle at the position b(t). As the light from the particle has taken the time t' = |b(t)|/c this is equals:

b(t) = d(t-t') = d(t-|b(t)|/c)

So now we have an implicit equation to calculate b(t) from a given d(t)!
Let's substitute it into our equation. Notice, that the x-coordinate doesn't change (it's always d_0) so we only need to investigate the y-coordinate: 

y = v*(t-sqrt(d_0 + y)/c)

(always read y as y(t).)

We can easily solve this equation vor y: 

First we introduce  = v/c and write the equation as

y - ct = - sqrt(d_0 + y)  
y - 2 ct y + ct =  (d_0 + y)
y*(1-) - 2ct y +  (ct - d_0) = 0  
y - 2 /(1-) ct y + /(1-) (ct - d_0) = 0  

y = /(1-) ct +- sqrt(/(1-) ct - /(1-) (ct - d_0))  
y = /(1-) (ct +- sqrt(ct - (1-)(ct-d_0))  
y = /(1-) (ct +- sqrt( ct + d_0(1-))

We can now think about what y(0) should be: because the particle is moving towards positive y-values, y(0) should be negative - it is a position the particle has been before t=0. So we need to take the negative root. (The other root belongs to the so called advanced particle [in comparison to the retarded particle] that is no physical solution to this situation). If we choose to substitute =v/c again, we get: 

y(t) = 1/(1-v/c) (vt - v/c sqrt( vt + d_0(1-v/c))

Now the current position of the particle is (d_0, vt) = (u, w) so the observed position as a function of current position and the velocity is

(x(u, w, v), y(u, w, v)) = (u, 1/(1-v/c) (w - v/c sqrt(w + u(1-v/c)))

What can be expressed more nicely by using =v/c and   = 1/sqrt(1-v/c) as

(x, y) = (u, (w- sqrt(w+u/)))

To find the solution in other coordinate systems you can now make simple coordinate transformations. (Note that it get's more complicated, as v -&gt; v(cos , sin ) because you need to know the direction too.)

It's a funny thing, that all those relativistic factors show up, even if we didn't choose to use any features of special relativity. 

[Can you think of the reason?](#s ""It's because the whole formalism of relativity is inspired from the asumption of retarded potentials. It needs to reproduce the feature of delay between the emission of a signal and the measurement of this signal at another position. Therefore these factors are inherent if you choose to consider finite speed of information."")
",null,0,cdiznjj,1r1xvr,askscience,top_week,3
shamdalar,"As stated, your problem seems to be overdetermined. Let's leave light out of it, and assume we get a message traveling at speed c in a straight line from the object traveling at velocity v in a straight line, but we don't know the direction it came from. We know the current position and distance d from the object. If the angle at A is theta, the angle at our position is arcsin(sin(theta)*(v/c)) by the law of sines, and we can use law of sines again to find the distance from A to B.",null,1,cdixxsq,1r1xvr,askscience,top_week,2
hikaruzero,"&gt;What is the formula for finding this observed location given the current location and velocity of the object?

It's the simple formula for displacement with a constant velocity.

d = d*_0_* + vt

You should remember this formula from high school physics class.  It was pretty much the easiest one.  :)

&gt;Since it's for a little relativity game, it would be nice if the formula/algorithm were expressed in matrix/vector manipulation (eg cross product, dot product, matrix multiplication etc) and simple operators such (eg multiplication, addition subtraction, square root, etc.)

Just copy the formula above three times (one for each spatial axis) and put a subscript labelling the axis on every variable except t.  Then put them vertically in square brackets.  The only operation is simple vector addition -- you can add the components independently of eachother.  You can also combine them into a single magnitude of the vector with the Pythagorean theorem, which you should have learned in geometry class in jr. high school:

d = (d*_x_*^(2) + d*_y_*^(2) + d*_z_*^(2))

Not sure what that has to do with relativity though.  Indeed it really doesn't have anything to do with relativity -- the relativistic formula is identical, it's just that the variables take on different values in different reference frames, and those variables are related to eachother in different frames by way of a [Lorentz transformation](http://en.wikipedia.org/wiki/Lorentz_transformation).

&gt;Trigonometric functions such as sine, arcsin cosine are tolerable but not so good. Solutions that require calculus probably can't be used for performance reasons alas.

You don't need any of this -- the problem's not *that* hard.",null,1,cdivtu1,1r1xvr,askscience,top_week,1
xenneract,"Odor depends on the release of small volatile molecules known as odorants. Inside orange peels is a small organic molecule known as [limonene](https://en.wikipedia.org/wiki/Limonene), which is responsible for the smell. Limonene has a vapor pressure of about 1 mmHg at room temperature, which means that it will exist almost entirely in the vapor phase. So as soon as you peel your orange, all of the limonene in the peel rapidly vaporizes and diffuses through the room.",null,0,cdivrnt,1r1xaf,askscience,top_week,8
sporclesam,"Even without forward facing vision, animals such as Horses &amp; other herbivores (majorly [Monocular Vision] (http://en.wikipedia.org/wiki/Monocular_vision)) have great field of vision but poor depth of field (Look up [Stereopsis](http://en.wikipedia.org/wiki/Stereopsis) for more on DoF). 
Other links: [1] (http://en.wikipedia.org/wiki/Binocular_vision), [2 (sorry for the garish blue!)] (http://www.artinarch.com/vp05.html)

SO navigation is no issue. Its interesting how there is a distinct link when it comes to predator-prey relations (with predators more frontal while prey being more lateral in their vision range).
",null,0,cdiy4hq,1r1wof,askscience,top_week,1
MarineLife42,"Not a dumb question at all.  
Rule of thumb: If you can see the eyes looking at the front of the animal, it can see you. So yes, they can see forwards but have poor depth perception. Also, there is usually a small area in front of their heads where they cannot see. Further forward, the angles of view overlap.  
Animals like this don't necessarily use their eyes for navigation, though they certainly use them to look for obstacles close to them. Primarily their eyes are there to warn them of predator's movements, while navigation is often done based on smell and sound.  
So a horse might wander to a farther place that smells good, while its eyes tell it about objects such as trees or the position of other horses in the immediate vicinity. ",null,0,cdj74p7,1r1wof,askscience,top_week,1
skorps,"With our eyes, an animal can not see. But vision is not a requirement for navigation. There are many other examples of ways animals navigate. Bats have echolocation in the dark, many bugs use smells, and many animals can detect vibrations. Some  environments do not require eyes (too dark, undergroubd) other animals just can invest the energy in evolving eyes when other less expensive methods work ",null,3,cdiramq,1r1wof,askscience,top_week,2
ddubspecial,"With today's understanding of the standard model, we have no real answer if what gives particles charge or spin. Before Higgs, we didn't have a definite answer for mass either.  
There are theories that attempt to explain these fundamental properties. In string theory, tiny 1 dimensional strings of energy vibrate around in 10 dimensions if space. It is thought that the shape of these dimensions and the way the strings vibrate through them gives a particle it's properties.  Just a theory though. It's all math at this point. ",null,3,cdistj9,1r1uhx,askscience,top_week,15
fishify,"Charges arise in physics due to the ways particles and fields are modified by certain transformations known as symmetry transformations.  Things that transform differently will have different charges.

In our understanding of physics today, one of the fundamental elements of a physical theory is the set of transformations that leave the equations of motion unchanged.  Such transformations are called the symmetries of the theory, and each particle and field will transform in some specific way under such a transformation.  (By the way, particles and antiparticles differ with respect to a variety of charges, not just electric charge.)  Antiparticles and particles will transform in opposite ways under a given symmetry transformation associated with their charge.

Note that your idea that there is some constituent that carries one or another charge to distinguish particles and antiparticles doesn't really change anything; why should *those* two constituents have opposite charges?  ",null,0,cdixkwe,1r1uhx,askscience,top_week,6
zeug,"Antiparticles are predicted by the Dirac equation, which was an attempt to get a quantum mechanical equation for the electron which satisfied special relativity and accounted for the two spin states for the electron.

Dirac was able to do this by writing down an equation similar to the wave equation, but that does not have any solution for real or complex numbers. The trick was to reinterpret it as a 4x4 matrix equation operating on 4 component vectors.

The first two components naturally appeared to be the two spin states of the electron, and the last two were also spin up and down, but had a difference in sign that caused quite a bit of a puzzle. One consequence of this change is sign is that these last two components would correspond to particles of opposite charge when electromagnetic interactions are added.

So basically, if you want to satisfy what is observed about quantum mechanics and special relativity, and write down an equation to describe a particle with two spin states, such as the electron, the simplest way to go is the Dirac equation, which predicts antimatter components.


So you don't need charge or an electromagnetic field to have antimatter, it is simply there. The neutrinos have no electric charge, and there are corresponding antineutrinos also without charge. 

But it is true that if you add a term to the equation that gives the particles an interaction with the electromagnetic field, the antiparticles have to have an equal and opposite charge to the particles.

",null,0,cditj9k,1r1uhx,askscience,top_week,5
iorgfeflkd,"No, charge is just an inherent property of those particles.",null,4,cdirxwh,1r1uhx,askscience,top_week,5
ddubspecial,"With today's understanding of the standard model, we have no real answer if what gives particles charge or spin. Before Higgs, we didn't have a definite answer for mass either.  
There are theories that attempt to explain these fundamental properties. In string theory, tiny 1 dimensional strings of energy vibrate around in 10 dimensions if space. It is thought that the shape of these dimensions and the way the strings vibrate through them gives a particle it's properties.  Just a theory though. It's all math at this point. ",null,3,cdistj9,1r1uhx,askscience,top_week,15
fishify,"Charges arise in physics due to the ways particles and fields are modified by certain transformations known as symmetry transformations.  Things that transform differently will have different charges.

In our understanding of physics today, one of the fundamental elements of a physical theory is the set of transformations that leave the equations of motion unchanged.  Such transformations are called the symmetries of the theory, and each particle and field will transform in some specific way under such a transformation.  (By the way, particles and antiparticles differ with respect to a variety of charges, not just electric charge.)  Antiparticles and particles will transform in opposite ways under a given symmetry transformation associated with their charge.

Note that your idea that there is some constituent that carries one or another charge to distinguish particles and antiparticles doesn't really change anything; why should *those* two constituents have opposite charges?  ",null,0,cdixkwe,1r1uhx,askscience,top_week,6
zeug,"Antiparticles are predicted by the Dirac equation, which was an attempt to get a quantum mechanical equation for the electron which satisfied special relativity and accounted for the two spin states for the electron.

Dirac was able to do this by writing down an equation similar to the wave equation, but that does not have any solution for real or complex numbers. The trick was to reinterpret it as a 4x4 matrix equation operating on 4 component vectors.

The first two components naturally appeared to be the two spin states of the electron, and the last two were also spin up and down, but had a difference in sign that caused quite a bit of a puzzle. One consequence of this change is sign is that these last two components would correspond to particles of opposite charge when electromagnetic interactions are added.

So basically, if you want to satisfy what is observed about quantum mechanics and special relativity, and write down an equation to describe a particle with two spin states, such as the electron, the simplest way to go is the Dirac equation, which predicts antimatter components.


So you don't need charge or an electromagnetic field to have antimatter, it is simply there. The neutrinos have no electric charge, and there are corresponding antineutrinos also without charge. 

But it is true that if you add a term to the equation that gives the particles an interaction with the electromagnetic field, the antiparticles have to have an equal and opposite charge to the particles.

",null,0,cditj9k,1r1uhx,askscience,top_week,5
iorgfeflkd,"No, charge is just an inherent property of those particles.",null,4,cdirxwh,1r1uhx,askscience,top_week,5
endocytosis,"Probably not.  It depends on how shortly after death (seconds, minutes, hours, days), and also on the recoverability of eggs and sperm.  Generally sperm need the semen that is produced in the ejaculatory ducts and seminiferous vesicles to remain viable and healthy, but it is [possible](http://www.advancedfertility.com/testicsperm.htm) to collect sperm with a syringe by literally (gently) draining off the top layer of sperm that collects in the epididymis with a needle.  This would need to be done very quickly if a person died.   
Eggs can be collected from the follicles [directly](http://www.genea.com.au/Library/Fertility-Treatments/Assisted-conception/IVF), but usually the woman is given the hormone FSH first to maximize the chances that the follicles will produce an egg that can be collected.  If this has not occurred, the chances would be negligible, because with at most minutes to spare, *and* the hope that the woman happened to be ovulating at the time, the surgeon would have to find the follicle that was about to release an egg, and then successfully harvest it, which is not a simple procedure.

TL;DR Since cells in the body start to die within minutes of no oxygen, and the timing would have to be exactly correct to collect eggs with no prior preparation, eggs and sperm would not be viable after death.",null,0,cditqux,1r1r5j,askscience,top_week,3
fladam,"I believe that 'funny feeling' you are referring to is actually the effect of inertia. 
Things which are moving in a certain trajectory tend to 'want' to continue moving in that same trajectory, or things which remain at rest tend to 'want' to remain at rest.. 

Let me explain, imagine being on a roller coaster which is travelling along a straight horizontal path, and then suddenly the path shoots up at some angle, say 45 degrees for arguments sake, then inertia is that force which pushes you down into your seat. In the same way, if you were to feel a sudden drop on the roller coaster, you feel yourself coming out of your seat and you get that butterfly feeling. So if you were at rest in zero-gravity and you were to propel yourself downward then your body will feel a force in the direction opposite to the direction you propel yourself due to inertia which is independant to gravity in this case. This is because the force will be provided by your own propulsion. ",null,0,cdiv1w5,1r1qqi,askscience,top_week,2
ignotos,"The predicted trajectory already takes the sun into account - it's already feeling the gravitational effect of the sun - getting closer to it doesn't really change this.

Also, the mass of the comet doesn't make much difference - it is still inconsequentially small/low-mass compared to the object it is orbiting, and often the mass of the orbiting object isn't even considered in cases like these.",null,0,cdjrz4b,1r1q27,askscience,top_week,1
Jobediah,"Problems with a whale farm range from being extremely impractical, to being biologically unrealistic, to not knowing what these animals need to survive to being uneconomical.

Impractical- You'd need to cordon off a huge area that included all the resources whales need.

Unrealistic- Many whales (most? all?) range widely, some thousands of miles from their wintering and birthing grounds to their feeding grounds.

Mysterious- We don't know all the things whales do or need and this would make it likely the experiment failed.

Uneconomical- yeah, this would fail because whales generally take a long time to mature and reproduce. Farming generally requires fast reproduction and growth to be sustainable. And see other ways this would fail above. 

These are all general problems. There might be individual species for which some of these issues could be avoided but I high doubt there are any for which all of these problems could be solved.",null,0,cdirg85,1r1o8p,askscience,top_week,9
atomfullerene,"Arthur C. Clarke one wrote a story about herding whales [The Deep Range](http://books.google.com/books?id=-fYpAAAAQBAJ&amp;lpg=PT152&amp;ots=xV-sRgT5v6&amp;dq=arthur%20c%20clarke%20whale%20herding&amp;pg=PT152#v=onepage&amp;q=arthur%20c%20clarke%20whale%20herding&amp;f=false)

It's not likely it will ever be done, but if you were going to domesticate whales you'd pretty much have to herd them rather than try and pen them up somewhere.  Guide pods of females around the antarctic ocean between zones of rich plankton, protect them from predators, harvest most of the offspring every year.  It would still be quite impractical, though.  The expense of just keeping ships out there to keep an eye on them....

",null,0,cdj31w8,1r1o8p,askscience,top_week,2
null,null,moderator,1,cdioent,1r1o8p,askscience,top_week,2
marley88,"There are plans for this. So yes, it is theoretically possible.
[Here.](http://www.theguardian.com/world/2002/jan/14/highereducation.research)",null,2,cdiqrkh,1r1o8p,askscience,top_week,2
desig_nate,"There's actually very little contribution of current oil prices to current gas prices, it's about a four-to-eight week lag, depending on who you ask. Much of the market pricing is dominated by futures prices (i.e. price speculation) on various commodity exchanges due to hedging or pure speculation.

[This](http://money.howstuffworks.com/oil-speculation-raise-gas-price.htm) article is a good primer on speculation. [This one](http://useconomy.about.com/od/supply/p/oil_gas_prices.htm) talks a little more about the supply-and-demand aspect of it.

Unfortunately it's hard to point to one or two direct determinants of gas prices. If you look nationwide, much of it is determined by access to oil and gas pipelines (check out [this map](http://www.eia.gov/pub/oil_gas/natural_gas/analysis_publications/ngpipeline/images/ngpipelines_map.jpg)). In local markets, pricing strategies are more dominant, since everyone has roughly the same access to pipelines.

Hope that helps somewhat.

EDIT: Wording.",null,0,cdj1zck,1r1n6h,askscience,top_week,2
SqueakyGate,"Yes, humans have always been omnivores. 

The hominin line and the pan line diverged about 6 million years ago from a common ancestor. The hominin line includes humans and all of our extinct relatives, like the neanderthals. The pan line includes the living chimpanzee and bonobo as well as all of their extinct relatives. This common ancestor was not a chimpanzee/bonobo and it was not a human. It was its own ape species. Based on some commonalities in both human and chimpanzee/bonobo diets (as well as inferences from other ape diets) we can surmize that this last common ancestor was also an omnivore. Chimpanzees and bonobos hunt for raw meat and it makes up a small portion of their diet. The rest of their diet consists of raw plant material, like fruits, leaves, roots etc.

There are a few major dietary milestones in the hominin lineage. 

1. The very early hominins, like [ardi](http://en.wikipedia.org/wiki/Ardi) are not associated with stone tools and they are not associated with fire. From these two observations we suppose that Ardi's diet was a lot like a modern chimps. Mostly plants and some scavenged meat (birds eggs, recently dead animals etc.).

2. Around 2.6 million years ago we see the first evidence of [stone tool use](http://en.wikipedia.org/wiki/Stone_tool) this may not have changed the way [australopithecines](http://en.wikipedia.org/wiki/Australopithecine) hunted for meat, they were probably still mostly scavengers. Australopithecine species lived from about 2-4 million years ago. 

3. With [H. erectus](http://en.wikipedia.org/wiki/Homo_erectus) we begin to see the first evidence of the control of fire. First we see changes in the morphology and dentition of H. erectus which some authors (listed below) link to a change in the diet. Cooked food allows an individual to have a higher caloric gain than raw food because the cooking process essentially pre-digests the food for us. These authors among others postulate that the cooked food diet is what finally enabled brains to grow larger. Brains are an energetically expensive organ and so is digestion. A constricting caloric intake meant that the brain could not grow too big even if their were environmental or social pressures for it to do so. By increasing their caloric intake by converting to a cooked diet this released the constraints on brain size and so it began to grow. So a cooked diet explains *how* brain size grew, but not *why*. Physical evidence for fire in the forth of hearths etc. dates back about 750,000 - 300,000 years. Thus ancestral Homo species were cooking food for a lot longer than humans have ever been around. However, we still don't know how pervasive this cooked food diet was, did all species living at this time know how to control fire? did all populations within the species? In any case some of them did - so their diet changed from raw to cooked, but not in composition. It was still mostly plants (fruits, nuts, tubers...) and some meat. They also had refined hunting techniques as tool cultures also became more complex. However, they were probably hunting small game (e.g. rabbits) rather than large game. I'll get into why this is probably the case if you like.

4. Humans arrive on the scene about 200,000 years ago. At this point Neanderthals are living in Europe and H. erectus has managed to spread herself all the way to the far corners of Asia (China, Java, India...). These early humans had the same tool cultures as these other species and they very likely also new how to control fire. Humans migrated out of Africa about 100,000 years ago and then things really began to get interesting. Tool cultures improved, humans kept expanding into new environments...we were innovators. We exploited and adapted. We became hunters. Our tools began to become so sophisticated that we could hunt from a distance (e.g. throwing a spear). In contrast it appears as if Neanderthal tool technology was stagnant at the time, and their spears could not be thrown - they were close ""combat"" spears. This is important because a lot of the neanderthal bones we have are broken, they have fractures etc. Humans on the other hand have less of these and this is attributed to the idea that we could hunt at a distance lowering the risk of injury. That being said big game still probably didn't make up a large portion of our diet. Modern hunter-gather societies have diets that consist mostly of plants, and some meat which is typically communally shared. This meat is more often than not small game. The success of small game hunting is on the order of every 1-3 days, whereas large game hunting is more on the order of once a month. You cannot reliably feed a population meat with large game using these early hunting techniques. So those depictions of early humans or neanderthals taking down a mammoth or a lion are very very unlikely. Try a fuzzy little rabbit, or collecting eggs from a nest.

5. 10-12,000 years ago humans begin domesticating plants and animals. Almost everything you find in the grocery today is a domesticated plant or animal, many of which date back to these first domestication events. Peppers, cauliflower, broccoli, corn, potato, carrot, wheat, barley, oats, rice, quinoa, lemons, grapefruit, oranges, apples, pears, olives, plums, millet, spelt, sunflowers, beans... all domesticated plants. Some of whose wild counterparts you would not even recognize - [teosinte](http://nrm101-summer2010.community.uaf.edu/files/2010/07/corn-and-teosinte_h1.jpg) or [wild potato](http://www.ars-grin.gov/nr6/bapotwso.jpg). So I often find those who argue that certain plants are not ""good"" for humans to eat because they have only recently been introduced to the diet (~10,000 years ago) often don't understand that the vast majority of our diet consists of plants and animals that never existed in their current form over 10,000 years ago. So humans around this time still had diets that consisted mostly of plants and some meat. However as populations began growing their own food and housing their own animals the types of things we ate changed. Moreover some populations began to acquire unique adaptations. For example, [lactase pesistence](http://en.wikipedia.org/wiki/Lactase_persistence) is a gene mutation which occurred about 10,000 years ago in certain populations. It allows an individual to continue to digest milk *naturally* into adulthood by keeping the production of the enzyme lactase turned on. Lactase digests lactose, the sugar which many people are intolerant too. Lactase persistence is more common in cultures/populations where dairying is also common. 

6. But what does this mean for our modern diets? Well as omnivores we can eat pretty much whatever we want (baring plant products which have a lot of cellulose, a sugar we cannot digest) without too much trouble. As long as we get all the nutrients we need we are fine. Today, many people live in places were access to food is easy. Moreover you can access a wide variety of foods from around the world that would otherwise be inaccessible. **We have the unique opportunity and privilege of living in a society and time where we can tailor our individual diets to reflect our preferences, our intolerances, our allergies and our moral or ethical obligations.** For example, being a vegan or vegetarian is not impossible but rather very much attainable in today's society because we have access to many different food resources which can make up the difference in terms of nutrients and caloric intake.",null,1,cdiu8do,1r1n2n,askscience,top_week,4
NAG3LT,"In part because it reflects light in the same way as white paper does - it scatters it. This scattering means that the reflected light is distributed over a larger area and thus is less intense. You usually don't feel blinded by a white matte paper in the sunlight either. 


Another important thing to mention is the fact that not every surface reflects all the light, but only some smaller amount of light gets reflect. Your mirror may reflect over 90% of light falling on it, but if it gets dirty the refelection will be dim. On average, Moon reflects just 1/10 of the light falling on it. However, Moon's surface has retro reflective properties, so its brightness at full Moon is more than twice the brightness at half Moon. Still, it never becomes a perfect reflector and the scattering of reflected light still lowers the intensity a lot. ",null,0,cdioqh4,1r1mf7,askscience,top_week,5
Madau,"Mutations involving albinism are related to pigmentation.  Any organism that produces a pigment can have a mutation in some part of the pathway of deployment, production, or structure that causes some form of ""albinism"" although I'm not saying all organisms can become white.

Fruit flies are a very well understood organism in genetics.  There are many variations of fruit flies that have different colors due to albino-like mutations.",null,0,cdivk5t,1r1m8u,askscience,top_week,2
ModernTarantula,First hydrogen peroxide is [not a good](http://www.webmd.com/a-to-z-guides/wound-care-10/slideshow-wound-care-dos-and-donts) for open wounds. Then is it good as a surface antiseptic? Probably [ok](http://apb.tbzmed.ac.ir/Portals/0/Archive/Vol2No1/8.Ghotaslou.pdf). Then does catalase help the bugs. Looks like it is [so.](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC301784/) not all aerobes make catalase,null,0,cdji36m,1r1lh6,askscience,top_week,3
Javi2639,"The idea is that these bacteria would break down the hydrogen peroxide with catalase into water and oxygen gas, which bubbles to the surface of the wound. These gas bubbles would also bring most of the bacteria to the surface, which then be washes away. The immune system can then take care of the smaller amount of bacteria left. There's actually a debate about whether or not this is effective, but that's the theory. ",null,0,cdj89vu,1r1lh6,askscience,top_week,1
lazlokovax,"When you say or hear a word, certain neural pathways related to that word's meaning fire in your brain. There is a self-inhibitory aspect to this activity: i.e. when they fire repeatedly, the firing becomes less intense with each repetition. You could think of it as the part of your brain that understands that word being overloaded, and so the volume is temporarily turned down.

It's called [semantic satiation](http://en.wikipedia.org/wiki/Semantic_satiation).",null,6,cdipge9,1r1lgt,askscience,top_week,28
syvelior,"It's called semantic satiation, and neuroimaging results suggest that it has to do with how people process semantics rather than sensory satiation / adaptation (Kounios, Kotz, &amp; Holcomb, 2000).

**References**

Kounios, J., Kotz, S. A., &amp; Holcomb, P. J. (2000). On the locus of the semantic satiation effect: Evidence from event-related brain potentials. *Memory &amp; Cognition*, 28(8), 1366-1377.",null,4,cdirb65,1r1lgt,askscience,top_week,13
albasri,"This question has been asked several times before. Here are a few of the previous posts:

http://www.reddit.com/r/askscience/comments/mwrzz/why_is_it_that_when_you_repeat_a_word_over_and/
http://www.reddit.com/r/askscience/comments/1i0x5z/can_the_effect_of_semantic_satiation_occur_in/
http://www.reddit.com/r/askscience/comments/scz2a/why_do_words_written_or_spoken_in_the_language/
http://www.reddit.com/r/askscience/comments/m9pji/why_do_words_become_meaningless_when_you_say_them/

Unfortunately, because of the high volume, a lot of really interesting questions and great answers can get buried. The searchbar can be a great way to explore /r/askscience, although it can be a bit difficult to get just the right question. I found the above links by searching for ""repeat word"" and ""say word again""

",null,1,cdizmb4,1r1lgt,askscience,top_week,7
lazlokovax,"When you say or hear a word, certain neural pathways related to that word's meaning fire in your brain. There is a self-inhibitory aspect to this activity: i.e. when they fire repeatedly, the firing becomes less intense with each repetition. You could think of it as the part of your brain that understands that word being overloaded, and so the volume is temporarily turned down.

It's called [semantic satiation](http://en.wikipedia.org/wiki/Semantic_satiation).",null,6,cdipge9,1r1lgt,askscience,top_week,28
syvelior,"It's called semantic satiation, and neuroimaging results suggest that it has to do with how people process semantics rather than sensory satiation / adaptation (Kounios, Kotz, &amp; Holcomb, 2000).

**References**

Kounios, J., Kotz, S. A., &amp; Holcomb, P. J. (2000). On the locus of the semantic satiation effect: Evidence from event-related brain potentials. *Memory &amp; Cognition*, 28(8), 1366-1377.",null,4,cdirb65,1r1lgt,askscience,top_week,13
albasri,"This question has been asked several times before. Here are a few of the previous posts:

http://www.reddit.com/r/askscience/comments/mwrzz/why_is_it_that_when_you_repeat_a_word_over_and/
http://www.reddit.com/r/askscience/comments/1i0x5z/can_the_effect_of_semantic_satiation_occur_in/
http://www.reddit.com/r/askscience/comments/scz2a/why_do_words_written_or_spoken_in_the_language/
http://www.reddit.com/r/askscience/comments/m9pji/why_do_words_become_meaningless_when_you_say_them/

Unfortunately, because of the high volume, a lot of really interesting questions and great answers can get buried. The searchbar can be a great way to explore /r/askscience, although it can be a bit difficult to get just the right question. I found the above links by searching for ""repeat word"" and ""say word again""

",null,1,cdizmb4,1r1lgt,askscience,top_week,7
arble,"Infinitesimal means smaller than any real value and yet nonzero. Saying that the difference between two values is infinitesimal is therefore meaningless because there is no real quantity that this difference could correspond to. For entropy change to be zero in a reversible process, the starting and ending temperatures must be the same. If they're the same (not infinitesimally close but actually the same), you can't extract any work from the engine no matter how long you run it.",null,2,cdirhbw,1r1kuj,askscience,top_week,6
Guanglais_disciple,"You mean the temperature difference between reservoir and heat exchanger is approaching zero, I'm sure (subtle but important distinction). Usually, a power cycle operating at Carnot efficiency does zero work, although I'm struggling to justify this. Maybe someone else can expand on my answer. 
",null,1,cdiuh75,1r1kuj,askscience,top_week,3
rocketgolfer,"The point of the Carnot cycle is not that there is no entropy increase, but that there is no wasted entropy increase. In the Carnot cycle, all energy involved in the temperature change from hot to cold is converted to work. In fact, peak efficiency is possible only with an infinitely large difference in temperature between the hot and cold resevoirs. If you go through a truly isentropic process, no net work will be done.",null,1,cdixb1v,1r1kuj,askscience,top_week,3
xxx_yyy,"&gt; ... the temperature differential between the hot and cold reservoir is infinitesimal ...

This not an accurate description of a Carnot cycle.  There is a finite temperature difference between the hot and cold reservoirs.  The working fluid cycles between the two temperatures in such a way that there is never any heat flow between finite temperature differences.

In an ideal Carnot cycle, the total entropy does not increase - it remains constant.  That's why Carnot cycles are reversible (can be operated in rather direction around the heat cycle).  One way gives you a heat engine, and the other gives you a refrigerator.

One needs to have a finite T(hot) - T(cold).  The efficiency of a Carnot heat engine increases as this temperature difference increases: 

Efficiency = [T(hot) - T(cold)] / T(hot)",null,0,cdjdm7b,1r1kuj,askscience,top_week,2
varodrig,"Not really, not in any significant way.   
Heat is the lowest/simplest form of energy (that we know of). This means that all forms of energy eventually degrade into heat. Let's take an example of two electric heaters. 'A' is just a heater. 'B' is a heater that also has a fan, a few LEDs and a speaker that reads out the time every hour.  
For both A and B, the electricity used by the machine's coils will all be converted to heat by the resistance. But the interesting thing about B is that even the air movement (fan), light (LEDs) and sound waves (speaker) will all degrade into heat as well.  
The only way A will be more efficient than B is via the MINISCULE energy lost by the sound waves and vibrations leaving your sealed room through the wall. ",null,0,cdj06vt,1r1jp1,askscience,top_week,2
DevinTheGrand,"The menstrual cycle is a primate only means of female reproduction.  The reason why female primates have a period is because they shed the lining of the uterus when pregnancy does not occur.  Most other mammals use an estrus cycle, which results in the re-absorption of the uterine lining by the uterus.  (As an aside, rats can actually re-absorb miscarried young through their uterus as well, which is pretty awesome)

Animals that use estrus also are usually only sexually active during their period of ovulation.  This is called ""being in heat"" which I'm sure you've heard before, even if you didn't know the context.  Occasionally bloody vaginal discharge will occur, but it is not necessary, and it does not contain uterine lining.",null,2,cditwrk,1r1jah,askscience,top_week,11
MarineLife42,"Apart from /u/DevinTheGrand's excellent explanation, please note that what we call ""period"" in women is only a very recent phenomenon. Up to about 100 years ago, a woman of fertile age would have maybe four or five menstrual discharges throughout her life; the rest of the time she would have been either pregnant or lactating. My own great-grandmother had fourteen children of which ten survived into adulthood - totally normal then.  
Consider that at that time, girls/women began puberty much later than now, and you can see that there was hardly time for her to get anything approaching a 'period' going.  
In fact, menstrual bleeding is a kind of emergency backup program that the body runs when no egg fertilization occurs, which explains why it causes so much discomfort for many women. As far as the body is concerned, an egg going unfertilized is not supposed to happen.  
With mammals not under human supervision, including marine mammals, it is like the olden days with humans - as soon as a female is in estrus, sex will occur and she will end up pregnant. In the wild his is the norm, not being fertilized is the exception. ",null,8,cdj7il5,1r1jah,askscience,top_week,0
trebuday,"This is still a topic of hot debate in climatology.  There are several ideas regarding the cause of the Mid-Pleistocene Transition (MPT), and a quick google search brought up these two papers:

[Clark, et.al., 2006](http://geosci.uchicago.edu/~archer/reprints/clark.2006.MPT.pdf)

[Martinez-Garcia, et. al., 2006](http://adsabs.harvard.edu/abs/2006AGUFMPP21B1688M)

Both of these papers attempt to address inconsistencies in various theories regarding the MPT, and both agree that the cause of the MPT was probably a very complex set of interactions between many or all of the various climate change processes (silicate weathering, deglaciation, changes in insolation, changes in ocean circulation, etc.). 
",null,0,cdj2kl9,1r1j7m,askscience,top_week,1
chaseoc,"A thin sheet of aluminum foil would offer very little protection against cosmic rays unless it were extremely magnetized. Cosmic rays are extremely high energy particles emitted from black holes and stars... they travel very very close to the speed of light. Some carry as much energy in one tiny little atom as a thrown tennis ball. Because they are so small, there is a very small chance they will actually impact any atoms in that sheet of aluminum unless it were very very thick... and even if they do impact, they usually create a shower of particles that will continue on if the material is not thick enough from the point of collision to absorb them.

The same is true for normal radiation. You need thick lead containers to fully absorb radioactivity.... and normal radiation is not nearly as powerful as a cosmic ray. On earth it would be easy to protect, but in space wrapping a spacecraft with lead plating is not really feasible. Magnetic fields are an option, but to be effective with such high energy particles it takes a lot of energy... and to generate a lot of energy usually involves a very weighty reactor. 

http://en.wikipedia.org/wiki/Cosmic_ray",null,0,cdizta3,1r1j7j,askscience,top_week,5
endocytosis,"[Chlorophyll](http://en.wikipedia.org/wiki/Chlorophyll), what plants use to make energy, absorbs most light in the UV or near-UV range of light, although they can absorb some light in the yellow-red range (they reflect green, so plants have greenish leaves).  An incandescent lamp won't provide enough light in this range, and usually most other light sources won't either.  Artificial sun-lamps basically just provide shorter wavelength, ""blue-purple"" color, or possibly UV, although that is a bit more dangerous to your eyes, so the plants can use that for photosynthesis.  Sunlight has all of visible light, in addition to other wavelengths.",null,0,cditxys,1r1icj,askscience,top_week,2
LoyalSol,"Yup,  even in mixtures like corn starch and water you can still sink if you stay still long enough and of course you can't breath it. ",null,0,cditifq,1r1i97,askscience,top_week,3
steeeeve,"Non-newtonian fluids can actually be more dangerous than newtonian ones, because as a person attempts to swim or tread water, the fluid increases in viscosity around their limbs. 

[Quicksand](http://en.wikipedia.org/wiki/Quicksand#Properties) is an example of a non-newtonian fluid, and though its dangers have been exaggerated in pop culture, it's certainly still possible to drown in it.",null,0,cdj0w3k,1r1i97,askscience,top_week,3
mrmayo26,"If I remember correctly, the trauma to the head causes a great deal of force and potential injury to the brain. Therefore in order to protect itself (although I'm unclear how this protects it or helps it exactly) it stops doing non-essential functions and reduces its activity. Although I'm sure someone else knows more about this subject than myself

",null,0,cdjaena,1r1hma,askscience,top_week,2
ericgdc,"There is a torsional force that takes place when you receive a concussive blow. This torsion 'stretches' various midline structures within the brainstem, areas that control breathing, alertness, etc. (think of wringing a wet towel, the center structures being 'twisted' the most). This is thought to play a role in the sudden loss of consciousness that takes place with certain blows to the head.



",null,0,cdjankh,1r1hma,askscience,top_week,2
GProteins,"Short answer: No.

Your tears are more than just water. This is unfortunate for your no-blink plan, but fortunate for your eyes! They contain things like lysozymes, which help prevent eye infections by killing bacteria. It also contains various lubricating oils and some salts, which may just be left over from being drawn from blood plasma.

But either way, if you lived in an environment humid enough not to blink, it's humid enough to have a LOT of bacteria around that you don't want in your various mucous membranes.",null,0,cdn9i8i,1r1fvd,askscience,top_week,1
DanielSank,"A relative of mine does this stuff for a living so I can tell you what I've learned by asking the same question. Note that this is a combination of *what I've heard* and what makes sense.

Issues that you have to worry about when sending electronics to space:

* Radiation. Without the atmosphere as a shield there is more radiation in space. When a circuit gets hit by this radiation a variety of things can go wrong. A digital memory element may change state. This could lead to errors in CPU or FPGA controller chips. This is particularly a problem if the physical features are really small, so you have to get ""old"" parts that don't use current lithography scales. Analog parts may have problems too but I'm not sure. Parts that are qualified to not have errors when exposed to space levels of radiation are called ""radiation hard"" or ""rad hard"".
* Vibration. During launch the payload is subjected to vibrations. Why is this a problem? I think it's because the connections to the chip can break. Microchips are usually fabricated on a substrate of silicon, and the contact pads for wires going in and out are pretty small in cases where many wires are needed. You can't just solder to that. A common way to connect to the pads is with [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding), essentially very thin threads of metal that are sort of mashed onto the contact pads.
* Packaging. Consumer electronics are packaged in plastic. If you look inside your keyboard you'll see a variety of black square or rectangular things with metal pins sticking out. The black is plastic and the silicon die is inside. Plastic absorbs water from the air. When you send that into space it gets cold and I think the problem is that the water freezes, expands, and cracks the package. This would probably rip off some of the wire bonds. Now, having said that a guy in my lab put a consumer FPGA into a 4 Kelvin cryostat and it worked so I could be completely wrong about this.

As for the cost, there are two factors. The first is that making something with the right packaging and robustness against radiation means you aren't using your standard fabrication line. That means the price is higher just because of supply/demand. That aside, an enormous cost is *testing*. When NASA builds a satellite they require not only that the parts in principle deal with the things listed above, but also that they are actually tested by

* Vibrating them
* Operating them under irradiation
* Thermal cycling
* ...and more.

That means you need extra test equipment, people, and time. when you buy a mouse for your computer that's just a part off of a fab line that makes a million of 'em a day. Maybe a small number of parts from each batch are tested for quality control. When you build a satellite, you test every single part.

EDIT: I think a lot of contemporary parts use degenerately doped silicon, so I guess carrier freeze-out isn't a problem. Does anyone know if this was a problem ~30 years ago?",null,0,cdilxiv,1r1bka,askscience,top_week,11
DanielSank,"A relative of mine does this stuff for a living so I can tell you what I've learned by asking the same question. Note that this is a combination of *what I've heard* and what makes sense.

Issues that you have to worry about when sending electronics to space:

* Radiation. Without the atmosphere as a shield there is more radiation in space. When a circuit gets hit by this radiation a variety of things can go wrong. A digital memory element may change state. This could lead to errors in CPU or FPGA controller chips. This is particularly a problem if the physical features are really small, so you have to get ""old"" parts that don't use current lithography scales. Analog parts may have problems too but I'm not sure. Parts that are qualified to not have errors when exposed to space levels of radiation are called ""radiation hard"" or ""rad hard"".
* Vibration. During launch the payload is subjected to vibrations. Why is this a problem? I think it's because the connections to the chip can break. Microchips are usually fabricated on a substrate of silicon, and the contact pads for wires going in and out are pretty small in cases where many wires are needed. You can't just solder to that. A common way to connect to the pads is with [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding), essentially very thin threads of metal that are sort of mashed onto the contact pads.
* Packaging. Consumer electronics are packaged in plastic. If you look inside your keyboard you'll see a variety of black square or rectangular things with metal pins sticking out. The black is plastic and the silicon die is inside. Plastic absorbs water from the air. When you send that into space it gets cold and I think the problem is that the water freezes, expands, and cracks the package. This would probably rip off some of the wire bonds. Now, having said that a guy in my lab put a consumer FPGA into a 4 Kelvin cryostat and it worked so I could be completely wrong about this.

As for the cost, there are two factors. The first is that making something with the right packaging and robustness against radiation means you aren't using your standard fabrication line. That means the price is higher just because of supply/demand. That aside, an enormous cost is *testing*. When NASA builds a satellite they require not only that the parts in principle deal with the things listed above, but also that they are actually tested by

* Vibrating them
* Operating them under irradiation
* Thermal cycling
* ...and more.

That means you need extra test equipment, people, and time. when you buy a mouse for your computer that's just a part off of a fab line that makes a million of 'em a day. Maybe a small number of parts from each batch are tested for quality control. When you build a satellite, you test every single part.

EDIT: I think a lot of contemporary parts use degenerately doped silicon, so I guess carrier freeze-out isn't a problem. Does anyone know if this was a problem ~30 years ago?",null,0,cdilxiv,1r1bka,askscience,top_week,11
humanino,"We first should acknowledge that we can only speculate about answers to those question. The final stages of black-hole evaporation are not well approximated by quantum field theory in curved spacetime, one would have to use a full non-perturbative theory of quantum gravity. 

Although very slow at first, the process eventually appears explosive. Because of this, (in my understanding) most people believe nothing is left behind. That is still explicit in Hawking's latest papers for instance. The total lifetime, estimated from the power emitted by Hawking's radiation (a formula which may not apply at late stages, but this stage should be short), varies like the cube of the mass of the black hole. For a black hole of one solar mass (10^30 kg), the lifetime would be 10^67 years. One should pause to contemplate such a number. The lifetime of the universe is a mere 13.7 billion years in comparison. Most black holes are (much) more massive than the Sun.

That is the reason people hoped microscopic black holes would have formed in the early universe, whose explosion we could possibly detect today. ",null,0,cdikbx3,1r17mc,askscience,top_week,4
Quantumfizzix,"All that /u/humanino  said is true, so keep that in mind.

There is no such thing as ""not enough mass"" for an event horizon to form. All that's required for a black hole is density. Obviously, it needs SOME mass, but that amount of mass can be as small as necessary so long that the volume is proportionally small. Once something becomes a black hole, it stays that way until completely evaporated. There is no remnant.

",null,2,cdiknps,1r17mc,askscience,top_week,4
5k3k73k,"There are at least two hypotheses:

Hawking radiation increases the smaller the black hole becomes. So much radiation is lost in the last few moments that it looks like the black hole explodes.

There is a threshold at which a black hole becomes too small to interact with other particles, halting Hawking radiation, and it becomes a stable WIMP.",null,0,cdj40qf,1r17mc,askscience,top_week,2
MayContainNugat,The C-14 scale is corrected for variations in atmospheric C-14 by calibrating with tree rings of definite known ages. ,null,0,cdipwgt,1r17l3,askscience,top_week,5
rupert1920,"Molecular weight doesn't quite matter - the filter doesn't operate base on [size exclusion](http://en.wikipedia.org/wiki/Size-exclusion_chromatography).

Your Brita filter removes most ions by [ion-exchange](http://en.wikipedia.org/wiki/Ion_exchange_resin) - these are stationary phases with pre-existing slots for ions to bind to. The ""exchange"" occurs when your impurity replaces the existing ions in binding to the stationary phase.

The [activated charcoal](http://en.wikipedia.org/wiki/Activated_charcoal) (the black powder that sometimes leaks out) is good for removing a large range of other impurities, and they work by non-specific [adsorption](http://en.wikipedia.org/wiki/Adsorption). In that sense, they _are_ able to remove dissolved organic compounds. However, if you're talking about something like dissolved sugars, you're often looking at concentrations way beyond trace amounts.",null,2,cdij2gr,1r16yk,askscience,top_week,11
myarlak,"yes, if it is a water reactive substance like calcium carbide water will react exothermically and ignite the gases produced",null,0,cdij0ig,1r15t3,askscience,top_week,11
FatSquirrels,"Besides a chemical reaction with water itself, anything that has an [autoignition temperature](http://en.wikipedia.org/wiki/Autoignition_temperature) below 100 C could theoretically be ignited by heating it with boiling water.  However, if the material did not react with water and you poured the water on top of it you might simply smother any flames that would be normally produced.",null,0,cdix6p7,1r15t3,askscience,top_week,3
null,null,null,6,cdij52z,1r15t3,askscience,top_week,1
Das_Mime,"Sound is a compression wave, it travels by massive (which in this context just means ""having mass"") particles like molecules bumping into each other and transmitting that bump forward. Imagine being in a dense crowd, and then a group of people charge into one edge of the crowd. Their momentum will be tranferred to the next person, and the next, and so on, propagating a wave outward through the crowd.

Radio waves are electromagnetic radiation, which occurs when you have an accelerating charge. The electric and magnetic fields (which can exist in empty space) keep oscillating and propagating themselves through space. They don't require a medium (other than spacetime and electromagnetic fields) because they aren't transmitted via massive particles.",null,2,cdij03b,1r15bm,askscience,top_week,36
BoxAMu,"This had 19th century physicists stumped too!  It used to be believed that a medium for EM wave propagation called the luminiferous ether pervaded all space.  However, this should change the properties of EM interactions (namely wave speed) for observers moving relative to the ether, just as motion with respect to still air changes the effective properties of sound.

Long story short, this change of wave properties is not seen.  Light always travels at c = 3 x 10^8 m/s in free space regardless of the relative motion of observer and source.  Moreover, based on the theory of relativity it is meaningless to even try to define some absolute space in which the ether could be located.  No medium for the propagation of EM waves exists.

So what IS an EM wave?  The question is hard to pin down.  The laws of physics correctly explain all observed EM phenomena, so it seems that this question is not directly relevant to experimental science.  But indeed, the idea of a wave without a medium is hard to visualize.",null,1,cdijd5s,1r15bm,askscience,top_week,15
Qazerowl,"I kind of get the feeling you don't know what radio waves are (if you do, ignore this).

Radio waves aren't actually sound, they're actually a type of light. Radio antennas can detect radio waves, and are programmed to make noise based on what signals they get. So, sending radio waves through space is more like Morse code with lasers than yelling into a megaphone.",null,4,cdikn7r,1r15bm,askscience,top_week,9
50bmg,"As i understand it, EM radiation is a wave created through *self propogating* electric and magnetic fields. That means there is no medium necessary for the wave to move through it - just the capability for empty space to have a quantifiable magnetic/electric field (which can be zero at any point, so it's not exactly a medium like water or air). The quantized form of this wave is a particle (the photon). Sometimes it is easier to picture radiation as photons traveling through space. ",null,0,cditcon,1r15bm,askscience,top_week,3
Roar_of_Time,"Sound can be recorded into a microphone, where it is converted into electrical energy and stored in a computer hard drive. The electrical energy can then be converted into a radio wave using an antenna. Radio waves are really small particles/waves called photons, and are a form of electromagnetic radiation. Visible light is exactly the same thing, just at a smaller wavelength, as are X-Rays, Gamma Rays, Ultraviolet Rays, etc. Photons require no atomic medium to travel, so they can travel through space. They travel at the ""Speed of Light"" through space. Photons can be picked up by other antennas and converted into electrical energy again, and then be converted into mechanical energy by a vibrating magnet, or a speaker. This mechanical energy is transferred through the air by the atoms in the air colliding with each other in a wave pattern. I suppose you could see them as a sort of atom sized photon, some people call them phonons. The colliding atom chain reaction enters your ear canal, where it is converted back into electrical energy and processed by your brain, which you perceive as sound. By changing the electrical energy that vibrates the magnet, the vibration can be faster or slower, making different sounds by changing the force applied to the atoms. 

Here's a nifty chart for the electromagnetic radiation spectrum if you're interested: 

http://mynasadata.larc.nasa.gov/images/EM_Spectrum3-new.jpg

And here's a rather cool video I found that shows that sound waves are simply moving atomic matter.

http://www.youtube.com/watch?v=wvJAgrUBF4w",null,0,cdinc4m,1r15bm,askscience,top_week,1
CoolStoryJohn,"A very simplistic answer is that sound is essentially a sequence of vibrations that travel through a region (molecules in the environment being the vehicles for vibration, i.e. molecules in the environment are responsible for the transportation of the sound) whereas a radio wave can be viewed as its own transport vehicle (""self-sufficient"").  Space has no atmosphere (and thus no molecules), so sound cannot travel.  However, as mentioned, radio waves aren't dependent on their environment in that way, so they are able to travel in space.  ",null,1,cdiy9vv,1r15bm,askscience,top_week,2
GlorifiedTapeOp,"Sound is a compression wave,  which means that energy is transferred between particles.  If there's no medium in space for the sound compression wave to transfer it's energy through, then the sound becomes void. 

Radio waves are essentially light (electromagnetic radiation) which don't require a medium like particles with mass for sound. 

Edit: Das_Mime answered this question better",null,0,cdiyezb,1r15bm,askscience,top_week,1
RationalAnimal,"
This is a good question.   And the posters above are not wrong, as far as their answers go, but there is a sense in which the original question is not answered. 

As others have pointed out, contemporary physics has it that light exhibits a wave/particle duality.  

The sense in which light is a wave is, in modern physics, only a metaphorical sense. In the literal sense, a wave is a higher-order property of some physical substrate.   Waves in water are the more voluminous sections of water in a pattern of more (wave) and less (trough) voluminous sections of water.   Waves of sound are patterns of sections of air with greater and then lesser air pressure.   Light ""waves"", however, are not now believed to be real waves in any other physical substrate.  

For a very long time, it has been known that light exhibits behaviors that waves exhibit.  Light diffracts through thin slits, for example, into fan-shaped patterns of different colors of light.  Light exhibits interference behaviors, like water waves do.  This evidence led scientists to believe, for a very long time, that light, too, was a wave.  So, they reasoned, if light were a wave there must be some physical substrate of which it is a wave.   Aether was posited as the substance patterned variations in which were light waves. 

It turns out, however, that there is no other evidence for aether other than the fact that light seems wave-like.   There is no detectable drag on moving objects in the vacuum of space.  There are no relative-motion effects exhibited by light relative to the direction of earth's motion around the sun, as one would expect if earth were traveling with respect to aether.  And so on.  

So, the notion that there is aether was dropped, but we were still left with the awkward exhibition of wave-like behavior of light.   That's pretty much where we are now, as far as a comprehensible physical model of light is concerned.  The claims that light is a ""self-propogating"" wave, or that light is the kind of wave that is not a wave in anything else, are not really meant to *explain* the puzzling sounding claim that light is a wave of nothing.  That is, these claims are not meant as ways of thinking of waves in a literal sense as a *physical model* of what light really is.  

They are just ways of saying that light exhibits wave-like behaviors sometimes, and that a representation of light as a wave permits one to predict the behaviors of real light accurately.   Even the claim sometimes made that we can think of light waves as ""probability waves"" isn't meant to offer a description of a physical model of light.  It is a shorthand way of saying that there are higher probabilities of detecting photons at certain places in beams of light and that the pattern of these probabilities has a formal resemblance to physical waves. 

So, the question of *how* light can be a wave without being a wave of something isn't really answered by modern physics.  It's better to think of modern physics as telling us that that macro-level, real waves are not a physical model of real light, but that light nevertheless exhibits wave-like properties.   What light *really* is is something, modern physics tells us, that has no analogue in our direct, macro-level experience.  But we know it travels through a vacuum.  So, there you go.  ",null,0,cdjj6kf,1r15bm,askscience,top_week,1
DanielSank,"&gt; What are the parts of the D-Wave quantum computer?

~Sigh~

What D-Wave is selling has not been proven to be a quantum computer.

&gt; what general parts are there in a quantum computer?

I have [this saved from when /u/whittlemedownz explained it a while back](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp) but I'll give me own spin.

A quantum computer has a bunch of individual information storage elements analogous to the bits in a classical computer CPU or memory. The physical incarnation of the bits in a quantum computer must be build so as to exhibit quantum mechanical behavior. As such they are called ""quantum bits"" or ""qubits"". There are many possible physical elements that can be used: atoms, electron spins, photon polarization, superconducting circuits, and more. The D-Wave machine uses superconducting circuits. Each one is a loop of superconducting wire. The current in the wire can flow clockwise or counterclockwise. Because it's quantum the qubit can also be in a state that is a superposition of both directions of current. The bits interact with one another via the magnetic dipole of each loop of current: each bit feels its neighbors' magnetic fields.

The computer also has wires that carry signals into the bits to control them in various ways. In the case of D-Wave this is by applying external magnetic flux to the loops. In fact this control circuitry is extremely difficult to get right and in my opinion is the most impressive thing about D-Wave's machine.

&gt; but what exactly am I looking at - some sort of scintillating CPU?

The thing in the center is the chip with the superconducting qubits. The rest of it is control wiring and a cryogenic mount. Look in the dead center. See the rainbow colors in the black square? That's the chip. The rainbow color happens because of diffraction off of the tiny lithographically defined features. Those sets of itty bitty parallel lines at the border of the black chip are [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding). The wire bonds connect wires on the chip to wires on the green circuit board mount. The circuit board has its own wires (the thin lines in the green board) which fan out and connect to those bundles of what look like braided copper wires (the brownish things with the white labels on them). The gold colored metal just a mounting apparatus. It's probably gold plated oxygen free copper, a commonly used material in cryogenic applications.

Source - I work in a quantum computing lab",null,0,cdina0c,1r140w,askscience,top_week,9
spirit_of_loneliness,"It's hard to explain 'quantumness' in quantum computers without refering to quantum physics, but in extremely simplified version it's about 'more than 2 states', as 'regular' computers work only on two states ('voltage' or 'no voltage', so 0 or 1) and everything is built on this. Now, what makes quantum computer 'quantum' is the fact, that it can work on those 'usual' 2 states (0 and 1) and everything BETWEEN them (let's say a current state can have 40% of '0' and 60% of '1' ), this is based on fundamental physical principle called [superposition](http://en.wikipedia.org/wiki/Quantum_superposition)  

Basically, as you probably know, ""quantum computer"" works on qubits (named this way to differentiate from regular bits, reason above), so it boils down to the question how those qubits are implemented in given machine.  
Scientists already managed to invent more than one implementation, just take a look [here](http://en.wikipedia.org/wiki/Qubit#Physical_representation)   Of course, leaving all the 'standard' electronics, quantum computer is usually built around hardware, that can interface with physical materials, for example. laser and photon detectors (when qubits are implemented with photons), in general 'something' that can read the current state of our physical implementation (yet another simplification - read how much % of a '0' or '1' is there currently)",null,7,cdineb8,1r140w,askscience,top_week,7
mutatron,"Right now it's Spring in the southern hemisphere, and it's just business as usual in the vast tropical regions. Here in Texas the leaves are only just changing, but we also have evergreens. I suspect you have evergreens in Massachusetts too. Not only that, but around 50% of the oxygen on Earth comes from plankton, and another 20% comes from algae.

There's a seasonal variation in local oxygen levels described [here](http://answers.google.com/answers/threadview/id/769958.html):

&gt; The levels of O2 are altered by the fall/winter in the northern
hemisphere, but not to a detectable level. Plants (both deciduous
(leafy) trees and many bushes and grass) do not perform photosynthesis
during the fall and winter months. This results in a cyclical
variation in Carbon Dioxide (CO2) levels. This season variation is
~5-6 ppmv: parts per million by volume. The total amount of CO2 is
approximately 380 ppmv. So the CO2 level cycles by ~1.5% annually.

&gt; O2 should change for the same reason, but the fraction of O2 to CO2 in
the atmosphere is 549:1 (by volume). Or O2 is 209,460 ppmv to CO2 ~380
ppmv. So the percentage variation of O2 is &gt; 0.002%.",null,1,cdiioni,1r13vn,askscience,top_week,10
Jeeebs,"There is a condition called Bromism (Not something from How I Met Your Mother), where someone ingests too much bromide, and it can react with things in the body causing pretty serious problems. Typically this is from bromide salts, but it's also handy to note that BVO is not a salt.

HOWEVER... Bromine atoms are great leaving groups, so in much of the harsh conditions of the body, it is very possible the bromides are leaving the fatty acid chain and causing the same havoc.",null,0,cdinsbq,1r12y1,askscience,top_week,2
MarineLife42,"Depends where in the body, and whether the heat can dissipate. There are many people now who have a brain implant that does just what you said - deliver a very low voltage of a specific point in the brain, helping with such things as Parkinson's, depression and other neurological issues. So when used correctly, it is even beneficial.  
Our entire nervous system relies on low voltages running as signals, controlling voluntary and involuntary movement of muscles. Here, a low voltage would interfere. ",null,1,cdj7oqo,1r1272,askscience,top_week,3
iorgfeflkd,We are near the centre of our observable universe.,null,0,cdii8uj,1r11s1,askscience,top_week,5
Lirkmor,"Everything is moving away from everything else (broadly, not taking into account galaxy collisions and such), so even if we were observing from another planet light years away, we would still see the same effect. This is because objects aren't moving away from each other *through* space, but rather space *itself* is expanding.",null,0,cdiin49,1r11s1,askscience,top_week,4
e_t_,"Space itself is expanding, so everything is getting further from everything else. It's not like inflating a balloon. One method of enlarging an image is to divide the original into a grid, then recreate each square but bigger. Every point on the big copy will be further away from its neighbors than on the small picture, but that doesn't give you any information about where the center is.",null,1,cdiit62,1r11s1,askscience,top_week,1
BoxAMu,"This is a general question about light, regardless of the source producing it (sun, candle, laser, etc.)  Light spreads out as it propagates, becoming weaker in intensity because the same energy is spread over larger and larger areas.  Light doesn't actually disappear unless it is absorbed by matter, in which case the energy it carries is converted into a different form (motion of particles for example).

It's definitely possible to trap light in a box or other type of resonator, just like sound can be contained in a resonant chamber (like an acoustic guitar body).  In a resonator, waves propagate around the structure continuously.  They don't decay unless they leak out of the resonator or are absorbed in the walls.  For light, this has been achieved with a variety of different structures.  Some of the best are made from micro-sized (~10^-6 - 10^-5 m) spheres in which the light is confined to circulate near the walls.  However, these can still only confine the light for a short period of time (micro or nanoseconds).  ",null,0,cdiiimb,1r1184,askscience,top_week,2
Mazetron,"First of all, wormholes are only theoretical at this point.  We have no evidence of real wormholes existing and we are no sure whether or not they could exist theoretically.

However, they are fun to think about.  [Here is a rendering of what a real wormhole might look like](http://en.m.wikipedia.org/wiki/File:Wurmloch.jpg)

Go check out the Wikipedia page, it's got some good stuff.",null,2,cdilx53,1r10y1,askscience,top_week,12
rupert1920,"There is no reason it won't be possible. You'll get a hydrogen ion - or a [hydron](http://en.wikipedia.org/wiki/Hydron_(chemistry\)), which is highly reactive.",null,0,cdiia6d,1r0y6n,askscience,top_week,9
myarlak,"you would make a proton, happens all the time
",null,0,cdij1oa,1r0y6n,askscience,top_week,8
un7ucky,"hi production horticultrlist here. 
no the exocarp (peel) of a fruit will not grow back, but it will (plant depending) attept to form a scab to protect undeveloped fruiting bodies. most mature fruit will likely rot. 

any specific plant you had in mind when you asked?",null,2,cdil5ru,1r0xqx,askscience,top_week,8
Truck43,No. The skin is part of the flower that becomes the structure on the fruit. It cannot grow back. ,null,3,cdijcjb,1r0xqx,askscience,top_week,3
BoxAMu,"First: the state of a system encompasses the whole trajectory of its particles consistent with a given energy.  One section of that trajectory moving to another part is not a change of thermodynamic state.

Second: entropy is a statistical concept.  The fundamental laws of particle interaction are time-reversible, and the second law of thermodynamics does not apply until one considers an ensemble of many systems.  Or, one could consider the interacting particles to have some internal structure.

What would happen if two macroscopic charged bodies were released and allowed to attract one another?  Eventually they would collide and bounce back, only to re-collide and bounce back again and again.  Either the system would continue to oscillate like this, or more realistically, some energy would be dissipated in each collision until the bodies stick together.  The conversion of lost energy to heat represents an increase in entropy.",null,1,cdiiymq,1r0xju,askscience,top_week,6
cephsdiablo,"""When two oppositely charged particles attract towards one another via the electromagnetic force, does the decreasing volume of the system""

If you isolate to opposite charges those charges will attract towards each other with one given equation (I'm sure you know which equation this is). This equation shows that they are interacting only with each other not the entire system.

""result in fewer microstates per given macrostate of that system (and therefore an evolution towards decreasing entropy)?""

You can't assume that two microstates coming together creating one macrostate creates a decrease in entropy. Here is an easy example. Hydrogen and Oxygen in two smaller ""microstates"" create a much larger explosive macrostate when combined. 

If we could combine an electron and proton together to the point where they could create a reaction, I'm not sure what would happen and I'm not even sure if that is physically possible. Has anyone done it before??

A decrease in volume does not necessarily dictate entropy decrease.",null,0,cdip0ww,1r0xju,askscience,top_week,1
Rocker232,"It really depends on what animal you are talking about, many adaptations have evolved. Some animals such as penguins use huddling as one technique, some frogs completely shut down their bodies and freeze. Other animals have a substance in their blood that prevents freezing. The amount of adaptations to combat against cold weather/temperatures could go on and on.",null,1,cdilly0,1r0x4h,askscience,top_week,4
sporclesam,"When its extreme, most [ectotherms](http://en.wikipedia.org/wiki/Ectotherm) go into stasis/torpor. Endotherms generally consume more nutrition &amp; conserve body heat through physical means such as those Rocker232 mentioned. Animals such as some bears will hibernate during extremely cold weather conditions. Many smaller animals such as hares can burrow in and conserve body heat in such fashion. Deer generally migrate in extreme conditions. 
You are underestimating the impact of fur, it is quite useful (along with subcutaneous fat) in such conditions. 

All these are physical means; there are numerous [bio-chemical ways](http://www.nature.com/scitable/knowledge/library/extreme-cold-hardiness-in-ectotherms-24286275) in which animals (and plants) fend themselves in the cold. ",null,0,cdiq2j6,1r0x4h,askscience,top_week,2
baloo_the_bear,"Not really. Calories are a measure of the energy stored in the food, and eating calorie-dense food does increase the sensation of satiety, but the food you eat gets broken down into sugars, fats, and amino acids and is absorbed. The food causes an initial spike in blood sugar levels, but it is quickly converted into long term stores if it isn't used pretty quickly. The body likes to maintain a relatively narrow range of parameters; staying alive requires very specific set points for things like serum glucose concentrations, pH, temperature, etc. 

The feeling of hunger has a few proposed causes, from emptying of the stomach to dips in blood sugar. We do know that there are hormones which can affect the feeling of hunger and satiety, and they are ghrelin and leptin, respectively.

Hunger is odd in the sense that a person can survive for days without eating, yet we experience hunger several times a day, and whether we need food to survive or not. 

Hormone states are constantly in flux, and insulin, glucagon, ghrelin, and leptin are some of the hormones that play roles in the feeling of hunger.",null,0,cdiio5d,1r0wnc,askscience,top_week,9
iorgfeflkd,"In that specific scenario, the source of the gravitational field would cease to be due to the rest mass of the star, but rather to the energy density of all the gamma radiation from where that star used to be. As it spread out, the gravitational field would change accordingly. This is because in general relativity, the source of gravity is not just mass but a more complex quantity called the stress-energy tensor, which is primarily dependent on energy density and pressure.",null,1,cdiiv2u,1r0tsg,askscience,top_week,4
armrha,"Have some aspirations of super villainy, eh?

Well, unfortunately, 'mind control' implies some kind of agency to it that they don't really have. They don't 'know' what they are doing, and the mechanisms they do it aren't super-complicated machines they install in the host's brain or anything like that. They're just little tricks that ended up working in their favor. It's evolution in action -- they never 'figure out' the brain of their host, they brute force it with the evolutionary algorithm.

Take the commonly dragged out example of Dicrocoelium dendriticum. It's a trematode which 'mind controls' infected ants to go hang out on top of blades of grass in order to get eaten by cattle and such. At some point, they were just like many other non-behavior affecting parasites, but eventually some kind of small change to their genome resulted in some very slight chemical imbalance inside the ant hosts, that resulted in slightly more of the fluke's ancestors reproducing than they otherwise would.

This could have been any number of things. A chemical that makes the ant work harder, giving it more time to get eaten. Maybe it secreted something that slowed the ant down, just making it an easier target. It's impossible to say exactly.

After the first change, generations go by which more success, and slowly tiny changes that result in more reproductive success start to take hold. One generation moves a little closer to the nerve center, things get more efficient. Whenever a change is not beneficial to the host, it ends up failing to reproduce (or at least not outpacing reproduction). So the flukes that had their ants hang out on grass in the blazing sun didn't make grandchildren. Selection slowly weeds out the most beneficial effects.

Viewed as a whole as they exist today, they almost seem to be aware of what they are doing, like little pilots steering an ant ship. But that's just the culmination of thousands of tiny little brain hacks they've discovered along the way, automatic parts of their life cycle that interact with the host in an interesting way.

Could such a thing happen to humans? Absolutely, but again, it'd be on an evolutionary timescale. I mean, parasites exploit what is available to them. Parasites piggyback on your biology, altering it somewhat, and anytime things are altered there's always the potential for the brain being effected. There's conjecture that Toxoplasmosis infection makes people more prone to risk taking, for example. 

There's a constant arms race between complex, cooperating groups of cells like ourselves and the smaller complex cooperating groups of cells that want to get in there and defect, taking all the free resources they can get their hands on. Ultimately, that's what the entire parasite game is about. So the answer is yes, but probably not in the way you were thinking. Especially as far as not being able to cause any specific, meaningful actions as far as we think of them as persons.",null,3,cdipevt,1r0qvp,askscience,top_week,9
wretched_beasties,"This isn't just with insects. Parasites are masters at manipulating host behavior, even in humans. For example, the guinea worm (Dracunculus medinensis) drives infected humans to soak their feet in water, facilitating the release of their eggs into a water source and continuing their life cycle.
",null,0,cdjelnm,1r0qvp,askscience,top_week,3
glarn48,"Great question! Not in my wheelhouse but I was intrigued. Any time you ask a ""why"" question about evolution, there's always some degree of speculation involved, but that doesn't mean we can't think of logical explanations regarding evolutionary pressure, and it turns out many researchers seem to have asked this very question. I'm going to summarize the findings of one paper (http://www.tandfonline.com/doi/abs/10.1080/1357650X.2013.824461#.UoxJv8Rwq6V), though they cite many other competing hypotheses.

Their theory is built around issues of mobility, given that mobility is crucial for survival and reproduction in all species, predator and prey alike. If part of the body is injured moderately or even fatigued, the brain can monitor the body via feedback loops and compensate for injuries by making dynamic adjustments to movement. However if the brain and body are both compromised by an injury, this compensatory ability is diminished. The authors posit that because a non-lethal injury that damages the head and body is likely to be unilateral, having contralateral control by the brain can preserve these compensatory effects. 

The paper does not discuss the issues surrounding sensory processing that you raise in your question though, but the explanation holds to a certain effect here too. A truly severe injury to the right side of the head might knock out the right eye completely AND the right occipital lobe leaving the left eye ineffective; but such a serious injury would not fall under evolutionary pressures because animals who sustained such a serious injury wouldn't live regardless of whether sensory control was ipsilateral or contralateral.
",null,1,cdimdq9,1r0qp0,askscience,top_week,3
lastsynapse,"The short answer is that we don't really know, but the evolutionary process occurred very early in the history of vertebrates.

[These folks](http://www.ncbi.nlm.nih.gov/pubmed/18780298) suggest it is because decussation is less prone to errors.  There's some more references on the [quora](http://www.quora.com/Why-are-so-many-brain-pathways-crossed-and-when-did-decussation-develop-in-phylogeny) question, too.  Others suggest it has to do with injury and injury prevention, that with crossed representations you can defend yourself on the side of attack, but that's probably hooey.

The weird thing is that you have ipsilateral cerebellar representations which are contralateral on the cerebral cortex.  Eyes are mixed, projecting both ipislaterally and contralaterally.  Everything crosses in such weird ways.  If there is a reason evolution selected for crossed representation, it has a whole lot of explaining to do, that won't be reduced down to one simple sentence.",null,0,cdmktyb,1r0qp0,askscience,top_week,2
SqueakyGate,"We can't know for sure but here are some interesting dates:

1. ~60,000-50,000 years ago it is postulated that humans reached a point known as [behavioural modernity](http://en.wikipedia.org/wiki/Behavioral_modernity). ""One [hypothesis] holds that behavioral modernity occurred as a sudden event some 50 kya (50,000 years ago) in prehistory, possibly as a result of a major genetic mutation or as a result of a biological reorganization of the brain that led to the emergence of modern human natural languages"" These humans were like us in every way, so it is very very likely that you could mate with and produce viable offspring with these humans. Moreover these humans could be brought up in a modern society and would be able to blend in perfectly from both a physical and behavioural perspective. 

2. Anatomically modern humans arose about 200,000 years ago. It is very likely that you could mate and produce viable offspring with these humans as well. Although the evidence is less clear that these individuals behaved like us. For example we are unsure if they could have developed the same level of language complexity as ourselves. These anatomically modern humans could be brought up in a modern society and would be able to blend in perfectly from a physical perspective, however they may not be able to blend in from a behavioural perspective. It is worth noting that ""he second [hypothesis] holds that there was never any single technological or cognitive revolution. Proponents of this view argue that modern human behavior is the result of the gradual accumulation of knowledge, skills and culture occurring over hundreds of thousands of years of human evolution"". Therefore the dates for ~60 to 50 kya may not be so hard and fast.

3. Other homo species. We know that about 1-5% of human DNA is [Neanderthal](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens) in origin. We know that early humans and neanderthals could interbreed and produce viable offspring. Since it is very likely that modern humans could reproduce with ancient humans living 200,000 years ago than it stands to reason that modern humans and neanderthals could interbreed and produce viable offspring. A few important caveats are worth mentioning:

* 1-5% DNA can be explained by a small number of interbreeding events. This means that interbreeding events were probably uncommon.

* Human and Neanderthal populations did not overlap completely, therefore not all humans encountered neanderthals or vice versa. This means any interbreeding that occurred was not widespread.

* Neanderthals evolved about ~600,000 years ago in Europe, conversely humans evolved about ~200,000 years ago in Africa. Humans migrated out of Africa and into Asia about 100,000 years ago. Humans and neanderthals only encountered each other in Europe from about 50,000-25,000 years ago. Thus neanderthals and humans had been evolving independently for thousands of years. Behaviourally Neanderthals and Humans were very different, even more so around the time when they first began to encounter one another. Humans were better communicators, better innovators, engaged in long distance trade, and were expanding into new environments as well as exploiting them in novel ways. In contrast Neanderthals were somewhat limited: they had a relatively stagnant tool culture and and their cultural expressions were not as complex as humans. Humans at the time were beginning to express themselves symbolically, whereas neanderthals seemed only to be able to make jewellery, possibly some cave art and possibly engaged in symbolic burials.

* We know nothing about the context in which the exchange of DNA took place...we don't know if it was consensual or not. This has important implications for how the two populations viewed each other. Were they hostile? friendly? did they compete over resources (archeological evidence suggests yes, they did) or did they cooperate with each other? These questions help us better understand the complexity of their social worlds and how we might delineate these two populations (separate species, subspecies or the same species?)

* We know that flow of DNA was in one direction. ""While modern humans share some nuclear DNA with the extinct Neanderthals, the two species do not share any mitochondrial DNA, which in primates is always maternally transmitted. This observation has prompted the hypothesis that whereas female humans interbreeding with male Neanderthals were able to generate fertile offspring, the progeny of female Neanderthals who mated with male humans were either rare, absent or sterile"". This raises questions about hybrid vigour, and questions the assumption that we were the same species. At the very least it appears as if we were speciating, given the level of behavioural separation which had already taken place.

What does this mean for other hominin species like *H. erectus*? We can't know with 100% certainty if modern humans or even ancient humans could mate with these other extinct species. However, given the low levels of breeding with Neanderthals, the extensive evidence which suggests we were very different behaviourally and the lack of support for strong hybrid vigour I would come to the conclusion that it is unlikely that humans, ancient or modern, could produce viable offspring with any other hominin species other than neanderthals.

[More on our coexistence with Neanderthals](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens)

[More information on the Neanderthal Admixture Hypothesis](http://en.wikipedia.org/wiki/Neanderthal_admixture_theory#Neanderthals)",null,3,cdiio7h,1r0k55,askscience,top_week,19
TwizzlyHashtag,"There is also the issue of the theorized hormonal arms race between the fetus and the mother/host, wherein the fetus produces hormones that attempt to take over the host's resources, and the mother produces hormones to counter that.

However, a human woman from, say, 50,000 years ago would theoretically be too far behind in the evolution of said hormone production, leaving her to be overwhelmed and killed by a modern fetus, because the modern fetus will have been evolved to produce said hormones more aggressively.  Maybe. 

Some example discussions of this phenomenon:
http://books.google.com/books?id=FJixmoT5olEC&amp;pg=PA177&amp;lpg=PA177&amp;dq=mother+fetus+hormone+arms+race&amp;source=bl&amp;ots=OdcOT29uu1&amp;sig=xI2MNYMVlQpyp7M6gfucTVSBQGQ&amp;hl=en&amp;sa=X&amp;ei=GpCOUo79H-igiQKlyYGQDg&amp;ved=0CDQQ6AEwAQ

http://www.ulm.edu/~palmer/Mother.htm",null,0,cdk0nup,1r0k55,askscience,top_week,1
NassT,"In addition to the oxygen that makes up water (the O in H2O) there can also be oxygen (O2) dissolved in water, just like you can dissolve sugar or salt in water.  That's what people are talking about in this case.",null,0,cdic7sc,1r0iwf,askscience,top_week,5
hatsune_aru,"Gas can dissolve in water.

Fizzy drinks like Coke, Sprite, Beer and fizzy Champaign all have Carbon Dioxide dissolved in it. Oxygenated water, as the manufacturer claims, has more oxygen dissolved in it than just normal water.

Fun fact: most water-soluble compounds, like sugar and salt, are more soluble at higher temperatures. However, most gases, such as Carbon Dioxide and Oxygen, are more soluble at lower temperatures. That's why warm coke seems to have less fizz! ",null,0,cditnyo,1r0iwf,askscience,top_week,3
pucklermuskau,"not necessarily limited per se, but definitely a major constraint. Nairboi, Kenya comes to mind, as a city that has definitely expanded, but its road infrastructure is causing major issues, and its hard to fix because the roads that need the most work are the ones that are used day in and day out. China is helping them build a new highway system to take the load off, but yeah: historical decisions are often headaches...",null,0,cdida95,1r0gty,askscience,top_week,1
JimmyGroove,"While existing infrastructure can certainly limit a city's growth (and your case is a good example of that) there is always the possibilty of removing and replacing it.  That becomes more and more expensive as time goes on, but whether or not it ever becomes prohibilitively so depends on a myriad of economic and government factors that would be really to complex to answer simply.  For instance, a city that is wealthy enough might be able to afford huge infrastructure changes, but if the government of that city has no way to secure rights from current property owners growth might never happen, while a poorer city with a government that has more power might actually have a chance to replace limiting infrastructure.",null,0,cdigpd7,1r0gty,askscience,top_week,1
joca63,"Unless I'm mistaken it should decrease net calories, but increase the calories you can digest. When something is cooked there will always be partial oxidation somewhere which decreases the energy contained in the food (think, if you over cook you get charcoal, if you really over cook you get carbon dioxide). However cooking makes most nutrients more easially digested. This is particularly true with fruits and veggies. Chewing raw veggies breaks surprisingly few cell walls to allow access the nurtients. Cooking breaks them on a large scale.

In short, cooking is almost a pre-digestion, it does use some calories in the food, but it allows access to more than it uses.",null,0,cdigq6w,1r0fjt,askscience,top_week,4
lasserith,"The calories themselves are derived from how your body naturally processes the chemicals included in them. Look at this guide for an example: http://www.nutristrategy.com/nutrition/calories.htm. As you can see each class of chemical in your food is associated with a certain energy density. Fats as a rule produce 9 calories when broken down by your body. As your body breaks down all chemicals of a similar class in the same way we are able to accept this 9 calories per gram of fat measurement and apply it to all sorts of foods. What this means is that as long as you don't drastically alter the chemical structure during cooking such as by burning it you can assume that calories in = calories out. Only a few chemicals used in cooking actually substantially change their dietary impact during the process, such as baking powder/baking soda but these are already negligible. 

TL,DR: Cookies are still bad for you but so so delicious.

Edit: As to the question of cooking adding or removing calories this is I suppose possible but I can't off the top of my head think of any thing commonly cooked with that would undergo a dramatic change. What little alcohol is cooked off when you cook with wine would help to decrease calories. There is I guess the possibility of degrading fiber if you use a high enough temperature to produce molecules which your body could possibly break down thus increasing the calorie cost but that would be far outside the range of temperatures at which you cook at.",null,0,cdie19g,1r0fjt,askscience,top_week,2
quantummonkey25,"Viral genetic replication, especially in the case of retroviruses, have few, if any, error-checking  mechanisms, which results in a high rate of mutation. Combined with the high numbers of viral particles that can spawn from a single cell, this results in a much more rapid rate of evolution.

As for the second one, I have not personally heard of such a case. I do know viruses are often implicatied in horizontal gene transfer, a property that is routinely exploited in genetics labs to deliver particular genetic information to a target organism.",null,0,cdicdvj,1r0dir,askscience,top_week,13
Farnswirth,"I asked something along the lines of the second part of this question a while ago.  Here's what /u/schu06 (a virologist) responded with:

&gt;Life is thought to have begun as RNA in the primordial soup - chemicals coming together and forming RNA (just a hypothesis as have yet to properly prove it). 

&gt;It has been shown that there are RNA molecules that can catalyse reactions such as self-splicing and self-cleaving (take [Hammerhead](http://en.wikipedia.org/wiki/Hammerhead_ribozyme) as an example). 

&gt;If an RNA can be be enzymatic it's not beyond the realms of possibility that it can develop function as an RNA-dependent RNA-polymerase and self-replicate. This self-replicating entity would just produce more and more of itself allowing evolution to kick in. Mutation would occur giving other ""species"" of the same RNA. Interestingly, it has also been shown that the same sequence of RNA can fold in two different ways to carry out two different functions (so will be able to do more than just replicate)...

&gt;It would get to a point where becoming compartmentalised from the outside world is advantageous to an individual RNA strand, so other molecules don't steal your enzymatic actions. This compartmentalisation could also be controlled if the RNA molecule has other enzymatic activities (as mentioned in the previous paragraph). Compartmentalisation gave the first ""cell"". Since this cell can replicate itself independently of anything else, it isn't a virus by any modern definition. So it then becomes a matter of semantics, would you call this self-replicating cell a bacteria? It isn't a virus because of self-replication

Basically, the answer to the second part, from what I've gathered is a matter of definitions and semantics.  As soon as something ""evolves"" from an RNA-based structure, it is no longer considered a virus, because by definition a virus cannot replicate without first infecting an existing cell.  

*tl;dr - At least for the second part of your question: we don't really know because no one has ever observed it.  But it is not outside the realm of possibility.*",null,1,cdigwl1,1r0dir,askscience,top_week,5
JeremyJBarr,"Ill chime in here with a slightly different angle. Viruses and ""living"" cells shouldn't be considered as completely separate entities. There is lots of new and exciting research suggesting much more of a symbiotic interaction between the two then we ever realized or thought possible. 

Two recent [papers](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001626) show that a bacteriophage (virus that only infects bacteria) tail spike and a human transcription factor that regulates the insulation of our nervous system (myelination), share the same protein coding sequence. This raises the evolutionary question of where did this transcription factor, that shares such homology with a viral protein, originate? 

It is possible that the gene was similar horizontally transferred. But another interpretation may hint at an ancient interaction between a phage particle and a single cell eukaryote, which may fit in your definition of something 'living evolving from a virus'.
",null,0,cdihu6s,1r0dir,askscience,top_week,5
zmil,"/u/quantummonkey25 answered the first question quite well. As for the second, it depends on what you mean by ""living."" Plenty of biologists consider viruses themselves to be alive, in which case the answer to your question becomes trivially yes. If you want to limit your definition of ""alive"" to cellular life, well, it's still a little tricky. It has been hypothesized that the eukaryotic nucleus evolved from a virus, this is known as the [viral eukaryogenesis hypothesis.](http://en.wikipedia.org/wiki/Viral_eukaryogenesis) I don't consider it a particularly *likely* hypothesis, however. There are many other cases of organisms co-opting a virus or part of a virus for its own purposes, but generally this is just a case of a few viral genes getting incorporated into an organisms genome; are the genes ""alive""? They're part of something living, certainly, but I don't think that's what you meant by evolving from a virus. This is known as horizontal gene transfer, as alluded to by /u/quantummonkey25. ",null,0,cdih1z7,1r0dir,askscience,top_week,2
owaisofspades,"because viruses have RNA, when they replicate their genetic material the RNA polymerase can replicate the entire strand, whereas DNA replication cuts a tiny bit off the end of the strand every time, therefore they can replicate their RNA a virtually unlimited number of times. Because of this unrestrained replication, the probability of mistakes arising during replication increases, and these mistakes can occasionally lead to beneficial mutations. As mentioned earlier, other organisms can only replicate a limited number of times in most cells, so the probability of having a mutation let  alone a beneficial one is relatively low.

The second question is beyond the scope of my knowledge",null,3,cdicdri,1r0dir,askscience,top_week,1
wishfulthinkin,"Sleep and fatigue, along with many other functions, are controlled by the hypothalamus, and the sleep cycle is regulated by neurotransmitters such as adenosine, dopamine, GABA, histamine, and hypocretin, but not by inhibition.


For example, caffeine is a stimulant because it is an Adenosine A1 receptor antagonist, meaning it binds to receptors that adenosine would otherwise bind to, preventing the effects of the adenosine.


Dopamine interacts with norepinephrine receptors, inhibiting its effects - which means a decrease in the production and release of melatonin. Interestingly, the researchers found that these dopamine receptors only appear in the pineal gland towards the end of the night, as the dark period closes. Therefore, the researchers conclude, the formation of these heteromers is an effective mechanism to stop melatonin production when the day begins and to 'wake up' the brain.


Each of the neurotransmitters I mentioned serves a difference purpose in regulating the sleep cycle.  Here's a good paper describing the effects of different neurotransmitters on sleep: http://med.stanford.edu/psychiatry/narcolepsy/articles/natureneuro5.pdf",null,4,cdigdbq,1r0dd7,askscience,top_week,10
null,null,null,2,cdigblo,1r0dd7,askscience,top_week,7
wishfulthinkin,"Sleep and fatigue, along with many other functions, are controlled by the hypothalamus, and the sleep cycle is regulated by neurotransmitters such as adenosine, dopamine, GABA, histamine, and hypocretin, but not by inhibition.


For example, caffeine is a stimulant because it is an Adenosine A1 receptor antagonist, meaning it binds to receptors that adenosine would otherwise bind to, preventing the effects of the adenosine.


Dopamine interacts with norepinephrine receptors, inhibiting its effects - which means a decrease in the production and release of melatonin. Interestingly, the researchers found that these dopamine receptors only appear in the pineal gland towards the end of the night, as the dark period closes. Therefore, the researchers conclude, the formation of these heteromers is an effective mechanism to stop melatonin production when the day begins and to 'wake up' the brain.


Each of the neurotransmitters I mentioned serves a difference purpose in regulating the sleep cycle.  Here's a good paper describing the effects of different neurotransmitters on sleep: http://med.stanford.edu/psychiatry/narcolepsy/articles/natureneuro5.pdf",null,4,cdigdbq,1r0dd7,askscience,top_week,10
null,null,null,2,cdigblo,1r0dd7,askscience,top_week,7
MCMXCII,"&gt;And the closer something does travel to the speed of light, the slower time goes relative to that object. Is this correct?

If you're moving at the speed of light *relative to me* **I** see **your** time passing slower.

&gt;So theoretically, if something travels faster than the speed of light, this object would travel back in time.

Something moving faster than the speed of light would violate causality.

&gt;I was hoping someone could explain to me why light and time are directly related.

The speed of light is sort of a ""conversion factor"" between space and time. If you take a time coordinate and multiply it by the speed of light, you make it into a distance in meters, even though it represents a time in seconds (or whatever other units of distance and time you choose).",null,5,cdiccso,1r0cxy,askscience,top_week,9
The_Duck1,"&gt; I was hoping someone could explain to me why light and time are directly related.

Light isn't special. There is a special speed, called c. Nothing can travel faster than this speed. All massless particles travel at this speed. Light consists of massless particles called photons, and thus travels at c. Light happened to be the first phenomenon discovered that propagates at this speed, so we call c the ""speed of light"" for historical reasons.",null,3,cdil5lv,1r0cxy,askscience,top_week,4
sloggz,"&gt; Why is it different for light? Using a similar example, if a star explodes, it takes time before that light reaches you. I would assume that if you outran the light, it would only mean that you wouldn't see that happen for a bit, and not that you were outrunning the event itself.

It's really just about your frame of reference. In almost all practical purposes, light IS the event. As far as we understand physics, you can't outrun the light at all. No matter how fast you run away from light, light will still (from your point of view) be moving at the speed of light. 

If a supernova goes off 100 light-years away, when does it make sense to say that the event ""happened"" here. When the first signs of that event become observable? If so, than that's the light. That's the whole ""Relative"" part of relativity. Whether or not you say ""That 100 light-year away supernova is happening now!"" and ""I'm seeing the light from a 100 year old supernova!"" is all... relative.

",null,4,cdiggxs,1r0cxy,askscience,top_week,4
null,null,null,5,cdiky3p,1r0cxy,askscience,top_week,2
nimobo,"Objects cannot travel faster than the speed of light. Because if an object travels at the speed of light, its mass becomes infinite. Moving any object requires energy. Infinite mass will need infinite energy to keep moving. Since the universe has finite energy, travelling at speed of light becomes impossible. So you should not compare sound with respect to light.

As an object approaches the speed of light, time is just measured differently for the object and a stationary person observing it. 

Lets say, you are taking a trip, (from point A to point B and back to A), approaching the speed of light, and I am observing you in a stationary position. For me it would seem that you have taken 10 hrs to complete the journey. But for you it will seem you have completed the journey in 6 hrs. So it would appear to me that time has passed slowly for you in completing the journey. This is called time dilation.

",null,7,cdiiz71,1r0cxy,askscience,top_week,3
juliuszs,"That ""clear screen"" is a metal shield with holes smaller than the wavelenght of you microwave's radiation, so waves get stopped. The metal walls are not going to let you use your walkietalkies, but regular walls are not metal. You do get attenuation due to scattering and absorbtion, but the rf radiation is not stopped by typical walls.",null,1,cdic6gg,1r0bov,askscience,top_week,4
glarn48,"A pretty interesting question. Intuitively I see no reason why genetics would predetermine someone to like one form of music over another. It seems much more likely to be due to culture and exposure. For example, if your 
parents played Frank Sinatra all the time, it's likely you too will like Sinatra because it's familiar to you. (See http://link.springer.com/article/10.3758/BF03201171 for a short-term demonstration of familiarity effects in music preference)

Is there any evidence for heritability of music preference? This question doesn't seem to be well-studied. A twin study is one of the most commonly ways of looking at heritability. An older study suggests music preference is not heritable (http://journals.cambridge.org/action/displayAbstract;jsessionid=BF7D791BC14FD12F7D16CC48CCFD7C7E.journals?fromPage=online&amp;aid=1362580). I found a press release from Nokia about the results of a pretty broad twin study that suggests a high heritability of musical preferences (http://press.nokia.com/2009/11/12/nature-or-nurture-study-reveals-musical-genes/). I'm very dubious of this study for several reasons, not least of which is that it doesn't seem to be published anywhere peer-reviewed, but also because the listed heritability is so high (for reference, a ballpark estimate of IQ heritability might be 50%).

Heritability isn't necessarily intuitive though. For example, the heritability of IQ is dependent on SES (http://pss.sagepub.com/content/14/6/623.short). That just makes it even more important to have some information about their methods and population of that Nokia study before you can interpret their conclusions. I wish I had a more rigorously scientific study to share, but hopefully someone else will come along with an insight. ",null,0,cdicgay,1r0as7,askscience,top_week,1
latent_variableZ,"Palmer, Schloss, and Sammartino (2013) stated as introduction to their study on human preferences in harmony: although empirical research on aesthetics has had some success in explaining the average preferences of groups of observers, relatively little is known about individual differences in preference. This about as true of a statement as one could make in regards to aesthetics. Neurobiologically, we can observe there is activity in such and such area of the brain when such and such stimuli are perceived. However, the variation in the subjective experience and idiosyncratic responses related to aesthetics do not lend them self to a this is fact statement of why we have different tastes. The best I can do with my background (I am a doctoral candidate in clinical psychology with an emphasis in forensic and neuropsychological testing) is tell you socialization and a persons cumulative learning history appear to have the largest effect size. The interaction between nature (biology) and nurture (the environment) is complex and most likely leads to the variations we observe in aesthetic taste, learning literally changes the brain.  However, learning appears to be a very powerful factor in determining aesthetics.  Palmer and Griscom (2013) provide a good review of the topic and give examples of studies that demonstrate aesthetic differences according to age, gender, and cultural; the take away while some preferences may be innate (infants have a bias toward looking at dark-yellow and light-red and a bias against looking at light-blue and dark-green, nearly the opposite of [western] adults) learning appears to be a powerful factor in determining preferences.  Learning is also implicated in the differences observed between gender preferences (girs tend to like pink/boy tend to like blue), theories tend to overwhelmingly postulate this happens due to socialization.  Historically, records show it was not until the mid-19th century the blue pink dichotomy emerged and became so prevalent.

References: 

Palmer, Stephen E. &amp; Griscom, William S. (2013) Accounting for taste: Individual differences in preference for harmony, Psychonomic Bulletin &amp; Review, 20(3), 453-461 DOI: 10.3758/s13423-012-0355-2  

Palmer, Schloss, &amp; Sammartino (2013) Visual Aesthetics and Human Preference, Annual Review of Psychology, 64, 77-107 DOI: 10.1146/annurev-psych-120710-100504",null,0,cdij457,1r0as7,askscience,top_week,1
albasri,"I don't have an answer, but there is an emerging field / group of researchers interested in [neuroaesthetics](http://en.wikipedia.org/wiki/Neuroesthetics). I'd start there.",null,0,cdizuds,1r0as7,askscience,top_week,1
Needless-To-Say,"Are you refering to the idiom of ""A sucker born every minute""?

If so we need to define the percentage of newborns that would be considered suckers.

Borrowing snusmumrikan's data of current birth rates and required births per year and making the (hugh) assumption that birth rates have been relatively constant and are linear with population. My best estimate would be when the world population was roughly 50,000,000 and according to my research this equates to about 1000BC",null,0,cdibryg,1r0agb,askscience,top_week,3
snusmumrikan,"Where are you getting the one every 8 seconds data? 

The data from the UN I've just looked up seems to suggest 2.5 births per second.

I tried to do some quick rough maths to answer your question but it means finding a year with half a million births, which will be so long ago there won't be any reliable records. ",null,0,cdibbop,1r0agb,askscience,top_week,2
iorgfeflkd,"Well, what is a magnetic field? A simple definition is that it's a region of space where moving charged objects experience torque. Light isn't charged, so it doesn't experience torque as it moves through a magnetic field.",null,2,cdibahb,1r09mw,askscience,top_week,9
SingleMonad,"There's not really a reason, at least not in terms of something more basic.

The electromagnetic field obeys the *principle of superposition*, which means that if you have the equivalent of two fields (either both electric or  both magnetic) in the same place, the resultant field is the (vectorial) sum of the two.  

For example, if you have two magnets, when you bring them close to each other you can imagine the two fields adding to each other in the space around them.  Depending on how you orient the magnets, you could make a stronger or weaker field in the common region.

Light is wave.  It propagates because as the electric field changes, it generates a changing magnetic field, which generates a changing electric field, and so on and so on.  This is the content of [Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell's_equations) #3 and #4.  If you aren't put off by an appeal to mathematics, really squint at those equations and you'll conclude that if the fields obey superposition (E_total = E_1 + E_2), then the formulas do too.  That's not an *explanation*, of course.  Maxwell's equations were deduced to agree with experiment.  There exist different physical media that *don't* obey the superposition principle, rubber, for example, if the amplitude is very large.

It would be a very weird universe if EM fields did *not* obey superposition.  I could be watching a movie, and if someone shined a flashlight across my view of the screen, I would see distortion.

At the risk of adding confusion, there is a deep connection between superposition of EM fields, the bosonic nature of the photon, and causality.  I wish I could explain it, but it's a result from quantum field theory, and I don't know a simpler explanation.

**TL;DL:**  That's just how it works.  The fancy name for it is [superposition](http://en.wikipedia.org/wiki/Superposition_principle).

Edit: Spleling",null,0,cdifs54,1r09mw,askscience,top_week,6
A_Mathematician,"Short answer is that light is not a charged particle and wouldn't be affected by any sort of H-field.  However check out the [Zeeman effect](http://en.wikipedia.org/wiki/Zeeman_effect) and the [Stark effect](http://en.wikipedia.org/wiki/Stark_effect) as an interesting result occurs.  In short, materials that are emitting light, if placed in a static magnetic field, will have their spectral lines (light waves) separate. This is how the magnetic field of stars and other astral bodies are measured.  If I find any of my experiments on it I will reply to my comment with photos in an edit.",null,0,cdimugr,1r09mw,askscience,top_week,2
chrisbaird,"Fundamentally, magnetic fields and light are just specific forms of electromagnetic fields. All electromagnetic fields are composed of photons. So, fundamentally, your question is really, ""why does one photon not bounce off another photon"". The reason is because they are bosons and they carry no electric charge.

All quantum particles can be classified as fermions or bosons. Fermions (such as electrons), obey the Pauli Exclusion Principle which states that no two fermions can occupy the exact same quantum state at the same location at the same time. So when two fermions approach each other, they must eventually ""bounce"" off each other to avoid ending up in the same quantum state. But bosons, such as photons, do not obey the Pauli Exclusion Principle and *can* be in the same quantum state at the same time. This leads to interesting effects such as lasers, superconductivity, and Bose-Einstein condensates. It also means that two photons cannot directly effect each other. Instead, they just pass through each other. What determines whether a particle is a boson or a fermion is how it spins. Integer-spin particles are bosons.

Also worth noting is that photons themselves carry no electric charge, so they are not effected by each other. Even if a particle is a boson, it can still interact with others of its kind if it carries charge. Such is the case with gluons, which carries color charge.",null,0,cdixln1,1r09mw,askscience,top_week,1
mc2222,"light is a *changing* Electromagnetic field.  If light travels through a region of space, we can use the analogy of a pond to describe the EM field.  Light is the ripple that travels on the pond after the surface is disturbed.  We can consider the depth of the pond to be how strong the *static* EM field is in this region.  since light is the ripple, the size of the ripple is completely unrelated to how deep the pond is.  So, if light passes through a region of free space with a magnetic field, it is unaffected.

Having said all that, magnetism in a *material* can effect how light travels through that medium.  Basically, the magnetic field effects the matter, and the organization of matter relates to how light propagates through it.",null,0,cdj3fhk,1r09mw,askscience,top_week,1
diazona,"It's not really so accurate to say microwaves have less energy than visible light. You can have any amount of energy in microwave radiation, just like you can have any amount in visible light. It just comes in smaller increments (i.e. photons) when you use microwaves.

In a nutshell, visible light doesn't work because it gets reflected, whereas microwaves tend to make it into the interior of the food. (Of course that's only the simplest version of the story.) See [here](http://en.wikipedia.org/wiki/Dielectric_heating) for more information.",null,1,cdigaqm,1r091o,askscience,top_week,9
baloo_the_bear,"Microwaves interact with water molecules and cause them to vibrate. On a molecular scale, vibration **is** heat energy. The heat from the vibration of water molecules heats the food being microwaved. The effect of microwaves only works on water, which may lead to rapid drying of the food, which is why when you over-microwave something it becomes brittle.",null,3,cdib7e7,1r091o,askscience,top_week,8
goatherder100,"Basically its like microwaves are pingpong balls (lower energy) and light would be basket balls (high energy). If I hit you with one ping pong ball, nothing. If I hit you with a basketball, you notice it. If I pelt you with 10million ping pong balls you will really notice it. Kinda the same thing. It is not the amount of energy EACH photon has to deposit, it is how MANY photons there are to deposit energy. A microwave creates a high density flux of a large number of lower energy photons that will each deposit all their energy into the substance causing the average kinetic energy of the substance to increase. That is what heats it up. ",null,0,cdii8a0,1r091o,askscience,top_week,5
myarlak,when molecules absorb microwaves it causes them to spin rapidly releasing heat due to the friction of the molecules rubbing against each other.  visible light cause electronic transitions which are not translated into motion thus do not cause friction.,null,2,cdij6qb,1r091o,askscience,top_week,3
king_of_the_universe,"&gt; Why can we use microwaves to heat our food, when microwaves have less energy than visible light?

But the light of your Zippo isn't as hot as the summer sun, while both are visible light. Yes, a microwave photon has less energy than a visible light photon. No, microwave light doesn't hence have less energy than visible light. Duration of exposure and amount of photons are important, too.",null,0,cdioxr2,1r091o,askscience,top_week,1
Weed_O_Whirler,"No. Instantaneous acceleration would require infinite force, and infinite force would cause all sorts of bad things to happen. 

So, what is actually happening? Well, you (and really, everyone) is a little squishy, so when you first get hit by the bus you slowly start to move, but moreso you start to deform- getting squished. Thus, you will ""slowly"" (slow being relative here, as it will still happen in a fraction of a second) move up to the speed of the bus. 

In fact, this speed is what matters. That is why if you were surrounded by a squishy ball, getting hit by a bus would not be as problematic, since that time it took you to speed up to the speed of the bus would be longer (and thus, you acceleration smaller)",null,0,cdiablh,1r086u,askscience,top_week,7
matts2,"You are a compressible being. So the force travels through you at different rates. Think of this: you are bruised on the side you are hit, not all through your body. What happens is that your are hit and there is force transference. Some of that force it too much for the local bonds and they break (broken blood vessels, broken bones, etc.) so moves though you and accelerates your body.",null,0,cdibs6x,1r086u,askscience,top_week,3
nonchalantkiwi,"No, not instantaneously. Someone getting hit by a bus is a great example of how Impulse works. Impulse is the force exerted on a particle (in this case, the bus exerting a force on a person) for a certain amount of time. Now with this collision, the time will be very small, but not instantaneous because if it were instantaneous then the impulse would be infinite, which is impossible.  ",null,0,cdiadfm,1r086u,askscience,top_week,2
Platypuskeeper,"Salt water is denser. As for how that affects buoyancy, that should be easy to figure out through [Archimedes' Principle](http://en.wikipedia.org/wiki/Archimedes%27_principle).
",null,2,cdib9j1,1r084w,askscience,top_week,6
Gargatua13013,"An subset of this problem is central to the study of the genesis of ore deposits associated to black smokers, namely what happens to the buoyancy of waters of grossly different salinities and temperatures when they interact. There is a seminal paper on this by Takeo Sato, (1972), ""*Behaviours of ore-forming solutions in seawater*"", Mining geology, **22**, 31-42.

Pressure is also an important variable. Turns out in the right circumstances, the brine plume will be denser than seawater and collapse to the seafloor where it may form a brine pool, which was observed in nature years after Sato's paper.",null,0,cdifh1g,1r084w,askscience,top_week,1
