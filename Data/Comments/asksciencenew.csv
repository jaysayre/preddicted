author,comment,distinguished,downs,id,post,subreddit,type,ups
Arrogus,"Loud noises and bright lights are damaging because of the energy they carry; scents, on the other hand, are merely particles suspended in the air. Sure, many chemicals could do serious damage to your olfactory receptors if you inhaled them through your nose, but it would be because of their reactivity, not their pungency. In such a scenario, damage to your lungs would probably be your primary concern.",null,36,cdnmjtk,1rim55,askscience,new,192
Zukuto,"/u/Arrogus has it. it isnt that they are too *smelly* that breaks your nose, but that they are comprised of *toxic* fumes; sometimes they are smelly and other times not.

one time i had to clean out a Hair salon next to a business i was working for in a strip mall; i was the only one who posessed a mask that also had eye protection. the salon owner had tried to get a stain off the floor using Bleach and Ammonia. i let all her hoses run onto the floor and pushed the watery mustard gas out of the building before calling the Fire Dept and a HAZMAT team. 

i got a free haircut for my trouble. the salon owner got the shock of her life.",null,0,cdnymdg,1rim55,askscience,new,7
ubcokanagan,"No, you perceive smells when aromatic compounds bind neurons in your nasal passages.  The binding causes these neurons to fire which send a signal to your brain letting you know you just stepped in dog crap.  A very strong smell will innervate many neurons but it wont damage them.

If the odour is present for a long period of time, desensitization of the neurons will occur, and they will be less likely to fire in the absence of an increase in concentration of the aromatic molecules.  This is why smokers don't realize that they smell terrible all the time (well that and any damage caused by the smoke).

If the strong smell is caused by something toxic then yes it can cause damage, but I believe this would be caused by a property of the offending chemical, as opposed to it overstimulating a nerve.",null,0,cdo0u8c,1rim55,askscience,new,3
null,null,null,0,cdo28pj,1rim55,askscience,new,2
Philosophisation,"As with any sensory input it can be damaged via chemicals. Your chemoreceptors in your sinus will not however get destroyed by excessive use. Imagine a bathtub with the plug pulled. This would be ~chemoreceptor being overloaded with scent molecules or similarly shaped molecules at least. Anything that fits through goes through and is registered. But add oil and hey? It doesn't go through for a while. This is one reason for desensitized smell. Another is that the sensory nerve endings present to receive the signal from the sensory organelle(dendrites) fires so often that the brain starts filtering it out as useless signal, same as white noise. So no excessive safe smell will desensitize but only harmful molecules may ruin smell receptors.",null,0,cdo2jpc,1rim55,askscience,new,1
freeze4111,"All odors, indeed anything that gets in your nose, damages it to a very small extent. The strength of the odor isn't necessarily the measure for its destructive capability however; mostly it is corrosive acids or things like smoke which do the real damage: look up tobacco smoking and its damage on the sense of smell to get an idea: http://www.ncbi.nlm.nih.gov/pubmed/22776624. 

Heavy odors, like blue cheese or something like that, may briefly block a number of receptors making your sense of smell not as good as what it could be, but this is very temporary (no more than a few seconds). In addition, your brain can adapt or habituate to odors, making them less noticeable (this is harder to do as the odor gets stronger). You'll probably notice this with your own perfume/deodorant throughout the day. 

Any damage done to your sense of smell is much, much easier to recover from compared to other senses because the neurons involved can turnover and regenerate (these are the only neurons that can). Some environmental experiences can make the turnover slower (as can age) but overall your sense of smell will recover from anything you throw at it. 

Something I find interesting- the neurons that take information from your sense of smell transmit that information to the brain through neurons passing a structure called the cribiform plate via little holes. If these neurons are severed, they can regenerate, but usually can't find their way through the holes again; a case of this unique quality being utterly useless! 

",null,0,cdo3j4q,1rim55,askscience,new,1
paulHarkonen,"I work in the natural gas industry and thus work with odorant (that rotten eggs smell in gas).  Odorant is one of the most powerful smells around but all the health concerns surrounding it involve how your body reacts to strong odors.  Very strong negative smells can pose a nausea risk, along with some breathing concerns because your body expects strong bad smells to also be toxic.  None of the Msds information covers permanent damage to your sense of smell.  (Although there is a short term effect as your nose becomes overwhelmed by the one strong odor and stops caring about other weaker odors).

Its not a super scientific source, but there has been a fair bit of testing to create the MSDS information.",null,0,cdo8mi4,1rim55,askscience,new,2
Astrokiwi,It's most likely flat. [This post in the FAQ](http://www.reddit.com/r/sciencefaqs/comments/v97po/is_the_universe_infinite/) should be useful.,null,0,cdnkzfg,1riij0,askscience,new,3
skleats,"Check out the dog genome sequence - there's lots of great examples of the role of repeated sequences in the selection history of various breeds. For some good sources:

[Here's](http://genomebiology.com/2011/12/2/216) an overview of the dog genome, with some info about repeated sequences.

[Here's](http://www.pnas.org/content/107/3/1160.full) a good rundown of the genomic variety between breeds.

And, for all the money, [here's](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1356118/) a study showing that the diversity is linked to SINEs.

Evolution gets driven by selection on random variation, but repetitive sequences drive random variation.",null,0,cdnwr9q,1rih74,askscience,new,2
astazangasta,"Most of the genomes that are full of crap like LINEs and retrotransposable elements are higher eukaryotes. Prokaryotic genomes are usually much tighter and filter these sorts of things out. That is probably because higher eukaryotes can tolerate the addition of some extra sequence in the genome - over the course of a large multicellular organism's lifetime, replicating a few extra base-pairs worth of sequence every time a cell divides is not that big a deal. A prokaryote, on the other hand, can optimize over each division, since each replication cycle produces a new generation.

This probably means that repetitive elements in the (e.g.) human genome are mostly harmless crap - they are annoying, but not that important. But if we could get rid of them (like prokaryotes), we probably would.",null,3,cdnuuqn,1rih74,askscience,new,1
SMURGwastaken,"The towel is definitely a good idea. Wash + towel is probably the best strategy, since I've done fingerprint tests in the micro-lab that have shown far more microbial growth on washed hands compared to unwashed ones (with certain soaps). You'd be surprised how readily the natural flora on some people's skin will attack anything new, so if you wipe out everything and then introduce something pathogenic it's likely to stick around.",null,0,cdnr8qa,1rih2r,askscience,new,2
cHEatsKYJelly,"Wet objects tent to hold things easier, for example wash your hands then stick them in sand.  We did an experiment in nursing school with the 5 second rule. What we did was got different objects and food, then drop it on the ground to see what holds more bacteria. The wettest objects had the most, jello, wet bread, stuff like that. The cleaner of the objects were the m&amp;ms, pills (tablets and caps). Hope that helps.",null,0,cdnswik,1rih2r,askscience,new,2
5amlawnshit,"A towel definitely helps - it serves as a physical barrier between your hand and the door knob. Therefore, the germs on the door knob do not come into contact with your hand.

Now to address whether a dry hand or wet hand transfer more germs from the door knob to your hand, I do not have a definite answer that wouldn't be based off speculation. 

tl;dr: Always wash your hands",null,1,cdnqksj,1rih2r,askscience,new,2
FlavaFlavivirus,"Yes! I work with Alphaviruses; these particles contain a fusion peptide which allows the contents of the capsid to enter the cytoplasm of the cell, by fusing the two membranes together and inducing a conformational change in the structural proteins. 
",null,0,cdnpmde,1rigsa,askscience,new,2
captsuprawesome,"I would not characterize it as ""catalyzing their own import"" but many proteins are capable of penetrating the cell membrane.  HIV-1 Tat is a well studied protein that can do such a thing.  You may be interested in this summary of [cell-penetrating peptides](http://en.wikipedia.org/wiki/Cell-penetrating_peptide).",null,0,cdnsl21,1rigsa,askscience,new,1
bearsnchairs,[Transferrin](http://en.wikipedia.org/wiki/Transferrin) is a neat protein for that. It carries iron into cells. When transferrin binds to its acceptor on the surface of cells it initiates endocytosis and is taken up by the cell. You can attach transferrin to a particle or protein of interest to incorporate it into cells. ,null,0,cdnvctl,1rigsa,askscience,new,1
LuklearFusion,"It really depends on the physical implementation of the qubits, as there are many kinds. An incomplete list of things that people use as qubits are:

1. The electronic structure of ions or atoms, which will be confined to some region of space by some sort of electromagnetic ""trap"".

2. A single electron's spin, where the electron has been trapped in a solid state system; a so called ""quantum dot"".

3. Superconducting circuits which have Josephson junctions can also be used as qubits.

4. Optical qubits use polarization or optical modes in light as qubits.

Each kind of qubit is stored (I'm assuming by stored you mean kept free from noise) and manipulated differently. Is there a particular kind you're interested in?",null,0,cdnnt7i,1rifph,askscience,new,6
DanielSank,"Whenever people ask about this I recommend reading [this post](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp). It refers only to superconducting circuit qubits, but it's definitely worth a read.",null,0,cdnvaq9,1rifph,askscience,new,1
Jeffy_Weffy,"Slow chemical reactions are happening. At one end, the reaction wants to take in electrons. On the other end, the reaction wants to give up electrons. The only way for the electrons to flow to complete these reactions is to go through the device you're powering.",null,0,cdnkh2z,1rifop,askscience,new,11
chillichill,"Lithium-ion batteries are common so I'll use those as an example. A battery has 4 major components, a cathode, anode, electrolyte and wires to connect the electrodes and complete the circuit (plus all the other bits that hold them together). This circuit is attached to a power supply during charge (to provide electrons) and a device when discharging (to be powered by movement of electrons).  The cathode and the anode are materials which can hold Lithium, while also allowing it to leave the structure reversibly. The electrolyte is a material which allows lithium to pass through, but not electrons.
When a lithium battery is being charged lithium ions (Li+) move from the cathode to the anode, through the electrolyte. When the Li+ reaches the anode it takes an electron from the power supply to form a (relatively) stable state. The Li is stored in the anode until a device that requires power is connected (discharging the battery). 
When a device that needs power is attached, the Li in the anode releases an electron to form Li+ which moves to the cathode. The released electrons cannot travel through the electrolyte therefore travel around the circuit, powering the device. At the cathode the Li+ recombines with an electron from the circuit. Once all the Li has moved from the anode to the cathode, the battery is completely discharged. ",null,0,cdnoov8,1rifop,askscience,new,1
stimulatedecho,"Hail forms in the presence of a strong updraft.  In this case, water precipitates, freezes and falls, but is blown back (continuously).  During this cycle more layers of ice form on the previously precipitated particles, until they get too heavy and eventually fall to the ground

If you get the chance, break open a hail piece (bigger the better) and you will find it is layered akin to an everlasting gobstopper.  ",null,0,cdnm2ax,1rid0v,askscience,new,2
ipostjesus,"basically, snow = ice from the beginning to the end of the process of water particles accumulating into larger structures. It doesnt always form hexagonal structures, there are many shapes of snow.

hail = a liquid or partially melted phase in the process, most likely involving a re-freezing event prior to reaching the ground. 

Ive never learnt much about snow formation, but i can tell you about hail. 
In a cumulonimbus cloud, water is cooled below freezing point but it hasnt frozen, called 'supercooled' water. supercooled water will freeze when it comes into contact with something that can start the crystal growth, such as a dust particle or some frozen water. So supercooled water is blowing around in the cloud, being pushed up by updraughts and falling back down when the updraughts cant hold its weight any more. It will cycle around the cloud falling and then riding updraughts back up again, all the while it will be accumulating water (some of it supercooled liquid water, some of it water vapour) until it doesnt find an updraught strong enough to hold it in the cloud and it falls out. Because the supercooled water is liquid and doesnt necessarily freeze instantly, the stone will be wet on the outside, which means stones will stick to each other by touching and then freezing. the sticking together of stones into larger stones makes the irregular surface of the ""random chunks"".
The larger a hail stone, the longer it spent accumulating while blowing around in the cloud. Which generally means larger clouds with stronger updraughts capable of suspending larger particles. ",null,0,cdnmhop,1rid0v,askscience,new,1
Freeoath,"The eraser works in a way that when you rub it, it removes the graphite from the papers surface. The rubber is more ""sticky"" then the paper and thus the graphite preferes the eraser over the paper. Another way some erasers works is the eraser damges the top  layer of the paper effectively removing the graphite that way. 
You can't use an eraser on ink (what a pen leaves behind) because the paper more or less absorbs the ink deeper making the erasers funcion useless. For these you can use ink remover that either changes the chemical compound of the ink removing it from the paper, or dying it white

",null,0,cdnk132,1ricv9,askscience,new,11
s3c7i0n,"It does, were you to look at the sun directly in space, you'd be blinded. That's the point of those gold visors on space suits. The reason it doesn't appear to is that there's very little for the light to strike. Think of shining a flashlight into the air, it has effectively no visible effect. Now if you shine it at a tree, it gets nice and bright. It's the same amount of light, but it doesn't appear so because we can't see it travel. Now if you mean why isn't space blue, like the sky, that's because when sunlight filters through the atmosphere, it's interaction with the various gasses scatter the blue light most, giving us that nice hue. Space, obviously for the most part lacking an atmosphere, doesn't scatter the light, hence the black. ",null,0,cdnj4u9,1riapc,askscience,new,28
alltat,"It *does* make space bright. The only reason space looks black is because it's empty: it's not black because it's dark, but because there's nothing there. If you look at pictures of spaceships and satellites in space, you'll notice that they're all brightly lit with strong shadows. That's because space is bright, as long as you're close to a star.",null,0,cdnjy5p,1riapc,askscience,new,7
VA_guy,"There are two parts.  First, the sun's brightness decreases with the square of the distance.  Meaning when you're twice as far, it is 1/4 as bright.  Four times as far?  4*4=16, 1/16 as bright.  That's because there is a finite amount of light being cast over an ever increasing spherical area.  So the sun would look quite a bit less bright from Mars or Saturn than it would here simply because we're closer.
  
But if you're asking why space isn't glowing, you need to think about what would cause that to occur.  If you have a spotlight on a clear night, it will illuminate a path in front of it but it won't make the entire surrounding area bright, right?  But if you were to shine that same spotlight into a white room, it would do a much better job of making the whole area look bright.  That would be because there are reflections in the second case which case the light to come at you from all angles, appearing to illuminate you from everywhere.  
  
So in space it would be similar to that spotlight.  If you look directly into the sun, it would be very bright (depending on your distance).  But otherwise, there is nothing else out there for the light to reflect off of, so it won't be as if the entire area is glowing or light is coming from you at all directions.
  
Hope that helps.",null,0,cdnj62j,1riapc,askscience,new,6
stuthulhu,"Things are bright because light bounces off those things, and strikes your retina. There's relatively little in space for light to bounce off of, and get redirected towards your retina instead of traveling away. ",null,0,cdnjqys,1riapc,askscience,new,4
Gitsumkikin,"Oh it does! Only specialized cameras can turn towards the sun... If a regular camera were to be facing the sun all you would see is white. Same thing if you were facin it...Kiss yer eye sight goodbye! I think its somethin like 350 degrees if you are in direct sunlight in space, -350 out of. Space is so unimaginably huge that if your back was to the sun,(it better be!) the light probably wouldn't be noticeable at all...nothing for it to reflect off...as I said,mostly educated guesses here. Aside from the temp and the specialized cameras, those are facts, temp may be off one way or another, but, not by much.",null,0,cdnj45h,1riapc,askscience,new,3
thetripp,"Your son is basically describing the theory known as ""Tired Light.""  The reason we don't think tired light is true is that we've never been able to come up with a mechanism that would cause energy loss in photons, yet still match our observed data.

For a tired light phenomenon to be true, it would have to:

1) Explain energy loss of photons over long distances, and match the observed redshift.

2) Not scatter photons so much as to induce blurring (since we don't observe significant blurring of distant objects).

3) Also explain the observed time dilation of distant events

4) Cause the same effect in every wavelength band, or in other words photons must ""tire"" in the same way, regardless of their frequency.

The wikipedia page on [Tired Light](http://en.wikipedia.org/wiki/Tired_light) has a nice list of some of the historical proposals related to this theory and why they don't match the observed evidence.",null,1,cdnl44c,1ri89x,askscience,new,13
stimulatedecho,"The distance related red shift is *evidence* of an expanding universe.  There happens to be a mountain of other evidence (search this sub for this question being asked previously to find specifics, mostly related to the cosmic microwave background, I believe), that suggests the same thing, i.e. expanding universe.  So, we don't really *know* that expansion causes the observed red shift, but it is certainly one valid explanation (as you already know), and it also explains other things we observe.  Additionally, and potentially more importantly, we have no experimental evidence to the contrary. 

That said, the requirement of ""dark energy"" energy seems to be a bit of a blemish...there is no doubt something in the recipe we have no understanding of.  I guess we'll find out exactly what as we go!",null,1,cdnlgh3,1ri89x,askscience,new,6
florinandrei,"&gt; ""but what if light just lost energy steadily as it went, wouldn't that look the same?""

No, it would not.

You are talking about an entire class of alternative explanations of redshift, grouped under the umbrella of the ""tired light hypothesis"". The have pretty much been debunked in bulk.

You cannot have light become ""tired"" by magic. There has to be some physical mechanism for photons to lose energy. If so, the energy loss will tweak the properties of those photons a little. As a result, a series of effects would become apparent:

- images from distant objects would become a little blurred, due to the scattering of the photons via energy-sapping interactions

- distant events would be observed to take place at the same rate of time; there would be no time dilation, like in the relativistic redshift models

Other effects would also become observable, depending on the particular ""flavor"" of tired-light theory, none of which have ever been observed.

Bottom line: the expansion of a relativistic universe is the only model that accounts for everything we observe out there.

http://en.wikipedia.org/wiki/Tired_light",null,1,cdnlxim,1ri89x,askscience,new,3
WhoH8in,"Well light does loose energy as it goes, in a sense anyway, every time it doubles its distance its energy is 1/4 what it was because intensity dissipates. This does not affect wavelength though. There are other phenomena that affect wavelength though, like movement. If somehting is moving toward us the light it emits doesn't seem to hit us any ""harder""(b/c light only goes c, no faster, no slower, ever) but that energy is accounted for, the light decreases its wavelength. If it is moving away then the wavelength increases which makes it appear redder.

Now when looking out into the stars hubble noticed that the further an object was the redder it appeared to be compared to what we know that objects [emission spectrum](http://en.wikipedia.org/wiki/Emission_spectrum) *should* look like. Now if the Universe were static then we would expect that, overall, half of all objects must be moving toward us and half be moving away and some tiny minority not moving relative to us at all. It is incredibly unlikely that, in a static universe, all objects would be moving away form us, we would ahve to be a truly uniqe body to observe that, literally the center of the universe. If the universe is expanding though it makes perfect sense because every object percieves every other object as moving away from it (ignoring of course nearby objects).

Use the expanding balloon analogy to understand it. If you have a barely inflated balloon and you draw three dots on it then start to blow it up those dots appear to be moving away form eachother but none fo them are actually moving, the space between them is expanding. This is why we think redshift is caused by expansion.",null,6,cdnjekl,1ri89x,askscience,new,2
bohr_exciton,"Yes, but only if you ionize the gas, manage to extract (at least in part) charge carriers of one type (say electrons), and then manage to somehow isolate the system such that charge neutrality cannot be re-established. In that case there will be a net charge within the gas, and the resulting repulsion would act as an effective increase in the pressure, which like you said could alternatively lead to a larger equilibrium volume if the container is flexible. ",null,0,cdnjcd7,1ri7it,askscience,new,1
TangentialThreat,"The charge will prefer to collect on the outside of the balloon, but the balloon material will repel itself and that may have the desired effect.

Is it cheating if I heat the contents of the balloon to 10,000 K? The rubber will melt, but for a brief moment you will have significantly increased the pressure using ionization.",null,0,cdnkify,1ri7it,askscience,new,1
__Pers,"Yes, but not in the way you think. In plasmas, as in ideal gases, 

P = n k_B T

If you ionize a gas to make a plasma, the density of independent particles n comprises electron density and ion density and the sum is higher than the neutral particle density prior to ionization. Also, in making a plasma from a gas, you generally make the temperature higher. Both will tend to increase the pressure. ",null,0,cdnsyil,1ri7it,askscience,new,1
OrbitalPete,"Tall mountains are generally a product of continental collision. That occurs when subduction processes close an ocean and collide the continents that  formed its margins.

When that ocean closes lots of the upper sediments get scraped off. They obviously get caught up in the collision zone, and involved in the thrust faulting and deformation that builds the resulting mountain range. Carbonates such as limestone are commonly among these uppermost sediments. Hence the fact you tend to see a lot of limestones at the top of mountain ranges.",null,0,cdnq1t9,1ri7ga,askscience,new,6
DangerOnion,"I don't think they're overrepresented.  The Alps and Rockies are mostly granite, and the Andes are chiefly igneous rock.  Like OrbitalPete says, mountain ranges are usually formed by the collision of tectonic plates, and whatever rock happens to constitute those plates is what gets shoved upwards.",null,0,cdnv4et,1ri7ga,askscience,new,3
Baloroth,"Putting a fan behind the space heater will produce forced convection, which will cool down the heater and heat up the room as a whole. This is the reason central furnaces have fans in the first place: it spreads the heat around (same for AC, but the reverse principle: you heat up the condenser and cool down the air).

What the net effect over a long period of time will be (i.e. if the room will end up warmer with or without the fan) depends on many factors, but generally I would venture that the room will be warmer overall with the fan than without. But short term, in a cold room, the fan will certainly speed up the process.",null,2,cdnilrg,1ri6wl,askscience,new,13
OlejzMaku,"It depends on the definition of ""heat up the room"". Do you want evenly districubuted heat or warmer area around the heater? How warm do you want the room to be? How big is the room? How well isolated it is? What temperature is outside?

If the room will be too big and/or badly isolated and/or it is very cold outside and/or your desired temperature is too high the fan might be contraproductive.",null,1,cdni8d1,1ri6wl,askscience,new,7
garycarroll,"You are correct that the result will be at least as much heat energy into the room, and more air movement (to a point) will result in more even heat. This may be better, or not. If the room is not sealed for instance, the door is open to the rest of the building) more heat may escape than if you had a warm side of the room away from the door. And as OlejzMaku implied, if the room is too large or cold, the space heater may be unable to make the whole thing comfortable but could heat one corner. 
Also, note that moving air may feel cooler than still air of the same temperature. 
It sounds like you are trying to heat the whole room. If so, the whole room will heat more evenly with better circulation, and this means the guy sitting next to the heater will not get warm as quickly. If I had brought the heater, I might prefer no fan.",null,0,cdnjfua,1ri6wl,askscience,new,3
Richard_Fitzsnuggly,"More information is needed as well as the previous responses.  Is the room a defined sealed space?  If not you will be attempting to heat fresh air instead of re-heated air as it circulates within the space.  The friction of the blades on the air does not impact the heat.  The speed in which the drag coefficient of the air on the blades versus the cooling affect of the ambient air, would need to be astronomically fast.
",null,0,cdnj58v,1ri6wl,askscience,new,2
expertunderachiever,"Ironically it could make the space around the heater hotter than desired as you cycle cool air over the heater basically nullifying the duty cycle [instead of shutting off for a bit it'll always be on].

From experience it will make the room hotter though.  I've used this trick in my basement on really cold days where you just need it to warm up.",null,1,cdniy0a,1ri6wl,askscience,new,2
DangerOnion,"Assuming an enclosed space with no open windows or anything, you'd be right.  The fan is producing a negligible amount of heat energy through friction, but it certainly can't reduce the temperature of the room.  He may be conflating the use of fans with the reason we use fans in the summer, which is that 1) moving air makes our skin feel cooler through evaporative cooling, but doesn't actually reduce the air temperature or 2) by circulating fresh outside air into a stuffy building, which doesn't apply here.  The temperature in one place might rise slower, but it's not reducing the amount of heat added to the room.",null,0,cdnuz1w,1ri6wl,askscience,new,1
WhoH8in,"Its completely aritrary. There is no objective way to identify ""up"". We choose the Earth's negative pole as north and assign that to the rest of the solar system and orient our images of other planets to that. If when cartographers started drawing maps they had placed the positive node on top then we would think of Antarctica as being Arctica. The only other way to get bearings in the solar system to orient yourself to the orbits of the planets. If you are looking toward the sun and the planets are going left to right you are oriented ""upward"" if they go right to left you are ""upside down"". But in reality none of this matters.",null,0,cdnj6g9,1ri48m,askscience,new,2
stuthulhu,"&gt;I'm visualizing the Solar System as planets orbiting the Sun in a flat disc. If we imagine that the disc is like a dinner plate, the standard view of Earth is that Antartica is orientated toward the bottom of the dinner plate. Is this actually correct?

Pick one. From a vantage point above the north pole of the Earth, the Earth would appear to revolve in a counterclockwise direction about the Sun. From a vantage point above the south pole, the Earth would appear to revolve in a clockwise direction. ",null,0,cdnjmdd,1ri48m,askscience,new,2
DangerOnion,"The plane itself is tilted pretty severely relative to the plane of the galaxy, making terms like ""up"" kind of meaningless in the first place.  But the simple answer is that you're right. The equators of most planets are roughly aligned with their orbital planes, and we invented astronomy so we get to decide which way is up :)  If we decide that Antarctica is ""down,"" then so is the hemisphere of Jupiter with the GRS in it.  Most pictures of the planets, despite being taken by satellites with no particular orientation, are rotated to be consistent with the way we visualize ""up"" in our solar system.",null,0,cdnv8pl,1ri48m,askscience,new,2
Astrokiwi,"The space between the stars in a galaxy is filled with a very thin gas - the ""interstellar medium"". Even between galaxies there is the ""intergalactic medium"" too. This means stars and galaxies aren't completely isolated in space - there's gas everywhere. This means you'll have matter and antimatter annihilating each other in the ""border regions"" between antimatter and matter galaxies. This would produce a constant stream of ~~\~2~~~1 GeV gamma rays in these border regions, which we'd be able to detect with our gamma ray telescopes. However, we don't see this.",null,2,cdnj4gy,1ri2no,askscience,new,9
rocketgolfer,"Antimatter functionally behaves the same as normal matter, it's just that there's much, much less of it and it annihilates as soon as it contacts normal matter. The parts of antimatter that are ""opposite"" are opposite only by convention (e.g. it doesn't matter whether we treat the electron as being positively charged or negatively charged, but it does matter that the proton has the opposite charge).",null,1,cdnkvjd,1ri2no,askscience,new,2
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,new,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,new,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,new,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,new,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,new,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,new,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,new,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,new,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,new,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,new,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,new,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,new,1
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,new,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,new,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,new,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,new,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,new,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,new,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,new,1
null,null,null,32,cdnftrg,1rhuln,askscience,new,29
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,235,cdnhkj4,1rhu7r,askscience,new,1077
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,33,cdnfpuh,1rhu7r,askscience,new,199
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,5,cdng9z2,1rhu7r,askscience,new,26
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,5,cdnkfb7,1rhu7r,askscience,new,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,0,cdng7o4,1rhu7r,askscience,new,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,3,cdnqm5k,1rhu7r,askscience,new,11
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,0,cdnni7u,1rhu7r,askscience,new,9
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,new,6
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,0,cdnj69w,1rhu7r,askscience,new,6
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,0,cdnfgcp,1rhu7r,askscience,new,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,new,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,0,cdnpywc,1rhu7r,askscience,new,4
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,new,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,2,cdnfmxv,1rhu7r,askscience,new,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,0,cdnguqo,1rhu7r,askscience,new,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,1,cdnril0,1rhu7r,askscience,new,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,new,3
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,new,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,new,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,0,cdnwy1g,1rhu7r,askscience,new,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,new,5
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,new,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,new,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,0,cdnpfal,1rhu7r,askscience,new,3
null,null,null,0,cdns99f,1rhu7r,askscience,new,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,new,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,1,cdnt98m,1rhu7r,askscience,new,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,new,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,new,2
Trill-Nye,"What do you mean by ""the two angles?"" Electrons will be diffracted by a crystalline material at a number of angles, each corresponding to a certain crystallographic plane with a reflection allowed by the structure factor. So each diffracted beam is due to a different d-spacing. the different n values in the Bragg equation correspond to higher order reflections, but these can generally be ignored. Does this answer your question?",null,0,cdnfqvw,1rhu16,askscience,new,3
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,new,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,new,1
Halysites,"Two reasons:

1. When a star is going through [fusion](http://en.wikipedia.org/wiki/Star#Formation_and_evolution), it will combine hydrogen to form heavier and heavier atoms. The basic reaction series would be: 

* hydrogen + hydrogen = helium
* helium + helium = carbon
* carbon + carbon = iron
* there will be other combinations of fusing atoms which would result in the creation of neon, silicon, etc. This is just a very simple list (I'm just a simple geologist and chemistry is not my speciality).

Once a star has burned all it's fuel to generate iron, it will undergo a supernova (or some other process, depending on it's size). Since iron was the last atom to be produced during the fusion process the star will be very rich with iron. If it's goes through a supernova it ends up ejecting most of it's material into the space around it; during this process heavier elements may form as well. So the space around it becomes enriched in a variety of elements, especially iron. This material is what will be used to form planets.

2. Planetary material begins to accrete from the rich stuff spewed out by a dying star. It is very hot and so the material is molten. As the planetoids get larger and larger, gravity becomes a stronger force. Gravity, coupled with a molten states, means that heavier elements are pulled towards the core of the planetary body quickly. Iron, being very heavy, will sink towards the core. Other heavy (metallic) elements will also sink to the core.

Viola, you have an iron-rich core for rocky planets. This process doesn't exactly apply to gas giants like Jupiter (which would be relatively depleted in iron).

Most of this information is from geology textbooks and courses I took in my undergrad. Although I imagine most of the info could be looked up on the internet.",null,1,cdnh1c9,1rhqp5,askscience,new,3
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,10,cdndirh,1rhpj0,askscience,new,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,new,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,1,cdnf7mk,1rhpj0,askscience,new,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,new,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,new,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,new,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,new,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,new,1
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,1,cdng2f1,1rhpdk,askscience,new,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,new,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,new,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,new,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,new,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,new,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,new,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,new,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,new,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing…
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,new,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,new,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,new,2
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,new,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,new,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,new,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,new,1
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,18,cdndsjx,1rhpc1,askscience,new,77
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,new,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,new,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,new,14
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,new,32
null,null,null,1,cdnfj49,1rhi55,askscience,new,10
null,null,null,0,cdngnvj,1rhi55,askscience,new,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,new,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,new,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,new,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,new,1
null,null,null,1,cdng7fo,1rhi55,askscience,new,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,new,2
xavier_505,"This is likely an issue with the way the MPEG streams are configured. Generally MPEG-2 encoding (I am not especially familiar with MPEG 4 but I would hazard a guess that it is similar, or at a very minimum has great flexibility in its configuration) uses three types of frames: I, B and P.

- I (intra) frames are full frames of data encoded similarly to JPEG (~7:1 compression). These do not reference any other frames.

- P (predicted) frames only have information describing the difference between the preceding I or P frame (lower size than I frames, ~20:1 compression).

Both I and P frames are called ""reference frames"" since the information they describe can be referenced by other frames.

- B (bidirectional) frames are encoded from interpolation of preceding/following reference frames (even smaller than P frames, ~50:1 compression).

These various frame types are typically sent in the following way:

    I B B P B B I B B P B B I P B B....

Where the distance between I/P frames (in this case it is 3) is configurable as is the distance between I frames (in this case there are 6).

Why am I telling you this!? Well, a P frame cannot be decoded without its referenced I frame, and B frames cannot be decoded without all of their referenced I/P frame. So, the longer the distances mentioned above, the greater affect a lossy channel will have. You would see the behavior you are describing in the following case:

    MPEG2 GOP structure (m=2, n=4): I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B ....

    MPEG4 GOP structure (m=4, n=16): I B B B P B B B P B B B P B B B I B B B P B B B P B B B P B B B ....

A lost I frame on the lower stream would blow away 16 frames of data, while on the upper frame only 4 frames. Similar for lost P frames.

Edit: Cleaned up description of IBP frames.",null,0,cdnfw8p,1rhf15,askscience,new,3
chucklesMtheThird,"A total stab in the dark here....it could be that MPEG4 channels are sent on higher QAM constellation carriers, which are more susceptible to minute changes in signal quality, and the MPEG2 are sent over lower constellation carriers?

For example, QAM64 receivers have a much higher tolerance range in amplitude and phase angle error than do QAM256 or QAM512 because of the lower symbol density. The higher you go in constellation density, the less room there is for error.",null,1,cdney2o,1rhf15,askscience,new,2
redallerd,"Yes. Since there is no limit as to how small fractions can be, there can be an infinite amount between two whole numbers. If you're finding it hard to understand, try adding halving fractions to see if you can get to a whole number for example : 1/2 + 1/4 + 1/8 + 1/16 and etc.",null,0,cdndbyb,1rhegr,askscience,new,10
Captain-Negative,"Yes. For example, consider all finite strings of digits beginning with ""1."" and ending with a varying number of twos.

1.2, 1.22, 1.222, 1.2222, 1.22222, etc.

If you count these numbers one by one, you'll notice that it goes on infinitely long. This is a type of ""countable infinity"" (alternatively, [aleph](http://en.wikipedia.org/wiki/Aleph_number)-zero or [beth](http://en.wikipedia.org/wiki/Beth_number)-zero), because it's an infinity arising from (surprise surprise) a list you can count through.

However, there are even bigger kinds of infinity -- each of which is ""uncountable"" -- one of which describes the number of numbers between 1 and 2. As it turns out, there's no way to come up with a method to list all numbers between 1 and 2 one-by-one simply because there are way-too-fucking-many of them. Perhaps paradoxically, though, you can show that describing all the numbers between 1 and 2 is just as hard as describing those between and 1000000, so the two infinities are actually said to be the same (specifically, it's called the ""beth-one"" infinity).

Very roughly speaking, it is unclear if there is an infinity between beth-zero (countable infinity) and beth-one (the type of incountable infinity we just discussed). Whether or not this ""beth-half"" exists depends on how you decide to model the world, and is the topic of something called ""[the continuum hypothesis](http://en.wikipedia.org/wiki/Continuum_hypothesis)"" in mathematics.",null,0,cdnee15,1rhegr,askscience,new,2
medstudent22,"There are several known benefits to neonatal circumcision. 

- **It prevents penile cancer.** Squamous cell carcinoma of the penis is exceedingly rare in circumcised patients. Circumcision alone may not be the preventative measure. [Phimosis](http://en.wikipedia.org/wiki/Phimosis) (the inability to retract the foreskin) can only occur in non-circumcised individuals and is associated with a higher risk of penile cancer. Phimosis, in many cases, is preventable with adequate hygiene. It should also be noted that penile cancer is extremely rare 1-2 out of 200,000 men per year. Also worthwhile to note that somewhere between 909 and 322,000 circumcisions would need to be performed in order to prevent one case of penile cancer. 

- **It reduces the risk of UTIs in early life and up to 5 years of age.** Uncircumcised males are 20x more likely to develop a UTI during the neonatal period. It should be noted that 111 circumcisions must be performed to prevent one UTI though. Some cost analyses have shown that there is still a cost benefit to performing circumcisions when just considering UTIs though.  
 

There are some claimed benefits of circumcision with varying amounts of evidence. 
 
- **It may reduce the spread of HIV** (to men, in heterosexual relationships). This is based on several large African clinical trials. It was not found to reduce the risk of transmission to women and has not been shown to reduce the risk of transmission in homosexual male couples. 

- **It may reduce the transmission of HPV and herpes (HSV).** In a study of 3393 men (1684 who underwent circumcision), after two years, 7.8% of the circumcised men had HSV-2 antibodies, 10.3% of the uncircumcised group did. In the same study, 18% of the circumcised men had evidence of HPV, 27.9% of uncircumcised men did. ([Study](http://www.nejm.org/doi/full/10.1056/NEJMoa0802556)) It should be noted that this study was performed in Uganda. Also worthwhile to note that most individuals clear HPV spontaneously and also that a [vaccine is available](http://en.wikipedia.org/wiki/Gardasil) for the most common HPV strains. Also worthwhile to note that HPV is associated with penile cancer, but more importantly cervical cancer in women.  

The reason I tried to note the conclusions which were drawn based on African studies is that the underlying prevalence of disease has an effect on the study and these results may not be considered generalizable to other populations.  

Multiple groups have issued statements on neonatal circumcision which may contain more information that may be useful to you. 

[The American Academy of Pediatrics](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCwQFjAA&amp;url=http%3A%2F%2Fpediatrics.aappublications.org%2Fcontent%2Fearly%2F2012%2F08%2F22%2Fpeds.2012-1989&amp;ei=FrGUUpKsOZHlsATz34HABA&amp;usg=AFQjCNGNikptx2aRUdOftngXQ3JyIFWU5g&amp;sig2=MiBQ3NWjASCvuKD_PjKXHw) states: 
&gt;Evaluation of current evidence indicates that the health benefits of newborn male circumcision outweigh the risks and that the procedure’s benefits justify access to this procedure for families who choose it. 

[The American Urological Association](http://www.auanet.org/about/policy-statements/circumcision.cfm) states: 
&gt; neonatal circumcision has potential medical benefits and advantages as well as disadvantages and risks.",null,4,cdne4ln,1rhdmd,askscience,new,17
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,new,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,new,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,new,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer – common)
8.5–8.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer – rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.5–8.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,new,8
null,null,null,3,cdn9iux,1rhdi6,askscience,new,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,new,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,new,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,new,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,new,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,new,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,new,2
Karnivoris,There is not much change at the bottom of the trench by inspection if you look at the size of the globe in comparison to the depth of the trench.,null,1,cdnrtdc,1rhdel,askscience,new,3
vashoom,"You can use Newton's Universal Law of Gravitation to calculate a decent approximation.  If the average radius from the center of the Earth to the surface is 6,371 km and the trench is 10.9 km deep, simply plug in 6,371 - 10.9 = 6360.1 km.

Crunching the numbers (gravitational constant times mass of earth divided by that radius (6360100 m) squared, gives me 9.8473 m/s^2.  So just a tiny bit above the average gravitational acceleration on the surface.",null,3,cdndqjv,1rhdel,askscience,new,3
null,null,null,5,cdnia0n,1rhdel,askscience,new,5
eebootwo,"As said by 1992^^?, not much different. However, depending on the object underwater, it might accelerate upwards due to buoyancy, or downwards faster than GM/r^2 if it is denser than water: which would be if it were a gas compressed to greater density than 0.998 kgm^-3",null,5,cdnnrfp,1rhdel,askscience,new,1
patchgrabber,"This is an unanswerable question. Different organisms mutate at different rates, we don't know exactly when life began (who knows how many different types of microorganisms were around near the beginning), the way a species is distinguished from another is inherently arbitrary, and we have no idea how many species have ever existed.",null,0,cdncaz4,1rhbru,askscience,new,6
biorad17,I've seen  estimates of this.  IIRC you only need one specieation event every million years or so to account for every species.  It's important to note that estimations like this are not necessarily biologically accurate.  They are mathematical models that provide parameters to begin thinking about evolution.,null,0,cdob0of,1rhbru,askscience,new,2
Hiddencamper,"In a nuclear reactor we use the fission process to release energy by splitting the atom. 

For the case of uranium235 the fission process looks roughly as follows

U-235 + n -&gt; Fragment1 + Fragment2 + ~2.4 n + energy

Those fragments are also known as fission products and are somewhat random in size. There is a statistical probability of what you can get. See the image at the top of this Wikipedia page. The fission fragments are where you get all the well known products like xenon, iodine, strontium, cesium, etc

http://en.m.wikipedia.org/wiki/Fission_products_(by_element)

Basically in a nuclear reactor there are fission products that are a result of splitting the atom, and there are also transactinides. What also happens, is the U-238 and U-235 can absorb neutrons but not undergo fission, causing them to become other heavy elements through a series of decay chains. ",null,1,cdnccjy,1rhamu,askscience,new,5
Proxymace,"In the uranium mined from the earth the ""active"" isotope makes up appx 0.7% this is enriched to appx 8-10% depending on the type of plant that will use it so there is a substantial amount of material that will be irradiated and will then decay into different elements to the ""active"" one",null,3,cdnbga6,1rhamu,askscience,new,1
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,20,cdnb7zo,1rh9np,askscience,new,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,new,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,new,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a “low heat” setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A “low heat” setting is more [energy] efficient than higher heat settings.    
&gt;7. A “less dry” setting is more [energy] efficient than “normal” or “more dry.” 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,new,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,new,2
null,null,null,2,cdnev7c,1rh9np,askscience,new,2
Platypuskeeper,"Two reasons. 1) Most chemical reaction rates increase exponentially with temperature. Water leaching into some stuck food, or something dissolving are chemical reactions. 2) The solubility of most (solid) stuff tends to increase with temperature.
",null,1,cdndlk4,1rh5ch,askscience,new,7
SimpleBen,"The viscosity of fats is dramatically altered by temperature. Think about it. Bacon fat in the package is nearly solid, but at around 200 degrees F it is pretty liquid. Fat changes so much with temperature that it is by far the dominant reason that warm water cleans better than cold (not to mention the fats in the soaps!) ",null,0,cdngody,1rh5ch,askscience,new,4
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,new,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesn’t grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,new,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,new,1
Platypuskeeper,"The color depends on the coordination environment of the Cu(II) ions that are formed. In a concentrated nitric acid solution, the copper ions coordinate to nitrate ions, giving a green/greenish-blue color. If the solution is more dilute (or diluted after oxidizing the copper), then you get a blue solution where the Cu(II) ions are coordinating to water instead.

And on a safety aside: Who the hell are these fools who play around with concentrated HNO3 outside of fume hood? That brown gas is toxic nitrogen dioxide!
",null,0,cdn9hbo,1rh4eg,askscience,new,4
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,new,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,new,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,new,3
EdwardDeathBlack,"Assuming you use the European convention of having a comma instead of a decimal point, you would get indeed 350,000 people. 

I find the idea of a 6.5 GWh plant weirdly low. A nuclear reactor can easily be a 1GW thermal, assuming 35% conversion efficiency, that's 350 MW electrical. Assuming 90% uptime , that'll be 365 * 24 * 350 * 0.9=~2800GWh. Most nuclear power plants have four or five reactors, so can easily generate 10,000GWh per nuclear plant per year. So a power plant with a total capacity of 6.5GWh per year certainly seems puny by modern energy use. Then again tidal is really not much of an energy source, more of a public relation toy , so maybe it is that puny.",null,1,cdn799s,1rh0eq,askscience,new,3
E_F_F_E_C_T,"So using this site for KWH/capita for china gave me 3,300 KWH/capita -http://data.worldbank.org/indicator/EG.USE.ELEC.KH.PC

Then using this site for the station's output - http://en.wikipedia.org/wiki/Jiangxia_Tidal_Power_Station

The instantaneous power of the station is 3,200KW (we'll ignore the solar stuff).

Multiply this by the amount of hours in a year gives you 28 GWH.

Dividing this by the 3300KWH/capita gives us roughly 8500 people.

Considering the second Wikipedia article states that ""The power station feeds the energy demand of small villages at a 20 km (12 mi) distance, through a 35-kV transmission line."" I feel that this isn't that unreasonable.

Hope this helps.",null,0,cdn7d47,1rh0eq,askscience,new,2
super-zap,"Compared to most other large power plants your favorite tidal power plant is tiny. 

http://en.wikipedia.org/wiki/List_of_largest_power_stations_in_the_world

It has 1000 times less generating capacity than most of the large ones and almost 6000 times less capacity than the largest power plant.

So, overall it is not surprising that it can generate power for only 350 000 people. I believe your math is correct.",null,0,cdn7dqd,1rh0eq,askscience,new,2
RelativisticMechanic,"&gt;Lets say you crush a planet down to mosquito size to form a blackhole.

Alright, we have a black hole of 1 Earth Mass.

&gt;Apparently it would evaporate really fast from your outside frame of reference.

Not really. The lifetime of a Schwarzschild black hole with the mass of the Earth would be about 500 trillion trillion trillion trillion years (as measured by those of us far from the event horizon for the duration).

&gt;But how could any effect pass over the event horizon to reduce the mass of the blackhole?

Nothing necessarily crosses the event horizon; rather, the curvature of spacetime near (but outside) the event horizon produces (nearly) thermal radiation that can be intercepted by those of us far from the black hole. In this process, the spacetime curvature relaxes, manifesting in a decrease in the surface area of the event horizon: the black hole shrinks.

One can, with suitable constructions, model this behavior as a tunneling process whereby particles from inside the event horizon tunnel out; this is analogous to other tunneling behavior wherein particles traverse a classically impenetrable barrier due to quantum mechanical effects.

&gt; I know that things can pass over the EH from their own reference frame - but not from an outside frame.

In fact they *can* cross into the black hole, even in a far-removed frame. The idea that they can't comes from an idealization where you neglect the mass of the infalling object (which we can reasonably assume is very, very small compared to the black hole mass). Even in that approximation, though, if we account for the quantization of light, there will be a final photon emitted from the infalling object. Once that photon is emitted, it will never again be seen by anything outside of the event horizon.",null,0,cdnag5o,1rh0ay,askscience,new,3
Daegs,"The simple version: Because the particle entering the event horizon has negative mass.

When the pair of virtual particles are ""created"", if one sticks around with positive mass, then the other must have a negative mass in order to cancel out (and they must cancel out, no free energy)

So the positive mass one shoots off away from the black hole, and the negative mass one enters the black hole which reduces its overall mass. ",null,0,cdn9pcd,1rh0ay,askscience,new,1
Nicked777,"The Hawking radiation is a deeply quantum mechanical effect, but here is an intuitive way to think about it. The uncertainty principle requires the creation of particle antiparticle pairs, everywhere, all the time. These particles locally violate conservation of energy, which is allowed in QM, as long as it happens on short enough time scales. This means the two particles annihilate very quickly, as if they were never there. 

The point of hawking radiation is if this happens very close to a black hole's event horizon, one of the particles can get sucked in, and the other will escape, albeit very reduced in energy from its trip. Because of this the Hawking radiation is believed to be very weak. A specialist in this topic could explain why it seems to be only the anti particles that fall in, and why we think this admittedly bizarre idea could me true, but I don't know off the top of my head. ",null,1,cdn9rz6,1rh0ay,askscience,new,1
claireauriga,"There are definitely equations that can describe what is going on! Heat and mass transfer are an important part of physics and engineering. 

In order to melt, the ice must be raised to its melting point temperature, then given enough energy to melt into liquid. This energy needs to come from somewhere. Heat moves from hotter to colder places, so the warm air will give energy to the ice (and water) until they are the same temperature. 

There are some complications in calculating all this, however. For example, if the air is stagnant then it will get colder as it gives up energy, which means transfer to the sculpture will slow down. If the air is moving, we also have to think about how fast it's going and if it's removing some of the water as vapour too. 

There are many more and less detailed ways of describing what's going on, but in the very simplest terms, the bigger the temperature difference between the air and the ice, the faster energy will transfer. The lower the ice temperature is below its melting point, the more energy needs to be added to make it warm up and melt. ",null,0,cdngq51,1rgzx3,askscience,new,3
StringOfLights,"Yes, it's possible to have multiple ova fertilized by sperm from different men. Sperm can live for several days, and multiple ova can be released over the course of several days in a single ovulation cycle. That means it's possible for more than one ovum to be fertilized and implant, resulting in a pregnancy of multiples with different paternities (I've only ever heard of this happening with twins, but triplets, etc., aren't impossible).

As DNA testing has become more common case reports have come out verifying the different paternities of twins. [Here](http://www.nejm.org/doi/full/10.1056/NEJM197809142991108) is an example from the 1970s, and [here](http://www.fertstert.org/article/S0015-0282%2897%2981456-2/abstract) is one from the 1990s. 

The phenomenon of having two ova fertilized in two seperate coital events is often referred to as ""superfecundation"". It technically refers to any instance in which more than one egg is fertilized in more than one act. Instances where the paternity differs is referred to as ""heteropaternal superfecundation"". [One study estimated](http://www.ncbi.nlm.nih.gov/pubmed/7871943) that 1 in 12 sets of dizygotic twins born to married white women in the US were the result of superfecundation, while 1 in 400 were the result of heteropaternal superfecundation.

Edited for clarity.",null,5,cdn6dbz,1rgzjd,askscience,new,30
sever0us,"A meniscus is caused by the ratio of the strength of the cohesive forces of a fluids molecules to each other and the cohesive forces of the fluids molecules to the container wall.

If a fluid has a higher cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a concave meniscus.
If a fluid has a lower cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a convex meniscus.

Since gels behave is a solid-liquid hybrid way, the presence or absence of a meniscus would most likely depend on the physical properties of the gel. It really depends on weather the cohesive forces described above are enough to deform the gels structure.

TL;DR: It depends on the gel. 'Fluid' gels such as shower gel stand a much greater chance of presenting a meniscus than 'solid' gels like ballistics gel.",null,1,cdn8dfo,1rgzf8,askscience,new,5
Dominus_,"When you're wiring your home surround system, no, pretty much not at all. But over long distances like on a concert where some cables run several tenths of meters, sometimes even a hundred meters, the resistance and interference has to be reduced, or else you're going to end up with artifacts and noise. ",null,7,cdnacxc,1rgzbv,askscience,new,46
thegreatgazoo,"For just about anything in your house, lamp cord is an excellent choice of speaker wire. Just make sure one side is marked so you keep the polarity correct. 

Anjou Pear speaker wires (and anything similar) are for delusional people who have too much money. 

",null,1,cdnckf0,1rgzbv,askscience,new,7
littlegreenalien,"yes.. and no. It's not so much the cable that's the problem, rather the interference it can pick up on the way. The longer the cable the more issues come into play (cable resistance, etc… as mentioned already). But at short cable distances it's mostly interference from power cables, and what not.",null,0,cdnb62y,1rgzbv,askscience,new,5
Kriemore,"Computer engineer here: wires are important, but if your question is 'should I spend $90 to get these cables I found at best buy for my home theatre?' Then probably not.

A bad cable will degrade audio quality significantly in addition to causing all manner of other problems with cutting out etc. 

Of course, expensive audio cables were famously compared to a coat hanger with no noticeable difference.


Now, if you're playing a massive theatre... these things start to matter a lot more.",null,0,cdngnf0,1rgzbv,askscience,new,4
jgrun,Ultimately there is always a degradation of signal quality when transmitting over a long distance. But since a digital signal is just binary 0s and 1s and you're only sending it 4 or 6 feet to the TV or stereo it doesn't matter. The receiving end will read the signal very clearly because it's hard to mistake a 1 for a 0.,null,7,cdn7wlm,1rgzbv,askscience,new,9
thisispointlessshit,"Not really. I just like getting cables that don't feel cheap... If that makes sense. The wire itself tends to wear over time if it has cheap shielding when I'm constantly coiling and uncoiling. Something with decent shielding usually lasts longer for me. For home use it might not be as much of an issue, because it's plugged in and never really moved.

In terms of sound quality? No difference.",null,1,cdng5vb,1rgzbv,askscience,new,3
lucaxx85,"You need to distingush three applications: 
1)analog signals in home setups
2)digital signals in home setups
3) live concert signals and similars...

For 1) everything works. Including coat hangers (for the power signals. You need shielding for line ones). The resistence and the impedence of such cables are such that they cannot affect in any way the final signal, which has a very low bandwidth. Only thing to be careful is to have cables large enough for the amp-speaker connection, if you have a very high power system (but I'd guess that you can still forget about this in any practical situation). 

2) Digital signals are more complicated. The bandwidth here is much higher, especially if you're also carrying video. That's why you have maximum lenghts and building them needs lots of care. Still almost any commercial cable is good if you're not trying to do something you shouldn't (e.g.: a 10 meters HDMI connection). In these case of course a 15'000 $ cable made from the finest rhodesian zinc, soldered in a full moon night by an african zoroastrian priest would work as badly as the cheapest one in the store.

3) For concerts and other applications cables can give actual problems. Still not those ""lamented"" by audiophiles. The first thing you look for in a concert cable is the *mechanical* resistance, especially of the connectors. Most of the cables break for a mechanical injury! Those things get torn everywhere. 
Then there is a problem with microphones/guitars signals. They're *extremely* weak. So they're sensitive to interferences. But, like before, those cables that claim to feature platinum in their alloy or even to have a special cristalline structure that favours the signal in a specific direction (how on earth would that work??!?!??!!) won't make *any* difference.   There are other tricks to solve the problem (balanced signals, preamplification before long transmissions etc...)

So you actually need a lot of care and you have a number of problems... But they're so not what the audiophiles claim!",null,0,cdni471,1rgzbv,askscience,new,3
Cyanmonkey,"I find the build of the cable more important than impedance rating, etc.

A properly built cable with Neutrik connectors and strain relief lasts much longer than your cheap Guitar Center POS, but as far as signal goes, as long as your not going over 200' it doesn't make a noticable difference.",null,1,cdnfrd9,1rgzbv,askscience,new,1
EvilHom3r,"For digital (i.e. HDMI), no it does not matter at all. Digital either works or doesn't, there is no in between.

For analog (RCA, speaker wire, TRS wires), you will always get better quality (even if just slightly) with a better wire. However for the average user they will probably never notice the difference, and more likely than not the quality bottleneck is elsewhere in the system.",null,5,cdncqhj,1rgzbv,askscience,new,3
Dyson201,"Not exactly an Audio Engineer, but this isn't a difficult question from a signals standpoint.

Transferring signals through a medium (cable) can pose a variety of challenges that are handled in many different ways.  Without going into extreme detail lets just say that electromagnetic forces could possibly come into play, as well as capacitance to ground producing noise in the circuit, etc. etc.

Long story short, if you're replacing a 3' cable for sound, I highly doubt you'll notice a huge dip in quality between $100 cables and coat hangers.  Both are capable of transferring the signal, and while the expensive cable will transfer the signal with a much greater Signal to Noise ration (SNR), at 3' and with modern noise abatement technology, you would be hard pressed to hear a difference.

Now that being said, I wouldn't wire up your home surround system with soldered together coat hangers, as distance plays a huge factor in the quality of the transmitted signal.  Also, if you buy a cheap ass sound system, expect to hear a big difference in quality between expensive and poor cables, even at 3'.  

Finally, audio quality sound is a very low frequency, and does not travel well over distances with a good SNR.  Quality cables are the only way to increase sound quality over distances (relative term, we're talking meters here not miles).  Technology has come a long way towards discerning the signal from the noise, but any reduction in noise is a huge positive in the quality of the signal.",null,11,cdn6sul,1rgzbv,askscience,new,6
generalelectrix,"This is a very vague question.

For digital signals, yes this does matter.  Digital audio signals require significantly higher bandwidth and run at higher frequencies than the audio content they encode, so transmission line effects become important.  If the characteristic impedance of the cable you use to transmit a digital signal is not matched to the source and destination, you can get partial reflections or standing waves on the cable, which can definitely cause errors in the reconstructed signal at the destination.  This becomes more important with longer cables.

For analog signals, the frequency is low enough that the characteristic impedance isn't really important.  So long as the conductors you're using are low-resistance (copper is great), coat hangers should work just as well as fancy cable.  Shielding in cables is important for line-level interconnects to prevent the cables from picking up noise from the environment, though this usually isn't too big of a problem in a home environment.

The only real exception to this is for speaker cables (carrying post-amplifier level signals) for electrostatic speakers, as the load they present to the driving amplifier is largely capacative.  Then the details of the impedance of the cable driving the speaker become a bit more important.

I'm a physics PhD in quantum electronics with a minor hi-fi addiction.

Edit: I give an in-depth and accurate answer and get a ton of downvotes?  SCIENCE!",null,19,cdn89ly,1rgzbv,askscience,new,5
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,2,cdn5cpe,1rgx9w,askscience,new,9
Physics_Cat,"In order:

Technically, yes. But the technical definition of temperature isn't what you think it is. More on that in a moment. 

Absolute zero is exactly the same as zero Kelvin. 

Who told you that the temperature of a black hole is absolute zero? That's certainly not correct. In fact, it's not possible for any matter to be at a temperature of exactly zero kelvin, due to the zero-point motion inherent in quantum mechanics. We can get incredibly close in a laboratory (somewhere in the range of hundreds of picoKelvin) but it's not possible to attain exactly zero kelvin. 

As for negative temperature: the colloquial understanding of temperature is something like ""temperature is the average kinetic energy of the constituent particles in a material."" That's a very useful tool for intuitively understanding things like heat capacity, but it's not the ""real"" definition. In thermodynamics, temperature is defined as the partial derivative of internal energy with respect to entropy (not sure how to format that symbolically, so I won't try). There are some kinda-convoluted, not-entirely-realistic examples of physical scenarios with negative temperature. That is, you add a bit of energy to the system, and the entropy goes down. For example, suppose you have N light switches, and each ""quanta"" of energy is represented as turning on one light switch. The entropy of a system is related to the number of configurations (microstates) that lead to the same macroscopic result, so let's say that you have N-1 switches turned on. Then the entropy is, more or less, N (since there are N ways to have N-1 switches turned on in a collection on N switches). Now you add one ""quanta"" of energy and turn on the last light switch. How many microstates are there now? Only one. There's exactly one way to have N out of N light switches turned on. Since we added a unit of energy and saw the entropy decrease, the system could be said to have ""negative temperature"" if you like. There are physical systems that come close to this analogy, but I think the ""light bulb scenario"" is easier to digest.",null,2,cdn51ib,1rgv22,askscience,new,9
fishify,"Absolute zero and 0 K are the same temperature.

Negative absolute temperatures are actually *hotter* than any possible positive temperature.  When you look at the mathematics, at any positive temperature, more energetic states are less likely to be populated than less energetic states (though at higher temperatures, the difference between those likelihoods is not as large as at lower temperatures); what you find is that if you had negative absolute temperature is that it would correspond to a situation in which more energetic states were *more* likely to be populated than less energetic ones.  (Lasers are a place where you might see such population inversions.)

Black holes have positive temperature, inversely proportional to their mass.",null,0,cdn54pg,1rgv22,askscience,new,4
auralucario2,"First, the statement about black holes is completely false.

Now, according to the law of thermodynamics, it is impossible to reach absolute zero, which is the same as zero kelvin. However, quantum mechanics butts its head in here and offers a workaround (kind of). It would be theoretically possible to achieve a temperature of some negative kelvin by having particles achieve a quantum state in which their entropy actually *decreases* as energy is added to the system. Needless to say, this doesn't exactly happen all the time, but it is possible.",null,0,cdnvd3b,1rgv22,askscience,new,1
brickses,"Zero kelvin and absolute zero are the same thing. There is no such thing as negative temperatures except in advanced thermodynamics exams.

Black holes are actually hotter than zero kelvin, like all warm things, they radiate (the same way humans radiate in infrared).",null,3,cdn4wu1,1rgv22,askscience,new,2
mingy,"I think you are right, but it is a minor error that probably got by the editors. I once read the final draft of a textbook written by a renowned expert in optics (long story) and found several errors (mostly units and arithmetic) and I knew maybe 1% of what the author had forgotten. He was grateful nonetheless.

In any event, even the 10 billion bits are wrong. The base pairs are grouped into 3s so you have 64 permutations, however this is not a binary or quaternary system, there are redundant codons and start and stop (http://en.wikipedia.org/wiki/DNA_codon_table) so there are 22 symbols of the 64 permutations.",null,0,cdn4k62,1rgu88,askscience,new,4
selfification,"Yeah that was a mistake in a way.  Each nucleotide base pair carries 2 bits of info.  So 5 billion base pairs carries 10 billion bits of info.

But there is the flip side that you have 2 separate sequences.  Each base pair is 2 codons.  Now you can consider that just 1 letter (because one of them precisely specifies the other) but I guess one could consider them 2 separate letters.  I mean...  2 copies of a file have twice the number of bits, even if the *information content* hasn't increased.  So in that interpretation, each base pair contains 4 bits of info...   and that would make Sagan's calculation make sense.",null,0,cdn4nl4,1rgu88,askscience,new,2
iorgfeflkd,"Each base-4 base can represent 00, 01, 10, or 11. So there is 4 times as much information as just binary.",null,4,cdn3qce,1rgu88,askscience,new,1
iorgfeflkd,"Yes, it's both. Just being still in a gravitational field (like we are now, on the Earth) causes time dilation relative to freefall, and orbiting satellites have to take both into account (this is the famous GPS relativity correction).",null,0,cdn3rku,1rgt18,askscience,new,4
Platypuskeeper,"There cannot be such a thing as a 'non-cohesive liquid'. A liquid is by definition a state where the attraction between the molecules is strong enough that the thermal energy is insufficient to let most of them leave the liquid. But unlike a solid, the molecules are still able to move about. 

If you have no intermolecular forces, you have a gas. 
",null,0,cdn6cb2,1rgslj,askscience,new,3
LoyalSol,"The curve itself, not really.  The function, definitely. There are so many uses it is hard to list them all. ",null,0,cdnph1q,1rgo2l,askscience,new,1
iorgfeflkd,"It's not changing its constant, it's just changing your units. If you use meter-kilogram-seconds unit then hbar is something like 10^-34 m^2 kg /s but if you use Planck units then hbar is 1, G is 1, and c is 1, and you can measure lengths in terms of (hbar G/c^3 )^(1/2), for example. This makes it easier to do theoretical work because you don't have to keep track of all these constants, but you'll have to do more work to get your results in measurable quantities.",null,0,cdn34jk,1rgmvd,askscience,new,14
MonadicTraversal,"&gt; Any faster or slower, closer or farther, or difference in direction of travel and the body would de-orbit, spiralling toward the planet or star it's orbiting or flinging off into outer space.

This isn't true. If you smacked a huge meteor into the Earth, you wouldn't knock it into the sun, you'd just change the shape of its orbit a bit. Spiral orbits don't actually exist under inverse-square forces such as gravity; you can show that the only possible orbits are circles, ellipses, parabolas, and hyperbolas. Spiral orbits don't exist except if there's some kind of drag force or whatever dissipating energy from the system; on an interplanetary scale drag doesn't matter. (Note that this is somewhat complicated by the fact that, e.g., Jupiter affects the orbit of the Earth, but in general the perturbations due to planet-planet interactions are small enough to not matter for stability purposes).

&gt; Isn't it difficult for us to keep our own man-made satellites in a stable orbit, requiring periodic adjustments? And yet, the moon is huge and it seems to be in an orbit that will last billions of years with no intervention.

Many man-made satellites are orbiting at an altitude where Earth's atmosphere can still exert some small amount of drag. The moon is so far away from Earth that the drag is essentially negligible. We also want the satellites to be kept in a *predictable* orbit; for a geosynchronous satellite, we want that orbit to be such that it's always above the same spot on the equator. The moon doesn't 'have' to be in any particular orbit, it just orbits wherever it orbits.",null,0,cdn0pyg,1rgi2m,askscience,new,9
iorgfeflkd,"For a circular orbit it has to be that precise, but many more initial configurations will lead to stable elliptical orbits, which are stable due to a balance of gravity and angular momentum. We live in a universe where the force of gravity decays with the square of distance, which is related to the fact that we live in three spatial dimensions. It turns out, there are only two types of forces that can produce stable orbits: inverse square, and linear (harmonic, like a spring). So, basically, we live in a universe where stable orbits can exist. Because of that, the fact that we do see stable orbits is not surprising.",null,0,cdn0bqk,1rgi2m,askscience,new,5
dirtpirate,"You seem to be misunderstanding the interaction. Comic book guy is asking for a very high number X, and mister Burns is retorting to Smithers ""Give hime Y"", where Y is much smaller than X. Thus a typical haggling scenario. 

The joke isn't that the two numbers are the same, just that instead of comic book guy saying ""I want a billion"", and Burns replying ""I'll give you a million"", they are instead using physical constants. ",null,0,cdnbei2,1rgi0o,askscience,new,7
iorgfeflkd,"The Faraday constant is the charge of a mole of electrons or protons, measured in Coulombs. Avogadro's number is 6x10^23 and a Coulomb is 6.2x10^19 fundamental charges, and the ratio is 96485 Coulombs per mole.",null,3,cdn0eae,1rgi0o,askscience,new,7
ecopoesis,"Metrics such as temperature describe the behavior of a system that is made up of components.  These types of properties are termed emergent properties because they are derived from the behavior of the system as a whole and are not observable if you were to look only at the components.

So, for your specific question, individual molecules do not have temperature.  They are not ""hot"" or ""cool"" exactly, although they do have energy that is zipping them around their surroundings.  Molecules with more energy will move faster and collide with other matter more frequently and with more force.  It is only when you begin to look at a system of molecules that ideas such as temperature start to be meaningful.  In that sense, a group of molecules with a certain amount of energy will correspond to a certain temperature.  If these molecules are ""hotter"" than other molecules, then they will be moving about much more rapidly and they will be less dense than the latter group of molecules.  Properties such as temperature and density are emergent from the system of molecules interacting with each other and interacting with their surroundings.",null,1,cdn5v46,1rgf8v,askscience,new,7
The_Evil_Within,"&gt;an area of hot air becomes less dense, and so it rises above colder areas of air. 

First, you need to look at it the other way around - hot air doesn't rise, cold air sinks.  As it sinks, it forces the hotter air upwards.

Now, think of a mess (and I do mean 'mess' for the imagery, not 'mass') of cold air, with the molecules fairly still and fairly dense.  Then, something heats up a bit of it near the bottom - what's going to happen?

The molecules of hot air will bounce around a lot more than the cold, and sometimes they're going to bounce up.  When they do, the less active cold air is more likely to fall into the gap than to move in another direction, and now there's nowhere for that hot air molecule to go because it will only bounce off the cold air molecule if it bounces downward again. (Transferring some heat in the process, but we can ignore that for the purposes of this explanation)

Multiply this by unimaginable numbers of interactions, and you end up with a column of hot air rising while all the cold air around it rushes in to fill the gap at the bottom.",null,4,cdn6q0h,1rgf8v,askscience,new,8
AltoidNerd,"&gt;&gt;But what if there was a single constituent molecule from that wood existing in the water? Would it float? It is neither densely nor sparsely aggregated, existing all by itself.

My reaction to this is no not really - a single molecule would have dynamical behavior that isn't familiar like the bouyant force example you gave.  I have no idea how to describe what that situation *would* be like - but I'm positive it would be invalid to treat it like a whole plank of wood floating.",null,1,cdn9dzb,1rgf8v,askscience,new,5
ramk13,"Wanted to add that at the scale of single molecules, static interactions are much more important than buoyant forces. A single molecule of wood will dissolve and behave like another molecule in solution. Even a few molecules of wood together will still be influenced by the hydrogen bonding between water and its external oxygen groups more than the buoyant force on the particle as a whole. All of this applies to your wooden plank example.

To answer your question: And if so, why do they act like an aggregated 'body' with those molecules around them, just because they are at the same temperature?

It's because even in air at atmospheric pressure molecules have a limited mean free path. In air it's [68 nanometers](http://en.wikipedia.org/wiki/Mean_free_path#Mean_free_path_in_kinetic_theory). That is that an oxygen or nitrogen molecule only travels so far before it hits another molecule and they bounce off each other. The molecules collide often enough that they influence each other over that short length. That influence leads to aggregate properties, as each collision redistributes kinetic energy.",null,0,cdnt7nx,1rgf8v,askscience,new,1
5secondstozerotime,"I do not think the rocket is directly launching into Geostationary orbit. Rather, it is going into a geostationary transfer orbit (GTO) that will then allow it to go into a geostationary orbit.

I cannot find a reason why this needs a window, however what you are saying about the rocket is wrong. 

[This article talks exstensivly about it](http://www.americaspace.com/?p=45686).",null,0,cdn8uw4,1rgf3r,askscience,new,4
Nicked777,"A little known consequence of orbital mechanics is that you must be directly under an orbital path to launch into it. SpaceX do not launch from the equator, so they cannot go straight into GEO, they have to start with a transfer orbit, and then do a plane change somewhere. They can only launch into their transfer orbit when this orbital path is directly overhead, which means waiting for the earth to rotate Cape Canaveral into the right spot, thus the launch window troubles. ",null,0,cdn973l,1rgf3r,askscience,new,3
neverdonebefore,"There is a bit more to it than that.  

In FWD cars, the front wheels are doing both the steering and applying the engine torque to the road.  And RWD, the rear wheels are only applying that torque to the road.  Essentially, your fwd cars are 'pulling' while rwd are 'pushing'.  

As you drive down a straight road, you are applying longitudinal force to the road to propel you forward.  As you enter a curve in the road, you add a rotational component to your travel.  The center of mass of the vehicle has to move laterally through the curve, while the vehicle itself has to rotate about that center of mass in order to be pointed straight as you exit the curve.  With a fwd vehicle, the direction of the force applied to the road by the tires changes as you turn your steering wheel, and the back wheels will follow in that path. Fwd vehicles have a tendency to understeer.  An object in motion wants to stay in motion: the inertia of the car in the longitudinal direction makes it want to keep going straight.  The tires want to follow the path on which they are pointed.  If the lateral acceleration into the curve cannot overcome the forward inertia, the car will understeer, or take a path with a larger radius than the curve.  In a rwd vehicle, the the tendency is to oversteer, or turn at a smaller radius than the curve.  This is because the force on the road by the rear wheels is along the path of the vehicles inertia.  The front wheels will want to follow their path around the curve, but the rear wheels will want to keep going straight.  This means it is easier to rotate about the center of mass.  This is how fish tailing and drifting (and spin outs) occur.
",null,1,cdn5t9m,1rgew2,askscience,new,8
wwarnout,"In a rear-drive car, when you accelerate, the center of gravity shifts toward the rear.  So, if the only consideration was getting good traction during acceleration, this would be the preferable configuration.

However, since most cars have engines in the front, a front-drive car will have better traction is slippery conditions because more of the weight is over the front wheels.",null,1,cdn0w9f,1rgew2,askscience,new,4
AltoidNerd,Take a shopping cart at the grocery store and compare pushing and pulling the cart.  This especially is useful if the back wheels of the cart don't rotate (in analogy to the car).,null,1,cdn9faf,1rgew2,askscience,new,2
Das_Mime,"The idea is that another star's gravity will tug the comet farther out at first, and then when the star passes (or just when the comet continues on its newly more-elliptical orbit), the comet falls back inward.

It should be noted that the Oort Cloud is very poorly understood, not really directly detected, and is basically used as an explanation for long-period comets. Most comets are on highly elliptical orbits, so even if several of them are perturbed by the same star, their orbits will be altered in different ways. Even if multiple comets are sent into the inner solar system in this way, they might arrive years or centuries apart.",null,0,cdn86xc,1rgeiw,askscience,new,3
Dyolf_Knip,"East takes you out, out takes you west, **west takes you in**, in takes you east.

The bold one is relevant here.  The star does attract the comet, but does so in a way to slow its velocity relative to the sun.  After the star passes by, the comet assumes an orbit suitable to its new velocity, which means it drops into the inner solar system.",null,0,cdne36w,1rgeiw,askscience,new,1
iorgfeflkd,"Nothing particularly interesting would happen. Light by itself isn't affected by temperature, and if the light is passing through a vacuum then temperature isn't a meaningful quantity. Depending on the medium that the passes through, its temperature can have effects on how the light absorbs it. For example, in an extremely cold dilute gas it is possible for the atoms to absorb light and stay in that configuration for a brief period of time, so the light is in effect trapped. This is the temperature's effect on the medium, however, not the temperature's effect on the light.",null,0,cdmyy7i,1rgcdh,askscience,new,4
stuthulhu,"Another thing to consider, even if the photons *could* be frozen you would not see your display freeze as though stuck in time. You would simply not see your display, since the photons responsible for creating that image are no longer able to *move* to your eye. 

An easy way to simulate what a room would look like if all the photons became frozen in space is to put a box over your head. ",null,0,cdndsia,1rgcdh,askscience,new,2
Das_Mime,"Light won't stop moving, even if it's going through a medium which is at absolute zero.

Temperature is about the thermal motion of particles which have mass, like electrons. The colder you get, the less kinetic energy they have. But light has no mass and its energy is proportional to frequency, so it usually doesn't make a great deal of sense to talk about light having a temperature in the same way that a physical material does (although a spectrum of light can certainly have a characteristic black body temperature, lower energy light doesn't travel any slower than high energy light).

Light, on the other hand, is comprised of massless photons. If they're passing through a medium (like water or air), then they will go somewhat slower than the speed of light in a vacuum. This change in speed can be affected (slightly) by the temperature of the medium, which is why you see effects like heat shimmers. Light can't stop moving, although [certain materials can slow it down to extremely slow speeds](http://www.news.harvard.edu/gazette/1999/02.18/light.html).",null,0,cdmz3om,1rgcdh,askscience,new,2
musubk,"Contrary to the other answers, the length of the day decreases at a near constant rate for most of the year. It isn't a sine wave, people! It looks more like a triangle wave with the points lopped off and rounded off. It superficially looks like a sine wave if you view it for lower latitudes because the amplitude is too small to see the shape, but try it for somewhere further north. Fairbanks, AK is a good choice. I just wrote a quick IDL routine to read the daylight hours tables the USNO website gives for a chosen latitude, [here it is for Fairbanks (65 North)](http://i.imgur.com/YMvVGf5.png).

And if you go even further north, like 85 degrees, [you get something silly like this](http://i.imgur.com/bHWfVqK.png).

The point being that the days don't start shortening at a slower rate as you would think for coming over the edge of a sine wave, the rate that days are shortening is actually constant over the majority of the year for a majority of the planet. This is still true at lower latitudes, and if you scale the graphs right you can see that:

[50 degrees latitude](http://i.imgur.com/na9TE3D.png)

[35 degrees latitude](http://i.imgur.com/8BLfwwu.png)

[20 degrees latitude](http://i.imgur.com/PrqgIpq.png)

[5 degrees latitude](http://i.imgur.com/OLRDXDP.png)",null,0,cdnbz0h,1rgcbc,askscience,new,6
iamtheonewhotokes,"As we approach Dec. 21 the days will shorten at a slower and slower rate. Similarly as you approach the summer solstice in June days will get longer at a slower rate the closer you get. And as you approach an equinox (in March or Sept.), the rate increases. 

See chart here: http://cycletourist.com/Miscellany/Length_of_day.html (the slope of the curve is the rate)",null,3,cdn12gy,1rgcbc,askscience,new,6
iorgfeflkd,"They're not actually instantaneous, they're just treating them that way because it's much simpler to do so in an intro to physics class. Real objects are made of compressible materials, and when they collide the objects deform.",null,0,cdmy3fx,1rgap2,askscience,new,4
cylon37,"Let's be clear here. Two events that are simultaneous in one frame of reference may not necessarily be simultaneous in another frame ONLY if the two events are separated by some distance. Conversely, if two simultaneous events happen at the same point in space, they are simultaneous in all frames of reference. A collision as described above is a single point in space-time. The two 'events' that you describe, A transferring momentum to B and B transferring momentum to A happen at the same location and are therefore simultaneous in all frames of reference.",null,0,cdn0i76,1rgap2,askscience,new,4
iorgfeflkd,If the mother and father were half-siblings.,null,3,cdmycqc,1rganp,askscience,new,8
ohheytherewhatsup,"No. Crossover events during Meiosis 1 are required to generate tension in the meiotic apparatus.  Without crossover, division will not occur, and crossovers cause mixing of chromosomes from the grandparents.  Each of your chromosomes is a chimera of your two grandparents DNA.",null,1,cdn9ihk,1rganp,askscience,new,5
laika84,"Although this would not add up to 50%, the child of a mother with Down's syndrome, (men are essentially infertile and women with DS can have a child but they are less fertile than those without DS,) there would be a 50% chance that the child receives the extra chromosome.

Since this chromosome resulted from a non-disjunction event in one of the grandparents, the child would have more than 25% of his/her genetic material from one grandparent.  Again, not 50%, but still interesting.",null,1,cdn6qe0,1rganp,askscience,new,3
null,null,null,0,cdn18w5,1rganp,askscience,new,1
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,new,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,new,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,new,2
CosmicWaffle5,"It's called positional alcohol nystagmus. Basically, there are these things in your ears called semicircular canals that are responsible for your sense of balance. The semicircular canals are supported inside of a fluid that is usually the same density as the semicircular canals, but when you drink alcohol it changes the density of the fluid surrounding the membranes and throws your balance system out of walk. 

http://en.m.wikipedia.org/wiki/Positional_alcohol_nystagmus",null,0,cdn7xz4,1rg8rs,askscience,new,7
Merrilin,"Anything with a temperature (a.k.a. all matter) is constantly emitting **blackbody radiation**. 

You can think of temperature of an object as being proportional to how much each constituent atom vibrates. The more intense it's vibration, the hotter it is. The short of it is that this vibration causes the release of a photon, which carries with it some energy from the atom, decreasing it's temperature. More on that if I ever get home. 

It so happens that the hotter something is, the higher frequency radiation, on average, it emits. That's why a piece of metal visibly glows when you make it very hot. At room temperature it is emitting light at a range of frequencies, but almost none in the visible light range. As you make the piece of metal hotter, it's blackbody radiation in the visible light range becomes significant enough that a human eye can detect it. 

So, no, matter cannot have temperature without also emitting some frequency of light. And there is no such thing as matter without temperature, so matter is always emitting light. ",null,0,cdmy126,1rg6wj,askscience,new,9
thumbs55,"Excellent question.

First of all what is heat and what is temperature? Are they not the same thing?

[Heat](http://en.wikipedia.org/wiki/Heat) is a measure of thermal energy (measurable in joules like all energies), it can be a measure of the ammount of (highly disordere heat typed energy) energy moving from one body to another.

[Temperature](http://en.wikipedia.org/wiki/Temperature) is a measure of the hottness or coldness of a body, two bodies with the same temp will not exchange any net heat and if one body is hotter than the other then the hotter will give energy to the colder in the form of heat.

&gt;everything I can think of that has heat also has light. Stars, lightbulbs, lava, fire, hot metal,

This type of light is called [black body radiation](http://en.wikipedia.org/wiki/Black-body_radiation).

&gt;Metal only emits light after it heats up past a certain temperature.

While it is true that the light becomes visible after a certain temperature is reached, the metal is actually always emitting invisible ""light"" (electromagnetic radiation) at any finite temperature due to said black body radiation.

All of space has [Cosmic microwave background radiation](http://en.wikipedia.org/wiki/Cosmic_microwave_background) which gives ""empty"" space a temperature. (It is not empty of [real] particles if you include the photons giving it said temperature).

The temperature of space (away from stars and such) is around 3 kelvin, so if you have something hotter than that it in space will get colder and if you have something colder than that it will actually warm up.

The heat energy is often stored in the energy of the jiggling of molecules. But for it to move from one place to another it mainly moves in the form of photons (light) but also phonons (sound). If you engineered a particularly exotic system that only exchanged energy in phonons then this system would emit heat with no light.

Nutrenos are also an example of a form of heat (they carry energy from the sun in a manner that is not work) nutrenos are not light. But many forms of nutreno generation would also produce photons.",null,1,cdmyhyr,1rg6wj,askscience,new,4
AltoidNerd,How about your hands.,null,1,cdn9ibx,1rg6wj,askscience,new,3
Chuk,"Metal only emits light after it heats up past a certain temperature. It can get very hot but still not be glowing. (That is, assuming you are only thinking of visible light.) Living creatures also emit heat without light, as do many other chemical reactions.",null,7,cdmxi6u,1rg6wj,askscience,new,3
uberhobo,There is no such thing as relative humidity above the boiling point of water.  It will all stay a gas in any proportion with air.,null,0,cdnb8t8,1rg6ug,askscience,new,3
whatsup4,it depends if there is something for the water to condense on. Basically think of it like cloud formation. Air high in the atmosphere can sometimes be super saturated and achieve higher than 100% rh because it is hard for the water to form droplets without a surface to form on. Given a large enough decrease in temperature you can see cloud formation.,null,1,cdnaiz3,1rg6ug,askscience,new,1
BoxAMu,"The energy of a photon is proportional to frequency, but this energy must match the energy of some transition in the absorbing matter.  The electrons in bonds in glass have transitions in the UV, but not the visible range.

This is the same reason why high energy X-rays are used for imaging: muscle and tissue are mostly transparent to X-rays, while the calcium in bones absorbs them.",null,0,cdmy00a,1rg4zy,askscience,new,4
therationalpi,"Basically a whistle is a resonator. You either have a Helmholtz resonator (like a beer bottle) or a standing wave resonator (like an organ pipe).

Driving the resonator is the variable airflow through the whistle. In most whistles you will have a hole with a blade shaped edge on it. When the edge is blown on, it creates turbulent airflow in the form of vortexes. These vortexes alternate from side to side in what is called a ""vortex street."" There's a good picture of that [here.](http://www.grc.nasa.gov/WWW/Acoustics/code/adpac/sample/CYLINDER_VORTEX_SHEDDING/) The alternating vortexes create a varying positive and negative acoustic pressure, setting up a wave in the resonator. The resonator, as a result, forces the frequency of the vortexes to align with the whistle's natural frequency. In this way the whistle amplifies the normally irregular vortex variations into a sound loud enough to be heard at a distance.

The reason you must blow at the correct angle is that the flow vortexes will depend greatly on how the moving air stream hits the blade. You must hit the wedge shaped part of the whistle fast enough to create unstable flow, otherwise the wave will not be generated.

Hope that helps!",null,0,cdmz9jk,1rg4zj,askscience,new,1
AltoidNerd,"It's puffy.  If highly energetic, roughly spherical.  

You can get fireworks to discharge in predetermined shapes by the way you pack the explosives.   By analogy, the shape of a space flame would depend likewise on the shape of the source,  

Spherical enough of course to feel good about 4/3 π r^2 in a physics calculation.",null,0,cdn9gny,1rg4lz,askscience,new,2
Nicked777,"The fire will indeed be spherical, this has actually been tried in Space before, it looks pretty cool (I'm on my phone so I won't link it.)

The flame changes colour because the lack of convection causes diffusion to be the dominant transport mechanism. Compared to a terrestrial flame this means the flame burns with more complete combustion, with less soot. (Glowing hot soot is the reason most terrestrial flames are yellow.)

Edit: More information here: http://carambola.usc.edu/research/microgravity.html
",null,0,cdn9k0r,1rg4lz,askscience,new,1
do_od,"Buoyancy is a force acting on a body as to oppose gravity when that body is immersed in a fluid. This force is proportional to the weight of the volume of fluid displaced. In zero gravity, the fluid has no weight and there is no direction in which buoyancy could act. Buoyancy requires gravity... or more generally a reference frame under acceleration. Example: If you spin a bucket of water on a string in outer space, a ball could be buoyant in the water. That would not be useful for propulsion though. ",null,0,cdmv0go,1rfxwf,askscience,new,10
blacksheep998,"Here's a good video answering your question. http://www.youtube.com/watch?v=bgC-ocnTTto

In it an astronaut places an alka-seltzer tablet into a spherical water drop. Without gravity the only major force affecting the bubbles is surface tension, which causes most of the bubbles to combine with each other and eventually form one large bubble in the middle of the water sphere.

There's also this video, http://www.youtube.com/watch?v=QPf5MJluhvo in which an astronaut injects an air bubble into a water sphere, and then injects small water droplets into the bubble.",null,0,cdn17rs,1rfxwf,askscience,new,4
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,4,cdmv2r3,1rfwje,askscience,new,45
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,3,cdn6q1y,1rfwje,askscience,new,14
Das_Mime,"&gt;I recall reading something along the lines of observing the orbit of any natural satellite of the object, but a more detailed explanation would be nice. 

If you can see a natural satellite of the object and you can reasonably assume the satellite to have much much lower mass than the planet*, then you can use mechanics to work out the host's mass. In this case I'll also assume a circular orbit, but you can also work out the mass from non-circular orbits.

The force of the planet's gravity on the moon is **F = G m*_p_* m*_m_* / r^(2)** where G is the gravitational constant, r is the orbital radius, and the m's are the masses of planet and moon. In the case of  circular motion, the force on the moon is equal to **F = m*_m_* v^(2) / r** where v is the orbital velocity of the moon. Set these forces equal to each other, and you get:

**G m*_p_* m*_m_* / r^(2) = m*_m_* v^(2) / r**

Canceling out common factors, you get

**m*_p_*  = v^(2) r / G** 

So if you know the distance of the planet and it has a moon (which for Solar System objects can be readily obtained via parallax methods), then you can directly calculate the planet's mass. 

Calculating the mass of a body without natural satellites is a bit more work. Prior to the Space Age, Venus and Mercury's masses were not well constrained, because the best way to measure mass is to measure its gravitational effect on other objects. Venus also exerts a gravitational influence on other planets such as the Earth, and so if you have sufficiently accurate position measurements of both bodies and if you know Earth's mass then you can calculate Venus's mass, but this is still not an ideal method.

Our best measurements of Venus' mass come primarily from spacecraft like the Mariners 2, 5, &amp; 10 (American) and Venera (Soviet) probes sent to Venus. [From analyzing their trajectories](http://adsabs.harvard.edu/full/1968AJS....73R.162A)--some of them were flybys, some were orbiters (e.g. Soviet Venera 15 &amp; 16, American Magellan and ESA *Venus Express*), and some have landed on the surface--you can determine Venus' mass to a high level of accuracy, but in the end this is essentially the same method as the first-- measuring the planet's tug on nearby objects. 

Finally, you can make guesses at the composition of Venus, and since its radius is easily measured with a telescope, you can get an estimate of its mass. This is much less accurate, of course, since it depend entirely on the accuracy of your guess about the composition. 

*true for all Solar System planet/moon pairs except the dwarf planet Pluto and its moon Charon, which is about 12% of Pluto's mass
",null,0,cdmyrqu,1rfuon,askscience,new,3
Ejb90,"Even for the simplified case of a planet-star system there are a few ways to find the mass of a planet. I'll describe a common one, [Astrometry](http://en.wikipedia.org/wiki/Astrometry).

From observations we can usually deduce the distance of the star from earth, the ""apparent magnitude"" (how bright it is to us) and the spectral class - what types of elements it's made up of by looking at the light received.
We can also find the period of orbit of the planet around the star by several methods - the transit method is most popular, though the Doppler shift method and others are used dependent on the circumstances).
The next part is the difficult part. The velocity of the star wobbling backwards and forwards must be measured. 
The planet doesn't actually orbit a stationary star - they both orbit their combined centre of gravity, though for the star, which is much more massive, this is relatively close to its centre of gravity. Hence the star itself wobbles backwards and forwards. This speed can be measured from earth via the doppler effect - the light when the star is shifting towards us is shifted slightly up in frequency and then when it is moving away gets shifted down slightly. This can be used to calculate the speed.

From the distance and apparent magnitude we can calculate the ""absolute magnitude"" - how bright it it from a standard distance (30ly IIRC). Using these and some hefty thermodynamics/ fluid mechanics/ stellar structure knowledge (or the simplified [mass-luminosity relation](http://en.wikipedia.org/wiki/Mass%E2%80%93luminosity_relation) or extrapolating roughly from the [Hertzsprung-Russell diagram](http://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram)) to find the mass of the star.
Also from the period of or it we can use Kepler's third law to find the radius of orbit.
Now, the speed of the body can be calculated as the distance it travels and the time it takes is known, and the speed and the mass of the star are known.
Finally these can be used with the conservation of momentum to calculate the mass of the planet.

This technique has several issues. Firstly, this only gives a lower limit, as the orbit may not be head on, so the star may be moving faster than expected. Also, some of the measurements needed aren't possible in some cases. Also, it must be noted that Kepler's laws and the mass determination of the star isn't exact. Finally the issue of having more than one body in the system. Because there are a large amount of bodies in the system, the equations aren't analytically solvable, so there is some error in the determination process.

The mass of moons we can observe is calculated much in the same way, using the planet as the main mass.
I'm not sure what you read about observing moons of planets. This certainly isn't possible with exoplanets - we're only just on the verge of being able to see the very biggest exoplanets as off this year.
",null,0,cdmxscf,1rfuon,askscience,new,2
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,new,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,new,5
siliconlife,"Actually what you suggest does happen, but it's not called subduction because continental crust is too buoyant to descend into the mantle like cold ocean crust.

The Himalayan orogeny actually is so intense that a process called [underplating](http://www.sciencemag.org/content/325/5946/1371/F2.large.jpg) actually takes place. Underplating is the positioning of crust or magma beneath an overriding crust. In the case of the Himalayas, the Indian continental crust is being thrust so strongly that it ends up completely beneath the Eurasian crust. [Link to paper](http://www.sciencemag.org/content/325/5946/1371.abstract)",null,0,cdmvwch,1rfs95,askscience,new,6
oloshan,"The Indian plate is indeed being subducted under the Eurasian plate. The Himalayas are the uplift of Eurasian crust, not Indian crust - although their elevation is certainly enhanced by the effects of having the Indian plate shoved beneath the Eurasian at the same time. But not only was the Indian plate subducted, the speed of the collision may have actually driven it deeper than typically subducted plates (probably meaning that it has had less time to melt since being subducted, and so can still be discerned beneath the Eurasian plate).

In addition, a fair amount of lighter continental sediments were essentially ""scraped off"" onto Eurasia by the collision, during the initial phases when the Tethys Sea closed. A similar process happened along the Pacific plate margins as well, and has contributed to the formation of Alaska, Japan, and other ""accreted"" terranes.",null,0,cdo7rsl,1rfs95,askscience,new,2
fastparticles,"The Himalayas are being lifted at least in part by this collision, however we do not have a specific mechanism worked out for it. The difficulty with this collision is that this is a continent on continent collision, and both are very buoyant. When you think of a traditional subduction zone you have oceanic crust hitting continental crust, and the oceanic crust is denser and so it sinks. In this case both are continental crust so there is little/no density contrast and India can't just sink.",null,2,cdmvdf1,1rfs95,askscience,new,3
hikaruzero,"It's pretty simple -- photons alone aren't the cause of attraction/repulsion.  It is the *fields themselves* that cause charged objects to attract or repel eachother.  Photons are created when charges accelerate, but if you have a bunch of stationary charges and no actual photons, those charges will still begin to accelerate and attract or repel eachother without emitting or absorbing any photons amongst themselves.

In the context of perturbative theories, this effect can be explained by saying that the vacuum is filled with virtual photons, and that the virtual photons end up exerting a force on the stationary charges.  But virtual photons are not detectable the way real photons are, and also virtual particles do not appear in non-perturbative treatments of electrodynamics, so it is something of a matter of debate whether they even exist at all (indeed in the theory of the strong force, perturbative calculations frequently end up being *wrong*).  Virtual particles can be thought of as simply a mathematical tool for calculating approximate answers -- it's best to just say it is the *fields* that cause charges to accelerate.

Now, real photons themselves are *disturbances* of the fields, and if the fields change, that will cause a change to the acceleration of a charged object, so real (detectable) photons *do* accelerate charged objects, but strictly speaking it is the field that is ""doing the work,"" the presence of photons isn't necessary for attraction and repulsion -- it's not like there have to be a bunch of photons flying around from one particle to the next in order for charged objects to accelerate (though if they are flying around and being absorbed or emitted, they will change how those charged objects are moving via transfers of momentum).

So it's the fields that do the acceleration, whether you want to interpret fields as being made up of virtual particles is something of a matter of philosophy, and not something that experiment can tell us is definitely true or false.

Hope that helps.  Some further (but more technical) reading:  [Wikipedia:  Static forces and virtual-particle exchange](http://en.wikipedia.org/wiki/Static_forces_and_virtual-particle_exchange) and [Wikipedia: Force carrier (particle and field viewpoints)](http://en.wikipedia.org/wiki/Force_carriers#Particle_and_field_viewpoints)",null,0,cdmsq9j,1rfqqd,askscience,new,5
aziridine86,"Wikipedia is an OK place to start, but I believe that the most basic answer to the 'why' question is this:

The hydrocarbons that we get from the earth come in a huge variety from gases like methane, ethane, and propane, all the way to thick waxes and tars. 

Because of the prevalence of internal combustion engines used in cars, trucks, planes, etc., we have a much higher demand for gasoline, diesel, and jet fuel than for other hydrocarbons which are heavier or lighter. 

Cracking is one way we can turn less desirable hydrocarbons like high-boiling petroleum into more desirable products such as those used in gasoline. 

If your talking specifically about using kerosene as the feed stock, then the products will contain larger amounts of small (C2-C5) products. For example, [this](http://pubs.acs.org/doi/abs/10.1021/i200024a026?journalCode=iepdaw) paper (full text not free) says that cracking of kerosene yielded significant amounts of ethene, propene, butene, and butadiene. 

These chemicals have many different uses, but a major use of this class of chemical (often called olefins) is to make plastics like polyethylene and polypropylene. ",null,0,cdnc9mz,1rfpcs,askscience,new,3
sf_torquatus,"The products of catalytic cracking are smaller hydrocarbons. The catalyst (usually a strong acid zeolite or precious metal) cleaves the C-C bond. You will find a distribution of products corresponding to the temperature, pressure, and catalyst. Kerosene itself is a product of catalytic cracking. One would want to crack it further to produce smaller hydrocarbons. 

Regarding the ""why"" - I'm a bit sketchier on these details, so I'd ask anyone with a better understanding to pitch in. Kerosene is used as jet fuel, and I found a patent that described cracking kerosene to yield gas-phase products, but I don't understand the advantages of such a process versus fuel injection, unless such a process improved the injection in some way.",null,0,cdmu5xz,1rfpcs,askscience,new,2
Surf_Science,"Everything in your cell is doing what is thermodynamically favourable. Proteins involved in transcription bind to a gene because that binding is favourable, they function because that is energetically favourable. The produced proteins bind each other causing actions that occur because those are also energetically favourable. ",null,0,cdms0zn,1rfooz,askscience,new,3
sparky_1966,"DNA alone can't determine what a cell does, you can think of it as storing information. That information can be turned into RNA, some of which regulates genes, proteins and a few specific reactions. The proteins handle most of the actual work.

So, in the simples example if a bacteria that can use multiple sugar types for energy is sitting in an environment that has no lactose sugar, it usually doesn't waste energy making enzymes to break it down. If suddenly lactose becomes available, a receptor protein can bind the lactose and either activate transcription of lactose digesting enzyme from the DNA, or more commonly in bacteria, change shape and fall off the DNA, allowing the gene to be expressed. As the enzymes break down all the lactose, eventually the receptor protein wont have any to bind to, and will switch shape again to turn off the gene. There are many other levels of regulation, but that's the simplest example.

As far as viruses, there are a number of different strategies they use to take over a cell. Almost never is it just a piece of naked DNA floating around, since the environment and cells are full of enzymes to destroy those fragments. Probably the easiest system to understand is a DNA virus with a protein coat. The protein coat protects the DNA, but also makes sure it gets delivered. The protein is usually shaped to bind to the bacteria or cell it infects. On binding, the proteins change shape and make a path through the cell membrane for the DNA to get in. There is energy stored in the shape of the protein and the winding of the DNA (taken from the last cell) that allows injection of the DNA without other sources of energy. Once in the cell, the DNA gets replicated and transcribed in to viral proteins and more viruses like any other DNA. That's the simple version, there are any number of different virus types, some use DNA, some RNA, some large viruses carry most of the proteins they need to begin replicating so they can shut down most of the hosts protein production, etc.  ",null,1,cdmsir9,1rfooz,askscience,new,1
darksingularity1,"Technically it's not DNA. That determines what a cell does. Think of it as a master blueprint for a house. It contains a great idea, but it's not actually contributing to the building of the house. The workers (proteins) are who/what do everything. The architect might be the direct liaison to the blueprint. He reads it and converts it into instructions for a worker function. Technically new workers are created in the analogy sense, but I'm sure you get what I mean. The proteins are what actually create changes in the cell. In fact, certain proteins even act on DNA to control the expression of other proteins. The DNA does nothing.",null,0,cdndkh6,1rfooz,askscience,new,1
chrisbaird,"You seem to be confusing length (m) with acceleration (m/s^2) which are different things. If gravitational acceleration is very small, that does not imply there is anything in the system with a small length scale. It just means the gravity is very weak. Quite the opposite case is more important actually: quantum effects and gravitational effects should intersect when there is a large amount of gravity in a very small volume (such as in a black hole).",null,0,cdmt1ks,1rfo90,askscience,new,1
steeeeve,"If you had a rigid bottle, a difference in pressure would build up as you travel further into the ocean. This difference in pressure will cause greater net forces on the water at the mouth of the bottle, causing it to enter the bottle more rapidly. The amount that this happens will depend on some complex fluid dynamics, as the air needs to leave the bottle as well as the water entering it. 

If the bottle were pressurized with air so that the pressure was at equilibrium between the inside and outside of the bottle at the bottom of the ocean, then only the difference in buoyancy will cause the bottle to fill, similar to the case of a few feet of water. In this case, the amount of time would be similar for both cases, though perhaps not exactly the same due to changes in the viscosity of the water and air at those pressures.

The collapse of an air-filled bottle would depend on what kind of bottle is being used. For a typical soda bottle, the bottle can be collapsed just by sucking the air out of it (say, using your lungs). This means that a pressure difference of less than one atmosphere will cause the bottle to begin to crumple. The pressure increases with depth at ~1atm/10m of depth, so the bottle would crumple long before reaching the bottom of the ocean.",null,0,cdmxqyd,1rfo1d,askscience,new,3
creepy_old_grampa,"Police Radar is tied to their speedometer and decremented from the total, Source, I used to convert old cop cars to taxis, and I could always find the speedometer signal wire spliced under the dashboard already when I went to put in a meter.",null,0,cdmttof,1rfltg,askscience,new,6
EpicEvslarg,"So a car is travelling at 100 km/h North

A police car is travelling at 100 km/h South

The relative velocity would be 200 km/h

So the police radar would either know what speed the police car is going at, and automatically calculate the velocity of the other car, or the policeman would have to do it in his head by looking at the radar, and his speedometer. 

In this example the radar would either say 100 km/h or 200 km/h, so it would be easy to calculate.

I hope I solved your question.",null,0,cdmsce3,1rfltg,askscience,new,3
Dyolf_Knip,"That's just a matter of subtracting out their own velocity.  The real problem is angles.  If you were traveling at 100 mph perpendicular to the beam of the radar gun, it would register your speed at basically zero, because it only measures relative velocity along the path of the beam.  Any deviation from that decreases the measured speed.  So what cops do is position themselves as much as possible such that are directly in the path of incoming traffic.  I.e., right at a bend in the road, on an overpass, etc.

Area radar systems get around that limitation by being smart.  The radio beam can't really tell you how fast something is going, but it can tell you where it is.  An attached computer says ""5 seconds ago it was there, now it's here, x miles away, ergo it's moving this fast"".",null,0,cdnjjys,1rfltg,askscience,new,1
Doener_wa,"I can tell you something about the Langmuir isotherm. To get to this equation you have some asumptions to make: first is you have an isotherm system, which means your temperature is constant and second your gas which will be adsorbed formes a mono-layer on your surface (there are equations which involve multi-layer adsorption). Therefor you get the coverage of your surface and your Langmuir-isotherme can describe how much you may adsorb until your surface is fully covered. Also Langmuir isotherms are used to describe how well an adsorber adsorbs a specific gas or a mixture of gases (all will adrob differently). This is very useful because you are now able to characterize reactions which are done using a (heterogene) catalyst or to cunstruct an adsorber like it is used in many chromatographie-applications. 
To your Freundlich isotherm: I think this must be a similar concept just using other asumptions.
I don't want to go in detail now, if you have further questions, just ask on.
Source: I am a graduated chemical engineer and I am currently visiting a lecture about adsorption.",null,0,cdniulx,1rflnd,askscience,new,1
openLIKEeuchromatin,"The WHY part of the question:
First think of it in terms of fitness (this is always a good idea when navigating through these types of organismal biology questions). The number one goal for life from a biological perspective is to reproduce and pass on your alleles. With that in mind, try to think of why these birds have all grouped together and are ""chatting"" away. Keeping fitness in mind (#1 goal in life is to reproduce) you know that the grouping and social communication behavior of these birds must be important in order pass on their alleles. Since these behaviors (phenotype) are important to the survival of the crow species, then they must have evolved via natural selection.

The HOW part of the question:
Birds calls have evolved for millions of years acted on by natural selection. The chirps, coos and shrieks you hear everyday are a product of that. Many birds have developed a communication system that allows them to recognize individual calls within that population. Much like humans can tell the difference between each others voices. Look at it from the birds perspective. Birds have very sensitive ears and a respiratory system with many airways that allows them to make the complex calls. Try not to fall into the ""anthropomorphism trap"". A large crowd of crowing birds in the eyes of a crow is very different than in the eyes of a human. A ""noisy crowd conversation"" from a humans perspective is loud and hard to decipher what an individual is saying (i.e. sporting events, concerts, etc.). This is not the case for birds. Some birds are able to pick up on some of the slightest changes in frequency to hear exactly who is calling and what the call is about (i.e. food, mate, predators, etc.). Many birds have a critical period during development where they learn specific calls usually unique to that  population. Some bird call are even genetically ""hardwired"" and do not require learning. Not all birds are social though and communication does vary from species. 

Last point:
Calling and crowing is not the only way birds communicate. A wide range of behavioral displays are used in junction with the calls in order to send a complete message to another bird (the receiver). 
",null,0,cdn0pf1,1rfkwr,askscience,new,3
chrisbaird,"Not enough to notice. You can test whether gravity has any noticeable effect easily. Pluck a guitar string whole holding it upside down and see it sounds any different from when plucking it upright. The relevant force for these instruments is the tension in the strings and drum membranes, which is enough stronger than gravity that you can ignore gravitational effects. 

Note you only asked about lack of gravity. I am assuming you mean there is still normal air pressure provided by a pressurized compartment. If there were no air, or lower air pressure, than that would definitely effect sound propagation. ",null,0,cdmt7yy,1rfkjv,askscience,new,3
lvachon,"An acoustic guitar has been on the ISS for a while.  According to Cmdr Hadfield, the only thing that required changing was his play style since he no longer had the weight of his arm to help move down the fret board.

Source : http://www.youtube.com/watch?feature=player_detailpage&amp;v=gWTndmDHZQc#t=59",null,0,cdn9no2,1rfkjv,askscience,new,1
Ocaiman,"No, plants cannot survive without oxygen.  They respire on O2 just like any other living thing.  O2 is a byproduct of energy production using photosynthesis and plants eventually give off more O2 than they take in to breath.

To your question, a plant needs oxygen to germinate and grow until it begins photosynthesis.  They do not store O2, thus they cannot live in an atmosphere of CO2 or they would suffocate (the O2 would diffuse into the environment).  They can live in a clear sealed container with access to light, as they continuously reuse the CO2 and O2 that was sealed in with them, but they reach a limit in growth.",null,0,cdmsibb,1rfkeu,askscience,new,5
iorgfeflkd,Its engine was cut off a long time ago. It is on a trajectory that takes it beyond the solar system.,null,2,cdmr7xs,1rfk5l,askscience,new,30
Gprime5,"I think you might have misinterpreted something because your description doesn't make sense.

Voyager 1 doesn't have any actual engines, only small thrusters that keep it pointed towards Earth. The craft is in a hyperbolic trajectory meaning it has enough velocity travelling away from the sun that it will never come back.",null,1,cdmrakq,1rfk5l,askscience,new,10
PorchPhysics,"http://www.jaymaron.com/asteroid/tour-l.jpg

As the others said, its on a hyperbolic path out of the solar system.  This means its its velocity is greater than or equal to the escape velocity required for the sun.  

As for your idea that ""we're always moving around something"" is not really true at all, but in the case of voyager, it is now and interstellar probe, no longer orbiting our sun or being considered part of our solar system, it now orbits the galactic center.",null,2,cdms1nn,1rfk5l,askscience,new,10
Osymandius,"You're right - there are lots of ways to kill bacteria. Antibiotics are selective ""weapons"" against bacteria which is why they're so important. Because they're specific to bacterial components, they're safe to give to patients without destroying their own cells.

Let's take another example of a way to kill bacteria: heat. Most bacteria give up at about 50/60^o C, some thermostable bacteria (see T. aquaticus) are good for a bit more - up to 85/90^o C. Yes - all antibiotic resistant bacteria will be killed by a 100oC burst, but then you've got the put the patient through that! 

Take any method that will kill bacteria that isn't antibiotics, and it'll probably do some damage to the host. Irradiation, particulate disruption, salt membrane disruption, electrostatic membrane disruption, intense dehydration etc.",null,1,cdmradq,1rfk3g,askscience,new,10
thedveeeee,"There's actually only a fairly limited number of ways to kill bacteria. To list a few, you can kill them through targeting protein synthesis, targeting DNA replication, and using cell wall synthesis/growth inhibitors. Some newer antibiotics are being produced that target ATP synthase, an enzyme that produces ATP for the bacteria.

Unfortunately, the specificity in these antibiotics lies in the fact that we can't administer compounds that are toxic to human cells. Many antibiotics (like methicillin) are mildly toxic to us so they must be modified. That being said, it takes years and millions of dollars to come up with solutions to these problems. 

Edit: To touch on antibiotic resistance, and this is a very simple explanation; when bacteria are exposed to sublethal doses of antibiotic, selective pressure can cause a change in their genome, in which the most advantageous traits are passed on. This leads to strains of bacteria that are resistant to antibiotics, and these bacteria can pass their advantageous genes on to other bacteria. You may have heard of the incredibly famous MRSA group of bacteria; Methicillin Resistant Staph Aureus. This is a strain of Staph aureus (a natural flora of bacteria found on your skin; it's very common) that has evolved to resistant methicillin antibiotics. ",null,13,cdmsn6g,1rfk3g,askscience,new,21
justin3003,"The big problem is that there are only so many ways to attack bacteria effectively. Many of our antibiotics center on attacking replication or protein synthesis, two areas of significant difference between humans and bacteria. This makes most modern antibiotics much less toxic to humans than they are to bacteria. Also, some bacteria are totally resistant to many antibiotics simply by their biology (ie. the drug cannot interact with it, etc.), limiting the available options to only a few drugs.

Unfortunately, because we only have these limited points of difference, antibiotic use over time tends to lend itself to the selection of bacteria that are not able to be killed by these mechanisms. As these elements become more resistant, we have more and more limited options to further address this problem. It is further compounded by the fact that antibiotics are not specific to the pathogen you are trying to treat; to eliminate one infectious pathogen you bathe all of the other bacteria in your body with the same drug. Thus you don't just drive resistance of pathogenic bacteria but also harmless bacteria in your body that, under the right circumstances, may become harmful. 

So, to get to your question, that is why we are terribly worried about antibiotic resistance. Bacteria are a constant presence in the environment and evolving faster than we can create effective, tolerable treatments.",null,9,cdmsg9e,1rfk3g,askscience,new,12
fazedx,"The most difficult part of drug design and discovery is to kill the thing you want to kill without harming ""healthy"" cells in the body. Most anti-bacterials are beta-lacatam antibiotics. That means they work by interfering with the building of the cell wall of bacteria. To put it simply, it disrupts penicillin binding proteins that are necessary for cross-linking of bacterial cell walls (kind of like the mortar in brick and mortar - without the mortar, the wall would not hold). Without the ability to reconstruct and expand cell walls, bacteria cannot grow or reproduce.

beta-lactam antibiotics work because they have similar structure to the penicillin binding proteins, but do not actually hold cell walls together. The bacteria use these to make their cell walls, but because they don't hold, the cell wall breaks down. It's kind of like giving a bricklayer sand instead of mortar to build a house. 

Some bacteria can produce beta-lactamase, which cleaves the beta-lactam ring and renders it ineffective. ",null,0,cdmv9no,1rfk3g,askscience,new,2
foamerc,"The short answer is there are many ways to kill bacteria, but few that discriminate between bacterial and human cells. Bacteria are cells too, and  they share many similarities with human cells, and a few differences here and there. Antibiotics exploit such differences such as bacteria have a cell wall and human cells don't. 

When discussing about killing them after they've infected someone within the body, you're pretty much left with antibiotics, which there are many subtypes working in different manners, but for all intents and purposes are chemicals ingested/injected into a human for the purpose of killing specific bacteria.

In addition you don't want to indiscriminately kill off all bacteria because that's how you select for resistant organisms, kill off normal helpful bacteria, and some nasty ones grow in their place. Look up C. difficile infections - a relatively new cure is to eat processed shit of other people.",null,0,cdo6tkv,1rfk3g,askscience,new,1
gfpumpkins,This isn't really an answerable question. The normal bacteria found in humans is incredibly unlikely to be pathogenic to ants. ,null,0,cdmr746,1rfix6,askscience,new,3
proule,"Your question seems to be based around the assumption that humans are bigger than ants, thus, our bacteria must somehow be stronger than the bacteria that colonize ant bodies.

There's no fundamental difference between bacteria that colonize ant bodies and those that colonize human bodies. Human bacteria don't need to be ""stronger"" to colonize humans; they're adapted to colonize humans just as bacteria in ants are adapted to colonize ants.",null,0,cdnh2jd,1rfix6,askscience,new,2
iorgfeflkd,"It's an amorphous solid, which basically means that it behaves like a solid (as most people would interpret them) but the atoms aren't arranged in a crystal lattice. This makes a difference if you try to measure heat transfer through the material, for example, or look at the diffraction of x-rays through it.

[Diagram](http://www.steelguru.com/uploads/reports/sss1-29-08-2008.jpg)",null,0,cdmq2n8,1rfih8,askscience,new,11
botanist2,"III&gt;Trees and plants existed millions of years before the first oxygen producing creatures

Photosynthetic organisms (mostly cyanobacteria that form [stromatolites](http://en.wikipedia.org/wiki/Stromatolites)) existed millions of years before the first oxygen producing creatures, but trees and plants as we know them today didn't evolve until much, much later.  

As to the rest of your question, there are a lot of other ways to make CO2 than just living organisms and one of the most likely sources of CO2 was volcanic activity.",null,4,cdmq5zo,1rfia5,askscience,new,10
sparky_1966,"I think you meant before the first oxygen consuming creatures. Trees and other plants weren't around for a long time after the start of making oxygen. The first photosynthetic organisms were single celled. When photosynthesis started, the atmosphere was thought to be a reducing atmosphere, so the excess oxygen taken up by iron and made rust, and there was a lot of methane that UV light made into CO2. Carbon was not necessarily limiting, since all the carbonate (limestone) had yet to form, and the oxygen comes from splitting water molecules. The oxygen cycle today is not necessarily the oxygen cycle at the beginning. ",null,1,cdms085,1rfia5,askscience,new,2
foamster,"Well, volcanic activity alone 'produces' a *lot* of atmospheric CO2. 

My understanding was that the atmosphere had very little oxygen initially, but plenty of CO2 at around the time that photosynthesis began to take off. Animal life wasn't really able to develop until the atmospheric oxygen concentration was high enough to allow for sufficient metabolic rates -- oxygen produced almost exclusively by algal photosynthesis.",null,1,cdmscd2,1rfia5,askscience,new,1
florinandrei,"Any battery has an internal resistance. Any resistance, when a current passes through it, it heats up. Therefore, a battery will heat up (or at least become a bit warmer) any time you either charge it or discharge it.

The higher temperature of a charging battery is not an indicator of it charging, it merely indicates that some current is passing through it. But same would happen during discharge.",null,1,cdmrmjz,1rfi8e,askscience,new,4
Guanglais_disciple,"The chemical reaction is endothermic and then exothermic (li-ion for example) but the joule heating (current ^ 2 * resistance) usually dominates. Since joule heating isn't a function of current direction, you see heating in both cases. For very low current, though, the chemical reaction dominates and it cools slightly. ",null,1,cdmtg6y,1rfi8e,askscience,new,4
dudds4,"It would be interesting if that was the case, but no. It'll help to understand why there is heat produced.

Basically the transfer of energy into the battery is not perfectly efficient. some energy is lost. Where does it go though, ( energy can not be destroyed) ? Heat, among other things, is the answer. 

Imagine a flowing stream of water. Some of the water laps up on either side of the stream, and gets absorbed by the land. Not all ( although nearly all) of the water makes it down the stream. Here the water getting absorbed is what we observe as heat


Same goes for discharging energy, it's just another form of energy transfer, and not perfectly efficient

",null,0,cdn4tzd,1rfi8e,askscience,new,2
polkasalad,"On discharge the batteries heat up due to the internal resistance.  Internal resistance increases as temperature decreases as well, which is why batteries last longer near room temperature, so if it were to cool the battery would actually lose capacity faster as you used it. Consequently, heating up the battery too much will damage the cell.  

I'm sure someone can offer more info, I'm in a graduate course relating batteries to hybrid-electric cars right now which is where I got my info from. ",null,2,cdmpz1e,1rfi8e,askscience,new,2
Weed_O_Whirler,"You would barely notice a difference. 

The main reason the magnet in the motor needs to keep being pushed isn't due to friction, but due to [Back EMF](http://en.wikipedia.org/wiki/Counter-electromotive_force) force. When spinning the magnet in the coil, a current is produced in the coil, and a counter-emf voltage opposes the current. These will always be of the same amount of energy- thus even without friction a magnet will very quickly slow down, as you will not be able to extract more energy from the magnet than you used to get it spinning in the first place. 

It is good to think of how these generators are not ""making"" energy, they just ""convert"" it. So, we burn stuff in order to move pistons, the moving piston spins a magnet, and the moving magnet makes electricity. Even without any friction or losses in the burning process, you'll never get more energy out of the generator than you put in. ",null,0,cdmqz7c,1rfhiw,askscience,new,7
CoryCA,"Only in that all life on Earth is related, and that they are both plants. (Though a mango stone reminds me more of a peach stone.)

A pumpkin is a squash variety of the species Cucurbita pepo of the family Cucurbitaceae. Acorn squash are also a C. pepo variety (the species is highly variable), zucchini are a different species in the same genus, and watermelons and cucumbers are part of the same family.

Mangos of genus Manifera of the family Anacardiaceae which also includes cashew, poison ivy, sumac and pistachio.",null,0,cdmrzhg,1rfgwc,askscience,new,2
proule,"There are many examples of ""convergent evolution"" in the world. That is, evolution that has caused very distantly related organisms to take on a similar appearance in some fashion. Another example of this is flying insects, birds and bats. Obviously you can see a large difference between insects, birds and bats, but birds and bats may seem like they're more related than not, right?

Birds are more closely related to reptiles than bats, which are mammals. If you look at the parts of their bodies specialized for flight, they *look* similar at a base level, but: A bird's wings are modified forelimbs (arms), and they still have distinct, separated feet. A bat's wings are a leathery extension of skin that stretches between the modified forelimbs, and actually reaches down to the legs. [Here's a picture to illustrate my point](http://upload.wikimedia.org/wikipedia/commons/3/38/Homology.jpg).

In biology, function is very tied to structure. Two structures can evolve to look very similar based on sharing the same end function, however, this does not necessarily imply a close relation.",null,0,cdnhdlw,1rfgwc,askscience,new,2
Trill-Nye,"In this case, it's better to think of color as a result of light absorption and emission, rather than reflection. When light hits a gas, it can be absorbed by various processes. Visible light just happens to be the right energy to excite the electrons bound to atomic nuclei in some molecules, such as those making up chlorine gas. These excited electrons, which have been given energy by a photon, then relax to their original energies, giving off new photons of a particular wavelength (and therefore color).

Electrons are unusual in that, due to quantum effects, they can have only certain discrete energies. This is determined by the structure and composition of the atom, and its interactions with other nearby atoms. Gasses that are not colored do not have electron excitation mechanisms of the correct energy to be excited by visible light, then give off light of a specific color.

If a gas were black, it would have to absorb most incoming photons, then give off accumulated energy as something other than visible light, such as photons of a wavelength that cannot be observed by the human eye. ",null,0,cdmsy9a,1rfggp,askscience,new,4
AznInvasian,"In easier terms to understand:

     Light wave goes into gas atom, energizes an electron and pushes it to a higher energy orbital. The electron doesn't like this, and returns to its original orbital, emitting that same amount of energy it absorbed. This makes it glow this specific colour (corresponding to the wavelength of light it absorbed).",null,0,cdnddj3,1rfggp,askscience,new,2
dontgothatway123,"In a specific practical sense when actively measuring the cardiac output (CO) of a person it is important to factor in the persons size.  This makes the CO calculation more relative.  For instance the average CO for a adult male is 5.6L/min (the volume of blood ejected from the heart every minute).  Now we'll introduce two people. One man is 5'2"" (157cm) 105lbs (47kg) with a BSA of 1.44m^2.  The other is 6'6"" (198cm) 285lb (129kg) with a BSA of 2.67m^2.  If we just considered CO (stroke volume x heart rate) would it make intuitive sense that if both of these individuals had a CO of 5.6L/min that would be ok for both?  No, some form of individualization is necessary.  This is obtained by taking the CO and including the BSA into the calculation.  This measurement is called the cardiac index (CI).  Clinically/practically it serves a better purpose and indicator for monitoring hemodynamic states in controlled situations.  Using the examples above the first man would have a CI of 3.89L/min/m^2 and the second man would have a CI of 2.1L/min/m^2.  Considering the normal CI ranges from 2.6-4.2L/min/m^2 the man in the second scenario is about a hairs breadth away from cardiogenic shock.

Hopefully that helps shows the significance of BSA inclusion within a certain situation.  As for whether there are better alternative parameters I am unsure. In research you tend to see body measurement index (BMI), ideal body weight (IBW), lean muscle mass calculations, body fat percentage (BF%), and body surface area (BSA) measurements used a bit.  Each has their own benefits and pitfalls.",null,0,cdn0ahy,1rfg3b,askscience,new,2
atomfullerene,"Height is  highly dependent on the amount and quality of food one receives as a child.  Poor farmers are often quite short.  People living in modern countries with plenty of food are taller.  Interestingly, skeletons of hunter gatherers before the dawn of agriculture were also often quite a bit taller than their immediate farmer descendants (though height does recover in the farmers somewhat over time) owing to the better nutrition of the hunter-gatherers as compared to the early farmers.

Farther back in prehistory early protohumans were often shorter than modern people.",null,0,cdmt3vc,1rfff0,askscience,new,12
Infinite_Ambiguity,"If galaxies are close enough to start with (as in clustered together, relatively speaking), then there's sufficient gravitational force between them to bring them together and to overcome inflation/expansion.  

To use an extreme example, inflation/expansion doesn't tear the earth apart, or the solar system apart, our own galaxy apart, or any other individual galaxy because the gravitational fields win each such cluster is sufficient to keep everything together.  Same concept between galaxies that are relatively close together.  

Many cosmologies believe that, I. Billions of years, our night sky will be totally dark and telescopes will be insufficient to see anything, except for the galaxies in our own cluster (which, I think, total something like 36 total galaxies).  ",null,0,cdmovvo,1rff0z,askscience,new,5
DarkLather,"Galaxies exist in groups. Galaxies within the same group can be gravitationally bound to each other. They can orbit each other and collide. Our Milky Way and the Andromeda galaxy, both members of the ""Local Group"", are currently on a collision course. ",null,0,cdmqwqr,1rff0z,askscience,new,2
keithb,"You are short-sighted. I can tell becasue your glasses have ""negative"" lenses, which cause a beam of light passing through them to diverge, to spread out. You can see the light which has been diverted in the brighter halo around the shadow of your glasses. If you were long-sighted you would glasses with lenses which are ""positive"", or converging, and there would be a bright spot in the middle of the shadow of the lens rather than a bright rim. 

The soft shadow of the lens appears darker than the carpet around it because the light passing through the lens is spread out over a larger area. The lenses will absorb a little bit of the light passing through them, but mainly they redistribute the light. That's what lenses are for.",null,1,cdmqwmz,1rfepp,askscience,new,18
iorgfeflkd,"There's a way of approximating functions called a Taylor series, where you add up diminishing terms with higher and higher powers. For example, the cosine of x can be approximated as 1-x^2 /2 + x^4 /24 - x^6 /720 ...

The tangent is the ratio of the opposite and adjacent sides of a right triangle, and for a 45 degree angle the tangent is 1. 45 degrees in radians is Pi/4. This means that the inverse tangent of 1 is Pi/4.

That series for Pi is based on the Taylor series of the inverse tangent function, substituting x=1 so that it equals Pi/4 (x=1 greatly simplifies the math because 1^anything is 1).

So basically, it's another way of saying that the tangent of 45 degrees is 1.",null,0,cdmpzwi,1rfdy0,askscience,new,15
Jetamors,"We've known about cancers for a very long time. [The oldest known description is Case 45 from this Egyptian papyrus from 1600 BC](http://archive.nlm.nih.gov/proj/ttp/flash/smith/smith.html), though I don't think it theorized about the cause. There's a great article about old Greco-Roman treatments [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820670/). According to the article, the oldest known theory about cancer (written by Galen) attributed tumors to an accumulation of black bile, due to the black veins that appear around many tumors. Galen was working off the [four humors theory](http://en.wikipedia.org/wiki/Four_humors), which was predominant in Western medicine from antiquity to about the 1800s.

Edit: I should correct myself, Galen's theory is the oldest one in the *Greco-Roman tradition*. I don't know much about medicine in other cultures, but I wouldn't be surprised if they (China particularly, but probably others as well) theorized about the origin of tumors at about that time or earlier.",null,0,cdmt0zr,1rfdsx,askscience,new,13
iorgfeflkd,"In [this](http://iopscience.iop.org/0143-0807/16/4/005) paper, they measured how likely it is for toast to land on the buttered side down, and found it was 62% (with thousands of tests), significantly more likely than random chance.",null,2,cdmozzv,1rfdo6,askscience,new,8
null,null,null,0,cdmubzf,1rfdo6,askscience,new,1
atomfullerene,"Murphy's Law is usually phrased ""Anything that can go wrong, will go wrong, and at the worst possible moment"".

It's meant to be taken tongue-in-cheek, it's not a physical law, but somewhere between a joke and a superstition.  If it was literally true, we'd all be dead. But it does have some level of validity, especially in the engineering context it was invented for.  Complex machines have lots of parts, and often only work right if _all_ the parts work together properly.  The probabilities that each part will fail get multiplied, making it more likely that something will go wrong.  And parts are more likely to fail under stress, which means while the machine is operating--often the worst time.  Eg, it's much worse if the wings fall off your test plane in the air than if it's sitting on the ground.  ",null,0,cdngwpa,1rfdo6,askscience,new,1
botanist2,"For the sake of reference [here's](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078092) the original article about these stone-tipped spears that you're asking about.  To clarify, they're not talking about aliens using spears, they're talking about different species of *Homo* (e.g., we're *Homo sapiens*, they could be talking about *Homo erectus*).  These spear tips were dated from the substrate in which they were found, they weren't dating the specific material of the spear (which could be much older). 

There's lots of ways to date different materials and the choice depends on what you're trying to test.  Carbon-14 is used predominately for organic materials, the article in question used [argon-argon](http://en.wikipedia.org/wiki/40Ar/39Ar) dating that is good for dating metamorphic and igneous minerals.   

&gt;When it comes to radio active decay, does it magically start over when you shape the object?

Not when it comes to minerals.  Radioisotope dating methods for minerals can only give you an idea of how long its been since the last time they cooled below their closure temperature (the temperature at which its assumed that isotopes aren't flowing in and out).  They tested the age of the substrate where the spear points were found because they wanted to know how long it had been since the points came to rest in that spot (and presumably when they were last used by their owner), not the age of the stones that were used to create the spear points.",null,0,cdmtews,1rfd9c,askscience,new,5
descabezado,"For radiometric dating in general, the clock starts once the object stops exchanging atoms with its surroundings.  For rocks, this means when the minerals of interest crystallized; for organic remains, it means when the creature died and stopped taking in air.  So, what they probably mean here is that the spear handle is made of wood that died 500000 years ago.  You are correct that dating the stone spear head would not be useful.

An interesting consequence of this is that you have to be very clear about what you've dated.  If you date pages of a book to be 2500 years old, it means the paper is that old, not the writing on it.  If you date a sedimentary rock to be 200 million years old using U-Pb dating with zircons, it means that the zircons were eroded out of 200 Ma old crystalline rock, but the timing of their erosion and deposition (i.e., the age of the sedimentary rock) could be any time between 200 Ma and yesterday.",null,0,cdmt83q,1rfd9c,askscience,new,3
tin_can_conspiracy,"Artifacts such as these are usually only dated by how deep they're buried (similar to how we date the dinosaurs) or by carbon dating artifacts found in the same site as the stone tools (wooden spear handles, bones, and such.)",null,1,cdmpbgx,1rfd9c,askscience,new,2
omgdonerkebab,"It's just a convex mirror.  The mirror is curved toward you, so that the rays of light that get to your eye come from a larger angle.  (Kind of like [this image](http://0.tqn.com/w/experts/Physics-1358/2009/06/Convex-Mirror.jpg), but with the directions of the arrows reversed.)  

This allows you to see a wider angle of stuff behind you, which has its obvious uses when driving.  But it also means that this larger angle is squished into a smaller area on the mirror, so the objects look smaller on the mirror.  Your brain might interpret it as the object being farther away, which would be wrong.  The object is closer than it appears to be.",null,0,cdmpcpr,1rfd8w,askscience,new,7
xenneract,"Sure. [You can hire a plant to do it for you.](https://en.wikipedia.org/wiki/Photosynthesis)

If that's not chemical enough for you, there is also active research in making [artificial photosynthetic cells](https://en.wikipedia.org/wiki/Artificial_photosynthesis) that perform the reaction you are describing.",null,0,cdmrnvi,1rfd6j,askscience,new,6
sodium_dodecyl,"We *can*, but it's not going to be terribly efficient (or necessarily fast, I don't have any kinects data). An example of a possible pathway: Reduce [Reduce CO2](http://en.wikipedia.org/wiki/Sabatier_reaction) to CH4 + H2O, then use electrolysis to split H2O --&gt; O2",null,0,cdmrezw,1rfd6j,askscience,new,1
steeeeve,"Yes, it is possible. However, the reason we make CO2 is because reacting carbon with oxygen to form CO2 bonds releases energy. The same amount of energy has to go into the bond to break it. Since power plants are not 100% efficient (and never can be) the re-separation will always cost more energy then we got from burining the fuel in the first place (assuming the fuel is almost all carbon, like in coal)",null,0,cdmxv3u,1rfd6j,askscience,new,1
kyaj21,"Technically, yes. CO2 is just 1 part carbon, 2 parts oxygen, as any school child who has taken introductory chemistry would be able to tell you. Yes, we could extract oxygen from the carbon dioxide, but the carbon would still be there. Reducing carbon emissions is a whole other matter, as in order to reduce carbon emissions, we would have to change the fuel sources or at the very minimum how we process them, and what we would do with the carbon once we extracted the oxygen from the carbon dioxide.",null,5,cdmrfta,1rfd6j,askscience,new,1
proule,"Curiosity drives you to ask questions, which, in being answered, can improve your chances of surviving. This ingenuity is perhaps the most evolutionarily successful means of avoiding death due to outside influence. Other evolutionary tactics would include simply being bigger than anything that could otherwise hurt you.

In animals capable of higher learning, curiosity is fundamentally a desire to learn and understand the world you interact with. At the most basic level, curiosity is important to be able to accomplish the key tasks for each living being: Survive and produce offspring.
",null,0,cdnh79b,1rfd57,askscience,new,2
spryspring,"Curiosity is a behavior that has probably been selected for in some animals by natural selection, or at least has not been selected against. Suppose that a ""gene for curiosity"" (I'm sure in reality it's not nearly that simple) arose in the ancestors of cats. Proto-cats that had this gene tended to have more offspring than those who didn't (we might guess that they, in being more curious, found more food).

Or maybe it's totally a learned behavior, I don't know. But in any case that's how behaviors can arise.  ",null,2,cdn1stc,1rfd57,askscience,new,1
jadiusatreu,"Great answers from the beekeepers. To add a little more information apart from honeybees, not all bees make a honeycomb. Bumblebees make honey pots in which they store their honey.  These bees make a cylindrical, sometimes round, pot. Just a little more information for you.",null,0,cdmtjth,1rfcsm,askscience,new,3
HCOOH,"There are so many wild-types of bees... they don't make nests.
And the ""normal"" honeybees make round shapes, but because of the melting of these round shapes thexy become hexagonal. The whole thing is more.. a succes through error",null,1,cdmp126,1rfcsm,askscience,new,2
steeeeve,"There's no 'up' and 'down'. However, your brain is somewhat accustomed to zero-g; it happens whenever we're falling. The fluid isn't really ""floating"" because to the best of my knowledge there's no air in the part of your ear that controls balance. Rather, there's hairs in the ear that will 'flex' under a current that is induced when you accelerate. ",null,0,cdmxa5d,1rfcei,askscience,new,1
Manhigh,"The only mechanism for heat transfer from the space station is through radiation.  In general, all of the electrical components on a spacecraft and solar incidence (when in sunlight) produce excess heat which needs to be shed.  If you look at a picture of the space station you'll see a series of panels that are perpendicular to the solar panels.  While it is generally desirable that solar arrays always face the sun, it's generally desirable to have the radiators edge-on to the sun, facing deep space.

Coolant passing through the radiators is cooled and then passed back inside to keep removing heat from the station.  If you wanted to heat the station, you could have the radiators face the sun slightly.

In this photo, the radiators are the white  accordion-like structures:  http://milesobrien.files.wordpress.com/2010/08/iss1.jpg",null,0,cdmsvb7,1rfc91,askscience,new,6
Truck43,"That's really two questions, whether or not the shell will act as a faraday cage, I'll leave to another, but, the microwaves will induce currents in the case that will produce enough heat to start a pretty serious fire, and probably cause catastrophic failure in the battery. ",null,0,cdn3oke,1rfc8w,askscience,new,2
auralucario2,"From my limited experience in putting metal in microwaves, I think that the shell itself would begin sparking, due to the movement of electrons caused by the energy of the microwaves. As for the insides, it would probably escape direct harm from the microwaves, but the heat and electricity thrown off of the casing would probably do some serious damage.

Please don't try it though.",null,0,cdnv98v,1rfc8w,askscience,new,1
Aniridia,"Yes, overweight women are 1.5 times more likely, and obese women 3 times more likely to become pregnant than women of normal BMI, regardless the type of hormonal contraception used.

[From this article:](http://www.ccjm.org/content/79/11/771.long)

&gt; Q: True or false? Hormonal emergency contraception is more likely to fail in obese patients.

&gt; A: True. Most recent evidence shows that whichever oral emergency contraceptive drug is taken, the risk of pregnancy is more than 3 times greater for obese women (OR 3.60, 95% CI 1.96–6.53) and 1.5 times greater for overweight women (OR 1.53, 95% CI 0.75–2.95).16 Of all covariates tested, those that were shown to increase the odds of failure of the emergency contraception were higher body mass index, further unprotected intercourse, and conception probability (based on time of fertility cycle). In fact, among obese women treated with levonorgestrel, the observed pregnancy rate was 5.8%, which is slightly above the overall pregnancy rate expected in the absence of emergency contraception, suggesting that for obese women levonorgestrel-based emergency contraception may even be ineffective.

&gt;This is in line with recent reports suggesting that oral contraceptives are less effective in obese women. More effective regimens such as an IUD or ulipristal might be preferred in these women. However, obesity should not be used as a reason not to offer emergency contraception, as this is the last chance these women have to prevent pregnancy.",null,0,cdmrmwo,1rfbsd,askscience,new,3
stevenstevenstevenst,"The most serious affect upon the body due to exposure to lower or zero gravity is atrophy of the muscles.  As you will weigh less or nothing at all, you muscles have to work much less and thus will begin to degrade.  This is the reason individuals on the ISS need to work out regularly by running on a treadmill or though other means.

As blood circulation is negatively affected by reduced gravity (due to the way this system has evolved to partially utilize gravity in its function), other health problems may potentially be associated with manned spaceflight, such a neurodegeneration- although this research is ongoing.",null,0,cdmo0v8,1rfbi7,askscience,new,2
mzyos,"There is some worry at NASA currently about Optic nerve atrophy. This is where the nerve carrying signals for sight from the eye starts to deteriorate. It seems that about a 3rd of astronauts have this, if they have experienced long bouts of zero G. They are studying on ISS at the moment using a goldmann tonometer which measures eye pressure. They don't really understand what is going on just yet, but it might be due to the lack of gravity causing some of the eye, and it's nerve's blood supply being slowed, or stopped in one way or another. ",null,0,cdojdvm,1rfbi7,askscience,new,1
bohr_exciton,"&gt;If we know the wavelength of a polarized photon... then why cant we determine where exactly a given photon will interact with the resist? I'm guessing something here will touch upon wave-particle duality...

Right, specifically it's the wave aspect that sets the limit. Light passing through a specific aperture or lens will not arrive in one infinitely sharp point but in a disk (e.g. the so called Airy disk for circular apertures). For far-field light, the size of this disk will be determined by a number of factors such as diffraction and the aberrations in the imaging system. The best possible case using simple far field optics is to obtain the diffraction limited spot, which is on the order of half the wavelength of the incident light. 

&gt;Part2: If we've got vapor deposition for things like gold ... why can't we vapor deposit a single atom later of the resist ... again being able to do away with the complex mask?

I'm not really sure I understand this question. Under certain circumstances it's possible to deposit metals uniformly for a desired number of monolayers. However, you need a mask if want something other than a uniform layer, e.g. patterning for an integrated circuit. ",null,0,cdmsmcx,1rfapt,askscience,new,1
LeoPanagiotopoulos,"The limit of the situation you're describing is a ratio of 1 where the planet and moon are indistinguishable because they're the same mass. It's unlikely but possible. You're correct in your suggestion that the distance from the 3rd, larger mass in the system is important. If The distance between our twin planets (or moons? or ploons? or [manets](http://2.bp.blogspot.com/_gJ6d5yFc7fw/TL72k9N-pqI/AAAAAAAAB_I/Gbu1fWmRxPU/s400/g013v_manet_lemon.jpg)?) is comparable to the distance to the larger object in the system, their orbits around each other will be unstable. 

[Consider reading about triple star systems](http://en.wikipedia.org/wiki/Star_system#Triple_star_systems). The situation we're talking about is labeled C on the linked diagram. It's true that interactions between stars that are very close to each other can be a little more complex that cold, non-fusing rocks (planets), but in most cases the dynamics are comparable. 

Almost forgot: the 3rd object is more often smaller and orbiting the two inner objects, which are orbiting each other. Still your situation is possible. ",null,0,cdnk8oc,1rf9zl,askscience,new,2
amvakar,"The first (and inescapable) factor in the large size of source code compared to the compiled binaries is the lack of information density inherent in any plain-text format -- you've got to keep things human-readable, which means that you're restricted to the alphabet plus enough special characters for basic formatting and organization. Each operation will involve reasonably-descriptive names as opposed to the pointers to their location in memory that the processor will see. Documentation will also be present. In short: you're describing what the computer will do so a person could understand it, while the computer will only need to be told the bare minimum about the operation to complete it. To see this in action, run the source through any compression algorithm -- the size will go down significantly.

The second factor in large software projects is the presence of code that might never actually be used. For an operating system, you'll end up with drivers for devices you don't have or support for CPUs you're not using. For applications, you might have support for different APIs or just functionality you choose not to include in the finished product. For debugging purposes there may be tests and additional information so that problems can be tracked down, and in debugging builds optimization may be turned off. 

In short: source code is far more descriptive than binary for human purposes and includes a lot of things that you may never end up using in the final build.",null,2,cdmstpb,1rf8w9,askscience,new,14
Platypuskeeper,"The [Golden Rule](http://en.wikipedia.org/wiki/Fermi%27s_golden_rule) says that transition probabilities depend on the overlap between the initial and final states. In a Rydberg atom, you're in a highly excited state where the electron is far from the nucleus, and its overlap with the ground state and lowest-energy states is quite poor. So direct transitions back down to there are improbable. 

",null,0,cdmr8jt,1rf8ta,askscience,new,3
Daegs,"On earth you can see million miles away yourself, right now!!! Just look at the stars.

Remember the sun is ~93 million miles away, most of the stars you see are orders of magnitude further away. 

We can see the andromeda galaxy with our naked eyes, so that is 14,696,249,500,000,000,000 miles away!

In space, you wouldn't have the atmosphere filtering photons coming from stars, so you'd be able to see even more.

This is why we have the hubble telescope in space, to avoid earth's atmosphere. ",null,0,cdmtpir,1rf86r,askscience,new,6
king_of_the_universe,"http://www.uitti.net/stephen/astro/essays/farthest_naked_eye_object.shtml

says:

&gt; Bode's Galaxy (M81), at 12 million (12,000,000) light years has been spotted by several people. This [page at SEDS on M81](http://www.seds.org/messier/m/m081.html) has a description of how to see it.

&gt; The trouble is, at Magnitude 6.9, M81 is dimmer than most consider naked eye. It depends on whose eye it is, and also where the feet are standing. It has to be an exceptionally dark sky site, probably at some altitude, at the right time of year, etc.

WolframAlpha's answer to ""12,000,000 lightyears in miles"" is 7.054×10^19 miles, which is 70,540,000,000,000,000,000 miles. (Take that, Daegs! ;)

The text also says that there could be bright events like super novas that could even be visible with the naked eye from further away for a few days.",null,0,cdncboq,1rf86r,askscience,new,2
stuthulhu,"&gt; once you pass the outer layers of our atmosphere you are weightless - why cant we achieve that speed?

Weightlessness is a state achieved when no force other than gravity is acting upon you. When a vehicle is accelerating/decelerating, that force will be acting upon you, and you will not feel weightless. You would feel pushed against the back of the vehicle by the force of the acceleration.

The shuttle must burn fuel to leave our inertial motion, and burn fuel to match that of its destination. Being likely far more massive, both become more expensive actions, and the more fuel required to do either action increases the weight even further. ",null,0,cdmtslm,1rf5vk,askscience,new,1
WendyMouse,"The shuttle is bigger.  A LOT bigger.

New Horizons is a very light spacecraft-- about the size of a grand piano, launched from a very powerful rocket.  It was the combination of the two that made it travel so fast, faster than anything else humanity had ever launched.  New Horizons does not have enough propellant to slow itself down to enter into Pluto's orbit.  The fuel to do that would be too heavy.


Escape velocity from Earth is everything.  Humanity hasn't mastered launching a bunch of things in pieces, merging them and having another separate launch in space yet. 

Just because something doesn't have weight, (you are not in zero gravity in space, you are in microgravity), doesn't mean it doesn't have mass or momentum.

",null,0,cdnst3v,1rf5vk,askscience,new,1
tigertealc,"Catalysis by definition is a process by which a substoichiometric reagent promotes a reaction by lowering the activation barrier of the reaction. So that would be the common denominator, I suppose. 

Working out the mechanism of a catalytic reaction is not always straightforward. Most often, mechanisms are proposed to follow mechanistic steps that have been determined for related systems, or using intuition. But a number of different control experiments must be run to differentiate between different possibilities. Often these experiments involve the kinetics of the reaction, whether it involves determining the rate law of the reaction or determining a kinetic isotope effect. Isotopically labeled reagents can also assist, by seeing where they end up in the product. Computation can certainly aid in the assignments of mechanisms, but empiricism is the main method. And of course, the exact experiments that one is able to run to elucidate the mechanism is largely dependent upon the specific reaction. 

If you have any specific questions about specific reactions, feel free to ask. ",null,0,cdmotdy,1rf5ro,askscience,new,4
Platypuskeeper,"There is no common denominator other than the fact that catalysts catalyze. One reaction might be catalyzed by acid, the presence of H^+ , which participate in the reaction but are released on a later step. Another reaction might be catalyzed by a Lewis base, where the base temporarily donates an electron pair to a reacting atom. Those two scenarios really have nothing in common other than that they fulfill the definition of 'catalyst'. The word describes a role something plays in a reaction, but the reactions can be as different as any chemical reactions. There's no general theory of reactions either.

",null,0,cdmt6k1,1rf5ro,askscience,new,4
Daegs,"This is not a 3D gif. 

A 3D gif would either require:

* two stereoscopic panels which you could view by changing the focus of your eyes so that the panels merge

* A single panel using red / blue shading and 3D glasses

* A single panel and special display to work along with polarized glasses.

This **non-3D** gif simply give perspective by being displayed over the ""break"" and the changing focus which cues our brain that there is 3d information being presented.

In other words, there is nothing special about 2 breaks, 3 breaks, 4 breaks, whatever.... the breaks are just used so that the gun can go ""over"" something that we perceive as flat. ",null,1,cdmtkie,1rf5et,askscience,new,6
ramk13,"Though diffusion is slower at lower temperatures, lowered vapor pressure is a much bigger influence. Most odors are either small solid particles or vaporized compounds. The equilibrium vapor pressure of a compound is exponentially dependent on temperature, so when it's colder a lot less of the compound gets into the air. Since it doesn't vaporize as much you smell less of it.

Also, most of the stuff you smell is more likely to be transported by convection (movement by temperature induced density gradients) or advection (forced movement) than diffusion.

For an empirical relationship between vapor pressure and temperature, you can use the [Antoine Equation](http://en.wikipedia.org/wiki/Antoine_equation) which is derived from the principles of the [Clausius-Clapeyron relation](http://en.wikipedia.org/wiki/Clausius-Clapeyron_relation).",null,0,cdmrge3,1rf3jg,askscience,new,3
stevenstevenstevenst,"At lower temperatures, vibration of particles is decreased due to the decreased energy of the system.  As diffusion of gases relies upon random vibrational motion for the even dispersal of a compound, gaseous compounds (such as any odor) will spread increasingly more slowly with decreasing temperature.",null,0,cdmp1lq,1rf3jg,askscience,new,1
Osymandius,"Contrary to the answers below ATP **is** produced within the chloroplast. ATP synthase is located in the thylakoid membrane/space and does make use of the proton motive force generated by either cyclic or non cyclic photophosphorylation. But - the ATP produced in the chloroplast just isn't enough to compared to the amount produced in the mitochondria. We move relatively minimal numbers of protons across the membrane during photosynthesis - the really important product of non-cyclic photophosphorylation is the generation of reducing equivalents (NADPH). This can then be used to fuel the Calvin cycle and the production of triose phosphates and sugar derivatives.

Once we have produced TP/sugars, these can be metabolised to produce NADH in the mitochondria. The proton motive force produced by the electron transport chain is considerably greater, and much more ATP can be generated than relying on chloroplasts alone.",null,0,cdmrffo,1rf3cf,askscience,new,3
quantum_lotus,"As /u/Osymandius says, both organelles can produce ATP (the most useful form of stored energy for a cell), but that mitochondria are much more efficient at making it.

But there is another consideration.  Evolutionary data and model point to chloroplasts being acquired *after* mitochondria.  So the cells that eventually became the plant lineage already had mitochondria in them before they captured chloroplasts.  ",null,0,cdn40cu,1rf3cf,askscience,new,2
botanist2,"No.  The purpose of the chloroplasts is to make the energy needed for respiration, they don't have the ""machinery"" necessary to put the energy in the most usable form like what happens in the mitochondria.  Your question is kind of like asking ""Why can't the gas tank run the car?""",null,2,cdmplym,1rf3cf,askscience,new,2
null,null,null,414,cdmn013,1rf2b3,askscience,new,1927
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,271,cdmlcrf,1rf2b3,askscience,new,1452
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,38,cdmmor1,1rf2b3,askscience,new,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,new,65
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,11,cdmlptk,1rf2b3,askscience,new,52
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,24,cdmrqlw,1rf2b3,askscience,new,54
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,8,cdmpnpe,1rf2b3,askscience,new,21
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,new,10
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,new,17
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,17,cdmm72d,1rf2b3,askscience,new,23
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,new,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,7,cdmqbvb,1rf2b3,askscience,new,14
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,new,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,0,cdmn10w,1rf2b3,askscience,new,5
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,8,cdmpswn,1rf2b3,askscience,new,13
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,7,cdmpwee,1rf2b3,askscience,new,13
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,new,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,0,cdmq96f,1rf2b3,askscience,new,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,9,cdmr4on,1rf2b3,askscience,new,12
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,new,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,new,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,8,cdmmecq,1rf2b3,askscience,new,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,2,cdmmxgj,1rf2b3,askscience,new,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,8,cdmn15l,1rf2b3,askscience,new,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,0,cdmn6cx,1rf2b3,askscience,new,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,8,cdmngbx,1rf2b3,askscience,new,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,8,cdmnh29,1rf2b3,askscience,new,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,8,cdmnzr4,1rf2b3,askscience,new,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,new,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,8,cdmo9dv,1rf2b3,askscience,new,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [Rayleigh–Bénard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,new,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,0,cdmphjt,1rf2b3,askscience,new,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,new,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,new,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,0,cdmtzvb,1rf2b3,askscience,new,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,new,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,1,cdmw6gb,1rf2b3,askscience,new,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,new,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,new,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,0,cdn5ejj,1rf2b3,askscience,new,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,new,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,0,cdna4y1,1rf2b3,askscience,new,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,new,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,9,cdmnnog,1rf2b3,askscience,new,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,new,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,new,1
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,new,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,new,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,new,13
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,26,cdmlm65,1rew42,askscience,new,167
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,8,cdmokr1,1rew42,askscience,new,20
bbqbollocks,"Because there are two ways a stm works. Constant current and constant height.

With constant current, the distance between the tip and the sample changes to keep the current flowing through the tip the same. This maps the topography of the surface. 

If the sample is flat enough then you can use the constant height mode. The constant height mode will keep the distance between the tip and sample fixed as it scans across the surface. So if you have 35 xenon atoms writhing range cor quantum tunneling to take place then a current flows where the atoms are. So no nickel atoms can be viewed. This mode looks at the density of states on the surface. ",null,0,cdmlwie,1revqt,askscience,new,11
breadmaniowa,"The real reason you feel the need to breathe is because of the carbon dioxide building up in your blood. Taking in oxygen removes the dissolved carbon dioxide from your body. So basically, the real reason you can't hold your breath for very long is that you need to expel the carbon dioxide from your body. You actually have plenty of oxygen still in your blood when you feel the need to breathe.",null,5,cdmlr1p,1revb2,askscience,new,12
fazedx,"There are two drivers in the human body that tells it to breathe. The first one is concentration of carbon dioxide in the blood, and the second one (backup, if you will) is the concentration of oxygen.

Carbon dioxide (CO2) is allowed to pass the blood brain barrier. High concentrations of CO2 diffuse into your cerebral spinal fluid (CSF), dissociates into hydrogen ions and lowers your CSF pH. This is picked up by chemoreceptors and signals your central nervous system to increase ventilation. This is your central, or main control of breathing.

Peripheral control is based on pO2 in arterial blood. If it drops below a certain point, it will send signals to your brain to start breathing.

You can reduce the pain from holding your breathe by hyperventilating before you hold your breathe, thus reducing the buildup of acid and the prolonging the time it takes for your brain to signal to you to breathe.

http://www.winona.edu/biology/adam_ip/misc/assignmentfiles/respiratory/Control_of_Respiration.pdf is a good source/summary",null,0,cdmuuor,1revb2,askscience,new,3
paolog,"The ""criss-cross"" distance between two points is called the Manhattan distance between the points, while the straight-line distance is called the Euclidean distance. What you're asking is whether the limit of the Manhattan distance as the grid gets finer is equal to the Euclidean distance. It's easy to show that this is not the case.

Let's take a 1 x 1 square. The Manhattan distance from one corner to the other is 2 (length of bottom edge + length of right edge, for example), while the Euclidean distance is, by Pythogoras' theorem, √2.

Now subdivide the square into a 2 x 2 grid of four squares. To get from one corner to another, we have to zigzag along four edges of the small squares, each of which is 1/2 a unit long. So the total distance is 4 x 1/2, or 2.

It's not hard to show that however we subdivide the square into smaller squares (or even rectangles), the shortest corner-to-corner distance measured along the edges of these squares will always be 2 and will never get anywhere near √2. Hence no matter how many turns we make, the Manhattan distance never equals the Euclidean distance.

So no, nothing changes as the resolution of the grid becomes finer. Furthermore, a diagonal is not imaginary - it is just different from walking along the edges.

EDIT: removed repetition",null,3,cdml7vm,1reu8x,askscience,new,21
Professor_Snuggles,"The fundamental point here is this: two curves can be visually similar yet have very different lengths. Imagine a bug taking inch long steps in a long zig-zag across the line. What you're doing is similar to saying that the bug can instead cut closer to the line and decrease the forward distance covered with each zig-zag. This could give a path that stays closer to the original line overall, but has the same length because it wiggles more.

The moral of the story is that curves that stay close to each other do not have to have lengths that stay close to each other. As for real life: yes there is a difference traveled if you take a small step up, then a small step right, etc. compared to directly walking the diagonal. This is easiest to see if you have a robot or something that you can guarantee will travel at a constant speed and a timer. A real life diagonal is not necessarily ""imaginary"", it's just that traveling near it in any way you want is not going to get the same results as traveling *on* it, or other paths that more accurately approximate doing so.",null,1,cdmrs35,1reu8x,askscience,new,7
jeff0,"The size of the grid doesn't matter. Say your rectangle is a 1 mile x 1 mile square. The length of the diagonal from A to B is sqrt(2) =~ 1.4 miles. If you instead alternate walking due north with walking due east, you end up walking a total of 1 mile east and 1 mile north = 2 miles total. The size of the grid will only effect the number of times that you turn.

The same idea underlies the [troll math](http://knowyourmeme.com/forums/meme-research/topics/8029-troll-math) meme.",null,0,cdmleqb,1reu8x,askscience,new,2
wgunther,"Just to prove the bit more formally instead of showing some examples: if you divide a 1x1 square into an n by n grid then the distance you are traveling in total n*(1/n+1/n); that is, imagining you are in the bottom left corner, you have to go up distance 1/n and right distance 1/n for each subdivision, and there's are n of them. Therefore, the distance you must travel is 2. 

Intuitively it makes sense: all your motion is either purely vertical or purely horizontal. You must move horizontally distance 1 and vertically distance 1. Therefore you must move distance 2. ",null,1,cdmmhao,1reu8x,askscience,new,3
ignorant_,"A diagonal line bi-sects a square at 45degrees. The question asked is regarding using horizontal and vertical lines to travel toward the opposite corner. These lines are at 90degrees. Suppose we used intermediate angles. 89 degrees, then 88 degrees, etc., and work our way down toward 45degrees. Wouldn't my distance begin to decrease as the angle approaches 45 degrees, and have a limit of square root of 2?",null,1,cdmw7ub,1reu8x,askscience,new,1
yeast_problem,"Quantum Mechanics would bring a limit to this, as the grid size gets smaller the uncertainty principle would mean your momentum could not be zero in the  direction perpendicular to travel. It would become impossible to say that you were actually travelling along the grid lines, at scales around 10^-34 meters.",null,4,cdmvy76,1reu8x,askscience,new,1
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,new,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,new,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,new,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,new,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,new,27
wazoheat,"No. Food and drink go bad due to [spoilage](https://en.wikipedia.org/wiki/Food_spoilage), which is usually due to the growth of bacteria and/or fungus, none of which will grow in plain water.",null,0,cdmi5o5,1remei,askscience,new,3
ides_of_june,As wazoheat said water doesn't spoil. It's possible that the container that the water is stored in could undergo thermal degradation making the water unfit for consumption (or at least undesirab. Also if the water is stored open to the environment it can become contaminated though in an indoor environment it's unlikely to become unfit for consumption.,null,0,cdmo21b,1remei,askscience,new,1
ramk13,"Depending the temperature ranges you could breakdown the disinfectant residual (usually chlorine or monochloramine at ~1 mg/L) that is normally present in tap water. If the residual is gone, then organisms (e.g. algae) are much likely to grow in your water if spores are present. 

Also if the temperature gets high enough you can have interactions between the water and its container. Metals are more likely to corrode and leach, and probably more relevant, plastics can leach plasticizers into the water. I don't know that there are studies that have quantified whether there are documented effects in animals or humans for tap/drinking water stored in plastic bottles, but many people are concerned about it.

The water itself won't spoil.",null,0,cdmrq6r,1remei,askscience,new,1
drzowie,"Jovian interference.  The asteroids are near a couple of major resonances with Jupiter; that gives them enough of a nudge to prevent them from coalescing.  (Source:  while I am not a planetary scientist I work in a lab with a passel of 'em).

A bit more: Small-ratio resonance orbits with major bodies typically have nothing in them, because over time the larger body kicks the smaller ones out of that orbit.  Think of pushing a swing, or operating a cyclotron:  you can transfer a *lot* of energy to an oscillating body just by kicking it gently in some pattern with a harmonic relationship to the oscillation.  Major bodies typically clear out their own orbits over time due to the 1:1 resonance with anything else in that orbit -- anything at, say, 0.999 AU would eventually have a near encounter with Earth, and get ejected.  That effect is why Ceres and Pluto are considered ""dwarf planets"" and not ""planets"" -- the dynamical process is part of our modern definition of a planet.  The 2:1 and 4:1 resonances with Jupiter define reasonable approximate boundaries of the asteroid belt, and there are noticeable gaps near small-integer ratios of Jupiter's period between those values.
",null,0,cdmoip2,1remcu,askscience,new,3
The_Evil_Within,"Jupiter exerts enough of a disruptive force on the asteroid belt to keep it thinned out - and due to their relative motion, individual asteroids are at least as likely to smash into *more* debris than they are to coalesce into one bigger mass.

At least, so I was informed when I asked this question here a while ago.  The detailed explanation was kind of over my head, as you might expect.

Given that explanation, I still have trouble understanding how Ceres could form in the first place, yet still not be capable of 'finishing' and collecting the remaining mass of the asteroid belt.",null,1,cdmojfi,1remcu,askscience,new,3
GumbyTastic,"Well you have to look at it like this. Why does saturn have rings? Why doesn't all that mass floating around it just make a new moon? Usually the mass doesn't have enough force to coldine and make new objects or there's not enough force keeping the mass together. The asteroid belt (don't quote me) like a big ring like saturn. It's just full of rocks and debris that get caught up in the suns gravitational pull. It looses mass and gains mass when new objects are knocked out and sucked in. Correct me if I'm wrong on anything. I enjoy learning and never see much coverage, hell I never see anything about the asteroid belt!!",null,4,cdmj64t,1remcu,askscience,new,3
bobbycorwin123,"bah, I cant find any links to the exact reason. I believe its because of the rotations of mars and Jupiter and the way the gravity of the two bodies prevent stuff from gathering too much...

All I remember is that Jupiter and Saturn have a 2/1 rotation ratio...which helps not at all in this",null,6,cdmjcle,1remcu,askscience,new,3
I_Gargled_Jarate,"Gravity isnt strong enough to compress asteroids into larger planets. It takes a high velocity collision for asteroids to fuse together. Gravity does play a part by attracting large bodies which may be potentially travelling at very high velocities, but just sitting next to each other is not enough to form larger objects.",null,4,cdmk4yf,1remcu,askscience,new,1
svarogteuse,"
$60 is not worth spending on a telescope. You will end up with a very low end wobble device and be disappointed.  Buy a set of binoculars first. If you decide that you aren't that into astronomy later binoculars have other uses a telescope really doesn't. Next go hang out with the local Astronomical society. Look at what they are using get to know their equipment before you make a purchase more than binoculars.
The smallest scope regularly used in our society 10 years ago was an 6"", well above the $60 price tag and the 2-3"" of the ones you mentioned.

15x70s are huge binoculars. You are going to have problems keeping them steady unless you invest in some sort of [mount for them](http://www.telescope.com/Orion-Paragon-Plus-Binocular-Mount-and-Tripod/p/5379.uts).  With those binoculars you will be able to see the moons of Jupiter, the ears on Saturn, maybe Titan if you are in a good spot, and clusters. They aren't really designed to see galaxies except the brightest ones. The standard binoculars used are 10x50s. Light enough to hold steady, or balance on a chair but powerful enough to see binocular objects (bright clusters, comets, birds). We really don't use binoculars for planetary observing not enough detail. I would recommend a set of 10x50s before the 10x70s. I have never owned nor known anyone to own such large binoculars except for special purposes like comet hunting and defiantly not without a mount.

Neither of those scopes are really worth using for more than a causal, hey that's the moon kind of use. I used a 6"" for many years around 2000 and it was the smallest scope in the group. 8"" is a standard entry level amateur scope. What matters in a telescope is aperture the size of the main lens or mirror. The larger the aperture the more light is concentrated onto your eye, the fainter an object can be seen. You want to spend your money on aperture! Magnification doesn't matter, most observing is done with relatively low magnification but the higher aperture the better.  

Long time amateur astronomer (30+ years), previous president local astronomical society. ",null,0,cdmnyge,1reljs,askscience,new,2
_NW_,"Having both works better.  Do the binoculars have a tripod mount?  At that kind of magnification, it's going to be difficult to hold steady.  Also, after a few minutes, your arms are going to start getting really tired.  My first telescope was a 60 mm Tasco.  A good pair of binoculars worked better.  Years later, I bought a 6 inch reflector and finally got see all the things that I couldn't with the Tasco.  I have a pair of 10x50 binoculars that I use alone or with the telescope.  When looking for something in the sky, it helps to find it with binoculars first before using the telescope.  Or, sometimes I just go look with the binoculars just because it's easy.  Also, because it's more than enough to see several galaxies, star clusters, nebulas, and Jupiters moons.  Actually, the Andromeda, LMC, SMC, and a few other galaxies are visable even without binoculars.  Stop at a store and pick up a copy of ""Sky and Telescope"" or ""Astronomy"" Magazine.  Both have a star map that shows what you can see for that month.",null,0,cdmzcq2,1reljs,askscience,new,2
botanist2,"I do a bit of bird watching and very amateur stargazing, so I have some experience in this issue.  One of the biggest problems with using binoculars for anything like bird watching or stargazing is that your arms aren't very steady, which isn't that much of an issue at lower magnifications (e.g. looking at birds in the tree above you), but is really bad at higher magnifications (e.g. trying to look at ducks way out in the pond).  I would suggest getting a telescope with a tripod because you'll get a lot more stability and you'll be able to see things more clearly as a result.",null,0,cdmmve6,1reljs,askscience,new,1
SilentCastHD,"Well, first of all, you have to differentiate subtractive color from additive color.

In the first case, all the colors give you black, in the second, all the colors give you white.

So to make that clear: If you take a flashlight, and shine it onto a white paper, you see white light. - Duh...

If you take a red marker and mark the page, you strip away ""all the light"" that isn't red and absorb it, so only the red light reflects. The dye subtracts the [wavelengths](http://upload.wikimedia.org/wikipedia/commons/c/c4/Rendered_Spectrum.png) that don't correspond to red.

So you transform white light to red light using the filter ""red marker dye"".
Going forward, with blue and yellow, you strip awaay more and more of the light, until no light is relected anymore, leaving you with black.

The other way around, you [add up colored light](http://upload.wikimedia.org/wikipedia/commons/2/28/RGB_illumination.jpg) to make white light.

So you shine red light onto a white wall, the reflected light is red. If you overlay it with the other colors, you'll get white again.

(This is why green looks black in ""pure red light"", since there is [no refelection of red light on green leafs](http://1.bp.blogspot.com/-hHcuVK0TGHg/TyVDCInAFWI/AAAAAAAAAT0/3tU3h7p1Zbw/s1600/Lights%2B1%2B-%2BOriginal.jpg))

So with that out of the way: What is grey?

Grey is the achromatic color between black and white.

So, since you get the two different colour-systems now, you see that grey in [RGB](http://en.wikipedia.org/wiki/RGB) displays (additive color) has to be different from the grey in a printed picture in [CMYK](http://en.wikipedia.org/wiki/CMYK_color_model)(subtractive color)

So, as you can see [here](http://www.aksiom.net/rgb.html) at the bottom the RGB value for grey is always something where R=G=B, and the stronger the individual light gets, the more you go up to white.

I hope this helps :)",null,0,cdmjc3a,1rekov,askscience,new,6
LoyalSol,"There isn't really one universal answer since different materials will react differently with acids/bases, but a large majority of them dissolve because of either oxidation like in the case of metals or through catalyzed reactions (the acid/base speeds up a reaction that normally would occur slowly).

Oxidation is pretty straight forward.  The metals have electrons taken away by the acid and once that happens they form stable ions which can be freely dissolved into solution.  In catalytic reactions the acid/base comes in and binds to a functional group on a molecule (usually organic molecules) and stabilizes the molecule in a way that it can undergo further reactions.  

http://www.organic-chemistry.org/namedreactions/fischer-esterification.shtm

That's an example of the forward reaction, but the reverse reaction is similar.    In large scale a organic molecules such as proteins, each peptide in the chain is linked together by an functional ground (amide group for proteins, O=C-N) and the acid/base will attack these links causes the chain to break apart.  Which is why they are generally detrimental to biological organisms. ",null,0,cdmm2e5,1rehln,askscience,new,2
NotFreeAdvice,"Answering your second question, glass is often used for two reasons.  First, the Si-O bonds that are the structure of silica compounds (like glass) are relatively inert.  Thus, they do not like to be broken by other compounds/chemicals.  Second, it is amorphous, which adds both strength to the vessel and well as a reduction in reactivity that can occur at the edges of crystal faces.  Hence, the amorphous nature renders the glass less reactive than it would be if it were crystalline silica.  

There are some things that are not good to store in glass, however.  Potassium hydroxide will etch away the glass, and hydrofluoric acid will do the same.  These are just two examples, but there are a number of chemicals that are not inert, with respect to glass. 

Hope that helps!",null,1,cdmmctc,1rehln,askscience,new,2
drzowie,"A superadiabatic gradient is what *drives* convection -- the free energy that gets converted to mechanical flow comes from the positive difference between the gradient and the adiabatic lapse rate.  Convection will happen at *some* level with any nonzero excess in the lapse rate above the adiabatic rate, since the material is a fluid.

In practice, the actual lapse rate doesn't get driven exactly to the adiabatic rate, but it's pretty darned close.  The actual offset is driven by the balance between heat flux and (effective turbulent) viscosity in the fluid.  Since stellar plasmas aren't known for their high viscosity, and the scales are large, the offset turns out to miniscule (negligible by orders of magnitude) in nearly all cases -- so you can treat the adiabatic lapse rate as a strict limit, and be good to go.

Let's apply your example of 10^-6 superadiabaticity to the Sun.   [The convection zone spans about 6 orders of magnitude, or about 14 scale heights, in density](http://solarphysics.livingreviews.org/open?pubNo=lrsp-2009-2&amp;amp;page=articlese1.html).  If the lapse rate differs from adiabatic by 1 part in 10^6, that corresponds to a temperature differential factor of e^(14x2/3x1.000001) compared to e^(14x2/3) across the whole convection zone - so if you assumed the lapse rate was exactly adiabatic, but it was really 1+1x10^-6 times the adiabatic rate (and you knew the photospheric temperature exactly), your calculation of the temperature at the base of the convection zone would be off by a factor or (1+1x10^-5).  Other effects (like convective overshoot and dynamo action) enter at the 10^-3 level, so the superadiabaticity is negligible.",null,0,cdmo4bd,1reh81,askscience,new,1
bkkgirl,"Well nothing's stopping you from using it except that few people know how to use it, and very little has been translated to it.

Also, people with different accents would _write_ differently. This is critically important in languages such as Chinese, where the differences would render every dialect mutually unintelligible, and somehwat important in languages like English, becuz eugeniks an da lik wud mak ritin litrl spekn had. Written language preserves etymology, whereas the IPA, which would produce different forms for the same word, does not.

Additionally, what is transcribed in the IPA is not entirely uniform, so representations would be ambiguous even among speakers of the same dialect.

Since people usually read by identifying words as a whole, direct transcription of what was said would be counterproductive and difficult to follow, and since that's what the IPA is for, it would be too.

Disclaimer: I can't speak AAVE, so my transliteration is probably shitty as fuck.",null,0,cdmkd36,1refad,askscience,new,9
protestor,"A thing about phonetic alphabets is that often two different sounds are interpreted as being the same phoneme in a given language (they are [allophones](http://en.wikipedia.org/wiki/Allophone)), but on a different language they might be distinguished. On a given language the preferred allophone might depend on region, for example. The fact that two sounds may be interchangeable is called [free variation](http://en.wikipedia.org/wiki/Free_variation):

&gt; When phonemes are in free variation, speakers are sometimes strongly aware of the fact (especially where such variation is only visible across a dialectal or sociolectal divide), and will note, for example, that tomato is pronounced differently in British and American English, or that either has two pronunciations which are fairly randomly distributed.

[Each language has its own set of phonemes](http://en.wikipedia.org/wiki/Phoneme#Numbers_of_phonemes_in_different_languages). Some languages don't use tone to distinguish phonemes (but use them for other things), others use a lot.

This kind of non-uniformity may negate any advantage in uniformizing our writing system.

I also find the latin alphabet pretty convenient to type in a keyboard, but the IPA is less so, because it has too much symbols. (also, IPA is sometimes too specific - how to represent a word that we don't know how to pronounce?)

(ps: I suppose you're suggesting we use IPA to substitute alphabets already in use, instead of using IPA just for phonetic transcription)",null,0,cdmqbkr,1refad,askscience,new,2
fishify,"Hybrids have both an internal combustion engine and an electric drive system, which enables them to achieve better efficiencies in a few ways. One is that they recapture energy that would otherwise be lost; regenerative breaking allows the energy lost to waste heat in a standard car to instead be used to store energy in the hybrid's batteries. Another is that the internal combustion engine can be smaller, and thus operate more efficiently more of the time, since the electric motor is available for peak demands. In addition, the internal combustion engine can be turned off in situations in which a car is idling.",null,0,cdmfog1,1recy4,askscience,new,2
Pachacamac,"Actually you can't carbon date stone at all. Carbon dating needs organic materials with carbon-14 in them (an unstable isotope of carbon), so we need floral or faunal material. Burned seeds or charcoal are the best, but other organic materials can be dated. There are other dating methods that can date non-organic things and can date much older things that radiocarbon dating (which maxes out at 75,000-100,000 years), and some of these are useful to archaeologists/paleo-anthropologists, but radiocarbon dating is the most common method that archaeologists use. 

I'll mention here that there is one method, obsidian hydration dating, that can actually determine how long it's been since a piece of obsidian (volcanic glass commonly used for stone tools) was broken, which happens when the tool is being made (basically you start with a larger rock and chip away at it to shape it into what you want), but this method has a lot of problems and isn't always reliable. It's about the only way to directly date stone tools that I can think of, though.

So, we can't date the actual. How do we determine how old something like a stone tool is? We rely on one of the key assumptions in archaeology that things found together were probably made and used at roughly the same time (radiocarbon dating has an error range of 25-100 years anyway, so ""same time"" can mean same decade or same century). If you find a stone tool within a fire pit, say, then you assume that someone threw it in there during a fire, and the fire pit will have lots of organic material that we can date, So we date the pit and assume that the tool is as old as the pit. That is the most straightforward example I can think of, but the basic idea, dating by association, is how we get specific calendar dates for most of our sites. Same thing if we get a stone tool and a piece of charcoal at roughly the same depth in a site that we know has not been disturbed, you can date it by association.

Edit: just took a look at the article you linked. They've dated those tools to 280,000 years ago, not 85,000 years ago, so they would definitely not be using radiocarbon dating. I don't know what they used. The article is a bit hyperbolic but just keep in mind that, especially with those really early sites, there is a lot of room for error or unknown things complicating the picture, and a ton of room for interpretation, so the big claims that the article makes might be a bit presumptuous. As always, more research is required.",null,0,cdmfnvd,1rectr,askscience,new,2
Solivaga,"There's a wide range of radiometric dating techniques, but as /u/Pachacamac points out, you can't use radiocrbon dating on inorganic materials (such as stone), and radiocarbon dating is only really accurate back to around 50k BP, and completely fails much beyond 75k BP.

The short answer is that we use context and stratigraphy to securely sequence artefacts and features - in turn this allows us to identify material as being conteporaneous.  This enables us to date other material that's from the same phase of occupation or activity as the stone tools.

Dating techniques that stretch further back than C14 include Potassium Argon, Uranium Series, Fission Track, Electron Spin Resonance, abd Obsidian Hydration.  The problem with many of these is that they date natural events (including volcanic rock formation, formation of calcium-carbonate etc.), so often we'll be using these dates natural events to constrain the archaeological materials - i.e. we know that this palaeolithic site was occupied sometime between x and y.

",null,0,cdndjg2,1rectr,askscience,new,2
Platypuskeeper,"&gt; Hybridization is a generally good theory, but it doesn't explain properties like magnetism.

Valence-bond theory actually explains the paramagnetism of oxygen, if that's what you're referring to. (and has since the start, it's in Pauling's ""The nature of the chemical bond) It's a common myth though, so anyway...

You have antibonding orbitals because of symmetry. Each 'even' (symmetric) state has a corresponding 'odd' (antisymmetric) state. Now, I don't expect you to get what that means, so I'll demonstrate:

Two hydrogen atoms get close, and their atomic 1s orbitals combine to a _molecular orbital_. (The 1s orbitals are spherical and have a wave function that's like exp(-r), if you neglect constants). We assume for the sake of this example, that they form a linear 'superposition'. The combined wave function is simply the sum of the functions times some constants. 

There are only two possible combinations here: which is 1s_1 + 1s_2 and 1s_1 - 1s_2. This is because the overall phase (sign) of the function doesn't matter. so -1s_1 - 1_s2 is the same thing as 1s_1 + 1s_2. 

In the first one 1s_1 + 1s_2, where they add up, then the electron density is above zero everywhere, since the 1s orbital is exp(-1) and above zero everywhere. So there must be electron density all the way between the two nuclei. It's a _bonding_ molecular orbital.

In the second molecular orbital these two can create, 1s_1 - 1s_2, there is a spot at the exact center between the two nuclei where 1s_1 and 1s_2 are the same (because it's the same 1s orbital and the same distance r from their respective nucleus). So the total wave function there is _zero_. There's a region between the nuclei that lacks electrons! This is an _antibonding_ orbital.

[It's easier to see the thing visualized](http://www.expertsmind.com/CMSImages/2087_bonding1.png)

The antibonding MO has higher energy than the bonding one (fortunately for chemistry). A visual rationalization for this is in there's a higher curvature of the antibonding MO. After all, from one nucleus to the other it has to pass through zero. In quantum mechanics, a higher curvature of the wave function (more tightly located electrons), means higher kinetic energy. So the kinetic energy is higher when you have a 'node' like this (nodes being these areas of zero density, as with where a wave is zero). 

All this holds true whichever orbitals you combine to form your MOs. An antibonding orbital is formed for each bonding one, and the antibonding one has higher energy. 

(Note that the 'formation' here, just as with hybridization, is really just a way describing things. MOs don't suddenly form at a particular distance, it's a seamless transition from AOs to MOs)
",null,0,cdmet34,1recfy,askscience,new,7
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,new,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,new,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,new,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,new,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,new,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,new,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,new,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,new,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,new,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,new,1
HV250,"You seem to be confusing voltage with current. Voltage is just the potential difference required for current to flow. How much current actually flows is what determines whether you have enough for all components. As the current is consumed, the voltage slowly dwindles over time, till a point where the potential difference is simply not sufficient to let the charges move. That's when you need to charge it.",null,0,cdmh1b3,1rebi1,askscience,new,10
kizzap,"There are a number of things that could be happening. 

First, it would be most likely be connected in *parallel* not in series, thus the processor will be getting the 3.7V as well. LEDs take such small current too that a single LED will run for quite some time off that battery.

Secondly, it is quite possible that there is a switching power supply in the controller, which changes a lot of things.

Third, not all LEDs are 3 volts... ",null,0,cdmicso,1rebi1,askscience,new,3
iorgfeflkd,"Yeah, for example a red dwarf orbiting a much brighter star. When the dwarf is transiting, there will be less total light coming from the system.

[Here](http://arxiv.org/pdf/1109.2055.pdf) is a paper where they tried to measure this loss of light from a red dwarf orbiting another star.",null,1,cdmfm8v,1reb7p,askscience,new,7
humanino,"You can access the article here :  
[Thermoelectrically Pumped Light-Emitting Diodes Operating above Unity Efficiency (pdf)](http://dspace.mit.edu/openaccess-disseminate/1721.1/71563)

Please note that they have not broken any thermodynamical law. They have a device which uses electrical power, and converts this power into heat and light. The power emitted in the form of light is larger than than the part of the electrical power directly used to create light. That is because the other part of the electrical power, which created heat, has also been re-converted into more light. That is really neat and clever, and it does have potential applications, but the ""communication"" part might have been misleading. ",null,0,cdnyxu0,1reb7j,askscience,new,2
rileymanrr,"Absolutely. This is the most basic idea of materials science. You asked specifically about an alloy of gold and tungsten, and off the bat I wouldn't know what to make of it. There are equations and general rules to help predict what kind of crystal structure, density, conductivity (thermal and electrical), and phases may be present for different materials.


The first place I would start is the gold/tungsten phase diagram and find out what percentage I had of each, so I could figure out the different phases present. Next thing I would do is grind it into a powder and throw it into an X-Ray Diffractometer, and see what it makes of it. This would give me more information on it's crystal structure.


Next up would be mechanical testing. I would use a Charpy impact test a several temperatures to try and determine its glassy transition temperature, if possible. Then a good old fashion tensile test for its elastic modulus and a compression test for its bulk modulus. 




As someone studying materials science I would guess that a gold tungsten alloy would have good thermal and electrical conductivity, but it's mechanical properties are a bit up in the air. The only thing I would feel confident in saying is that it would probably be very dense.",null,0,cdmq26o,1re8v6,askscience,new,2
stevenstevenstevenst,"One way it is possible to determine age of a material vs. when a tool of that material was crafted is to compare the age of the material (easily determined by any number of techniques, such a radiocarbon dating and other isotopic methods) and to compare the quantity of atmospheric carbon adsorbed to the surface of the tool.  Quantity of adsorbed surface carbon (also known as adventitious carbon) is proportional to the amount of time the surface has been exposed to atmospheric conditions, and thus a comparison of adventitious carbon quantity of a surface known to have been exposed in the manufacture of the instrument and the isotopically-determined age of the material is informative.  

Other techniques are possible, but various analysis of oxidation, carbon adsorption, or other surface chemical phenomena are generally utilized.",null,0,cdmdtae,1re8mg,askscience,new,2
jessickofya,"To date when the tool was used we would look at residue on the tool and date that. So for example, if we found a stone tool with blood we can use dating techniques to get a estimation on when the tool was used. Depending on what material you are dating - you would use one of many different techniques.

There are also ways to break down rock into a gas and estimate the age of formation. Archaeology is all about context too. If we found the tool with a hearth or camp we could look at dating other items and estimate the age of when the artifacts were used based on the dates of surrounding artifacts in the same area. We can even use tree rings, dendrochronology,  to estimate the age of the wood used in the site and assume the age would be similar",null,0,cdmdxqu,1re8mg,askscience,new,2
Pachacamac,"Someone else just asked a pretty similar question and I saw theirs first, so I answered it first, and left a pretty detailed response. [You should probably just take a look at it.](http://www.reddit.com/r/askscience/comments/1rectr/how_do_scientistsarchaeologists_carbondate_human/cdmfnvd)

Basically, with most types of stone we can't date the stone at all (so we don't know how old it is, expect by talking to geologists who tell us that it comes from a certain formation of a certain age. But we don't typically care about that). We figure out when the tool was made by assuming that it was made, used, and discarded within a relatively short period of time (a century can be ""short"" to us because of the error ranges that all the different dating methods have, but stone tools wear out and break quickly so anything was probably used and tossed away in the same year that it was made). So because it was discarded at a site, we assume that it is as old as the site itself, and we date the tool by association; i.e. it was found with other things that we can date directly (like charcoal on younger sites, or layer of volcanic ash for sites as old as the one in this article), so we assume that it is as old as those things. So the fact that the rock itself might be 400 million years old doesn't matter; we find a tool at a site that we can date to 280,000 years ago and we assume that the tool and the site are the same age, as long as there is no evidence to suggest otherwise.

Now, I said with most stone. Obsidian is different. There's a method called obsidian hydration dating that we can actually use to date obsidian tools, which are what was found at the site in the article. When you make a stone tool you are always chipping away and breaking the surface, so when the tool is brand new it will have a fresh surface. Obsidian weathers at a known rate so you can look at the surface and determine how old it is by how much weathering is on it. This isn't a perfect method and it can't really tell us exactly how old the tool is (because there's so much variation across regions), but it can tell you that one tool is older than the other. Maybe they can get actual calendar years for Ethiopian obsidian too, I don't know (I'm not familiar with the area).",null,0,cdmfyqt,1re8mg,askscience,new,2
StringOfLights,"It is not so much that terrestrial mammals were big back then, it's that they're small now. Mammals [increased in size following the Cretaceous-Paleogene extinction and maintained that large body size](http://www.sciencemag.org/content/330/6008/1216.short) for nearly 30 million years years.  Then there was an [extinction at the end of the Pleistocene](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.1991.tb01149.x/abstract). Most vertebrate taxa made it through this extinction, but a lot of large-bodied animals, and especially large-bodied mammals, were hit particularly hard. Some 150 genera of megafauna (defined as animals &gt;44 kg) existed 50,000 years ago; [97 of those were extinct by 10,000 years ago](http://www.sciencemag.org/content/306/5693/70.full):

Given how geologically recent these extinctions are, it's extremely unlikely that anything would have been able to fill the gaps left by the loss of megafaunal mammals, as there appears to be a [maximum rate](http://pnas.org/content/early/2012/01/26/1120774109.abstract) that mammals can increase in size. In that sense it's completely expected that a recent extinction event would leave a gap in body size. 

Also, in all of this discussion it's worth bearing in mind that we're generally talking about terrestrial mammals. There are plenty of large marine mammals still around (for the time being), including the blue whale!

**Edit:** Forgot something! In terms of dealing with cold weather, having a larger body size actually slows heat loss because it lowers the surface area to volume ratio. So while larger mammals had to eat more overall, they [spend less energy per unit of body mass](http://www.planta.cn/forum/files_planta/511_131.pdf) producing heat. This was the original logic behind [Bergmann's Rule](http://en.wikipedia.org/wiki/Bergmann%27s_rule).

",null,1,cdmdkaq,1re5lq,askscience,new,9
KarlOskar12,"That depends what you mean...The major regulators of the cell cycle are [p53 and p27](http://puu.sh/5sB2h.jpg). They both halt the cell cycle, p27 specifically does it by binding to and blocking the action of cyclin and CDK preventing the cell from entering the S phase of the cell cycle (DNA replication phase). Once the cell cycle is halted, the cell is either repaired (let's say for DNA damage). If repair is not possible or too costly, the cell is told to undergo apoptosis (kill itself). This is done by activating [Caspase 3](http://en.wikipedia.org/wiki/Caspase_3) which systematically breaks down the cell by expelling all the water, chopping the DNA up in an orderly manner, degrading the nuclear membrane, degrading the golgi apparatus, blebbing the cytoplasmic membrane, etc.",null,0,cdmen5i,1re5ig,askscience,new,2
Platypuskeeper,"The electromagnetic field. It's everywhere.

Somebody is inevitably going to chime in here with virtual particles and whatnot, which are quantum-level descriptions of _how the field works_. But at the end of the day, the 'medium' is the same: Space itself.
",null,1,cdmdzfk,1re5f5,askscience,new,11
fishify,"Not every wave needs a medium other than the vacuum in which to travel. Nineteenth century physicists did not recognize this, and thus postulated that the universe was filled with a substance they called *the ether*, which would serve as the medium for light waves.

Einstein in 1905 showed there was no need for an ether. As we understand it today, light travels through space just as an electron does. One way to picture this is to remember that light is made of photons (particle of light), which readily travel through space and which form electromagnetic fields and waves.",null,1,cdmfxew,1re5f5,askscience,new,10
killer_alien,"Light is an electromagnetic wave, and therefore does not require a medim to propagate through. On the other hand, waves that need a medium are mechanical waves. Theses include longitidinal, transverse and torsional waves. e.g. sound waves are longitudinal waves",null,0,cdmi1mi,1re5f5,askscience,new,2
animationb,"When a field gets enough energy, it ""manifests"" as some fundamental particle. For the electromagnetic wave, energy creates a photon. In sort of the same way matter helps ""create"" (or comes with) a gluon, the fundamental particle for gravity.",null,2,cdmng0x,1re5f5,askscience,new,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,new,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,new,45
null,null,null,13,cdmjqhe,1re305,askscience,new,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,new,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,new,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,new,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,new,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,new,1
eliareyouserious,"A presynaptic (fibre) volley can be observed in extracellular field potential recordings. It is caused by activation of (several) presynaptic fibres (usually using a stimulation electrode), which in turn fire and activate their postsynaptic partner. A brief negative potential preceding EPSPs is indicative of presynaptic action potential(s) and is termed the ""presynaptic volley"". Fig.2C on page 92 in this book indicates the volley in a recording: http://books.google.ch/books?id=y_ucmaDffXsC&amp;dq=presynaptic+volley&amp;hl=de&amp;source=gbs_navlinks_s (The link to the book chapter also serves as reference here). ",null,0,cdnryhk,1re1qb,askscience,new,1
fishify,"Depending on your background, this article might be helpful to you:

""The Pumping of a Swing from the Standing Position."" William B. Case, American Journal of Physics, 64, 215 (1996).


",null,1,cdmg0m4,1re182,askscience,new,3
Shitler,"As I understand it, motion happens because the swinger shifts their center of gravity, causing gravity to have to recenter the pendulum. However, as is in the nature of pendulums, gravity overdoes it and the swinger ends up on the other side of equilibrium, at which point they shift their center of gravity again. And so on.

Energy is introduced into the pendulum when the swinger shifts their center of gravity by extending or contracting their legs.",null,1,cdmgk9l,1re182,askscience,new,3
jofwu,"I'm just going to describe the process...

When swinging forward you lean back, stick your legs out, and pull on the chains. *By leaning your torso back and kicking your legs out you apply torque to your body.* This torque is balanced by pulling on the chains. Imagine trying to perform this action without holding on to or pushing off of something- you can't. Note that the chains bend where you hold them. The line of action of the tension in the chains is *behind* your center of mass. This is where the balance in torque comes from: force (tension in chains) x distance (between force's line of action and your center of mass). On the backswing, everything is the opposite. You pull your torso forward and bend your knees back in, and to balance this out you need a torque in the opposite direction. So you *push* forward on the chains, and the line of action of the chains is *in front* of your center of mass.

*Making these transitions leading up to the peak of your swing is the key.* The movements don't do anything if they aren't timed right. By performing the forward swinging motions, you add some gravitational potential energy at the top of the front of your swing. The back swinging motions add energy at the back end of the swing. *The energy gained is thanks to that little distance you create between you and the chains' line of action- putting you a little bit higher from the ground than if you had just swung freely like a pendulum.* Of course this gravitational potential energy results in more speed/momentum at the bottom. *And I think it's worth mentioning that you don't conjure this extra energy from nowhere. It comes from you body.* The gravitational potential energy you add wouldn't be possible without applying a torque to your body. 

In the end, it's not that much different from swinging on parallel bars. Rather than balancing your torque by pushing/pulling on a chain, you apply a counter torque directly to the bar you hold (with a firm grip).",null,0,cdmmc2l,1re182,askscience,new,2
GlowInTheDarkDonkey,"My understanding, as a (uh oh) layman, is that a person on a swing is basically taking advantage of angular momentum in the same way a figure skater tightening their limbs in a spin makes them spin faster.

A shortening of the total length of the swinging body on the upward swing means gravity is being applied to a total body that has less distance to travel (is a shorter swing-arm), and then on the downward swing the thrusting of legs outwards allows gravity to work on a longer swinging body... which again is then shortened on the upward movement.

Some of the angular momentum of the legs themselves also adds to the total forces being shifted around.

When someone is standing on a swing seat you'll notice they put all of their mass to the seat on the down-swing, and then they stand on the upswing.  This, similarly, means gravity is pulling a longer swing-arm (in terms of average mass distribution towards the outermost edge of the arm) on the downward stroke compared to the upward stroke.

I'm curious if someone in a white-coat finds this answer agreeable or not.",null,6,cdmdqxr,1re182,askscience,new,5
fishify,"The energy of the initial and final states in beta decay, as in other processes, have the same energy. The W boson that appeas as an intermediate particle in the standard desecription of the process is a so-called virtual particle. In particle physics, our calculational scheme known as perturbation theory tells us that we can calculate what happens using intermediate states known as virtual particles which have the energy, momentum, and other conserved quantities you'd expect; but this also means they have the 'wrong' mass.  We say they are *off mass-shell*.

These virtual particle that appear in calculations are never actually observed. Any W boson you actually detect will have the expected mass of 80.4 GeV/c^(2), or just under 86 proton masses.",null,1,cdmfttr,1re0d7,askscience,new,3
cass314,"In both beer and soda, the bubbles are caused by carbon dioxide coming out of solution.  The big difference is what's there to ""catch"" the bubbles and hold them.  In soda, there's not much at all.  In beer, there are proteins.

Soda is mostly water, sugar, salt, and acid.  There's not a lot to give structure, so the bubbles die out quickly, and after a few minutes you can hardly tell there was ever any foam.  Beer, however, has proteins leftover from both the mash (wheat or barley, usually) and the yeasts that did the fermenting, and it's the proteins that give beer such an interesting head. Proteins, especially hydrophobic proteins (they ""like"" oil better than water) and denatured proteins with their inner hydrophobic parts exposed, tend to clump together into structures (many to avoid interacting with water).  These structures can trap air bubbles.  

You can think of it like a less extreme example of whisking sugar water vs. whisking sugar and egg whites.  If you whisk or shake water, you'll get bubbles, but they'll pop very quickly after you stop.  If you whisk egg whites long enough, you'll get meringue.  ",null,0,cdmczfs,1rdy9r,askscience,new,3
Truck43,"The lighter works because the butane is a liquid under pressure, opening the valve lets it spray out and be ignited by the flint. When it's very cold, it contracts, reducing the pressure in the fuel vessel, and it's less volatile, this reduces the amount of fuel that is expelled.  ",null,3,cdm9hep,1rduf4,askscience,new,8
adlermann,butane's(what bic lighters use for fuel) vapor pressure drops to near zero at atmospheric pressure about 40F not enough gas is released to fuel a flame.  That is why natural gas and propane are used for heating despite butane's higher energy potential,null,2,cdm9ieg,1rduf4,askscience,new,4
Platypuskeeper,"&gt; I'm assuming the lighter fluid has less energy therefore it's lazy.

That's one way of putting. A more formal but roughly equivalent way would be to say that the pressure over the liquid butane in it, is lower when at a lower temperature. The equilibrium is shifted towards more liquid and lower pressure at lower temperatures, higher pressure and less liquid at higher temperatures. 
",null,2,cdmae2f,1rduf4,askscience,new,4
Platypuskeeper,"Sea salt is from evaporating seawater, table salt either comes from the sea or from salt mines. 

When you say ""table salt"" you're referring to one single compound: Sodium chloride. The vast bulk of the sea salt, and virtually all of what's 'table salt' is sodium chloride. Sea salt has some other salts in it, how much and what depends on where it's from. Table salt is often [iodized](http://en.wikipedia.org/wiki/Iodised_salt), meaning they've added some iodine salts as a dietary supplement. (Lack of iodine causes developmental disorders and thyroid problems) Depending on the salt it might have small amounts of stuff to avoid it caking together too, which aren't usually added to the stuff marketed as 'sea salt'. (I don't believe there's any _requirements_ on this though)

So sea salt has some other minerals in it, but it's such a small part and you use so little salt, that it probably doesn't make a significant impact on your overall mineral intake. The iodization of salt has had a measurable impact on iodine deficiency-related stuff since it was introduced in the 1920s. For the individual any health effects would depend on whether you get enough iodine from other sources. 

The biggest differences are really taste and texture more than anything, though.
",null,2,cdm93o8,1rdu42,askscience,new,6
225274,"Sea salt is the salt produced by evaporating sea water. Table salt is the same thing, just crushed into a fine powder, with fewer impurities of other salts like KCl, and is often iodized, i.e. has added iodine salts. 

Table salt is healthier as iodine is not so commonly available in our diet, but is an essential mineral. 

",null,2,cdm8et4,1rdu42,askscience,new,4
chuck10470,"The difference between table and sea salt is the iodine. Fresh from the factory it contains 50 ppm iodine. That's all. 50 ppm. As it ages, the iodine evaporates out, losing half every 40 days.

All salt comes from the sea. Or a lake. Mined salt has precipitated out over thousands of years and built up thick beds that became buried through mountain building. Ironically, most of these mines are today quite some distance from the ocean. The Swiss city of Salzburg has several salt mines,  though it is hundreds of miles from the ocean today. 

And some of this mined salt is quite old. But whether it precipitated out at the bottom of the Tethys Sea 65 million years ago or last week in Sardinia, it's still 99.9% NaCl and 0.1% other minerals. Most of the table salt, industrial salt, road salt, animal feed salt, etc, is mined. Sea salt is precipitated out in huge evaporation ponds. It should be noted that sea salt can be iodized and become table salt, and much of it is. ""Sea salt"" is a marketing name given to un-iodized salt produced by evaporation. It supposedly has better taste, but since it's nearly impossible to determine which minerals give it a specific favor profile, maybe it does, maybe it doesn't. It depends on where it's from. The expensive $10/lb culinary salt is usually sea salt, but with additives like smoke or truffles. The various types of salt available at the grocery store differ mostly in the iodine content and the shape of the crystals. That's it. You can use regular iodized table salt for nearly every application you have,  except for canning. The iodine turns some stuff brown.",null,0,cdmf52z,1rdu42,askscience,new,2
Smoothened,"The machinery behind X-inactivation specifically targets the X chromosome as opposed to any chromosome. For example, the gene XIST is located on the X chromosome and is required for its inactivation. When this gene is expressed, its transcript, a long noncoding RNA (Xist) coats the respective chromosome, becoming involved in its silencing. A chromosome lacking XIST would not undergo inactivation. If you insert the XIST gene in an autosomal chromosome, that chromosome can then be inactivated. 

A more interesting question is how is inactivation targeted to one of the chromosomes in each cell. That question is not entirely answered, but it is believed that an autosomal gene encodes a blocking factor that prevents one X chromosomes from being inactivated. Interestingly, even when there's more than 2 X-chromosomes present, only one remains active in each cell. ",null,0,cdmbqmg,1rdsv9,askscience,new,5
ProfEntropy,"Postmortem fluid and tissue toxicology is able to quantify both the drugs and alcohol present at the time the samples were taken.

Connecting that back to the amounts present at the time of death can sometimes be difficult. For example, many drugs are known to partition into different parts of the body after death. Knowledge of this, and sampling tissues and fluids from the proper place will help get more accurate measurements.

Many other factors must be considered when looking at ethanol concentration. See [this article](http://www.sciencedirect.com/science/article/pii/S0379073806002891) for a good review of postmortem alcohol concentrations and how they relate to BAC at time of death.",null,2,cdm82ym,1rdosg,askscience,new,8
fastparticles,The event would melt most of Earth and put the upper mantle into orbit around Earth. At this point the moon is thought to come from Earth because they are so isotopically similar. The compositions of the moon and Earth do differ especially in terms of volatile elements (the moon for example is relatively depleted in potassium). ,null,0,cdm6y13,1rdhkw,askscience,new,2
Ejb90,"This revolves around the maximum power transfer theorem. There are two ways to look at it.
Firstly from a circuit-theory point of view, when power energy is transferred from one component to another, the maximum is transferred if they are the same resistance. Impedance is the more generalised, complex form of resistance.  This means that if they are matched in resistance (impedance) then the most power is transferred, which is most efficient.
The second way to look at isn't is from a waves point of view. At the frequencies you get in a transfer cable the currents can be modelled as electromagnetic pulses. When they reach a boundary some are reflected and some are transmitted, just like light is when it passes between air and water. When the two mediums either side of the boundary have the same ""resistance"" to the wave, more of the wave propagates through, as it's almost as if there is no boundary, so the maximum energy is transferred, as expected.",null,0,cdm4hjj,1rdd6o,askscience,new,8
SwedishBoatlover,"You should *really* watch this [video](http://youtu.be/DovunOxlY1k) from Bell labs, where the host use a wave machine to visually show how waves work. You can actually get an intuitive feel for what the impedence matching does, it's very interesting!",null,0,cdm9s27,1rdd6o,askscience,new,2
rat_poison,"This is the distilled wisdom of my Microwave Networks experience regarding impedance.

The most important defining feature of the transmission line is its characteristic impedance. It is affected by the shape of the transmission line and materials that make it up. Generally, for a TL extending to the z direction, we can divide the whole length in small parts of length Δz. Τhose parts can be arbitrarily small: so much so that we can ignore any radiating properties within that Δz. We can therefore make a lumped-circuit equivalent of that Δz length of the tramsission line.
Movement along the length of the transmission line will mean some ohmic resistance (R) and some inductance (L). The neighboring of metal surfaces will cause capacitance (C) and the material between them will cause dielectric losses based on a conductance (G).

As the electromagnetic wave travels through the transmission line along direction of propagation J, we can generally define functions I(z) = I+(z) + I-(z) and V(z) = V+(z) + V-(z)

So current and voltage are made up of two constituents: the first (+) representing movement along the direction of propagation and the second (-) representing the part of the current and voltage that are caused by reflection and therefore are moving opposite the direction of propagation.

Characteristic Impedance is the ratio Z_0(z) = V+(z) / I+(z)

for the length of Δz I have described earlier, it is calculated as Z_0 = sqrt((R+jωL)/G+jωC)) (j = sqrt(-1))

In most cases, trasmission lines are uniform in the z direction, or they are made up of a cascade of uniform parts. Either way, for every uniform TL, the characteristic impedance is the same no matter which Δz I choose within it (as long as it's small), so that's why it's such a defining property of a TL.

We can then define another quantity, Γ(z)=V-(z)/V+(z), called the reflection coefficient. This tells us what is the ratio of reflected and forward waves. Its amplitude is 0 if there are no reflections, 1 if the reflections and the forward waves have the same energy therefore leading to standing waves not able to propagate energy, or an-inbetween state for the other values in between.

If z=l, then we have calculated that Γ_Load = (Ζ_L-Z_0)/(Ζ_L+Z_0). For a lossless transmission line, this will have constant amplitude throughout its length.

Now we want to minimize energy lost in reflections. So Γ should be 0.

If you look closely at my last equation, you'll see that this can only be true if Z_L = Z_0.

Regarding the part about the return line.

When dealing with high frequency circuits, a return line is not necessary. BUT NOT because of impedance matching. If the outer shell or the inner wire of a coaxial cable didn't exist, it wouldn't posses the geometrical properties that induce the field to operate in the desired way. There wouldn't be two points with different potentials along the direction of propagation around which the EM energy could oscillate back and forth. In fact the concepts I have just described break down. BUT there are transmission lines that don't have a return wire: these are waveguides. The wave DOES oscillate back and forth, but the points are not as strictly defined as in two-wire TL's or coaxial cables. Instead, we have modes: depending on the ratio of the wavelength and the dimensions of the waveguide, there are (possibly several) nulls and peaks at the transversal plane, that are defined by how many half-waves fit into that dimension. These nulls and peaks are now the places around which the energy fluctuates in order to propagate forward. Waveguides are the reason you should be careful when using current and voltage concepts on microwave circuits. Therefore you should just stop thinking about TLs in terms of a phase line and a return line, but a single structure which guides the waves along a direction and (maybe) causes reflections along the opposite direction.
",null,0,cdmbm4q,1rdd6o,askscience,new,1
selfification,"http://www.youtube.com/watch?v=DovunOxlY1k

This is a classic that explains all phenomena.  Standing waves, interference, impedance matching, refraction, reflection...  everything.  All in one video.",null,0,cdmcdgz,1rdd6o,askscience,new,1
s3c7i0n,"As a basic reply, dogs, like cats, have a reflective coating at the back of their eye, which helps them see in low light situations. The color of the coating is based on the color of the eye, which has some evolutionary benefits having to do with common colors in various environments, but the gist of It is that the colors are caused by the iris colors. 

(edit) the blue eye is red due to a lack of pigment in the reflective layer, so you're actually seeing the reflection of the blood vessels in her retina. ",null,1,cdm3dji,1rdd4v,askscience,new,3
owaisofspades,"Your thyroid hormone is responsible for regulating the metabolic rate of most of your body. When you have thyroid insufficiency, your metabolic rate drops and your body no longer functions at full effectiveness. The concentration problems are likely a secondary effect of the lethargy and weakness that are caused by hypothyroidism",null,0,cdmb0lv,1rdckt,askscience,new,2
iorgfeflkd,"Beta decay involves a neutron turning into a proton and emitting an electron (beta particle) and an antineutrino. Static electricity involves movement of pre-existing electrons. Nuclear reactions generally involve much higher energies than electronic or atomic. For example, beta particles from potassium decay in bananas have as much energy as if they went through a 1.5 million volt potential, and static discharge is typically in the thousands. However, static discharged generally involves a lot more electrons compared to most radioactive sources.",null,1,cdm28la,1rd953,askscience,new,7
zalaesseo,"When Benjamin Franklin said that Charge can only be collected and lost, he really meant it. When you discharge electricity, electrons just moves to the metal object.

Until beta decay. Beta decay literally creates a new proton electron pair and an antineutrino^irrelevant. You're not collecting charges, you're MAKING charges appear from nothing.   ",null,0,cdm2ovh,1rd953,askscience,new,3
cosmicosmo4,"When you get a static shock, you're typically experiencing millions-billions of electrons being transferred over thousands or tens of thousands of volts, and they're only doing that because the recipient object (be it you or the metal railing) has a positive charge, meaning there are places for those electrons to settle once they get there.

When a beta particle is emitted, it comes with an energy in the range of millions of volts, and there's no predesignated spot for it to settle, meaning it will fly straight through things until it happens to find a spot to settle, often by displacing some other electron. This is what makes it ionizing radiation.",null,1,cdm357l,1rd953,askscience,new,4
iorgfeflkd,"Protons and neutrons are held together by the strong nuclear force (or a residual form of it, sort of the equivalent of van der Waals forces for nucleons), which in stable nuclei is much stronger than the electrostatic repulsion between protons. If a nucleus has too few neutrons then the repulsion will break it up.",null,1,cdm1vxx,1rd8yh,askscience,new,5
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,new,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,new,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,new,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,new,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,new,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,new,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,new,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,new,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,new,13
null,null,null,9,cdm64ib,1rd8ip,askscience,new,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,new,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,new,1
brawnkowsky,"different ethnicities will have different genes that express proteins differently.  For example, [degrees of lactose intolerance vary between regions, from 5% in north europe to 90% in some african and asian countries](http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0100-879X2007001100004&amp;lng=en&amp;nrm=iso&amp;tlng=en).  this is simply because of altered protein expression (lactase in this example), which is a factor in all protein expression in our body.  also, people will have varying levels of gut microorganisms depending on their environment, what they eat, and their own immune strength; this natural flora is important in digestion.

",null,0,cdnfwoy,1rd803,askscience,new,2
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,new,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,new,16
patchgrabber,"Well, kelp are basically algae, so they are quite different from land plants in pretty much every way except photosynthesis. Although their holdfasts resemble, and may be a primitive form of, plant roots, kelp are fundamentally different. 

In most land plants, while very limited photosynthesis may occur in the stalk of the plant, most of its photosynthetic activity is in the leaves. Kelp, in contrast, photosynthesize in every part of the organism (although different parts have different levels of photosynthetic ability depending on age), allowing for more and making light less of a limitation than it is in land plants.

The environment the kelp lives in is also a big factor. Since it is under water, light is attenuated differently than above water. Due to the large amount of particulates, blue light is attenuated rapidly in coastal waters, and blue light is much more valuable than red light that can penetrate deeper at a higher intensity.

While there are products out there that purport to use kelp in them to make plants grow faster, I'm thinking this is only because of the nutrients, not any special property that is linked to kelp growth. I cannot think of any way at present to genetically transfer this quality to land plants; their limitations are different, their environments are different, and they are just fundamentally different organisms. Kelp would be much better used as fertilizer, as you suggest, than as a source of genetic information, although in the future that may change.",null,0,cdmahyq,1rd7fs,askscience,new,2
MarineLife42,"As /u/patchgrabber said, Algae are very different from plants.  
Here, the main difference is that (higher) plants grow, i.e. create new tissue only at specific regions on their body. Usually this is at the tip of the plant or leaf, the apex. In grasses (grains) it happens at the nodes too.  
Kelp, on the other hand, creates new tissue along the entire length of its thallus (the big leaf) which is why in grows so fast.  
Another big difference is that the thallus doesn't have much, if any speciation; it is composed of more or less the same kind of cell. Higher plants, in contrast, have an internal structure of xylem, phloem, bark etc. that requires many different specialized cell types.  
Both these differences work together to prevent us from simply transplanting this ability into our crop plants. 
BTW - some bamboo species can also grow very fast, up to 10cm a day or so but there is trickery involved. In fact the plant tissue has been created at the usual speed beforehand, but compressed. During the elongation phase, the plant sucks up much water and fills the cells so it telescopes upwards. ",null,0,cdn4dss,1rd7fs,askscience,new,1
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,new,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,new,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,new,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,new,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,new,6
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,new,43
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,10,cdmh86v,1rd6th,askscience,new,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,new,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,new,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,new,1
incognegro76,"You can graphically illustrate a line with this equation but it will form an asymptote very rapidly to zero.

y = 2^-x",null,1,cdm6f0c,1rd6ok,askscience,new,3
rlee89,"y=-ln(1-t)/ln(2) seem a good place to look.

For a runner running at velocity 1, y is the number of terms you have added to get the runner's distance at time t.  It is rather obvious that no matter how many terms you add, you will never reach the runners distance for any time after t=1.

This is derived from the closed form of the partial sum 1/2 +1/4 + 1/8 ... 1/2^n = 1-2^(-n).",null,0,cdmam7a,1rd6ok,askscience,new,2
ultimatety,"The answer to this is actually more complicated than you would think.  It all boils down to the fact that the surface layer of the ice underneath the object is partially melted.  However, the reason for how this top layer melts is somewhat of a scientific controversy.  People used to believe that the pressure exerted causes the ice to melt, however, this appears to be false.  
The two current theories are that: 
1) The friction of the moving object causes the top layer of the ice to melt
or 2) The top layer of water molecules are unable to bind correctly to the layers underneath and thus stay in a quasi water-like state.

TL;DR There is a little bit of liquid water on top of that ice, and liquid on top of something smooth makes it slippery.",null,3,cdm1crb,1rd6cm,askscience,new,19
ace425,"Adding on to this, why doesn't waters ability to form hydrogen bonds affect the slipperiness of ice? It seems like since water likes to form hydrogen bonds that ice would not be slippery but instead have a lot of traction, but this obviously isn't the case. Can someone expand on this please?",null,1,cdmjimu,1rd6cm,askscience,new,1
sharp12180,"When you step on ice, you apply pressure to the ice directly below you. This pressure decreases the freezing point of ice and so there is a thin layer of liquid water formed between your feet and the ice. Its this difference that causes ice to so slippery.
http://www.youtube.com/watch?v=Stx6kLd9dYI",null,20,cdm153f,1rd6cm,askscience,new,6
rupert1920,"Plastics are long polymers, and can undergo [polymer crystallization](http://en.wikipedia.org/wiki/Polymer_crystallization) when stressed. It is the formation of these ordered structures that causes scattering in the material - which is why it looks white.

In some plastics this process can be reversed by heating the plastic (for example, boiling it in water for a few minutes).",null,1,cdm8ncc,1rd67w,askscience,new,5
bohr_exciton,"The most probable explanation is that by bending the material you are creating defects, i.e. inhomogeneities in structure, density, etc. These defect sites can then act as scattering centers, which in turn reduces the transparency. This is a similar effect to scratching the surface of ice, for example.",null,1,cdm3rsa,1rd67w,askscience,new,4
iorgfeflkd,"tl;dr: If the laws of physics don't depend on location, momentum is conserved.

Noether's theorem says that for every symmetry in a process, there is a conserved quantity. For things that are translationally symmetric, that conserved quantity is momentum. This means that if you consider a collision on a highway, and then the same collision a couple of miles down the highway (translation), if they behave the same (where on the highway it is didn't matter), then momentum is conserved.

That's fairly complicated, another but less rigorous way of looking at it is that momentum changes when a force is applied, and if no force is applied then the change in momentum is zero, so in the absence of external forces the total change in momentum is zero.",null,1,cdm156w,1rd5ys,askscience,new,9
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,new,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,new,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,new,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,new,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,new,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,new,1
null,null,null,32,cdm23mb,1rd5mf,askscience,new,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,21,cdmhzt8,1rd5mf,askscience,new,92
null,null,null,8,cdm6gn5,1rd5mf,askscience,new,13
null,null,null,30,cdmgdmn,1rd5mf,askscience,new,22
xtxylophone,"Aside from the comparatively 'busy' time around the Earth's formation, nothing has changed. They are just infrequent and the evidence they leave lasts a long time.

Check out: http://en.wikipedia.org/wiki/Impact_event

Impacts that can change geography are about on the scale of the length of Human civilisation. Don't take the data for one being 'due' though. ",null,2,cdm0yuw,1rd5c8,askscience,new,7
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil™ alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barré Syndrome (paralysis lasting for years, or permanently — sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven “there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.”

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,new,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,new,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,new,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,new,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,new,3
null,null,null,31,cdm2wxo,1rd53a,askscience,new,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,new,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,new,19
null,null,null,11,cdm3al3,1rd53a,askscience,new,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,new,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,new,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,new,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,new,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,new,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,new,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,new,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,new,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,new,1
null,null,null,22,cdm2cds,1rd53a,askscience,new,16
rossk10,"In my realm (structural engineering), wind tunnels are used to simulate and predict expected wind loading to structures during specified gusts.  Smaller, to-scale models of buildings are built, hooked up with load sensors, and then placed in a wind tunnel that simulates a design storm and provides load data at critical points.

As for your specific question regarding smoke trails with cars, understand that my fluid knowledge comes from two fluid dynamics classes in undergrad.  I think that these trails are used to demonstrate how particles travel over the surface of a car, giving useful information about the aerodynamics and drag coefficient of the car.",null,1,cdm0p63,1rd4yo,askscience,new,4
phdpeabody,"There's a lot of different types of wind tunnels, and there's a lot of different tests that can be performed in wind tunnels. The tests with the smoke that you mention are to observe [laminar vs turbulent flow](http://www.youtube.com/watch?v=TqTSyFz6DJc), but these are far more often done under CFD analysis than tunnel testing anymore since the models have become highly accurate. This is done because the more disrupted the airflow, the more ""drag"" that is produced. Most of the data captured is just visualized data captured by cameras that is often interpreted mathematically. I work in supersonic research, and one of the more common aeroelasticity tests is to observe things like [flutter at transonic speeds](http://www.youtube.com/watch?v=Xqbkdy3tBdA), which can often be quite destructive. In other tests, like the vertical tunnel, [spin conditions are provoked in models](http://www.youtube.com/watch?v=M7QkTBKtyw8) to examine the performance limitation of an aircraft or research on how to recover from spin conditions such as the infamous [unrecoverable flat spin of the F-14](http://articles.baltimoresun.com/1993-04-06/news/1993096244_1_f-14-flat-spin-tomcats) made famous in Top Gun. The high-temperature liquid oxygen injected tunnel is another type of tunnel used to experiment with new [hypersonic engine designs](http://www.youtube.com/watch?v=F7b76SPlV2E&amp;t=3m50s) that allows researchers to observe performance characteristics at speeds of up to Mach 7 or more recently to observe the performance of inflatable heat shielding during re-entry conditions, and determining the endurance and tolerance of these new materials. One thing about modern wind-tunnel testing however, is that the tests are normally used to verify and validate what predictions the models have already made, as the newer and more advanced testing facilities can cost a project over a hundred thousand dollars to operate for a week or two.",null,0,cdmcdq3,1rd4yo,askscience,new,3
meerkatmreow,"The data from the smoke is a type of qualitative flow visualization.  Based on the behavior of the smoke, conclusions about laminar v. turbulent flow can be drawn.


The data from wind tunnel tests can come in many forms depending on what you're trying to do.  Full field quantitative measurements (using something like Particle Image Velocimetry or Pressure/Temperature Sensitive Paint) can be useful for exploring the entire flowfield.  Point measurements using pressure transducers can provide the needed data if you're interested in a certain area.  Data such as overall forces and moments on the model may be what you're after.  Qualitative measurements such as flow visualization uses smoke lines or laser induced fluorescence can help identify areas where additional investigation would be beneficial (ie, separate flow).

What you want to measure and how you measure it are very tightly coupled.  When you do a wind tunnel test you can often choose how you measure things by what you're interested in rather than using a one size fits all approach.",null,1,cdm59n4,1rd4yo,askscience,new,2
AbsolutePwnage,"The smoke shows were the air flow is laminar and where it starts becoming turbulent and therefore, where parasitic drag starts appearing. It also looks cool, which is why they show it very often in ads and other media.",null,0,cdojkoc,1rd4yo,askscience,new,1
user2097,"3rd year aerospace engineering student here. Wind tunnels are used largely for models to mimic equivalent flow conditions, and the data from the testing includes qualitative and quantitative data.

Sometimes your test is performed to verify dynamic stability, examine stall characteristics of aeroplanes), examine flow condition (separation, turbulence, mixing...), etc. Other tests will produce data based off sensors attached to the model or tunnel such as force on a wing, dynamic response to an input, measure location of separation with hot wires, etc. ",null,1,cdm4stc,1rd4yo,askscience,new,1
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,new,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,new,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,new,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,new,18
redmeansTGA,"First off, let’s look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I don’t know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldn’t be different) receive what’s called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldn’t have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally don’t have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands don’t often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, let’s look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (‘perching’) birds didn’t evolve to the early cenozoic either. Temperate deciduous forests also didn’t exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, it’s clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,new,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,new,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,new,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,new,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,new,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,new,2
skleats,"Cells receive and respond to survival/apoptotic signals independently, so the senescence or death of one cell does not directly impact those around it. This is key since [controlled apoptosis is a normal part of embryonic development](http://people.ucalgary.ca/~browder/apoptosis.html) in multicellular organisms. However, a multicellular organism relies on coordination of activities between its many cells, so having a large proportion of senescent or apoptotic cells would be likely to impact the ability of those cells to contribute to survival of the organism. [This article](http://www.ncbi.nlm.nih.gov/m/pubmed/15265523/) describes an *in vitro* model which mimicked chronological aging and showed reduced coordination between cells as they aged.",null,0,cdm6ky2,1rd0kh,askscience,new,2
miczajkj,"If you talk about two different charged particles, that only interact as a closed system (so no external magnetic or electric fields) the problem is equivalent to the [Hyrdogen atom](http://en.wikipedia.org/wiki/Hydrogen_atom). 

Therefore there are quantized stable orbits and a radiation of photons is not allowed without a time-dependent perturbation.",null,0,cdlym8i,1rcxw0,askscience,new,1
Platypuskeeper,"Classically, if you're accelerating a charged particle (and a particle moving in a circular pattern is being accelerated constantly), then you will give off radiation. Obviously if you had one particle orbiting another, you would need some kind of outside energy to sustain this, or the thing would give off all its energy and spiral into the other particle.

[Synchrotron light](http://www.iop.org/publications/iop/2011/page_47511.html), which is up in the X-ray range, is produced by moving electrons around at relativistic velocities.


",null,1,cdlyxjg,1rcxw0,askscience,new,2
KerSan,"This is *precisely* the problem that made physicists develop quantum mechanics. The answer to your question is 'no', because otherwise the particles would lose energy and crash into each other. Unless the particles are going to merge or something, this is a violation of the Heisenberg Uncertainty Principle because then you would know too much about both the position and momentum of each particle.",null,0,cdlz5t5,1rcxw0,askscience,new,1
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,new,47
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,new,2
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,new,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,new,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,new,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,new,1
PENIS_VAGINA,"90% of the blood flow leaving the glomerulus through the efferent arterioles perfuses the cortex (10% to the medulla) under normal conditions. The main purpose of this is to keep the medulla interstitial fluid hypertonic so that concentrated urine can be produced. I suppose that vasculature changes (i.e. arteriolar constriction) could reroute some of that 90% of blood flow into the medulla to aid in decreasing the hypertonicity of the medulla. 


I'm not 100% sure though. I do know that in normal conditions it is hypertonic to aid in urine concentration (your original question). ",null,0,cdlxyv5,1rcwz4,askscience,new,1
Farnswirth,"It's actually very simple.  Pure silver is softer and more malleable than silver alloys.  Just like how pure gold is much more malleable than gold alloys.

http://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness#Hardness_.28Vickers.29",null,0,cdm0s7d,1rcwaq,askscience,new,5
bellcrank,Pretty sure you could get away with a plane parallel approximation in this scenario.,null,0,cdmksb0,1rcvv4,askscience,new,2
adamhstevens,"If you're talking about long wave radiation from the Earth, I think this is a fairly standard textbook problem. I'll try and look it up when I get home... if I remember!",null,1,cdmildz,1rcvv4,askscience,new,2
shiningPate,"The original computer architectures used different circuitry for retrieving bytes  assembling into word sizes matching the register size in the CPU. When you were looking at the memory sequentially, independent from the CPU, you needed to know which way the CPU assembled the data into register values to understand why your calculations where coming out wrong",null,0,cdm27j3,1rcvso,askscience,new,2
ofeykk,"I am going to attempt to translate, as best as I can, your problem into a mathematical question. I suspect you are making a bunch of assumptions here which I will try to make more formal.

First, with the view of simplifying as much as possible yet retaining the crux of the original problem, you can make the following assumptions (removing each would yield a slightly different problem to solve):

1. Look for curves not in 3-space but in 2-space.
2. Simplify curvature of earth to be flat — seek planar curves.
3. Simplify sun to be on this flat plane as a point.
4. Simplify yourself to be a point on the sought curve.
5. 180 degrees view is equivalent to dividing the plane by the tangent to the curve at your location (point).
6. Choose an orientation for the curve. This helps fix what it means for a point to be in your field of view or equivalently, to be in the ""correct"" half space of the tangent to the point (you) on the curve.
7. Parametrize looping around the curve to be traveling along the unit interval [0, 1] with the end points {0} and {1} identified — essentially a fancy way of saying that the end points of the interval are glued.
8. Assume that the sun is fixed relative to the time taken to loop around the curve once.
9. Finally is the curve sought smooth or not ? (A circle is a smooth curve whereas a triangle isn't one.) I believe it's easier (at least for me) to imagine smooth curves. Also, will exclude wild curves like space filling curves simply because I am not comfortable dealing with those !!

Now, the question is to find a curve that maximizes the view of the sun when you loop around once.

It appears to me that solution would depend on whether you would wish to make a further assumption about whether your plane is unbounded or not. 

If the plane is unbounded, the answer is simple — any straight line not through the point representing the sun would do.

If the domain on which the curve is sought is compact (or technically if its closure is compact) — think finite if you wish, like a square plot or a circular plot of land — then it depends on where the sun is located relative to this domain. Some examples that come off the top of my mind are as follows:

1. Circular region with the sun in the center: Take any diameter and take a tube around the diameter. This tubular region would have a boundary curve composed of two straight lines (chords of the circle) with a small part of the circular's plot's boundary (two of these actually). You can make this tubular region as small as you please and would provide you with a curve for which the sun is visible for as long as you please. (In other words, give me a number for which you wish the sun to be visible, say 99.99%, and I can give you a *width* of this tubular region for which it would be realized. Make it 99.9999999% and I'll give you a different number for the width and so on.)

2. A circle with the sun not in the center: Use the same idea as in example 1. Drop a radius from the center of the circular plot to the boundary that passes through the sun. Measure the distance, say r, from the sun to the boundary of the circle. Draw a smaller circle within the larger circular plot with radius r and centered at the sun. Repeat example 1.

3. A square with the sun at the intersection of the diagonals: Repeat example 1 with the a circle of length equal to the distance from the sun to any one of the vertices of the square.

4. A square with the sun not at the intersection of the diagonals. Easier to say that one should fall back to example 3 but rather simply draw a circle centered at the sun with a radius equal to the shortest distance from the sun to the boundary of the square.

Can go on but would stop here. I have to say that I did this as a fun Sunday morning exercise, and tried to reason mathematically which may or may not have been what you were looking for ! (I enjoyed it though !) :-)",null,0,cdlztop,1rctma,askscience,new,4
Bondator,"Human walking speed is roughly 6 km/h so if you circle around north or south pole at 23km radius, you'll do a full circle in 24 hours, keeping the sun in front of you 100% of the loop.

As for your triangle, you didn't go deep enough. Don't do an equilateral triangle, do an isosceles triangle. Mathematically expressing, if we mark the short side with x, and the equal sides with y, and choose the orientation in such a way that x is the part where you don't face the sun, then uptime of sun in face is lim(x-&gt;0) 2y/(2y+x) = 1.",null,0,cdm9rcs,1rctma,askscience,new,2
musubk,"I once drove an 18 hour loop in Alaska in the summer with the Sun shining on the left side of my face for all but about an hour of it.  I suppose if you go above the Arctic Circle at the right time of year and just walk toward the Sun at a constant speed, you'll end up where you started 24 hours later making a complete loop.",null,0,cdnb6l9,1rctma,askscience,new,1
Das_Mime,"The expansion of the universe and the speed of light have different units, therefore you can't compare them.

The Hubble Constant is the fraction by which a given parcel of space will grow in a given amount of time. It has units of inverse time, s^(-1). The speed of light, of course, has units of distance/time, m s^(-1).

The Hubble Constant is usually given in units of kilometers per second per megaparsec, but the two distance units just cancel out and you get the result that the universe expands by about 0.00000000000000002% per second.

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

There will eventually come a time when there are no other visible galaxies in our observable universe (except for nearby ones that are gravitationally bound to us).",null,3,cdm01m4,1rcr2g,askscience,new,10
IAmMe1,"We in fact know that far-away parts of the universe are receding from us faster than the speed of light. However, this is not a problem. It's better to think about the expansion of the universe as an change in space itself rather than the motion of the things in that space; think of it as extra distance appearing between far-off objects. In this way, nothing is moving faster than light in the sense of any actual motion; instead, the distance between us and such an object increases faster than light can traverse that distance (i.e. more than 1 light-year of distance is added per year).

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

Yes indeed. It will be a dark and dismal universe that day far, far into the future!",null,1,cdlyqtu,1rcr2g,askscience,new,7
Luminarie,"Based on what we know about physics right now, we have no reason to believe it won't happen. Lawrence Krauss puts it quite succintly: ""Nothing can move faster than light in empty space, but space itself can to whatever the hell it wants"".

And yes, that's what would happen. At some point, the space between galaxies will be expanding faster than light, and at that point they will disappear from our region of the universe, as light would need to be faster than the expansion to be able to get to us. Therefore we will be causally disconnected from the rest of the universe.

Beyond the point where it accelerates faster than light, extrapolations based on an unchanging acceleration end in a [Big Rip](http://en.wikipedia.org/wiki/Big_Rip). Basically, the increasing speed of expansion overcomes all physical forces, and the universe would seem to end in a singularity (the scalar factor that defines expansion becomes infinite) at the moment when this happens.",null,0,cdlz409,1rcr2g,askscience,new,2
Azurity,"If you're up for a bit of fun and history, here's an ancient (1961) paper that originally investigated various mechanisms of polypeptide assembly: [ASSEMBLY OF THE PEPTIDE CHAINS OF HEMOGLOBIN](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC221568/pdf/pnas00219-0005.pdf)

It's actually a fun article to read if you've got an hour or three and you feel like working out a molecular puzzle using 50-year-old methods and logic. Scientists weren't sure if proteins were synthesized from one end to the other, or started at both ends, or if there was actually a giant cellular ""stamping"" machine that knit every amino acid of a protein together at once! Essentially, they used a series of radiolabeling and quenching experiments and froze moments in time as proteins were being made to mathematically derive a mechanism of N-terminus to C-terminus translation. Cool stuff!",null,0,cdm5qng,1rcqnv,askscience,new,6
PENIS_VAGINA,"Well read the section about Crick's contribution to molecular biology here: 


http://en.wikipedia.org/wiki/Francis_Crick#1951.E2.80.931953:_DNA_structure


I'm not sure that there was ONE definitive experiment that determined the mechanism. Perhaps there was, but I can't seem to find it.


If you are wondering how you can prove it now, a simple example would be using GFP to follow a DNA sequence to an mRNA transcript and then to a fluorescing protein. There are other experiments that rely so heavily on mRNA as the transcript of DNA that it basically 100% accepted as the mechanism. ",null,3,cdlytmo,1rcqnv,askscience,new,4
quantum_lotus,"It seems like you want the actual experiments that led to our understanding of the triplet nature of the genetic code.  I'll offer you two resources that explain the experimental procedures (and the logic behind them) that led to our current understanding.  Both are at a basic undergraduate level, so I doubt you'll have trouble following.

The first is from the Nobel Prize website called[ ""Crack the Code""](http://www.nobelprize.org/educational/medicine/gene-code/history.html).  I'd read the historic background, but the explaination of the experiment starts with the ""A Clever Experiment"" section a little further down the page.

The second is as [PDF](http://basic.shsmu.edu.cn/jpkc/cellbiota/resource/exper/11.pdf) I found with a basic google search (for ""cracking the tRNA code"") that is unattributed, but hosted on a site from the Shanghai Jaio Tong University.  It appears to be from an undergraduate level textbook and gives a more in-depth look at the same history and experiments.",null,0,cdn4cyp,1rcqnv,askscience,new,1
ignorant_,"While we're waiting on someone with better credentials, I'll throw in that for every 4 inches in height over 5ft, a person has a 16% increased risk of cancer. So converse to your statement, it is my understanding that more cells, thus more cell divisions, means greater risk of cancer development. ",null,1,cdlztvc,1rcpmh,askscience,new,5
Aniridia,"Obesity does increase the risk of several cancers. The inverse, does being ""skinny"" lower the risk of cancers, is less clear, and I'm not sure if it has been directly studied. There's a [Lancet article](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(11\)60814-3/fulltext#article_upsell) that deals with many of the health risks of obesity, including cancers. (The article is free, but you must log in.) Here is a [PDF PowerPoint type presentation](http://www.mhsimulations.co.uk/Documents/WangC.pdf) of the article from the author. ",null,0,cdmr14g,1rcpmh,askscience,new,3
therationalpi,"Sound waves definitely *are* affected by the wind. Since sound waves travel through a medium, and wind is a bulk flow of the medium, the sound speed in a windy environment (which is normally the same in all directions) suddenly becomes direction dependent. Specifically, the speed of the wave becomes the speed of the wind in that direction plus the speed of sound at rest. Moreover, since wind tends to move faster the higher you move up from the ground, there is usually an effective sound speed gradient as well. In the presence of a sound speed gradient, sound waves tend to refract towards regions of lower sound speed. As a result, sounds sent out against the wind will tend to refract upwards, and sounds made with the wind will tend to refract downwards. Sounds made cross-wind will tend to refract downwind and up. And since your listeners tend to be near the ground (relatively speaking) the net effect is that sounds carry further with the wind than against it.

As for electromagnetic waves (light/radio), I don't believe there is any notable effect, but I would wait for verification from someone with more experience in the field.",null,0,cdlz4kd,1rcooj,askscience,new,7
KerSan,"Energy is a consequence of a symmetry in the laws of physics. This is a deep point, so let me try to explain.

It is first worth explaining probably the most important concept in physics, momentum. Early on in your education, you are told that it is simply mass times velocity. This turns out not to be true generally, because momentum is really what is called the 'generator' of translations.

Consider a point particle that has a definite position in space. That position is actually an arbitrary label, since it refers to your chosen co-ordinate system rather than some actual property of the universe. You could just as easily switch co-ordinate systems by, say, moving the origin of your co-ordinates somewhere else. This action of shifting co-ordinates is called 'translation'.

Now let us suppose that the particle is moving (relative to you, the observer). No matter which co-ordinates you chose, you will notice that the position of the particle is changing over time. If you want to predict where you will see the particle next, you need to translate the last known co-ordinates of the particle by some amount. That translation is determined by the momentum of the particle. Keep in mind that the momentum is not changed when you translate your co-ordinate system. The momentum is, in a technical sense, dual to position.

What does this have to do with energy? Well, energy happens to be the generator of translations in time. This is because *energy is defined to be that quantity which remains invariant under translations in time*, much as momentum is that quantity which is invariant under translations in position. Your co-ordinates for time are just as arbitrary as your co-ordinates for position, because your choice of starting time is your choice and not some property of the universe.

Energy therefore tells you something about how quickly the properties of an object change over time. The more energy, the faster changes can happen. I could write essays explaining this further, but I don't want to lose you.

Work is the addition or subtraction of energy from a system. That energy can be added in a variety of ways, but it is common in early physics education to think in terms of force rather than energy. Energy is actually more fundamental than force, so I'll explain the force times displacement rule by first explaining force in terms of energy.

Force is the derivative of potential energy with respect to position. If you think of the potential energy of a particle as a function of the position of the particle (for instance, gravitational potential varies linearly in the height of the object), then force experience by the particle at a given position is the slope of the potential function at that position. In other words, force is the negative change in potential energy (i.e. the work) divided by displacement (in an infinitesimal sense). Multiply both sides by displacement and you have the relation work = force * displacement.

Hope that helps.",null,2,cdlzluj,1rcojz,askscience,new,7
miczajkj,"What is energy?

This is a very interesting question - but we we don't have an answer to this. We formulate physics in equations and stuff and there is this certain quantity that shows up in some of them and seems to be conserved under certain circumstances. 

The first kinds of energy you encounter, when you start to think about physics are the kinetic energy and the potential energy.
Assume a body at the position x that is exposed to a force F(x). Following Newton, you get the differential equation

m*x°° = F(x) 

The second derivative of the bodies displacement with respect to time times its mass is equal to the force on the particle.  
If it is possible to write the force as the derivative of a potential, namely F(x) = -V'(x) you see, that the following quantity is conserved for every solution of the mentioned equation: 

E = 1/2 mx°² + V(x) 

If you take it's derivative, you see: 
E° = m*x°*x°° + V'(x)*x° = x°*(mx°°-F(x))

So if the differential equation is fulfilled, then E' = 0 follows directly. 

You can see: because of the form of the differential equation that describes the movement, there is a certain quantity, that won't change - we call it a conserved quantity. 

If you dive deeper into the mathematical foundament of physics, you find that symmetries are responsible for the most conserved quantities. The energy for example, is only conserved, if the physical laws describing the process are time independent. 

All in all: energy is just a number. A phrase like 'pure energy' is nonsense. Energy is just a quantity, that appears in our equations. 

Work is defined as the change in energy you need to apply to a system, to move it from one state to another. In most cases this movement is a spatial movement from one place to another - then work is just the difference in potential energies, 

W = V_1 - V_2

or because V'(x) = F(x) 

W = integral F(x) dx

For forces that don't depend on position, you get 

W = F*x


I guess that was a whole lot of different concepts - maybe you've never heard of differentation, integration or newton's 2nd law: but I can't come up with an easier way to describe energy, that is not too simplified or wrong. ",null,0,cdlztxi,1rcojz,askscience,new,2
dampew,"Work has units of energy -- they're basically the same thing.  For instance, if you want to know how much work you've done, the answer would be in units of energy (joules, calories, whatever).

Why is it force times distance?  Well, you do more work if you have to push something harder through a larger distance.  I'm not really sure how to answer that question -- it's just a name for something, really...",null,0,cdm0kyk,1rcojz,askscience,new,1
FeckSakeLads,"work is the amount of energy you must add using a force of a certain strength to move an object with mass a certain distance in a certain direction (the displacement). the equation is:



work = force (joules) x displacement (metres) x cos(theta) (theta being the angle between the direction of the force relative to the direction of displacement).



See [this](http://www.physicsclassroom.com/calcpad/energy/) for more.",null,3,cdlzbtd,1rcojz,askscience,new,1
shiningPate,"First, this effect only occurs in a vacuum. In air, the gas molecules rapidly absorb the emitted electrons. Basically the reason xrays are emitted is because of the static electricity generated by separating the tape film from the surface below it. As the tape is pulled away, an electrical field is formed at the point of separation. As the roll rotates away, the film separates from the layer below it, reducing the strength of the field between the point that just separated, and the point at which it was previously touching. Meanwhile a new point has just separated from the roll, and higher strength field is formed at that point. The result is an electrical field at a gradient intensity in the wedge between the roll and film. At the point of separation, the field strength is so high electrons are ripped away from the film material. The film material was also electrically charged as part of the manufacturing process to get it to roll up smoothly. Electrons emitted into the gradient electrical field will begin accelerating along the gradient. Accelerating electrons emit xrays",null,0,cdm2gu7,1rcmj7,askscience,new,3
dampew,"http://www.nature.com/news/2008/012345/full/news.2008.1185.html

""The leading explanation posits that when a crystal is crushed or split, the process separates opposite charges. When these charges are neutralized, they release a burst of energy in the form of light.""",null,0,cdm0glp,1rcmj7,askscience,new,2
cromonolith,"The only thing stopping this from being intuitive to you is what ""bigger"" means. 

You're used to judging the sizes of things by counting each of them and comparing the numbers. That is, if I gave you two bunches of apples, you might count and see that there are 10 apples in the first bunch and 14 in the second, and conclude that the second bunch is bigger. 

That's fine, but it's not a good way of measuring the size of infinite collections. So here's a better way of comparing the size of two collections: make a pairing between the collections, and the one that has stuff left over is bigger. 

As an example, let's say we had a huge auditorium and a huge crowd of people who want to see a show there. What's the best way to see if we have the same number of seats as people? You can count the seats and the people, but that's dumb. The smart thing to do is to tell everyone to sit down. As long as no one sits down stupidly (two people in the same chair), then you can easily check. If there are empty seats left over there are more seats than people. If there are people left standing there are more people than seats. 

What does this have to do with infinities? Well, this kind of pairing (like pairing people with seats) is a function. If no two people sit in the same seat, the function is called ""injective"" (or ""one-to-one""). If no seats are left unoccupied, the function is called ""surjective"" (or ""onto""). If a function is both injective and surjective (one person to a seat and all the seats are filled) the function is called ""bijective"". 

Bijective functions are what we use to compare the sizes of all sets, including infinite ones. Two sets are *defined* to be the same size if there exists a bijective function between them. So for example, the set of all natural numbers **N** = {1, 2, 3, 4, ....} is the same size as the set of even numbers 2**N** = {2, 4, 6, 8, ...} because, as you can check, multiplication by two is a bijective function from the former to the latter. That is, the function f where f(n) = 2n hits every even number and never sends two numbers to the same even number. 

On the other hand, you might want to compare the set of natural numbers as above to the set **R** of real numbers (the whole number line). This isn't obvious, but there's a [relatively straightforward proof](http://www.mathpages.com/home/kmath371.htm) that no matter how you try, it's impossible to make a bijective function from **N** to **R**, so they can't have the same size. Since **N** is actually a subset of **R**, we say the size of **N** is smaller than the size of **R**. ",null,1,cdlzcjv,1rcjr9,askscience,new,18
rlee89,"There are several notions of size when it comes to infinity.  The most common is *cardinality* which in lay terms asks whether one infinity can be fit (or more formally, mapped) into another.

For example, if you have a list of the positive integers, there is a way to place a rational number next to each positive integer in the list in such a way that each rational number is next to some positive integer, and vice versa.  We would formally call this a bijection between the positive integers and the rational numbers, and it would demonstrate that the two are the same cardinality.

[Cantor's diagonal argument](http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument) shows that any attempt to match up the positive integers with the real numbers will necessarily miss at least one real number.  Thus the of real numbers won't fit into the poisitive integers, and thus have a larger cardinality than the positive integers and are a bigger infinity.",null,1,cdlznix,1rcjr9,askscience,new,2
protocol_7,"Here's another way of thinking about it: Any natural number can be represented by a finite amount of information. (For example, you can write it down in base 2 as a finite string of 0's and 1's.)

However, a real number has an *infinite* string of digits past the decimal point, so there's no guarantee that any particular real number can be expressed using a finite amount of information. (And, in fact, most real numbers *cannot* be uniquely identified by any finite amount of information.)

So, intuitively, it's the difference between finitely many versus infinitely many ""degrees of freedom"". Since specifying a single real number can involve an infinite number of choices of digit, there are vastly more real numbers than natural numbers.

This is just a vague intuition, though. You can make it precise using [information theory](https://en.wikipedia.org/wiki/Information_theory), but that's rather technical. Instead, here's how to see very quickly that some infinite sets are larger than others — in particular, that given any set (finite or infinite), there's another set that's larger.

By definition, two sets have the same [cardinality](https://en.wikipedia.org/wiki/Cardinality) (basically, size) if they can be put into [one-to-one correspondence](https://en.wikipedia.org/wiki/Bijection) with each other — that is, if you can pair up elements of the sets so that each element of one set is paired up with a unique element of the other set.

**Theorem** ([Cantor](https://en.wikipedia.org/wiki/Cantor%27s_theorem))**.** Let S be any set, and let P(S) be the [set of all subsets of S](https://en.wikipedia.org/wiki/Power_set). Then S and P(S) do not have the same cardinality.

*Proof.* Let f be any function from S to P(S), that is, for each element x in S, we assign a unique element f(x) in P(S). Since each f(x) is a subset of S, we can ask whether x is an element of f(x). In particular, let T be the set of all elements x of S such that x is *not* an element of f(x).

Now we can ask, is there some element y in S such that f(y) = T? Suppose there was: then we can ask whether y is an element of T. If it is, then y is an element of f(y), so by the definition of T, y is not an element of f(y) — a contradiction. If it isn't, then y is not an element of f(y), so by the definition of T, y *is* an element of f(y) — again a contradiction. Therefore, it's impossible for such an element y to exist. So no element of S is paired up with T.

Thus, *any* attempt to pair up elements of S and elements of P(S) fails, which means that S and P(S) don't have the same cardinality. ∎

But we can embed S inside P(S) by sending each element x in S to the [singleton set](https://en.wikipedia.org/wiki/Singleton_%28mathematics%29) {x}, a subset of P(S). So, in fact, S is strictly smaller than P(S).

Notice that there's no mention of whether S is finite or infinite. (If S is finite with n elements, then P(S) has 2^n elements.) So the proof is valid regardless, meaning that it's true for infinite sets, too.

For instance, if we denote the set of natural numbers by **N**, then P(**N**) is strictly bigger than **N**. (Actually, it turns out to be the same size as the set of real numbers.) And the set P(P(**N**)) is bigger than that, and so on.",null,1,cdlzsre,1rcjr9,askscience,new,2
null,null,null,3,cdlz5a1,1rcjr9,askscience,new,2
biffym,"It wasn't to make them believe it was real so much as to make it feel more real. They knew it was an experiment and that being arrested was part of it, but it adds to the feeling that they aren't there by choice. If they'd walked in of their own free will the jail would have felt less oppressive.",null,0,cdltsfb,1rcjmy,askscience,new,3
DougWC,"What is most interesting about the experiments and others like it is not the obvious.  It's the implications for ""real world"" situations of authority and subordination - that they are no more truly legitimate than are contrived ones.  They are all contrived and all should be seen through.",null,0,cdm1916,1rcjmy,askscience,new,1
Platypuskeeper,"There is no motion in a classical sense. Electrons have kinetic energy and that, but they do not follow specific trajectories. The probability of knowing where you might find the electron is all you've got. For specific energy states, these probabilities don't change with time. Electrons have no size of their own, their position-probability distribution in space is basically where the electron is.

The wave function/probability distribution, which for a single particle has [solutions like this](http://chemwiki.ucdavis.edu/@api/deki/files/8855/Single_electron_orbitals.jpg), is analogous to a standing wave in three dimensions. The angle-dependent part of the functions are the spherical harmonics.

Photons and electrons are particles, they both have energy but they have more than that as well. They carry both linear and angular momentum, for instance.
",null,3,cdlufy4,1rcj7e,askscience,new,5
The_Serious_Account,"&gt; So do electron move like those diagrams of standing waves? Or do they not wave like that, but whiz around like a particle? If I was reduced in size, is the electron a hard ball or is more of a packet of energy like a photon?


I think the most sensible thing to do is to give up the concept of particles. There's no such thing as a particle. Just drop the concept entirely. We keep it in language, but as a mental picture it's dangerous and misleading. 

The electron doesn't move around within the standing wave. The electron *is* the standing wave. The word electron should refer to the standing wave and nothing else. It's an extremely hard concept to accept, but repeat it to yourself. There is no particle. There is no particle. There's only the wave function. ",null,2,cdlucvm,1rcj7e,askscience,new,2
Polyknikes,"Short answer: Stimulation of the vagus nerve (CN-X) which induces bradycardia.

Long answer:

I had not heard the term apneic pause before but from googling it you are referring to a cessation of breathing for at least 10 seconds, commonly referenced in relation to sleep apnea.  I didn't know the term but I do understand cardiac physiology.

In 10 seconds your oxygen saturation levels do not drop by a measurable level.  Try holding your breath while using an oxygen saturation meter and see if you can even get it to go down by 1% - it's really difficult even if you can hold your breath for several minutes.  So I don't believe it is related to lack of oxygen unless we are talking about a much longer duration apneic pause.

During normal inspiration and expiration the heart rate increases and decreases, respectively.  This is thought to be induced by changes in vagal nerve tone (activity) but the mechanism by which breathing influences vagal tone is not understood.  The vagus nerve acts to reduce heart rate by hyperpolarizing the intrinsic pacemaking cells of the heart.  

Stimulating the vagus nerve would decrease heart rate and this can be accomplished by various means including hypoventilation, hypoxemia, respiratory acidosis, or vigorous inspiratory effort against a closed airway known as the Mueller's maneuver.  In the case of obstructive sleep apnea a person may be attempting to inspire but cannot due to a blockage of their airway (usually seen in obesity) which could stimulate the vagus nerve by the Mueller's maneuver mechanism.  But again, the exact mechanism by which the vagus nerve is stimulated by these pressure changes is not known, it has simply been observed indirectly.

I hope this at least partially answers your question!",null,0,cdlwfmx,1rci0v,askscience,new,3
meerkatsrgay,"so, I have never been full body immersed in -90. However, I have ineracted with huge standup freezers that cool biological samples to -86C (I have seen -89 once on the readout) Generally when interacting with this environment you wear protective gloves. Grabbing a cooled piece of metal can be dangerous if held for more than a few seconds as it will freeze the moisture on your hand. But, through my interaction with these freezers I would assume that standing in an environment like that naked you would be fine for even up to 15mins provided you only contacted anything solid through your feet (and had shoes) AND there was absolutely no wind. In an environment with no air movement your body is able to build up a very tiny layer of warm air close to the skin. This is the same reason why holding dry ice or sticking your hand into liquid nitrogen is fine, the evaporated gas that is instantly created between you and the material acts as an insulator. 

Either way, I wouldn't recommend going outside naked in -90C weather because you prolly don't look as good as you think and no one wants to see you naked.",null,2,cdm6hb4,1rchgw,askscience,new,11
shiningPate,"See the information on the South Pole station [300 Club](http://en.wikipedia.org/wiki/300_Club). To get into the club, you have to run naked from the 200 degree sauna, down the access tunnel, outside 20 feet to the south pole marker, touch it, and return back to the sauna. The dash has to be done when the temperature outside is -100 F. ",null,5,cdm1xw7,1rchgw,askscience,new,10
null,null,null,2,cdm2oo5,1rchgw,askscience,new,4
Ruiner,"If you were in a planet without atmosphere, then it would, but since we have air friction, the bullet would land with its terminal velocity.

correction.: if its terminal velocity is bigger than its speed when it is fired. Otherwise, it will land with a smaller velocity, given the energy it lost to friction.",null,3,cdlt0kg,1rch1e,askscience,new,19
Ruiner,"First, kudos for doing your own experiment and trying to interpret results.

Now, my suggestion is that you should try striking with two coins instead of one. And afterwards, use a coin that's a bit heavier than the others.

After you're done with it, try reading the physics explanation on this [page](http://en.wikipedia.org/wiki/Newton's_cradle).",null,1,cdlsz1r,1rcgby,askscience,new,6
Platypuskeeper,"Ice behaves like a normal solid, the [density increases with lower temperature.](http://en.wikipedia.org/wiki/File:Density_of_ice_and_water_%28en%29.svg). 

Nothing particularly interesting happens at the molecular level, unless you have a phase change. The average distance between molecules decreases because they vibrate less at lower temperatures.
",null,1,cdltrht,1rcdt3,askscience,new,2
Wrathchilde,"There are many forms of solid water [ice](http://www1.lsbu.ac.uk/water/ice.html), and which form exists is a function of temperature and pressure.  Some forms, as described in the link above, are more dense than liquid water.

However, since you did not include pressure changes in your question, let's assume a constant 1 atm.  This [phase diagram](http://en.wikipedia.org/wiki/File:Phase_diagram_of_water.svg) shows that below about 70k ""normal"" ice will change to ice-XI, a more structured crystal.  However, as the first link describes, the density of ice-XI is pretty much the same.
",null,0,cdlufvc,1rcdt3,askscience,new,1
NotAStructrlBiologst,"I hope this gives you a better picture of what is going on at the molecular level.

Even in the crystal/solid state molecules may not move but their atoms continue to vibrate. Bonds contract/expand and angles wobble ever so slightly. Continuing to cool something will also decrease these motions, increasing the order",null,0,cdm0rnu,1rcdt3,askscience,new,1
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,207,cdlyfi3,1rccq1,askscience,new,1144
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,new,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,19,cdlusqz,1rccq1,askscience,new,41
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,11,cdlwc5k,1rccq1,askscience,new,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,new,17
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,12,cdlvk3k,1rccq1,askscience,new,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,new,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,11,cdlushn,1rccq1,askscience,new,17
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,9,cdlz6us,1rccq1,askscience,new,13
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,new,15
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,12,cdm5uag,1rccq1,askscience,new,16
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,10,cdmelu9,1rccq1,askscience,new,14
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because ಠ_ಠ, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,11,cdlyunp,1rccq1,askscience,new,15
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,new,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,new,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,new,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,new,12
null,null,null,10,cdmg52h,1rccq1,askscience,new,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,new,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,8,cdm4kp8,1rccq1,askscience,new,11
weinerjuicer,"without drawing energy from the environment, it seems like it should in principle be possible to exchange kinetic energy due to translational motion for gravitational potential energy: if they are going slower horizontally after the dive-down-then-pull-up move it may not violate conservation of energy.",null,1,cdlurjr,1rc9xn,askscience,new,8
drzowie,"Without some effect to bring energy to the hang-glider, diving and rising will always cause the hang-glider to go down.  That will always happen faster than if the hang-glider pilot just flew along at his best-sink-rate speed and attitude (which should be close to his best-glide-ratio speed and attitude)

That said, energy is not particularly well conserved in the hang-glider's system.  An experienced hang-glider pilot can make use of many counterintuitive effects, mostly involving wind shear or vertical winds, to scrub energy from the environment.  

Hang-gliders near Torrey Pines in San Diego, CA can fly all day without any thermals at all due to the vertical wind break at the Torrey Pines cliffs.
",null,2,cdlw5tk,1rc9xn,askscience,new,4
zlatan08,"According to conservation of energy, the total energy, which is the sum of gravitational potential energy, kinetic energy, chemical potential, magnetic potential etc.., must remain the same. In this case, let's consider just kinetic and gravitational potential energy. At any point in time, as long as no outside forces act on the glider (i.e. thermals), their sum must be constant. When the glider dips down, it trades gravitational potential energy for kinetic; when it rises back up, it trades the kinetic for gravitational potential. If it tries to rise to high, the kinetic will come close to zero and the glider will stall. In real life, if the glider keeps trying to dip down, rise, stall and then dip down again, drag forces on the surfaces on the glider will cause it to lose energy and every time the glider stalls, its height will be lower than the previous time. Drag would be considered the outside force in this case and energy would not be conserved.",null,1,cdlv4jy,1rc9xn,askscience,new,2
_Jordan,"I have heard that competition gliders, under the rules of the competition get towed to a certain height, and see how long they can stay in the air.

A trick they use, is to carry a tank of water with them when they get towed to the starting altitude. They immediately dive down to the ground (trading altitude for speed), then dump the water low to the ground, and go up (trading speed for altitude). Using this trick, they can rise up above the starting height, and stay in the air for longer.

If you see a hang glider diving off of something high, and then rising higher than they started, look to see if they dropped anything at the bottom of their dive.",null,1,cdm1ln5,1rc9xn,askscience,new,1
Platypuskeeper,"&gt; What exactly is an oil? [the requirements to call something oil]

Typically something with a lot of long-chain hydrocarbons in it, but in the broadest sense (e.g. essential oils, vegetable oils) it could really be any liquid composed of non-polar (oily) compounds. It's not a precise (or 'technical') term as far as chemistry is concerned.

&gt; Are there oils which do not have carbon in it?

[Silicone oil](http://en.wikipedia.org/wiki/Silicone_oil), although the term 'oil' there is more because of its use as a lubricant than its chemical composition.

&gt; Is it true that kerosene is not technically an oil? And why?

I don't see what usable definition of 'oil' would exclude kerosene. It's not _crude oil_, it contains a more limited subset of hydrocarbons. But it's still a hydrocarbon mixture.


",null,1,cdm06t8,1rc9wk,askscience,new,4
dapwnsauce,"Most oils do not dissolve in water as they have two characterizing features, a polar(hydrophobic) and non polar end(hydrophobic).  An example is a [Micelle](http://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Micelle_scheme-en.svg/532px-Micelle_scheme-en.svg.png).  Kerosene is an really long hydrocarbon chain in comparison to others which are used as **fuels**(although there are some which are longer).  Kerosene does not have a polar end, it is in other words a potential backbone to an oil.  Considering this, it would be classified more as a **fuel** rather than an oil.

Silicone oil does contain carbons and it maintains its ""oil"" status due to the polarity of the Si-O bond.  Non-polar end being the carbon groups that are attached to it.  The design of silicones have multiple chemical uses which sets them apart from conventional oils.  
  
The fluidity of an oil really depends on the hydrocarbon backbone.  The arrangement of the molecules can determine whether they become more rigid and less viscous or more fluid.  When oils are able to tightly pack together, they tend to form more viscous structures and even solid structures (ie butter/shortening).   When they have rigid backbones (ie kinks in the chain) they tend to form more fluid structures such as olive oil.  

Are there any oils that do not have any carbons in them whatsoever?  Theoretically there should be some that exist, though none come across my mind at the moment.

Hope that helps.",null,0,cdmkda9,1rc9wk,askscience,new,2
soylentblueissmurfs,"One of the reaons inbreeding can be harmful is you run a much higher risk of recessive genetic disease since your relatives are more likely to carry the same damaged alleles. However, if you inbreed enough those mutations will be weeded out so the answer is basically: they are SO inbred it's a small problem.",null,0,cdlut33,1rc9t0,askscience,new,5
mak484,"I'll handle the follow-up question. Mice are an ideal model organism for many reasons- they have a relatively short gestational period, produce many offspring, and reach sexual maturity very quickly. All of these factors lead to incredibly short generation times with a large exponential increase in population size with each generation. This allows scientists to very easily weed out deleterious recessive alleles, leaving breeding stocks with very uniform and well-understood genotypes.Compare mice to humans- it takes 12-16 years to reach sexual maturity, and females can only give birth (naturally) to 1-2 offspring per year. Factor in a high level of allelic heterozygocity, and you can see where aliens would have a difficult time creating a genetically uniform breeding stock.*I got a little morbid below, sorry if this disturbs anyone*Now, if I were the aliens, assuming I had unlimited resources and appropriate technology, I would find brother-sister pairs of paternal twins, and harvest their sperm and eggs. I would then fertilize all of the eggs simultaneously, and begin genetic testing once the fetuses reached several weeks. The offspring with the highest levels of homozygocity would be selected for the F1 generation of breeding, where I would repeat the process. After maybe 5-10 generations complete homozygocity and lack of deleterious genes could be reached. Since I started this process with numerous genetically diverse brother-sister pairs, I could develop multiple lines of humans that carry whatever combination of genes I want. ",null,0,cdlvtyw,1rc9t0,askscience,new,4
do_od,"Mountains on Earth will never be much taller than they are now. That is  because at some point the weight of the mountain causes such enormous pressure that the base of the mountain will start to liquify and deform.  

Planets with lower gravity can have higher mountains. A prime example is [Olympus Mons](http://en.wikipedia.org/wiki/Olympus_mons), the tallest mountain on Mars at 22 km. The surface gravity on Earth is about 2.7 times that of Mars. Because pressure scales linearly with height, we can expect the tallest mountains on Mars to be about 2.7 times taller than the tallest on Earth. 2.7 * 8.9 km = 24 km, is in that ballpark.  

Wether or not Mt Everest is higher than the atmosphere depends on your definition. You could say that it is because humans can't survive there for very long.",null,0,cdm3ptj,1rc9ea,askscience,new,3
JohnSmith1800,"There's sort of two features going on here, the macro and microscopic, so I'll detail them both.

The light first passes through the pupil and lens, which focus it in particular on a small patch of the retina known as the fovea. This is where the concentration of cone cells (those which detect colour) is highest. You also have a lot of rod cells (just detect light generally, more sensitive in dark settings) here, but they're more common as you move away from the fovea. Collectively rod and cone cells are photoreceptors. These photoreceptors are hooked up to bipolar cells, horizontal cells and ganglion cells, which together do some initial ""processing"" of the image before it passes down the optic nerve to the brain. Interestingly, because the horizontal, bipolar and ganglion cells are actually between the retina and the pupil, the optic nerve has to travel through the retina, which creates a blind spot in each eye, about 15degrees off-centre (which your brain lies to you to fill in).

On the microscopic level, photons travel through the eye to the retina, where some encounter either rod or cone cells. In rod cells there is a stack of ""plates"" which are coated in an enzyme called rhodopsin. When a photon is absorbed by rhodopsin, it changes conformation and can activate another protein known as transducin. Transducin is what is known as a ""G-Protein"", when activated it in turn activates another protein in turn, which then changes cGMP (a small second messenger) into 5'-cGMP. This leads to a closure of Na^+ channels. This hyperpolarises (makes more negative) the cell. Neurons only release neurotransmitter when they depolarise, so this reduces the release of neurotransmitter. I'm not familiar with the exact pathway in cone cells, it is photopsin rather than rhodopsin which absorbs the photon, but otherwise I believe it to be generally similar. This whole process actually takes quite a while, about 200ms from memory between when the photon hits and when your photoreceptor's sodium channels close. This is because of the ""protein cascade"" which occurs, it takes time. However, it does greatly increase your sensitivity to light: A single rhodopsin can activate ~500 transducins, which will in turn do ~500 cGMP's each. This will close ~100 sodium channels, stopping 10^~11 ions, and hyperpolarising the cell by almost a mV. That is to say, your eyes are actually very sensitive to light.

Intriguingly photoreceptors are actually inhibitory neurons, they release a neurotransmitter which hyperpolarises other neurons. As such, when a photon is absorbed their rate of firing decreases, which increases the firing rate of bipolar cells. The rest of the neural pathways are way over my head, but it involves ""receptor fields"" and the other accessory neurons.

Edit: 
Source: I'm a 2nd year physiology student / L. Sherwood ""Human Physiology: From Cells to Systems"" 8th Edn.
Also, I've left out a protein or two in the signalling pathway, but in terms of answering the question I think it's sufficient?",null,0,cdlu2z4,1rc9cl,askscience,new,2
rupert1920,"You can read about it in the Wikipedia article for [phototransduction](http://en.wikipedia.org/wiki/Phototransduction).

Basically, it involves the photoisomerization of a molecule of [retinal](http://en.wikipedia.org/wiki/All-trans_retinal) - the incoming photon excites an electron in that molecule, and allows for a rotation of one of the double bonds. This causes a conformational change in the protein that houses it - [rhodopsin](http://en.wikipedia.org/wiki/Rhodopsin) - which then sets of a cascade that leads to the nerve signal.",null,0,cdlu45k,1rc9cl,askscience,new,1
zalaesseo,"Stationary electrons generate electric fields. No current

Moving electrons generate magnetic fields. Constant current

Accelerating electrons generate electromagnetic fields. Changing current.

Oscillating circuits generate changing currents, and thus electromagnetic fields. 

Then we can either amplitude or frequency  modulate signals into the carrier wave.",null,0,cdludhm,1rc957,askscience,new,3
drzowie,"zalaesseo gave a nice answer.  Another, perhaps even more simple, is:

Shaking electric charge produces electromagnetic waves, just as shaking objects in air produces sound waves (the physics is different but the fundamental waveness is the same).  We make electromagnetic signals by shaking electric charges.  

Every time you switch on or off a circuit, you create electromagnetic waves - so there's a lot of electromagnetic noise all around us.  To punch through that interference, devices that signal each other (like radios, or phones, or wifi units, or whatever) pick a particular frequency and shake electric charges at almost exactly that frequency.  That's like cutting through the noise of a large room full of drunk people, with a particular clear tone (say, from a flute).  You can discern the flute even though it's not any louder than the people, because all the energy is concentrated into a single tone.

Small variations in the strength or frequency of the electromagnetic signal carry the information people want to transmit.  For an old-style AM radio, the strength of the signal indicates where the speaker cone of the receiver should go.  The station gets ""louder"" and ""softer"" very rapidly to move your receiver's speaker cone around, forming sound waves.  FM radios use small changes in the pitch of the radiofrequency tone to control where the speaker cone should go.  Digital radios like wifi or modern cell phones use variants on those two strategies to communicate streams of bits.  Those bits encode the sounds and internet packets that are being transmitted.
",null,0,cdlw28q,1rc957,askscience,new,2
chrisbaird,"Causing electric charges to oscillate (bump, jiggle, shake, collide, change energy levels, transition between states, etc.) *always* creates electromagnetic waves, and not just in fancy circuits. The chair you are sitting on is emitting electromagnetic waves right now (mostly in the infrared) because its electrons are slamming together due to thermal motion. There are many ways to oscillate electric charges, and so there are many ways to create EM waves:

- heat them up so they collide more (incandescence)
- shake them up and down a wire using applied voltages (antenna radiation)
- excite electrons into different states and then have them transition back down (lasers, fluorescence, phosphorescence, gas discharge, chemiluminescse)
- send electrons passed a system of magnets that makes them wiggle or circle quickly (free electron laser, cyclotron radiation)
- smash a charged particle at high speed into a material (Brehmstrahlung)

If you want to send a signal on an EM wave, then you need to precisely control the shape of the EM wave. You must therefore precisely control the movement of electric charges. Electric circuits come in handy for that.
",null,0,cdmu1if,1rc957,askscience,new,2
PENIS_VAGINA,"Interesting question. I'll try to answer this. 


First off sugar does not neutralize the acidity. However you are correct that sourness is based on H+ ion mediated receptors on the tongue (TRP family receptors). 


I don't believe the sugar changes our tongues ability to taste sour because sweet and sour receptors are distinct from each other so there should not be competitive inhibition of sour receptors when a substance that activates sweet receptors is present. In fact, it is possible that the low pH from the sour substance is enhancing ligand binding to sweet receptors. This is what happens when you ""Taste Trip"" with miraculin. 

My thought is that you are experiencing both tastes simultaneously and therefore your brain in an attempt to process both tastes is not pronouncing the sour taste as strongly as it would if sour was the only taste happening. 

I am looking for a source to confirm this and will update if I can find something. 

Edit: May as well add (because its a common misconception) that the classic ""taste map"" that shows different areas of the tongue to have varying densities of different kinds of taste receptors is FALSE.",null,0,cdlxpl2,1rc6y9,askscience,new,1
Ruiner,"I'm having trouble understanding exactly what you mean. But let me try to answer:
(First, keep in mind that if there is a liquid inside, things are a lot more complicated because of convection, which makes thermalization faster than if there was only conduction)

You start heating the bottom of the point. At this point, that surface gains heat through the source and loses heat through conduction, fine. If you leave the source on, it will at some point reach a steady state. This state is not really thermal equilibrium, but it's a state in which you have a constant flux of heat from the bottom to the top, so there is a temperature gradient. You know that you reached the steady state because the temperature of everything inside the pot is no longer increasing, but all the energy coming in is compensated by energy going out. (btw, if you're trying to boil a pot of water, you never get to see the steady state, since the average temperature keeps on increasing).

Once you turn off the source, the steady state will now be approximately thermal equilibrium. It would be thermal equilibrium of the pot was a closed system, but since it interacts, it will still have a temperature gradient - the coldest parts being those that interact the most with the outside. ",null,1,cdlt6gm,1rc3z2,askscience,new,2
Quant_Liz_Lemon,"You need to be awake during brain surgery in order to ensure that nothing important is damaged during the procedure. This is especially important if surgery is being conducted near functional areas of the brain. Otherwise, you might risk permanent brain damage. Depending on what area of the brain you're near, a surgeon might ask you to make specific movements, count, say specific phrases, etc, while performing the surgical procedure. 

[source: Mayo Clinic](http://www.mayoclinic.org/awake-brain-surgery/about.html)",null,1,cdluba1,1rc3d0,askscience,new,7
U235EU,"I work for a medical device company, one of our products is used to treat movement disorders by deep brain stimulation. The patients are conscious during the implant so that the doctor can insure the proper location of the stimulating leads by direct feedback from the patient, and by neurological monitoring. See this video:

http://www.youtube.com/watch?v=lUG8iFxukig",null,0,cdm1wfl,1rc3d0,askscience,new,1
Polyknikes,"Some surgeons still perform operations while the patient is awake but a more modern technique is to use various functional brain imaging techniques prior to the surgery while asking the patient to perform certain tasks, seeing which areas of the brain light up near the tumor, and then avoiding those areas during the surgery.  With this technique the patient can be fully sedated during surgery.",null,2,cdlxqlz,1rc3d0,askscience,new,1
brawnkowsky,"not all of it is rinsed, some of it passes into the epithelial cells.  once it passes through the epithelium, it is able to inhibit cyclooxygenase, an enzyme necessary for the creation of prostalgandins.  prostaglandins are involved in pain and vasodilation, two major components of inflammation, which is what a 'breakout' is

source: wikipedia, student of medicine",null,1,cdm050e,1rc303,askscience,new,2
owaisofspades,"yup, you have about 10 layers and 5 different types of nerve cells  on your retina, and the photoreceptors are one of them. They have pigments on them (rhodopsin for rods and iodopsin (?) for cones) which respond to light, and transmit the signal through the rest of the layers into your optic nerve",null,0,cdlos9c,1rc0vh,askscience,new,4
dakami,"Genes (OPN1SW, OPN1MW, and OPN1LW) express one of three proteins, called opsins.  When an opsin is hit by a photon, it has some chance of isomerizing.  This process causes an electron to be released, and the chance is related to two things:

1) The wavelength of the photon
2) Which opsin is hit

Your ""red"" opsins are more likely to be isomerize in response to ""red"" light.  (This is an extreme oversimplification.)

As you can imagine, nerves are quite good at responding to electrical signals, and creating complex computational cascades.  So vision stops being about light really, really early.

Once isomerized, the spent opsin is recycled.  Your retina is actually among the most (if not the most) metabolically active portions of the body.  Really, your eyes work a lot more like living film than CCD/CMOS silicon.",null,0,cdlqugn,1rc0vh,askscience,new,2
EdwardDeathBlack,"You are asking two questions. The first one is what do thermal fluctuations look like in a  DNA molecule. The second is can DNA melt. 

First, the double helix of DNA is indeed held together by Hydrogen bonds. Above a certain temperature, the energy is high enough to overcome those bonds. The double helix melts and the two molecules separate. This has been extensively [studied](http://en.wikipedia.org/wiki/DNA_melting) and is an essential tool of biotechnology (and especially of [PCR](http://en.wikipedia.org/wiki/Polymerase_chain_reaction) ). If you have any background in the thermodynamics of how, say, water, freezes and melts, you will find it very similar to look at DNA melting. 

Now, to address the first point, what does a hot (but not melted) DNA molecule look like. First, a DNA molecule will [""ball up""](http://en.wikipedia.org/wiki/Random_coil), it doesn't stay a nice stretched thing. That little ball jiggles and wiggles along with the solvent, exhibiting [Brownian motion](http://en.wikipedia.org/wiki/Brownian_motion) . It will also have [thermal phonons](http://en.wikipedia.org/wiki/Phonon), of the [1-dimensional kind](http://en.wikipedia.org/wiki/Phonon#One_dimensional_lattice). 

If the temperature is high enough, but not enough to melt completely the DNA, there will also be [""bubbles""](http://www.bu.edu/meller/research_bubbles.html) forming alongside the DNA of partially melted areas.. These will have a limited lifetime, will occur predominantly in areas of weak Hydrogren bonding (AT rich areas), and have a lot of roles to play in living organism. 

Anyway, all that is already a lot. Maybe I'll let you read what I linked to and ask more questions rather than drone on...",null,0,cdloddj,1rbz3z,askscience,new,2
SingleMonad,"You don't have everything pinned down in your question.  Namely you need to know how big an ice cube.  Given a 500 ml glass of water, it may well be impossible with what would be considered a conventional ice cube.  

Assuming that no heat is lost to the environment, setting the heat lost from the water equal to the heat gained by the ice, using values in Wikipedia for specific heat and heat of fusion for water, and assuming the final temperature is 0 c, I get that the original ice cube warms by the following amount:

**Delta T = 200 M/m** (degrees c), where M is water mass, and m is ice mass.  Since the final temperature (0 c) is 271 degrees above absolute zero, it had better be a pretty big cube.

Disclaimer:  my arithmetic sucks.  Don't bet anything important on the basis of my answer.

Edit:  also assumed the water was initially at room temp (21 c).",null,0,cdlodsp,1rbyvm,askscience,new,11
Farnswirth,"Due to the conflict between /u/just_helping and /u/InexplicableContent results I did my own calculations, which came out to:

167000+2100(Initial water temperature, C) = 576.03(mass of ice) - 2.11(mass of ice)*(temperature of ice, K)

with all masses in grams.

When the initial water temperature is 25C and the temperature of the ice is absolute zero, the required mass of ice needed to freeze the whole cup is: **381g, which agrees with** /u/just_helping 

With an initial water temperature of 21C, and ice temperature of absolute zero, the mass of the ice is: **336g**

For a more realistic scenario, you could assume the initial water temperature is at 1C, and the ice has been cooled with liquid helium (4K).  This gives an ice mass of: **298g**  For liquid nitrogen (77K) the ice mass is: **409g**",null,2,cdlr4kj,1rbyvm,askscience,new,4
null,null,null,0,cdlpv6f,1rbyvm,askscience,new,2
tysongrey,What temperature is the room?,null,0,cdlnj0m,1rbyvm,askscience,new,1
Gradri,"That depends on the initial temperature of the water (probably about 21 °C), and the mass of the ice cube.",null,0,cdlnxxe,1rbyvm,askscience,new,1
EdwardDeathBlack,"I am not sure you are asking a ""clean"" question. 

First,  let us assume room temperature (293K) and atmospheric pressure (101300Pa or so) for most/all of our discussion. Under different condition, water cohesion can change drastically. 

In the absence of gravity, water will easily for an orb the size of a baseball. SO water cohesion

In the presence of gravity, water reacts like a viscous material. It means the rate of deformation of the water is proportional to the stress applied. Note that a viscous material will always deform in the presence of an arbitrarily small stress.  So if I consider this, you are asking how to make water into an elastic material, instead of a viscous material. Freezing seems the obvious answer. 

There are also polymeric materials that you can add to the water to bind it in place and make it an elastic material, [Jell-O](http://en.wikipedia.org/wiki/Jell-O) being particularly well known. 

No sure I answered your question, but as I said, I am not sure I completely grok'ed your intent...",null,0,cdlo75d,1rbyp8,askscience,new,1
SmellyRaghead,"Yes, you can alter the surface tension by using surfactants. As for making a giant ball of it, probably not, unless you were in zero gravity.",null,0,cdlr0fy,1rbyp8,askscience,new,1
dreemqueen,"If you dissolve ionic compounds like NaCl into water, the water becomes more polar and cohesive.  If you dissolve sucrose which is an organic covalently bonded molecule in water,  the water becomes less polar and less cohesive.  You can see the difference if you measure the wetting or contact angle.   Salt increases the contact angle, sugar decreases it.  This is the best way to measure relative surface tension.",null,0,cdmqcd4,1rbyp8,askscience,new,1
goingforth,"It is likely a combination of both, but your former suggestion likely has the largest effect. Moving clouds tend to maintain a consistent shape over relatively short periods of time, and the distortions that do occur are often just that, and don't involve the addition or subtraction of parts of the clouds (again, this is on a short time frame) Likewise, clouds tend to move faster with higher wind speeds, suggesting a correlation. Your second suggestion is responsible for clouds forming, changing shape, and disappearing, but not as much for the movement of clouds.",null,0,cdlptxo,1rbymp,askscience,new,2
neverlupus16,"The question here is phrased incorrectly: when it's a cold morning and you breathe out, you see the water in your breath. But it's not water VAPOR. Water vapor is gaseous water. When it is cooled sufficiently, it will condense into liquid water. 

That condensation is what is actually happening when you see your breath. You see VERY tiny droplets of liquid water suspended in the air. They are so small that the current of air from your lungs will suspend and move them in front of you AS IF they were a gas.",null,1,cdlokzd,1rbyel,askscience,new,18
MarkWW,"Supersaturation is the point at which you can no longer stir sugar into your tea, because the water - at that temperature - can no longer dissolve solids into it. Cool it further and more of the sugar comes out as a solid.

The same happens with cold air &amp; foggy breath. Think of other forms of condensation - water appearing on the outside of a cold glass, or on a window. This is water vapor (gas) from the air turning into a liquid, because the surface is so cold that it turns the gas into a liquid.

The same is true for the breath you can see on cold day. The air outside your body is so cold, that the warm, moisture laden air inside your body instantly turns into liquid - albeit, in very tiny droplets.

Something similar happens when you create rock candy &amp; I encourage you to make rock candy with your brother... Because, yum.

Basically warm gas (or liquid) can support lots of gaseous liquid (or solid) in it because it's so chaotic and full of energy that it can keep the liquid (or solid) a gas (or liquid).

As it cools down, it loses energy and more of the matter that's right at the edge of being a gas/liquid reverts to the state you normally associate it with at that temperature. In other words, water isn't always a liquid between 0C and 100C - it's in a constant state of flux, with more water being gaseous the warmer it gets.

For more mind blowing facts - research Swamp Coolers, which cool the air by adding water to it.",null,0,cdlr7a9,1rbyel,askscience,new,4
chrisbaird,"Water vapor always comes out of your mouth in gas form (water vapor) when you breath. Water comes out of your mouth in liquid form when the air is cold enough to condense the gaseous water that you always breath out into small drops of liquid water, which we see as a white cloud of steam. ",null,0,cdmu6un,1rbyel,askscience,new,1
therationalpi,"Common misconception about sonic booms: You don't just create a boom at the moment when you ""break the sound barrier."" In truth, for the entire duration that an object is traveling faster than the speed of sound, it generates a shock front. This is more apparent when you look at an image of the effect. Here's some [Schlieren photography](http://library.thinkquest.org/12228/Page4.html) of a supersonic jet. The dark colored bow shocks that start in front of the plane are the ""sonic booms"" that it's creating.

So, in answer to your question, nothing particularly special happens when you reach 2 or 3 times the speed of sound. Indeed, you will still be creating sonic booms at those speeds, but you would likewise be continuously generating booms if you were traveling at 1.2x the speed of sound.

Hope that helps!",null,5,cdlnxzg,1rbw8z,askscience,new,24
rocketsocks,"You don't create a sonic boom only when you pass the speed of sound.

When you are traveling at or above the speed of sound you trail a cone shaped shock front which travels at the speed of sound. You usually only here the sonic boom once as an observer because the shock front only passes over you once.",null,2,cdlo7lt,1rbw8z,askscience,new,5
iorgfeflkd,"The sun's power is distributed evenly over an area. When you are focusing it with a magnifying glass, you are essentially taking all the light that reaches an area the size of the lens and combining it to a region the size of the focus.",null,2,cdlpmaf,1rbw5f,askscience,new,3
Mazetron,The sun is a powerful source of light on many wavelengths.  The mafnifying glass just focuses light.  You could burn something with an electric light and a magnifying glass of the light was poweful enough.  I have done it with a laser pointer.,null,0,cdlqqmx,1rbw5f,askscience,new,1
chrisbaird,"Light carries energy. Energy causes damage to materials when it is absorbed in a given area faster than it can be dissipated from that area. The ability of energy (and therefore light) to damage materials therefore is a result of a high energy delivered per unit area per unit time, which we call power density. A lens does not create energy, it just focuses the energy so that there is a high power density in one small region and lower power densities in surrounding regions. If the power density is high enough in the focal region, the material heats up faster than it can cool to its surroundings, and its temperature steadily rises. With high enough temperature, materials will melt, burn, ignite, etc.

Light can be focused because it is a wave and waves can be bent (refracted) at the interface between two optically dissimilar materials (such as glass and air). ",null,0,cdmufp9,1rbw5f,askscience,new,1
neverlupus16,"It's the infrared light of the sun. The fusion reactions of the sun release electromagnetic radiation across the spectrum. You have infrared photons, which explains why you feel heat. You have visible light, which explains how we can see things using sunlight. And we lastly have ultraviolet light, which is how our skin burns and tans.

The wave nature of light allows it to be refracted by traveling through a medium such as glass. By focusing the wave (and all of it's energy) to a single point, you change the energy from being diffuse and spread out to being concentrated in one small region. It is now easy for the infrared energy to be transferred to another object. If the energy is transferred faster than it can be dissipated, the object will increase in temperature and possibly reach ignition.",null,8,cdlon4k,1rbw5f,askscience,new,1
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,new,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,new,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,new,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,new,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,new,2
blacksheep998,"I encountered [this study](http://www.ncbi.nlm.nih.gov/pubmed/12042333) a few years back about whales. They found that the energy demands of accelerating their huge bodies to lunge-feeding speeds to fill their massive mouths with seawater and krill are extremely high.

Massive whales are up against the law of diminishing returns, where each unit they increase in size gives them less and less of a return on that investment.

More info: http://www.americanscientist.org/issues/issue.aspx?id=8779&amp;y=0&amp;no=&amp;content=true&amp;page=2&amp;css=print",null,1,cdlt3h1,1rbu2s,askscience,new,6
Izawwlgood,"Some of it has to do with what other organisms are doing. For example, one of the precipitous drops in insect size was due to the development of birds, which are superior fliers. Bison twice as large may be too cumbersome to evade a pack of wolves, for example.",null,1,cdlxbjl,1rbu2s,askscience,new,3
promega,"The largest discovered organism on earth is actually a fungus.  

""The discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, believed by most to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. Based on its current growth rate, the fungus is estimated to be 2,400 years old but could be as ancient as 8,650 years, which would earn it a place among the oldest living organisms as well.""

Source: http://www.scientificamerican.com/article.cfm?id=strange-but-true-largest-organism-is-fungus

In theory such an organism could continue to grow until it exhausted one of its resources.",null,0,cdmqwxw,1rbu2s,askscience,new,1
null,null,null,3,cdls9rb,1rbu2s,askscience,new,1
deruch,"You need to be more careful with your terms.  How are you defining size?  By mass, volume, area, etc.?  Do you really mean ""animal"" instead of ""organism""?  In terms of organisms, I can make the claim that the largest one is an [aspen forest](https://en.wikipedia.org/wiki/Pando_%28tree%29) in Utah, or maybe a [fungus colony](https://en.wikipedia.org/wiki/Largest_organisms#Fungi) in Oregon.",null,17,cdlse7h,1rbu2s,askscience,new,11
patchgrabber,"Copper is toxic to algae and bacteria in moderate to high concentrations. A copper plate should have an antimicrobial effect, yes; brass doorknobs are known to have antibiotic properties and sterilize faster than, say, an aluminum doorknob.",null,0,cdm5yoq,1rbtxa,askscience,new,2
abstrusey,"""Normal"" is usually defined by sampling lots of apparently healthy individuals and then using statistics to calculate an expected range into which the large majority will fall. This range is often referred to as a ""reference interval."" For aspects of physiology (e.g. heart rate, respiratory rate, temperature, blood pressure, etc.), these ranges are typically set as a standard that you read in a book and/or memorize. For test values (e.g. sodium/potassium/pH value of the blood, red blood cell count, etc.), they are often established individually by the testing facility, and they typically print that range next to the value of the sample they analyzed. 

A reference interval can be established by collecting test results from at least 50 healthy adult animals/humans. In this example, therefore, the range would only represents adults. At least 50 juveniles (and you'd have to define the age range) would have to be collected to have a new ""normal"" range determined. Two statistical analysis techniques are used, based on the distribution of the data. For normal distribution (also called gaussian distribution), there is a ""bell curve"". The data has ""variance,"" which is a representation of how widely scattered most of the animals are when compared to the average of all of them (e.g. they may all be tightly clustered near the average (low variance), or they could be very high and very low away from the average (high variance)). This variance is represented by a number called the ""standard deviation."" In gaussian and non-gaussian distributions, you can use the average ± 2 standard deviations to select for the ""middle"" 95% of the data. The lowest and highest numbers that you collected are now your range, and putting a dash between them makes it a reference interval. In vet med, we have reference intervals for most parameters for dogs, cats, horses, cattle, goats and alpacas, and many exotic species as well. 

It is very important to remember, though, that if the range only represents 95% of the samples -- 100% of which were apparently healthy -- then you should expect about 5 of 100 individuals to have values outside of the range and *still* be okay.",null,0,cdlm8h3,1rbtd9,askscience,new,10
gettingoldernotwiser,"Another way of defining ""abnormal"" is increased risk of death/disease.  People with high blood pressures can have an increased risk of cardiovascular disease, stroke or death compared to those with normal blood pressure.

Similarly, abnormal lab values (glucose, cholesterol) confer a higher risk of disease than normal ones. ",null,0,cdlrvxj,1rbtd9,askscience,new,1
Asrat,"In hospitals and other medical facilities, we typically take an individuals blood pressure every 4 to 8 hours and start generating a baseline for the individual. Your primary care physician does this as well to identify any changes. We also ask what you normally run if you are competent enough to answer. Using that information, for example, we can tell if you normally run 95/55 and today your pressure is 125/75, something is wrong and we need to identify that.
",null,0,cdlsaif,1rbtd9,askscience,new,1
Jyesss,"Antibiotics target certain essential enzymes and proteins that bacteria have but humans do not, thus giving their specificity. These proteins are coded from genes, so in a round about way, yes, antibiotics are based off of genetic targeting in that they disrupt the proteins that those genes create. Bacteria generally do not become antibiotic resistant by changing the protein the antibiotic targets because this would probably kill the organism in the process. Instead, they evolve new genes that code for enzymes that will break down the antibiotic before it can harm the bacteria. ",null,0,cdme16k,1rbr4k,askscience,new,1
leftnuttriedtokillme,"There are a couple of hurdles.  For one thing, you can't target ""just bacterial DNA/RNA"", since the actual structure isn't any different from human DNA/RNA.  You can go after the proteins that form the nucleic acids, because those are slightly different.  And there are some antibiotics that do exactly that.  But it's rather difficult to target bacterial nucleic acids themselves specifically.

There has been some research into using artificial nucleic acids to target specific segments of a genome and basically turn it off, but I don't think they've been able to get it into a practical form that could be used in medicine quite yet.

There has, however, been success in using DNA to determine what antibiotics a particular organism is susceptible to.  Currently one of the normal steps in treating a bacterial infection is to culture and ID the organisms involved, and also to perform a susceptibility test on them, which determines the effectiveness of a number of common antibiotics.  

Traditionally this was done by exposing them to the antibiotic at certain concentrations and seeing whether or not it grows.  The new process allows us to look for the genes responsible for certain types of antibiotic resistance.

The best example of how this is useful would be for MRSA.  If a doctor suspects a MRSA infection, he would traditionally do a culture, which would take 1-3 days to tell him anything.  The new genetic method could tell him if it was *Staph. aureus* in a few hours, and whether or not it was the resistant form at the same time.",null,0,cdmhr6b,1rbr4k,askscience,new,1
Farnswirth,"You can absolutely see one atom thick graphene sheets, this is one of the things that makes it such a remarkable material.  Just look at [This picture](http://3.bp.blogspot.com/-2UU-zkkrxm0/UPXIX6at0RI/AAAAAAAACNc/LsODcsw_1Oo/s1600/High-Speed+Graphene+Circuits.jpg), [the bottom of image a in this picture](http://www.nature.com/srep/2012/120921/srep00682/images/srep00682-f2.jpg), and [image c at the bottom left in this one which shows two single-atom graphene sheets layered on top of eachother on a PET film](http://www.nature.com/nnano/journal/v5/n8/images/nnano.2010.132-f2.jpg).  

We can see graphene when it is only one atom thick because it is exceptionally good at absorbing light.  Graphite and graphene are extraordinarily good at absorbing light mainly because the individual sheets of graphene have an extremely low [band gap](http://en.wikipedia.org/wiki/Band_gap) (pretty much 0eV).  This is also one of the reasons it is such a good electrical conductor as well.  Notice diamond has a very high band gap, which makes it transparant to visible light and a poor electrical conductor.  Finally, take a look at the [physicochemical properties of Boron Nitride](http://en.wikipedia.org/wiki/Boron_nitride#Physical).  While its structure is remarkably similar to graphene, it has a high band gap, which makes it appear white or transparent.  ",null,2,cdlranh,1rbqyc,askscience,new,7
organiker,"If your application calls for multiple layers, then you grow it as multiple layers.

If you're making electronic devices, you lay it flat on a substrate like glass, silicon or plastic, then add electrodes.

If you're making a coating, you layer it on whatever you want to coat. Probably with a protective layer on top.

If you're making batteries or capacitors, you mix it with your electrode materials.


",null,0,cdlo8z4,1rbqyc,askscience,new,1
EdwardDeathBlack,"You incorporate it into another matrix. You layer it in a sandwich of other materials...all that to exploit either its electrical or mechanical properties. 

It is not a first...here is a [TEM](http://large.stanford.edu/courses/2007/ap272/kimej1/images/f2.jpg)  of a gate oxide (a semiconductor term about one of the common layer in modern electronics) that is only a few atoms thick. Or here is  a [STEM](http://origin-ars.els-cdn.com/content/image/1-s2.0-S0038109805008914-gr1.jpg) of a quantum well of the type often use in vcsel and the like. Also atoms thick...

In a world where ""everything"" is made of atoms, all we have is made of ""stacks"" of single atom. The art is to stack them in the right order to make new properties arise that are useful to us. ",null,2,cdlnz09,1rbqyc,askscience,new,2
Platypuskeeper,"Alcohol (ethanol) and water are miscible fluids because the -OH group of the ethanol molecules forms [hydrogen bonds](http://en.wikipedia.org/wiki/Hydrogen_bond) with water, just as water does with itself. 

Hydrocarbons, as in oils, only bond negligibly with water molecules, which means it costs energy to separate the water molecules from each other and stick a hydrocarbon molecule between them. The lowest energy situation is if you lump all the hydrocarbon molecules together and minimize the contact area with the water, that is, form an oil phase.
",null,3,cdm0b7z,1rbl1i,askscience,new,5
LoyalSol,Density has nothing to do with it unless the two materials don't mix.  In the case of oil and water there is a mismatch in how the molecules interact with each other so they prefer to stay separated.   ,null,1,cdm0l07,1rbl1i,askscience,new,2
TehMulbnief,"Couple of reasons. The most direct reason is that alcohol and water are miscible. They mix together very nicely, so much so that you can't tell them apart once they are mixed. The resulting solution is sort of like a metallic alloy. When you look at a stick of bronze, you don't see copper and iron, you just see nice, homogenous bronze.

The other less obvious (and maybe a bit niggly) reason is the Second Law of Thermodynamics. This law introduces the idea of ""entropy"" or randomness of a system. When you add water to alcohol, the tendency is going to be for the two to mix, until the alcohol molecules are perfectly and uniformly distributed amongst the water molecules and vice versa. Once you're at this point, the system is going to stay that way because that's the most stable way for the system to be.",null,0,cdmx3br,1rbl1i,askscience,new,1
rupert1920,"Ash is material is leftover material that cannot undergo further combustion - in the case of complete combustion. This is the white ash you often see when something is completely burnt. An example of this would be [wood ash](http://en.wikipedia.org/wiki/Wood_ash), which consists of the trace mineral compounds in wood that don't combust, such as calcium and potassium (which comes from the word [potash](http://en.wikipedia.org/wiki/Potash), with a similar etymology in how it was made historically - from ash).

Since different compounds will have different amounts of these non-combustible elements, they will naturally have different ashes.

This concept also has direct impact in [gravimetric analysis](http://en.wikipedia.org/wiki/Gravimetric_analysis) - which relies on very careful measurements of weight of a compound before and after some process. You can attempt to weigh out your analyte, then completely burn it at high temperatures, and weigh out the ash that is left over in order to find the chemical composition of the analyte (for example, finding the stoichiometric ratio can give you oxidation states). You'll also find filter papers used for this purpose to be ""ashless"" - it produces purely gaseous products under combustion so it won't skew the results of such measurements.",null,1,cdlzjby,1rbk7h,askscience,new,3
DeadVirus0,"Earth's solstices and equinoxes are based exclusively on its 23.5 degree axial tilt.

For example, our next perihelion will be on January 4th, 2014 while the upcoming winter solstice will be on December 21st. This means that the northern hemisphere's longest night is 2 weeks away from the Earth's closest orbital position to the Sun. [Source: US Naval Observatory](http://aa.usno.navy.mil/data/docs/EarthSeasons.php)

Additionally, 4 Vesta is, relative to other celestial bodies, small and amorphous.

So, I guess what I'm trying to say is that your assumption that solstice/equinox are related to perihelion/aphelion is flawed.",null,0,cdm19tt,1rbj3h,askscience,new,1
bohr_exciton,"Air is not completely transparent, however we perceive it as such for two reasons:

1) The various molecular and atomic species that make up the atmosphere can only absorb light at specific frequencies. If you look at the absorption spectra of the species making up thee majority of the atmosphere, such as nitrogen (N2), you will see that for most of them there will only be negligible absorption in the visible part of the spectrum. 

2) The density of air is so low that we can't perceive the minimal absorption that does take place. The best example of this is water. Water actually has a blue color, which you can see by say looking at the sea. However water only absorbs weakly in the visible, such that to observe this color light needs to pass through meters of water before we can observe its absorption. If you just look at a glass of water, it will just look transparent. Now the density of water vapor in the atmosphere is much much lower than in the glass, and therefore you can't possible see this effect in air with your bare eyes.

Finally, as for why you can see the atmosphere from space, or why the sky looks blue, the answer is that while the atmospheric gases do not significantly absorb visible light, they can scatter the light. Moreover, the major scattering mechanism is so-called Rayleigh scattering, which occurs more strongly for higher energies. Because of this blue light is scattered more than red light, which makes the atmosphere look blue. ",null,1,cdlvjoo,1rbhiu,askscience,new,4
chrisbaird,"Air is not perfectly invisible. Look at the sky during the day. That blue color is air. Look at a distant mountain on a clear day and compare it to a very close hill. The distant mountain will seem to be covered with a whitish haze. That haze is air. Clean air is composed of very small molecules that are very far apart. For this reason, you need a lot of air in order to see it with your naked eye.",null,0,cdmul9m,1rbhiu,askscience,new,2
Qvanta,"Materia excerts light ""color"" only if it has absorbed and emited the light struck by it. Here is the catch, each materia has a specific amount of energy in form of light it accepts. If the light shining on a materia is below or above the requierd amount of energy it needs to possess, light will just pass by.

The atmospheric light comes from the scattering of the blue spectrum when the light passes through our atmosphere. So you actually dont see any color only the smudging of the suns blue light.  ",null,2,cdluxwu,1rbhiu,askscience,new,1
meerkatmreow,"Black both absorbs and radiates better.  The net result can be a lower equilibrium temperature.  For example, the SR-71 was originally not painted black.  However, it turned out that the black paint lowered the temperature enough to allow the structure to be lighter even with the extra weight of paint, resulting in a net savings.",null,0,cdm5fnf,1rbfb3,askscience,new,2
miltoniousbastard,If you have ever touched a tinted window you will notice that it is noticeably hotter than non tinted windows on the same vehicle. The tinted window is absorbing most of the light (energy) which keeps it from being transferred to your cars interior. The heat on the window is radiated to the surrounding outside air vs. the heat radiating off your seats/dash/whatever else is in your cars interior.,null,0,cdm0fzy,1rbfb3,askscience,new,1
Greyswandir,"I think perhaps this is a good lesson that aesthetic concerns sometimes (often?) trump engineering ones.  I think the real answer to your question is that many people don't want other people peering into their cars, and as you pointed out, other coatings are more expensive.  I imagine that concerns about heat probably weren't too much a part of the design process.

And, as plenty of other people have pointed out, having the light absorbed by your windows which are in contact with the outside and have a high surface area, keeps the heat from instead being trapped inside your car.",null,0,cdmvmx1,1rbfb3,askscience,new,1
sharp12180,"Electrons are bound to an atom and his bond has a certain amount of energy. If the is an incident photon with energy greater than or equal to this bond energy, it can cause the electron to become unbound. When enough electrons become unbound, you have a current. In a solar panel, some of the photons from the sun have the right energy to dislodge electrons in the panel which creates usable electricity. ",null,0,cdm0x32,1rberh,askscience,new,2
rupert1920,"You should also take note that it is the [photovoltaic effect](http://en.wikipedia.org/wiki/Photovoltaic_effect), not the photoelectric effect, at play here (hence a ""photovoltaic cell"").",null,0,cdm8iuh,1rberh,askscience,new,1
owaisofspades,"It's a bit complicated, but I can give you the general idea.

During glucose metabolism you go through glycolyis, which gives you pyruvate. Pyruvate then gets converted to acetyl CoA. Now here's where it gets tricky. If you need energy, your cells are going to send the acetyl CoA through the Citric acid cycle to make loads of ATP. If you don't need energy, the acetyl CoA gets shunted to fatty acid synthesis.

YOu have some enzymes involved that activate the acetyl CoA into malonyl CoA, and this allows you to add an acetyl CoA to it, forming a chain. After a certain number of extensions you get palmitate, which is a freefatty acid, which can then be modified to form other fats or phospholipids, or which can be esterified with a glycerol molecule to form mono-, di-, triglycerides.

This takes place in the liver. Your liver then packages the TAGs into lipid containers (chylomicrons) and then put them into the blood stream. Then lipoprotein lipase on the surface of adipocytes grabs the chylomicron and pulls the TAGs out of them.

Now for the second part of your question. fatty acids don't get converted back into glucose as far as I know, but into acetyl CoA through a process known as beta oxidation (for short and medium chains, I don't remember the long-chain degradation process ATM). The acetyl CoA is for the liver to use (it's not water soluble so can't get transported in blood). For other tissues, the liver converts the acetyl CoA from Beta oxidation into ketone boies, which are water soluble and can be transported to other organs such as the brain through the bloodstream.

Hope this helps, feel free to ask for clarification. As a med student with my biochem final coming up soon i'm trying to keep my knowledge from disappearing haha",null,0,cdmbnot,1rbdz6,askscience,new,2
iorgfeflkd,"Freefall doesn't get rid of tidal gravitational fields. The difference in Earth's gravitational field between the front and back of the ship could be detected with precise instruments, and would be absent in intergalactic space.",null,1,cdm16ju,1rbd4j,askscience,new,4
AbouBenAdhem,"When the temperature of a gas changes, its density changes. When its density changes, its index of refraction changes. When light passes from one substance to another substance with a different index of refraction, it travels at a different speed; and when it meets the interface between two such substances at an angle, it bends.",null,3,cdlghd7,1rbb96,askscience,new,23
Polyknikes,"I don't think anyone will have an exact answer to your question because it would depend on which cell you are talking about, how well stocked they were beginning the fasting period, and how much energy they are being asked to expend over a given amount of time.  As an alternate answer I will discuss what happens during starvation which hopefully will answer your question.

In the normal course of starvation we first burn carbohydrates which basically refers to glucose.  Glucose is stored in many cells, but particularly in the liver hepatocytes, in the form of glycogen.  The breakdown of glycogen is referred to as glycogenolysis which releases glucose into glycolysis for energy production.

After all the glycogen is used up the body begins catabolizing (burning) proteins.  Protein catabolism involves the breakdown of bodily proteins into amino acids for use in synthesizing more glucose in a process called gluconeogenesis (which takes place in the liver).

Sometime during protein catabolism your body will begin the process of lipolysis which is the breakdown of triglycerides into fatty acids which can be oxidized into multiple units of Acetyl-CoA and fed into the TCA cycle for energy production, bypassing glycolysis.  High rates of fatty acid oxidation will lead to ketogenesis, or the creation of ketone bodies.  Ketone bodies are another form of high-energy molecule like glucose which can be metabolized by many tissues and are especially important for the brain during the starvation state as it has high energy demands and cannot directly metabolize fatty acids.

TLDR: During starvation your cells first use glycogen (stored glucose), then catabolize proteins into glucose, then burn fats in the form of ketone bodies.

Hope this answer helps.
",null,0,cdlx6pc,1rb8zj,askscience,new,2
owaisofspades,"a-helix and b-sheets are due to hydrogen bonding. Random coil is due to hydrophobic reactions if I remember correctly. There's also di-sulfide bridges, which only cysteine can form. 

Whether a protein will form an a-helix or a b-sheet depends on the sequence (the amount of residues between the two interacting residues determines which one will form)",null,0,cdmbq05,1rb8il,askscience,new,2
reddishpanda,"Short answer: the types of interactions needed to produce a secondary structure can be made by a variety of amino acids. 

Helices and sheets are made by the backbone interactions between the amino acids of a proteins, so almost anything goes (except for proline, which would be too geometrically limited to form a helix or sheet and is limited to loops and random coils). You can use many combinations of different amino acids to make one structure or another, but interactions among the side chains of amino acids (where you will find hydrogen bonding and hydrophobic interactions as well as salt bridges between say glutamate and lysine). 


It might be information overload, but try playing around with a protein database like [RCSB](http://www.rcsb.org). If you just come up with an amino acid sequence of your own creation, you can use [Phyre2](http://www.sbg.bio.ic.ac.uk/phyre2/html/page.cgi?id=index) to get a prediction of what your protein might look like and the secondary structures that might form it. ",null,0,cdmedut,1rb8il,askscience,new,2
stevenstevenstevenst,"I do not know a lot about what the atmosphere is actually like within the ISS, but I can tell you that in a pure oxygen environment, as was used on some NASA flights, those on board have reported that it is difficult to hear one another.  This is perhaps related to the pressures in these vessels more than atmosphere itself, as a denser atmosphere transmit sound waves more readily.  

Kind of just a curiosity, so I apologize if your question was not answered in its entirety.  The atmosphere being thicker or thinner is related more to pressurization than the content- although content does play a role.",null,0,cdmgh7o,1rb86r,askscience,new,2
Proxymace,"At high concentrations oxygen is toxic to organisms, at 100% humans get a ""high"" from breathing it. This tends to make people calmer and is why planes deploy oxygen masks and not just air masks. For the iss I imaging its due to weight limits on supply ships. Carrying a load of nitrogen that you don't need isn't very good.",null,1,cdmjb36,1rb86r,askscience,new,3
chrisbaird,"""I saw somewhere that in the ISS and other stations that they have a 100% oxygen environment"" You saw wrong. The composition of air on the ISS is definitely not 100% oxygen, and is in fact intentionally regulated to match the composition of air on earth's surface, with about 21% oxygen:

http://www.nasa.gov/missions/highlights/webcasts/shuttle/sts112/iss-qa_prt.htm

There are a couple of reasons for this:

- Pure oxygen is highly flammable. NASA unfortunately learned this the hard way with the Apollo 1 accident
- High oxygen concentrations are unhealthy to humans as our bodies have evolved to work most efficiently with the oxygen levels common at earth's surface.",null,0,cdmutcf,1rb86r,askscience,new,2
Smoothened,"Microorganisms are the main cause of spoilage, but there are other ways in which food can go bad. This varies both with the content of the food and the environment it is exposed to. For example, food that is partially composed of water can dry out. Protein and other molecules in the food would undergo degradation, which will result in changes in both texture and taste. Another thing that can happen is the separation of ingredients in different phases. These changes would be less obvious in foods that are homogeneous and mainly composed of simple molecules such as sugars. All in all, the changes would generally occur more slowly than in the presence of microorganisms. Most likely the resulting food wouldn't make you as sick as if you ate food spoiled by bacteria or fungi. ",null,0,cdmc7d1,1rb7jz,askscience,new,2
StarshipEngineer,"There is no such thing as terminal velocity in an airless environment. It doesn't matter what the terminal velocity of an object in air is, if there is no air for the object to interact with through friction, the object will keep accelerating as it falls until it hits a solid surface.",null,1,cdlfu8p,1rb77o,askscience,new,12
PepperJack_delicacy,"The ""pins and needles"" feeling is referred to as **paresthesia**, which occurs when a nerve and the arteries supplying the nerve are compressed. This prevents the nerve from carrying electrical impulses that transmit the sense of touch, which you feel as ""pins and needles"". 

It's harmless when it occurs transiently (like after you fall asleep on your hand) because once the pressure is removed, blood supply to the nerve will be restored. However, there are certain chronic cases that are indicative of neurological disease or more traumatic nerve injury, which are more serious.

In the case of a blood clot, though, you are created a plug in an artery that prevents blood flow to a tissue. There is nothing you personally can do (such as switch body positions) that will remove the clot. Furthermore, in the case of a heart attack or stroke, you are preventing blood supply to the heart or brain--two of the most important organs in the body, which absolutely need blood for you to survive. 

So overall, it's true that ""pins and needles"", strokes, and heart attacks are caused by circulation problems. However, in the case of ""pins and needles"", the blockage is readily reversible and the organ that is losing blood supply is no where near as important as the brain or heart. 

Sources:

http://www.ninds.nih.gov/disorders/paresthesia/paresthesia.htm

http://www.urmc.rochester.edu/encyclopedia/content.aspx?ContentTypeID=1&amp;ContentID=58",null,0,cdm065z,1rb75x,askscience,new,2
brawnkowsky,"'pins and needles', which is called Paresthesia, is caused by pressure applied to a nerve.  This inhibits its ability to conduct a signal and eventually leads to a limb 'falling asleep' (a dead leg).  a lack of blood circulation does not cause this.

lack of circulation (specifically tissue perfusion) results in a failure to deliver oxygen to systematic cells and to remove metabolic waste.  This lack of oxygen causes cells to create energy through alternative pathways that create acidic products (lactic acid is common), causing acidosis.  Eventually, the cells will die because they fail to maintain the pH needed to function; this is called Ischemia.  Ischemia in organs can lead to organ failure, which will kill a person.  ",null,0,cdm0add,1rb75x,askscience,new,1
adoarns,"Pinch a nerve long enough, and it becomes permanently damaged. The nerve fibers will wither back, and you will lose sensation until the nerve grows back (about 1 mm/day). Even then you may expect that not all the withered fibers will find their way back to their proper locations.

Heart muscle and brain tissue are much more metabolically active, and take much less time without proper blood flow to be permanently damaged.

Unlike peripheral nerves, brain tissue and heart muscle does not grow back. You lose it, and it's gone for good.",null,0,cdmc3ds,1rb75x,askscience,new,1
_Momotsuki,"If you squeeze just one vessel of your arm, there are many collateral vessels to take up the slack and perfuse the rest of the arm. This principle applies to your heart because there are only a few main vessels that supply the heart (with great variation between individuals, and very simplistically, there's the left and right coronary arteries with the left splitting very early to become the left anterior descending and circumflex artery). Any blockage in one of those 3 vessels will cause ischaemia and lead to death of the tissue areas that are meant to be supplied. Indeed, there are collaterals present in the coronary circulation. However, these are usually functionally non-patent and can't really help with distributing blood because they have such a small lumen. This is especially the case when there is ischaemia due to a sudden thrombo-embolic event. With that said, in *some* cases where there is a slow build up of atherosclerotic plaque within arteries, you can get slow opening of these collateral vessels to help perfuse the heart.",null,0,cdn9eo8,1rb75x,askscience,new,1
sharp12180,"The force of gravity due to an object with mass is never zero. It can be very small if the object has very low mass or you are far away from the object. For a galaxy, which is very massive, you can get far enough away where the force of gravity acting on you is negligible but it will never be zero. In fact, the gravitational force is proportional to the inverse-square of the distance between two objects, meaning if you double your distance from an object the force of gravity decreases by a factor of four. Still, this force will never be zero.  ",null,0,cdm0ted,1rb6tc,askscience,new,3
Ejb90,"In a way, no - the gravitational field produced by mass is infinite, so the field has an effect throughout the universe.
In another way, yes - there exist points called Lagrangian points where the gravitational fields of objects cancel to produce zero net acceleration. These are quite common, there are several around earth, being utilised for their stability. Though here the rotational effects of the entire galaxy have been ignored and considers the Sun-Earth system as relatively stationary.",null,0,cdm0vay,1rb6tc,askscience,new,2
ShwinMan,"Short answer: no

LADEE is a small spacecraft, it's only 2.37m high and if it were visible then so should all the other spacecraft there now as well, including the Apollo descent stages, lunar rovers, debris etc. Even Hubble wouldn't be able to make it out.",null,0,cdmxh0z,1rb6hn,askscience,new,1
Ejb90,"I think you're misunderstanding the structure of an atom. The ""shells"" the electrons occupy aren't determined by their distance from the nucleus, but by the relative energies of the electrons in each. Why this happens is explained by quantum mechanics.
In the classical case, the ""radius"" of the first shell would simply expand with the nucleus, though that would never be a real problem - the nucleus is 10^-15m across and the atom 10^-10 - that's 10,000 times bigger, so any atom that big would be inherently unstable.
However in reality the electrons aren't hard point of mass whiz zing around the nucleus, they're a ""cloud"" of delocalised charge with certain characteristics, also described by quantum mechanics. If you want to know more there are loads of online resources about this.",null,0,cdlgd0k,1rb1wp,askscience,new,7
Platypuskeeper,"Why would it 'force electrons out of the lowest shell'?
",null,1,cdlg0vx,1rb1wp,askscience,new,3
thumbs55,"Basically no.

If you have a very small object (less than the DeBrogle wavelength of the electron ~ 10nm) then the electrons in the outer shell of this object may have descrete energy levels and be treated as a giant atom.

This is refferd to as a [Quantum dot.](http://en.wikipedia.org/wiki/Quantum_dot)

So these quantum dots are much larger than the lowest shell of a Hydrogen atom but still dsiplay quantum properties.

Sure the nucleus is like 15 orders of magnitude smaller than the electron shells but if we had a nutron star and some how made it positevly charged, and placed an electron to see if it would orbit it:

Then we would find that the electron and the proton in the star join together to create a neutron due to all of that pesky gravity.

If the system is changed such that the lowest shell no longer exists then the next lowest shell by definition becomes the lowest. This is just simantics and is a bad argument since the lowest energy is an s shell and the next lowest is a p shell and behave measurable differently. And if you did get rid of the first s shell then the new lowest shell would be an s shell at a higher energy.",null,0,cdlgc3l,1rb1wp,askscience,new,1
miczajkj,"In fact this is something, that can happen but not for electrons. 

You may know, that there are three generations of quarks and leptons, while our universe consists mostly from the first generation (up- and down-quarks + electrons and electron-neutrinos). But there are certain natural processes, that produce particles from higher generation, for example the cosmic radiation in the earth's athmosphere. 
The electron equivalent in the second generation is the muon - and if you construct muon-atoms your question gets important!

Like the others already mentioned, 'orbiting' electrons (or muons) don't really orbit the nukleus: their position gets described by a probability density; those densities are the absolute squares of the particles wave function. 
If you calculate those wave functions you find, that the probability density of the ground state has it's maximum at the classical Bohr radius, that can be calculated by using a semi-classical force approach (for muonic hydrogenium): 

Let the Coulomb-Force (ℏαc/r^2) equal the centripetal force (p²/mr). Also keep in mind the the uncertainty principle: pr ~ ℏ. It follows:

ℏαc/r^2 = ℏ^2 /mr^3 
r = ℏ/αmc

Now, because the mass of the muon is 200 times bigger than the mass of the electron, therefore it's Bohr Radius is 200 times smaller and if you also consider the finite circumference of the proton it is possible, that the muon may have finite probability to be found inside of the nukleus, especially if you talk about heavier nuklei. 

The main consequence is a displacement of the energy niveaus. This fact was used, to find the much used formula

r = 1.2 fm * A^(1/3) 

for the radius of the nukleus depending on the Mass number. 

Another consequence is the increased probability of decays like the K-capture, where a muon from the K-shell reacts with a proton:

µ^- + p -&gt; n + ν_µ",null,0,cdlsooe,1rb1wp,askscience,new,1
Ruiner,Only by the amount of mass it consumed. ,null,0,cdlt15h,1rb1bz,askscience,new,4
Infinite_Ambiguity,"Steven Hawking has shown that black holes also radiate energy because of quantum effects near the event horizon.  Consequently, black holes might increase in mass/energy by the amount of mass/energy consumed, but they are also radiating mass and energy (equivalent by e=mc-squared) and thereby also simultaneously evaporating to some degree. .  ",null,0,cdltx5v,1rb1bz,askscience,new,2
snusmumrikan,"[This paper](http://www.academia.edu/372962/Giants_on_the_landscape_modelling_the_abundance_of_megaherbivorous_dinosaurs_of_the_Morrison_Formation_Late_Jurassic_western_USA_) discusses it for herbivorous large dinosaurs and says a few tens of each per sq km. 

[This one](http://earth.geology.yale.edu/~ajs/1993/11.1993.06Farlow.pdf) discusses the limiting factors in population density of large carnivores and the balance between food availability and having enough of each species to ensure a mating fequency high enough to avoid extinction.

It seems your question can't be answered reliably as so much depends upon the preservation of dinosaur remains for that. Looking at the variables and comparing it to modern-day predators might be the best option?",null,3,cdlea2h,1raxgi,askscience,new,9
invariance,"No. It is simply inconclusive, because there exist series which diverge and series which converge for which the ratio test gives 1. For the series a_n = 1/n, the sum diverges. For the series a_n = 1/n^2, the sum converges.

The ratios are not 1 in magnitude, except in the limit. Note also that both ratios converge to 1 from below. A refined version of the ratio test will tell you that if |a_{n+1}/a_n| &gt;= 1 for sufficiently large n (so the ratios converge to 1 from above), the series will diverge. The proof for this follows from what you have already said. So in fact, the only inconclusive case is if the limit of the ratios converges to 1 from below.

So the short answer is that if the limit of the ratios converges to 1 from below, the test is inconclusive because there are examples which converge and examples which diverge. ",null,1,cdlud5b,1rawis,askscience,new,6
iCookBaconShirtless,"The issue is not which direction that the limit is approached, as you conjecture.  While approaching 1 from above assures divergence, approaching from below does not assure convergence.  A simple example is the harmonic series.

They key to understanding the ratio test is to more precisely understand this statement that you made:  

&gt; I understand what happens when the limit is smaller than 1 (every element of the series is smaller than the previous by a factor L, hence the series tend to stabilize and converge).

The fact that every element of the sequence is smaller than the previous by a factor of L (at least in an asymptotic sense) implies that the sequence converges to zero **exponentially fast**.  Basically, it looks like a geometric series at large n.  This exponential convergence of terms is enough to ensure that the series converges, but it's strictly stronger than what is needed.  Plenty of series have terms that converge more slowly than exponentially, but still converge as a series.  Any p-series with p&gt;1 for example (e.g., 1/n^2 ).

tl;dr Ratio test determines exponential convergence of terms, which is more than is needed for convergence of series.",null,0,cdlv91r,1rawis,askscience,new,4
wgunther,"In order to understand the ratio test you have to understand the proof. The proof is if the ratio a_{n+1}/a_n goes to L&lt;1 then eventually the of consecutive terms is less than 1-epsilon for some small but positive epsilon, and therefore, the series is smaller than the geometric series a(1-epsilon)^n for some suitable a. Thus it converges by direct comparison. 

If L&gt;1 then you can do the same thing but with 1+epsilon. 

The problem is this proof doesn't work if L=1. The ratio test will only work when the sequence of the summand of the series converges *faster* than something geometric whose series converges or *slower* than something geometric whose series diverges. In the case when L=1, one can not compare the series to a geometric series that converges or diverges. ",null,0,cdmmc8x,1rawis,askscience,new,2
onyablock,"People can become immunocompromised through various ways including pregnancy, viral infection, steroid treatment etc. etc.

The reason it is important when considering vaccinations is that being immunocompromised can increase your risk of obtaining the infection to which you're being immunized against (if the vaccine is 'live' virus) or being vaccinated can be pointless as no immunity will actually be gained from the vaccine.

The effects of an immune deficiency on vaccines varies greatly depending on the vaccine and its application regiment. For example it could be recommended that you receive multiple booster shoots or don't receive the vaccine at all if you are immunocompromised. 

In the case of the flu shot, obtaining the killed-virus vaccine won't allow you to actually contract the flu, however the chances of you generating good and long lasting immunity to the flu is reduced if you are immunocompromised. Obtaining the 'live' attenuated vaccine would not be recommended for immunocompromised patients as there is potential for infection, hence why the box is there.

I hope that makes sense.

Source: 4th year immunology student.",null,3,cdlcf5d,1rawff,askscience,new,18
tthershey,"Live vaccines are generally contraindicated in immunosuppressed patients because these patients will not be able to mount a sufficient immune response to the vaccine.  Consequently, the live vaccine could induce an infection.

Inactivated or component vaccines won't put immunosuppressed patients at risk, but the patients might not gain much protection from the vaccine.  This is because immunosuppressed patient's won't be able to generate antibodies to help fight off future infections.

Immunosuppressed patients most certainly need to get vaccinated because they are at greater risk for getting serious complications from infections.  It's important for health care providers to know a patient's immune state in order to deliver the right kind of vaccine.",null,0,cdlk7wj,1rawff,askscience,new,3
Chandley54,"There're three reasons why you need to be immunocompetent when vaccinated. 

1. It is rare nowadays for a vaccination to be live &amp; pose a direct threat, some of them may revert to being harmful, for example attenuated vaccines commonly have a harmful gene removed before they are given to the host so they can be dealt with by the immune system without any significant risk. In some cases the process may not be perfect and so some live unattenuated virus may get into the vaccines, so although in theory the virus should not be able to establish itself and replicate, it sometimes can and is therefore much more hazardous to immunocompromised patients. The same is true for killed virus vaccines - in theory all of the virus should be dead, but whatever process they use to kill the virus (e.g. exposure to UV light) may be ineffective, so again, live virus may end up being present in the vaccine and lead to an active viral infection.
2. If a patient is immunocompromised, there is a possibility that the body will not be able to respond effectively and generate the necessary memory lymphocytes for the vaccine to be effective. This would mean that should the person then encounter a live version of the virus, the vaccine would not have provided them with any protection at all as the memory cells were not generated at the initial vaccination. so are not available to respond.
3. Although the vaccine you received was killed, the form you filled out was probably written by some legal team at some point in history when they were using a different type of vaccine, so they're basically just covering their own backs, and if may be expensive for them to change the documentation.

I would imagine it is likely a combination of the above 3 reasons!
Hope this was helpful.

Source: Veterinary Surgeon/Anatomical Pathologist",null,2,cdlcbxz,1rawff,askscience,new,3
Urgullibl,"The point of a vaccine is to stimulate your immune system into producing antibodies against whatever it is you're being vaccinated against. If your immune system is suppressed, there is no point in vaccinating, as the reaction would not result in protection from infection.

In case of the flu vaccine, we're talking about a dead vaccine, i.e. there are no attenuated whole viruses in it, hence there is no risk of getting the flu from it. In case of vaccines containing whole attenuated viruses, an immunosuppressed patient might get sick from such a vaccine.",null,1,cdlk4zy,1rawff,askscience,new,2
killer_alien,basically there are two types of vaccines: antibodies and dead virus or w/e cells. Anti bodies is a short term vaccine which basically grands you immunity whereas dead cell ones stimulates your body to create antibodies which is a long lasting treatment. (This is the most basic i can put it w/o making to wrong and confusing),null,0,cdmicxd,1rawff,askscience,new,1
SingleMonad,"What you're asking has been done.  It called a pulsed laser.  You are imagining making little pulses of red light no more than a few femtoseconds long.  The output is not red.  It is *white*.  The light has a broad spectrum, centered about red, the width is inversely proportional to the pulse duration.

Wiki ulatrafast, supercontinuum, frequency comb, laser.  If you disperse the output, you will see the individual colors in the laser.

http://grad.physics.sunysb.edu/~meardley/fiber/weiss5.jpeg",null,2,cdloqib,1rawb8,askscience,new,12
null,null,null,15,cdlfycp,1rawb8,askscience,new,8
Sunscorch,"Ok.

For a second, forget that the Earth and Moon are orbiting, and picture the Earth falling straight down towards a stationary Moon. At the very start of our thought-experiment, the Earth is also stationary and its oceans are equally spread out across the entire surface.

The Earth then begins to accelerate towards the Moon, as is begins to fall. The ocean nearest the Moon experiences the greatest acceleration, because it is closer and therefore is exposed to the greatest force. Likewise, the ocean furthest from the Moon experiences the lowest acceleration for opposite reasons. The Earth itself, of course, experiences an average acceleration.

So! The Moon-side ocean begins to move away from the surface of the Earth, as it is accelerating faster than everything else, creating the bulge that is easiest to understand. The ocean on the other side, however, gets ""left behind"" because it is accelerating slower than the Earth. Essentially, the Earth is moving away from it, which creates the opposing bulge on the far side from the Moon.

That is how it works in our thought-experiment, but exactly the same thing happens in orbit because the Earth and Moon are essentially falling towards each other, and are constantly accelerating because of the constant change in direction. That is why there is a tide on each side of the Earth.",null,2,cdlgab2,1ravmk,askscience,new,12
thumbs55,"Excellent question:

[Sixty symbols did a video on it.](http://www.youtube.com/watch?v=YO3eDYzFp8Y)

[In this image](http://hendrix2.uoregon.edu/~imamura/123cs/lecture-2/tides.jpg)

Thinking in vectors the first image is of 5 vector forces acting toward the moon. Note that the farther they are the weaker they are and the top and bottom are not parrallel to the other three.

But we dont live in space we live on the earth so if the earth moves we move with it and dont notice so for that reason in the second image the middle vector is subtracted.

Giving a zero vector in the middle since anything minus itself is zero.

The top and bottom vector are pointing in since they were already pointing a little bit in.

The vector nearest to the moon is a lot shorter but still pointing toward the moon.

And most interestingly the vector farthest from the moon is actually pointing away from the moon.",null,2,cdlgink,1ravmk,askscience,new,7
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,new,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Piškur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Piškur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,new,8
ColdWaterEnthusiast,"Good question. It is almost pointless to try and find out whether the Sahara is growing or shrinking, because of the sheer size of the desert (as well as demarcating what exactly constitutes 'desert'). In the late 90s to mid 2000s, the thought was that the Sahara desert was expanding southwards by a certain extent each year. This was of course somewhat exaggerated. On the other hand, so is the perception that the deserts are 'in retreat' as these articles seem to imply.

The thing is, between the late 70s to late 80s, there was a significantly dry period in the Sahel region (the transition zone between the Sahara and the savanna) which exacerbated the effects of desertification, leading to the perception in the 1990s-2000s that the desert was indeed expanding. However, over the last 15-20 years in terms of precipitation, the region has been in a comparatively very wet period. Relative to the significant drought the area previously experienced, it may seem that the deserts are in 'retreat' but arguably that is essentially what is expected in terms of how vegetation has responded (it gets a bit more complicated because some of the previous mesic vegetation has been replaced by xeric vegetation in certain areas so while it is greener, it is not quite the same)

I hope this help. I could go into more detail but this should give you an idea of how complicated it is to understand.

Source - My dissertation research has broadly to do with understanding how vegetation responds to moisture events",null,1,cdlfsdd,1ravcx,askscience,new,6
PepperJack_delicacy,"Smoking cigarettes essentially speeds up the aging process of the skin, which leads to wrinkles. The main reason this happens is because **nicotine** is a **vasoconstrictor**, meaning that it narrows the blood vessels that supply the skin. When you impair blood flow, it has a harder time getting oxygen and absorbing nutrients such as **Vitamin A**, which normally keeps the skin hydrated and protects it from oxidative damage. Furthermore, it will have a harder time repairing wounds and synthesizing a protein called **collagen**, which keeps the structure of the skin intact. 

In summary: 

*Smoking cigarettes ==&gt; decreased blood flow to skin ==&gt; skin gets less oxygen and nutrients ==&gt; skin has a harder time protecting itself from damage and repairing wounds.* 

Sources:

http://dermnetnz.org/reactions/smoking.html

http://www.mayoclinic.com/health/smoking/AN00644",null,2,cdll8tr,1rav3s,askscience,new,7
iorgfeflkd,"The way it would manifest itself is through a change in density, but because water isn't very compressible, its density only changes by like 3% even at the bottom of the ocean, so the transmission of light from a source at that depth isn't too much different.",null,2,cdlclzj,1rarw1,askscience,new,5
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,new,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,new,2
Chandley54,"Yes, in veterinary medicine we can categorise hyperthyroidism based on where the tumour is within the hypothalamic-pituitary-thyroid axis. It slightly alters our treatment options for it, but as far as I know, hypothalamic/pituitary surgery is almost never carried out in animals clinically, so many general practice vets will simply treat the general hyperthyroidism. I imagine in human medicine where surgical removal of the tumour is a possibility they are a lot more rigorous with determining where the cause is in all cases!",null,3,cdlcihm,1raq8w,askscience,new,6
mklevitt,"theoretically, yes, but i don't know that one has ever been proven or written up in the human scientific literature. secretory (hormone-producing) tumors of the hypothalamus are rare in general, and to find one that specifically comes from clonal expansion of TRH neurons, and then actually secretes? i couldn't find an example among the human literature. a much more common (albeit still very rare) cause of hyperthyroidism is TSH-secreting pituitary tumors, like you said. most hypothalamic tumors cause dysfunction by damaging/suppressing 'normal' functioning hypothalamic nuclei. thus common hypothalamic tumors like craniopharyngiomas can cause hypopituitarism (from suppression of nuclei that stimulate the pituitary) but not hyperpituitarism.",null,0,cdmppa5,1raq8w,askscience,new,1
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,new,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,new,5
rupert1920,"Disclaimer: I'm not an expert in protein folding, and would love to be educated more on the matter.

Check out [Levinthal's paradox](http://en.wikipedia.org/wiki/Levinthal's_paradox), which states that the sheer degrees of freedom a protein has makes it highly unlikely to spontaneously fold into the energetically stable conformation. Which means that there must be other effects - other than thermal sampling - that ""guide"" the protein into the proper conformation. This could be chaperones, or stable intermediates.",null,1,cdl9son,1raob3,askscience,new,5
Osymandius,"As /u/rupert1920 has said, Levinthal's paradox states that it would take longer than the age of the universe for a polypeptide of 100 residues to fold into the correct configuration by ""trying"" all phi/psi angles. Anfinsen et al (Anfinsen, CB et al (1961) ProcNatAcadSci 47, 1309-1314) showed that primary structure directly determines tertiary fold, therefore trying all the possible angles is not required - as was evident by proteins folding on a biological time scale and life existing to begin with!

Your question, therefore, is a very good one: if primary structure does determine tertiary structure, why bother with chaperones?

Ken Dill answers this nicely in an excellent review [here](http://www.nature.com/nsmb/journal/v4/n1/abs/nsb0197-10.html). He encourages you not to think of protein folding pathways, rather protein folding tunnels where there exist multiple routes to the most stable configuration (i.e. the lowest energy). Through these multiple routes can exist ""energy traps"" - local energetic minima which require energy input to overcome such that the polypeptide can reach the final fold. This is where chaperones come in. You can sort of think of them as proteins which recognise improper folds - say extensive hydrophobic stretches facing the surface of a protein - pull them apart and say ""try folding again"". This is why we don't get trapped in an infinite loop: who folds the chaperones? The chaperones aren't specific to any one protein, rather they recognise common folding mistakes.

Edit: His review really is rather good - if it's trapped behind a pay wall, reply and I'll get it for you - he explains it much better than I do.",null,1,cdlatkk,1raob3,askscience,new,5
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,840,cdlaovd,1raftj,askscience,new,2810
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,18,cdl8mfx,1raftj,askscience,new,115
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,12,cdlb9od,1raftj,askscience,new,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,new,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,0,cdl9sks,1raftj,askscience,new,20
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,3,cdldi1w,1raftj,askscience,new,12
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,1,cdl8ok4,1raftj,askscience,new,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,5,cdl8sip,1raftj,askscience,new,7
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,new,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,new,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,new,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,new,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,new,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,new,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say ⌂V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (⌂V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming ⌂V_l &lt;&lt; ⌂V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as ⌂V -&gt; ⌂V_l) [⌂m/⌂V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,new,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,6,cdl9v5f,1raftj,askscience,new,7
null,null,null,0,cdl8egp,1raftj,askscience,new,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,new,20
null,null,null,7,cdl88u6,1rae6m,askscience,new,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,new,1
null,null,null,3,cdlc4wt,1rae6m,askscience,new,1
iorgfeflkd,"Matter doesn't contain gravitons, and if gravitons are used to describe gravity they are neutral particles that are their own anti-particles.

Anti-matter is expected to behave normally in gravitational fields, although it's hard to get enough of it in one place to test this. There is an experiment at CERN working on it, [here is their 1990s looking website](http://aegis.web.cern.ch/aegis/).",null,1,cdl9d7n,1ra79d,askscience,new,14
stevenstevenstevenst,"As explained, antimatter will behave classically with respect to gravity.  If, however, you want something with the repulsive properties described, you need negative energy (or, correspondingly negative mass).  While this is a rather abstract idea, negative energy has in fact been observed-notably in the Casimir effect.  

In another example, lasers are reported to produce energy in an oscillation of positive and negative energy.  Assuming this is correct, it is easy to imagine a series of rotating mirror which could then separate the negative and positive energy.  This is the beginning of a discussion on how to expand a singularity in a rotation wormhole blahblahblah time travel etc.  For this process it would be necessary to isolate negative energy.

http://en.wikipedia.org/wiki/Casimir_effect",null,0,cdmfqwz,1ra79d,askscience,new,1
Izawwlgood,"To your general question, yes! A number of groups are working on culturing induced pluripotent stem cells into whole organs, by either decellurizing pig equivalents and seeding the collagen matrix with the iPSCs, or causing the tissues to grow onto some other matrix. It's pretty cool and exciting work!

But as to why some transplants need to be grafted to a person; the host body will facilitate the vascularization of the tissue, and 'feed' it, which will allow it to grow and remain healthy. At a later date, when the graft is ready, surgeons will move it. 

Ever seen someone with a crush wound have their hand stitched to their chest or side? It's to promote blood flow into the damaged tissue. Similar idea.",null,0,cdl8bvk,1ra6bi,askscience,new,5
KarlOskar12,"The reason they grew the nose/facial skin on the person is because a complex medium is required to grow human tissue. It *can* be done in a lab, but it is much easier to attach it to the person and use their blood supply to get all the required nutrients for growth.

As to your main question: we will absolutely be able to have organ farms. Take this [mouse](http://en.wikipedia.org/wiki/Vacanti_mouse) for example. A live host is - for now - the easiest way to do it but that most definitely does not have to be the case.",null,0,cdl8szw,1ra6bi,askscience,new,2
now_you_listen_here,"I'm going to start by explaining it very simply, and then if you want more details I can expand on it!



As far as the part about being non-reactive to CMV (cytomegalovirus), that simply means that you do not have antibodies to the virus.  This means that you have not been exposed to CMV before.  It doesn't mean that your blood ""type"" is non-reactive; we don't speak in terms of blood *types* being reactive or non-reactive to a virus.


As far as your O(-) status, I'm afraid she gave you some misinformation.  That does not mean that your blood is ""good"" for newborns.  It doesn't mean it is ""bad,"" either.  The (+) or (-), as you probably know, is referring to the presence or absence of Rh factor, which is a protein on the outside of red blood cells (you are either Rh+ or Rh-).  You DON'T have it.  Therefore, if your blood was exposed to blood that DID have it, your immune system could form antibodies against it, because it is recognized as something that is ""foreign"" to your body.


The importance of this would be if you would become impregnated by a male who is Rh(+).  In that case, the baby might inherit the gene from the father and also be Rh(+).  Therefore, if your blood (which is Rh-) was exposed to the baby's blood, whether during birth or some event during the pregnancy where bleeding occurred, your immune system could recognize the Rh factor on the baby's red blood cells and form antibodies, which would allow for an immune response at sometime in the future.  Your first baby would be just fine (the immune response takes too long that first time for it to put that baby in danger), but any babies you have after that are also Rh+ would be at risk of your immune system attacking their red blood cells, something we call [hemolytic disease of the newborn](http://en.wikipedia.org/wiki/Erythroblastosis_fetalis). 


There is good news, though!  We have developed something called Rhogam, which can prevent all this!  It's an injection that basically suppresses the mother's immune response to the Rh factor.  We can give it to pregnant women during their pregnancy and prevent the bad stuff that might have happened otherwise.  Science is great, isn't it?
",null,0,cdl8582,1ra6bd,askscience,new,6
abbe-normal1,"I'm going to expand on /u/now_you_listen_here since there are a few points I believe they didn't explain as well as could be.  CMV non-reactive as previously stated means you haven't been exposed to CMV.  The reason this is important is because CMV while causing a relatively minor infection in healthy adults is considerably more worrisome for immunocompromised individuals and babies.  See more information [here](http://www.mayoclinic.com/health/cmv/DS00938/DSECTION=symptoms).  The reason your blood is good for these individuals is that you can transmit the virus to them through a blood transfusion.  See [here](http://www.mayomedicallaboratories.com/test-catalog/Clinical+and+Interpretive/62067) again for more information.

As for O-, again as stated previously you lack the Rh factor that can cause an immune response in a recipient of a blood transfusion.  Your blood isn't just 'good for babies' (that's the CMV- part) you are actually known as the universal donor.  Your blood can be given to anyone regardless of blood type because you not only lack the Rh factor, but you also won't react with A or B blood because you being O lack those antigens as well.  Therefore your blood can be given to anyone A, B, AB or O without a life threatening reaction from their body.  In an emergency O- is given to a patient when there isn't time to check their blood type or until cross matched blood is available.  Also, others with your blood type can only receive O- so blood is harder to get because it is rarer.  

TLDR:  GIVE BLOOD!  O- blood is always in need and you would do a great service by regularly donating your blood to help others! ",null,0,cdl8f7o,1ra6bd,askscience,new,3
user31415926535,"""CMV non-reactive"" means that your body has never developed antibodies to cytomegalovirus (CMV) - that is, negative for both CMV-[IgM](http://en.wikipedia.org/wiki/Immunoglobulin_M) and CMV-[IgG](http://en.wikipedia.org/wiki/Immunoglobulin_G). A positive CMV-IgM test would mean you have a current CMV infection. A positive CMV-IgG test would mean that you had been infected some time in the past. What's important is that if you have been infected some time in the past, you still have some level of the virus in your body; a negative CMV-IgM test doesn't mean that you are entirely free of the virus. 

[CMV is an extremely common virus](http://www.cdc.gov/CMV/index.html) in humans; worldwide, 40% of adults have antibodies to CMV. [It's not particularly dangerous to healthy adults](http://www.mayoclinic.com/health/cmv/DS00938) (though it is implicated in some cancers and rare syndromes). If you notice an infection at all, it's usually similar to [mono](http://en.wikipedia.org/wiki/Infectious_mononucleosis). But to humans with undeveloped immune systems - newborn infants or immunocompromised people - it can be deadly. [In infants, CMV can cause blindness, deafness, neurological deficits, even death.](http://en.wikipedia.org/wiki/Congenital_cytomegalovirus_infection) 

So we need to be sure that blood used for infants has no trace of CMV in it. Hence, we look to people like you as a source of CMV-negative blood. 

[*Source: I'm not a medical professional, rather I'm an immunocompromised adult who has to know these things to survive.*]",null,0,cdl8jtj,1ra6bd,askscience,new,1
Ejb90,"The auroras depend on two main things:
1) Solar activity
2) latitude

Solar activity produces the solar wind - a stream of charged particles from the sun. These interact with the Earth's magnetic field, being drawn up along the field lines to where they intersect the Earth, at the poles. The interaction of these particles produces the lights. There are more particles when the activity is greatest. At the moment we are nearing a sunspot maximum, so the activity should be greater.
The Forster North/south the more field lines, so the more prominent the lights are.

The lights fluctuate daily, but there is a good chance of seeing them. Going in winter doesn't make too much of a difference, though it is darker for longer so they will be clearer for longer.",null,0,cdli1qs,1ra4dm,askscience,new,2
bohr_exciton,"That's actually a tricky question. The most direct explanation is that ice is slippery because it behaves as being wet, i.e. that there is liquid water between the bulk of the ice and an object gliding on the surface. However, what causes ice to be wet with respect to objects moving on it is still under debate. There was a Physics Today [article](http://scitation.aip.org/content/aip/magazine/physicstoday/article/58/12/10.1063/1.2169444) that came out a few years ago that neatly describes possible explanations. To briefly summarize them, they are as follows:

1) Pressure melting. Due to the fact that ice (at least the common form) is less dense than water, applying pressure reduces the melting point of the ice. To apply this example to say a skater, the idea is that a skate bay locally increase the pressure to a sufficient extent that the ice can melt, making it slippery locally and the water then refreezes after the skate passes. This is the most common explanation that has been invoked historically, but the problem is that it's not clear whether this additional pressure can be enough to melt the ice.

2) Frictional heating. When objects move across a surface there is (virtually) always some friction which results in local heating. Again, this heat may be enough to melt the ice.  This explanation, however, can't really explain why ice is slippery even for stationary objects immediately before moving. 

3) Ice may be intrinsically wet. In describing solids, we often tend to ignore surfaces to a first approximation, because this simplifies the description, but surfaces can behave very differently from the bulk. In the case of ice, it's been speculated that the different local environment of the first few layers of water molecules may result in this molecules being less strongly bound to the bulk than is the case for molecules within the body of this crystal. Because of this, these surface layers may behave as being liquid, which would then cause the ice to be slippery under all circumstances. ",null,0,cdl7ngd,1ra493,askscience,new,8
stillealles,"Because either 
a) a thin amount of water is on top (in the case of ice skating and such)
Or 
B) when you apply pressure to ice, it causes it to melt because of properties of water (this also applies to ice skating and such, but is easier to think about if you have a piece of ice on the floor and step on it and slip) 
Veritasium has a good video on it ",null,0,cdl8vq4,1ra493,askscience,new,1
jericho,"This is one of those great questions that results in arguments around the water cooler in the Physics Dept. Short answer; we don't know. [Here is a nytimes article that cover some bases.](https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;ved=0CFwQFjAH&amp;url=http%3A%2F%2Fwww.nytimes.com%2F2006%2F02%2F21%2Fscience%2F21ice.html&amp;ei=nPKQUt78OoXMqgHX2oGwCw&amp;usg=AFQjCNH9Hpt9VbWa5zPA0g36hCt64d71cQ&amp;bvm=bv.56988011,d.aWM)
",null,0,cdlac1j,1ra493,askscience,new,1
inmate992,"Self renewal is the ability of cells to continually replicate - preserving genetic information in all subsequent daughter cells. Stem cell reservoirs are usually tightly regulated in the human body to prevent them becoming cancerous.

As for the difference between Autologous and Embryonic, embryonic stem cells are much more pluripotent ie given the right external environment they can assume any cell type (eg cardiac, neurons, immune cells etc).
Autologous stem cells on the other hand are often precursor cells, for example OPC (oligodendrocyte precursor cells) cells are cells that are able to differentiate into immune cells, but their cellular fate is already decided, so I suppose they lose their pluripotency to an extent as their fate is already decided.

I hope this makes sense!",null,0,cdlgfhw,1ra41i,askscience,new,2
Osymandius,Could you clarify your question slightly? Partially differentiated haematopoietic stem cells in my hands happily divide lineally for 50 rounds of divisions in culture with no change in genotype or phenotype. eSCs are similarly capable. ,null,0,cdlayhh,1ra41i,askscience,new,1
NotFreeAdvice,"hmmm...well, it depends on the reaction that you are interested in.  Multiple things can happen.

1) You can just denature proteins at high/low pH.  It is *very* common to ""cook"" fish in acids (like lemon juice) or bases (such as lye).  These foods would be called ceviche or lutefisk, respectively.  
2) You can hydrolyze bonds that are holding in the flesh together, in which the acid or base acts as a catalyst for this reaction.  

There are probably more reactions of interest, but as a chemist, these are what immediately comes to mind.  ",null,0,cdl81pd,1r9zbg,askscience,new,3
Jyesss,"I think the question that you are getting at is ""what happens at the sub-cellular, or protein level, when a strong acid breaks down flesh?"" Proteins are made up of amino acids, and the way that these amino acids fold in 3-dimensional shape largely determines their function and appearance. Their shape depends on the stabilizing interactions between positive and negative charges on the amino acids, on hydrophobic interactions, and on hydrogen bonding. A strong acid increases the concentration of hydrogen ions and thus affects the protonation/deprotonation status of the side chains on the amino acids. The correct folding of the protein will be interrupted if a group that is normally deprotonated at physiological pH is changed to being protonated, thus resulting in the change in appearance. This same thing also happens when the temperature is increased, increasing the vibrational energy in the bonds which can eventually overcome the stabilizing forces at play. This is why an egg looks different when cooked. ",null,0,cdlf7c8,1r9zbg,askscience,new,1
iorgfeflkd,"You'd have to sum up the contribution from each height.

If you assume a constant density and cross section ( which isn't exactly true for a person, but you could make it more complicated if you want) would be the integral from r=6380 km to r=6580 km (or whatever) of GMdA/r^2 by dr where G is the gravitational constant, M is the mass of Earth, d is the density, A is the cross sectional area, and r is the radial position.

For heights much less than the diameter of the Earth, just using mg isn't too too wrong.",null,0,cdl94jj,1r9r7m,askscience,new,4
goingforth,"If we're going off of your theoretical proposition, the solution would be rather simple. If only half of the person's mass is influenced by gravity, then they would consequently have half their original weight. However, that concrete of a border doesn't exist. The above comments describe the mathematics of the issue, but the conclusion will be that, assuming the person's head is around low earth orbit, their weight will only change by a very small amount, as acceleration due to gravity doesn't change very appreciably from the surface to orbit (about 9.7 rather than 9.8) This effect will, of course, be amplified if the person's head is, say, on the moon, and will thus require definitive calculations in order to come to an accurate conclusion. ",null,0,cdlq4ba,1r9r7m,askscience,new,1
glittercheese,"People taking statins (cholesterol lowering medications) are often told to avoid grapefruit and grapefruit juice because consuming grapefruit can cause higher-than-expected levels of the drugs in the blood. Statins are metabolized in the liver by the same type if enzymes that metabolize grapefruit. If these enzymes are busy processing grapefruit, they are unable to metabolize the drugs, leading to more of the drug circulating in the body. This can cause an increase in adverse side effects of the drugs. 

I think the reason other fruits don't cause the same effects is that they are not metabolized by the same liver enzymes. 

This isn't overly scientific but I am a nurse and I wrote a short pamphlet on this subject when I was in nursing school. ",null,2,cdlao57,1r9qsz,askscience,new,8
justin3003,"Glittercheese gives a good basic overview of how the process works. However, to add, a substance called bergamottin and the related 6,7-dihydroxybergamottin are thought to be the culprits present within grapefruit specifically. 

These chemicals are potent inhibitors of CYP3A4, which is part of the cytochrome P450 family of enzymes. These enzymes are responsible for liver detoxification of substances in the blood (generally by making them more soluble for excretion in urine or bile). The exact mechanism isn't important, but what is important is that these guys are potent inhibitors of CYP3A4 and thus prevent CYP3A4 from detoxifying a whole host of important things, including a large number of pharmaceutical drugs. Given that many drugs have a fairly narrow therapeutic index (blood concentration range at which they are safe and have an effect), inhibiting their liver metabolism can cause blood levels to rise to toxic or even deadly levels quickly. Also, while they aren't the strongest inhibitors of CYP3A4, they are the most readily available, common, and seemingly innocuous and thus potentially dangerous.  

There is a good list of things CYP3A4 metabolizes at the bottom of this wikipedia article (under substrates): http://en.wikipedia.org/wiki/CYP3A4",null,0,cdli08l,1r9qsz,askscience,new,2
the_dan_man,... the same way any cultivated crop survives predation. Human intervention.,null,1,cdl45l4,1r9omj,askscience,new,2
the_dan_man,"http://en.wikipedia.org/wiki/Tunica_media

Muscle cells are arranged in a circular fashion around the vessel's interior. When these cells contract, the lumen (inside space) of the vessel grows smaller. It's a radial-ish contraction.",null,1,cdl4a5l,1r9ilc,askscience,new,2
frogdude2004,"A balloon is inflated when the pressure inside the balloon is greater than the outside: the air inside the balloon's walls stretches it while it pushes back. This continues until the combined force of the balloon and the air outside the balloon equals the force from the air inside the balloon. If the pressure inside the balloon equals the pressure of the outside of the balloon to start, then the balloon exerts no force: it is 'deflated'. If you were to put a balloon that was filled to a certain pressure into a box with the same pressure and then let the balloon open, the sum of the balloon and outside air force would be greater than the force inside, and it would then contract until deflated. I hope this answers your question. If it wasn't clear, please let me know.",null,0,cdl8ze2,1r9hf4,askscience,new,3
Naf623,"No, it cannot stay inflated without something blocking the neck. The rubber skin if the balloon will always be stretched and exerting a force to expel air. 
There is one exception; if the neck on the ballon is open to a source of gas, but the rest of the balloon is placed at a sufficiently lower pressure, then the balloon could be inflated by suction. ",null,0,cdlaggw,1r9hf4,askscience,new,2
iorgfeflkd,"Protons and neutrons are made of quarks, held together by gluons. Electrons and quarks, to the best of our knowledge, are fundamental, and so are neutrinos. With better experiments this understanding can change, but right now as far as we know, certain particles are fundamental.",null,2,cdkxbap,1r9g54,askscience,new,14
ekohfa,"Solar photovoltaic panels create current due to the [photoelectric effect](http://en.wikipedia.org/wiki/Photoelectric_effect).  Incoming photons cause electrons to jump across the energy ""band gap"" in a semiconductor, typically made of silicon.  The silicon consists of a positively doped layer and a negatively doped layer, just like a diode.  Metal leads are applied to each side to allow electrical current to flow to an external circuit.

Edit: To be clear, the metal leads are not oxidized; they are present only to carry the current created in the semiconductor.",null,2,cdkynzl,1r9es2,askscience,new,4
hal2k1,"&gt; Where are these electrons sourced from?

This is a misconception. Electrical current is charge flowing in a [circuit](http://en.wikipedia.org/wiki/Electrical_circuit).

&gt; An electrical circuit is a network consisting of a closed loop, giving a return path for the current.

A circuit is a [**loop** of conductor](http://en.wikipedia.org/wiki/File:Ohm%27s_Law_with_Voltage_source_TeX.svg). 

The carriers of charge are normally electrons (as your question suggests). The electrons are already part of the conductor. A current is merely a flow, or movement, of charge (electrons) around the loop of conductor (the circuit).

The solar panel merely converts photons into a ""push"" for the charge carriers (electrons), like a pump does for a fluid. The solar panel then ""pushes"" the electrons (which are already in the conductors) around the circuit. In electrical circuits, we do not call this pushing force ""pressure"", but rather we use the term ""voltage"".",null,0,cdl4dv3,1r9es2,askscience,new,1
jayd42,"My understanding of current is that the 'flow' is actually photons moving between electrons as the electrons change energy levels, gaining photons to raise in energy and losing photons to lower in energy, and not the physical movement of electrons through a material.

From this understanding, solar cells become very easy to understand from a non technical point of view. The cell gets hit with photons raising the energy level of the electrons in the cell and instead of reflecting the photons back as light the photons are redirected into an electric circuit as current.

I'm sure that's not 'exactly' right but close enough is good for me.
",null,0,cdl8979,1r9es2,askscience,new,1
iam_sancho2,"A solar cell is made of layers of different materials. The top layer will be some kind of anti-reflection surface. The next region is a thin layer of n-type material followed by a larger region of p-type material. The juxtaposition of these two differing materials creates an internal electric field. 

When a photon with sufficient energy enters the inner material, it generates electron-hole pairs, which are immediately swept to apart from each other by the internal electric field. With the electrons going one way and the holes going another, a photocurrent is generated within the solar cell. 


There is no net loss of electrons in the material. ",null,0,cdlif43,1r9es2,askscience,new,1
clever_cuttlefish,"I think you're mistaken about the current.

Electrical current is a flow of electrons, but no electrons are created or lost in the panel. The energy carried from the photons creates a voltage, which causes the electrons to move around, but none are created.

Think of it like this: All the electrons are happily bound to their atoms, but the photon can bump into one and knock it out of place. However, all the atoms and electrons this has happened to would much rather that another electron comes to fill it's place. So electrons move across the panel to fill in the gaps. This flow of electrons is the current, and is what we get the energy from.",null,3,cdl029v,1r9es2,askscience,new,3
Tacomelt,"I can give you a short run down.

Light emits photons, the photons are captured and piled up on the plate causing a voltage.  A current is then formed from the resulting voltage producing electricity.  The electricity is stored into a capacitor or battery system.

The electricity is a direct current, most systems go through an alternator changing the dc to ac.  It is now capable to power your home.

",null,4,cdkyc5n,1r9es2,askscience,new,2
RelativisticMechanic,"Note that when dealing with roots and complex numbers, you actually get multiple results. For example, -1^(1/2) actually has two values:

1. i
2. -i

Similarly, -1^(1/3) has three values:

1. -1
2. cos(π/3) + i\*sin(π/3)
3. cos(π/3) - i\*sin(π/3)

Finally, note that 1/2.5 = 2/5. Thus, (-1)^(1/2.5) = (-1)^(2/5) = 1^(1/5). We therefore need to find the fifth roots of 1. There are five of them, and they are

1. 1
2. cos(2π/5) + i\*sin(2π/5)
3. cos(2π/5) - i\*sin(2π/5)
4. cos(4π/5) + i\*sin(4π/5)
5. cos(4π/5) - i\*sin(4π/5)

Any of these numbers satisfies x^(5/2) = -1 (and x^(5/2) = 1), since they all satisfy x^5 = 1, and -1 satisfies -1^2 = 1.",null,44,cdkxquq,1r9elk,askscience,new,334
Tsien,"To elaborate on what RelativisticMechanic wrote, the roots come from Euler's formula: e^(ix) = cosx + isinx. If you're familiar with Taylor series, this formula can be derived from taking Taylor expansions around x = 0 for e^(ix), cosx, and sinx:  

e^(ix) = 1 + ix - x^2 /2! - ix^3 /3! + x^4 /4! + ix^5 /5! - ...  
cosx   = 1 - x^2 /2! + x^4 /4! - x^6 /6! + ...  
isinx   = ix - ix^3 /3! + ix^5 /5! - ix^7 /7! + ...  

So, using this formula, we know that e^i\*2kpi = 1 where k is an integer. So 1^(1/5) becomes

e^i\*2kpi/5 = cos(2kpi/5) + isin(2kpi/5)

Using k = 0, 1, -1, 2, -2 (in that order) gives RelativisticMechanic's answers. Notice that since cos and sin are 2-pi periodic, if you try to use other values of k, you'll end up getting one of the 5 answers already listed.",null,3,cdkyeo1,1r9elk,askscience,new,15
regnirps,"This should help you out: [Graph of (-1)^x in Wolfram Alpha.](http://www.wolframalpha.com/input/?i=%28-1%29%5Ex)  This graph show the real vs. imaginary parts of the powers of (-1).  Notice that (-1)^(1/2.5) is in the part of the graph with nonzero real *and* nonzero imaginary parts!

In general, I think your question has to do with the properties of the function f(x) = a^x, but I'm not exactly sure what explanation you want for ""why"" beyond the graph linked above.",null,5,cdkx2fk,1r9elk,askscience,new,17
SidusObscurus,"For this question, you have to compare an inverse function, as defined by convention, with the solutions to an equation involving a function that is not one-to-one (invertible).

In order to define a function, you need exactly one output for each input. For possible multivalued functions, we select one solution completely by convention (sqrt function, inverse trig functions, and many others).

Solving x^2 = -1 has two answers, +i and -i, both imaginary.
Solving x^3 = -1 has three answers, e^(i*pi/3), -1 = e^(i*pi/3 +2*pi/3), and e^(i*pi/3 +4*pi/3).

To define the inverse functions of these equations, we can only pick one of these answers, so we pick one answer entirely based on mathematical convention and general agreement. Typically we pick the real number solutions first if we can (ex. cube root), positive part solutions after this if we can (ex. sqrt), and if it is still multivalued pick the solution set that intersects 0 if we can (ex. inverse trig functions). This defines the inverse functions x^(1/2) and x^(1/3). If we cannot do any of those, it's less clear what we pick, and mathematicians will usually explicitly state the solution branch they are using for their math, so no one is confused.

What about x^(2.5)? Once you have defined all the integer rational roots and powers, the standard convention is to interpret x^(5/2) as either of the two equal expressions,
sqrt(x^5) = sqrt(x)^5.
There is no ambiguity in these expressions. We could also solve
x^(2/5) = -1, which would have multiple solutions. Our solutions would be
e^(i*pi/2.5 + 2*pi*k/2.5) for each integer k.
See [Roots of Unity](http://en.wikipedia.org/wiki/Root_of_unity) and [Euler's Identity](http://en.wikipedia.org/wiki/Euler%27s_Identity) for more information. In this case, we would only have 5 total solutions, due to the 5th root.

For irrational numbers, everything gets a little more complicated, as irrational powers of negative numbers aren't very well defined. But that's another issue entirely.

*Edit: Corrected a really silly mistake of mine.*",null,0,cdl8h2z,1r9elk,askscience,new,3
Borlaug,"When the exponent is a fraction, they want you to find the root of the coefficient using the denominator as the radical. 

For the first one, no known number when multipled by itself results to -1. In other words, the square root of -1 is imaginary. This imaginary is represented by the letter i. 

The second one is asking for the cube root of -1, which is -1. i. e. -1*-1 *-1=-1",null,3,cdl6se0,1r9elk,askscience,new,5
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,new,18
nairebis,"That's all in the BIOS (Basic Input/Output System), which is a program stored in a special chip on the motherboard. That provides a standardized interface between the hardware and the operating system, which is why you can load Windows on any Brand-X motherboard. OS/X has the same concept, though of course Apple only officially supports certain motherboards.

How much power it uses would be dependent on the motherboard design, but if it uses electronic starting, then it uses some small amount of power to monitor the button. I believe all modern motherboards do it this way. In the relatively distant past, they used to use a mechanical on-off switch, but there were a lot of advantages to electronic power on/off, so that became standard.

Whether you can use a different key, etc, is whether that feature is programmed into the BIOS. Here is a page that describes typical BIOS settings:

http://www.tomshardware.com/reviews/bios-beginners,1126-8.html

BIOS settings are usually accessed by pressing a special key when you start up the machine, but before it starts loading the operating system. I've typically seen the DEL key, the F2 key and the F8 key. I don't know how Apple does it. If it works like how Apple does other things, then they picked some arbitrary key that no one else uses. :)

Edit: As haikuginger pointed out, the more modern version of BIOS is EFI, which allows changing some settings from within the Operation System (particularly in the case of Macs). But the principle is the same, the controlling program here comes from the motherboard firmware.",null,0,cdlach2,1r9c6s,askscience,new,4
Daegs,"no one says it ""must"" be. 

There are 4 forces: strong, weak, electromagnetic and gravity.

The first 3 are all very similar, and in fact the weak / electromagnetic have been joined into a single force, the electro-weak. It *seems* that the strong also fits in very well mathematically, and it is expected in the coming years / decades that we could actually integrate all 3 (strong, weak, electric) into a single force. This is called ""Grand Unification"":

http://en.wikipedia.org/wiki/Grand_Unified_Theory

So the problem is asking why are 3 out of 4 of the forces so similar to the point where they can actually be shown to be the same underlying force, while gravity is such an oddball and so much weaker. 

One *possible* explanation is that all 4 of the forces are equal in strength, and could be unified into a single force, while gravity is special because it ""leaks"" its force to an unseen dimension. This would allow it, in theory, to be unified with the other 3 forces.

There are other explanations, including that gravity is an emergent property of our universe (such as holographic universe theory) rather than a fundamental force.

Or, it could simply be that gravity is fundamentally different from the other 3 forces, which would just be troubling for physicist.

Without an explanation such as extra dimensions or more exotic behavior we haven't seen, there isn't going to be a way to unify gravity with the other 3 forces.",null,4,cdkxnpf,1r9c24,askscience,new,17
iorgfeflkd,"I'm not sure I understand your question. Did your teacher say that gravity needs to be as strong as the others, despite not being?

It is thought that at extremely short distances, there is so much energy from interactions of other forces (consider two electrons very close together for example) that the energy itself starts to gravitate. At these scales (typically, Planck scales), gravity would overwhelm other interactions. However, we don't have any way of testing this.",null,0,cdkwv4t,1r9c24,askscience,new,2
theoreoman,"Gravity is a very weak force and  it takes an entire planet to counteract an electrical charge on a small object.  This thing is no one knows why exactly why yet, there are theories but one of the holy grails of physics it to figure out how gravity relates mathematically to the other forces ",null,1,cdkwzlp,1r9c24,askscience,new,2
OverlordQuasar,"Essentially, it seems out of place. We have 2 forces that we have demonstrated via experimentation to be part of one underlying force (electromagnetism and the weak force becoming the electroweak force), another that has been mathematically shown to be part of that, which is awaiting experimental confirmation which requires a bit higher energy particle accelerators (strong force). All these are part of one thing, but gravity isn't. It is so much weaker, and refuses all efforts to unify it with the others mathematically without resorting to things on the level of higher dimensions.

TL;DR It's out of place to have 3 forces that are easy to unify and one that is much weaker and appears to be completely seperate.",null,0,cdl2y71,1r9c24,askscience,new,1
dudley_love,"Great question, putting the finger on the limit of the SF theory. 

One variation of the SF is [Yanagida](http://www.ncbi.nlm.nih.gov/pubmed/2082730)'s, where the actin-myosin crossbridge is not a solid-ish state, but a ""loose"", transient one. 

So instead of a ladder with strong actin ""hands"", you have a ratchet with potential for slippage.",null,0,cdmjgkh,1r9a7m,askscience,new,4
KarlOskar12,"[Eccentric contraction](http://medical-dictionary.thefreedictionary.com/eccentric+contraction) happens when you apply force to the muscle through its range of motion while the muscles contractile force is less than the applied for causing the muscle to stretch. The farther you stretch  the [muscle](http://puu.sh/5qXWU.jpg), the less myosin heads will be able to bind to the thick filaments and the amount of force the muscle can produce rapidly decreases as the stretch progresses.

This [article](http://muscle.ucsd.edu/musintro/contractions.shtml) explains it pretty well.",null,2,cdl90c6,1r9a7m,askscience,new,2
sporclesam,"If you mean [Sulphate-reducing bacteria](http://en.wikipedia.org/wiki/Sulfate-reducing_bacteria) (which use sulphate and not sulphur as terminal acceptor) then, yes, but not like us but more like plants/algae. These anaerobes use dissimilatory sulfate reduction to obtain sulphide as waste (somewhat similar to plants releasing oxygen from water)

Check out links on [chemosynthesis] (http://en.wikipedia.org/wiki/Chemosynthesis) *vis-a-vis* photosynthesis. ",null,0,cdl3l9h,1r99pm,askscience,new,2
Platypuskeeper,"Not really, they reduce SOx to H2S, while we reduce O2 to H2O. H2S doesn't perform any functions beyond that, while we use water for quite a lot of other things. Sulfur-reducing bacteria still use water for all those other things.

It's also a very different enzyme and mechanism. 
",null,1,cdl64y4,1r99pm,askscience,new,2
Updatebjarni,"Consider the fact that not only are the electrons in the extension cord running back and forth since the cord carries AC, but it's also running down one conductor while it is running up the other inside the cord. So regardless of exactly how electrons behave with respect to gravity in a conductor, that would seem to answer the question.

Same with the battery; there is one conductor carrying electrons from the negative terminal of the battery to ground, and one carrying them from ground to the positive terminal.
",null,1,cdl15ac,1r933v,askscience,new,12
iorgfeflkd,The voltage required to move an electron through a gravitational field is very small but nonzero. It is about 5x10^-11 Volts/meter.,null,2,cdl1mfs,1r933v,askscience,new,10
Farnswirth,"Heat engines require a heat differential to function, by definition.  

However, what /u/JimmyGroove said was correct as well, and is merely another way of stating the first law, which is:  The change in total internal energy in a system (in this case an engine) is equal to the heat in, minus the heat out, minus the work done by the engine.  This is one of the most fundamental principles of thermodynamics, [the first law.](http://en.wikipedia.org/wiki/First_law_of_thermodynamics)

For instance, you could have a very energetic system with no heat differential (eg. a cold car with a full tank of gasoline).  In this case, the heat inside the system (the car) would be at thermal equilibrium with the outside air because you haven't started the engine yet.  In this case, there would be no heat differential in the beginning.  The net reaction for the car as a system would be a conversion of internal energy (the chemical energy of the gasoline) into work.  *It's all a question of how you define your system.*  

So to answer your question - a heat engine requires a heat differential, by definition, because that's how a heat engine works.  But depending on how you define your ""engine"" (thermodynamic system) - you don't necessarily need a heat differential- for work to be produced.  But the energy must come from somewhere, and it must have somewhere to go.

Edit: Another good example of a system that does not require a heat differential to do work is a fuel cell, which directly converts chemical energy into electrical energy with no heat differential.  Obviously this isn't a heat engine, but you could definite it as an ""engine"", and it is used as such in some vehicles. ",null,1,cdkwnan,1r90aa,askscience,new,8
JimmyGroove,"You only get a net flow of heat from areas of high temperature to low, so a heat engine could only work off ambient heat if the ambient temperature was different from that of the engine (or can be manipulated into being different, the most commonly used way being to change the pressure of a gas).",null,0,cdkt7bu,1r90aa,askscience,new,3
theoreoman,"Law of thermodynamics says that heat can only flow from hot to cold, so for work to happen the heat needs to flow from hot to cold, the larger the differential the more work that can be done.  Quantum mechanics might have a different answer that I'm not aware of",null,0,cdkwkpo,1r90aa,askscience,new,1
Jonex,"Not really answering your question - as it's already answered - but an interesting addition:
You can extract ambient heat energy without having a heat differential - if you add more energy than you extract. This is used in earth heating systems where energy is taken from the ground at a few degres to add to the indoor heating at around 20 degrees.

To to this you need an heat pump, powered by for instance elecricity. There are physical limitations of efficiancy. But it's a popular way to increase the efficiancy of electricaly powered heating.

I'm a bit tired so not an awesome explanation, someone who has done their thermodynamics more recently can hopefully expand and clear things up a bit.",null,0,cdl4g2a,1r90aa,askscience,new,1
oxymoron1629,"The Sickle Cell gene is incompletely dominant with the wild type gene. That means that both will be expressed and will create an intermediate phenotype. One who is heterozygous for the Sickle Cell gene is considered to express the Sickle Cell trait, but is not suffering from Sickle Cell Anemia. A heterozygous person will have some normal, some sickle shapes and some in between. Since a red blood cell had many hemoglobin molecules, when both genes are expressed each red blood cell gets some proportion wild type hemoglobin and some mutant ones. The amount of the mutant ones determines how bent the red blood cell will look. 

Tl;dr Not exactly a 50/50 split, but some completely normal, some completely sickle shaped, and many in between. ",null,2,cdkstzy,1r8w0j,askscience,new,6
meaningless_name,"&gt;And how does this help fight against the malaria plasmodium?

[A relevant paper on this topic](http://www.nature.com/news/sickle-cell-mystery-solved-1.9342)

Basically plasmodium, after infecting a red blood cell as part of its life cycle, hijacks RBC actin (a naturally occurring cell-strucure protein), to help it transport a protein of its own (adherin) to the RBC surface to make it stick to surfaces and other RBCs, which helps the plasmodium.

For sickle cell individuals, the mutant RBC actin can polymerize into long, stiff ""rods"" that distort the shape of the RBC and make it ""clog"" in capillaries. 

However, the plasmodium cannot effectively ""hijack"" the mutant RBC actin, which is the source of sickle-cell mediated malaria resistance.

Sickle cell homozygotes have mostly sickle-cell RBCs, which causes the clinical symptoms of sickle cell anemia.

Sickle cell heterozygotes have a mixture of sickle cell and non-sickle cell RBCs (which is not quite 50/50, as oxymoron 1629 explained), meaning they retain the malaria resistance while avoiding the worst of the anemia.",null,2,cdkuzei,1r8w0j,askscience,new,5
pravl,"No, they aren't.  The predominant form of hemoglobin in normal individuals, HbA, consists of two alpha-globin chains and two beta-globin chains.  People with sickle cell disease have only mutated beta-globin, which pairs with alpha-globin to form HbS.  The mutated beta-globin chains pair to alpha-globin with less affinity than normal beta-globin, so individuals who have both normal and mutated beta-globin, i.e. people with sickle cell trait, end up having slightly more HbA than HbS.  Usually around 60% HbA, 40% HbS.  And that is the hemoglobin itself, not the percentage of actual sickled red blood cells.  If not otherwise sick/stressed, people with sickle trait usually have no or very few sickled cells on a blood smear.",null,0,cdl1ovy,1r8w0j,askscience,new,2
mutatron,"Past performance is no guarantee of future results.

Overall fertility has been declining for a long time, and is expected to continue to decline. The figure of 10 billion is arrived at by looking at this decline in fertility.

Replacement rate fertility is 2.1 children per woman, or 21 children for every 10 women. Many countries, mostly developed ones, are now below replacement rate, some as low as 1.3 children per woman. Mexico has dropped from around 4 children per woman to 2.3 in just a couple of decades, and other less developed countries are expected to follow suit as poverty declines, healthcare improves, and education becomes ubiquitous, especially among women. Studies have shown that the most effective deterrent to fertility is the education of women.",null,22,cdkrdzc,1r8tbm,askscience,new,78
4698458973,"Look for a torrent of a video by Hans Rosling called, [Don't panic: the truth about population](http://www.bbc.co.uk/programmes/b03h8r1j). He is a fairly famous statistician, and he's a great speaker.

The short answer is that birth rates have fallen worldwide, in some places like they fell off a cliff. Population growth is still happening because we have at least fifty years left of people getting older: fifty years ago, women were having more babies than women are now, and those babies are going to be around for a while -- and, themselves, have babies. But the next generation will be barely at replacement level for most of the world.",null,3,cdkys9c,1r8tbm,askscience,new,18
AshRandom,"It's only predicted to stabilize at 10 billion by the most conservative of estimates. Dr. Michio Kaku gives a lecture where he talks of the coming super abundance of the future. Should the fusion power plant designed by [ITER](http://en.wikipedia.org/wiki/ITER) successfully create a [Tokamak](http://en.wikipedia.org/wiki/Tokamak) reactor capable of converting plasma directly into electricity (without all the fuss of boiling water and turning a big turbine steam engine) the cost of electricity would plummet. Not only would the cost be many thousands of times lower, the amount of energy available would be many thousands of times greater. 

Cheap and abundant electrical energy combined with modern desalination water processing would turn every bay and every inch of oceanic coastline into a fresh water river. Vast amounts of currently unfarmable land would become viable. Additionally, the ability to build, light, and water indoor hydroponics farms would be possible at tremendously reduced costs. This would make skyscraper greenhouse projects highly profitable, where they are currently cost prohibitive. 

Clean energy, fresh water, and a super abundance of food would have obvious consequences on the expansion of future populations. And should the vast uninhabited stretches of the world's surface become utterly filled with cityscapes, moving underground would then further magnify the square footage available for hydroponic farms and human habitation. Pushing Earth's capacity for human population into the trillions would not be unthinkable. 

Partial Source: *Dr. Michio Kaku N.Y.U. Institute for Advanced Study*
",null,6,cdl6b92,1r8tbm,askscience,new,9
OctoRock33,"As countries progress through development they go through multiple stages of birth, industry, and death. In the final progression of development Stage 4. The birth rate stays at or below the death rate, which could lead to either a stable population or a slowly decreasing population. 
Source: I took a college course on Human Geography
",null,0,cdl6pyd,1r8tbm,askscience,new,2
Zedred,"Predictions calculate there is a tipping point at which insufficient arible land and water exists to sustain the population, factoring in the amount of land and water required produce enough food to sustain each human and the livestock they require.  That number stands at about 24 or 25 acres per human given today's technology and weather models.

Theoretically at a population of somewhere between 4 and 16 billion ( average 10 billion) every single acre would have to be in use for farming  or housing given today's technology and water availability and assumes air quality is sufficient.  Before that limit is reached, population grosth would start to decline due to econonimic factors.  UN calculations no doubt take all these factors into account.

http://en.wikipedia.org/wiki/Human_overpopulation

 Global warming, nuclear disaster, war, or unforseen causes of crop failure could result in less food due to less farmable land, more sea water, and less fresh water, further constraining population growth.  Now facter in the potential for a plague disaster along with antibiotic resistence in a large population, and natural changes in fertility rates due to pollution or declining economic conditions that prevent access to birth control. 

It is not hard to understand that a planet with finite limits on the resources required to sustain human life will sustain only a finite maximum of people.  Technology  improvements and wiser land/water/air use could raise that maximum number, but it will stabilize again at the higher number unless additional resources are introduced into the food and energy production  system from other planets. 

This upper limit on population on earth is why space exploration is so critical to to the future of humanity.   Lets also hope China is smart enough not to launch a population  boom by lifting their one child policy amidst their current economic expansion, lest we reach that upper limit faster than technology can solve the problems that would cause.",null,1,cdl73zt,1r8tbm,askscience,new,2
TheMuslinCrow,"Each species has a population threshold for the environment it's in, known as the carrying capacity (K). This is determined by species requirements, as well as the environmental constraints on these requirements (food, space, etc). When a species population goes above this level, there's usually a setback in the form of disease or famine, and the resultant deaths bring the population back near the carrying capacity.

In the case of humans, we are able to artificially increase the size of our K through technology, medicine, urban planning and such. However, this planet has finite resources and space. So what is the carrying capacity for humans on Earth? We really won't know until we reach it. Some places such, as Japan and parts of Europe, seem to be nearing their K for their respective environments.

Source: I'm a zoologist.

EDIT: Am I being downvoted for providing a biology based answer about population, because this is an earth sciences subreddit? There's too much segregation in science, and that holds us back.",null,6,cdl76cg,1r8tbm,askscience,new,6
Filipinolurve,"Hans Rosling: Religions and babies

https://www.google.com/search?q=ted+talks+hans+rosling+religion+and+babies&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;client=safari

OP I tried to copy and paste the link above (I'm on my phone) anyways this stats guy on TED talks explains how the population trend will go and why, the video starts off talking about if religion and does it affect the population growth but it'll def answer your question.",null,2,cdl6cjs,1r8tbm,askscience,new,2
arumbar,"Maternal and fetal blood do not normally mix.  Maternal blood is pumped through the [maternal blood vessels in the placenta](http://www.biog1445.org/media/placenta.jpg), where nutrients and oxygen are allowed to diffuse into the fetal bloodstream due to the close proximity of fetal vessels.  There are a number of [fetal anatomic features](http://img.docstoccdn.com/thumb/orig/107478990.png) that work together to make this system work, as the fetus will be obtaining oxygen from the mother rather than its lungs.  The expansion in maternal blood volume and cardiac output is simply a consequence of having to perfuse not only her normal organs but also divert a significant amount of bloodflow to perfuse the uterus and placenta, which then feed into fetal circulation - but they do remain separate.

Typically having a baby with a different blood type is not an issue.  However, a Rh negative mother (eg O-, AB-, etc) can become sensitized to the Rh antigen if her fetus is Rh positive, resulting in complications for future pregnancies.  This usually occurs as tiny volumes of fetal blood (&lt;0.1mL) enter maternal circulation and trigger formation of anti-Rh IgG antibiodies, which can then cross the placental barrier in subsequent pregnancies and cause hemolysis (destruction of fetal red blood cells).  ABO alloimmunization is less commonly an issue - for a variety of reasons the antibodies associated with Rh factor are more prone to cross the placenta and cause disease (they are IgG rather than IgM, and fetal rbcs express more of the Rh antigen).  There are a few other high-risk antibodies associated with fetal hemolysis (eg anti-Kell).",null,3,cdkqj3h,1r8r2g,askscience,new,29
cryptorchidism,"There is a [maternal-fetal barrier](https://en.wikipedia.org/wiki/Placenta#Placental_circulation) in the placenta, similar in function to the [blood-brain barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier) (or for that matter the [blood-testis barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93testis_barrier)/blood follicle barrier). It prevents large molecules from passing, like pathogens, immune cells, and blood antigens, the last of which determine blood type.

^(Thanks, now ""blood"" looks funny.)",null,3,cdkpyc2,1r8r2g,askscience,new,14
NassT,"No, mothers and babies have separate circulatory systems, but the mother's blood does have to carry all of the oxygen, nutrients, etc. for the baby as well.  Pregnant women also tend to put on weight in addition to that of the baby, which also stresses the heart.",null,2,cdkq3u4,1r8r2g,askscience,new,5
Naf623,"No; the mother's heart pumps blood through the placenta, which makes for a small increase in where it needs to be pumped. The baby has it's own completely separate blood circulatory system which also goes through the placenta. 
In the placenta the blood vessels get very small, thin walled &amp; close so that nutrients can be transferred between them. 
I'm very surprised by the 30-50% increase figure; ",null,0,cdkq3hb,1r8r2g,askscience,new,3
TangentialThreat,"Yes. Well, [sort of](http://www.the-scientist.com/?articles.view/articleNo/32997/title/Electrical-Bacteria/).

*Desulfobulbaceae* are forming living wires that connect electron-rich upper sediment with the electron-poor deeper sediment. It is taking advantage of a natural electrochemical potential - in other words, a living wire that's feeding off a battery. Tell me that's not cool. [Study](http://www.nature.com/nature/journal/v491/n7423/full/nature11586.html)

Injecting synthetic ATP into your arm is probably a bad idea. There is never much free ATP in the body except after a major injury. This may cause several body systems to freak out, [including your heart](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3265710/).

",null,1,cdkw2ds,1r8phn,askscience,new,10
JimmyGroove,"I can think of no theoretical reason why such a bacteria couldn't exist.  The practical reason for why one doesn't exist already is that free charge differentials aren't very common in the natural world, don't tend to last terribly long, and don't occur over distances that would allow a bacteria to travel from one to another when their first electrical source evens out.",null,0,cdktc9s,1r8phn,askscience,new,2
JeremyJBarr,"Actually there is a whole scientific field called ""Microbial Electrochemisty"" utilizing this phenomenon! I spent a little part of my PhD working on a microbial fuel cell (MFC) where the basic premise is that you feed a microbial community a waste product in an enclosed reactor operating as an anode, that is connected to a separate reactor which operates as the  cathode. [Google images picture](http://www.technologyreview.com/sites/default/files/legacy/hydrogen_x600.jpg)

In these systems waste products are used as the food source for mixed microbial communities in the anode reactor. These bacterial communities degrade the organic components present in the waste product, while utilizing the anode as an electron sink, and dump protons into the surrounding media. The electrons are then sent via a conductor across to a separate cathode reactor generating current! The protons generate in the anode then flow across to the cathode via a specific membrane that permits their transport, while keeping the microbial communities separate. 

Microbial fuel cells (MFC) were once suggested as a renewable source of electricity from waste products (typically wastewater). However, current MFC designs do not produce sufficient electrical current to make them sustainable. But a recent suggested use for them has been to pump electrons into the MFC, essentially feeding the microbial communities electrons, and force them to produce bioproducts of extreme value. There is lots of research going on in this field investigating novel MFC designs to generate more current, and the formation of high-value bioproducts from wastewater.

Some sources [1](http://www.sciencemag.org/content/337/6095/686.full) [2](http://link.springer.com/article/10.1023/A:1025484009367) [3](http://www.nature.com/ismej/journal/v1/n1/full/ismej20074a.html)",null,1,cdlc8gu,1r8phn,askscience,new,3
temuchan,"Nucleotides can be synthesized ""de novo"" from precursor molecules (obtained from the breakdown of food, for example).  The major organ involved in this process is the liver.  However, nucleotides can also be [recycled](http://en.wikipedia.org/wiki/Nucleotide_salvage) through a process that synthesizes nucleotides from the components of degraded nucleotides.",null,1,cdkqro4,1r8p24,askscience,new,6
sphenopalatine,"The new nucleotides are synthesized from a large number of other precursors, such as folic acid, glutamine, glycine, etc. The method of synthesis differs between purines (A and G) and pyrimidines (T and C). 

The purine synthesis pathway is [quite long](http://gallus.reactome.org/figures/denovo_IMP_synthesis.jpg), but can be summed up as resulting in the end product inosine monophosphate (IMP). This can be interconverted to GMP or AMP. Two more phosphate groups are added on to give the triphosphate tail of a nucleotide. These ribonucleotides (NTPs) are then converted to deoxyribonucleotides (dNTPs) using [Ribonucleotide Reductase](https://en.wikipedia.org/wiki/Ribonucleotide_reductase) and a dNTP is born.

The pyrimidine synthesis pathway is of [similar length](http://gallus.reactome.org/figures/denovo_UMP_synthesis.jpg) and gives uridine monophosphate (UMP), which is then converted to UTP (used in RNA synthesis). UTP can be interconverted with CTP and TTP. Ribonucleotide Reductase once again converts the NTPs into dNTPs.",null,0,cdkriwp,1r8p24,askscience,new,3
oxymoron1629,"The same mechanism that creates energy from your food has a built in arm that takes the energy in food and instead of creating energy for later use, it uses the energy from food to create the bases needed for DNA replication. But it only does this when the cell decides to replicate so most of the time, it just stores energy from food. ",null,0,cdksxgh,1r8p24,askscience,new,2
Pallidium,"Yes. This would not be innate though, and would result from the person learning to identify themselves. The ""higher"" brain regions, such as the prefrontal and parietal cortices, would register it as ""me"" or ""my face,"" and in turn use this to alter activity in ""lower"" regions. There probably would not inherently be any any mechanism in the visual cortex or fusiform gyrus (a brain region heavily implicated in facial recognition) for a person identifying their own faces, but these regions activity would be modulated by prefrontal and parietal input, which could lead to different activity viewing oneself as compared to others faces. [Here is a study](http://www.ncbi.nlm.nih.gov/pubmed/18656465) which shows difference between self-recognition and other-recognition, and shows fusiform involvement. I'd like to restate that this is NOT an innate ability of the fusiform and probably results from modulation by higher brain centers. The related citations section of that pubmed abstract (right hand side) has some other abstracts about the neural correlates of self-recognition.",null,2,cdkydec,1r8hem,askscience,new,11
iorgfeflkd,"More energetic photons are actually more likely to interact with materials (above a certain threshold), because it increases the likelihood of pair production.

[This is essentially a graph of how likely a photon is to interact, vs energy](http://en.wikipedia.org/wiki/File:Attenuation_Coefficient_Iron.svg).",null,4,cdkoadv,1r8gmf,askscience,new,11
StarshipEngineer,"Conservation of energy prohibits a photon from interacting with vacuum itself, and so pair production is restricted to the vicinity of relatively heavy atoms. Therefore, interaction with vacuum and an electron-positron cascade is probably not something that can occur; if it could, such an event would likely have spread across the observable universe long ago. (Unless, of course, photons of such energy are exceedingly rare. However, even one photon would be enough to trigger such a cascade, so it is unlikely that it is possible at all.)

It is true that, from the perspective of particle physics, even a pure vacuum is filled with ""virtual"" particles born of quantum fluctuations. What makes them virtual however is that they appear and disappear in such a short time that their brief existence does not violate the uncertainty principle, and under ordinary circumstances, they are not directly detectable. If a photon were to interact with such a particle, it would have to reduce the energy of the photon at least enough to account for the particle's mass-energy, in order to obey conservation of energy. This would prohibit such a runaway cascade.",null,0,cdkq03p,1r8gmf,askscience,new,2
D0ct0rJ,"I think a photon of sufficient energy will interact with free space strongly enough to prefer to spew out electrons and positrons, which will radiate as they experience forces in material, leading to more high energy photons, and so on: an electron-positron cascade. Gamma rays interact with heavy metals due to pair production, or so the current theory goes.",null,3,cdknpx7,1r8gmf,askscience,new,2
TheCrazyOrange,"Because sound is just compression waves traveling through the air, and our ears are highly sensitive to the frequencies in the range of human voices.

But when you blow air, your attempting to physically move the air with enough velocity that it will reach the person. Contrary to what it seems, air has mass, and thus the air separating you can diffuse and block the air you blow our.",null,2,cdkn173,1r8gcu,askscience,new,9
selfification,You can't throw a handfull of water that far without a lot of effort but you can create a small splash on a pond and have the ripples travel for a long long distance.,null,3,cdkoctf,1r8gcu,askscience,new,5
therationalpi,"I feel like you haven't gotten a satisfying answer yet, so I'm gonna try my hand at this.

To start, let's imagine what the air is like at rest. All of the air particles are bouncing around randomly, but *for the most part* they are all bouncing around at equilibrium. All of the air particles are where they want to be.

Now, let's say I blow on the air. I'm introducing a slug of fast moving air, with a good deal of kinetic energy. That air has not found it's equilibrium point, so it's going to carry forward. In the process, the air is slamming against the air around it that was formerly at equilibrium, imparting kinetic energy, and carrying it forward. A moment after the air first comes in, you now have a larger slug of slower moving air all trying to find an equilibrium position. The process then repeats, with more and more air going slower and slower. Eventually, the air stops and finds its place.

Now let's look at sound. For sound, you have the same transfer of kinetic energy when the molecules slam into eachother. The big difference is that after they slam into the next group, they *bounce back* and end up right where they started.

As a result of going back to the old equilibrium position, none (or at least very little) of the kinetic energy ends up permanently stored in potential energy. When you blow, part of the energy goes towards moving the next cluster of air molecules, and part of it goes towards choosing a new equilibrium position. For a sound wave, pretty much all of the energy is able to keep moving forward. Look at the first figure on [this page](http://www.acs.psu.edu/drussell/Demos/waves/wavemotion.html), paying careful attention to how the red dot isn't really going anywhere.

Hopefully that answers your questions, but if you would like any clarification please don't hesitate to ask!",null,0,cdkus3i,1r8gcu,askscience,new,2
Bbrhuft,"Tsunamis are caused when the sea floor is displaced up or down within just a few seconds or minutes, this is typically caused by a megathrust earthquake, where oceanic plate is forced under continental plate, creating a violent earthquake and sea floor displacement. 

In the largest megathrust earthquakes, 10,000 to 100,000 km2 or more of sea floor can be displaced upwards several metres in only a few minutes, displacing vast quantities of ocean water. This displaced water radiates away and eventually hits land as a tsunami.

Mud volcanoes, on the other hand, are much smaller and happen over a much longer time frame, days to even years. When the mud is erupted to the surface, the surrounding land surface or ocean bottom may subside slightly. But none of these changes are ever on a large enough scale or fast enough to create a tsunami.",null,0,cdkpc7x,1r8g1s,askscience,new,6
GreenAdept,"Landslides and impacts can actually cause what is know as a Megatsunami, with potentially greater impact than the typical earthquake tsunami. I'm not very familiar  with mud volcanoes (last I knew they were fairly poorly understood) but it's reasonable to surmise if one were to ""erupt"" in an area that could cause a massive landslide it could be responsible. http://en.wikipedia.org/wiki/Megatsunami",null,0,cdkxyky,1r8g1s,askscience,new,5
Daegs,"Sharpening is basically scraping off material until it resembles the shape you want (in this case, a thin edge).

To start sharpening, a large grit is used, which translates to very deep ""trenches"" and irregularities in the resulting material, these are refined with finer grits to even these trenches (scratches) out. Doing this progressively leads to a smoother finish which approximates the shape you want (again, a low angle straight edge).

",null,1,cdkp9xn,1r8fth,askscience,new,6
robged,"Before I answer the question, I need to describe a speakers construction: an electromagnetic coil produces magnetic fields when electricity is applied. A rare earth magnet attached to a flexible cone moves in response to this magnetic field. The flexible cone compresses air generating sound. Bass is low frequency sound which is generally between 32 Hz at 512 Hz. The overall range of human hearing is between 20 Hz and 20,000 Hz. 

Speakers are less efficient at producing low frequency sounds than high frequency sounds, as during the low frequency cycle, air has time to equalibriate. To get around this inefficiency, it is possible to build the speaker larger and having it travel farther. Fortunately, it only needs to vibrate at lower frequency than high frequency speakers, so this is possible. Also, note that not only can high frequency speakers be smaller, but they also *must* be smaller and have less travel, because if they were as big as bass speakers the high amplitude combined with high travel would rip them apart.

Now, knowing that, it becomes clear why there is such a thing as a crossover circuit in good quality speakers. A crossover circuit is a low-pass filter before the bass speaker, and a high-pass filter before the treble, such that the small high frequency speaker doesn't get the large amplitude bass signals which will cause it to rip, and the low frequency speaker doesn't get the high frequency signals which will make a distorted sound as it is too large to respond to the high frequency signals.

Sound quality is *defined* as the ability of a system to produce an even response across all frequencies, at high volume, without distortion. If you like more bass, on a good system, you can always use an equalizer to add more bass without distortion.

EDIT: The energy of sound waves is independent of frequency (I get confused with light) so I had to delete a sentence.

TL;DR? The big speakers are for bass, medium speakers for mid range, the small speakers are for treble.",null,1,cdkm3ga,1r8c16,askscience,new,7
JoolNoret,"Sea level is the average level of the water in the ocean between tides.

The Netherlands is below sea level (hence the name). So is New Orleans, which resulted in flooding when the levees broke during Katrina.

Just because an area is below sea level doesn't mean the water will climb over a hill to fill it up.",null,2,cdklqri,1r8bxo,askscience,new,7
Astrokiwi,"That's exactly it: you can't! This is actually a big part of the theory of relativity: there is no universal concept of ""not moving"". You can say things are moving or not moving relative to something (as you mentioned), but to simply say that something is ""not moving"" does not actually make sense.",null,1,cdklc3q,1r8bbn,askscience,new,7
Naf623,"We can't tell if something is absolutely moving; only if it's position is changing relative to us. So we still wouldn't know for sure if we're both moving or just one, or which. 
One method is red shift if light based on the Doppler effect. Similar to how sounds are distorted as an object moves (sirens coming closer then further away) so is the light from/reflected off it. If the light is more red then the object is getting further away; more blue ; it's approaching. ",null,0,cdklyi0,1r8bbn,askscience,new,1
StringOfLights,"Crocodylians, which include alligators and crocodiles, are not dinosaurs. They are the closest living relatives of dinosaurs, however (because birds *are* theropod dinosaurs).

Dinosauria is a group that was originally defined by anatomist [Richard Owen](http://www.nhm.ac.uk/nature-online/science-of-natural-history/biographies/richard-owen/) based on a few described taxa, including [*Iguanodon*](http://en.wikipedia.org/wiki/Iguanodon) and [*Megalosaurus*](http://en.wikipedia.org/wiki/Megalosaurus). There are a few more technical ways to define the group, but no matter what it falls out being comprised of two smaller groups: [Ornithischia](http://en.wikipedia.org/wiki/Ornithischia) and [Saurischia](http://en.wikipedia.org/wiki/Saurischia), although these groups were not recognized at the time. Ornithischia includes dinosaurs like *Triceratops*, *Iguanodon*, and ankylosaurs. Saurischia includes sauropods and theropods.

Crocodylians, dinosaurs, and a couple other groups (including pterosaurs) are [archosaurs](http://archosaurmusings.wordpress.com/what-are-archosaurs/) (side note: people often refer to pterosaurs as dinosaurs, but they're actually not). 

To get more at the heart of your question: Crocodylians are widely perceived as these unchanging, prehistoric animals. They're really not. Crown-group crocodylians (that is, the group consisting of the common ancestor of all living species and all of the descendents of that ancestor) first show up in the Late Cretaceous, around 84 million years ago. This actually isn't a terribly long time ago, and it overlaps with the non-avian dinosaurs for about 20 million years. For reference, the [oldest known placental mammal](http://www.livescience.com/15734-oldest-placental-mammal.html) is 160 million years old. 

It is true that crocodylians do have relatives that extended back much further, because archosaurs started to diversify in the Triassic some 250 million years ago, but the crocs you see today are highly derived, not long-forgotten vestiges of the Mesozoic. It's true that some have had a fairly stable body plan, but it's also a body plan that has cropped up multiple times in vertebrate evolution, including in [temnospondyl amphibians](http://en.wikipedia.org/wiki/Prionosuchus) some 270 million years ago. In a lot of these cases it has evolved independently.

The major radiation of archosaurs that includes modern crocodylians is known as Pseudosuchia, and it [first shows up about 250 million years ago](http://openi.nlm.nih.gov/detailedresult.php?img=3194824_pone.0025693.g012&amp;req=4). These early [relatives of crocs](http://web.missouri.edu/~hollidayca/Croc_epipterygoid/Fig%202.jpg) looked more like [this](http://en.wikipedia.org/wiki/Hesperosuchus) (in that cladogram Crurotarsi = Pseudosuchia). Nothing like a modern croc. 

Even as we move up the tree towards Crocodylia, early crocodyliforms looked like [this](http://en.wikipedia.org/wiki/Protosuchus). These were fairly gracile, terrestrial animals. Again, a similar croc body plan pops up in a few lineages, like in the [phytosaurs](http://en.wikipedia.org/wiki/Phytosaur), which are likely a basal pseudosuchian but not closely related to crocodylians.

[Mesoeucrocodylians](http://en.wikipedia.org/wiki/Mesoeucrocodylia), a grade of crocodyliforms that isn't a valid taxon but useful for referring to groups outside the crown group, often look more like the body plan associated with typical crocodylians, but they also show significantly more morphological diversity than that. Pholidosaurs (like [*Sarcosuchus*](http://en.wikipedia.org/wiki/Sarcosuchus)) and dyrosaurs have a similar body plan. Metriorhynchids like [*Metriorhynchus*](http://en.wikipedia.org/wiki/Metriorhynchus_superciliosus) were marine and had flippers. Notosuchians like [*Simosuchus*](http://en.wikipedia.org/wiki/Simosuchus) are very different. *Simosuchus* probably wasn't even carnivorous. It was also pretty adorable. 

The oldest members of crown-group Crocodylia are more morphologically similar to extant crocodylians. However, you still have morphological variation within Crocodylia, such as the [pristichampsids](http://en.wikipedia.org/wiki/Pristichampsidae), which were terrestrial. Terrestriality shows up again even in the family Crocodylidae (with the Mekosuchinae, including *Quinkana*).

The oldest members definitely attributable the genus *Crocodylus* [date to the Late Miocene](http://www.bioone.org/doi/abs/10.1643/0045-8511%282000%29000%5B0657:PRADTO%5D2.0.CO%3B2) (paywalled, sorry), and the genus probably diverged in the last 10 million years or so. That's pretty recent in the grand scheme of things, and some 55 million years after the non-avian dinosaurs went extinct.

",null,0,cdkrh86,1r8b8y,askscience,new,15
null,null,null,2,cdkow4g,1r8b8y,askscience,new,2
NAG3LT,"Well, almost all 3D stuff that is made today is Stereoscopic 3D. It means that two images are created with a slight offset in camera position. When one image is shown to one eye, while the other eye sees the slightly different picture we perceive it as a single 3D image. So to display such 3D content, your TV must be capable to display two different images to the two eyes at the same time.


Your standard TV is likely an LCD panel with 50 or 60 Hz refresh rate. Your both eyes see the same image on TV, which is slightly shifted between eyes and you perceive it at the same distance as the TV is. To allow your eyes to see different images you need some additional tricks. The simplest solution is to use special glasses with filters capable of filtering different stuff. 


There is one type of 3D your TV can show with no issues - those 2 colour [Anaglyph images](http://en.wikipedia.org/wiki/Anaglyph_3D). You then use glasses with red and cyan filters (or other colour combo) and one eye sees only red and other only the cyan image. The main issue of this method is colour reproduction, which is awful, so it works best with black and white content. Some colour choices are less bad, but there is no perfect choice for this method. Another type of 3D that normal TV can display is [active shutter 3D] (http://en.wikipedia.org/wiki/Active_shutter_3D_system). You use a glasses with LCD shutters which show 1 frame to right eye while covering the left, second to the left and so on. TV meanwhile displays frames in pairs: R-L-R-L-... , which you also perceive as a 3D view. The refresh rate you see is effectively halved and the smaller it is the more perceptible the flickering is. Using it on a 60 Hz display is possible, but far from comfortable, so TV's using it have 100 Hz or higher refresh rates to get 50 Hz of more perceived 3D refresh rate. 


Another interesting property of light used for 3D displays, but mostly with projectors is the polarisation. The light can come in two different polarisations, like linear vertical and linear horizontal or CW circular and CCW circular. These are just some of possible combinations, but what is important that they are exclusive. You can use a special polariser to pass only CW polarisation while completely blocking CCW. Polarisation is also independent from colour and can be filtered by a passive filter, making glasses with polarised filters relatively cheap. So by showing image for one eye in CW polarisation and other image in CCW, glasses with corresponding filters can allow you to see a 3D film. This technique simply requires the special hardware to show different polarisations and wont work on a normal TV. BTW, all LCD displays emit a linearly polarised light, but only in one polarisation. 


If you don't want to use glasses there are some solutions, but they are less reliable so far and less used. Some TVs use micro lenses to show some pixels only from a specific angle of view. This allows them to show different images to your eyes, which look at TV with a slight difference in angle. Their main problem is the fact that you can only view them from specific spots, otherwise the 3D effect breaks down. 


There are other methods as well, but all of them still require you to have additional hardware to show separate images to each eye. The methods with the highest quality require hardware solutions that normal non-3D TVs almost always lack.",null,0,cdkn5i6,1r8ayj,askscience,new,5
threegigs,"3D TVs have alternating pixels which are polarized horizontally and vertically, allowing it to display one vertically polarized image and one horizontally polarized image, which are then selectively filtered out by 3D glasses (one lens horizontal, one vertical).

Alternately, active shutter glasses can be used to make any TV a 3D TV, by presenting alternating images for the left/right eyes, and by using special glasses which alternate turning each lens opaque, thus presenting different images to each eye.",null,1,cdkn8xx,1r8ayj,askscience,new,3
wazoheat,"**tl;dr: Wind gusts are usually very shallow, only within a kilometer (0.6 miles) or so of the ground ([simulation video](http://www.youtube.com/watch?v=G7aOwKigyTk)), and are due to turbulence.**

Sudden wind gusts are caused by a few different phenomenon, but the most common is just plain old turbulence. Through most of the depth of the atmosphere, wind speeds are relatively constant over short periods of time. Wind speed and direction above the ground changes all the time, but over the course of hours, unlike the ""gusts"" we experience which only last a few seconds. Friction in the [planetary boundary layer](https://en.wikipedia.org/wiki/Planetary_boundary_layer) (the layer of air closest to the ground) means that the wind above Earth's surface is almost always going to be stronger than the wind near the surface, where people spend the majority of their time. This means that there is a region of high wind speed flowing over a region of relatively low wind speed near the ground. The boundary between these two regions is inherently unstable, which results in turbulence. This turbulence has the consequence of sending some areas of high-velocity wind down towards the ground

[Here is a simulation that probably gives the best visualization of this phenomenon](http://www.youtube.com/watch?v=G7aOwKigyTk). In that video, high winds are marked in red, and calm winds in green. You can see that as time goes on, areas of high winds are brought down to the surface due to the turbulent motions of the boundary layer, causing what we know as a ""gust"" of wind. So to answer your initial question, a cross-section of a gust would look like one of those green areas in the video; it does not extend throughout the atmosphere. The depth of the boundary layer is usually [around 1 km (0.6 mi or 3300 feet)](http://www.met.rdg.ac.uk/~swrhgnrj/teaching/MT36E/MT36E_BL_lecture_notes.pdf), so this is about how ""tall"" a gust would be.",null,0,cdkni9i,1r8apk,askscience,new,15
strummingmusic,"Even though there are no ""crows"" in South America, there are plenty of other corvids. Different types of jays and such in the genuses Calocitta, Cyanocorax, Cyanolycra, etc. Here's a photo I took of a Green Jay in Venezuela some years ago: http://imgur.com/aVoI4Lp

Maybe over time the range of crows will extend down into there, who knows - we're looking at such a short timespan when you really think about things on an evolutional level. ",null,39,cdkorbl,1r8ake,askscience,new,209
carolnuts,"But we have corvids here in Brazil! We call them ""gralhas"" down here.  
The most common is the azure ray ( gralha-azul) , who lives in southern  Brazil. [Link](http://farm5.staticflickr.com/4087/5176353885_f94fbabfc1_z.jpg)


But we also have the white-naped jay ( gralha cancā ) , who lives in the Northeast semi arid region.  [Link](http://www.criadourosonhomeu.com.br/sonhomeu/images/gralhacanca.jpg)",null,13,cdksjqx,1r8ake,askscience,new,73
AshRandom,"This might seem overly simplistic, but the short answer is that there are a vast number of bird species in south america which out-compete the north american crow. 

Also, it's possible that it is a question of the motivation for continental movement which triggers the spread of species. Crows in the southern parts of their range appear to be resident and not migratory. This tendency might also contribute to the explanation for why they have so far failed to take up residency in the southern continent. 

Partial Source: *Dr. Kevin J. McGowan, Cornell Lab of Ornithology.*",null,9,cdkp7e8,1r8ake,askscience,new,65
AcaAwkward,The migration pattern observed in crows excludes both New Zealand and southern parts of South America. There is no concensus on the reason behind this,null,0,cdknamr,1r8ake,askscience,new,4
null,null,null,1,cdkn03v,1r8ake,askscience,new,1
NightmareOfLagrange,"Stimulation of epithelial cells in the upper airway from tobacco smoke (both first and secondhand) has been shown to result in both an immediate inflammatory response (SEVERELY exacerbated by any concurrent allergic reactions, I might add) as well as long-term remodeling of the tissue.  Without getting too technical, constant damage and stimulation of the airway tissue causes the tissue to remodel in an attempt to repair and acclimate to the smoke.",null,0,cdkl5ck,1r8ahf,askscience,new,1
iorgfeflkd,"Yes, that's why you get two-slit interference patterns when you do the experiment with electrons.",null,3,cdkkuxr,1r89ry,askscience,new,9
nanopoop,Another example is neutron scattering.,null,1,cdkl3y0,1r89ry,askscience,new,4
selfification,Also consider bonding and anti-bonding orbitals.  That's literally electron wave functions overlapping in phase and out of phase.,null,1,cdkopgo,1r89ry,askscience,new,4
Trill-Nye,"The two most prominent examples, diffraction of electrons and neutrons, have been mentioned. To expand on this, all particles can be made to interfere if suitable conditions are imposed. This is easiest to do with subatomic particles. By shooting energetic electrons or neutrons at some kind of grating for which the spacing of the grates are on the order of the wavelength of the particles used, one can produce a sort of ""many-slit"" experiment. 

Crystalline materials are good for this, because they have regular atomic spacings, such that the atoms scatter incoming particles, generating quasi-point sources of scattered particles similar to the slits in a double slit experiment. If you then have detectors set up to measure where these particles end up, you will see an interference pattern where certain places get many hits (where constructive interference of the wave-like particles occurs) and other places get few or none (where destructive interference occurs).

Interestingly, diffraction using [x-rays](http://en.wikipedia.org/wiki/X-ray_diffraction#Overview_of_single-crystal_X-ray_diffraction) (photons) gives similar results to [neutron](http://en.wikipedia.org/wiki/Neutron_diffraction) and [electron](http://en.wikipedia.org/wiki/Electron_diffraction) diffraction, because all three behave as waves in a scattering experiment, even though the latter two are particles.",null,0,cdklu16,1r89ry,askscience,new,3
bertrussell,"Destructive interference doesn't mean that the matter disappears or cancels itself out.

Destructive interference means that the there is an interference pattern in the position/momentum pattern for the objects.

When light undergoes destructive interference in one location, there is necessarily constructive interference in another location. This means that the light is more likely to interact in the constructive interference location than in the destructive interference location. The same is true for matter that interferes.",null,0,cdkrgwe,1r89ry,askscience,new,2
Brodken,"I think your question have been answered already, but I will contribute with an example I personally like a lot. 

You can macroscopically see matter interference with Bose-Einstein condensates. Around 1995-1996 there started to be a lot of experiments of interference with this condensates (formed by millions of particles). All this macroscopic system *behaves* like one giant quantum particle, and as such, it behaves as a huge wave. This is what we call a giant matter wave. 

I find this so incredibly awesome, the fact that we can actually see in a experiment, in a direct way, the inteference of two macroscopic clouds of atoms.",null,0,cdkuce8,1r89ry,askscience,new,2
clade_nade,"One not-yet-posted example is [Anderson localization](http://en.wikipedia.org/wiki/Anderson_localization), which is basically the result of electron wavefunctions canceling themselves out in disordered solids.*

*Actually I'm not sure if this has been experimentally observed for electrons, but the theory is quite solid and it's worked for photons, so...",null,0,cdl8gli,1r89ry,askscience,new,2
polandpower,"Yes, it's not a ""strong law"" in that you could, theoretically, observe the reverse happening. However, the chance of heat flowing from cold to hot on human observable time scales is so *astronomically* low that in practice the law is never violated.

If you look for instance at a gas, you can compute the microstates and see just how gigantically more likely heat is to flow from hot to cold. 

If you're interested in reading more about the topic, I definitely recommend going to your (University) library or Amazon to read [""Thermal Physics"" by Daniel V Schroeder](http://www.amazon.com/Introduction-Thermal-Physics-Daniel-Schroeder/dp/0201380277). It's very well written and pretty readable if you've had high school physics. 

",null,0,cdkp8za,1r88ue,askscience,new,6
Mooslletoe,"Everything goes from high to low, unless you do work. That is the second law of thermodynamics. The hot and cold try to equalize and reach equilibrium unless you do work on the system. This also applies to grades in school (assuming you start with an A), your grade will drop unless you do work. ",null,10,cdklifh,1r88ue,askscience,new,3
afranius,"Bitcoin and peer-to-peer data sharing are about equally anonymous. When people say bitcoin is anonymous, they don't mean that it's impossible to find out who paid for what, they mean that you don't have to use a bank account or credit card number (which usually requires supplying your real name, DOB, etc and stores a history of your transactions). You still need to use a computer to transmit the information, which exposes an IP address, which can in principle be traced back to you. The same is true for peer-to-peer data sharing. You can mitigate this by not using a connection registered to you (internet cafe, public wifi) and taking other steps to anonymize your traffic (proxies, etc.), and that applies equally to both bitcoin and peer-to-peer traffic.

To summarize, if the FBI wants to prove that you paid for something with bitcoin, they can show that the payment data came from your computer, just as they can show that your computer was involved in peer-to-peer file sharing. Methods to obfuscate your identity are equally applicable to both.",null,0,cdknq1u,1r87g0,askscience,new,8
Daegs,"While other posters are correct that bitcoin isn't anonymous, often transactions are made through the Tor network, which makes it anonymous.

The reason for this working for bitcoin and not P2P, is that the Tor network has a lot of overhead, leading to high latencies and very small bandwidth. 

For transmitting a bitcoin transaction, only a few ID's and keys need to be passed around, so even on extremely slow connection, it will be very fast. For P2P, this might make a 5 min download take a day or more. ",null,0,cdkp7yv,1r87g0,askscience,new,3
null,null,null,1,cdknxni,1r87g0,askscience,new,1
KarlOskar12,"Mythbusters actually demonstrate how much a stomach can stretch before bursting in this [clip](http://www.youtube.com/watch?v=93vjY9RY4-k). However, one technique competitive eaters use is to use abdominal muscle contractions to force food from the stomach into the small intestines creating more room for them to continue eating. Normally this would likely cause one to vomit, but through practice they have been able to bypass the reflex. Their bowel movements must be massive after a hot dog eating competition, although it would probably feel better if they just vomit a lot of it up. And they drink plenty of fluids so they don't get dehydrated.",null,0,cdkyz6w,1r86rk,askscience,new,2
proule,"Simply put: Genes come in different versions referred to as ""alleles"". Different alleles show different degrees of dominance over each other, and on top of that, some alleles may show incomplete dominance when expressed along with a certain other allele. 

In the dominant-recessive gene interaction, one gene's visible end effect dominates and is all you see. In incomplete dominance, two alleles are expressed where neither fully dominates the other.

In reference to the eye colour question: Eye colour tends to be more of a dominant-recessive relationship. The allele for brown eyes dominates that for blue, so we don't get any sort of brown-blue intermediate. This works similarly for green and blue.

Skin pigmentation is the result of several (I believe around 6?) separate genes relating to the amount of pigment produced, the ability for pigment to be shipped to the proper place in the body, stuff like that. In this case, the myriad of different genes contributing to skin pigmentation result in expression that mirrors incomplete dominance (but is actually slightly more complicated than the normal single-gene dominance question).

There are some concrete reasons that some traits mix and others don't. People with blue eyes have significantly reduced melanin in their irises, so any allele for eye colour that isn't blue will just overwrite blue eyes, since the trait is due to little pigmentation (so it can just be overwritten essentially, by the presence of said pigment). The reasons for other allelic interactions vary by case, and there's likely no satisfactory answer that's all encompassing.",null,0,cdklnkc,1r847u,askscience,new,5
ToThink,"I'm only in my 3rd year studying genetics, so I hope I can get this through properly :)

First we have to establish the fact that within a gene, there are alleles of that trait. Alleles are just different forms of a gene, so for eye colour there is a ""green eye"" allele and there is a ""blue eye"" allele. I put the quotations there because eye colour isn't due to a specialised pigment which reflects that colour of light (e.g., blue eyes aren't due to a pigment which reflects blue lights), rather eye colour is dependent on the amount of melanin in the eye. So blue eyes = very low melanin
Similarly with skin colour, the amount of melanin in your skin determines how dark you are.
Currently, one of the explanations behind why a parent with blue eyes and a parent with green eyes don't make a turquoise eye baby is because of dominance of alleles. The child will most likely have green eyes because the green eye allele is ""dominant"" to the blue eye allele. This isn't completely true but eye colour works in a ""dominance-recessiveness"" sort of manner.
There are new studies which reveal this may not be the case, rather some new studies reveal eye colour is dependent on many different genes. (I tried looking for them, couldn't find them).",null,1,cdkkf68,1r847u,askscience,new,6
Fignot,"Codominance can be a funny thing. There's a type of codominance that is unique to genes located on the X-chromosome, because women develop as genetic chimera's of themselves.

Another example is in blood where the A and B are codominate. This is because most genes you have are being expressed, even the recessive ones. In the case of the blood genes though any amount of antibody production is enough to make certain types of blood incompatible with you. Since people with the AB blood type are producing A and B antibodies, they will react with blood that contains the relevant antigens.

On the other hand though you have traits like hair colour. Say you have the blond hair allele, and the brown hair allele. You'll make the pigments of both colours, but the brown will overpower the blonde. So you'll still have brown hair, but maybe you'll have a slightly lighter shade of brown, or some blond highlights.

*source: I went to school for Bioinformatics.",null,1,cdkrzvy,1r847u,askscience,new,2
shavera,"well it does always travel at the same speed. And if the space between two points is expanding, you could imagine that it might be that light would appear *as if* the light was being slowed, since it wasn't covering the right amount of distance in time. 

What **actually** happens is that the light is stretched out while it travels, so that the speed stays the same, even though distance is added between things. 

The other thing maybe you're getting confused about (I'm not sure) is that both wavelength is elongated and frequency is reduced. If you work out the wave equations, they still work for speed of light = c",null,3,cdkjmnc,1r845k,askscience,new,6
bohr_exciton,"The red shift of light is a consequence of the fact that energy is not Lorentz invariant. In simpler terms, an object will be perceived as having different energies depending on your relative motion with respect to it. For instance, take a basketball on a bus. In the stationary frame of reference of an observer on the sidewalk, the ball has a kinetic energy due to its motion with the bus, but in the frame of reference of someone on the bus, the ball would appear stationary and have no kinetic energy.

Now think of light (or photons) as the basketball. The energy of the light is given by h*f, where h is Plank's constant and f is the frequency. Two observers moving relative to one another will observe the photon to be moving with the same speed. However, at the same time, they will measure different energies for the light, which due to the relation above means that they will observe different frequencies/wavelengths. 

Since there is a known relationship between the change in apparent frequency and the relative motion between two reference frames, we can use this information to calculate the relative motion of say an object emitting that light towards us assuming we know the energy of the light in the frame of reference of the body emitting it. ",null,1,cdklf0z,1r845k,askscience,new,3
jimustanguitar,"Does the wavelength shift along with time and space, and I shouldn't be thinking of speed and wavelength as being locked together?",null,0,cdkiz30,1r845k,askscience,new,1
LaLife,"**EDIT:** *It turns out that it's both: there is both Doppler (related to the motion of the point of light emission) and Cosmological (related to space expansion during travel) redshifts, and these both play into the redshift component. The funny thing is that redshift at relativistic velocities can happen even when the objects are not moving away from each other, due to time-dilation effects (Ives-Stilwell experiment).*

To put it simply: the wavelength of the light is a different quantity than the velocity. Any given photon travels at constant velocity c. It's the wavelength of the photon that is 'stretched' at the point and at the time of emission of the photon, due to the relative speed of the light source to the observer.

The redshift is not a measure of the expansion of the universe, it's a measure of the velocity of the light source. We know the universe is expanding by virtue of the fact that the furthest a galaxy is, the faster it's moving away from us.",null,0,cdkred8,1r845k,askscience,new,1
Spiralofourdiv,"I see a lot of kinda complex answers here, so let me take a crack a a simple one:

Light is indeed a fixed speed for all reverence frames, etc. but the color of light has nothing to do with speed, it only has to do with frequency.

Dopler shift **doesn't** ""speed up"" or ""slow down"" light, it's actually entirely dependent on the fact that neither can happen! Since light moves at a fixed speed for all observers, if the source and/or target are moving, then the wavelength needs to adjust correspondingly so that there can be n/2 wavelengths between the objects (that's the ""quantum"" part of quantum mechanics, no ""23.1495 wavelengths"" allowed, only 1/2, 1, 3/2, 3, ...). When the wavelength is ""pulled out"" because the targets are moving away from each other, the light is red. It's not slower light, it's simply lower wavelength/frequency light. 

This actually makes some intuitive sense if you consider that frequency is a measure of energy. The following is a bit naive, but I think it's fair. It makes sense that light has to travel farther to reach targets that are moving away, and traveling farther takes more energy, so the light we perceive is lower frequency/energy. Similarly, objects moving towards each other ""give"" energy to the light since it's reaching it's target in shorter distance. The energy can't disappear, so the frequency is higher.",null,0,cdkwrch,1r845k,askscience,new,1
-Rookery-,While the speed of light is still the same certain aspects of the light change. As the wavelength of the light increases it red shifts. To ensure that the speed of light is the same (c=wavelength * frequency) the frequency of the photon needs to decrease.,null,0,cdlpoek,1r845k,askscience,new,1
shavera,"Okay so imagine you have the simplest circuit. Current flows around a simple loop of wire. Now, you cut the wire. The electrons are still in motion, so one end of the cut gets a little bit depleted of electrons, and the other one gets an electron pile-up. But this won't last too long, since the depleted end now has a positive charge, and the pile-up end is negatively charged, the electrons will be pulled in the opposite direction somewhat.

A simple analogy is like a circular pipe of water that's got water flowing around through it. Imagine you were to cut the pipe *and* seal the ends simultaneously. There would be a back pressure at one end, and a low pressure at the other end, causing the flow to slow down and stop. (neglecting some other effects we'll get to in a bit like ""ringing"")

So what if you wanted the effect to last longer, to take a bit longer before the circuit slowed to a halt, what could you do? Well you could take the ends of each wire and add more and more metal to them, giving the electrons more space to spread out on the pile-up end, and more electrons to draw from on the depletion end. (we'll call the pile-up end the negative plate, and the depletion end the positive plate from now on). And if those plates were closer together, the electrons would ""see"" the opposite plate more easily and so not feel the effect as strongly (ie, electrons travelling toward the negative plate and being repelled would also be attracted to the positive plate just slightly beyond it, and so the overall repulsion would be relatively less). 

So that describes our basic ""parallel plate capacitor:"" The amount of charge you can ""store"" in the capacitor, its 'capacitance' is proportional to the area and inversely proportional to the distance between the plates. C = k A / d (where k is just a constant to show proportionality).

---

Now let's look at the next step. We have these plates of charge, one positive and one negative. The current has stopped. well since we have a potential difference, that's going to drive a current in the opposite direction, no? The electrons on the negative plate are going to be pushed off of it and pulled onto the positive plate. But since they can't cross the gap between the plates (deal with this a bit later) they have to go the opposite way of the current that charged the plates up; they are 'discharging.'

---

So now let's put a little more into it. Clearly if you just pushed the electrons harder, you could push more electrons onto the plate, so we can also make a new equation Q=CV, where the charge Q is given by the structure of the capacitor itself (how it distributes charges) and the Voltage (how strongly charges are being pushed onto the plate).

Furthermore, we'd like to see how these plates change over time, right? Well, the amount of charge being added or removed from the plates is going to relate to how much charge is already on the plates, right? If they're really charged up, there are going to be very few new charges added to them (if we're charging) or a *lot* of charges leaving them (if we're discharging). And if we want something where the rate of change is proportional to the value at that moment, we want an exponential. Some kind of e^t function. 

Well let's consider discharging full plates. They start with some charge Q0 and get rid of a lot quickly. But as time goes on, there's less pressure to push new charges out, so they lose less charge. So if we look at e^t, we see that it goes from 0 at t=-infinity, passes through ""1"" at t=0 and then climbs high after that. What we'd like to do is flip that graph around the t=0 axis, right? So that it starts with some value and trends toward 0 after very long times. Flipping about that axis takes t to -t. So we can say that Q(t) = Q0 * e^-t . 

Now let's think of charging. Again we know that to start there's no charge, so we want rapid change. But we also want it to level off to some constant value, where the ""back-pressure"" from the charge on the plates is equal to the ""forward-pressure"" we're using to charge it with. So we look at our e^t again. Well that won't work since it goes up to infinity, which isn't reality. We try flipping it about the t=0 axis again (e^-t ), since that at least levels off. But this graph is decreasing charge, and we want increasing. So we multiply the whole thing by a -1, giving us -e^-t . Well the charge is increasing from a negative 1 to zero... that's not right either, since it starts at 0 and goes to some steady value. So let's add 1 to it, and we get 1 - e^-t . It starts at 0 and increases to 1 in a way that's proportional to the amount that's on the plate. That's nearly what we're looking for. If we just multiply the whole thing by its steady constant charge (Q0) then we'll have it. Q(t) = Q0 * (1 - e^-t ). ",null,2,cdkkfxd,1r841l,askscience,new,46
njaard,"Water metaphor:

Imagine a sphere in which right in the middle is a membrane made of rubber, and tubes go out on either end:

           ___
    ======( | )======
           ---

If you push water on one side, the rubber membrane starts stretching in the other direction. If you release that pressure, the membrane pushes water back.

Now replace ""water"" with ""current"" and ""push"" with ""apply voltage"".",null,2,cdkjvwd,1r841l,askscience,new,7
PorchPhysics,"A lot of the examples given here are great, but capacitors have another use in AC circuits.

Capacitors in AC circuits have decreased impedance at higher frequencies.  Impedance is effectively like resistance, but is more dependent on other factors in the circuit. 

One place where this comes in handy is for tweeters in speakers, they want only the high frequency sounds to come out so that the lower sounds are not overpowering.  Hook up a capacitor in series and it will filter out lower frequencies by resisting them more than the higher ones.  Since lower frequencies are already overpowering when coming out of a subwoofer, they don't need a similiar filter (but you could in theory use an inductor to the same effect on higher frequencies).",null,2,cdklbkp,1r841l,askscience,new,5
flawless-contempt,"The capacitor for a hydraulic system would be your accumulator and it basically does the same things.  Stores ""presure/voltage"" until a need arises, and creates a shock absorber for the system in case any irregularities occur as not to damage the system.  It has a very fast discharge rate and both are equally dangerous when not handled with proper care and due respect. 

Edit: Also a capacitor will only block direct current (dc)  it allows alternating current (ac)  to pass through. 
An accumulator does this by tying into a single direction flow. Meaning it only has one line in and will push its charge directly into a single line of the system there by absorbing impacts to the system or similarly ultimately supplying power.   Hydraulic systems and basic electrical systems are very identical. ",null,1,cdkl7qd,1r841l,askscience,new,2
Lost_Afropick,"I was using some at work yesterday for power factor correction.

You can read about it [here](http://www.kwsaving.co.uk/Business/pfc/pfc-simple.htm)

But I had to supply an inductive load, a big transformer with a certain amount of amps and because of it's impedence that would take a certain amount of volts to do.

Only my supply doesn't have that kind of V/A power and that current is slightly higher than what my supply regulator will allow (it's windings may burn).   So I used capacitors to get me a few extra amps.  

Use the link i put above to see how.  But In a purely resistive circuit your current will be in phase (rise and fall at the same point of a cycle) as your volts.  With an inductive load like my transformer, the current lags by 90degrees.  With a capacitive circuit the current leads the volts by 90degrees.  So adjusting the capacitance lets you play around with the angle of the current in this RLC circuit.  The current the transformer sees is still it's high rated current but I'm drawing less amps from my supply.  

[Here's more](http://en.wikipedia.org/wiki/Power_factor#Power_factor_correction_of_linear_loads)",null,1,cdklepb,1r841l,askscience,new,2
Mathness,"A simple system that will show the effect of charge and discharge is an astable multivibrator with two bulbs/LEDs, the time each bulb is on and off can be set by the capacitors (and resistors). Also fairly simple to calculate the times and show that the theory fits reality.

Another is an AM radio, which can be build with very few components.

Capacitors have a lot different uses, to list some basic uses. Block AC and let DC pass (and vice versa), change the frequency response of a system (filtering sound/noise for instance) and generating a specific frequency (for example in a radio).",null,1,cdkma0z,1r841l,askscience,new,2
SkyDolphin99,"Thanks for your replies everyone. I appreciate your effort to explain it to me, but I really can't understand I'm afraid. I would just like you to go slower in terms of explaining. :)",null,0,cdkrrep,1r841l,askscience,new,1
kajarago,"Other folks here have discussed the physical theory of the capacitor.  This explanation outlines some of the uses of capacitors in a circuit.

The function of the capacitor will depend on the type of input voltage (alternating or direct) as well as the placement of the component relative to the circuit.

The capacitor is very useful in a circuit because its impedance (electrical ""resistance"") is a function of the capacitance and the frequency of the signal as Z = 1/(2*pi*f*C).  This means that a system can be tuned to a certain band of frequencies or can be used as a filter depending on the application.  Take this simple circuit as an example:

http://upload.wikimedia.org/wikipedia/commons/e/e0/1st_Order_Lowpass_Filter_RC.svg

The capacitor will ""hold"" the lower-frequency components of the signal so you will see DC signals have the full* amplitude of the input signal, and the amplitude will roll off to zero as the signal frequency component increases to infinity.

In other more extreme cases, the amount of charge stored in a capacitor is very high and can be released to power devices like a camera's flash or an electric car.",null,0,cdksko5,1r841l,askscience,new,1
fourpenguins,[Related question](http://www.reddit.com/r/AskElectronics/comments/1pru9j/what_does_a_capacitor_do_what_are_they_used_for/) from three weeks ago in /r/AskElectronics,null,0,cdky9te,1r841l,askscience,new,1
knflrpn,"One of the useful properties of capacitors that I don't think anyone has [directly] mentioned yet is that the current through them can change instantaneously, while the voltage across them can not.


This is why they're used to ""smooth out"" voltage.  If the current in some device needs to change very quickly (as in, for example, a computer's CPU), but the power supply is relatively far away (e.g. somewhere else on the motherboard) then capacitors nearby will prevent the voltage from changing at the device (or at least mitigate it).


The reason that being ""far away"" matters is the corollary to the capacitor: the inductor.  For an inductor, the voltage across it can change instantaneously but the current can not.  Wires have inductance, so if the power supply is somewhere else, the inductance of the wires can cause problems with fast changes in current.",null,1,cdkzi8u,1r841l,askscience,new,1
whatzefuk,"best to test and check it out on a oscilloscope and you will understand it fully.
it pretty much packs electrons and releases them when its full , and you can influence curbs and yeah theres plenty of uses like Mathness said , you can pulse dc , use it as a filter , to turn from ac to dc its a diode bridge you will use , you dont have to use a cap but its highly recommnended to filter out.

same goes for neon lights , balasts you need a jolt top turn on the chemical reaction in the neon light but after that it can run on very low power , to create that jolt your gonna use caps.

also since caps hold on electrons after putting something on power be advised that it might hold a charge , small farad caps you can worry too much but once you get to the big boys it can be lethal , turning off the power and time will have it decay its charge.

Some capacitors have poles also , in my course it was always funny to hear a gunshot somewhere in the class you knew someone plugged his cap on the wrong pole.
",null,4,cdknkys,1r841l,askscience,new,2
800gpm,"Considering the radius of a bottle opening, the additional thickness is not a minuscule factor. I can't measure right now, but estimating the radius at 2cm and the thickness of a towel under pressure at 2mm, the moment of force would increase by 10% (assuming the same force). 

Another factor may be that the towel makes it less painful and thus lets you apply more force before reaching your pain limit.",null,0,cdkkmii,1r833i,askscience,new,3
ozone_one,"Cell towers generally have a relatively low power signal (a few miles to tens of miles of range), and are meant to cover a specific area. When you get close to the border of a cell you are handed off to the next cell.

You would not be likely to receive a cell signal in a plane at 35,000 feet for a couple of reasons...  1) the vast majority of the country doesn't really have much cell service.  Look at a coverage map - all of those white areas have no coverage.  and 2) You are moving so fast in the plane that your phone is not able to lock on to a particular tower, and if it does lock on it will be unable to hand you off to the next tower.
",null,0,cdkk8fo,1r7xrc,askscience,new,7
BizQuit,"3d printers that use DLP projectors are more expensive because they require a powerful projector to initiate polymerization of photosensitive resin. 

The more common hobbyist 3d printers are simply melting plastic and drawing lines with it requiring little expense as they are only slightly more than a hot glue gun. These common printers are very limited in just how fine a line they can draw, and they must create their work very slowly drawing out the entire parts layer.

With that understanding, It is easiest to think of a Dlp based 3d printer as a photographic process. Each layer is ""drawn"" not line by line but entire layer at once. An image of each layer is projected for a few seconds. Areas where pixels are dark remain fluid and uncured. Pixels that are illuminated cause a chemical reaction which causes solidification of the resin. The printer must then advance in only one direction UP or down depending on the design of the printer, allowing new resin to flow, and another layer can be ""exposed"". 

The tradeoff here comes in resolution and maximum part size. In the melt and deposit printers where you are limited by the fineness of the drawn line (minimum features of 200-400 microns at its best) you can, within the boundaries of the mechanical construction of your printer print parts of any size. The fineness of motor movement can make these parts seem quite smooth, but the ""line thickness"" can significantly limit small features.

Whereas the DLP printer has a fixed resolution in general no greater than 1920X1080.  If you fix your projector at a focus in a similar size to the line of an extrusion printer you only get to print (@200microns 15.1X8.5 inches) and you do not get the smoothing effect of a motor gliding along. BUT unlike these melt and draw printers, a DLP printer can attain VERY fine focus creating individual pixels 50 microns, 20 microns, a few have successfully managed as fine as 5 microns with off the shelf projectors. But each step down shrinks the total field printed. @20 microns you only get 1.5X.85 inches but the resulting part is amazingly detailed as a result. 

All that said, Photopolymer is more expensive than filament. The projector adds expense. And for many hobbyists the resolution is not worth the tradeoff in build size. If you are looking to make large objects it is not the right technology. If you are looking to make fine jewelry, intricate small scale models, dental work (yep labs are making dental crowns with these printers) right now the speed and resolution cannot be beat by any other hobbyist attainable technology. ",null,0,cdl6044,1r7wig,askscience,new,1
Marsdreamer,"We breath oxygen today primarily because of organisms called cyanobacteria which created organic compounds from H2O and CO2; turning it into O2. Before this time there was no or very little atmospheric oxygen and it was actually toxic to most other organisms. This catapulted the Oxygen Revolution and is largely responsible for shaping the world as we know it today. 

Fast forward to your more pertinent question, which the answer is The Electron Transport Chain. You see, in order for our bodies to generate chemical energy in the form of ATP we metabolize organic compounds. Through some very lengthy and complex chemistry electrons are stripped from atoms and other molecules to generate an electrochemical gradient responsible for powering the generation and packaging of dense energy molecules known as ATP. 

During this process of ripping negative charges (Electrons) from organic compounds a build up of electrons takes place and they *must* go somewhere. 

Enter Oxygen. Oxygen loves electrons and it is *highly* electronegative. Because of this our bodies have adapted to harvest oxygen for the use of dumping the excess electrons generated from the Electron Transport Chain. Oxygen takes the electrons, binds with Carbon, and then is exhaled. 

*Gen Bio was a long time for me, but this is what I remember. I'm going to double check my sources now and would also appreciate other biologists to check my explanation as well.*",null,0,cdkjwie,1r7w97,askscience,new,17
Jetamors,"Marsdreams does a great job of answering the Why Oxygen question. You also asked Why Not Nitrogen: the answer is because most nitrogenases (enzymes that break down N2) are inhibited by oxygen; if there's any oxygen around, they won't work. If you're a bacterium or a plant with roots in the soil, you have areas where you can use those enzymes, but if you're an animal living in the air, there's no way to avoid oxygen, so nitrogenases aren't going to be useful for you.",null,0,cdklcvn,1r7w97,askscience,new,6
proule,"Many organisms actually use nitrogen as a terminal electron acceptor (essentially ""breathing"" it in the same way we breath oxygen). The difference is that these organisms are very small and can live on significantly less energy than higher multicellular organisms.

Oxygen is significantly more electronegative than nitrogen (especially considering nitrogen in the atmospheric form N2 is already satisfied with its electron configuration and won't accept more electrons). This difference in electronegativity results in a significant increase in energy liberated from giving oxygen electrons. 

You can think of giving oxygen electrons as a starting point and an end point (your electron donor, and your electron acceptor, which is oxygen). If you simply give oxygen the electron right away then you're losing a bunch of energy to heat. So, between the oxygen and the electron you essentially put up walls for the electron to push (walls being an analogy for cellular work). Oxygen is so powerfully attracting the electron that it pulls it in despite these walls, with the movement of the walls performing work in the cell.

Nitrogen is less electronegative than oxygen, meaning that nitrogen can't pull electrons through as many walls. Giving an electron to nitrogen just doesn't result in as much energy because it's not pulling on the electron as hard.

Higher organisms, mammals especially, have tremendous energy consumption. From a single glucose molecule, a fermenting organism like yeast will produce two molecules of ATP (the energy currency of any cell). The only way for complex multicellular organisms to evolve was to get more energy out of a single molecule of glucose. With oxygen and the electron transport chain we get more like 38 molecules of ATP. If multicellular organisms extracted energy from glucose as inefficiently as fermenting organisms, we'd have to (very simplified) basically consume 19 times more food to get the same energy.

Animals are all nitrogen dependent, but luckily, consuming other organisms yields plenty of nitrogen, because it's the base of amino acids that make up proteins. Trees fix nitrogen in their roots (with the help of symbiotic relationships to bacteria; they don't do this themselves) because they get their energy from the sun, and light obviously can't provide the tree with nitrogen.",null,1,cdkm17n,1r7w97,askscience,new,5
Son_of_Thomas,"The different blood types refer to the different types of antigens on the surface of our red blood cells. There are 3 main ones: A, B, and D. A and B determine the type of blood you have (A, B, AB, or O type) and D determines whether your blood is positive or negative- the presence of the D antigen (also known as the Rh factor mentioned in another comment) means you are positive. For example, having the antigens A and D make your blood A+. If you have A and B but no D, your blood is AB-. Having only the D antigen without A or B means you have O+ blood, and a lack of all 3 antigens means you have O- blood. 

The reason this is a problem during blood transfusions is because if you have a certain antigen, it means you have an antibody for the other antigens you dont have. For example, if you have type A+ blood, meaning you have the A and D antigens, you have a B antibody. If your A+ blood were to come into contact with a B type blood, the B antibodies in your blood would react with the B antigens in the B blood, causing the B blood to coagulate and virtually be unusable. 

This is why type O- is the ""universal donor"", because it has no antigens for anybody else's antibodies to react with. This is also why AB+ is the ""universal acceptor"" because it has no antibodies to coagulate any type of donor blood. 

also, I do not know *why* we have different types. 

EDIT: clarification

EDIT 2: gold?! success! thank you, kind stranger. ",null,543,cdkkjeh,1r7v5g,askscience,new,2027
Yes_That_Guy,"Have had this saved after answering this question a few times.  Much better summary than I could write up with the time I have this morning.  Edited for content.  Credit to /u/Its_the_bees_knees

**If you have any other specific questions after reading this please let me know.  I feel this should satisfy most if not all of your questions**

**Antigen**- The tag that identifies something. These antigens exist, to help our body distinguish molecules as ""self"" or ""non-self."" Our immune system will recognize these antigens and react, or wont react. When there is an antigen-antibody reaction that is what causes the destruction of these cells and a massive clot forms.

*Edited expansion*- In my analogy. The antigens are the Locks. And the Antibodies are the keys. When your body encounters a Lock that doesnt belong to you, it tries to destroy it (open it) Once your body encounters this lock, it has now seen it and has now seen its design. After encountering a foreign lock, your body will now make keys to open and destroy these locks. Some of these locks are small (ABO blood group) and the keys can be made immediately and the reaction occurs immediately. However the locks of Blood type of +/- (explained more detailed below) are relatively big and can take a while for the appropriate keys to be made. So you constantly have these keys floating around in your blood constantly looking for a compatible lock. When a key matches a lock, that's what activates your immune response, destroys the lock and in turn causes all the negative effects of a transfusion reaction (explained more below)

**Why are there different types of antigens?** This boils down to their molecular structure. All blood types have the same ""core"" structure, but what differentiates the types is a Polysaccharide molecule that sticks out from the molecule. This polysaccharide (or Sugar in simpler terms) is what is the antigen and helps our body identify them as different blood types.


**Antibodies**- The main part of our immune system. Antigen-antibody interactions follow a lock (antigen) and key (antibody) When the key fits the lock that's what causes and immune reaction which ends up destroying the Red Blood Cell (RBC)

The locks are pre-determined due to the sugar ""tag."" Now one thing people dont realize is that certain blood types will have the specific antigen, but WILL HAVE THE OPPOSITE ANTIBODY.


**What does that mean?**

*Blood Type A*: Has A Antigen, and B Antibody

*Blood Type B*: Has B Antigen, and A Antibody

*Blood Type AB*: Has A and B antigens, No antibodies

*Blood Type O*: Doesn't have A and B, Has both A and B antibodies 

*edit*- (as pointed out by nearquincy below, Blood Type O actually contains H antigen, which is the precursor antigen to both antigen A and antigen B.
There are other antigens that is present on the surface of the blood type O itself although they are not significant in ABO system.""






**What does that translate into the real world?**

*Blood Type A*: They only have 1 antibody-Type B, so if that antibody finds its partner antigen (found in Type B or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type B*: They only have 1 antibody-Type A, so if that antibody finds its partner antigen (found in Type A or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type AB*: Has both antigens, but no Antibodies. (Because if it had any antibodies to either A or B, the person wouldn't exist in the first place, because the Antibodies would keep finding their Locks, and self-destructing the RBC's) Because blood type AB has no Antibodies, they are known as **Universal Recipients** Meaning that because there are no keys, there will never be keys to correspond with the locks.

*Blood type O*: Has no antigens, but does have both types of antibodies. Because of the lack of antigens, they are known as **universal donors** meaning their blood can be transfused/mixed with any other blood type and there will not be a reaction.





**Below an even simpler version of what this means**

*Type A blood*- Can Receive type A or Type O Blood. Will react with type B or type AB.

*Type B blood*- Can receive Type B or Type O Blood. Will react with type A or type AB.

*Type AB blood*- Can receive all types of blood- Type A, B, AB, or O. Will not react with any.

*Type O blood*- Can only receive type O blood. But can be donated to any blood type person. Will react with types A, AB, and B (when type O is the type of the receipient)

*edit*- cheat sheet for blood types http://i.imgur.com/fTw8AIj.png




**What is Positive and Negative Blood?**

Well this refers to the presence or the lack of another type of antigen, known as the Rh Factor or also known as the D antigen.



**Why is being positive or negative important?**

Well, this is most important for pregnancies. Specifically a mother's **second pregnancy**. It occurs in a Rh(-) Mother and an Rh(+)Baby. What does being Rh (-) or (+) mean? Well like I said it depends on whether or not you have the Antigen or you don't. During the 1st pregnancy the fetal blood will mix with the mothers blood. Because the mother has no Rh factor, but the fetal blood does- the immune system will recognize these particles as foreign and will in turn start making antibodies to it. Anything your immune system recognizes as foreign, it assumes its an enemy and will start to attack it.



**Why did you mention 2nd pregnancy and not first?**

Well these antibodies belong to a different class than the ones that react with blood typing. These antibodies take MUCH longer to form. Hence it wont affect the first pregnancy, but can affect the second pregnancy.


**Also, can a A- receive A+ blood?**

A+ can receive blood from a A- Negative donor, or A+ donor.

A- can only receive blood from A-.

Its the antigens that make up the difference. Negative means you have no antigen, so even if - donor blood present inside of a + person's body, the body doesn't recognize it as foreign, so its okay.

The problem exists when - person receives + blood. + Blood does contain the antigen, so when - Persons body sees the + blood, it sees the antigen, recognizes it as foreign and attacks it. So this blood transfusion is incompatible


**What happens when you have an incompatible blood type?** 

You get-

Fever

Jaundice

Decreased blood pressure

Increased heart rate

Increased breathing rate

Acute kidney failure (if severe enough)

Blood in urine (after it reaches the kidney failure stage)

Shock ( if severe, and untreated)

Death ( if severe and untreated due to the sudden loss of blood pressure and the shock)

Basically I described it as self destruct, because the host antibodies attack the foreign antigens and cause these red blood cells to be destroyed. When your red blood cells are destroyed they release a molecule know as hemoglobin. Hemoglobin is normally a good molecule which is responsible for oxygen transport, but that is only of its attached to/inside of the Red blood cell. When there is too much free floating hemoglobin in the blood- that is what causes all of the above symptoms.


**Can transfusion reactions be treated? And is it symptomatic treatment or does it require another full transfusion?**
Yeah it's basically sympomatic treatment.

But remember, that symptoms that appear are really variable and dependent on how severe the reaction is. 
But for a complete overall treatment of the above symptoms you would require-

Treating and anticipating the Shock (Which would also treat the low blood pressure and fast heart rate

Treating the kidney failure (Dialysis to filter out the hemoglobin) (If the reaction is severe enough, it may require a complete re-transfusion)

Treat the subsequent clot that will form. (When there is a transfusion reaction there will also be a massive blood clot that forms due to all of the now destroyed Red Blood cells)

**Certain blood types have shown greater incidence with certain diseases/infections. But there is no increased susceptibility**

Blood Type A: Hepatitis, Small Pox

Blood Type O: Black Plague, other digestive system infections, Autoimmune disorders(when your body attacks itself)

Blood Types A, B, AB: Clots in your veins


**Also some other interesting things to note is that-**

ABO (+/-) Blood typing will fit and categorize about 99.97% of the worlds population. But there is still that 0.03% who's blood has neither A/B Antigens nor the A/B Antibodies.

For these special people we have to to EXTREMELY careful when transfusing any type of blood, also we have to use other systems to type/categorize their blood

When a Blood Type O patient, receives blood from a blood type A patient. This causes the most severe of the transfusion reactions.

About 40% of the population is Type O. Type AB is the rarest.

About 80% of the population is +. So being AB- is the rarest of blood types


**Are there any benefits to having either A, B, AB, or O blood types?**

Type O can donate to all types of blood.

Type AB can receive all types of blood.

Types- A, AB, B dont have a distinct advantage over the other. *edit*- as pointed out by /u/hammurarbisan "" both A and B types routinely have elevated platelet counts versus the other blood types - most especially female A types - and thus more frequently tested for platelet counts and targeted for apheresis collection. Platelets are commonly used for transfusion during surgery.""

Being + however is an advantage over being negative. + Can receive from + or - people. Negative people however can only receive from Negative people.

*edit*- Type AB can donate PLASMA to all people, since they dont have any antibodies (the donation rules for plasma and whole blood are opposite in terms of antigens and antibodies)
",null,23,cdkmrvv,1r7v5g,askscience,new,143
swamp14,"A lot of people have been asking about the ""why"" and rightly so because it was asked in the OP but not yet answered. 

WHY humans have different blood types is akin to why some humans have more or less of a specific protein that helps break down alcohol, why some humans tend to be taller or shorter than others, why some humans have different proportions in arm/leg length... etc. It's simply genetic variation. Different genes being expressed because of genetic diversity in the human species, amplified by sexual reproduction.

To all the people asking what evolutionary advantage different blood types offer - just because a species has a particular trait doesn't mean that trait helps the species survive and reproduce more than if the species lacked that trait. Traits can be passed on simply because of luck or a number of other factors. Evolution works because traits that tend to be favorable for survival and reproduction tend to get passed on and thus become more prevalent in the species. But that doesn't mean disadvantageous traits don't get passed on. They're just less likely.

""Isn't it more evolutionarily advantageous to have a single blood type?""
Perhaps. I can't give a straight answer because I don't know. But this question indicates a misunderstanding of evolution. It's like asking ""Isn't it more more evolutionarily advantageous for humans to be stronger, faster, and smarter?"" So, even if the answer is yes, it doesn't mean it should be true. 

What this tells us is that evolution is not perfect. It is the process that results from random mutations and natural selection over long periods of time. We have different types of blood because of random mutations and those blood types got passed on. Whether or not those different blood types contributed to our survival and reproduction, I cannot tell you.

But what I can tell you is: 1) traits are not perfect, and asking why they are not ""better"" usually demonstrates a misunderstanding of evolution; and 2) traits do not necessarily need to contribute to survival and reproduction in order to be passed on, and assuming that they MUST be is not reasonable. Yes, traits usually are advantageous because if they weren't advantageous, they'd be less likely to be passed on. But again, this does not mean they absolutely need to be.

Edit - Thanks so much for the gold~

",null,14,cdkkr1g,1r7v5g,askscience,new,83
cracked_chemist,"In terms of evolution:

Reading a bit the article ""ABO blood group glycans modulate sialic acid recognition on erythrocytes"" (Cohen, M, et al. Blood, 2009), they suggest that the different blood type ""antigens found on human erythrocytes modulate the specific interactions of 3 sialic acid-recognizing proteins...  with sialylated glycans on the same cell surface."" These antigens ""modulate sialic acid-mediated interaction of pathogens such as Plasmodium falciparum malarial parasite."" Thus the different blood types may affect the host-pathogen interaction. To be fair hematology is not my field, but their model seems plausible. ",null,18,cdkmgly,1r7v5g,askscience,new,79
Andrenator,"And [an interesting thing](http://andrenator.tumblr.com/post/66202105636/detenebrate-0xymoronic-shitarianasays) about the B antigen (and a little of the A antigen),

is that during the black plague, the bacteria mimicked the B antigen, so if people had B type blood, they would have virtually no defense against it.  A type was a little better, people could recover from it.  Type O blood developed in small villages where accidental marrying of distant cousins happened, and O blood reacted violently to the plague.

So that's why B blood is so rare now, A is a little more common, and O type blood is so common.",null,5,cdknftz,1r7v5g,askscience,new,30
mobilehypo,"Interestingly, some blood types do give resistance to disease. The lack of the Duffy antigen in the majority of Black Africans has been shown to give resistance to two species of malarial parasites. This blood group system has been studied closely and we are finding that specific combinations of this antigen on the surface of red blood cells might have impacts on other diseases. [This part of the Duffy article](http://en.wikipedia.org/wiki/Duffy_antigen_system#Clinical_significance) on Wikipedia gives a rundown of what we have found so far.

This question has come up before, here's a set of search results that might help further:

* http://www.reddit.com/r/askscience/search?q=blood+type&amp;restrict_sr=on&amp;sort=top&amp;t=all

Here are some posts that address some whys:

* http://www.reddit.com/r/askscience/comments/iepiv/why_did_we_evolve_with_different_blood_types/

* http://www.reddit.com/r/askscience/comments/f19p4/how_or_why_did_blood_types_evolve_is_there_any/

",null,1,cdkjtyc,1r7v5g,askscience,new,27
Cabin_Sandwich,"And what connection, if any, does this have to ""eating for your blood type""?  I have a hippy friend who is always telling me I need to eat according to my blood type, how the blood types arouse from different groups of people who were eating different things and therefore I should eat what my ancestors ate.  I think it's nonsense.",null,7,cdkkrn4,1r7v5g,askscience,new,16
Chl0eeeeeee,"Differences in blood types arise from different protein expression on the red blood cell. So, first with the Rh factor (named so because it was first discovered in Rhesus monkeys). You can be Rh + or -, which means that your red blood cells have or don't have this certain protein on the surface. Problems can arise when a mother is Rh - and is carrying a child who is Rh+, as an immune reaction can occur (because the mothers body sees the Rh+ proteins as being foreign) if there is blood exchange between the two.  Usually the first child is fine, but when there's blood exchange in child birth, the mother builds up anti-Rh antibodies. To help with this, all Rh- mothers are treated with Rhogam to suppress that response. 

Now, on to the A/B/O blood type. Again, this is code for the type of proteins that are on the surface of the blood cell. The differences arise from genetic inheritance with one gene determining ABO inheritance. So, without making it too complicated... The presence of at least one ""A allele"" will cause the body to make specific glycoproteins, while the absence causes anti-A antibodies. The same goes for the B allele. If a person has one A and one B, then they are type AB. Because they are AB, they don't produce either antibodies. If they have two As, they are type A and likewise for B. In this gene, both the A and B genes are co-dominant. Type O arises when the person has neither an A allele nor a B allele, and instead has two recessive alleles. This person would not have any glycoproteins on the cell, and would produce anti-A and anti-B antibodies. This is why they are the universal donor (transfusion of blood won't set off an immune reaction, especially if they are O-). AB blood types are considered the universal receiver because they don't have the antibodies for any of the blood types, and thus can receive any blood. 

Hope this helps!",null,4,cdklo01,1r7v5g,askscience,new,9
Fazaman,"There are several great explainations in this thread as to what the different blood types are and how they interact with each other, but the question is *why* do we have these different types (just evolutionary/genetic differences?), are there any effects to having different types of blood (Say, A is more prone to getting disease X, or some such).",null,1,cdkq47v,1r7v5g,askscience,new,9
billyvnilly,"A,B,AB,O are designations for cells that express antigens.  D is also an antigen, and its significance is that it's reactive with antibodies as much as A and B.

We have evolved to develop these blood groups.  When Red Blood Cells (RBCs) are made they express a carbohydrate chain (millions) which is called H.  We have evolved over time to have different enzymes that modify this carbohydrate chain.  Most people believe B was the original enzyme.  A is speculated to happen afterwards.  So enzyme A or B act on the carbohydrate chain H, and modify it.  The modified chain is thus termed A or B.  If neither enzyme acted on the H chain, its termed O.  The enzyme A and B behave totally differently (enzyme activity and thus amount of H chains they convert), so if you have enzymes A and B, its not an equal split (thats a minor point, doesn't matter for this).  So these are just two examples of antigens on RBCs.  D is a third antigen that is clinically significant and very common, that is why people know about it.  What they don't know is D is a combo of many antigens.  D,C,c,E, and e.  but we always say if you're D+ then your blood is positive.  there is no d antigen.  
What most people don't know is that there are tons of antigens (100s) on red blood cells.  Some are there from the development of the RBCs' nucleus, and some are adsorbed from secreted antigens circulating in the blood stream.  
Why are all these important?  Well as your body makes antigens, it is also recognizing those antigens as ""self"" ... our bodies have the ability to then recognize ""not-self"".  So if you only have enzyme A, and not make any RBCs with B on their surface, you make antibodies to B, but not to A.  People with type A blood cant receive type B or AB blood because they have antibodies to B.  
Why do we have different blood types?  Well a good example is a blood group called Duffy.  The blood group has two antigens: Fya and Fyb.  And again these antigens come from two enzymes.  If you lack both enzymes, you lack the antigens.  And if you lack both antigens you're termed Fy(a-b-).  Why is this evolutionary?  Well because Plasmodium vivax (malaria) uses the antigen site to enter cells.  So if you lack the antigen, malaria cannot enter the cell (evolutionary!  This is why the majority of African-Americans are Fy(a-b-).",null,2,cdkm1ad,1r7v5g,askscience,new,8
kroxldyphivian,"Everyone is answering *what* the different blood types are. And while that's really informative and helpful, the original question in the post title (and the much more interesting topic imo) is *why* we have different blood types. Can anyone shed some light on this?",null,0,cdkobtd,1r7v5g,askscience,new,5
arumbar,"Blood antigen variation is thought to be a response to pathogens like viruses spreading from person to person and carrying some component of membrane protein with them, such that individuals who created an antibody response to foreign ABO markers had more success fighting off infections.  [wiki explanation here](http://en.wikipedia.org/wiki/ABO_blood_group_system#Origin_theories), [journal article here](http://www.ncbi.nlm.nih.gov/pubmed/15293861)",null,0,cdl3nvf,1r7v5g,askscience,new,7
Mumma_Sooz,"I've been lurking for a while but this has had me step out of the shadows.. I run a blood donor centre in Western Australia.  I encourage and applaud anyone who donates blood, be it whole blood, plasma or platelets.  People do it for so many reasons and in oz they get nothing tangible in return.  1 in 3 people will need a blood transfusion but only 1 in 30 donates.  If you have ever considered it but are hesitant, please don't be.  I can tell you from experience that it's nothing to worry about and the feeling of having contributed to saving lives is amazing.  ",null,1,cdl40ig,1r7v5g,askscience,new,4
pleasantliving,"I am in the Rare Donor Program and I never really understood what it means. All I know is I get a letter every few months making sure my address hasn't changed. On my donor card, my blood type says things like K:-1 and Jk(b-). Can anyone explain this to me? Why do some people need this rare blood? Does this mean I need rare blood? ",null,1,cdkzjdh,1r7v5g,askscience,new,5
OldMarmalade,"http://www.dnalc.org/view/15404-Chromosome-9-gene-for-blood-group-Matt-Ridley.html

AB - Very resistant to cholera

O - Susceptible to cholera

Among other things this may be one of the evolutionary pressures that explains variation in bloodtype. If you want to create the maximum number of ABs you'd have a population that was ~41% A and ~41% B to randomly generate ~18% AB supermen (in the time of cholera). This is indeed what you see in many populations http://www.blood.co.uk/images/content/pie-charts.jpg ",null,0,cdklqsz,1r7v5g,askscience,new,3
WasIsMitDenKohlen,"I haven't seen an answer to the first two questions. WHY are there different blood types. Why was there a need to have those distinct blood types. 

Would it make more sense evolutionary to have one blood type only, why bother preserving those distinct mutations, and everything that goes along with maintaining those? Or if not, why are there 3 distinct ones, and not 1000? What is the driving force to create 3 blood types? ",null,2,cdkpw6a,1r7v5g,askscience,new,4
redplate12,A clarification of the discussion of 'antigens' that represent the various groups. An O type does have an antigen structure but nobody has antibodies to it. The O antigen is a chain of 4 six carbon sugars. The A antigen has an added six carbon sugar (N-acetyl-galactosamine bound to second sugar in a 'branch) while the B antigen has an additional six carbon sugar (galactose) that is found bound to the fourth six carbon sugar core. An AB has both the A and B  five sugar chains present. Thus the difference is the presence of a single sugar moiety added to the core 4 sugar chain.,null,0,cdkswmc,1r7v5g,askscience,new,2
slakist,"I have no knowledge on the subject but have always been interested so this thread was great. I see some people have answered as to why we have different blood types, as well as explaining the difference between them. 

I would like to know if there is any disadvantage to having a specific blood type. For instance, I am O-; am I more susceptible to different kinds of diseases and disorders based on my blood type alone, more than a person with say, blood A? ",null,3,cdktzqz,1r7v5g,askscience,new,4
beliefinphilosophy,"Bonus factoid:

The different blood group antigens (or surface markers) are
Different kinds of sugars, (glucos, carbohydrate) and the +/- is a protein.





For example, the antigens of the ABO blood group are sugars. They are produced by a series of reactions in which enzymes catalyze the transfer of sugar units. A person's DNA determines the type of enzymes they have, and, therefore, the type of sugar antigens that end up on their red blood cells.
In contrast, the antigens of the Rh blood group are proteins. A person's DNA holds the information for producing the protein antigens. The RhD gene encodes the D antigen, which is a large protein on the red blood cell membrane. Some people have a version of the gene that does not produce D antigen, and therefore the RhD protein is absent from their red blood cells.

More [here](http://www.ncbi.nlm.nih.gov/books/NBK2264/)",null,0,cdkubis,1r7v5g,askscience,new,2
12and32,"I don't think the question of ""why we have different surface antigens"" has been answered. 

So why do we have differing surface and blood borne antigens? Are the genes that code for them related in any way, say by a few SNPs, or even just a single point mutation? I would think that they're related if we can predict heredity through simple Mendelian genetics, and because they do seem to have a high degree of compatibility (hemolytic newborn disease aside). There doesn't seem to be any particular reason for the Rh factor, nor does there seem to be a reason for antibodies that can only attack the A and B antigens.",null,0,cdkw1vb,1r7v5g,askscience,new,2
Jrj84105,"The different blood groups reflect differences in antigens present on the red blood cell surface. Because some of these antigenic variants confer resistance to infections by organisms such as malaria, certain blood groups are more common in different regions of the world. For the most part though, prior to the advent of blood transfusions, these differences were largely of no impact, essentially benign polymorphisms.

These kind of benign variations aren't just limited to our red cells but are present throughout our bodies. As we continue to challenge our bodies with new medical therapies, there turn out to be lots of previously irrelevant benign differences between people that now are seen as increasingly important factors in response to therapy and risk of side effects. ""Personalized medicine"" is largely an attempt to tailor therapy not just to a person's disease but also to the way in which any individual's response to therapies is a product of their unique set of variations.",null,0,cdl12gq,1r7v5g,askscience,new,2
MrGrow,"Our red cells consist of many different antigens (something that can trigger an immune response). Think of them like land mines. What people have can vary a ton (Kidd antigens, Duffy, Rh, Lewis, and so on). You simply make what you don't have. If I am A+ (very common) I am going to make the antibody to B. If you were to give me B blood, because I don't have that antigen on my own cells, my body will recognize that as foreign. What will more than likely happen is antibodies will coat the B blood cells, and when they are taken to the spleen, they will be ripped up and destroyed. This is called a transfusion reaction. There are different kinds of these and they each have there own symptoms. Why do humans have different types? Good question.     ",null,0,cdl68l5,1r7v5g,askscience,new,2
bigdaddymatty,"Whenever I donated blood the first time I became really interested in blood types and wondered this same question. I gave blood and didn't know my blood type before, but found out after that it was O- by looking on the donation companies website and logging in to see my donation session.   
I expect to receive a phone call each quarter now and every time I donate blood that ask me to do double red blood cells since I'm the universal donor! Giving blood is definitely worth it though",null,0,cdl6fqd,1r7v5g,askscience,new,2
laika84,"I checked comments and did not see this brought up as far as what the blood types ""actually mean.""  People have mentioned antigens and that these antigens are glycoproteins, which is correct.  However, I think it's also interesting/important to go a little further into how these different antigens are made and their significance, in terms of the ABO portion.

ABO bloods types are distinguished by the individual alleles that encode an enzyme.  The different enzymes have long names and essentially add different sugars via O-linked glycosylation to what could be a ""base"" sugar chain that is added first.  This ""base"" sugar chain exists on our RBCs and receives a fucose residue by fucosyl-transferase and having this enzyme alone constitutes the ""O"" phenotype/antigen.  The ""A"" and ""B"" enyzmes each add a different sugar to this base carb chain with the fucose and that creates the different antigens that we see.  You can think of it as the ""A"" enzyme adding the ""A"" sugar/antigen (N-acetylgalactosamine) and the ""B"" enzyme adding the ""B"" sugar/antigen (galactose).  It may seem trivial, but I thought it was interesting when I learned it and enhanced my understanding of what these antigens ""looked like"" on a biochemical level.

For a really cool tangent, do a search for the ""Bombay"" phenotype!",null,0,cdl02x8,1r7v5g,askscience,new,2
Shekho,"I don't remember where I heard this but i'll try to be nice, and simple....

Lets say a disease came along and wiped out all people with (x) blood type because they don't have a certain antibody that protects them from disease then other people with (y) blood type would live because they naturally can protect themselves..

If we all had the same blood type, wouldn't that mean we could easily be wiped out as a race from diseases? 

Please correct me if I'm wrong.",null,1,cdl33v2,1r7v5g,askscience,new,2
quiktom,"I was told that the blood types evolved over time. The O being the original and oldest when we ate anything we could find. The A type evolved when we got into agriculture and the B type when we started living in concentrated cities. Apparently AB is only 600 or so years old. AB blood can accept any blood before it (since those tpes were around when it evolved and essentially are a part of it) but no other type can accept AB, whereas at the opposite end O can be accepted by any but can accept none. All the carbons, sugars and whatnot are the details of how the evolution took place.

Apparently this affects what we should eat according to our blood types but I'm an O type and as much as I'd like to eat steak and nuts and raw veg it'd get really expensive without bread or pasta to fill me up.",null,3,cdl4ckm,1r7v5g,askscience,new,4
SparklePonyBoy,"I'm surprised that no one mentioned CMV, cytomegalovirus.  When blood is tested, spun, etc., they also test for this because it can kill immunocompromised individuals, also babies.  I know this because I used to donate blood every 8 weeks and they told me I am O- CMV- and that each donation can save up to 5 babies' lives.  Supposedly up to half of the general healthy population may be walking around with CMV and not even know.

http://www.cdc.gov/cmv/overview.html",null,0,cdl4r5d,1r7v5g,askscience,new,1
A_Soggy_Sheep,"This is real pseudoscience, but i would hazard a guess that it is beneficial for the human race to have different blood types for survival, incase some kind of disease was incredibly deadly to a specific blood type...

A bit like [diversify your bonds](http://www.youtube.com/watch?v=FTsNEUZx8v8) .

As i said earlier i may be totally wrong, so feel free to patronise me into oblivion. ",null,1,cdkkwwv,1r7v5g,askscience,new,2
Whisket,"Scientist discovered the [chemical reaction series](http://www.google.com/imgres?imgurl=http://www.theozonehole.com/images/ozoned43.jpg&amp;imgrefurl=http://www.theozonehole.com/ozonedestruction.htm&amp;h=438&amp;w=580&amp;sz=29&amp;tbnid=2B5I5wnJEI6IvM:&amp;tbnh=90&amp;tbnw=119&amp;zoom=1&amp;usg=__VSGqa4LvGkOQioh7a5z697w7CNA=&amp;docid=n7L0JaEnpXluyM&amp;sa=X&amp;ei=5pCPUsSTKKW42wXn0IA4&amp;sqi=2&amp;ved=0CDQQ9QEwBA) where chlorine acted as a catalyst for ozone depletion.

CFC stands for chloro-fluoro carbons. For those without O-chem knowledge,  carbon atoms can bond with up to 4 other atoms. In CFC's, there are at least one chlorine and one flourine atoms bonded to the central carbon atom, with the rest being hydrogen atoms (there are multiple variations based on numbers and positions of these atoms). When these CFC's get into the atmosphere, they start to break down due to UV radiation, and chlorine atoms are released. These chlorine atoms then proceed to catalyse the chemical reaction series I linked earlier.

[Here's a link from the US EPA with more detail](http://www.epa.gov/ozone/science/process.html)

A quote from the article: ""It is estimated that one chlorine atom can destroy over 100,000 ozone molecules before it is removed from the stratosphere """,null,0,cdkk6rk,1r7s9w,askscience,new,3
richard_woodhouse,"It's a bit of a long process but there were 4 major breakthroughs associated with the discovery of the Ozone Hole:

1.)  **The Fate of CFCs:**  Mario Molina was a postdoc working with Professor Sherwood Rowland at UC Riverside in the 70's.  His project was to determine the fate of CFCs in the atmosphere.  CFCs were used as refrigerants for a lot of industrial processes because they were incredibly stable.  They happen to be so stable that nothing in the troposphere can break them down, so no one really knew what happened to them when they were released.  Molina eventually discovered that the CFCs would finally break down high in the stratosphere (~20-40km) when they absorb high-energy UV light (~175-220nm).  This releases Cl radicals into the stratosphere.  Their famous Science paper [here](http://www.nature.com/nature/journal/v249/n5460/abs/249810a0.html) (with a great typo in the title, see the [pdf](http://www.nature.com/nature/journal/v249/n5460/pdf/249810a0.pdf)) presented this initial warning that CFCs could deplete ozone.  However, this reaction needed observational evidence now.

2.) **Observations of enhanced ClO:**  In 1976 James Anderson of Harvard lead a field campaign to the south pole and made in situ observations of enhanced ClO.  This confirmed that chlorine was present in the stratosphere and verified Molina's proposed CFC fate.

3.) **Observations of the Ozone Hole:**  This one's kinda funny in hindsight, in 1985 Joe Farman published a paper in Nature ([here](http://www.nature.com/nature/journal/v315/n6016/abs/315207a0.html)) showing dramatic ozone loss in the austral spring.  This was in direct contrast to satellite observations over the south pole at the time.  It turns out that the satellite data was being filtered out because concentrations were so low.  Essentially, the scientists had assumed the satellite measurements must have been noisy and were throwing out data that was ""unrealistic""!

4.) **Clouds and Crazy Chemistry:**  The original chemical reaction chain that Molina proposed was not the cause of the ozone hole (the source of chlorine was right but the catalytic reaction mechanism was not).  The ozone depletion that Farman noticed occurred in the spring at first light in the south pole.  There's three reasons for this depletion:  (1) the polar vortex keeps the south pole somewhat isolated from the surrounding regions, (2) typically the chlorine can be ""locked up"" in reservoir species such as chlorine-nitrate, however Susan Soloman discovered that [polar stratospheric clouds](http://en.wikipedia.org/wiki/Polar_stratospheric_cloud) can form due to the extremely cold temperatures and provide a surface for heterogeneous chemistry to occur.  The chlorine nitrate can react with HCl to yield Cl_2 + HNO_3 and the HNO_3 will dry deposit over the winter.  So in the spring there's a lot of Cl_2 that will photolyze and no nitrogen to lock it up.  (3)  In 1986 [Luisa and Mario Molina discovered](http://pubs.acs.org/doi/abs/10.1021/j100286a035) that ClO can self react: ClO+ClO -&gt; ClOOCl and that the dimer will break at the Cl-O bond, not the weaker O-O bond.  This means that ClO self reactions can produce radical chlorine and deplete ozone without radical oxygen atoms!  

In summary, Mario Molina proposed that CFCs could deplete ozone in the stratosphere.  His original paper correctly pointed out the source of chlorine in the stratosphere but didn't identify the actual reaction that would cause the ozone hole.  2 years later Jim Anderson then verified that enhanced chlorine was present in the stratosphere.  10 years later Joe Farman noticed the ozone hole.  3 years later Susan Soloman, Luisa Molina, and Mario Molina discovered the crazy set of chemical reactions that lead to the ozone hole.

Edit:  Added a note about the typo in the Science paper title.",null,0,cdm3m9e,1r7s9w,askscience,new,3
Surf_Science,"So an antidote can vary widely but I'm not aware of any 'traditional' antidotes that might think of from the movies. 

In the case of venoms we often use antibodies generated by exposing animals to the venom. These antibodies bind the venom, may prevent it from acting and may help our body remove it (antibody opsonization). 

In the same way an antibody will bind a dangerous substance we will sometimes use chelators to bind other molecules that may be problems (heavy metals, iron). 

In some other cases the antidote may be using something that competes with the dangerous substance for a receptor. In the case of ethylene glycol poisoning for example (and this is what happens when a dog for example drinks antifreeze) ethanol can be used as a treatment. Ethanol will compete for the target receptor with the ethylene glycol preventing the action of ethylene glycol (ethanol has a higher affinity for the receptor).  ",null,0,cdkjlta,1r7rep,askscience,new,3
alex199119,"I guess there are two main ways that drugs (which could be an antidote) are discovered. 

It's possible to test a large number of chemicals in a number of different systems and see what effect they have and if any of those effects are positive or beneficial. So upon testing a certain chemical, you may find that the chemical in question works as an antidote for a certain chemical poisoning. 

Or it's possible to look biologically at the effect a poision has on a body, and then try to 'engineer' a solution to combat that poison. So I suppose in someways you could say the latter method derives the antidote from the posion, but not from the actual presence of the poison but from the way the poison has an effect on the body and an understanding of this.",null,0,cdktj6s,1r7rep,askscience,new,1
snusmumrikan,"Realistically there is no difference, which the anti-GMO lobby refuses to admit. The development of hardier and more productive crop strains has been done for thousands of years through crossing plants and hybridisation, along with the development of larger load-bearing horses and farm animals.

Our ability to do it from an informed position and alter specific genes through 'genetic engineering' only makes it more precise and targeted and less likely to have the Brassicoraphanus problems that Karpechenko had in the 1920s; where he tried to cross radishes and cabbages to get to root of a raddish and the head of a cabbage. Unfortunately he got the useless root of a cabbage and the useless head of a raddish in one plant.

For you paper are you considering talking about Armand Pusztai and his flawed release of information regarding GMO potatoes poisoning rats? - refuted endlessly by reliable peer-reviewed studies and yet still one of the major reasons GMOs have a bad name. Also Prince Charles and his irrational vehement fight against GMO without any expertise in the area itself?",null,0,cdkks93,1r7qtk,askscience,new,2
ToThink,What Borlaug essentially did was he maximized the growing efficiency of wheat as well as increasing the number of disease resistance genes in wheat to increase overall crop efficiency of wheat. He did this on the macro scale by breeding several varieties of wheat together and selected for the best phenotype. An example would be how he bred wheat which were more likely to resist cold temperatures together for many generations. He kept doing this with other environmental conditions until the wheat became very resistant to any extreme environmental conditions altogether. He did genetic engineering on the macro scale by actively selecting favourable traits (selecting for the best trait by sexual selection) over several generations of wheat.,null,0,cdkko4q,1r7qtk,askscience,new,1
Cherrysquid,"The ball will land back in the boat if the boat is moving at a constant velocity. When you throw the ball straight up the ball not only has your vertical velocity that you gave it with your hand, but also the same horizontal velocity the boat has. The only acceleration acting on the ball is vertical, gravitational acceleration. With no horizontal acceleration the ball will maintain the same horizontal velocity (velocity of the boat) until it lands.",null,0,cdkg2oq,1r7qcs,askscience,new,13
AleccMG,"Start by considering that you, the ball, and the boat are all moving at the same velocity (speed and direction).  In the frame of reference of the boat, your velocity is zero (as is the ball before you throw it).

When you throw the ball straight into the air, you give it an initial vertical velocity.  With respect to the boat, it has no horizontal velocity.  The ball's motion is now entirely determined by the initial conditions (how you threw it), and the forces acting on the ball.  Since we are neglecting air resistance, the only force remaining is gravity which acts towards the boat.

Since we have no force in the horizontal direction, and your throw imparted no horizontal velocity, the ball will not move horizontally from the perspective of you on the boat.  If you don't catch the ball, you'll likely end up with a good knot on your head!",null,0,cdkhifr,1r7qcs,askscience,new,3
ArmyOfFluoride,"This is a really big question you're asking, but I'll try to give a bit of a foundation for you.   If you've ever taken a class in the life sciences you've likely heard the phrase ""structure determines function"".  I first learned this in the context of proteins: the amino acid structure of a protein determines its biochemical function.  This is also true for tissues however.  The way cells are organized in your heart is what allows it to work as the core of your circulatory system.  Your brain is no different.  The structure of the tissue in your brain is (as far as we know) what determines its functioning.    You also likely know that the structure of your body is determined by both your environment and your genetics.  ""Instinct"" then, can be thought of as the behavior that arises from the brain structure that can be attributed to genetics, as opposed to the environment.  This is where things get messy, as ascribing any trait as complex as behavior to entirely a genetic or environmental cause is impossible, because we all have both environmental and genetic histories.  Does that help?",null,3,cdklzjv,1r7q41,askscience,new,18
bags_of_geckos,"
I think it’s easier to use the term ‘innate behavior’ rather than ‘instincts’ because the latter has a lot of more metaphysical connotations in humans, at least. I can describe an innate behavior in a simpler organism, like a fly. 

Fly mating behavior is what we call ‘stereotyped’- given certain stimuli (like a young female fly or something that smells like one) a male fly will initiate a series of behaviors that are pretty much the same every time: a little wing dance, an attempt to mount her, etc. The neurons that sense the odor of the female fly connect to a few parts of the fly brain, including an area called the lateral horn. It’s a little oversimplified, but neurons in that region connect with downstream neurons that control the fly’s muscle movements that cause them to do the dance, and more complex series of movements that include mounting. It’s a circuit of neurons that starts with a sensory stimulus and ends in a behavior, with a little processing in the middle. 

So where to genes come in? Genes are what tell the brain how to structure itself in the first place. In each neuron there are a series of genes transcribed that tell the neuron how to develop, what kind of neuron to be, where to migrate to in the brain during development. The neurons in the lateral horn that bridge the gap between the smell of a female (sensory stimulus) and the mating (behavior) are where they are, and fire when they fire, because of the history of genetic switches that got them there. 

I should add, there is no one gene for this behavior or that, every single cell is the product of thousands of genes turned on or off throughout the organism’s development. Cells (such as neurons) are the sum of their genetic history, and behaviors are the sum of the neural activity that was elicited by a stimulus. 

You can check out the work of [Vanessa Ruta](http://www.rockefeller.edu/research/faculty/labheads/VanessaRuta/), for a more in-depth explanation. I am probably butchering my description her work but you might find more of what you are looking for there.

Most of human behavior is learned, though we can suck as infants and grasp things, most of what we do during the day we had to learn at some point. We don’t have a specific brain structure devoted to drinking coffee that is the same for all humans. Less complex organisms tend to have a much greater percentage of their brains devoted to innate behaviors than we do.




",null,1,cdknup8,1r7q41,askscience,new,15
zzerrp,"Yeah this is a pretty huge field of research, involving people working on a variety of brain circuits and the genetics that underlie their function.  A couple examples: Hopi Hoekstra studies how genetics affect the shapes of the burrows that mice decide to build (simplification; see her [lab website](http://www.oeb.harvard.edu/faculty/hoekstra/projects_behavioral_genetics.html) ).  And all the people who study [FOXP2](http://en.wikipedia.org/wiki/FOXP2) and its associated gene regulatory networks, which are involved in innate ability to acquire language and vocalize.  It's a very cool field of research, but it's tricky.  One big reason is that there is a lot more to the story than just the sequence of bases in the DNA.  There are [some 250,000 exons and a similar number of introns](http://www.ncbi.nlm.nih.gov/pubmed/15217358) known in the human genome. That gives a rough idea of how many distinct genes our bodies have to work with -- half a million.  Compare that with the over 100 BILLION (prob closer to a trillion) neurons that are in the human brain.  There isn't anywhere even close to enough genes to specify the properties of each neuron individually.  So it's all about how the body uses the products of these genes in combinations, and the program of development that is set up by those combinations, such that a functional network of neurons grows.  Innate behaviors are driven largely by the particular forms of the connections that have a tendency to keep arising out of that biological program of development.  ",null,0,cdkwlqa,1r7q41,askscience,new,2
null,null,null,9,cdkmeh4,1r7q41,askscience,new,4
TehMulbnief,"Right up my alley! I worked at a food science company for a few months earlier this year and wouldn't you know it, we were developing caramel colorings!

I'm not too sure why ammonia would be added, or I should say, we never used ammonia in any of our colorings, but sulfites are used as a preservative. In fact, if you punch sodium sulfite into [wikipedia](http://en.wikipedia.org/wiki/Sodium_sulfite), one of the uses listed is a preservative.

Since sodium sulfite isn't exactly tasteless, there is a lot of pressure from consumers to eliminate its use in certain foodstuffs. Also, with the ever growing presence of chemophobia regarding food, consumers want less stuff added. However, sodium sulfite doesn't just limit bacteria growth.

The caramel coloring I was developing was actually based on a particular strain of corn starch. To extract the starch completely from the raw kernels, the corn was soaked in a dilute solution of sodium sulfite overnight. This helped loosen the intermolecular connections between the starch and cellulose present in each kernel. Higher yields of starch means more coloring which means lower prices for our buyers and their customers.

Let me know if you have any followup questions!",null,0,cdmxea3,1r7o3s,askscience,new,2
rupert1920,"Check out [pair production](http://en.wikipedia.org/wiki/Pair_production), where a particle and its antiparticle can be produced from a photon. This is essentially the reverse of [annihilation](http://en.wikipedia.org/wiki/Annihilation).

I'll also add that ""energy"" used to create ""mass"" is a bit of a an awkward wording, as both of these are properties of a system. It's akin to saying can ""height"" be used to create ""width"". In the example above, it's a case of particles with no rest mass creating particles with rest mass.",null,27,cdkftg9,1r7mfl,askscience,new,144
50bmg,"Absolutely. If you add enough energy to a particle, and collide it with another, you can create new particles. This happens in particle accelerators all the time, and it is the method by which we create and study antimatter (usually by bouncing protons off iridium and creating antiprotons). However, it does take a LOT of energy to do, which is why antimatter is the most expensive material (per gram) every created. ",null,8,cdkg0ub,1r7mfl,askscience,new,35
technogeeky,"I'm surprised to not see a direct and relevant answer.

**Yes** and this is exactly what high energy accelerators do. Really. All of them. Stanford's Linear Collider. The LEP. The LHC. The Tevatron (before it was closed).

You could ask yourself: If the [Large Electron-Positron Collider](http://en.wikipedia.org/wiki/Large_Electron%E2%80%93Positron_Collider) collided electrons and positrons (which weigh 1/1836 as much as a proton), how could that machine have possibly seen the W and Z bosons (which weigh ~80 and ~90 times more than the proton)? Combining the two, where did this extra ~150,000 electron masses come from? There isn't anywhere near enough mass to create these particles in the collision!

The point of particle colliders is to provide the particles with enough extra energy that they can produce interesting particles as outputs. Extra might be understating it a bit: the vast majority of the energy of colliding particles in these machines is the kinetic (motion) energy of the particles, and not the rest mass of the particles.

In other words, if a lot of energy **could not** be used to create mass, then particle physics would not exist at all.",null,3,cdkm8h5,1r7mfl,askscience,new,14
yinz_n-at,"Yep they're equivalent. But in the nuclear world, energy is very big and mass is very small (from our point of view). This is why nuclear power works (convert a small amount of mass into equivalent energy). So going in reverse will take a tremendous amount of energy but is still possible (particle accelerators). 

Look at Binding Energy per nucleon for the ranges of fission and fusion. About iron is the most stable nucleus 

Source: I'm a Nuclear Eng. ",null,2,cdkkfnm,1r7mfl,askscience,new,7
QCD-uctdsb,"In any experiment used to determine mass, pure energy (photons/gluons) is actually indistinguishable from matter. Imagine you have a mirrored box full of photons, and let's say that the total energy of all these photons is 9x10^17 joules. If you measure the mass of this box (say you put it on a scale) then from your mass-energy equation the box will have a mass of 10 kg. 

As another example, up and down quarks have a mass of roughly 3 MeV. You have 3 of these in a proton, so you'd guess that the proton would have a mass of 9 MeV. But nope!.. the measured mass is actually 938 MeV. This is because protons have extra ""binding"" energy (mostly in the form of gluons) which contribute to the total proton mass.

To answer your question then... yes, it happens all the time. If you wanted to create *matter*, well that's a different question. You would need some mechanism (pair production) and experimental setup (particle accelerators) for creating quarks and leptons from photons and gluons.",null,0,cdkli31,1r7mfl,askscience,new,1
JonathanFeinberg,"Indeed yes. A fast thing has more mass than a slow thing. A compressed spring has more mass than a relaxed spring. Of course, under ordinary circumstances, this differences are vanishingly small. But, as other answers have pointed out, you can actually create new particles from the kinetic energy of a collision of existing particles.",null,0,cdkms66,1r7mfl,askscience,new,1
_ridden,"To put it shortly, yes. The addition of energy into a system can be used to create mass, this is why in the Large Hadron Collider (LHC @ CERN) particles are sped up (thus giving them energy) and colliding two particles together, creating new particles which may overall have a greater mass then before.",null,0,cdkpi9s,1r7mfl,askscience,new,1
roh8880,"You're talking about matter to energy/energy to matter transference. 

The issue that has arisen during this research is in order to construct any material object is that you have to build each molecule and have them stabilize. It's the stabilization that is the problem, if I recall the article I read correctly.",null,0,cdkydl1,1r7mfl,askscience,new,1
jakkes12,This is what happens in CERN. As the particle that's being accelerated it gains energy.The energy is transformed into kinetic energy until it reaches a speed close to the speed of light. Nothing can go faster than light therefore the energy added cannot be transformed into kinetic energy at this point. Instead the energy is transformed into mass. The particle being accelerated will start growing.,null,0,cdl5a12,1r7mfl,askscience,new,1
null,null,null,16,cdkhys9,1r7mfl,askscience,new,6
MayContainNugat,"It feels cold precisely *because* it conducts heat so well. If the metal is at 20C, and your body temperature is 37C, then the metal will very efficiently conduct heat away from your body, and that feels cold. It does this much more efficiently than wood or plastic. ",null,9,cdkejkr,1r7l11,askscience,new,92
ww-shen,"Metals generally are good conductors, but they are good capacitors aswell. So they can 'take' many heat 'fast' from your hand. The cooling process will last as long as the temperature of the  metal is equal with your hand's. The metals inner temperature will be almost equal (becouse of the good coductivity) and will last 'long' to warm it (becouse of the capacitivy). Ceramics for example are good capacitors and bad conductors. It takes longer time to warm up, but they will radiate the heat longer. (coclkle stove). Plastics and air (and vacuum) are bad in both, so they can be used in heat insulations.",null,12,cdket3p,1r7l11,askscience,new,2
Das_Mime,"Tidal locking means that the same face of the Moon always points toward Earth.

This occurs because tidal forces--the difference in gravitational tug from the Earth between different parts of the Moon-- cause small distortions and stretches in the shape of the Moon. These distortions dissipate the moon's rotational energy, slowing its rotation until eventually ~~it stops.~~ its rotational period is equal to its orbital period.",null,5,cdkfbfz,1r7ivu,askscience,new,16
EdwardDeathBlack,"The positive/negative temperature scales of Celsius and Fahrenheit are arbitrary. The more common scientific scale is the [Kelvin](http://en.wikipedia.org/wiki/Kelvin). The Kelvin never goes negative. It goes towards zero, the famous ""absolute zero"", the temperature at which ""jiggling"" is at its absolute minimum (! But not zero jiggling , see [""zero point energy""](http://en.wikipedia.org/wiki/Zero-point_energy)). Absolute zero is -273.15 Celsisus, -459.67 Fahrenheit and of course, 0 Kelvin. 

As for why life isn't very fond of negative temperature, one place to look at is of course the freezing of water. We are mostly made of water, so freezing isn't very desirable. At atmospheric pressure, water freezes at 0C, 32F or 273.15K. Below those, things get trickier (but not impossible, plenty of life forms have evolved to live in freezing weather). 

There are other places to look to for reasons why life on earth doesn't strive in very cold weather , chemistry kinetics for exemple. But we can get to that if you have follow up questions. ",null,1,cdkjjky,1r7i1w,askscience,new,9
spookyjeff,"Besides what others have mentioned here, at low temperatures proteins can undergo ""cold denaturaiton."" Hydrogen bonding and the hydrophobic effect are dependent on temperature, they are also responsible for the shapes of macromolecules in the body. At low (and high) temperatures these molecules can lose their shape and function.",null,0,cdkrgt2,1r7i1w,askscience,new,2
ramk13,"Freezing is a major (but not the only) problem. Life depends on the diffusion or active transport of chemicals across concentration gradient. For example, if there is more sugar outside a cell than inside, some can diffuse in. When you freeze the cell, mass transport grinds to a halt. Chemical reactions also slow down as temperature decreases, but not all at the same rate. Also, freezing water expands and can rip apart cellular structure. The combination of all those things disrupts most of the basic processes in a cell (or organism).",null,0,cdkogih,1r7i1w,askscience,new,1
GruntingButtNugget,"The Rings are Saturn are much more dense than the asteroid belt. The rings themselves are actually seven sets of concentric circles that circle the planet. There are gaps between each set of rings big enough that Cassini was able to fly between two sets of rings on its way closer to the planet. Flying THROUGH a ring would be similar to how you would probably have originally pictured the asteroid belt

Edit: spelling is hard on a phone",null,0,cdkjzau,1r7hch,askscience,new,3
DarylHannahMontana,"One thing to note here is that the *definition* of an integral is ""the area between the function and the x-axis"" (ok, the real definition is a little more technical than that, but the main idea is the same; if you remember Riemann sums from your calculus class, *that* is the real definition of the integral^\*).

It is a *fortunate coincidence* (i.e. a theorem) that the integral of a function is related to its antiderivative; this is the fundamental theorem of calculus (and more generally, there is something called [Stokes' theorem](http://en.wikipedia.org/wiki/Stokes%27_theorem), of which FToC is a specific example).

So, if you can find an antiderivative for a function, that gives you an easy way to calculate an integral ""by hand"" (and, as others have said, there always *is* an antiderivative, but it may not be ""elementary"" (see JoshuaZ1's post)).

But if you can't find an antiderivative (and usually, you can't), you can just integrate using the definition (adding up the area of very narrow rectangles or [trapezoids](http://en.wikipedia.org/wiki/Trapezoidal_rule)) and get a numerical value that is as precise as needed.

That is, is some sense, using an antiderivative is the exceptional way of computing an integral, and numerical integration is the ""standard"" method.

\*: there's also a Lebesgue integral which is defined differently, but this is not the time to discuss it fully.

EDIT: misspelled 'Lebesgue', and added wikipedia link to Stokes' thm",null,1,cdki2kc,1r7hb8,askscience,new,11
JoshuaZ1,"The issue simply put is that there are functions without an elementary anti-derivative. For our purposes, a function is [elementary](https://en.wikipedia.org/wiki/Elementary_function) if it a combination of x, log, constants, exponentials, closed under addition, subtraction, division, multiplication, and composition. Note that we get trig functions from using exponentials with complex numbers, but if you want to stick to the reals you can instead throw in the basic trig functions and inverse trig functions. You'll get essentially the same set for the totally real functions but it turns out this is a harder definition to  work with.

[Liouville's Theorem](https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra\)) severely restricts what anti-derivatives can look like, and can be used to prove that there are elementary functions that don't have elementary anti-derivatives. Now, one can extend what one is working with to a larger set of functions. For example, one can also throw in the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function). But these functions only help so much, and they are about as hard to approximate as simply doing the numeric approximation of the integral you care about. But at another level, this isn't so bad, you are in practice going to need approximations even for elementary functions for practical purposes, so this is a comparatively minor inconvenience for engineers. Generally,  it is mildly inconvenient for physicists, and is substantially more inconvenient for mathematicians who may care about the exact behavior of a function, and may have to get that way from analyzing its derivative rather than the function directly. 
",null,2,cdkgcbp,1r7hb8,askscience,new,11
ohsohigh,"The approach we take to solving integrals is based on the fact that integrals are antiderivatives. To oversimplify quite a bit, we basically look at a function and say this looks like the derivative of this other function, so that must  be the integral. If a function doesn't look like the derivative of some other function then we don't know how to integrate it analytically and have to use a computer to approximate the answer.",null,2,cdkf9ma,1r7hb8,askscience,new,5
Sirkkus,"What exactly constitutes an analytical solution or closed-form solution is subjective and arbitrary. Take for example the differential equation dy/dx = y. Most people will say that this equation has an analytical solution, the exponential function. But what exactly *is* the exponential function? It can be defined in a number of ways, but ultimately if you want to calculate it's value you have to use some numerical definition. The only reason that the exponential gets to be considered an analytic solution is that it's very common and most people are very comfortable with it's properties.

Some integrals may not have solutions *in terms of other functions that we're familiar with*, and that's all we mean by not having an analytic solution. If a particular integral comes up a lot you could give it's solution a name and include it in your new definition of elementary functions. This could be useful later if you find that some other complicated integral can be written in terms of this one, meaning you may not need to write a new numerical algorithm to compute it.",null,2,cdki7sj,1r7hb8,askscience,new,6
doctorbong,"In addition to the good answers already here, I'd like to point out [this thread](http://mathoverflow.net/questions/66377/why-is-differentiating-mechanics-and-integration-art) on MathOverflow. In particular, I like Terry Tao's answer. In summary, integration is a global operation: In fact, when one defines a Lebesgue integral, a pointwise definition of a function isn't even needed. On the other hand, the derivative is an *extremely* local property of a function - it only deals with limits over arbitrarily small neighbourhoods of a point.",null,1,cdl0l9k,1r7hb8,askscience,new,5
hideous-bike,"A differential allows for the driving wheels to spin at different rotations per minute therefore allowing a car to turn corners. This is because the inside wheel in a turn has less distance to cover than the oudside wheel on the vehicle in the same turn. 

Think plastic coffe cup on it's side. It will roll in a circle / turn because the bottom is narrower than the dinking end of the cup. What a differential essentially does is give power to the wheel with the least resistance. This is a desirable trait in wheels on a road surface, because otherwise the driving wheels start to squirm and bounce. 

In mud however this is not, because when a drive wheel looses traction and starts to spin the power of the engine will just be sent to the spinning wheel and therefore not adding to the traction of the vehicle. To overcome this some cars can lock their differentials to ensure that both wheels will start churning at the same speed giving a lot more traction to the vehicle.

On a side note: don't lock your diffs on a road surface (if your car has the option). That is, unless you really like your mechanic, and like to give him, or her, fist fulls of money.     

",null,3,cdkdzw6,1r7e4c,askscience,new,24
stairwaytoheaven57,"Where did you hear this info?  Locking differentials are almost always paired with four wheel drive.  It certainly does get more traction in addition to 4x4, but a locking differential without 4x4 wouldn't be as effective.  With four wheel drive you have power going to every wheel unless you lose traction in one wheel from the front and one wheel from the back, but until then you're pulling with all the tread so it's less likely that you'll get in a situation where you lose traction.  With just a locking differential you would have to lose traction to both back wheels which is more likely.  First, the back generally has less weight on it, second, you can steer the front wheels back and forth to try to grab onto the sides of the hole, and lastly, without 4x4 the front wheels would be introducing drag instead of pulling. ",null,3,cdkeg17,1r7e4c,askscience,new,23
MrPickleCoppter,"Locking differentials are best suited for off-road or low traction environments. With a locked differential both drive wheels will turn at the same speed making turning more difficult if you have full traction.  This will give more wear and tear on your tires on asphalt and your tires will chirp as you go around a turn.

I have just installed an auto locker in to the front differential of my Jeep.  Its is only active if I put it in 4 wheel drive and if I apply gas.  If 4 wheel drive is not engaged it will still preform like an open differential but with a ratcheting mechanism.

[install process of the locker](http://imgur.com/a/HlDGq) 
 ",null,0,cdkfdva,1r7e4c,askscience,new,8
LibertyBill,"A locking diff isn't always necessarily better than 4x4. 
Not only that but 4x4 accompanied with a limited slip rear diff is much better suited to everyday driving due to the ""flexibility"" of a limited slip diff vs a locked diff.

A locking diff would have very limited use in a primarily road vehicle. This is mostly due to the fact that the tires will spin at the same speed even when taking turning into consideration. On pavement this will cause ""wheel hop"" and a lot of strain is put at the diff wearing it down/damaging it. Wheel hop will occur in vehicles with 4x4 with open or limited slip diffs as well but these options are more ""giving"" in terms of stress on the diffs.

Also, it's a myth that 4x4 means all four tires have ""power"" going to them.  The only case this would apply to is if both the front and rear diff are locked. I believe this option exists in some Hummer models. In many ""All-wheel"" and 4x4 designs there is usually one or two ""open"" diffs involved. An example would be my Tacoma. It has 4x4 but both the front and rear diff are open. This means (unfortunately) the energy will be routed to the tires that give the least resistance. I say unfortunate as these tires will spin in place. To clarify this point even more I actually got my truck stuck in 4x4. Both my front left and rear left tire were on slippery ground.  I was however able to get out by cycling the steering wheel back and forth.

So to answer your question: Given all the possible scenarios of ""4x4"" I'd still take the 4x4, even with both front and rear open diffs, over just a locked rear dif any day.",null,0,cdkkbbr,1r7e4c,askscience,new,2
pbae,"Most cars have ""Open Differentials"" which already has been explained, let's the outside wheel turn faster than the inside wheel when a car is making a turn.

A MAJOR disadvantage of an ""Open Diff"" is that the wheel with the LEAST amount of traction gets the MOST power provided by the engine.  For example, if you take a car with an open diff and jack it up on either the left or right side of the car and then rev the engine, the wheel that's in the air is the one that is going to spin, not the wheel that is touching the ground.

Now imagine being off-road.  If one of your driven wheels gets stuck in the air, you would be stuck because your car or truck isn't going anywhere because the only wheel getting power is the one in the air.  You would be able to get out of this situation if you had a locking diff since you could lock the diffs and send power to both wheels at the same time.

And to answer the OP's question, a rear-locking diff is better than four wheel drive (4WD) for the reason explained above because some 4WDs have two open diffs.  The best would be 4WD with a locking diff.

",null,0,cdkq885,1r7e4c,askscience,new,1
patchgrabber,"Cats are [obligate carnivores](http://en.wikipedia.org/wiki/Carnivore#Obligate_carnivores), meaning that they need to get all their nutrition from animal sources, and they also lack the digestive enzymes to break down plant material. Kibble can be used as a food source because it contains some animal meat and likely an extraneous source of taurine, which is very important for cats.

Dogs, however, are also carnivores. Their short intestines and colon are not well-suited to digesting plant matter either, as most digestion for dogs is in the stomach and they lack the long intestines required for proper fermentation of plant matter by intestinal microbes, although they do have higher levels of amylase as [this](http://news.sciencemag.org/plants-animals/2013/01/diet-shaped-dog-domestication) study found. This means that over time having been domesticated and fed starchy foods, they have somewhat adapted to be able to process these foods a little better. This is similar to people; those from Europe and Asia have more amylase production than people from Africa. It is not advisable to only feed your dog vegetables, as they are still carnivores like their wolf counterparts. Dogs' stomachs are also different than a feline's. Dogs have strong muscles that can push large and confusing things through their digestive system. An old veterinarian friend of mine would tell me stories of socks and other assorted weird things being passed through a dog's system.",null,3,cdkeiwk,1r7cxp,askscience,new,20
mrsix,"Cats are [Obligate carnivores](http://en.wikipedia.org/wiki/Obligate_carnivore#Obligate_carnivores) - their metabolisms are unable to produce a few essential nutrients out of vegetable matter, and therefore take it in directly from animal protein. A very important one specifically for cats is [Taurine](http://www.vcahospitals.com/main/pet-health-information/article/animal-health/taurine-in-cats/3857), which [some of the first 'cat food' in the 50s-70s lacked](http://www.onlynaturalpet.com/knowledgebase/knowledgebasedetail.aspx?articleid=113) which unfortunately lead to many cat's premature deaths until about the 80s. Most animals are able to produce it from methionine which is commonly found in plant material, whereas cats simply ingest it from other animals.",null,3,cdkehqp,1r7cxp,askscience,new,7
arumbar,"There is a 'minimum' lung volume - this is known as the [residual volume](http://www.admit-online.info/fileadmin/materials/images/cd_rom/1221_lung_volume.gif).  This is the volume of air that remains in the lungs after maximal expiration.  You can see some of the other typically referenced volumes in that figure there - the narrow band in the middle (tidal volume) represents air movement during regular breathing, while the peak (inspiratory reserve) represents how much additional air intake occurs during maximal inspiration.  Similarly, expiratory reserve tells you how much additional air you can blow out to reach residual volume.  These values are often clinically very significant in describing pathologies of various lung diseases (for example, someone with restrictive lung disease such as pulmonary fibrosis typically has reduced volumes due to inability to fully expand the lungs, while someone with obstructive disease like COPD/asthma may have larger volumes due to air trapping).  We also use [spirometery](http://en.wikipedia.org/wiki/Spirometry) to further characterize how air moves in and out of lungs.

Outside of intentionally breathing out to collapse a lung, this can also happen with a pneumothorax, where the pressure gradient normally created by the pleural space is lost (eg due to a puncture wound to the chest).  This can result in collapse of the [lung on one side of the chest](http://www.daviddarling.info/images/pneumothorax_radiograph.gif) - note the loss of pulmonary markings on the right side of the chest (left side of the picture).",null,0,cdkejtb,1r77hw,askscience,new,7
Astromike23,"First off, MAVEN does *not* have the ability to detect methane. There were plans to include an instrument that could do so, but it was removed due to budget cuts. Such an  instrument will likely be included on the 2016 ""Trace Gas Orbiter"", though.

With that said, it's still an important measurement to make with a spacecraft. Over the past few years, we think we may have detected methane on Mars using Earth-based telescopes - but this is a very hot topic, and very controversial. There are quite a lot of people in the planetary science community that are not convinced by the evidence, and yet also quite a lot who have based their research on it being true.

The biggest problem here is that it's an exceptionally difficult measurement to make from Earth, because our own atmosphere's methane gets in the way. When looking at a spectrum, it's hard to tell exactly what's coming from our planet, and what's coming from Mars. By measuring this from outside Earth's atmosphere, we can instantly remove that source of confusion from the data. 

The current data taken from Earth is pretty ragged and down in the noise. It's equally suspicious that the methane seems to come and go in both location and time, with only some groups detecting it, and others not able to do so (the Curiosity rover, so far, has not). Maybe it's an exciting dynamic process that suggests Mars is not a dead planet...or maybe it's just imprecise measurements. 

Either way, it's an important enough result that it bears checking with greater precision by a spectrometer outside the Earth's atmosphere. The Hubble would have a difficult time doing this - the easiest place to measure methane is in the mid-infrared (methane has strong spectral lines at 3.3 and 7.8 micron wavelengths), which is outside the range of Hubble's spectrographs.

**TL;DR**: MAVEN cannot sense methane due to budget cuts, which is a shame. From Earth, methane is very difficult to detect on Mars because our own methane gets in the way of the observation.",null,3,cdkdp3s,1r75ya,askscience,new,6
adamhstevens,"MAVEN isn't going to measure methane at all. Its entire science goal is to measure water loss in the upper atmosphere. I'm pretty sure it couldn't measure methane if it tried. However, the recently launched [Indian mission](http://en.wikipedia.org/wiki/Mars_Orbiter_Mission) and the European [Trace Gas Orbiter](http://en.wikipedia.org/wiki/ExoMars_Trace_Gas_Orbiter) both have instruments essentially devoted to detecting methane.

As to your actual question, we can do it from Earth and it has been done, and actually terrestrial results are the most convincing. (e.g. [Mike Mumma's results](http://images.spaceref.com/news/2009/Mumma_et_al_Methane_Mars_wSOM_accepted2.pdf)). However, some people (quite rightly) question terrestrial spectra. This is because there's an awful lot of methane in the Earth's atmosphere (roughly a part per million - ppm) compared to Mars' atmosphere (if there is any - it's around a part per billion - ppb) so you have to 'correct' for the terrestrial atmosphere. This can be done, but only to a certain level of accuracy (since the atmosphere gets stirred up a lot), which turns out to be about accuracy you need to be able to detect methane on Mars at all. However, there's a bit of leeway given by the fact that spectra taken from Earth of Mars are either slightly red-shifted or blue-shifted depending on the planets' relative motion. This means you should be able to distinguish martian methane by a slight shift in the methane absorbance. Unfortunately there are other issues that mean this isn't quite as straightforward, either.

So basically this all means that we need to send instruments to Mars so they can image the atmosphere without interference. Curiosity's in-situ experiments set the level of methane as having to be very low (they didn't detect any, so it must either not have been there or be there at a level below the instrument's precision) but that was only measured at one place on the planet at one time, and the measurements we do have suggest that any methane is highly variable in the atmosphere, so really we need long term observations on a planet-wide basis, which is what orbital spectroscopy allows us to do!

Anyway, this is actually what I'm doing my PhD on so if you have any more questions, feel free to ask.",null,1,cdkdppm,1r75ya,askscience,new,6
neha_is_sitting_down,"No idea, but if we haven't found it yet, odds are that we can't detect it from here using regular techniques (maybe it is very scarce. Or maybe we don't have the right conditions to use spectrography) and so the only way to be sure is to go there and measure directly.

Just thinking about this, the only way to do spectrography at that distance would be to have a light source behind the planet (correct me if I'm wrong about this). Because mars is outside our orbit, it will never be between us and the sun, so we will never have that light source.",null,4,cdkcp54,1r75ya,askscience,new,2
Lillelyse,"Yes it does. The time it takes for signals to reach our brain depends on the length of the axons involved. The speed of an action potential, depending on the nerve, is generally very fast, so delays are in the ms range. 


Taller people, who need longer axons to connect one part of the body to another, will take longer to perform for example a knee jerk reflex as the signals will have to travel a longer distance. When it comes to things like vision and hearing, I don't think there will be much difference between pathway lengths from person to person (unless you have a massive or abnormally small head), so any differences will be miniscule. This does of course not apply to people with diseases that affect neurone ability to propagate action potentials, such as MS. ",null,0,cdketnw,1r7531,askscience,new,3
clockwerx,"You may also be interested in learning about blindsight (http://en.wikipedia.org/wiki/Blindsight), which suggests ""comprehension"" of an image takes place very differently from many of the earlier stages; or why we have certain frame rates (http://en.wikipedia.org/wiki/Frame_rate#Background); which imply our brains are stitching together inputs into a seamless experience.

Interestingly, that stitching together of information that our brains do is far from perfect - Optical illusions (http://en.wikipedia.org/wiki/Optical_illusion) demonstrate we've got a few helpful rendering optimisations that work for most, but not all scenarios.

When you think about it, it suggests we live in a slightly laggy simulation of reality just kind of guessing our way through it all. 

It works well enough to avoid being eaten by large animals or falling off cliffs, so after that the pressure is effectively off for our bodies to get any better at this sort of thing.
",null,2,cdkf3gv,1r7531,askscience,new,6
batmantis25,"Our brains actively compensate for this ""lag"". Effectively, we generate images before we actually ""see"" them. I've seen sources which suggest a lapse range amongst individuals but I'm having trouble finding that at the moment.
http://www.livescience.com/4950-key-optical-illusions-discovered.html",null,0,cdkhcqq,1r7531,askscience,new,3
SimpleBen,"OK, some numbers. 
  
Sense of touch: Light touch  
Finger to brain: 20-30 msec  
Toe to brain: 50-60 msec  
  
Sense of burning pain  
Finger to brain: 1 second  
Toe to brain: 2 seconds  
  
Itch:
Finger to brain: 2 seconds  
Toe to brain: 4 seconds (ever wonder why that mosquito seems to get such a good head start??)  
  
Sense of vision to brain: 30 msec or so  
  
Sense of hearing: Ear to brain
About 10 msec
  
These time lags will vary from person to person, but not that much. Light touch, for example, may have a 5 msec range across people.",null,2,cdkinxn,1r7531,askscience,new,4
afcagroo,"In addition to the large sensory lag, there's a small lag that ensures that we are always living in the past.  If you have a CPU that executes instructions at a rate of 600 MHz, that means that it completes one instruction every 1.67 nanoseconds. Which is how much time it takes light to get to you from a computer monitor 1/2 meter away. ",null,0,cdki8c8,1r7531,askscience,new,2
Jyesss,"Yes, the action potential must travel from the nerve through the spinal cord to the brain, the brain must process the information, and then send an effector response back. The rate that the signal can travel along the nerve can be increased by increasing the diameter of the nerve. This is important for larger organisms, and a lot of what we know about nervous transmission was learned by studying octopus' nerve. They are huge and can be studied under just a light microscope. Mammals developed another way to expedite nerve transmission via myelination. This helps to insulate the nerve signal and increase rate. There is a theory proposed that posits that athletes with fast response times is due to the body further myelinating the appropriate nerves to convey the action potential faster. ",null,0,cdme9lo,1r7531,askscience,new,1
DEATH-OF_RATS,"What others have said is more or less correct. There are a few stipulations, though they're tangential.

The knee jerk reflex only changes with length of the nerves between muscles around the knee and where they they hit the spinal cord. It's a locally generated spinal reflex, so the brain has no part in it (this is a general definition of a reflex - that it's generated in the spinal cord).

In addition to some variance of axon size across people, different sensory inputs have different axon conduction speeds. Proprioception is a very fast sense, where vision is relatively slow. /u/SimpleBen summarized delay times nicely. As /u/batmantis25 pointed out, the brain also compensates for the lag by extrapolating current vision to what we should expect to see next.

The same thing happens in the sensory system with tactile and proprioceptive inputs. The brain predicts what future tactile and proprioceptive input it expects to receive based on the current tactile and proprioceptive inputs in combination with the current motor output.

This sharing of motor output is called ""efference copy"" - the motor cortex sends commands directly to sensory cortex, as well as to muscles. The sensory cortex can use that information to predict what it should next feel, allowing much better control than systems that are purely reactive. This is one problem you run into with rudimentary robotics - they adjust activity to compensate for sensed (past) inputs, instead of having a good generative model of what should come next.

What this all means at a higher level is that, in very trained movements (like an olympic wrestler executing a take-down during a match), your brain knows how to predict what will come next based on current input, because it has a very good model of how the system acts. Here, ""the system"" includes the player's own body, the floor, the opponent... everything that interacts. As a result, the athlete knows quite well what move their opponent is doing once the opponent starts the move, which means they know what the future state of the opponent will be, and can respond to that future instead of the present, which is really the past. A novice wrestler does not yet have a good model built and relies only on reaction time (usually in the 200-400ms range), which is usually not enough to compete against someone with training.

The idea of ""compiled"" actions also comes into play, and are the reason for repetitive drills. Instead of having to process each step in a sequence of moves, a trained person can initiate the sequence and the body automatically completes it; this is the idea behind muscle memory. This is why you don't have to think much about a practiced movement (like tying your shoes).",null,0,cdmjahz,1r7531,askscience,new,1
Platypuskeeper,"There's a closed-form solution for a Dirac potential, with a bound state. (and for the double-Dirac potential as well)

I don't really see the connection. Our knowing about a closed form solution doesn't make those stable, bound states any more special than the stable, bound states of potentials that can't be solved analytically. Bertrand's theorem is inherently linked to trajectories - which quantum particles don't have (or at least not in that sense), and conservation of angular momentum - but the ground state solution to the Schrödinger equation for a central potential is always l=0. 

",null,2,cdk9tp8,1r74bj,askscience,new,4
proule,"Animals like cats raise their fur with the help of arrector pili muscles in each hair follicle. This serves a myriad of purposes, from thermal insulation (less movement of air close to the animal's skin) to intimidation (hair stands up, animal looks bigger).

Humans actually have this arrector pili muscle, but it's strongly reduced in function. Humans do actually raise their hair, but the main thing you'd notice is goosebumps. You'll get goosebumps in response to cold (again, to reduce air flow around the skin), but since our hair isn't very thick in most places, this largely isn't effective anymore.

The arrector pili muscle in humans is thus largely vestigial (a remnant of a structure that served a purpose that isn't necessary anymore). 

This weakness in our arrector pili means that the hair on your head just largely will not move, even with these muscles engaged. They are, however, still present in the scalp, and if you get goosebumps you can feel the action of your arrector pili on your scalp. You know the sensation, it's just not as readily obvious as when a cat's hair stands up.",null,2,cdkmc4v,1r737x,askscience,new,5
iorgfeflkd,"The quantity that is often cited as being the best measured is the magnetic moment of an electron (related to how an electron responds to magnetic fields). It is very close to two, and half the deviation from 2 has been measured as 0.00115965218073(28), where the brackets indicate uncertainty on the final digits. This means the magnetic moment has been measured to a part per trillion.

The theoretical value, based on quantum electrodynamics, is .001159652180{85(76)}, where I put curly brackets where it deviates from the experiment (within the uncertainties). This is the most accurate verified prediction in science.",null,3,cdk98m9,1r734s,askscience,new,11
Trill-Nye,"A very [recent experiment](http://arxiv.org/pdf/0709.2996.pdf) managed to make a measurement with an accuracy approaching the Heisenberg uncertainty limit. This means that the uncertainty associated with this measurement is roughly as small as is physically possible, because quantum mechanical considerations preclude precise knowledge of the state of a system at higher accuracies. 

This measurement was of the interference of two photon that were only slightly out of phase with one another.",null,0,cdkifpr,1r734s,askscience,new,1
Local_Motion,"Interesting question. Antibiotics can disrupt the normal gut flora. This can be seen with penicillin-based antibiotics and cephalosporins, among others. 

What's more interesting to me is the end result. You are treating one infection, but in the end can cause another ""infection"". These antibiotics, when they disrupt certain gut flora, allow the excessive proliferation of other gut flora, specifically Clostridium difficile. C. diff can overgrow since the competition has been reduced, and with it's toxins, it can produce pseudomembranous colitis and some bad diarrhea. 

So what do you do? Treat it with another antibiotic, namely, metronidazole. Makes the differences in the bacteria structure/virulence and antibiotic mechanism of action an interesting study.

Source: A lowly med student",null,5,cdkgzr3,1r72qt,askscience,new,22
null,null,null,22,cdkad4i,1r72qt,askscience,new,32
HushaiTheArchite,"One of the proposed functions of the [appendix](http://en.wikipedia.org/wiki/Vermiform_appendix#Maintaining_gut_flora) is preventing this kind of gut flora-pocalypse from having a long term impact. That said, gut bacteria can get very far out of whack and have been thought to contribute to some illnesses of the colon. You're interested, look up stool transplants. They're simultaneously disgusting and a really fascinating example of the interaction between humans and their bacteria.",null,0,cdki6j5,1r72qt,askscience,new,4
ModernTarantula,Most digestion does not involve bacteria. So you will still get calories and proteins. But diarrhea is a common complication of antibiotics. Most absorption of antibiotics takes place in the mostly sterile small intestine.  The colon mainly absorbs water and is the locale of bacteria.  A serious compliction is altering the gut flora to have more serious and infectious bacteria (*Clostridia dificile*) ,null,1,cdkc31q,1r72qt,askscience,new,4
null,null,moderator,2,cdkjwbi,1r72qt,askscience,new,2
codyish,"It can and does.  Not enough to kill it all off but enough to inhibit digestion of some food that rely heavily on gut fauna. AFAIK, this is one reason why it's recommended to not consume dairy while on a antibiotics. ",null,10,cdkabfh,1r72qt,askscience,new,10
tehgreatblade,Why does any body part make cracking noises? Is this a sign of injury or is it benign?,null,0,cdkue60,1r72q9,askscience,new,2
Zangetsux20a,"Between the joints in your body are little pockets of synovial fluid. These pockets contain gases like nitrogen and CO2 bubbles.They're there to prevent your bones from rubbing up against each other when your move around. The neck has these joints as well, and the same synovial fluid pockets. When you ""crack"" a joint, you deform the shape of the pocket. This increases the pressure on the bubbles in the synovial fluid, with make a popping sound. ",null,0,cdlxta9,1r72q9,askscience,new,1
expertunderachiever,"You can see forever.

What you mean to ask his how finitely of an arc can I resolve at a given distance.  

The definition of 20/20 vision for instance [see http://en.wikipedia.org/wiki/Visual_acuity] means that at 20 ft you can tell two lines that are 1 arc minute [about 1.75mm] apart.  So you could see something 100ft away but it would have to be separated by at least 89mm.  At 1km it would be 290mm, etc and so on.

So given enough brightness you could see two lights from 100,000km away if they were separated by about 30km.
",null,2,cdkfhfg,1r72q2,askscience,new,15
RetraRoyale,You'd be able to see to the edge of the universe. All it takes for you to see something is for light to enter your eye. The only reason you don't see that far in some directions is because there are objects in the way.,null,1,cdk9y4b,1r72q2,askscience,new,2
erlegreer,"Absent of haze (atmosphere) and other objects, you would be able to see objects at least up to millions of light years away.
source: Normal healthy naked human eyes can see the Andromeda Galaxy, which is about 2.54 million light years from Earth.",null,0,cdka0uz,1r72q2,askscience,new,1
I30T,"It depends on what you want to observe.
Technically you can observe as far as an object can give light to. Stars can be observe from light years away. However if you want to observe another person then (remembering from 7 years ago) you can only see about 300 miles.

VSauce has done a video on this.",null,0,cdkdxly,1r72q2,askscience,new,1
Dominus_,"Theoretically, to the end of the observable universe (14 Gigaparsec, or 45 billion lightyears), but in reality, your eye cannot see anything that takes up less than 0.07° of your field of vision (average). 

This is equivalent to about 1.2 mm at a meters distance. So at one and a half kilometer, you would just barely be able to see an ~average person. ",null,0,cdkewbt,1r72q2,askscience,new,1
temuchan,"The book ""Why we get sick"" by Nesse and Williams discusses this in one of the chapters.  Natural selection selects for genes that increase an organism's ability to pass on their own genes (reproduce and make sure your offspring reproduce), regardless of whether or not the individual desires the trait associated with that gene.  In humans, miscarriage occurs in a fairly high percentage of pregnancies, often without the mother ever knowing she was pregnant (This is why IVF clinics implant fairly large numbers (~7) of fertilized eggs).  Nesse and Williams discuss that myopia (near sightedness) may decrease the percentage of pregnancies that result in miscarriages.  So even though someone with myopia may not want myopia, the trait will be selected for if it means that person will have more offspring than a non-myopic person.  Edit: Also, having myopia today is not as life threatening as it may have been for cavemen.  If you live in New York you are unlikely to be eaten by a tiger regardless of how good your vision is!

Another example of this is Huntington's disease.  ~~The fact that it has not disappeared yet means there is some selective pressure for keeping those genes.~~ (Edit: See Darkaardvark's comment)  Huntington's symptoms tend to begin around age 40.  Since many people reproduce before age 40, the debilitating effects of the disease do not decrease the ability to reproduce and the gene is not selected against.

Edit: More food for thought, if a hypothetical mutation appeared that caused exceptional vision, but also made the person sterile, it would never be passed on, no matter how desirable eagle eyes are.",null,3,cdkcm8o,1r72ls,askscience,new,19
Klinefelter,"optometrist here: one of the theories for the increase in myopia is that an increased amount of near-work increases myopia. so for instance, studying in school have been shown to make you more nearsighted. There has also been shown to be a genetic component for refractive error. Since glasses can correct for refractive error, poor uncorrected eye sight does not make you less likely to be able to reproduce so poor eye sight is still prominent in our society. ",null,0,cdkfzcw,1r72ls,askscience,new,5
atomfullerene,"Myopia is a disease of modern life, like diabetes, asthma, and obesity.  In premodern societies, it is very rare.  Even in most modern societies, it has vastly increased in prevalence due to environmental condition in the recent past.

It is _not_ the case that selection doesn't select against nearsightedness.  Nearsightedness has historically been a huge fitness disadvantage, and only within the last few generations (since the spread of eyeglasses) has this decreased.  That's not enough time to cause eyesight to degenerate, since when selective pressure is removed from a trait, it tends to remain the same, not degenerate (except over very long timespans)
",null,0,cdkmq7q,1r72ls,askscience,new,1
bdlxb3,"I'm willing to bet you start to lose your eyesight around the time you lose the ability to reproduce. Evolutionarily speaking you are a waste of space after you turn 30 or 40. Yes I know there are advantages to having grandparents alive to care for their young, but I don't think Cro-Magnon cared too much about his 401K. People with truly horrible eyesight are able to survive thanks to modern science, just like diabetics, pussies allergic to gluten, and fat-asses with bad thyroids. Still a human with sub-par sight has better sight than other animals. 
",null,5,cdkza88,1r72ls,askscience,new,4
PepperJack_delicacy,"To understand prolapses, you'll need to know just a little bit about the anatomy of the pelvis. Basically, all the pelvic organs (bladder, intestines, uterus, vagina, etc.) are held in place by a sheet of muscles and ligaments referred to as the **pelvic floor**. Here is a very short clip that gives you a 3D view of what's going on down there: http://www.youtube.com/watch?v=cWZdRebxsdM
If you take a look, you'll see that it pretty much acts like a ""net"" holding the organs in place and has passageways that let the anus (and in women, the vagina) pass through. 

The immediate cause of a rectal prolapse is a weakening of these muscles and ligaments. **Once the weight bearing down exceeds the strength of the supports, you'll get a prolapse**. What exactly causes the weakness though? The important thing to keep in mind is that we don't know the **exact** causes but there are several conditions that can increase the chances of you getting a prolapse. I'll try to outline some of the biggest factors.

* **Straining**: If you have a long history of straining while pooping because of chronic constipation, you're more likely to wear out the muscles.
* **Old age**: As you get older, your muscles get weaker in general. The same applies to the pelvic floor muscles.
* **Nerve damage**: If the nerves that supply the pelvic floor muscles don't work properly, the muscles will have a harder time supporting the organs.

Again, since we don't know the exact causes of a prolapse, it's a bit hard to say for certain why women experience it more often. The most likely explanation is that women experience pregnancy and child birth, which absolutely do weaken the pelvic muscles. 

If you're worried about getting a prolapse, remember to have enough fiber in your diet and to drink adequate amounts of water. If you notice that you're constipated a lot and having to strain, have it checked out by a doctor. 

Let me know if you want me to clarify anything.

Sources: 
http://www.emedicinehealth.com/rectal_prolapse/page12_em.htm#prevention
http://my.clevelandclinic.org/disorders/rectal_prolapse/hic-rectal-prolapse.aspx",null,0,cdkmnbq,1r72is,askscience,new,3
GoThirdParty,"There is a canvas of muscles called the levator ani. Think of it as a basket on the bottom of your pelvis.

The other thing that holds them in is that your guts are on a mesentery. This is basically a membrane that the colon hangs on and is anchored to the posterior intraabdominal space.

EDIT: [Here is a video.](http://www.youtube.com/watch?feature=player_detailpage&amp;v=Qw-97RU2NFs#t=182) The mesentery is the flappy looking thing the colon is on.",null,0,cdkmhiq,1r72is,askscience,new,1
henriquetk,"bubbles are a delicate equilibrium of forces. It problaby bursts when other force is applied to it( example the contact between the bubble and other surface).The bubble can also burst when it's surface dehydrate and can't no longer sustain it self.
I'm not sure, any scientist can correct me.
Sorry for the poor english, I hope I've helped you. ",null,0,cdkdwvm,1r7118,askscience,new,2
Trill-Nye,"Bubbles are usually characterized by a force balance between the pressure of the gas inside the bubble and the surface tension of the material comprising the bubble film. The pressure pushes outwards on this surface, which would make the bubble expand. However, the surface itself resists this expansion, because it would require stretching of the film. This is similar to a balloon, the more air you blow into it (increasing the internal pressure) the more the surface stretches. 

Anything that affects the mechanical properties of the bubble surface, such as heat or contact with another surface, can cause the bubble to pop. The real source of the popping is usually either a decrease in the strength of the surface material or an event that causes the escape of the gas inside, similar to the popping of a balloon.",null,0,cdki6x8,1r7118,askscience,new,2
blakemerkes,"Not too sure, and not my area of expertise. But I do know that many cows are being fed Corn in farms to fatten them up. And one of the side effects is the buildup of pathogens in their alimentary canal. But I believe that is because corn contains much less fibre than grass.",null,3,cdkagdg,1r70u0,askscience,new,2
iorgfeflkd,"The speed of sound in any real material can't exceed the speed of light.

The speed of sound in the early universe, confirmed through observations of the large scale structure of universe, is believed to be the speed of light divided by the square root of three. (1.7x10^8 m/s).",null,0,cdk8flo,1r70oa,askscience,new,12
Daegs,"iorgfeflkd is of course right, but the question itself shows a misunderstanding of relativity.

The speed of light is not simply a ""speed"", it is a property of our universe that literally changes reality around us to remain true. Both space and time change (length contraction / time dilation) in order to ensure the speed of light remains both constant and unattainable for anything to travel faster.

",null,0,cdku9h9,1r70oa,askscience,new,1
Das_Mime,"There's no evidence that the fields or radio emissions from alternating current have any negative health effects, attempts to detect things like claimed [electromagnetic sensitivity syndrome](http://en.wikipedia.org/wiki/Electromagnetic_hypersensitivity) (not a recognized medical condition) have all turned up the result that there is no detectable effect on people.

Any conductor will stop a radio wave. Some electronics are shielded inside what are known as [Faraday cages](http://en.wikipedia.org/wiki/Faraday_cage), which is basically where you surround it with a mesh of conducting wire. This is commonly used near radio telescopes, where the radio waves from electronics might otherwise interfere with the telescope.",null,0,cdka5rl,1r6ymz,askscience,new,6
Mazetron,"Electromagnetic radiation can harm you, it can be harmless.  For example: 
-UV radiation in sunlight causes sunburn
-gamma radiation from space can cause cancer
-X-rays can cause problems if the dose is too high

But visible light, radio waves, and lower energy waves are harmless.

Also, electromagnetic fields (emf) are completely harmless.

And both DC currents and AC currents produce magnetic fields, the difference is an AC current will have a changing field while a DC current will have an unchanging field

Things stop electromagnetic radiation all the time: whenever light hits an opaque object.  For blocking high energy radiation that typically passes through objects that visible light cannot, dense materials like lead are used.  Lead pads can block x rays and gamma rays.

Electric blankets and power lines are harmless.

EDIT: Alright, technically you could kill someone with a magnet or burn their face off with visible light.  You can also die from drinking too much water.  The dosage of visible light and magnetism you find in everyday objects is way too small to harm you.",null,1,cdk7wsr,1r6ymz,askscience,new,6
ohsohigh,"Electromagnetic radiation can be dangerous. If it is a high enough frequency it can ionize molecules in your body which can adversely affect DNA and cause cancer. This requires high frequency radiation in the UV or X-ray region of the spectrum. Lower frequency radiation can transfer heat to your body, which can be bad for you if you get a sufficiently powerful dose all at once; this is how a microwave oven works. AC running through power lines isn't going to do either of these things. It is not going to produce high frequency ionizing radiation and if it was putting out radiation at sufficient power to burn you it would render the transmission of electricity incredibly inefficient not to mention the fact that that would be impossible for anyone to miss as you would be able to feel the heat. I am not aware of any other mechanism by which electromagnetic radiation can have detrimental effects on human health.",null,0,cdk7y5q,1r6ymz,askscience,new,1
LegateDamar,"For your example, no. Increasing temperature will not reduce the pressure needed to form diamond. Here is a phase diagram for carbon. Notice that diamond will only be formed at extremely high pressures and increasing temperature will just melt or sublimate the graphite. In fact, higher temperature increase the pressure needed to form diamond. 

http://www2.chemistry.msu.edu/courses/cem152/snl_cem152_SS12/_images/carbonphase.png

In the general case however, yes there are minerals whose crystal structures can be changed through heat. An example would be quartz. Here is a phase diagram for quartz. 

http://www.geo.arizona.edu/xtal/geos306/silica_phase_diagram_large.gif",null,1,cdkajxr,1r6xvg,askscience,new,3
EdwardDeathBlack,"Here is my understanding. Right right after the big bang, the universe was overall an electrically conductive material. EM waves can not propagate very far in a conductive material, so there is no remaining radiation from that immediate time. It couldn't ""propagate""

A little while later, matter starts forming hydrogen, and the universe becomes transparent, allowing radiation to propagate. We can see the radiation that existed at the point of time in the universe by looking ""far enough"". It is known as the [cosmic microwave background](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation) . 

My understanding is the universe was [~400,000 years old](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation#Features) when it happened. So out of the 13.798±0.037 billion years of the universe, we can pretty much see all the way up to 13.798 10^9 - 400,000 ~ 13.797 10^9 years...so pretty far back. ",null,4,cdk7pk0,1r6xdz,askscience,new,11
MaskedEngineer,"Not only is it possible, it's easy. Or used to be. If you turn on an old-school analog TV, connect an antenna, and tune to an unused channel, much of the static you see is from the [Cosmic Microwave Background radiation](http://en.wikipedia.org/wiki/Cosmic_background_radiation). These are photons that are red-shifted so far that they've become radio signals. They're essentially coming from the edge of the discernable universe.",null,1,cdkl46l,1r6xdz,askscience,new,1
Blacklightzero,"As said before, we can see nearly as far back as the big bang.  They are working on a map of the universe using the Hubble right now.

And, if you look at the map so far, you can see that we are right smack dab in the center of the universe.  We can see equally distant in all directions all the way back to when things started emitting light.  And also that the oldest objects we can see look just like the ones that we can see right next door.  Curious, isn' it?",null,1,cdklhth,1r6xdz,askscience,new,1
joshhinz,"Hello. I'm a mechanical engineering student, I noticed you have not had any responses yet, so although this is not strictly my area of expertise, I will share my thoughts on your question with no pretense of absolute certainty...If you were to model an arm or leg for example, as a set of mechanical elements, it might be reasonable to model the bone as a cylindrical beam, and the muscles and tendons as a set of parallel springs and dampers on different sides of the beam. Additionally, it might be reasonably to say that: at rest these springs are likely in a small amount of tension(putting the bones in a small amount of compression). Also, the human body is only able to actuate these springs to either relax them or to put them into greater tension. Therefore my conclusion is this: Yes! muscles and tendons provide strain relief especially in tension because the muscles are able to ""share the load"" in tension and only a minimal amount (if any) in compression because the muscles are not likely to be brought past their relaxed (small tension) state into a state of much compression, the body is simply not able to physically actuate muscles in this way. ",null,0,cdkadb9,1r6vj0,askscience,new,3
NightmareOfLagrange,"Interesting question.  Muscles and tendons actually apply stress to the bones they are attached to.  Bone structure can be thought of as a composite material composed of collagen and mineral.  Structure of this composite is what gives bone its mechanical properties, and it's been shown that stresses acting on bone at a stage of incomplete secondary remodeling can affect the resulting structure.  In a way, you could say that yes, the stresses applied to the bone will help it develop into a design better suited to bear those loads, and physiologically other types of tissue do help in impact absorption to relieve stress on the skeletal system.",null,0,cdkldls,1r6vj0,askscience,new,1
Lost_Wandering,"The strain to failure will not increase due to the tendons/muscles since it is a property of the material not the biomechanical system. They will however increase the amount if load that can be applied to the system since some of the stress will be transferred to the soft tissue. Increasing the strain to failure would happen if composite composition changed, the collagen content is increased  (and consequently decreasing the brittle hydroxyapatite phase) creating a more viscoelastic material response, but decreasing the ultimate strength.

Source: I do computational material modeling of bone-like biocomposites",null,0,cdl9vnq,1r6vj0,askscience,new,1
galinstan,Some iron **is** oxidized to form iron(II) oxide in the slag and fume. Table III in the link provided by the OP mentions that 3.2 % of the charge is lost to slag and fume formation.,null,12,cdk8pey,1r6vha,askscience,new,49
SmellyRaghead,"Iron oxide will be destroyed at temperatures that high. The oxygen would be free until it encountered silicon or other elements with high melting point oxides.

When you weld (say) mild steel, you are relying on the fact that the surface coating of oxide melts at a lower temprature than the bulk material.

I can't remember the exact temp but certain iron oxides are quite easy to destroy with heat. ",null,11,cdkafnj,1r6vha,askscience,new,31
dmd53,"In short, this occurs because of the relative free energies of the various oxidation reactions at such elevated temperatures. The creation of SiO2 is far more energetically favorable than the creation of, say, Fe2O3, and as such the oxygen will selectively bind with the Si and prevent rust formation. Elements like Si are intentionally added to the steel melt to scavenge oxygen and prevent rust formation (as well as to form ceramic inclusions which increase the strength/hardness of the steel); these elements are known as [deoxidizers](http://en.wikipedia.org/wiki/Deoxidizer).

",null,1,cdkh6c9,1r6vha,askscience,new,9
ShoutyCrackers,"At the heat level used (3000°F +), most impurities are destroyed. Medieval blacksmiths referred to this as ""crucible steel"" and for a long time the only place to get it was the Middle East until the Vikings found a route to Iran from the Volta. There's a great NOVA (PBS) piece about it called ""Secrets of the Viking Sword"" that tells the whole story. Its on Netflix.",null,16,cdkatrq,1r6vha,askscience,new,20
372xpg,"If you look at a chart of the oxidation potentials of these metals and the temperature you can see that most of these metals/elements oxidize more readily than iron especially carbon, the big difference between pig iron and steel. Its a handy property, however some impurities do not like to oxidize this way.

An example of a similar process where iron is oxidized(with air) before another metal is in nickel converting.",null,0,cdkkyn4,1r6vha,askscience,new,2
mjwaters,"I think the answer you are looking for is that there is a lot of carbon in the molten iron at this point (http://en.wikipedia.org/wiki/Pig_iron). When they blow the oxygen in, it will preferentially react with carbon forming carbon monoxide and releasing a bunch of heat. With good mixing from the oxygen lance and good exhaust pull from above, the CO is stirred and pulled out. Silicon also reacts with oxygen before iron, but the primary reaction is combustion of carbon. If there was no carbon in the iron, you would see very fast oxidation into iron oxide. Source: I am a materials scientist and I have read this for fun http://www.amazon.com/Making-Shaping-Treating-Steel-Refining/dp/0930767020 ",null,1,cdklt4m,1r6vha,askscience,new,2
ImpossiblePossom,"As with all things in chemistry and thermodynamics: The reactions and resulting mixture allow for a lower Gibbs Free Energy (GFE) at the conditions of reaction. 

At room temperature and in the right environment, iron reacts with oxygen to form a few different iron oxides we commonly consider rust. One could assume that this would also occur in the furnace during the production of steel from pig iron.  However iron oxides that make up rust do not end up forming at high levels because other reactions occur instead. These reaction(s) are known as decarburization reactions.  In the case of steel making the exact reactions are O + C -&gt; CO and CO + O -&gt; CO2 (per OP’s link). Some iron oxide are formed in the BOS process (FEO). However these are not the same oxide’s present in rust (Iron III Oxide and Iron III Hydroxide). The FeO that formed is either blown out the top of the furnace’s effluent gas stream or forms a separate phase in the form the slag that can be removed.

I think the crux of the question is: Why do some reactions (decarburization) occur preferentially and others do not (Iron Oxide formation). This is where Gibb’s Free Energy comes in.  At the conditions in the furnace the decarburization reactions and resulting products require an overall lower Gibb’s Free Energy.  Since Gibb’s Free Energy is always minimized at equilibrium, the reactions and resulting mixtures that require a lower GFE will occur preferentially. This is why the majority of the carbon in the pig iron reacts to form CO, and a much smaller fraction or carbon forms CO2, and an even smaller fraction of Iron Oxide (FeO) is formed. 

So why doesn’t this happen at room temperature too? Well at room temperature Iron-O2-Environment system changes dramatically such that rust formation gives a lower GFE and the Iron Oxides in rust are preferentially formed.

Thanks for the question. It really challenged me to answer it in an accurate, concise, and understandable way.

Source: I’m a chemical engineer who specializes in the production process of advanced materials.

Link to wiki on decarburization:
http://en.wikipedia.org/wiki/Decarburization

Link to wiki on BOS Process (IMO it’s a bit less technical than the link OP posted)
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to the wiki on the Bessemer process (because why not! it’s a fun read but not really necessary to answer this question):
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to wiki on GFE (in case you need help sleeping tonight):
http://en.wikipedia.org/wiki/Gibbs_free_energy
",null,0,cdkma57,1r6vha,askscience,new,1
ramk13,"It's true iron and oxygen can combine to form rust (Fe2O3/FeOOH), but that doesn't mean that the two will fully react in that combination. The practical extent (how much forms) of the reaction depends on the thermodynamics of the reaction, the environmental conditions (temperature, pressure) and the kinetics (speed) of the reaction.

In this case, as a couple other people have said. Rust is less stable at high temperatures (thermodynamic) and the oxygen prefers to react with other compounds first (both thermodynamic and kinetic).",null,0,cdknkde,1r6vha,askscience,new,1
NightmareOfLagrange,"Although the original question regarding formation of impurities has already been answered, I'd just like to add that formation of rust specifically is a reaction where iron is oxidized, but rust is not just iron oxide.  Rust is either the hydrated oxide, or a hydroxide where water and oxygen are reduced to form hydroxide ions with electrons from what is assumed to be the iron oxidation.  
",null,1,cdkkvdq,1r6vha,askscience,new,1
OrbitalPete,"There are several problems with forecasting reversals. Firstly, they do not occur at regular intervals. 

I was going to post a big explanation with nice figures, but in searching for nice figures I found this, which does the job brilliantly. http://all-geo.org/highlyallochthonous/2009/02/is-the-earths-magnetic-field-about-to-flip/

The long and short is we can go for millions of years without reversals, and a reversal is not an instantaneous thing but can take decades to millenia to occur.  

The causes are due to changes in convection direction in the outer core - convecting a fluid around a rotating sphere leads to instabilities. Then the bulk fluid direction changes, so does the field polarity.

It's worth noting that earth's field is currently not a true dipole either, there are significant positive and negative magnetic anomalies (particularly one over the South Atlantic), and if you look at the magnetic field at depth we are currently in a fairly multipolar condition, with several North and South magnetic poles. This may or may not be an indication that the poles are in the process of reversing. ",null,0,cdkd57i,1r6vce,askscience,new,3
McBented,"Some background: 

1) Reversals do not occur on regular intervals. It could happen after 100k years, and then not happen for a million years after that. 

2) Sometimes, the dynamo also tries to reverse, but end up the same polarity after the ""reversal"". These are called ""excursions"".

3) The reversal process itself usually takes about ~20,000 years, so it's very gradual to the time-scale we live in.

There are some people in the field who believe that we are at the beginning of a reversal right now. In my opinion it's pretty much impossible to know what the dynamo is trying to do until we're well into the reversal, so I find it pointless to speculate.

Dynamo reversals occur due to nonlinear behavior of the convecting fluids in Earth's iron-core, and its interactions with the magnetic field it generates. We can now simulate these fluids by solving the appropriate equations with a computer. In our models, we do get spontaneous reversals for geodynamo models. Since these equations are non-linear, they are not predictive for very far into the future (just like how weather forecast models are only good for the next few hours/days or so).

Interestingly, we can also model a cyclic dynamo with predictable reversals with the right parameters, like the Sun's dynamo.",null,0,cdkjlp1,1r6vce,askscience,new,1
UncertainHeisenberg,"[This USGS FAQ](http://www.usgs.gov/faq/?q=categories/9830/3353) does a good job of explaining why larger ruptures can't be prevented using smaller, harmless, ""controlled"" earthquakes. 

The main reason is that you need thousands of smaller quakes to release the equivalent energy. For example, 32000 M3 earthquakes release equivalent energy to one M6 event. This scales to around *one million* M3 events if you want the equivalent energy of an M7 earthquake!

Secondly, how do you guarantee that the earthquakes you trigger will be minor enough to cause no damage? Now guarantee that for every one of the many thousands of quakes you will need to trigger.",null,83,cdkcb4a,1r6u2f,askscience,new,448
null,null,null,2,cdkaw9q,1r6u2f,askscience,new,28
MrsWerf,"Earthquakes also don't universally decrease stress in the surrounding area. Seismologists are now able to model the changes in stress ([Coulomb failure stress - the bottom of the page](http://www.geology.um.maine.edu/geodynamics/AnalogWebsite/UndergradProjects2010/PeterStrand/html/Introduction.html)) and show that some areas, particularly the tips of the ruptured fault, are areas with increased stress after an earthquake. ",null,4,cdkczko,1r6u2f,askscience,new,26
Anomander82,It's not really viable method of reducing stress at tectonic boundaries if one simply considers the scale of energy involved. The energy dispersed from the well during hydraulic fracturing is typically confined to a radius of a few metres to a few tens of metres from the wellbore. The commonly cited incident in the UK in 2011 is really an isolated occurrence. The amount of energy required to relieve large-scale tectonic stress is orders of magnitude greater than what would typically be applied to an unconventional hydrocarbon play. Also consider that there are numerous risks associated with the drilling process in a tectonic province with a high level of activity. Overall it's simply not feasible.,null,0,cdkgr5m,1r6u2f,askscience,new,5
ripitupandstartagain,"You can't state that hydraulic fracturing only creates small earthquakes, just that it seems to be responsible for increased seismic activity and that the events attributed to this cause have so far been small. 

The frequency of different magnitudes of earthquakes in a set area obeys the principles of chaos theory (ie the frequency of the event is inversely proportional to the energy released). There will be a stress point reached that will trigger a quake but the size of the quake produced is pretty random based on the odds of a certain amount of energy being released. For example a magnitude 6 earthquake releases approximately 32 times the energy of a magnitude 5 so over a set area you would expect magnitude 5 earthquake to be about 32 times more frequent than a magnitude 6 quake. 

Rather than causing small quakes, fracing can be said to be loading an area with stress past its trigger point. The earthquakes recorded at hydraulic fracturing sites tend to be on the order of about 3. So one could reasonably expect a magnitude 6 quake to occur within the time taken to create 16000 magnitude 3 quakes (32x32x32/2 for average). 
Now if a mag 3 quake is happening once a week (which seems to be very unlikely) that would mean a mag 6 would be expected after around 2300 years (obviously it could happen at any tipping point but the cumulative odd suggest about then) or about 70 years for a mag 5.

This is based on fracing being the major cause of the earthquakes which would be the case for the majority of sites as they are away from active faults (such as with the Blackpool quakes which halted UK fracing the other year). ",null,1,cdkhvgu,1r6u2f,askscience,new,5
Male_Rikku,"This idea is actually similar to a concept in plasma physics whereby you steadily bleed off free energy at the plasma-wall boundary by inducing magnetic oscillations. It's used to keep large events that could damage the wall from happening. In *practice* we may not have a good way to do this with geophysical problems, but in *principle* a way could exist if we could model everything well enough and had a lot of control over the ""shape"" of our perturbation.",null,3,cdkgjjj,1r6u2f,askscience,new,5
classycactus,"Also, induced seismicity is more closely linked to fluid injection. Particularly salt water/waste water disposal wells (class II), the induced earthquake that are being the cause of controversy are those causing earthquakes in parts of the crust that are fairly tectonically dead (like the mid west US). Also the earthquakes that cause real damage(~magnitude 6.5+) are much much much larger then anything you might get from fluid injection(IIRC the usually the earthquakes are from ~2.0-4.5, which is a fraction of 6.5, as the Richter scale is a log scale)",null,0,cdkkjej,1r6u2f,askscience,new,2
astazangasta,"At last, a good use for pie charts!

[Here](http://feww.files.wordpress.com/2009/07/total-seismic-moment-released-by-earthquakes-1906-2005.jpg?w=420&amp;h=361) is a graph showing total seismic release divided by magnitude of events. As you can see, a single event like the SF 1906 earthquake is a small fraction of the total, even though it had a huge impact. Meanwhile, there are single events that dominate this chart, implying it would take MANY SF-scale events to relieve the stress of one of the big movements. [Here](http://scienceblogs.com/greengabbro/wp-content/blogs.dir/265/files/2012/04/i-51c95bfa0685b39b350ab449ef2bd73e-2004-2005-seismic-energy.png) is another image of 2004-2005 events, showing how total energy release from a single event is much larger than many many small-scale events put together.",null,0,cdkkbue,1r6u2f,askscience,new,1
KarlOskar12,"Well all we can really do is look at what has worked in the past for organisms and what success they have had. As it currently stands the most successful organisms are single celled prokaryotes. They have been around for billions of years and their DNA replication has many errors in it. They mutate very quickly as a result and as a result they have been able to adapt to literally every environment on this planet. They have been around far longer than anyone else, and they will probably be the last surviving organisms on this planet. So you could easily make an argument that a higher rate of error is optimal in a DNA replication system.",null,0,cdk7wao,1r6tl1,askscience,new,1
snusmumrikan,"Agh the paper I'm trying to find is eluding me! It's from this month though.

The potential for variation is selected for evolutionarily. Without the potential of random mutation, a species will not be able to evolve and respond to varying or novel selection pressures in their environment (which is always changing, and on an evolutionary timescale the environment can change significantly quite quickly).

If you are 'stuck' without any change in the genome then a species will be at a significant disadvantage as there will be no way for the gene pool to adapt. As such it is actually preferable for the DNA machinery to have a background level of error, otherwise the species would die out. 

If anyone can find and link the article I mean, I think it is Nature this month, something like 'The capacity for variation is selected for...' I would be very grateful. It was also on many normal news sites as a side note article",null,0,cdkl6hf,1r6tl1,askscience,new,1
cmuadamson,"It's an interesting question.  I can think of a number of conditions that would have to be met, but none of them rule out the possibility completely. Perhaps someone else can think of an impossible requirement.

*  The planet would have to be slow rotating, so that the synchronous orbital distance is far enough away that the two bodies don't tidally rip each other apart.

* The planet would have to be extremely more massive than the moon, so that their center of mass (the barycenter), about which they both are truly orbiting, is very close to the center of mass of the planet.

* The moon would have to have been captured, not formed by normal gravitational collapse of a gaseous field. Otherwise, the clumps falling together to form the moon would not have had a chance to ""sweep"" through the area of its orbit. Someone may be able to argue down this point.

* The moon must be orbitting directly in the sidereal plane of the planet, i.e. directly above the equator, which is unlikely to occur. If its orbit has any inclination, it is no longer a ""stationary"" orbit, but could be a ""synchronous"" orbit.

* Similarly, the moon's orbit must be circular. Any elongation means the moon's speed changes throughout its orbit, also changing it from a ""stationary"" orbit to a ""synchronous"" orbit. This is also very unlikely to happen coincidentally.
",null,0,cdk7lmj,1r6sqm,askscience,new,3
Gargatua13013,"Nope

Spider and scorpion are arachnids (and indeed 8 limbed)

Crabs are crustaceans, together with lobsters and barnacles (and actually 10 limbed)

The closest marine relatives of spiders are  Pycnogonids (see: http://en.wikipedia.org/wiki/Sea_spider). Although pycnogonids are *not* spiders, or even arachnids, they are a somewhat related class.",null,0,cdkl2xw,1r6shx,askscience,new,2
thenotoriousFIG,"Yes, they are both Arthropods. http://en.wikipedia.org/wiki/Arthropoda",null,0,cdk76p7,1r6shx,askscience,new,1
tigerhobs,"No, human cells do not, but surprisingly your gut microbiota can produce it if a yeast gets into and persists in your intestines.  Your intestines are largely anaerobic, and these yeast will ferment sugars to make alcohol, and you can get drunk.  It doesn't sound like a very pleasant condition, but it should be easily curable with anti-fungal agents.  [I first heard about it on NPR, click me!](http://www.npr.org/blogs/thesalt/2013/09/17/223345977/auto-brewery-syndrome-apparently-you-can-make-beer-in-your-gut) .  The literature does not have much on this relatively rare condition, but it seems to be possible.

To reiterate- your own cells cannot, but your gut, supplemented with alcohol producing yeasts, might.",null,4,cdkb7by,1r6pmw,askscience,new,15
null,null,null,6,cdka124,1r6pmw,askscience,new,6
NightmareOfLagrange,"I'm gonna go out on a limb here with this answer, but hopefully my simplifications won't be too offensive:

Soda is mostly water, sugar, and carbonation.  Based on their molecular structures, although water has decent surface tension (and can form bubbles), based on interaction with the rest of the water molecules and mutual attraction, neither of those are very good at holding a structure like a bubble.

Ice cream introduces lipids into the solution of your float, which, based on their structure, are more likely to retain bubbles (like how you can make bubbles with soap but ice cream tastes a lot better).",null,0,cdklmbz,1r6pjl,askscience,new,1
thetango,"When a bottle says aged 12 year they mean 12 year in a barrel not the bottle.   Liquor, for example whiskey, picks up flavor from the cask/keg it is barreled in.   The cask/keg can impart very subtle textures and flavors to a whiskey.

Wine, on the other hand, ages differently than liquor.  As wine ages, a chemical called 'tannins' breaks down and the result is a ""smoother"" tasting wine.",null,1,cdki31j,1r6pfu,askscience,new,4
Proxymace,"Potentially yes. But any orbit through the solar system undergoes changes from all of the planets gravities. These are likely to have a much greater effect than a solar flare which does not have much impact mass, even if it does contact directly with solar material",null,0,cdmjdr7,1r6p3q,askscience,new,2
Drunk-Scientist,"Adding to Proxymace's answer; another thing that can alter a comet or asteroids trajectory is simply the sunlight itself. [Radiation pressure](http://en.wikipedia.org/wiki/Radiation_pressure) from the photons colliding with an object can lead to minute changes in orbits that add up over time. In fact, one suggested technique to shift an asteroid headed for Earth would be to paint one half of it white and let the radiation pressure from the Sun do it's business.

However it should be noted that both this effect and that of Solar Flares is orders of magnitude less than those due to gravity, for example by close interactions with Jupiter. So unless a comet remains in a totally undisturbed orbit close to the Sun for many millennia (an unlikely state of affairs for eccentric comets), the effects of the solar wind and radiation pressure will be negligible.",null,0,cdn1ib2,1r6p3q,askscience,new,2
GProteins,"Okay-- so I'm going to assume that you're talking about things like cuts/abrasions/etc. 

The first thing that happens is your blood vessels release a variety of cells, some to eat bacteria, some to help close the hole by a) plugging it and b) secreting chemicals that help plug it in a more stable manner (that thing we call a 'scab'). 

Then, your body rushes to fill the gap with really fibrous tissue-- mostly collagen and some elastic fibers. This is your initial scar. It's red and leaky and not terribly strong, but it's better than nothing. Your body then goes through and slowly replaces that fast-placed fibrous tissue with the normal cells that belong in the area. Eventually, the fibrous tissue gets replaced (sometimes it doesn't entirely go away), but the tissue that's there is a LITTLE smaller than the tissue that was originally in the wound area, which is why scars can feel ""tight"". 

Basically, when you're talking more severe wounds, you're just seeing more fibrous tissue, meaning a longer time for the body to replace it with normal cells. And sometimes when you have such fast cell proliferation (even though it's slow to you, it's fast for your body), the cells ""forget"" things and you get little mutations here or there-- darker or lighter skin sometimes. And you can lose architecture that was there already (hair cells, moles, etc).",null,0,cdn9ep2,1r6o58,askscience,new,2
codyish,"I'd be interested to see a source confirming your idea. My understanding is that capsaicin is fat soluble, so milk washes it away more effectively than most drinks, and that olive oil would do an even better job. ",null,0,cdkai3o,1r6lk9,askscience,new,2
rupert1920,"The existence of a permanent dipole moment is independent of whether you put it in the microwave or not.

Some functional groups in olive will have dipole moments along their bonds - for example, the C=O double bond - but there are also large extents of the molecule that doesn't have a strong dipole moment - such as the hydrocarbon chain. The end result is that the molecule as a whole has a small dipole moment.

This, combined with the size and weight of the molecule (i.e., high moment of inertia), makes it much less responsive to microwaves than, say, a water molecule.",null,0,cdk6p7b,1r6liv,askscience,new,5
walluwe,"No. The oil is made up of several fatty (hydrocarbon) chains connected to carboxylic acids. While the carboxylic acids themselves have a dipole moment, the molecule on a whole is not really affected by that, just due to the very long hydrocarbon chains (the carboxylic acids are so much smaller than the fatty side chains). The water molecules themselves will vibrate, but the fatty acid will not be affected by this radiation. ",null,3,cdk5m1p,1r6liv,askscience,new,2
rupert1920,Check out the Wikipedia article on [Roche limit](http://en.wikipedia.org/wiki/Roche_limit).,null,1,cdk78nz,1r6kn2,askscience,new,4
Drunk-Scientist,"Interesting question. The above answer on Roche Limits is correct; within a certain radius (governed by how strongly held together each planet is), the gravity from each would rip them both apart. This is why [Saturn has rings](http://www.astro.washington.edu/users/smith/Astro150/Tutorials/Roche/) and why [Comet ISON might not make it through](http://www.asterism.org/tutorials/tut25-1.htm) it's close interaction with the Sun.

Much of the material from this interaction would collide and form a [disc of material](http://news.bbcimg.co.uk/media/images/63566000/jpg/_63566727_r3400571-artwork_showing_the_moon_s_formation-spl.jpg) around the point where the planets came nearest. Some material would also likely be thrown off into the solar system. This disc would then re-accrete back into a (much larger) planet. In fact that outcome is similar to [what happened to Earth 4.5 billion years ago](http://en.wikipedia.org/wiki/Giant_impact_hypothesis) when a Mars-sized body collided with us and the resulting debris disc coallesced into the Moon.

HOWEVER, if you ignore the 'planets' and 'dense atmospheres' part of your question, then such an occurrence can and does happen in the universe. [Contact Binary Stars](http://en.wikipedia.org/wiki/Contact_binary) are stars that orbit so tightly that their atmospheres are connected. Mass and energy are transferred between the stars and eventually the smaller companion may even become swallowed entirely by the larger, but they may last happily together for hundreds of millions of years.

Going back to the question in hand though: What would actually happen if these planets could touch atmospheres without being ripped to shreds (eg, assume they are made of adamantium). Well, as the gases mix some of the molecules would become gravitationally bound by the planet above and cross the gap to the other, effectively. It would also cause huge atmospheric disruption on both planets, possibly with supersonic winds created that might blow around the planet for days.

However, such a scenario would be unlikely to last for more than a couple of orbits: the atmospheric drag on both would reel in their aphelion until, rather than just kissing atmospheres, they were slamming into each other head-first. But I guess if these really are indestructable planets they would just form a strange bow-tie shaped world; a bit like a [Contact Binary Asteroid](http://en.wikipedia.org/wiki/Contact_binary_(asteroid).",null,0,cdn0yzy,1r6kn2,askscience,new,1
Hiddencamper,"Nuclear engineer here.

The RBMK is graphite moderated. This means that the cooling medium and the moderation medium are separated. 

Under normal operation, water enters the bottom of the reactor, and flows upward. The water heats up on its way through the core, and the amount of steam voids by volume increases on your way up through the core. 

Steam is drastically less dense than liquid water, and is virtually transparent to neutrons when compared to liquid water. As such, an increase in voids means that my water is absorbing less neutrons. It also means that my neutrons will have a longer mean free path length, and ultimately means more neutrons will be able to get to my moderator. tl;dr, Increase in steam voids = increase in moderation = increase in power.

Inherently this has stability issues. As I increase my heat output, I'm going to increase the amount of voiding I have. This will then increase power, which further increases voiding. Active control systems which adjust control rods can come down to compensate for this. The control rods for this reactor design drop in from the top, which makes sense as the top is where the highest neutron flux is likely to be seen. Active control rod motion suppresses any power excursions and maintains the reactor in a stable operating state.

At high power levels, you are producing a lot of steam flow, and as a result, you have a high flow of water through the reactor. With high flow rates through a reactor, your boiling boundary remains fairly constant, and it takes quite a bit to have a runaway excursion, as the forced flow of water into your reactor tends to push voids out quickly and ensure cooling water gets to where it needs to be. Additionally at high flow high power conditions, the voids do not have a dominiant contribution to reactivity, meaning small changes in voids have small changes in reactivity.

When you are at low power, you are in a situation where you have low flow. Your boiling boundary is higher. Your voids have a much larger contribution to reactivity in the reactor. At low flow conditions with low control rod density conditions in boiling reactors, we observe the boiling boundary is somewhat unstable. Steam voids take longer to get out of the reactor, and they begin to have a stronger impact on reactivity. As such, anything that changes your boiling boundary even a little bit is going to have an amplified effect on your neutorn flux and power output. Little things like random noise in your pump flow can start these oscillations, causing the boiling boundary to move, which starts causing power oscillations. In the RBMK, to respond to the power oscillations, the control rods will start moving in and out to try and stabilize this. Having graphite tipped rods combined with having a control system with a response time constant in seconds (which is similar to the fuel's thermal time constant) just means that the control rods are going to be trying to catch a power change, but will have trouble keping up because it will be causing some of its own problems. 

All of these things together will drive power oscillations in the core. Under a worst case condition it can drive a runaway condition requiring a reactor scram. When operated appropriately, the reactor scram (even with the graphite tips) will have sufficient margin to preclude a steam explosion. When not operated appropriately (with nearly all control rods out), a power excursion can occur which leads to a steam explosion and loss of the unit.

The reason this is an issue at low power is due to the way the boiling boundary behaves at low flow conditions with low control rod density. The voids have too much contribution to the core's reactivity under these conditions, and small changes in voids drive large changes in flux. Because the voids contribute so much to reactivity under these conditions and the possibility of an instability can occur, RBMKs have a safety limit which requires a minimum number of control rods to be inserted at all times, to ensure that the voids do not carry enough reactivity to drive a core damaging power excursion. At Chernobyl they removed these rods past the safety limit because they were in the xenon pit, and trying to reach a specific power level on their reactor.

I'm more familiar with BWRs for instabilities. Standard BWRs like those in the US have stability issues as well at low flow low control rod density conditions, however the because the moderator and coolant are the same, they tend to be self limiting and are not capable of undergoing a power excursion/steam explosion. Most BWRs also have a system (called OPRM) that detects core stability issues and initiates an automatic reactor scram. What we see in BWRs is an oscillation with a time constant that is usually 1-2 seconds. We will start seeing small oscillations, then the oscillations will start growing. Oscillations in a BWR grow slowly. If the plant has an OPRM, it will scram the reactor prior to it increasing past certain limits. If the plant does not have an OPRM, it will grow over several minutes until the reactor hits either the high or low flux scram setpoints. The main danger in BWR type reactors is that you can cause localized thermal stress and plastic strain on the fuel (localized fuel cladding damage), but no power excursion.

For BWR light water reactors that have it equipped (all US BWRs have this) the OPRM (Oscillation Power Range Monitor) looks for counts (how many oscillations am I getting in a row), period (are the oscillations in the right time constant that is indicative of thermalhydraulic instabililty or is it just random noise), and growth (is my oscillation diverging with a &gt;1.0 decay ratio). There is also a confirmation density algorithm which uses a factor that looks at the above factors across the core to anticipate these factors before they start. If the OPRM gets enough counts, on the right period, with growth, it will initiate a scram immediately. US BWRs are forbidden from entering the region where core thermalhydraulic oscillations exist, and are required to insert an immediate reactor scram if they enter the region.

Generally, the only time a BWR enters the instability region is if they have an inadvertent loss of their reactor cooling pumps. 

Another poster mentions xenon. Xenon can cause long term issues which force you into a low rod density low flow situation, however xenon will not cause the instabilities that created the chernobyl event. Xenon does not respond fast enough (hours), while the boiling boundary response is in seconds. He's not entirely wrong, xenon transients can force you into an instable core operating region, but they do not cause the instability. Proper reactivity management can ensure that you pass around the instability region without going into it. RBMK reactors have a very tough time with the xenon pit and are more likely to put themselves into an instability, while other BWR type reactors can deal with it just fine.

I hope this helps. If you have any other questions please let me know.",null,0,cdks0mi,1r6hig,askscience,new,3
grillkohle,"Are you referring to the Chernobyl incident? I will try to explain the problems without looking at that incident.  
Controlling nuclear reactors is all about controlling (slow) neutrons. The slow neutrons basically are the chain reaction.  
A big problem with reactors in low power mode is Xenon 135. It is known as a neutron poison, because it has a high probability for absorbing neutrons (and thereby reacting to the stable Xenon 136). It is formed by decay of Iodine with a half life of 6.6 hours. When the reactor is running in normal operation mode, there is a balance between the ""production"" and the ""reduction"" of Xenon 135.  
If you lower the power output, there is more production of Xenon 135 because of the delay (decay from Iodine). Hence, it is more difficult to control the reactor, you can't increase the reactivity, because the Xenon will absorb your neutrons. Usually you have to shut it down and wait until most of the Xenon is decayed itself (half life 10 hours).  
But if you try to increase the reactivity (by pulling out control rods for example), you increase the reduction of Xenon (obviously the production stays at a lower level for some time because it takes some time for the ""production"" of the Iodine and its decay), which further increases your reactivity because you have more slow neutrons because less neutrons are absorbed by the Xenon. This happens to all nuclear reactors.   
The next problem is the positive void coefficient, which basically means: more thermal activity (more reactivity) =&gt; less cooling water (because more steam (much lower density!), more bubbles) =&gt; more thermal activity because of the void coefficient [because of the graphite still moderating at higher temperatures].  
These 2 effects support each other and make controlling the reactor in a low power state quite difficult.  
In a water moderated reactor (negative Void coefficient) for example the chain reaction stops or slows down if the water vaporizes too quickly, because there is less moderator to slow neutrons down. It is sort of ""self stabilizing"".  
I don't know how much you know about reactors, but I hope I could explain it to you. I know that stuff because I took a course at the university about nuclear reactors.
There are some other factors regarding the Chernobyl incident which lead to the explosion.
",null,0,cdkehwb,1r6hig,askscience,new,2
zopamine,"Forget everything you've ever heard about right brain/left brain. For the most part, it's a myth.

However, functions can be localized to certain hemispheres of the brain. For example, when we see human faces or pictures of faces, that process has been said to take place mostly in the right occipitotemporal (fusiform) gyrus, which is above your right ear and a little further back. 

Basically, there are cognitive functions that are localized to each hemisphere of the brain, but the whole being more left-brained (colloquially logical) or right-brained (colloquially creative) just isn't true. We use both hemispheres equally. 

Source: Nielsen, Zielinski, Ferguson, Lainhart, &amp; Anderson (2013) 

It's on PLOS One, so it's widely accessible if you're interested. I'd link it but I'm on my phone. Hope that was helpful!",null,1,cdk98lr,1r6hfl,askscience,new,4
3asternJam,Most famous example of left/right differences in brains is speech - Wernicke's area (speech/writing comprehension) and Broca's area (speech production) are only found in the left hemisphere (generally speaking; there are exceptions). The corresponding areas in the other hemispheres are more general sensory/premotor areas respectively.,null,1,cdkg8e2,1r6hfl,askscience,new,2
Platypuskeeper,"Without experiment, a guess from homology is the best you can do. ",null,1,cdk7kj3,1r6g8q,askscience,new,4
Siny_AML,I think the next question to ask would be the possible nature of the enzyme. You would have to test the Kd values of the reaction with and without the protein of interest to see whether the rate of the reaction either stays constant or changes. I may be wrong but I don't think that this is something that you would be able to test without using some kind of in-vitro system. Relying on computational methods would only give you a theoretical system with no biological basis.,null,0,cdk7ltk,1r6g8q,askscience,new,2
edge000,"I think this question is usually approached from the opposite direction. Biologists tend to be interested in processes, so I would think people wouldn't typically investigate a random protein unless there was some interesting reason to be looking at it. A scientists is more likely to be investigating some phenomenon and investigate what causes it. 

Genetic knockout experiments are carried out to elucidate a metabolic or signalling pathway. Basically, someone will identify that a suite of genes controls some pathway and will start [silencing genes](https://en.wikipedia.org/wiki/Gene_silencing) for enzymes along the pathway and see what substrate builds up in the pathway. 


After doing these types of experiments enough times we can build the database that people can search like you mentioned. ",null,0,cdkipub,1r6g8q,askscience,new,2
Baloroth,"t'=t/sqrt(1-(v^2 /c^2 )). So for a v of 370,000 m/s, that gives a rate of 1.000000760556423223785 seconds in our frame for every second in the ""stationary"" frame. So, time in our frame is passing at 99.9999239% the rate of the CMB stationary frame.

Note: that is with respect to an observer in the CMB frame. In our frame, it's the CMB frame that is moving through time slower (hence, relativity), by the exact same amount.",null,1,cdk7px4,1r6e8s,askscience,new,5
adamsolomon,"You phrased your question in two subtly different ways. The correct way, at the end of your post, ""how slowly are we experiencing time, relative to an observer who is stationary to [the] CMB?"". That's a valid question and has been answered already.

But in the title of your post you asked how slowly we're experiencing time relative to the CMB radiation itself. And that's actually *not* a valid question, since radiation doesn't experience time!",null,0,cdkdxq2,1r6e8s,askscience,new,2
redit_,"It produces lightning in basically the same way that thunderstorms do. It's a release of the accumulated static charge. Volcanic ""ash"" is actually a large cloud of rock dust. These particles bump against each other until they build up enough charge to release a spark. It's the same as when you rub your feet on the carpet except for a much larger spark.

And snowstorms can produce lightning, it's just very rare. They actually have a name for it; thundersnow. It's really, really cool to experience. It almost sounds like an avalanche because the reverberations are muffled by the snow. You don't hear a sharp crack and the echos. It's more of a deep rumble from out of nowhere.",null,1,cdk5nyf,1r6ag9,askscience,new,7
Sannish,"If there is lightning there will be thunder, but we might not hear it over the sound of an erupting volcano.

In typical thunderstorms it is the collision of ice and water that creates the charge separation that results in lightning.  We do know that there is charge separation in some volcanic plumes that produces lightning but it is currently unknown what is causing the charge separation.

It could be ash particles, it could be water in the plume, or it could be something else entirely.  Part of the problem is that it is very hard to take measurements inside of a volcanic ash plume.",null,0,cdkiq6w,1r6ag9,askscience,new,2
antpuncher,"In the first generation of stars, the only elements are hydrogen and helium, and they suck at cooling.  So when stuff collapses, it stays very hot.  That means there's a lot of available pressure, so for a blob of hydrogen to collapse by its own gravity, it has to be huge.  The first estimates are hundreds or thousands of solar masses, more recent estimates permit stars as small as 10-20 solar masses.  That's still _way_ bigger than a gas giant, which 10^-3 solar masses.

(The alternate scenario is that the rocky core forms first, but that won't work because rocks are silicon, carbon, and iron, and you need to make stars first, before you can make S,C, and FE.)",null,0,cdk0nqx,1r68w8,askscience,new,5
baloo_the_bear,"A person has 2 types of sweat glands, apocrine and eccrine. Eccrine glands are present all over your body and act to aid cooling. Apocrine sweat glands are present only certain areas like in the axilla (armpits) and groin. They develop during puberty and though they also contribute to cooling the body, they also excrete a small about of cytoplasm when they do so. This leads to the body smell. ",null,1,cdjxdqo,1r5y1m,askscience,new,22
lemons47,"Body odor generally starts becoming noticeable on a person when they reach puberty. Sweat glands in the groin and armpits become surrounded by dense, thick hairs around this time which provides a better environment for the growth of bacteria. Part of the smell is a combination of the bacteria producing odious chemicals from their own biological processes as well as the breakdown of molecules in sweat into more volatile chemical constituents. Because of this, your diet can sometimes affect your body odor because the chemical composition of secreted sweat can change which changes the volatile breakdown products produced by your skin microbiota.",null,0,cdkzlkd,1r5y1m,askscience,new,1
HexagonalClosePacked,"Not an aerospace guy, but it likely has to do with fault/failure tolerance.  Each propeller on a helocopter represents a single point of failure, meaning that it is a single part or component that causes the entire vehicle to become inoperative if it fails.  A helicopter cannot fly properly/safely if either one of its propellors is disabled.  Contrast this with a plane, which due to the lift generated by air moving over the wings, can keep flying if it loses one or more engines (even if it loses all of them it can glide for a while).

Say you have a helicopter with two props.  That means there are two points on the vehicle where a malfunction will cause a major accident/crash.  If we increase the number of propellors to 6, as is common in some RC helicopters, we have now tripled the number of points of failure, since if any one of these 6 props are disabled the aircraft will become unstable.  Someone might be able to correct me, but I'm assuming it would be much more difficult to design a 6 prop helicopter that could keep in the air after losing one propeller than it is to design a 6 engine plane that can do the same.",null,4,cdjxbul,1r5tcm,askscience,new,12
jvs_nz,"There are a few reasons why multicopters have not been brought to life size yet. I can see it being feasible one day, but improbable.

* Most multicopters you see (4,6 and 8 rotors) control their pitch, altitude, and yaw by varying the speed of the blades. They can do this because the inertia of the blades is quite small, so the motors don't have a big job of changing the speed. This wouldn't be the case for full size blades needed to lift a multi-ton craft. 
This can however be offset by using variable pitch blades, much the same as on a normal helicopter - this obviously increases complexity however and also the number of components that can fail.
* 4 rotor crafts will crash if a prop fails. All 4 are fundemental to having controlled lift. 6 and 8 rotor crafts can survive quite easily if 1 prop or motor fails (provided the failure doesn't take any of the other motors out, like a blade flying through neighbouring motors). 
* The electronics and systems needed to keep multicopters afloat are still very new - only in the past 5-8 years has the technology matured enough to make such a thing feasible. 
* Cost. A single engine, single rotor heli is expensive enough. Now times that by 6 - and for what? There are very few scenarios where a traditional helicopter can't do a job a multi could. Now times the running costs by 6 as well. (I say 6 because you wouldn't build a 4 rotor due to above mentioned safety/redundancy reasons).

The reason multi rotors have taken off in the hobby and small scale world is because traditional single rotor model helicopters have been very difficult to control and learn to use - as well as horribly dangerous. Because multi rotors are controlled by software - it's easy enough to make that software user friendly, so it doesn't take much to pick up and learn. They are also cheaper in a lot of the cases - you can build a multi rotor for a couple of hundred $. 
They can also be quieter. 

I wouldn't rule out full size multi's in the future, I just don't think it'll be very common for a long time, if not ever.",null,1,cdk2y6y,1r5tcm,askscience,new,9
Farnswirth,"A lot of people here are arguing that more rotors = more complexity.  While this is true, it does not mean that more rotors = worse design.  Take the B-52 for example, it has 8 jet engines.  Or the V-22, it has two engines built on rotating pivots.  Complexity is not always bad, sometimes you are merely trading simplicity for functionality.  In the case of the B-52 you are gaining redundancy and power.  In the case of the V-22 you are gaining speed and VTOL capability.  

There is no reason you couldn't build a full sized multi-copter.  I am sure there are companies working on one right now.  The problem with multicopters is not necessarily the complexity of the *design* but the complexity of the software and control systems needed to build a full scale one.  For example, if you loose one engine on a B-52 and the flight computer fails, a pilot can still fly the plane manually with no problem.  If the flight computer fails on a multicopter - you are screwed, especially if a rotor fails as well.  A pilot simply can't react fast enough.  We've seen examples of unstable aircraft with extensive flight control systems: the F-16, the [F-22](http://www.youtube.com/watch?v=faB5bIdksi8), [V-22](http://www.youtube.com/watch?v=n3lbKqStvHI), [B-2](http://www.youtube.com/watch?v=_ZCp5h1gK2Q), F-117, etc.  Consequently, most of these planes have had horrific crashes where the flight computer fails and the pilot can't react fast enough - that's just the nature of the plane.  An unstable helicopter is just an added level of danger and complexity, it's just asking for trouble.  That's not to say it's not possible, it's just very challenging.  The technology is new.  It's maturing.  

[People have done it](http://www.youtube.com/watch?v=L75ESD9PBOw).  [The military has played around with it.](http://en.wikipedia.org/wiki/Curtiss-Wright_VZ-7) Commercial and modern military versions are likely on their way.  It's just a matter of time.  ",null,0,cdkggl3,1r5tcm,askscience,new,2
SitnaltaPhix,"Because single-rotor RC helicopters are **extremely difficult** to learn how to fly. Not only do aerodynamics work differently on small aircraft (and therefor make them very unstable), but you are not operating in the same reference frame as a regular pilot would be. Even experienced RC pilots crash all the time. That's rather disconcerting to someone who just wants to take aerial shots with their camera.

The advent of Lithium polymer batteries, more efficient motor controllers, and better computing power have allowed multi-rotor RC helicopters to be possible. Allowing not particularly experienced pilots to fly.

Larger aircraft are just inherently more stable. Their rotors have more air molecules to push against and more inertia to keep everything in line. They don't really need extra stability, making the engineering feat of designing gas-powered engines driving multiple propellers not economically viable.",null,2,cdkakpm,1r5tcm,askscience,new,2
good_n_plenty,"So... not an expert of this but my understanding is that one of the advantages of having two rotors rather than one is that the rotors spin in opposite directions, so their torques cancel each other out. With only one main rotor, you need a tail rotor to control the angle of the helicopter and keep it from spinning in the opposite direction as the rotors are moving. That's why helicopters with two main rotors don't have a tail rotor. Having 3 (or 5, or 7) main rotors would defeat the purpose of having multiple rotors. I don't know about 4. You should ask r/helicopters",null,3,cdk1dic,1r5tcm,askscience,new,2
PepperJack_delicacy,"The other poster is completely right but since you're teaching a science class I thought maybe your students would appreciate a slightly more detailed answer.


Alcohol is metabolized predominantly in the liver and there are **3** important enzymes that you should be aware of. First, alcohol is converted to acetaldehyde by **alcohol dehydrogenase** or **cytochrome P-450**. In turn, acetaldehyde is converted to acetate by **acetaldehyde dehydrogenase**.

Both alcohol dehydrogenase and acetaldehyde dehydrogenase create molecules of **NADH** during these reactions and NADH will inhibit gluconeogenesis and fatty acid oxidation. The important thing to take away from this is that **this leads to a very fatty liver when you chronically abuse alcohol**.

Also, an alcoholic will have an increased amount of cytochrome P-450 (this is because the body tries to adapt to the increased intake of alcohol). Cytochrome P-450 handles many other reactions and will cause the body to create an excess amount of **reactive oxygen species**. Having a lot of reactive oxygen species around creates unnecessary inflammation, cell death, and fibrosis. 

*So overall, an alcoholic will have a fatty liver and an excess of reactive oxygen species. Together, this creates a lot of damage in the liver tissue, which over a long period of time will lead to cirrhosis.* 

I hope this answers your question. Let me know if you want me to clarify anything. 


Source: http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hepatology/alcoholic-liver-disease/",null,2,cdjxr3r,1r5rxu,askscience,new,14
then_and_again,"the body processes ethanol by oxidizing it to acetylaldehyde, and then to acetyl-CoA. a lot of the enzymes needed are in the liver, and the liver takes the brunt of alcohol metabolism. Why some people get cirrhosis is still unclear, we know that the liver undergoes oxidative stress, which causes apoptosis and eventually fibrosis. What we don't know is why this only happens to some people, most people get hepatitis instead.  ",null,2,cdjv9o2,1r5rxu,askscience,new,7
wishfulthinkin,"We have adapted to live in this pressure.  Reduced pressure is not equivalent to increased health, as evidenced by the death of many deep sea fish and crustaceans when they are brought too close to the surface.  Their bodies have evolved for life in the high pressure of the sea floor, and upon losing the pressure, their bodies lose structural integrity and they simultaneously increase in size and decrease in stiffness.  The blobfish picture everyone has seen is a good example.  In its natural habitat, the blobfish looks similar to most other fish, but when it is brought up to the surface, a low pressure area for it, its body collapses and it looks like a gelatinous blob.",null,6,cdjw6hn,1r5qol,askscience,new,16
goldistastey,"The atmospheric pressure is balanced by our internal pressure which is just due to us being a bunch of compressed stuff, and our cells and tissues have evolved this pressure to match the atmospheric pressure. That's why in space, with no atmospheric pressure, our internal pressure would kill us.",null,2,cdjz0eb,1r5qol,askscience,new,6
adamhstevens,"Mainly because we are mostly composed of incompressible materials (i.e. water). In fact, the airspaces in the body (i.e. lungs, stomach, sinuses, ears) are generally in equilibrium with the atmosphere, so air moves freely in and out, preventing a pressure difference.

Problems actually occur in certain situations (e.g. diving) where these airspaces are blocked from equilibrating and the pressure difference can cause tissues to rupture, but most of your body is unaffected.",null,1,cdkgsba,1r5qol,askscience,new,3
Javi2639,"Soap is the salt of a carboxylic acid with a long hydrocarbon tail. The ion head will interact strongly with polar water, while the hydrocarbon tails will interact strongly with each other. This forms a bilayer with the tails pointing inward towards each other and the heads pointed outwards towards the water, forming a sphere. It is the same principle that creates the cell membrane of all living creatures. ",null,1,cdjw2rs,1r5o2u,askscience,new,3
slartibartfastfive,"Presence of surfactants; water is a racist.

Water is very cohesive because it's small and can form networks of hydrogen bonds. This property leads to surface tension, which is the force that opposes the formation of bubbles--the water doesn't want to be spread out in a thin surface touching the air, it wants to be huddled up in a ball with as many water molecules as possible touching each other.

Surfactants, like soaps or other amphiphilic molecules, have a structure where one part of the molecule is quite polar (water-loving, hydrophilic) and one part is less polar (""oil""-like, hydrophobic, hates water). The surfactant molecules arrange themselves at the air-water interface with the nonpolar parts touching the air. The nonpolar parts of the surfactant are much happier in contact with air than water would be, and the polar parts are pretty happy touching water, so this arrangement lowers the interfacial tension at the bubble surface, allowing it to grow beyond a certain size.

Bubbles will form in soapy water or milk because of this. Mayonnaise and salad dressing are ""emulsions"" of oil and water stabilized by similar surfactants.",null,0,cdl2lc5,1r5o2u,askscience,new,1
Overunderrated,"Good question!

Intuitively, if you have a rigid container which contains a tube at equilibrium, if you then create a vacuum within that tube, you'll be increasing the pressure of the rest of the container.

But earth's atmosphere isn't a rigid box; it exponentially decays towards zero density far from the surface. The hydrostatic pressure you feel at the surface of the earth is due to gravity acting on all the air above you. Now, gravitational attraction decreases with height... and if you were to create a vacuum chamber at the surface, this would have the effect of pushing air up, towards higher altitude, where gravity is weaker. So I could see that creating a vacuum chamber at the surface of the earth might actually *lower* the ambient atmospheric pressure. Analogous to the notion that if Earth's radius doubled, but the total mass of air stayed the same, the ambient pressure would be much lower.


I'm just spitballing here, so corrections/refutations/alternatives welcome.",null,1,cdjv1la,1r5n2t,askscience,new,13
Javi2639,"Differing intermolecular forces. Milk is mainly water, which is polar and can form strong hydrogen bonds with other water molecules. Chocolate is mostly fat, which only have van der Waals interactions. This will cause the two to interact with each other and exclude the other, separating them. ",null,0,cdjvy8k,1r5lx8,askscience,new,5
wishfulthinkin,"Improvements in dental care have significantly extended the lifespan of humans.  The wealthy in pre-modern-medicine times very frequently died from cavities, as only the wealthy were able to afford sugar.  In fact even nowadays, there are instances where people get cavities in a molar that eventually extends back through the mandible and into the brain, causing infection and later, death.",null,2,cdjw217,1r5lfe,askscience,new,10
TrainerGary,"The concept is called [proprioception](http://en.wikipedia.org/wiki/Proprioception). 

I'm sure someone will give a more in depth answer, but essentially there are multiple body maps in your brain, and these maps are used in conjunction with your peripheral nervous system in order to determine body position. 

For example, proprioceptors are found in skeletal muscles. The relative stretching/compression of these proprioreceptors gives information about the position of the limb.",null,4,cdju34l,1r5l7s,askscience,new,26
Lillelyse,"As already mentioned this is called proprioception. To expand a bit on how sensors in the muscles tell you about limb position: The body's sense of its position in space is determined by stretch receptors in the muscles, joints and tendons. Muscle stretch receptors, or muscle spindles, are specialised muscle fibres in a fibrous capsule. These detect changes in muscle length.

Joint receptors are mechanosensitive axons in the connective tissue of joints. They are good at detecting the movement of the joint, but not so useful when the joint is kept still. 

Finally, at the junction of a muscle and a tendon we find Golgi tendon organs. These register muscle tension, or force of contraction. The information from all of these sensors come together to tell us where our limbs are. ",null,1,cdjxdl4,1r5l7s,askscience,new,9
SpacemanSpiff1222,"Without getting too in depth, your body will use proprioceptive sense organs to obtain this information. In the muscle itself, your muscle spindles will pick up how fast and how intense a contraction. Muscle spindles exist all throughout the muscles and in different numbers in different muscles. For example, the sub occipital muscles have a much higher amount of muscle spindles than, say, the biceps femoris. Inside the tendonous attachments to the bones you have Golgi Tendon Organs which will pick up information regarding tension. The information will then travel to the spinal cord through peripheral sensory nerves, in to the spinal cord in the medial lemniscal system. From here it will travel up through the medulla and decussate (cross) at the nuclei gracilis and cuneatus as internal arcuate fibers. The tracts will then head on up to the thalamus (VPM and VPL) and out to the post central gyrus in the cortex of the cerebrum. Skipping a lot of stuff, but that is the basics of it. 

ninja edit: This is purely proprioceptive. Your body will also use the vestibular system to track the position of the head. Also, as simple as it sounds, your body will keep track of your segments using your vision as well. ",null,0,cdk15wc,1r5l7s,askscience,new,2
lengendscrary,"Like it was mentioned before visual stimuli seems to be important in creating your body map. If you're interested in getting a better understanding of proprioception, you can read a short story called the Disembodied Lady by neurologist Oliver Sacks. It is sort of a case study of what happens when you lose your sense of proprioception. It's pretty cool because the lady would not know what her limbs were doing and where they were unless she was looking directly at them.",null,0,cdkj53i,1r5l7s,askscience,new,1
ModernTarantula,The [cerebellum](http://neuroscience.uth.tmc.edu/s3/chapter05.html) may be responsible for most of the body map. It is not part of consciousness. Such that you don't have to think exactly about the movement necessary to touch your knee. I heard tell that if you hold a stick it will become part of the cerebellar map.,null,2,cdkc572,1r5l7s,askscience,new,1
darksingularity1,"I'm assuming you mean the relative positions of your limbs and not just the relative position of your entire body(well just your head really) in space. The latter deals with the vestibular system near your ears. The way I like to think of it is that it covers the three planes of space (x,y,z) with its three orthogonal ""pipes"". 

Now to the issue of knowing where your hand is at any given moment. There are two ways that we can generally figure it out. Or at least there are two theories. So theoretically, in order to find the arms position, the brain can just ""ask"" it or if the arm just moved somewhere it can calculate the new position from the old position and the force/energy/etc that was used to get it there. 

First of all, the general sense of position is called proprioception. We have different sensors on our body to figure out our positions. Most importantly, we have things called muscle spindles within out muscles. These give the brain an idea of how contracted/relaxed the muscle is.
Also, when messages are sent to muscles to do something, they sometimes send something back, called an efference copy, which kinda gives the brain more info on position and stuff.
The brain also uses the traditional senses for this as well. Vision plays a huge part in this. Seeing where your arm is gives you a great idea of where it is (amazing right?!). ",null,8,cdjxkkv,1r5l7s,askscience,new,3
goldistastey,"They're made of generally the same stuff, but in different amounts. More or less fast-action muscles (aka white or dark meat), more or less fat in the tissues, more or less fat around the tissues, more or less fibers between the tissues, more or less gelatin in the bones, etc. Cooked very precisely, you could probably make many parts of a turkey or duck taste like many parts of a chicken.",null,1,cdjz6nm,1r5fge,askscience,new,3
then_and_again,"It does refract light, the refraction index of the vitrous humor is 1.336, water's around 1.333 if i remember correctly. We don't have fish eye vision because your brain is the organ that 'sees' not your eyes. What you see is just your brain's interpretation of certain light signals, so yes the light is refracted, and that refraction causes your brain to see it as 'normal'. if you look through an lens that corrects this refraction, your brain will interpret it as a different image and your vision will be distorted. Basically, your brain is used to light being refracted, so that's the basal image",null,0,cdjvfca,1r5d2l,askscience,new,3
wishfulthinkin,"The vitreous humor does refract light, but the brain is able to counter this by processing the image before you ""see"" it.  Likewise, the image of the world that reaches your brain via the optic nerve is upside down, and the brain flips it right side up.  Similarly, your brain hides the small blind spot that is created by the complete lack of rods and cones on your optic nerve.  When you see anything, it's actually your brain's interpretation of the real image of the world.",null,0,cdjvw0i,1r5d2l,askscience,new,2
FatSquirrels,"It definitely falls into the definition of a molecule, but we tend not to think of it that way when talking about polymers.

When working with [macromolecules](http://en.wikipedia.org/wiki/Macromolecule) we will instead break things up into repeat units or subgroups.  For polymers we look at the repeat units, for copolymers we look at the individual pieces/blocks and for proteins we look at the amino acid sequence.  When looking at something small, a single picture of the molecule is all we need to define, but for things where one molecule is sometimes arbitrarily large we determine the properties of the substance by analyzing smaller pieces and we don't even think about the term ""molecule.""

However, the fact that a piece of vulcanized rubber might be one big molecule is very important to its material properties.  You could theoretically melt it or dissolve the whole thing but it would remain completely connected to itself, just one big connected molecule that has very strange properties when compared to ""normal"" sized molecules.",null,0,cdjrxb3,1r5ckr,askscience,new,6
Brewe,"It is one molecule, but it's usually not treated as such. It's like with crystal structures that are also one big molecule (at least as long as we're talking about a perfect crystal), here we look at a unit cell, the smallest part of the crystal that explains the whole structure. The same is done with polymers, where f.x. a polymer could be shown as [-CH2-CH2-]n, where n is a large number.

Polymers, crossslinked polymers, crystals etc. are technically one very large molecule, but they are not treated as such.",null,2,cdjr831,1r5ckr,askscience,new,4
owaisofspades,"yup, eventually the receptors for it on your tongue will get down regulated with continued exposure. 

Also, it's slightly toxic, so it might even lead to tolerance by damaging the cells that contain the receptors themselves, but I'm not entirely sure ",null,1,cdjx7uz,1r5c17,askscience,new,2
PorchPhysics,"I can answer this from a physics standpoint, but I might be uninformed on a particular topic:

No.

Nuclei are held together by the strong nuclear force, if it were not for this force, then all the protons would repel one another greatly due to the electric force between two charges.  Protons are all positive and would want to repel away.  

If the two nuclei you propose are close enough for the strong nuclear force to keep them from repelling significantly, they would be merged, as the drop-off in strength of the strong nuclear force is extremely rapid over all but the smallest of distances.",null,1,cdjrjz7,1r5bzl,askscience,new,5
Platypuskeeper,"Yes, [halo nuclei](http://en.wikipedia.org/wiki/Halo_nucleus).",null,3,cdjx255,1r5bzl,askscience,new,4
hotshot_sawyer,"Check this out: [antiprotonic helium](http://en.wikipedia.org/wiki/Antiprotonic_helium). It's an electron and an antiproton orbiting a helium nucleus. They make it in 3% yield by simply shooting antiprotons into helium. It lasts for microseconds.

The antiproton is bound in an orbital with n around 38, which is different from normal atoms where you fill up orbitals from n=1 and up with very few exceptions. This seems interesting but I'm pretty helpless with quantum stuff and it would be great if someone else could unpack this aspect a little.

There's also [positronium](http://en.wikipedia.org/wiki/Positronium), which is a bound electron-positron pair that behaves a lot like hydrogen but doesn't last very long.",null,0,cdlzl7r,1r5bzl,askscience,new,1
TheSkyPirate,"ELY5: 

1. Opiates make you feel good, so you want to keep taking them. 

2. Your body uses natural opiates to regulate feelings if discomfort. You are constantly experiences a ton of slightly painful stimuli, and your body uses natural opiates to block these and keep you feeling ""normal.""  Taking artificial opiates causes down-regulation of these natural opiates and the receptors that they bind to, so when you're off them your body is less able to compensate for pain. You end up feeling nauseous and achy because your body can't properly regulate discomfort.  

This is different than normal addiction. You don't just crave opiates, you're consciously aware the you need them to not be in pain, and you want to take them in order to feel happy. ",null,1,cdjz371,1r5a61,askscience,new,5
Javi2639,"Narcotics are opiates, which are chemicals that imitate natural endogenous painkillers like endorphins, enkephalins, and dynorphins. The body releases these in life or death situations, such as when you are running from a predator. If you break your ankle doingso, you will collapse in pain and die without these painkillers. The brain focuses on survival first and repair later in these situations. However, if there is a chemical that can bind to the same opioid receptors as these endogenous painkillers, the receptors will become overstimulated. The brain adapts to the constant stimulus, but upon stopping the opiates, the brain thinks it is in pain, so you take more. This is where the addiction potential comes in. ",null,3,cdjwd3x,1r5a61,askscience,new,4
Takagi,"Percocet is an opioid which increases dopamine levels in synapses (Online edition of Harrison's Principles of Internal Medicine, 18e), and dopamine is a chemical associated with rewards and ""feeling good"". That's the neuro part of why it makes us feel good. There's withdrawal that comes with people who use percocet which can develop between 6-8 weeks of chronic use (ibid).",null,1,cdjswxf,1r5a61,askscience,new,1
Surf_Science,"It varies, even with Salmonella for example a few bacteria (10-20) can cause illness in the right circumstances where in other cases it will take millions. 1 Bacterium could cause illness but it is unlikely. ",null,2,cdjrf7z,1r57f2,askscience,new,10
wishfulthinkin,"As stated before in this thread, it varies depending on the strain and species of the bacteria.  It is theoretically possible for an individual to become ill from a single bacterium, but depending on the strength of the individual's immune system relative to the aggression of the bacterial strain.  In the vast majority of cases, a single bacterium would be destroyed by the immune system before it got a chance to reproduce and further infect the body.",null,0,cdjvsro,1r57f2,askscience,new,2
endocytosis,"As others mentioned, yes.  With [TB](http://iai.asm.org/content/73/10/6467.full) it is widely hypothesized that a single bacterium can successfully infect someone.  This does not necessarily mean that infection will happen every time, but the that it *can* happen.  The journal article is from a scientific group that studied *Mycobacterium bovis*, a related strain of bacteria that was also used to make the BCG [vaccine](http://en.wikipedia.org/wiki/BCG_vaccine).",null,0,cdjznhe,1r57f2,askscience,new,1
Davecasa,"Bone mass decreases, but the bones don't actually get much smaller, just less dense. The height increase is mostly due to lengthening of the spinal column due to not fighting against gravity... the same happens every night when you sleep, to a lesser extent.

Some more info:  
http://www.nasa.gov/mission_pages/station/research/news/spinal_ultrasound.html#.Uo4-hMR9te5  
http://science1.nasa.gov/science-news/science-at-nasa/2001/ast01oct_1/",null,0,cdjqu8k,1r57d7,askscience,new,10
dkreat,"There is a constant gravitational force on your entire body; this is something that your bones, ligaments, cartilage, muscles and joints all have to oppose and overcome to produce the common movements and actions you perform daily. This competition creates strain, and the cartilage and bones over time compress and shrink due to the act of holding your body up.

As /u/Davecasa said, your spinal column becomes decompressed in space and your bones decrease in density. This can compensate for a good portion of your height since the spinal column counts for nearly 25% of the average person's height. The cartilaginous discs that intercalate with our vertebrae can change shape and compress to allow for weight bearing to shift from the bones themselves to the cartilage in certain positions. 

In space, and when you aren't walking bipedally, the weight bearing role of your spinal column decreases, so the cartilage can return to its resting shape and density, and your height increases, even if by a little bit.

Source: I'm in medical school",null,0,cdk5ak7,1r57d7,askscience,new,1
__Pers,"One of the hypotheses consistent with the data (at least back in the 1990s, when I last followed this field) is that gases like N2 and O2 dissociate during collapse, form different chemicals (e.g., peroxide), and thus are lost to the water during implosion. Noble gases don't undergo such chemical changes, meaning then when one has trace amounts of such gases there's a surfeit of argon or xenon left behind to compress and radiate. ",null,0,cdjxnxa,1r55qg,askscience,new,2
zmil,"Think of the human genome like a really long set of beads on a string. About 3 billion beads, give or take. The beads come in four colors. We'll call them bases. When we sequence a genome, we're finding out the sequence of those bases on that string. 

Now, in any given person, the sequence of bases will in fact be unique, but unique doesn't mean completely different. In fact, if you lined up the sequences from any two people on the planet, something like 99% of the bases would be the same. You would see long stretches of identical bases, but every once in a while you'd see a mismatch, where one person has one color and one person has another. In some spots you might see bigger regions that don't match at all, sometimes hundreds or thousands of bases long, but in a 3 billion base sequence they don't add up to much. 

edit 2: I was wrong, it ain't a consensus, it's a mosaic! I had always assumed that when they said the reference genome was a combination of sequences from multiple people, that they made a consensus sequence, but in fact, any given stretch of DNA sequence in the reference comes from a single person. They combined stretches form different people to make the whole genome. TIL the reference genome is even crappier than I thought. They are planning to change it to something closer to a real consensus in the very near future. My explanation of consensus sequences below was just ahead of its time! But it's definitely not how they produced the original genome sequence.

If you line up a bunch of different people's genome sequences, you can compare them all to each other. You'll find that the vast majority of beads in each sequence will be the same in everybody, but, as when we just compared two sequences, we'll see differences. Some of those differences will be unique to a single person- everybody else has one color of bead at a certain position, but this guy has a different color. Some of the differences will be more widespread, sometimes half the people will have a bead of one color, and the other half will have a bead of another color. What we can do with this set of lined up sequences is create a *consensus* sequence, which is just the most frequent base at every position in that 3 billion base sequence alignment. And that is basically what they did in the initial mapping of the human genome. That consensus sequence is known as the reference genome. When other people's genomes are sequenced, we line them up to the reference genome to see all the differences, in the hope that those differences will tell us something interesting. 

As you can see, however, the reference genome is just an average genome*; it doesn't tell us anything about all the differences between people. That's the job of a lot of other projects, many of them ongoing, to sequence lots and lots of people so we can know more about what differences are present in people, and how frequent those differences are. One of those studies is the 1000 Genomes Project, which, as you might guess, is sequencing the genomes of a thousand (well, more like two thousand now I think) people of diverse ethnic backgrounds. 

*It's not even a very good average, honestly. They only used 8 people (edit: 7, originally, and the current reference uses 13.), and there are spots where the reference genome sequence doesn't actually have the most common base in a given position. Also, there are spots in the genome that are extra hard to sequence, long stretches where the sequence repeats itself over and over; many of those stretches have not yet been fully mapped, and possibly never will be.

edit 1: I should also add that, once they made the reference sequence, there was still work to be done- a lot of analysis was performed on that sequence to figure out where genes are, and what those genes do. We already knew the sequence of many human genes, and often had a rough idea of their position on the genome, but sequencing the entire thing allowed us to see exactly where each gene was on each chromosome, what's nearby, and so on. In addition to confirming known sequences, it allowed scientists to predict the presence of many previously unknown genes, which could then be studied in more detail. Of course, 98% of the genome *isn't* genes, and they sequenced that as well -some scientists thought this was a waste of time, but I'm grateful the genome folks ignored them, because that 98% is what I study, and there's all sorts of cool stuff in there, like ancient viral sequences and whatnot.

edit 3: Thanks for the gold! Funny, this is the second time I've gotten gold, and both times it's been for a post that turned out to be wrong, or partly wrong anyway...oh well.",null,198,cdjoort,1r54d1,askscience,new,1053
Chl0eeeeeee,"Even though everyone has unique DNA, genes still would occur in the same location in the genome (exclusive of any mutations that would add/delete a nucleotide). Basically what genome mapping does is look at multiple samples of DNA from different people. It aims to understand what regions are coding versus non-coding, and to annotate the genome (see what the coding genes control). This has been done for other species. ",null,17,cdjopps,1r54d1,askscience,new,77
Tass237,"Complete mapping of what sections apply to what.  A redhead and a blonde both have a gene for hair color, and the location of that hair color gene can be mapped.  The fact that they have *different alleles* doesn't mean it's a different gene or in a different location.",null,7,cdjq7af,1r54d1,askscience,new,29
zedrdave,"In addition to other answers in this thread, one important clarification: when one says that a person's DNA is unique, that's still no more than somewhere around a 0.01% difference, out of the entire sequence, between two individuals.

Most nucleotides (the small bricks that make the DNA sequence) are the same for all individual of the same species (humans, for instance), with a very few single nucleotides changing here and there (these changes are called [SNPs](http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism)). Just the same way that moving a single cog in a complex mechanism, or modifying a single byte in a computer program, will give out a completely different result, that single nucleotide modification can have huge consequences on the person's appearance, health etc.

Mapping the first genome, meant mapping *a* genome (with its specific SNPs), with the implicit idea that we were first interested in the parts that were common to everybody. Now that sequencing is a lot cheaper and more widespread, there are a number of efforts to map genomes for a number of individuals, in order to figure out more specifically which positions in the sequence can occasionally differ (see ""1000 genome project"").

**Edit:** I should have also mentioned that, while some SNP variations have huge effects on the resulting organism, other SNP mutations are completely silent (""synonymous mutations""), thanks to the redundancy of the DNA-Amino Acid transcription code (i.e. different triplets of DNA can end up coding for the same AA). Because such silent mutations do not affect fitness (and therefore are more likely to be passed down), they are a lot more common than you would expect from pure chance.",null,1,cdjud86,1r54d1,askscience,new,9
tdcarlo,"Each person's DNA is unique, that is true.  But the difference between you an me is incredibly small.  

DNA is made up of nucleotides.  There are four kinds of nucleotides.  Think of nucleotides as legos each kind being a different color....let's say Aqua, Green, Cyan, and Teal.  A gene is composed of nucleotides in particular order.  Imagine stacking legos.  Using the first letter of the colors from the legos, the insulin gene is 450 nucleotides long and looks like this.

Aqua Green Cyan Cyan Cyan Teal Cyan Aqua GGACAGGCTGCATCAGAAGAGGCCATCAAGCAGATCACTGTCC
TTCTGCCATGGCCCTGTGGATGCGCCTCCTGCCCCTGCTGGCGCTGCTGGCCCTCTGGGGACCTGACCCAGCCGCAGCCTTTGTGAACCAACACCTGTGCGGCTCACACCTGGTGGAAGCTCTCTACCTAGTGTGCGGGGAACGAGGCTTCTTCTACACACCCAAGACCCGCCGGGAGGCAGAGGACCTGCAGGTGGGGCAGGTGGAGCTGGGCGGGGGCCCTGGTGCAGGCAGCCTGCAGCCCTTGGCCCTGGAGGGGTCCCTGCAGAAGCGTGGCATTGTGGAACAATGCTGTACCAGCATCTGCTCCCTCTACCAGCTGGAGAACTACTGCAACTAGACGCAGCCCGCAGGCAGCCCCACACCCGCCGCCTCCTGCACCGAGAGAGATGGAATAAAGCCCTTGAACCAGCAAAA

So we know what a gene is...the next thing to understand is a chromosome.  A chromosome is a long stack of DNA that contains numerous genes.  There are 23 chromosomes in the human genome.  The longest human chromosome is about 250 million nucleotides long the shortest is around 50 million nucleotides.  Each chromosome contains hundreds of genes along with some other ""accessory"" DNA that is beyond the scope of this explanation.    The entire size of the human genome is around 3 billion nucleotides.

Human being the clever types have been able to determine the precise order of all of the nucleotides in each human chromosome and have identified most if not all of the genes on it.  So each chromosome has the location of each gene mapped. Pretty amazing.

Your DNA is unique but the percentage of the 3 billion nucleotides that are different than mine is less than 0.0001% and most of the differences will be in the so called ""accessory"" DNA.  ",null,0,cdjtoip,1r54d1,askscience,new,8
nanoakron,"I feel the need to write this because whilst all the previous commenters have gone into great depths to explain the science behind genes and genomes, they have failed to address a fundamental misunderstanding the OP has:

Your DNA is **NOT** unique. Only about 0.1% of it is. You are somewhere around 99.5-99.9% **genetically identical to every other human on the planet**.

You're also 98.8% **identical** to every chimpanzee, 98.4% **identical** to every gorilla, 88% to every mouse, 65% to each chicken and 47% genetically **identical** to a fruit fly.

This means you have the exact same codes (give or take a letter) for the most essential 'housekeeping' functions - the ones that process energy in your cells, allow your cells to reproduce, build cell walls, cell skeletons and the other basic stuff all multicellular life needs to do. As a side note, this is very strong evidence that these abilities evolved only once in a distant ancestor, and then because they were so successful compared to all species around at *their* time, they outcompeted them and all their descendants now share those genes.

The closer you get to a human in genetic relatedness, the similarities extend beyond simple housekeeping genes to those which allow us to be 4-limbed, air-breathing, visually-dominant omnivores. Cows are 4 limbed - we share the same genes which switch on in embryonic development which cause 4 limbs to develop. We *also* share these with fish - after all, these are the genes which were first used to make fins, they were just 'repurposed' to make limbs through mutation and natural selection.

And so on with all 30,000 genes that make us human. We're not even genetically the best at doing many things in the animal kingdom - plants 'eat' sunshine, some bacteria detoxify alcohol better than we can, and as for our radiation susceptibility, we're pathetic. We just so happen to carry the baggage of every creature that came before us that was able to reproduce.",null,3,cdk283p,1r54d1,askscience,new,10
knobtwiddler,"I work in genetic informatics and we sequence and analyze human genomes.  ""complete mapping,"" rather optimistically, means is that we have assembled a reference genome of a number of pooled humans' gene sequences, so we know where a typical human's sequences fall in the chromosomes from beginning to end (around 50 billion base pairs).  This assembly is used as a reference to compare against.  Currently we are using a reference genome sequence called HG19. HG20 (human genome v20) is coming
out soon.   It's an ongoing process.


From this reference genome we can align pieces of sequenced dna from samples in an effort to to say where those pieces of dna came from in the genome.


This is far from an exact science, and there are large portions of the genome for which we have no clue about their function.  However we have identified around 56,000 protein-coding genes (the exome) and a large number of ""intronic"" non-protein-coding regions which do code for RNA (lncRNA), some of which are functional, most of which we don't know anything about (previously referred to as ""junk dna""). 


believe me though, as far as understanding the function of all these genes, let alone the non-coding regions, the process is far from complete.",null,0,cdjqpyk,1r54d1,askscience,new,7
BillieHayez,"How interesting that you ask this question today. Fred Sanger, a pioneer in the mapping of the human genome, aged 95, and winner of two Nobel prizes has just passed. Maybe you were tuned in this morning, as well.",null,0,cdjqzu4,1r54d1,askscience,new,6
jams2014,"Think of the Genome like the spec sheet for a car, except it's been broken up into 46 text files and compressed so that the data is all mashed together into 46 strings, and somewhat difficult to parse out. Somebody didn't comment their code. If we were just trying to read the strings, and infer what they mean, we would fail. But luckily! there's also an automatic, computer-controlled factory that reads the strings and builds stuff! (Cells in the body.)

In the simplest sense, genome mapping is about making the factory build from *parts* of these strings, so that we can see what they do.  Imagine that you run your fictional automatic car factory like normal - it builds you a hot little red Corvette. Now imagine that you take part of the instruction string and copy/paste/copy/paste that part until you've made that section repeat a bunch of times. When you run the factory again, the car comes out a *deep, vivid red* instead of the ordinary red from before. 

You've found a gene for the paintjob, but you don't know for sure whether you've found the gene for red paint only, or for the whole thing. Now, that section might be a little bit different in someone else - like, maybe it's a different color. If you enhanced that section in someone else's instruction sheet, maybe you'd go from blue to a more vivid blue (if all of the color selection is in that part). Or maybe you would just add red, so that someone's purple paint would approach pink.

Anyway, what you've found is the meaning of a section of the instruction sheet, but it can be difficult to determine exactly which of the machines are activated by each string. Sometimes the instructions trigger other instructions, and wind up causing lots of parts to move. Sometimes they trigger something very tiny - like spinning a part of one machine. And sometimes they don't do anything at all (like bits of commented-out code). And sometimes they do something, but don't appear to unless certain conditions are met - imagine instructions to turn on or off some safety feature on the factory floor.

- EDIT - 

To perfect the analogy - we're not talking here about running the whole apparatus to create new cars. That would be like making changes to an embryo's genes and letting them grow up, which is unethical.

It's more like flipping switches in the factory while the assembly line is down, just to see which machines start to spin, or spray paint. ",null,1,cdjw201,1r54d1,askscience,new,4
tsacian,"The best way to understand what scientists are doing with the human genome, it is best to look at a much smaller and simpler genome (such as the **Japanese Rice Genome Project**).  It is simpler because the rice being mapped only has 9 chromosomes, whereas humans have much more.

http://rgp.dna.affrc.go.jp/E/GenomeSeq.html

Here you can click on a chromosome and literally see the sequences which have been directly mapped.  The difference is the wealth of knowledge already learned from this project due to its ""simplicity"", such as finding genes responsible for specific proteins and tracing them all the way back to the base pair patterns.  You can search through the big discoveries, and even look for specific proteins.

Click on chromosome 1 and then click the link for the first accession.  This first set has 31,687 base pairs (bp) (think ATCG).  You can then click on a gene and see the sequence that scientists believe is responsible for a gene.  The reason it is a ""gene"" is because it has the correct properties for coding of a gene, including a start sequence (a pattern they look for that is typical for the beginning of a gene), and a stop sequence (called codons).

Additionally, you can click and see a specific pattern of base pairs responsible for coding an mRNA and even specific proteins.  Using these ""Maps"", scientists can study each chromosome and find which genes are responsible for specific attributes of the organism.  We can find which sections of DNA are responsible for specific proteins, and use that to find mutations that result in the absense or mutation of a protein that causes harm in an organism.  There is really a wealth of information.",null,0,cdjwez6,1r54d1,askscience,new,3
XSlayerALE,"Mapping the Human Genome is like identifying the parts of a car. Sure, a wheel can be Pirelli, Firestone, Goodyear or whot not but we know its a wheel and its not the axle or the brakes or that funny triangle sign on your dashboard that no one really knows what it does....
",null,0,cdk6q2y,1r54d1,askscience,new,3
futuregp,"simply speaking, think that all humans have the same genes that have specific functions (and every human being needs these to be considered human)

but each gene can have different traits (blue eyes, brown eyes etc)

complete mapping of the human genome is to identify all those functional parts of our DNA (most of our DNA is technically not 'functional' and doesn't play a part in protein synthesis)

Each functional part ('functional gene') would have different traits, and every human being is composed of permutations/combinations of these millions of gene traits combined (e.g. let's say we only have 2 genes, A/B. Gene A has 2 traits - male (m) or female (f). Gene B has 2 traits - tall (t) or short (s).

I'm a short male. I would have A(m), and B(s) genes. You are tall and female. You would have A(f), and B(t) genes.
We're both unique, but that doesn't mean you have to map both of us to realize that there are 2 genes.

By mapping a single human being, you can map all the genes of the human genome. The uniqueness comes not from which 'gene' you have but which 'trait' of the gene you have.",null,0,cdjot9c,1r54d1,askscience,new,3
Drfilthymcnasty,"I may be wrong, but I think a complete ""mapping"" means a complete understanding of all the functional genes in our DNA. So while we may know the general sequence of nucleotides, our understanding of how/why certain segments get translated into proteins is not yet complete. Also we still have a long way to go understanding epigenetic changes and controls.",null,3,cdjouzf,1r54d1,askscience,new,6
liteerl,"Each person's DNA is unique, but their DNA differs from others at certain genes. You could record the common variants of each gene (these variants are called alleles), although certainly you would be very like to miss some of the variation. Individual's DNA sequences are not all completely random, but differ from each other in predictable ways.",null,0,cdjrxb1,1r54d1,askscience,new,2
shanebonanno,"Everyone's DNA is unique, however, nearly all of it is shared with every human on the planet. Only a very small part is unique. When scientists talk about the genome of any given species, they basically mean a list of the genes in the DNA of the species and eventually what they do.",null,0,cdjuwzx,1r54d1,askscience,new,2
dreamhunters,"Or think about it this way: it is not some much about the content but about the placement. The genes are somewhere in the genome, their position is much more fixed that the genes themselves. That is why we use mapping, because as with a map it is about location. ",null,0,cdk9y90,1r54d1,askscience,new,2
Hillsbottom,"I am a biology teacher and I use the following analogy. 

Think of the genome as a recipe to make bread. A recipe is basically a list of instructions that need to be followed in a particular order to get the desired result. These instructions are analogous to genes. 

Bread is not all the same; you get white, brown, wholemeal granary, bananana, pumpkin etc. These differences are due to slight changes in the instructions to the recipe eg putting white flour in instead of brown. The instructions are basically the same they are just different versions of it (in genectics these are called alleles; different versions of the same gene).

What scientists have done is got lots recipes (genomes) for many differents type of bread (people, including Ozzie Osbourne!) and worked out the order the instructions (genes) go in. They have created a map of how to make a bready human.

The instructions you have as a human are almost indentical to all other humans however the the combinition of which type of instructions you have is unquie to you (with a few exceptions).

So now we have this massive recipe of how to make a human that we can compare with indivdual humans and look for difference and similarities. 

",null,4,cdkdx9l,1r54d1,askscience,new,4
Sherm1,"I don't think that a true complete map could ever exist, because it would have to know how each sequence of code would react to every given environmental stimulus, including what its surrounding genes might be. How do we know that some of these stretches of ""junk dna"" don't become active when bombarded by some special radioactive waves that aliens can emit from their space ships? 

Point is, we need to not only know what the sequences are, but also what they do, and what they do is always determined by their interaction with the environment.",null,1,cdjoxib,1r54d1,askscience,new,3
the_sex_kitten,"Although each sequence is unique, there are still common gene codes that exist in each of us. By mapping the genome, they are able to locate these codes. For example, the gene for cystic fibrosis is located [here], and since we know that we are able to specifically look [here] for that gene. CF is way more complicated than that because there are a number of different genes that can be mutated, but that's just one example. Basically it allows us to determine the relative location of where potential mutations can occur. Apologies for the lack of sources and simplicity in my response. And please anyone feel free to correct me if I'm wrong!",null,1,cdjrcly,1r54d1,askscience,new,2
smfdeivis,"Only around 0.1% of the DNA between humans is different! So 99.9% genomic human DNA is the same. That 0.1% accounts for observable characteristics (phenotypes) like hair,eye, skin colors, and many others. Complete mapping of the human genome is basically mapping these conserved 99.9% of the DNA which codes for various essential peptides that make up proteins that give rise to tissues. There is a new project on the way called, ""the real human genome project"" Prof. Erick Lander gave a great summary of it on youtube!",null,0,cdjsquj,1r54d1,askscience,new,1
civilizedanimal,"This really depends on what definition you are using. Strictly speaking, mapping a genome is marking out where genes are located on the chromosome. Again, we are talking genes, or chunks of DNA that code for something. Most frequently, when people talk about mapping the human genome what they are actually referring to is sequencing the human genome. Sequencing the human genome is simply recording the sequence of nucleotides in a complete set of human DNA. They do this by sequencing more than one person's DNA and then averaging it. In order to map the genes, they would need to do a lot more research. When we finally get all the genes mapped, we will know what portions of human chromosome code for something. Even after mapping out all the genes it still takes a long time before you can determine what genes code for what.",null,0,cdjtir5,1r54d1,askscience,new,1
DLove82,"Mapping tells us the relative location of stretches of DNA that actually encode something (genes). This arrangement is very very similar between individuals (rarely, duplication, deletion, or transposition events can add, move, or delete a region of DNA, but that is uncommon), even if the genes themselves differ slightly on occasion. The genes are arranged in a group of 23 different unique chromosomes, or HUUUUGE stretches of DNA that are wrapped up really tight.

Mapping tell us the location of one gene relative to another in one dimension (along a line). (EDIT: 3-dimensional genome sequence is all the rage now - it actually looks in 3D at which stretches of DNA are in contact or close to which others - this is very important because those local interactions between genes REALLY far away have turned out to really impact gene function) Each of these genes is composed of a sequence of building blocks, or nucleotides, of which there are four - A, T, C, G (each is a slightly different molecule). The sequence of these nucleotides in a gene determine almost everything about its function - when it turns on and off, what it makes, what cells it's active in. Between individuals, the sequence of these genes is nearly identical, because the products of most genes (proteins) only function if they are composed of precisely the correct sequence of molecules (amino acids). Some, however, can work to varying degrees when the sequences are slightly different. If these occur in more than 1% of the population, they're called ""polymorphisms."" If they occur in less than 1% of the population, they're regarded as ""mutant"" forms of a ""wild-type"" (or normal) gene. 

So, in fact, mapping a bunch of individuals genomes actually allows researchers to come up with a heat map of the building block changes that occur in individuals. Genomic mapping is actually what tells us specifically what areas of the genome are unique between individuals. This can be immensely helpful in disease research where large regions of chromosomes are duplicated, lost, or moved. By mapping genomes, we can say which genes specifically are lost in a certain disease, narrowing down the number of genes which might cause the disease. For example, Down syndrome is caused by an entire extra copy of a chromosome (I think it's 21). That means these individuals have an extra copy of ALL the genes on that chromosome. And since we've mapped where all the genes in the genome are, we can identify which genes might be involved in Down syndrome (this is just an example, it's not really all that practical since the chromosome encodes THOUSANDS of genes).

tl;dr: The unique components of a person's genome are very few relative to the HUGE size and homogeneity (""sameness"") of the genome as a whole between individuals. For the most part, we all have the same number of chromosomes, each with the same number of genes in the same orientation. Complete mapping of the human genome allows us to build up a heat map of the few little areas where genes actually are unique, and see how common those changes are; if they're associated with disease, etc.",null,0,cdjtw4g,1r54d1,askscience,new,1
SMURGwastaken,"It means we've sequenced all of a person's DNA and worked out what each part codes for - whether it be amelase for digesting simple carbohydrates or amelogenins for producing tooth enamel, or the homeobox genes for deciding which organs and body sections go where. Since all humans are essentially identical in terms of how they work, all humans will have the genes for these things. Only about 0.1% of your genes are different to another human, and you'd be surprised at how little the difference between you and any other vertebrate (or even any other eukaryotic organism) is.",null,0,cdjuvtu,1r54d1,askscience,new,1
EvOllj,"There are differences on individual DNA that get completely ignored/lost when they are read, because the reading mechanism is very error tolerant. And there ate a LOT of differences that never get read.

And the differences in appearances are so small compared to the whole genome, that the genome of all humans is basically the same, all genes do the same thing, some are just more active and rarely a few barely important genes are disabled or damaged.",null,0,cdk5fgr,1r54d1,askscience,new,1
S7R4nG3,"You should check out [Newton's Law of Cooling](http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html). Its a fairly simple differential equation that has solutions as an exponential increase or decay that, in your case, would be dependent on the thermal properties of the cups.

You end up with an equation like:
T (t) = Ta + (To - Ta) e ^ -kt. 

-Where Ta is the surrounding temp. of the room.

-To is the initial temp. of the water in the cup.

-k is a unitless constant based on the thermal properties of the cup. The negative value is due to the solution of the original differential equation.

-t is the amount of time the cups are left in the room.

-T(t) is the temp. the cup falls to after time t, has elapsed.

Obviously to find one value, you would need to know the other three.

Edit: I can't read the link I posted.
",null,1,cdjrze0,1r53ot,askscience,new,4
SonOfOnett,"Since it's equally valid to model the situation as heat dispersing or cold dispersing with the diffusion equation, in an ideal situation they will both reach room temperature at the same time.

In actuality the thermal conductivity of the water and cup will actually vary with temperature, meaning that it might be easier to transfer heat into or out of the cup at higher/lower temperature. Therefore the answer actually depends on the equilibrium temperature you are trying to reach.

Another complicating factor is phase transitions, but I think you want both the cups to be full of liquid water at and the room to be at standard temperature and pressure.
",null,4,cdjt7hb,1r53ot,askscience,new,5
cmuadamson,"The amount of energy required to change the temp of a liquid by 1 degree does depend on the starting temperature.  Imagine you have a pipe from a steam plant that it used to heat buildings (This is a real scenario, there are underground steam pipes all over Pittsburgh, to the hospitals, universities, etc)  Suppose the steam starts at 500 degrees and goes into building 1, comes out at 400 degrees, then goes into building 2, and comes out at 300 degrees.

Both customers seemed to take 100 degrees out of the steam. If you bill customer 1 the same as customer 2, you're cheating customer 2, because 1 got more energy out of the steam. This is the ""enthalpy"" of the steam. No, I did not *mithpell* that, it is [enthalpy](http://en.wikipedia.org/wiki/Enthalpy).

Similarly, if you look at the energy difference between room temp and your cold cup and hot cup, the hot cup has a bigger enthalpy difference, and has to radiate more energy to get to room temp  than the cold cup has to absorb to get to room temp. So I would say the hot cup will take longer.
",null,0,cdjx5gp,1r53ot,askscience,new,1
null,null,null,2,cdjqppn,1r53ot,askscience,new,1
wishfulthinkin,"Clots are created by platelets and fibrinogen, which form fibrin, a web of solids that clots consist of.  Normally, this happens when blood pools at the location of a cut, forming a scab to prevent infection and allow for the formation of new skin or scar tissue.  The fibrin cannot be formed when blood is circulating at normal speeds, as any beginnings of a web will be destroyed by the moving fluids around it.


This has many benefits but some disastrous consequences.  If our blood did not behave in this way, every time we got a cut (on our skin, or internally, anywhere) we would slowly bleed out until we died, if we didn't die from infection first.  However, sometimes this will result in fatal blood clots, aneurysms, etc.",null,2,cdjwdjx,1r52wz,askscience,new,5
abstrusey,"Veterinaran here: I know of no evidence to support that idea.

I was unable to find the source, but there have been some instances of different species that were so phenotypically similar (I believe the distinction was visual patterns in birds or moths) that they confused other non-same species as a member of their own species. It resulted in sufficient unsuccessful matings that one of the species was ultimately wiped out (or could no longer be found). 

Ultimately, it begs the question of defining ""mating preferences"" more clearly. Not many species mate for pleasure. Also, it appears that they are biologically driven to reproduce because they are wired that way, but it's not evident that they have any conscious ""thought"" in the decision of whether to mate or not to mate with another animal.",null,0,cdl10a3,1r52dn,askscience,new,3
chrisbaird,"Spacetime itself contracts and not just the objects in spacetime. The size of objects, atoms, etc. *relative to their own spacetime* stays constant. In other words, if a spaceship contracts in half due to high velocity, a ruler in the rest frame of the spaceship will also contract in half, so measuring the length of the spaceship using that ruler will give you the same number, no matter what frame you are in. This is important, because the laws of physics must be the same in all frames even despite relativistic effects. ",null,1,cdjt5fg,1r513d,askscience,new,9
NGC2467,"A Planck length is just a unit of measurement. It has no special significance. (Quantum field theory is often said to ""break down"" at the Planck length, but you could just as easily say it breaks down at two times the Planck length, or pi times the Planck length, or 0.12345 times the Planck length) What is a Planck length-long stick in a comoving frame will not be a Planck length-long stick in another, the same way that a meter-long stick will not be a meter-long stick in a different reference frame.",null,3,cdjtr82,1r513d,askscience,new,6
KerSan,"First, I want to be clear about a misconception you share with many professionals. Objects do not appear to shrink at relativistic speeds so much as they rotate in space-time. The ""Terrell rotation"" is pretty well explained by [this YouTube video](http://www.youtube.com/watch?v=JQnHTKZBTI4) (bad quality, sorry). Other replies mention this.

As for your question about the effect on various particles, you should remember that elementary particles are point-like -- they don't really have an extent in space beyond the spread of their position-basis wavefunction. The position-basis wavefunction can indeed undergo relativistic effects, but the more important consideration is the energy of the particles.

Finally, your question about the Planck length. The Planck length is just a unit of distance, like a meter or an inch. If you run really fast, do you think that the inch changes? No, it's a definition. The number three also doesn't change if you run really fast, either. The Planck length is an abstract quantity, not a physical thing.",null,1,cdk2gae,1r513d,askscience,new,2
null,null,null,4,cdjqxe2,1r513d,askscience,new,3
then_and_again,"It's most likely due to the timed expression of different genes. Not all genes are expressed at once; at certain points your body will activate genes that code for the expression of, say, a mustache or increased bone growth, etc. Most likely the hazel and dark brown genes were not being expressed until you hit a certain age. Since brown hair and hazel are caused by increased melatonin expression you wouldnt even need to 'turn off' the blond/blue gene, just upregulate the melatonin gene. Hope that made sense...",null,0,cdjuk2g,1r5110,askscience,new,1
then_and_again,"bad breath is caused by bacteria in your mouth. One of the mouth's best ways of cleaning itself is saliva flushing, where saliva carries the bacteria down your throat and into the stomach. When you sleep, saliva production generally slows down. No saliva, no cleaning, and therefore, bad breath.",null,0,cdjub41,1r509m,askscience,new,12
SigmaStigma,"The basis of electrogenesis is derived from a salt pump. These pumps generate ion gradients with an electrical potential. It uses sodium (Na^+) and potassium (K^+) ions, which are actively pumped in or out, to generate en electrical potential. Remove K^+ and you'll get a negative charge inside of a membrane. [This article](https://en.wikipedia.org/wiki/Resting_potential) goes into depth on that topic.

Some animals have evolved mechanisms that use these ion gradients in pretty interesting ways in what are called electric organs. You can think of them as essentially batteries, which they are constantly charging. These battery-like muscles store a charge, and the animal discharges them for whatever purpose they use them for. Some use it to paralyze prey, others as defense, and others as communication and navigation.

Electric eels aren't the only ones with electric organs. [Atlantic stargazers](https://en.wikipedia.org/wiki/Atlantic_stargazer) have electric organs derived from the sonic muscles. In other fishes the sonic muscles produce vibrations on the gas bladder. The croaker, or drum, is a good example of a fish that uses sonic muscles. Another stargazer species has its electric organ derived from eye muscles.

The [elephant fish](https://en.wikipedia.org/wiki/Mormyridae) use them in very turbid, or cloudy, waters to navigate.

And on a related note, some animals have electro-recepting organs. Sharks, for example, have what are called [ampullae of Lorenzini](https://en.wikipedia.org/wiki/Ampullae_of_Lorenzini), with which they can detect the muscle movements of animals. This is helpful for burrowing in the sand or mud for food that may be hiding.",null,0,cdk3zgw,1r4zeu,askscience,new,5
Davecasa,"A 200 meter telescope operating at the diffraction limit would be able to resolve objects about 1 meter apart on the surface of the moon using visible light. One meter resolution makes a lunar lander about 4 effective pixels wide. Probably not quite good enough to actually confirm what you're seeing, but I'll call 200 meters a lower bound. The largest optical telescope on Earth has a 10.4 meter primary mirror.

The Lunar Reconnaissance Orbiter has a 0.195 meter primary mirror, and photographed Apollo landing sites from an altitude of 31 km. This should give it a resolution of about 10 cm, but the stated resolution at this altitude is actually 50 cm, so diffraction on the primary mirror is not directly a limiting factor on resolution. [Here's an example image of the Apollo 11 landing site](http://featured-sites.lroc.asu.edu/view_site/14).",null,1,cdjtjo8,1r4z3k,askscience,new,5
PorchPhysics,"I don't know about being able to see them from Earth, and I actually doubt it.  HOWEVER, i do know that the LRO (Lunar Reconnaissance Orbiter) did manage to photograph the landing site of Apollo 11 in enough detail to make out the small paths left over of the footprints surrounding the remains of the LEM as well as a small excursion that Armstrong took to a nearby crater.",null,0,cdjrmu4,1r4z3k,askscience,new,2
Dannei,"I would argue that globular clusters *are* part of the galaxies they're orbiting - galaxies have a lot of structure below their large-scale size! There's no real reason for the clusters to break up - like the Solar System, they are bound much more strongly to themselves than to the galaxy they are part of. They also spend much of their time very far away from anything else that is very massive, such as other clusters or molecular clouds, and so don't get disrupted. However, we do see evidence of globulars being disrupted as they pass through the disc of the MW on their orbits - these tend to form tidal streams of stars that precede/follow the cluster in its orbit, as well as ""stealing"" some.

Clusters (both globular and open) are generally thought to be made by the collapse of massive clouds of gas. These form large amounts of stars in a small volume of space, and so the stars remain gravitationally bound to each other. Specifically talking about globulars, they are suspected to have been made during the formation of the galaxy, collapsing to form stars before the rest of the Milky Way had fully formed - hence why their orbits do not resemble those of stars in the disc, which were formed at a later stage.

For reference, open clusters are those that have formed, and are still forming, in the disc - these do tend to break up quite quickly, firstly as they become unstable once the gas cloud they are forming from dissipates, and also due to gravitational interactions with other structures in the disc.",null,1,cdjmj5f,1r4ym5,askscience,new,5
wishfulthinkin,"You feel with your sensory neurons, and the vast majority of your sensory neurons are concentrated on your skin and especially at your extremities (finger tips, toes, face).  As such, there are very few sensory neurons in your internal organs, so it's very difficult to localize feeling in your intestines.  However, there is an abundance of nociceptors (pain receptors) in your internal organs as well as your skin, in case of injury.


That is because the purpose of pain is to alert the brain about bodily damage, and to condition you to avoid pain causing activities in the future.  Damage can be caused everywhere, but light touch and pressure sensing is only necessary on your skin and extremities.",null,0,cdjxo4r,1r4yaq,askscience,new,2
assay,"Heya mate, you can feel pain in your intestines, it just doesn't localize well.  Your spleen and liver pain localizes a bit better, but what you're feeling as sharp pinpointable pain is due to irritation of the peritoneum (a lining in your abdominal cavity that has somatic (and not visceral) pain receptors.  Take another example: appendicitis.  It often starts out as pain in the mid epigastric area above the belly button, but once the infected appendix touches the peritoneum lining the abdominal cavity, the pain becomes sharp and very localized to the lower right quadrant.",null,1,cdjsp6m,1r4yaq,askscience,new,2
The_Serious_Account,"Let's take the Riemann hypothesis. It's statement about *all* primes. The naive approach is to let a computer run through all primes, one by one, and check if the hypothesis is correct for that prime.

You know how many primes there are? It's a lot. Like a lot a lot. Infinitely many primes, in fact. Such a computer would need to do infinitely many calculations. Neither normal or quantum computers are capable of such a thing. 

So unless we discover more exotic forms of computation, this approach will never work. ",null,3,cdjqhva,1r4x1p,askscience,new,14
Vietoris,"How do you think a computer work ?

Computers are not magical creatures. They can do only what we teach them to do. They cannot solve problems that humans cannot solve.


The only thing computers can do better than us is doing very large computations in a very short amount of time. But it is important to understand that any computations that can be done by a computer could be done by a human using paper and pen. It will just take the human much more time.

Now the last piece of thing you need to understand is that almost all interesting unsolved maths problems are not computational problems, but theoretical ones. It's not the computational power that we lack, but the method to solve the problem.",null,4,cdjrs1v,1r4x1p,askscience,new,17
farhaven,"Because these math problems are not about calculating some large number. They are about fundamental properties of numbers and their relations. It takes a human mind to analyze these relations, form theories and finally proove them. Computers can aid in this, for example by doing prime factorization of very large numbers, but they are only a tool, never the solution.",null,1,cdjqfii,1r4x1p,askscience,new,5
selfification,"All proofs come down to starting with a fixed set of rules, a set of start points and a goal and then figuring out a way to use the set of rules you have to somehow transform the start points into the goal.  It's like being given a maze, a starting position and set of simple rules like ""you can go forward by 2 spaces"" or ""you can turn left, but only if you've already visited the square or your right"" or stuff like that.  If the maze is fairly limited, then a computer can just exhaustively search it faster than a human can intuit his way through it.  If the maze is extremely large, then exhaustive searches are out.  A computer stills needs to be taught heuristics - how to search through a large search space without doing unnecessary work.  These heuristics are human heuristics and are limited by how clever we are.  We do have machine learning algorithms and expert learning systems that try to emulate human pattern matching and inference skills, but these are still programmed to emulate some fixed, limited subset of human intuitions and skills.

That said, if we do have a certain intuitive ""inkling"" as to how a proof needs to proceed, we can easily program computers to exhaustively verify the parts that are impractical for a human to do.  See http://en.wikipedia.org/wiki/Four_color_theorem .  The first proof of this was based on a *massive* computer generated proof that generated 2000 or so possible cases and then solved every one of them mechanically using one set of techniques.  No human could have walked through all of those quickly or correctly.  The output proof was virtually uncheckable by humans.  One could verify that the computer code was correct, but the proof itself was too voluminous to verify by hand.  That's one example where computer step in to *assist* humans in tasks that we're not good at.",null,0,cdk367m,1r4x1p,askscience,new,2
Glimt,"Actually, we can, but it will take very long time. We can define formally when a string of letters is a proof of a given sentence with a given set of axioms (first-order logic). It is then easy to write a computer program to check if we have a proof.

Now all we need is for a computer to check all strings, in some order, and check if any of them is a proof of a given conjecture, or of its negation. If the conjecture is not undecidable, the program will surely halt.

This method is, of course, completely non-realistic, due to the number of strings that need to be checked.",null,0,cdkeomw,1r4x1p,askscience,new,2
jjandre,"From a layman's perspective, computers are programmed with math, the math to make those promblems machine solvable has to be written. You'd have to either already have the solution, or be pretty confident you were already headed pretty deep in the right direction of a solution just to write the code. Sounds like more of a job for AI. ",null,0,cdjzuof,1r4x1p,askscience,new,1
winduken,"A typical use of computers to solve an interesting math problem takes two steps. First, analyze the problem to boil down to some possible large but finite number of configurations that can be checked for correctness. Then, either check each configuration by hand or write a computer program to do the checking instead. The 4-color theorem for planar graph (http://en.wikipedia.org/wiki/Four_color_theorem) was solved this way. Unfortunately, it isn't obvious how to do the first step for most interesting math problems.

Incidentally, the P = NP problem (http://en.wikipedia.org/wiki/P_versus_NP_problem) in computer science is one the Millenium problems. It asks a simple question: Given a combinatorial problem, if every guessed solution can be checked for correctness by a computer in a number of steps bounded by some polynomial, is there an actual algorithm to come up with a solution in a number of steps bounded by a (possibly different) polynomial?",null,0,cdk83hh,1r4x1p,askscience,new,1
rapist1,"I am surprised to find that all the comments this far do not answer your question correctly, so I will. 
You are right to think that a computer can try to brute force search for a proof of any statement in your axiomatic system, however if your axiomatic system can express 0,1,2,3... etc, then you will never stop searching for true statements. That is sort of redundant however, but it gets worse; even if you pick a specific sentence you want to search for a proof or disproof of, the search may never halt because some sentences are ""independent"", and cannot be proven true or false in those axioms. It is stated in the second paragraph of [Godel's incompleteness theorems](http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems)",null,0,cdn9tdl,1r4x1p,askscience,new,1
PorchPhysics,"All atoms have small motion from the kinetic energy at an atomic level.  When you put matter under pressure, you increase the number of collisions between neighboring atoms which leads to more chaotic motion of atoms and constant transferring of this energy.  Planets are constantly applying pressure to their cores due to the outer layers pushing inward, which adds more energy and so on.

it can get much more detailed than this, but that is the general idea.",null,1,cdjqyq1,1r4r9g,askscience,new,7
chrisbaird,Gravitational potential energy is converted to heat when the gravity of an astronomical body crushes it to a higher pressure.,null,0,cdjsju8,1r4r9g,askscience,new,5
goldistastey,"The original theory was that the heat comes from the leftover formation energy of the planet. After all, for heat to diffuse through the entirety of the earth into space would take billions of years. The calculations, however, found that this does not account for all the heat, so now scientists think that it's a mix of the contributions of radioactive decay and formation energy. Consensus is that there is no fusion in the Earth I believe, btw.",null,0,cdjza2l,1r4r9g,askscience,new,1
haikuginger,"I can talk about two things that were factors in the long development process of color TV: backwards compatibility and display technology.

These two topics are heavily linked together. For example, during the early days of color TV development, one system, CBS Color, was a filter that spun in front of the screen, sequentially showing red, green, and blue. With that system, each frame was transmitted three times sequentially; one time each as red, green, and blue. The TV would time the spinning of the disk to its reception of each colored frame; the red frame would be drawn on the screen while the red filter was in front, and so on.

However, to see why that wouldn't work very well, we need to take a step backwards to see how black and white TV worked. Reduced to a basic level, a stream of electrons was fired at the inside of a glass screen coated in phosphorus. When electrons struck the phosphorus, it lit up. By using electromagnets, that single beam of electrons could be moved to point at any part of the screen. 

So, to form a picture on the screen, the electron beam would sweep across the screen, line by line, with the intensity of the beam determined by the amplitude of the TV signal arriving in the set at any given moment.

So, if you fed a CBS color signal into a black-and-white TV, it simply wouldn't work. TV at that point relied on the signals coming down exactly as the set expected them to, and on being able to ""line up"" those signals so that the first bit of a frame arrived at the TV when the electron beam was pointing at the top-left corner of the screen. CBS color just couldn't do that.

So, an alternate route needed to be taken. Color TV couldn't use an additive color space (composed of separate red, green, and blue signals), it needed a subtractive color space- one that started out as a complete monochrome picture (with equal amounts of red, green, and blue) that any black-and-white TV could read, but that had bits of extra information carried alongside it that could be used to turn that monochrome picture into color.

This introduced a new challenge, though. TVs at that point in time didn't have any ""memory""- they were sending the signal from an antenna or cable directly to the picture tube. If what was coming in didn't get drawn on the screen immediately, it wasn't getting drawn at all. And, with a subtractive color space, all the color was coming in at once. Unlike with CBS color, a TV couldn't use a mechanical trick like a spinning wheel to alternate between colors being drawn by a single electron beam- it needed to draw all the colors at once.

So, the logical answer to that problem was to go from using one vacuum tube producing an electron beam to three; one for each of red, green, and blue. Each tube would fire only when aimed at a bit of phosophor designated specially for it; each bit of phosphor would have a tiny filter in front of it so that, from the front, it would appear to glow red, green or blue.

As you might guess, this arrangement introduced a new problem. Before, it didn't matter if your electron beam was aligned exactly right or if it was particularly fine; the entire tube of your TV was coated in phosphor, and the picture would show up no matter what, even if the electron beam was angled over to the left a bit.

With the new arrangement, the problem was that the electron beams weren't fine enough and couldn't be aligned exactly right 100% of the time. This meant that ""blue"" phosphor dots were being lit up by the ""red"" electron beam- and the same for every other color. A reliable color picture wasn't being produced.

That's where Werner Flechsig's shadow mask comes in. The concept is simple, but brilliant. Take a metal sheet, and poke thousands of tiny holes in it. Then, put it directly behind the screen, which has your pattern of differently-colored phosphor dots on it. Now, because you're using three different electron beams, they're each coming from a different location- and each will enter any particular hole in the shadow mask, as the metal sheet is called, at a different angle. This allows you to restrict where the electrons from each beam land on the screen.

You can try it yourself. Take a sheet of paper and poke a hole in it. Then, holding the paper still, look through that hole from different angles. You'll see different things through it. Similarly, even if the electron beams were too big to focus and align down to the sizes they needed to be, shooting them through a tiny hole from slightly different places resulted in precise control over which points on the screen they struck.

Those are the things that were necessary for color TV to take off. We needed a way to send a color TV signal that could be viewed by people with black-and-white TVs. We needed a way to put each component color of that color signal on the screen simultaneously. And we needed a way to  keep each color at the point on the screen it needed to be.",null,1,cdjv089,1r4qud,askscience,new,12
cheeseynacho42,"Well, a huge obstacle towards broadcasting colour television was the massive amount of bandwidth it took. This went away slowly with time, as we got better and better at transmitting radio signals. 

As far as the actual television goes, there were a lot of ideas as to how you could make a TV colour. There was combining three monochrome images (red, green, and blue ones) to make a colour one, and shooting electrons at a phosphor plate (knows as ""Telechrome""), but probably the best-known one is Technicolour.

Technicolour evolved a lot over its lifespan, and while itw as eventually replaed, throughout most of the late 40s and 50s it was the dominant way of filming in colour and broadcasting in colour. Early Looney Toons used Technicolour, and the Wizard of Oz was filmed using it. 

Also, the way you talk about black-and-white TV makes it sound like we could have filmed in colour, but chose not to. That's not the case at all - it was very hard to figure out how to properly capture colour on film, much harder than filming black and white images. They weren't stripping the colour, or changing it, that was just what the film could capture. It's like asking the fish that live in the Abyssal zone why, when there's so much in the world to see, they only have light-detecting discs as eyes. It's the best they can do. ",null,0,cdjs5gp,1r4qud,askscience,new,1
Dannei,"Yes, GPS jamming is possible - as with any radio communications, you just need to fill the relevant frequencies with enough ""junk"" to drown out the actual signal.

It's also possible to spoof GPS signals, by creating a much more powerful signal that overrides the normal one. This can be used to convince a receiver that it is somewhere else than it actually is - examples of possible uses include misdirecting planes/ships to another destination, by carefully changing the signals you send it over time.

In both cases, yes, the area you cover would be limited by your transmission power.",null,2,cdjmmtx,1r4orq,askscience,new,11
zerbey,"You didn't ask, but it's important from a historical point of view if you're interested in GPS.  When GPS was first deployed, there was a function known as Selective Availability, which means that the US Government could intentionally block the GPS signal to all users except those who had authorised receivers.  The basic idea was that it would cause all ""civilian"" receivers to be inaccurate, so it'd be useless for tracking.  This functionality was turned off permanently during the Clinton administration and new Satellites are incapable of using it.
",null,0,cdjsx0o,1r4orq,askscience,new,3
Bradm77,Dannei gave a good answer.  I'm an engineer and I used to work on anti-jamming devices for the military.  We built devices that would prevent enemy jamming equipment from effecting our military's GPS devices.  ,null,1,cdjpp0x,1r4orq,askscience,new,3
LoyalSol,"Opening the window at best will make your apartment as humid as the outside, but Canada in general is a pretty dry area on the relative humidity scale so not really sure how much more opening the window will do.  ",null,1,cdjnz18,1r4geg,askscience,new,2
FatSquirrels,"That somewhat depends on what humidity you are talking about.

Absolute humidity is how much water is in the air.  Opening the window will equalize the absolute humidity between inside and outside.  Hot air can hold more moisture, but that does not mean that it necessarily has more water in it.  If it is dry outside then likely your enclosed space inside with showering, respirating humans/animals, cooking and such is actually more humid.

Relative humidity is the amount of water in the air relative to how much it can hold.  I think this is more relevant to humans as it determines the perceived temperature and how close the atmosphere is to precipitation.  Warm air can hold more water, so if you increase the temperature inside by opening a window the relative humidity will go down (assuming equal absolute humidity inside and out).  Opposite if it is colder outside.

Really what this means is that you need to know both the temperature and the relative or absolute humidity both in and outside to make this call.  Likely, your best bet is to keep your windows closed and buy a humidifier.  They aren't very expensive and they could help you out if the dry air is bothering you.",null,1,cdjs7zk,1r4geg,askscience,new,2
Problem119V-0800,"More force, yes. The light should exert twice as much pressure (aka transfer of momentum) on the reflecting surface as the black one, assuming other things are equal.

Looks like the first experimental measurement of this was in the early 1900s.

(Note that Crookes radiometers— those light powered spinning things in glass tubes that people sometimes have as toys / science demonstrators— actually work by thermal expansion of the air near the heated vane, not by photon pressure. Photon pressure is a much smaller effect. and would turn the vanes the opposite direction anyway.)

[a mildly interesting article on the subject (pdf)](http://dujs.dartmouth.edu/2002S/pressureoflight.pdf)",null,17,cdjixs9,1r4g5u,askscience,new,120
do_od,"A [Nichols radiometer](http://en.wikipedia.org/wiki/Nichols_radiometer) is essentially a small scale that measure the inertial force of light, or radiation pressure. It uses a pair of mirrors.

A perfectly reflecting scale would register a pressure twice as high as a perfectly absorbing, pitch black one. This is because of conservation of momentum. In total reflection the momentum of the light is reversed, while in absorption the momentum is transferred into the surface.",null,6,cdjj4sd,1r4g5u,askscience,new,20
SproutsCrayons,"Yes it would. The absorbed light, would be reemitted in a random direction or absorbed as heat. In both cases the net force is bigger for the mirror. 
If you are thinking about these things the gravity wave detectors, e.g. GEO600 and LISA, might interest you as they deal with these kind of problems. It might also be interesting to look in to [optical tweezers] (http://en.wikipedia.org/wiki/Optical_tweezers) , [laser cooling] (http://en.wikipedia.org/wiki/Laser_cooling) and [cavity optomechanics](http://en.wikipedia.org/wiki/Cavity_optomechanics). 

",null,1,cdjk3l8,1r4g5u,askscience,new,6
null,null,null,27,cdjo3q8,1r4g5u,askscience,new,6
TangentialThreat,"They're called [fulgurites](http://en.wikipedia.org/wiki/Fulgurite).

Cool, huh?",null,1,cdjhga1,1r4em9,askscience,new,17
slumberprojekt,"You get a piece of fulgurite if you're lucky.

http://en.wikipedia.org/wiki/Fulgurite",null,1,cdjhow8,1r4em9,askscience,new,8
Criticalist,"In 2009 a group of researchers published a paper in the New England Journal of Medicine in which they reported on direct measurements of blood oxygen levels as they ascended to the summit of Mount Everest. At this altitude, 8800m, the partial pressure of oxygen is thought to be at the lower limit of what acclimatised humans can tolerate- according to the article, under 4% of people who climb Everest do so without supplemental oxygen.

The researchers took samples from their femoral arteries (located in the groin) at various altitudes as they ascended to the summit; the samples were transported down to the base camp for analysis. Above 7300m, the researchers used supplemental oxygen, but took it off for the sampling process.

The mean pO2 value, which is the partial pressure of oxygen dissolved in blood, was around 20mmHg at 8000m - about one fifth of normal.
However, the arterial oxygen content, which reflects the amount of oxygen bound to haemoglobin, was not nearly as reduced; it was 150ml/l, compared to a normal value of 200ml/l. This reflects the degree of acclimitisation the researchers had - non acclimaitised subjects exposed to that degree of low oxygen rapidly lose consciousness, whereas the climbers were functioning reasonably normally.

The pO2 levels recorded in this study are amongst the lowest recorded in healthy humans  and probably represent the lowest limit of tolerance.

[Link to the paper](http://www.nejm.org/doi/pdf/10.1056/NEJMoa0801581)",null,5,cdjk2e3,1r4egz,askscience,new,6
SmellyRaghead,"The amount you absorb is proportional to the partial pressure. At 1 atm., ppO2 is about o.21 atm. At 2 at., 0.42 atm, etc.

With a lower partial pressure, the less mass transfer of oxygen occurrs, obviously causing a lack of oxygen in the blood.

The opposite is also true - when diving beyond certain pressure thresholds, the ppO2 becomes far too high and can cause serious damage, as oxgen in too great a concentration is quite toxic.

In short, we are most 'comfortable' at sea level and 1 atm of total pressure.",null,0,cdjjkmo,1r4egz,askscience,new,2
irreligiosity,"It's complicated.. Your body adjusts to it's climate's oxygen levels to assure the appropriate intake of oxygen is met for cellular respiration. Hemoglobin supplies oxygen to your body. People who live at high elevation with little oxygen can have twice as much hemoglobin as those at lower elevations. This happens to ensure that there is a higher uptake of oxygen because there is so little available in the air. This is also why people have to wait at base-camp for 3 months before climbing Everest - so their body adjusts to the appropriate hemoglobin levels. If they do not wait, and your body does not get enough oxygen then the electron transport chain that is used to form ATP (energy for your body) is disrupted. This is hard on your body, and there are other effects of lack of oxygen also. 

With too much oxygen in the air your body receives too much oxygen. This does various things that are unwanted to the body. Oxygen can bind to things in your body, and it is used in important cell regulating. It is possible for the oxygen molecules to bind to protons forming a hydroxyl group which is undesirable. Too much oxygen it stressful for your bodies normal cellular functioning. ",null,3,cdjjs85,1r4egz,askscience,new,4
Minifig81,"It's something they call a stroboscopic effect and it's something that you commonly see in Western movies on television, so they also call it the wagon wheel effect. It's actually just an optical illusion in which the spoke wheel just appears to rotate differently than the true rotation. Actually it may look like it's rotating faster or not at all or even in the opposite direction.

It's illusion that's very similar to what we see with camera strobing that we might see in dance club or a party, when you have a flashing light that kind of freezes dances in a series of images.

If we think of the wagon wheel and we imagine that one of the spokes is painted white and it's pointed straight up - so like at like 12:00 on a clock - if we rotate that wheel at once per second and then we take images once every second, in our minds, it's going to look like that spoke never moved, because it's always going to be pointing at the 12.

If we were to suddenly now start capturing those images faster than once a second, so faster than it's rotating, that white spoke that's pointing up at 12, the next time we see an image, it's not going to make it all the way around, so it's going to be pointing at 11. And then the next image that we take, it's going to be pointing at 10. And so we're going to imagine that it's going to be moving in this counter-clockwise direction instead of the clockwise direction. ",null,1,cdjhxsk,1r4dw4,askscience,new,12
Mossman11,"I can't comment on why a cheaper high octane fuel isn't available, but considering the relatively small market and the fact that 93 octane for cars is $3.50, $6 for 100LL doesn't seem ridiculous to me.  Check out race fuels for cars, they're upwards of $10/gallon.

I do know a thing or two about motors, and octane, and compression so I can clear some stuff up there.  The 100 in 100LL is the motor octane rating.  This is a measure of how resistant to knock the fuel is, and how slowly it burns.  Especially in high compression engines, if a fuel is too volatile and its knock resistance is not sufficient, the fuel can pre-ignite due to compression alone.  When combustion happens while the piston is still on the way up, the piston now has to be fight through the high combustion pressures generated, as opposed to using them to propel it down again (the power stroke).  This is called knock, or ping, and is really bad for engine internals.  I know on some older aircraft the pilot had to control timing advance.  This would be equivalent to running too much timing advance all the time, except you can't push a lever to rectify it.  I've never flown a Cessna but I imagine if your motor grenades mid-air, that's not a good thing.  

As for why aircraft use high compression engines I would imagine it's to make the most power from the smallest/lightest engine.  Also the fact that you're flying in thin air means that you're sucking in less oxygen,nitrogen,etc. at altitude and in order to have sufficient power at altitude you'd need a high compression, knock prone engine at sea level.  

Also, I found this on wikipedia which seems like a nice feature of 100LL that possibly the proposed replacements don't have: 
""Avgas has a lower and more uniform vapor pressure than automotive gasoline so it remains in the liquid state despite the reduced atmospheric pressure at high altitude, thus preventing vapor lock""
",null,1,cdjo282,1r4coe,askscience,new,4
king_of_the_universe,"Yes, more heat will be trapped, but this heat is irrelevant in comparison to the heat that your hands are able to feel when they touch the screen: That heat is *not* caused by the light itself but by the mechanism that creates it / by other components of the screen.",null,0,cdjj1v5,1r49bc,askscience,new,1
rupert1920,"Would this not depend on how much water there is in a bucket, or the size of the bucket?

What scientific principle are you trying to elucidate here?",null,1,cdjduam,1r43z8,askscience,new,4
endocytosis,"Depends on how angry you are…just kidding.
I doubt you'd get hypothermia, but it wouldn't be pleasant.  Your head will transfer some thermal energy to the ice water, raising its temperature slightly.  Your head will lose the thermal energy, causing among other things, blood vessels to contract, cellular metabolism to slow, and you to get a splitting headache (long story but tried it once on a dare-wouldn't recommend it).  You may feel the initial effects of hypothermia, uncontrollable shivering, confusion, disorientation, etc., mainly because your brain is having difficulty getting enough oxygen, nutrients, etc. and is chilled, but your heart is still pumping warmer blood to your brain to keep it going, assuming it's a reasonably warm (70 deg F, 25 deg C) day.  Edit: grammar",null,0,cdjm5il,1r43z8,askscience,new,1
fishify,"When a particle interacts with its environment, the state of the two-particle system can change, disentangling the previous entangled state.

Entanglement simply means that a two- (or more) particle state cannot be written as a product of separate one-particle states.  If one of the particles interacts with its environment so its state becomes definite, then it is no longer entangled with its one-time partner.",null,0,cdjd7fq,1r41ug,askscience,new,7
KerSan,"I think /u/fishify's answer is incomplete. If systems A and B are entangled, why should interaction with system C diminish the entanglement between A and B?

It turns out that [entanglement is monogamous](http://arxiv.org/abs/quant-ph/9907047). If systems A and B are perfectly entangled, it turns out that they cannot be entangled (or even correllated) with any other system C. More generally, the amount of entanglement between A and B places a limit on the amount of entanglement that either system, or both considered together, can share with C. The right way to count entanglement is still an open question, but this odd behaviour is the reason why particles become un-entangled.

The most important thing to realize about entanglement is that it *cannot arise without interaction*. That is to say, systems that do not directly interact cannot become entangled. Therefore, a particle P that is entangled with a system S generally becomes less entangled with S if P interacts with another system R.

This of course begs an interesting question: is there such a thing as an un-entangled particle? All particles might be entangled with *something*. This comes down to an interpretation of quantum theory, which I don't think is worth trying to discuss on reddit. Those kinds of discussions are mostly fruitless anyway. Emphasis on 'mostly'.",null,1,cdk2z6l,1r41ug,askscience,new,1
baloo_the_bear,"Electronegativity is how strongly a particular atom attracts electrons. 

To understand what that means, you need to know what attracts electrons, and what factors can affect that.

Electron orbitals have very specific shapes in 3 dimensions, and can be thought of as a waveform. These waveforms are most stable when filled with an appropriate number of electrons. The first valence shell is *s* and the first level of the *s* orbital prefers 2 electrons, this makes it most stable. Hydrogen is nothing more than a proton and an electron, but it is very stable when it can fill its valence shell with 2 electrons, hence a high electronegativity and why it is so weird. Helium has 2 protons and 2 electrons (and 2 neutrons, but they don't matter for the moment), so its valence shell is full at 2 electrons, and it does not attract electrons to fill and sort of void, and therefore has a low electronegativity. 

Now, we talk about ionic forces.

Electrons are negatively charged, and as such they are attracted to positive charges, such as the nucleus of an atom. The force of ionic attraction is proportional to the charges of the two objects, and inverse of the square of the distance between them. The higher the positive charge of an atomic nucleus, the stronger the force of attraction, and the smaller the distance between those charges, the stronger the force of attraction.

As you move from the left to the right of the periodic table, you have more and more protons, which means an increasing positive charge in the nucleus. This increasing positive charge exerts an increasing force of attraction on the electron orbitals. This causes the **size** of the orbital to **decrease** as you move across the periodic table. This is why the **electronegativity increases going from left to right.** 

Now to add another concept.

Valence shells exist in orbitals that have different levels of energy. The fact that energy is discrete (dividable down to quanta) means that the orbitals have discrete levels, or layers. Not only do more layers increase the distance between the nucleus and the electron it is attracting, but those layers are all negatively charged and will act to repel another negatively charged electron. 
This is why as you **move down** the periodic table (increasing levels of valence shells), the **electronegativity reduces.**

Now remember that we talked about valence shells being most stable with certain numbers of electrons? The next 'magic number' of valence electrons that make the orbitals stable is 8. Now count over from left to right on the table. Florine has....**7** electrons in its valance shell, just one short. It strongly attracts that last electron not only because it is small and has a large charge in its nucleus, but because gaining another electron makes it have a **more stable** valence structure. 

OK, so we've talked about what electronegativity means, and what factors have an affect on it, but why do we care?

We stated that a stable valence shell has 8 electrons. This is why Carbon, with 4 electrons in its valence shell, will make 4 covalent bonds. Covalent bonds, as their name suggests, are when 2 atoms **share** an electron so that **both** atoms can have a stable valence shell. in the case of a covalent bond between atoms that are the same, the electron is shared equally, because the **electronegativity** of each atom is the same. However, if one atom in the covalent bond has a higher electronegativity, the electron is attracted more to that side of the bond. What happens when you're attracted to something? You want to spend more time there. Because of this unequal sharing, the bond becomes **polar**, in that one side of the bond has a slightly negative charge, and one side has a slightly positive charge. 

This matters in incredibly significant ways. Water, for example, is a polar molecule. Because of this water is liquid at room temperature, held together by hydrogen bonds (a consequence of polar molecules). DNA is also held together to their complementary strands by hydrogen bonds. 

edit: the stability of 8 valence shell electrons are also why the noble gases are very unreactive. They do not need to share electrons to be stable. The low reactivity of this group of elements is why they are called 'noble', as nobility kept to themselves.
",null,27,cdjc2e8,1r3wa9,askscience,new,126
MJ81,"I'm not sure there's a ""rigorous"" explanation of electronegativity - there are quantitative ways to calculate electronegativity (see [the Wiki article](http://en.wikipedia.org/wiki/Electronegativity)), after a fashion, but it's primarily a way to formalize available experimental data &amp; observed trends.",null,3,cdjeduy,1r3wa9,askscience,new,6
M4rkusD,"In short. Hydrogen is so weird because, in contrary to the other elements, its nucleus is just a proton. That means its properties (and especially those of its H+ ion, essentially a naked proton) are partly different from those of other molecules.

The next element, helium, which has 2 protons already needs 2 neutrons to stabilise its nucleus.",null,6,cdjl4ss,1r3wa9,askscience,new,4
ThePizar,"Atoms want to fill their valance shell (s &amp; p sub-shells) and to do that they need an electron. As you go across a period atoms get closer to having a full valance shell and thus they want that next electron. Going down a group atoms get bigger and the valance shell is farther from the nucleus and thus they have a weaker pull in each other. This leads the atom to not want the electron as much. 

You may notice a drop after the Cu/Ag/Au group. That is because the next group has all its existing outer sub-shells (s &amp; d) filled. 

Edit: Lewis Dot structure is a simple way of showing how molecules interact. Actually molecules are in 3d space. There are two main ways of showing/describing interactions between molecules. [Valance bond theory](http://en.wikipedia.org/wiki/Valence_Bond_Theory) and [Molecular orbital theory](http://en.wikipedia.org/wiki/Molecular_orbital_theory). 

Source: AP Chemistry class",null,9,cdjblqa,1r3wa9,askscience,new,4
CatalyticDragon,"Yes. All the objects you have listed contain the same elements but in different total amounts and ratios. And they all behave according to the same universal physics.

Here is a quick page on the makeup of the earth, stars, and even interstellar dust clouds. You'll see all of this ""stuff"" is made up of the same ""stuff"" because it all comes from the same place. Exploding stars;

- http://spiff.rit.edu/classes/phys240/lectures/elements/elements.html",null,0,cdjbbg6,1r3v70,askscience,new,3
iorgfeflkd,"There are only 90 or so elements that can be found in nature. Asteroids and planets are all made from them, in different abundances.",null,0,cdjczor,1r3v70,askscience,new,2
Bbrhuft,"There would be no difference. Riffling causes the bullet to spin, it acts like a gyroscope, giving it extra stability that keeps it from tumbling randomly and deviating from its intended target due to variable wind drag.

But there's no air in space, no wind drag, so whether a bullet tumbles or not, it will still follow the same path. 

And indeed when calculating the orbits of asteroids there's no need to take into account their rotation period.",null,9,cdjdg2i,1r3sde,askscience,new,44
TangentialThreat,"It depends on whether you need the bullet to hit pointy-end first.

Many bullets have either a hollow point so it fragments on impact and does more damage, or a hardened armor-piercing tip. Rotation helps keep this end towards the enemy. A bullet that hits sideways or backwards is still pretty lethal on Earth, but you are presumably shooting at either your fellow kevlar-suited cosmonauts or battling an unknown alien menace. The terminal ballistics will be much less predictable and this could be a problem.

It should be noted that firing any gun in space is probably a rather delicate skill due to conservation of momentum. Rifling will make this even more complicated. Best of luck out there and try to make Earth look good.

[Loose wads of nitrocellulose](http://www.youtube.com/watch?v=QnDZ_cO5Ln4) refuse to burn in vacuum.

[Double-action revolvers](http://www.youtube.com/watch?v=hUdkIn7C9fA) do appear to work, which I would not have expected.",null,3,cdjhdea,1r3sde,askscience,new,9
Evomon,Wouldnt it be impossible to shoot a gun in space as the conventional  bullet wont have o2 to ignite ? ,null,10,cdjdkgf,1r3sde,askscience,new,9
Claclink,"ive tested this in the lab with a spectrometer and it depends on the color and person, but in optimal conditions you can detect a change of about 2nm.",null,1,cdjet36,1r3qnw,askscience,new,5
Greyswandir,Human eyes are surprisingly sensitive to changes in wavelength.  My research used to involve a colorimetric assay (one where the main output is a color changed) which involved me both taking pictures of the samples and measuring their spectrum.  A relatively small change in the shape of the spectrum (~20nm of peak broadening) could change the perceived color of the sample from green to orange.,null,0,cdjqi6j,1r3qnw,askscience,new,1
Das_Mime,"Particles certainly exist, although on a quantum level everything exhibits particle/wave duality. Particles, instead of being the hard pellets that we sometimes envision, are actually described by a probability function which tells you how likely the particle is to be in any given location. So in that sense the particle can be thought of as a probability field.

In the Standard Model (which is the more or less unanimously accepted model of particle physics), mass is caused by particles interacting with the Higgs field, whose existence is supported by the discovery over the last few years of the Higgs Boson.",null,1,cdjk1ri,1r3pfa,askscience,new,4
null,null,null,0,cdja5vr,1r3pfa,askscience,new,1
datums,"Taking a photograph on film is a chemical reaction. When light hits film, it causes chemical changes in the film itself, and the pattern of these changes makes up the image.   
  
Like any chemical reaction, it takes a particular amount of time. If I expose a particular film to light, it might take 1/24 of a second for the chemical reaction to occur, as is the case for standard movie film.   
  
If I want to increase the frame rate with that film to 1/48 of a second, I am going to have a problem. The same chemical reaction will have to happen in half of the time. A potential solution? Double the amount of light to compensate.    
  
Another way to analogize this is that is takes a particular number of photons to record an image. If I want to capture images more quickly, I can simply increase the number of photons per second, ie. the brightness, to compensate. ",null,1,cdjajxm,1r3nxd,askscience,new,7
johnwilkesbandwith,"Running at 24 FPS the shutter, a physical spinning disk, exposes the film at 1/48 rotations per second with a 180 degree shutter opening. This means every second half of the oval blocks the light and half exposes the film as it runs through the gate. 

The gate is a plate system that holds the physical medium in place. At this speed, with the proper F/stop, the film is exposed properly.

When you increase the frame rate, you need more light because the speed that the film is going through the gate is getting the proper exposure.

You can compensate for a higher frame-rate by either switching to a higher shutter speed, which can effect the motion blur of the image or you can open your lens up to a lower F/Stop. 

If you don't have the available light, you can compensate with the shutter speed but it does effect the visual aesthetic of the image.

The image gets dimmer because the film loses about a stop/multiplication of speed that you run the film. 

This concept applies to digital filmmaking, but its effects on 3D photography are more complex.",null,0,cdji3t2,1r3nxd,askscience,new,1
brawnkowsky,"depends specifically on what you're allergic too and whether that allergy exists on that particular animal.  but most cat allergies are caused by a specific protein (Fel d 1) that is produced by cats and released via sebaceous glands on skin.  this protein interacts with IgE and IgG4 to create an immune response.

research shows that this protein is present on some big cats, so if this is the protein you are allergic too, then chances are you will have a reaction when coming in contact with any cats.  but there is a chance you are not allergic to this protein.

http://www.ncbi.nlm.nih.gov/pubmed/1695231

Edit: research shows that a Fel d 1-like molecule is present.  same results, different cause",null,0,cdjg5om,1r3lwr,askscience,new,4
justin3003,"I hate to keep doing this as a means of answering questions here, but I wrote a blog post on this exact topic a few months ago. The short answer, based on my research, is ""kind of."" Big cats are genetically related but distinct from house cats (as well as distinct from each other) and have different forms of the allergenic protein Fel d 1. The big cat version of the protein can cause an acute allergic reaction in humans but not all to the same extent as Fel d 1 does. I cite the same paper as brawnkowsky does; in it they have a chart of histamine release vs. concentration of proteins as well as relative release of the relevant immunoglobulins from big cats compared with Fel d 1. The lion, for example, has a significantly lower reactivity in a cat allergic patient than both a tiger and Fel d 1 cause (and fel d 1 &gt; tiger). The chart also shows dramatic (at least 50%) lower immunoglobulin release during exposure for big cat dander vs. Fel d 1/house cats. While the conclusions presented in the abstract are technically correct, since they can/do elicit an immune response, the actual data within the paper much more clearly demonstrates the nuances of their conclusion: while many are still allergenic to some extent, Fel d 1 has a much higher activity than any protein present in the big cats by a fairly wide margin for both IgE and IgG4. 
Link to my blog post: http://theweeklypaperblog.com/2013/02/13/lions-and-tigers-and-bears-oh-my-allergies-and-cross-species-issues/
Link to the ScienceDirect site (if you have institutional access to papers, sorry I can't post it directly for you): http://www.sciencedirect.com.libproxy.usc.edu/science/article/pii/S0091674905801307#",null,0,cdksxlm,1r3lwr,askscience,new,1
ModernTarantula," you have to imagine yourself smaller that what you are building and building it vertical not horizontal. Next watch a spider make it's web. You will see how it happens. Next is speculation from that. They need to have a central hub to make the spokes from . They follow the old line to the middle and then can hold up a string out with a leg to make the next radial. The contehcntric circles are made as they walk around the radials like a racetrack. at each pint connecting to the radial. 
so spokes and a tight spiral is simpler than measuring out a grid in the air.",null,0,cdjh1ch,1r3lv4,askscience,new,1
Platypuskeeper,"It's a common misconception, from the [Bohr model of the atom](http://images.tutorvista.com/cms/images/38/bohr-atom-model.JPG), that there'd be any 'empty space' between the atom and nucleus. Electrons are quantum-mechanical particles, which means they don't normally occupy a precise location, nor radius from the nucleus, they're sort of 'smeared out' in space. Atoms actually 'look' more [like this](http://kaffee.50webs.com/Science/images/orbitals/1s.gif), with a diffuse 'cloud' of electrons around them, where the density represents the probability of finding an electron in the vicinity of a point. The density is actually _highest_ near the nucleus, and drops off exponentially as you move away from it.

So there's no real 'empty space' there, but some parts of space are emptier than others. (That said, there's still a concept of the vacuum in physics). 

One thing that could be said to be true of the Bohr model is that it shows that the 'size' of an atom is the 'size' of the volume which its electrons occupy. Although that's a diffuse 'cloud', it's still 4-5 orders of magnitude larger than the 'cloud' of the nucleus. 

Electrons and protons are not very 'solid'. An electron has no radius of its own, and the protons (which are composite particles) are also spread out. Due to this quantum smearing-out, electrons (if they have opposite 'spins') can occupy the same location, and the electron and proton can be in the same location, which wouldn't be possible for classical point-charges, as it would lead to infinite energies. 

The classical concepts of things 'touching' and having surfaces and volumes don't apply here. You have to flip it around: _Because_ the electron density drops off so rapidly as you move away from the nucleus, the electrical forces between two adjacent atoms get enormous within a short distance, once their electron clouds start pushing into each other. Things push back with more force than you can exert over an imperceptibly small distance, which is why things appear 'solid' to you at the large-scale, macroscopic, level. At the atomic level, nothing is solid though.

",null,5,cdjbguz,1r3ju6,askscience,new,13
r2k,"We can't ""see""  atoms or subatomic particles. To properly resolve atoms,  we need to use electrons or other particles which have a smaller wavelength on the same order as the atoms. Electrons are considered point particles, and the nucleus of an atom is composed of protons and neutrons. These subatomic particles are themselves composed of things called quarks. 

You can't probe a proton/neutron to see how hard it is -  because you would have to use something much smaller than it in the first place. In reality,  it is not a sphere,  and scientists are still trying to determine what the fundamental building block of matter is. String theory proposes that ultimately,  everything is composed of bundles of energy. 

The space between a nucleus and the orbiting Electrons is the same space that exists between the moon and Earth. It really is nothing. ",null,10,cdjarhu,1r3ju6,askscience,new,4
s8nlovesme,"""Gray hair, is simply hair with less melanin, and white hair has no melanin at all. Genes control this lack of deposition of melanin, too. In some families, many members' hair turns white in their 20s. Generally speaking, among Caucasians 50 percent are 50 percent gray by age 50. There is, however, wide variation. This number differs for other ethnic groups, again demonstrating the effect of genetic control.""",null,0,cdjazwp,1r3fe1,askscience,new,1
iorgfeflkd,"We don't need particularly fancy telescopes to detect the cosmic microwave background, and recent missions like WMAP have given us pictures of it with extraordinary resolution.

Too see farther back than the CMB (about 400,000 years after the big bang), we can not longer use light (or any EM radiation) because the universe was opaque. Neutrino or gravitatioonal wave observatories could bypass this, but our capability in those areas is much much worse than with light telescopes.",null,1,cdj6d9b,1r3fak,askscience,new,15
quality_is_god,"This would depend on the nature of the restriction. 

Only if the restriction were isentropic would the pressure at C not be affected by the upstream restriction. 

Isentropic restrictions are very specific devices (like a nozzle and diffuser) but under the vast majority of restrictions, there will be unrecoverable pressure losses that propagate downstream.",null,0,cdje3hg,1r3f2v,askscience,new,2
202024,"Yes it will, the restriction will have a pressure drop, depending on the type some of it will be recovered, but there will still be a permanent pressure drop caused by the restriction.

http://en.wikipedia.org/wiki/Orifice_plate is a good place to start if you want some more technical information.",null,0,cdjn8dk,1r3f2v,askscience,new,1
Randomaway,"Your pump will adjust to provide a constant flow rate.  Let's call point C some kind of spray nozzle.  That spray nozzle will require a certain pressure to produce the desired flow rate.  Introducing a restriction (B)  upstream will not affect the required pressure at C.  Point B  does introduce extra pressure drop, so the pressure at point A will have to be higher to produce the required pressure at point C.",null,0,cdkmxmc,1r3f2v,askscience,new,1
whatsup4,"Point C should not see any difference theoretically. I say theoretically because if for some reason the blockage at B caused some kind of turbulence at point C it could alter what is happening downstream. Something like a vortex upstream forces your fluid to vortex downstream. But if that is not the case or C is far enough away from B then no there shouldn't be a difference.

Here's one way to look at it if you have a pipe with a certain diameter and a certain back pressure from the piping after it then there is only one pressure it can be for a certain flow rate. If it's pressure increased its flow would need to increase if its pressure decreased its flow would need to decrease. Since its flow must remain constant so must its pressure.",null,0,cdlj3wc,1r3f2v,askscience,new,1
evertjm,"Yes, pressure is caused by a resistance to flow. As you have stated that point B will restrict flow, if the diameter of A and C are equal then C will have less flow and pressure.

Simple test is to kink your garden hose while the tap is on fully. If you kink the hose at the halfway point, and the end of the hose is unrestricted, then you will notice a considerable drop in flow and pressure.",null,1,cdjb2g4,1r3f2v,askscience,new,1
__Jay,"One very important aspect that has gone unmentioned is your Reynolds number.  Loosely speaking, an indication of smooth (laminar) vs turbulent flow.  Your Reynolds number is a function of distance, i.e., as distance increases your flow will become less stable and inevitable losses are incured which call for pressure increases to maintain flow. 

 In technical terms, you have boundary layer separation with distance.  A mechanical energy balance will show, any losses with kinetic energy (fluid velocity) lead to a neccessary pressure increase to maintain fluid velocity at constant cross section (pipe diameter).",null,1,cdjdbut,1r3f2v,askscience,new,1
iorgfeflkd,"If the planet passes in front of the star periodically, its radius can be determined by how much the star dims as the planet passes in front, while the mass can be determined by the planet's gravitational effect on the star's velocity, which is based on the stars chemical spectrum changing with the Doppler shift. Combining these, a density can be calculated.",null,1,cdj6ius,1r3ezh,askscience,new,4
iorgfeflkd,"If you want to look at it that way, you can find the average kinetic energy per molecule (sort of, it's a bit more complicated) by multiplying the temperature by Boltzmann's constant (which is equal to the ideal gas constant divided by Avogardro's number). However, the average molecular energy isn't the best definition of temperature; it actually relates to how energy changes with entropy.",null,1,cdj6gu5,1r3ez4,askscience,new,12
__Pers,"In some subfields of physics such as plasma physics, temperature is indeed quoted in units of energy, typically electron Volts (eV) or kilo-electron Volts (keV), referring to the average kinetic energy a particle from the ensemble would have.",null,1,cdj77de,1r3ez4,askscience,new,4
natty_dread,"Temperature and Energy are connected by the so called Boltzmann constant.

That means, that there is a fixed ""exchange rate"" between temperature and energy. You could choose your system of units in a way that will result in the constant being equal to 1. This will render temperature and energy equal in terms of value.",null,2,cdj880a,1r3ez4,askscience,new,2
arumbar,"Enzymes are proteins - what happens when you ingest proteins?  They are digested into their substituent components and lose any structure/function they once had.  This is why replenishing insulin in diabetic patients orally does not work - they need injections instead.  The one scenario that I know of where we give people oral enzymes is for pancreatic insufficiency.  In this case, they are missing digestive enzymes that would normally be secreted into the gut, so artificially adding some in works well.",null,0,cdj6qmf,1r3em7,askscience,new,7
KarlOskar12,"The enzyme isn't missing from the blood, it is missing from lysosomes. Tay-Sachs is a *lysosomal storage disorder*. The problem with giving supplemental enzyme to someone with Tay-Sachs disease is that [HEXA](http://www.news-medical.net/health/Tay-Sachs-Disease-Research.aspx) is too large to pass the blood brain barrier where it is needed to break down GM2 gangliosides.",null,1,cdj9tei,1r3em7,askscience,new,5
gilgoomesh,"I don't know what measure of signal strength you're looking at or what WiFi hardware you're using so there could be 100 different answers. Three possibilities stand out in my mind:

a) When idle, your connection's sync rate will sit on a ""best guess"". When you're actually downloading, it can use the data sent and the error rate on that data to measure the *real* signal strength and it turns out that it's much lower than the guess so it needs to lower the sync rate.

b) The router antenna increases its power when sending data. Maybe this causes the power supply to create more interference.

c) You're getting a lot of multi path interference and your own WiFi signal bouncing off walls in the room is interfering with itself.",null,0,cdj90vh,1r3eju,askscience,new,3
superAL1394,"For a more complete answer, input from an EE or a physicist is necessary, but in general wifi strength is a 'best guess' estimate by the operating system based on its reception of packets from the router. When transmitting, you might be able to hear the router just fine, but your packets aren't being heard by the router. This is often the case if the power on your router has been increased, or you are using a lower power device (like an iPhone).

When you receive data, you will transmit back acknowledgements as defined by the IP protocol. This helps the system determine how fast it can send packets based on the number of collisions and dropped packets. If you are hearing the router just fine at idle, it will show a high strength, but as soon as you start receiving and the acknowledgements aren't being received by the router, the operating system will revise its guess on the signal strength to reflect your inability to effectively communicate back. The operating system will know the router isn't receiving acknowledgements because it will be receiving packets multiple times as the router will assume transmission failed and resend the packet.",null,0,cdjxq13,1r3eju,askscience,new,3
snusmumrikan,"[Yes it appears so](http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291096-8644%28199808%29106:4%3C483::AID-AJPA4%3E3.0.CO;2-K/pdf)

If you don't have access through an academic institution to that article this is the reference: American Journal of Physical Anthropology, 106: 483–503.

It's all on Wikipedia.

(Edit spelling)",null,0,cdj67pa,1r390m,askscience,new,3
ooburai,"It's not definitive, but The Straight Dope has this article on the topic.

http://www.straightdope.com/columns/read/619/why-do-we-nod-our-heads-for-yes-and-shake-them-for-no

Also this: http://en.wikipedia.org/wiki/Nod_(gesture)

It seems that most people nod for yes, but there are some specific exceptions.  Bulgaria and Albania are cited.  That would seem to imply that it's not completely innate, or possibly that if it is that it can be overridden by culturally learned behaviour.",null,0,cdjb1d3,1r38ut,askscience,new,5
iorgfeflkd,It is indeed held together by gravity. The collective gravity of the sun is a lot stronger than that of Earth.,null,1,cdj8lbv,1r38kq,askscience,new,6
__Pers,"For the most part, gravity keeps the sun together. That said, the solar atmosphere (the chromosphere and corona) is composed of hot--hundreds of thousands of Kelvin to millions of Kelvin plasma--that indeed floats off into space. This hot, expanding plasma is called the solar wind.

Edit: typo",null,1,cdj8onh,1r38kq,askscience,new,4
Professor_Snuggles,"A simple thing that no one's mentioned is to put the piece of paper on top of and underneath the book when you drop them, rather than side by side. If they actually fall at different speeds, they would separate in one case. They'll fall together in both though, because the paper is shielded from air resistance effects by the book. If air resistance wasn't what was causing the difference, then this wouldn't matter.",null,0,cdjem5e,1r37tg,askscience,new,16
IAmMe1,"The key difference here is between force and acceleration. You won't really be able to get away with avoiding inertia to explain this.

See, the gravitational *force* is indeed stronger for an object with more mass. However, that object also has more mass and thus more inertia. This means that it takes a larger force to reach the same *acceleration* for that object. It just so happens that the dependence on mass cancels out for gravity; every object experiences the same *acceleration* due to gravity.

However, the *force* of air resistance does not depend on mass; it depends on the shape of the object (and its speed). The result is that, while all objects of the same shape (moving at the same speed) experience the same *force* of air resistance, due to different amounts of inertia (mass) they experience different *accelerations* due to air resistance. In particular, since the heavy object has more inertia, the same force produces less acceleration than on a light object.

To summarize: gravity produces more force on a heavier object than a light one but the same acceleration, while air resistance produces the same force on each but less acceleration for the heavy one.",null,1,cdj7jf3,1r37tg,askscience,new,7
Weed_O_Whirler,"So, to answer this you have to understand the difference between ""force"" and ""acceleration"" (I know, you're thinking- of course I do! But, your explanation shows a misunderstanding between them). Gravity attempts to *accelerate* everything at the same rate- and it does this by pulling on heavier things with more *force.* 

A nine year old might not be able to fully understand Newton's second law (F = m\*a if you need a refresher) but you can probably explain it to him pretty well. Tell him to imagine a rocket hooked up to a car, and how that rocket can make the car go fast. Now imagine that instead of a car, it is a big truck. The rocket, which puts out the same force regardless, will push the truck slower than the car. And now hook that rocket up to a train, and the train might not move at all. Or if it does, it will move slowly. This is the basics of Newton's second law- if you apply the same force to objects, the heavier ones will move slower than the light ones. You can do this experiment by trying to push a book across the table, or a stack of books- you'll have to push harder for the stack. 

OK, so gravity isn't a rocket. Gravity pulls harder on things which are heavier. In fact, if you double the mass of the object, gravity will pull twice as hard. So that is like if you made a car twice as heavy, but also attached two rockets- the acceleration would be the same regardless. So, gravity provides *twice the force* on an object twice as heavy, but due to Newton's second law, that is *the same acceleration.*

So now, add in air. Air resistance comes from the object having to move air molecules out of the way as it falls. So, it makes sense that the force of air resistance would be dependent on two things- the ""surface area"" or shape of the object in the direction it is falling and the speed at which it falls. The larger the surface area (again, only in the direction of falling, a book turned up on its spine would have less air resistance than a book lying flat), the more air molecules it has to move out of the way. Also, the faster it is falling, it will hit more air molecules it has to move. So, the larger those things are, the larger the force of air resistance. 

Thus, as you might expect, a book and a single sheet of paper should have the same air resistance (at least, when it first starts to fall- eventually the book will have more because it is moving faster). So, they have the same force pushing up on them- but that *force* causes more *acceleration* on the lighter object (the sheet of paper) than the heavy one (the book). 

Playing with some numbers (we'll choose easy ones). Imagine you have a 1 kg book and a 2 kg book, and we'll say the acceleration due to gravity is 10 m/s^(2). The, using Newton's 2nd law we can see that the force due to gravity on the first book is 10 Newtons (A Newton is the SI equivalent to a pound, it has units of force) and the second book at a force of 20 N. But now imagine each of them have 5 Newton's of air resistance acting on them. So, the total force acting on book 1 is 5 N, and the total force acting on book 2 in 15 N (Forces add- and since they are in opposite directions you are getting 10-5 and 20-5). So now, we can use Newton's second law to calculate their acceleration:

&gt; a = F/m (just re-arranged) 

&gt; a1 = 5N/1kg = 5 m/s^2

&gt; a2 = 15N/2kg = 7.5 m/s^2

So, the book that weighs more (but has the same air resistance) accelerates faster. ",null,0,cdj932w,1r37tg,askscience,new,4
DanielSank,"Please, please please show your son this:

Hold the paper and book up in the air shoulder width apart and drop them at the same time. The book hits the floor first.

Now place the paper on top of the book. Make sure it's pretty flat and hugs the top of the book as best you can get it (edges of the paper must not extend past edges of book). Now drop the book. They fall together.

If nothing else this will astound him enough that he won't forget it and will continue to seek answers.",null,0,cdjh47y,1r37tg,askscience,new,4
dampew,"Smart kid.  Tells it like it is.

Wind resistance is proportional to the area of the object.  But heavier objects are heavier and push down harder on the air.  So if two objects of the same size are falling through the air, the heavier object will fall faster.",null,0,cdjgo0j,1r37tg,askscience,new,3
ww-shen,"Little OFF: If you explain something to your kid (or any kids) it is a common wish for them to understand it. Sometimes explain the matter more advanced way (without the simplification). This way the kid will learn that there are things he cannot understand, and in time, that many things you don't understand either. It will open his perspective about the nature of knowledge.",null,0,cdji8zp,1r37tg,askscience,new,3
Surf_Science,"Dropping things may not be the way to go. You should try rolling them down a hill. I think this was the way Galileo went about some of his experiments (though not a hill and more precise). I think the only time you'd get into a problem would be with weird shapes so you should be able to do a bit better experiment with different weighted balls. 

With respect to the paper book issue. Isn't the paper loosing part of its downward speed from gravity by moving in a lateral direction? That may be easier for a kid to understand.  ",null,0,cdj7jdk,1r37tg,askscience,new,2
jayman419,"For an simple unscientific demonstration, you can [build a parachute out of a plastic bag, some tape, and some yarn](http://www.wikihow.com/Build-a-Plastic-Parachute) and have him experiment with different weights, to show that wind resistance is a limited thing that only slows objects by a certain rate.

Like hook a toy action figure up to it, and drop it from the balcony (if you have one) and then hook up something heavier, and he can see that the same parachute and the same air affect objects differently. You could even hook up the book to the parachute and race it against the sheet of paper.

That way you don't have to get into the maths of wind resistance, and you can show him the difference in practical terms.

EDIT: There's also the Apollo 15 video (one example of it is here: http://www.youtube.com/watch?v=5C5_dOEyAfk) where they dropped a hammer and a feather on the Moon, you can let him see what happens when there's no air.",null,0,cdj7oyv,1r37tg,askscience,new,2
natty_dread,"The thing is, that gravity **does** pull with different force on different things.

The actual constant value is *acceleration* not *force*. The sheet of paper and the book are being accelerated equally towards the ground. Since their shape is the same, the force of air resistance should be roughly the same too. However, since the book has more mass, the earth pulls harder on the book, thus making it fall faster.


The mathematical description is as follows: (This is meant to give **you** additional insight in order to give you well funded knowledge to share with your son)

 
Newton's law of gravity states that F_Gravity=G * m * M/r^2 (G being the gravitational constant, M&amp;m the masses and r the distance between them)

Now, Newtons Axiom states that the movement of a mass under the influence of a force is given by F=m*a

If we substitute F with F_Gravity we get G * m * M/r^2 = m * a.

As you can see, m can be canceled out of the equation.

This leaves us with a = g = G*M/r^2 .
This equation shows, that the acceleration of all masses in a gravitational field is, indeed, equal.

Then why are some things heavier than others?

To answer this question, we have to ask ourselves, what weight is. Weight is the force with which an object is pulled towards the earth.
Since the force, is given by F=m * a, and the mass of different object differs, the force which objects are pulled towards the ground is does not necessarily have to be constant.

",null,2,cdj81fn,1r37tg,askscience,new,4
joshhinz,"I always found the dropping two objects experiment misleading and unintuitive, because of the very problem you encountered! It simply isn't true in many cases on earth because there is air resistance on earth and even buoyancy forces (most evident with helium balloons). Maybe try to explain to him this fact to some degree then show him the [Apollo 15 Hammer and Feather Drop](http://www.youtube.com/watch?v=-4_rceVPVSY) ",null,0,cdj8gkl,1r37tg,askscience,new,2
tagaragawa,"I think one of the first mind-blowing things one learns in life is when you are taught that *everything* attracts *everything else*. Please include this in your discussions. It is not obvious at all, and only nowadays do we have experiments sensitive enough to measure the influence of, say, a solid sphere upon another test object. You can however easily convey this by pointing out that the earth moves around the sun, and the moon around the earth. The simplest explanation is that both the sun and the earth 'exert gravity', and from there that everything 'exerts gravity'.

Related: does anyone know when this idea was conceived? Did the early inventors of heliocentric models immediately make this implication, or was it not until Hooke and Newton that is was fully realized?",null,0,cdj95vn,1r37tg,askscience,new,2
therationalpi,"Great questions, I'll try my best to answer this.

Your questions are about a [vibrating string](http://en.wikipedia.org/wiki/Vibrating_string). The equation for the frequency of a vibrating string is given by

f=(1/2L) \* √(T/µ)

Where f is frequency (pitch), L is length, T is tension, and µ is linear density (mass per unit length). The answers to your first two equations should be apparent from that equation, but I'll explain each by itself.

1. You increased the length while holding T and µ constant, thus f decreased.

2. You increased L while also increasing T and lowering µ (µ decreased because the mass of the rubber band is conserved, just spread over a longer band now). µ ought to change as µ=µ_0\*L_0/L, where µ_0 and L_0 are the initial linear density and length respectively. Thus, the new equation becomes: f=(1/2) \* √(T/µ_0\*L_0\*L). Tension will also increase as you increase the length, but if it's increasing more than just linearly, you'll find the frequency increasing.

As for three, I'm not certain, but I would assume that temporal resolution is to blame.",null,1,cdj7r5w,1r36co,askscience,new,5
IAmMe1,"Let's talk quickly about what determines pitch. The pitch of the sound wave is determined by its frequency, which is set by the number of times per second that the object producing it (the rubber band) wiggles up and down. Now, the wiggling of the rubber band itself is set by two things: the wavelength of the wiggles and the wave speed in the rubber band.

The wavelength is basically how long a wave has to be to ""fit perfectly"" into the amount of rubber band available to wiggle. This means that the wavelength is basically the length of the wiggling part of the rubber band (over 2 for the fundamental frequency, but that's not important).

The speed of waves in the rubber band is set by the band's density (a property of the material it's made out of) and the tension in the band. Higher tension makes the wave travel faster.

Also, you should know that pitch goes up if wavelength goes down and pitch also goes up if the wave speed goes up.

1) You have kept the tension in the band the same while increasing the length. This means you haven't changed the wave speed, but increased the wavelength, so the pitch goes down.

2) You have increased both the tension (EDIT: and decreased the density) and the length. Increasing the tension (EDIT: and decreasing the density) should increase the pitch, and increasing the length should decrease the pitch, so it's a question of which effect is bigger. It turns out that for your rubber band, the effect of tension was bigger. This may not be true for all rubber bands at all points in their stretch.

3) Not my field, so I'm going to have to leave this one for someone else.",null,0,cdj7spc,1r36co,askscience,new,3
KarlOskar12,"3) Our eyes can see about 30 FPS. When the rubber band *blurs* as you say, it just means that it is cycling at a faster rate. Side note, if you get a strobe light where you can alter the rate of light flashing and you set it to flash at *the same rate as the rubber band is vibrating* the rubber band will appear to be stationary (as long as you shut all the other lights off).",null,0,cdjabwu,1r36co,askscience,new,2
kooksies,"I believe fibrocartilage is stronger than hyaline cartilage. 
This is because it contains type I collagen which form thicker fibres and are packed more densely together than collagen found in hyaline cartilage. Meaning it has good tensile strength, flexibility and general rigidity.

Fibrocartilage can be found in tendons and ligaments, but they are mainly found in the tissue that separates your vertebrae.   

It shouldn't break as easily as hyaline cartilage, but i have no idea how it would affect you during working out. You should probably seek a  doctor's opinion !",null,0,cdjarwg,1r369o,askscience,new,2
KarlOskar12,"Cartilage is poorly vascularized tissue and as a result of low blood supply (amongst other things) it has poor regeneration capabilities. When damaged it is not replaced by another type of cartilage. However, cartilage that is repeatedly injured becomes highly vascularized and consequently calcified and it starts to turn into bone. This process causes arthritis.",null,1,cdj9cjp,1r369o,askscience,new,1
mc2222,"when shooting with a round aperture in a camera, every point will be imaged as a tiny round point.  When imaging something like a star (a very good point source), the image will be a point, but the out of focus image will be nothing more than a big round disk.  It gets a bit more interesting when we change the shape of the aperture.  Let's put a disk in the center of the aperture to block some light - [like the design of some telescopes](http://nimax-img.de/Produktbilder/normal/10215_2/Meade-Schmidt-Cassegrain-telescope-SC-203-2034-8-UHTC-LX90-GoTo.jpg).  The aperture for such systems is a doughnut shape.  When you try an image a star, it will be a pinpoint at the focal plane, but will be doughnut shaped either inside or outside of focus, like [this](http://legault.perso.sfr.fr/airy_collim_2.gif).

Bookeh is simply exploiting the shape of the aperture to make out of focus point sources take on that shape.  Want heart shaped bokeh? Simply cut out a heart shaped pinhole and put it in front of the camera.  The camera will still focus on what you want it to, but everything that's out of focus will look like a heart.",null,0,cdjdc4h,1r35z3,askscience,new,1
quality_is_god,"Engineer here.
Essentially the answer is never. 

Unless the force exceeds the yield stress of the steel in the spring causing the steel to fail, or if the spring is heated up and cooled down (re-crystallizing) in the deformed shape, or it completely rusts, it will spring back when the force is released.

Metals can fail under repeated compression/release cycles if close to the yield stress, but a constant load like a weight will not cause steel to permanently compress (with the  above noted exceptions).

Many of the engineered structures you take for granted rely on the permanence of the elasticity of steel below the yield stress.",null,0,cdjdjkq,1r35c8,askscience,new,13
nosignificanceatall,"It is common to describe the creep rate of a material by an empirical power law of the applied stress, i.e.  
dε/dt = k\*σ^n where k is a constant determined by the creep mechanism, temperature, etc.  If we substitute ε=σ/E, then we have a simple differential equation for the time-dependence of the stress.  It can be solved to yield  
σ(t) = ( σ(0)^(n-1) - E\*k\*t\*(n-1) )^1/(n-1)

This model predicts that the stress never reaches zero, so there will always be some expansion when the load is removed, but that the expansion becomes arbitrarily small as time increases.  If you run experiments/look up tabulated values to determine E, k, and n, then you can use the above equation to predict how much your material will expand when you remove the applied load after some time t.

Adjustments can be made to the differential equation to better suit the particular material that you are working with.  For example, when the dominant mechanism of deformation is dislocation creep (as is probably the case for your example of a spring), it is common to replace σ^n with (σ-σ\*)^n - that is, the material ceases to creep once the stress drops below some threshold σ\*.  In this situation, the material will still expand by σ\*/E even if you leave it under stress for an indefinite amount of time.",null,2,cdje1vg,1r35c8,askscience,new,5
Trill-Nye,"Materials scientist here. Technically, all materials will, over a long enough time scale, relax at the atomic level such that any stresses are negated.

Stress, like almost everything else, really happens at the atomic level. When you compress a string, the atoms in that metal must rearrange in some way to accommodate this deformation. This can means stretching of atomic bonds or simply the grinding together of microscopic grains within the metal, for example. These structural rearrangements will leave the atoms in a high energy state.

Thermodynamically, atomic systems tend to relax to their lowest energy accessible state. Thus, when the atoms are rearranged during deformation, they will then move around until they adopt an arrangement that minimizes the material's internal stress, one similar to that exhibited prior to deformation. The problem is that, at room temperature, this process is extremely slow. The motion of atoms, called diffusion, is, for most materials, only appreciable at high temperatures.

So yes, the relaxation time of an applied stress can be calculated if a lot of material specific values related to atomic vibrations, diffusion, grain boundary slide, structure defects, etc. are known. For most engineering materials, at room temperature, this will yield very long times (sometimes geologic timescales). Plastics, on the other hand, often relax very quickly, such that it can be observed in laboratory experiments. A good place to start if you want to learn more would be to read about the highly industrial-relevant process of [annealing](http://en.wikipedia.org/wiki/Annealing_(metallurgy)). ",null,0,cdjey0z,1r35c8,askscience,new,4
FlyingSagittarius,"Springs are usually made of metal, which has already been discussed.  Ceramics act similarly, within the elastic limit at least.  Polymers, on the other hand, can have both a strain and a strain rate, so if you apply a constant strain to most plastics they will eventually flow to relieve the stress.

The basic equation for this behavior is e = (S / E) + (T / V)*t.  e is the normal strain, S is the normal stress in the material, E is the tensile modulus, T is the shear stress in the material (which always exists, even if only normal force or stress is applied), and V is the viscosity.  If you set a value for e and you know everything else, you can solve for the time.  E and V are material properties, T can be calculated from S, and S can be calculated from e.  The resulting expression is a differential equation that shows the stress in the material asymptotically approaches zero, and the strain rate asymptotically reaches zero.  The time constant depends on the material properties.

This is for a constant strain, though.  What about for a constant stress?  This would be like suspending a weight with a polymer rope.  As the polymer rope supports the weight, the polymer flows to relieve the stress.  But the weight responds by dropping down, and ""reapplying"" the stress.  So the stress stays constant, and the rope would respond with a constant strain, plus a constant strain rate.  So it'll stretch initially as the weight is applied, and will stretch even more as time goes on.  (Eventually it'll stretch too much and snap, though.)",null,0,cdjij1v,1r35c8,askscience,new,3
OrbitalPete,"Erosion is overwhelmingly responsible.

Once the plate has moved sufficiently for an island to lose its source magma, there is no replacement material being supplied. AGes along the chain look like this: http://www.uhh.hawaii.edu/~kenhon/GEOL205/Chain/chnmap.jpg

It's basically impossible to say exactly how big the other islands and seamounts were at their peak, but there's no reason they couldn't be similar sizes to Hawaii. There may also have been smaller islands grouped clser together at times. Once you're looking at the old seamounts it's likely that the material preserved now is simply the highly agglutinated material surrounding the vent itself, with large amounts of the surrounding pillow lavas and hyaloclastite (which form the bulk of submarine volcanics) having been strongly eroded.",null,0,cdjiimo,1r329d,askscience,new,4
RageousT,"Well, the hotspot is, somewhat unsurprisingly, hot, and thus the crust above it is hot, and thus buoyant. This contributes to Hawai'i being as high as it currently is. As the hotspot moves away from the old islands, they cool, and thus sink. Not entirely sure how much this compares to the contribution from erosion, however.",null,3,cdj8ng3,1r329d,askscience,new,2
ScootMaBoot,"A penny would sink.

[The density of water at the bottom of the Mariana Trench is only ~5% greater than at the top.](http://en.wikipedia.org/wiki/Mariana_Trench)

Liquid water (and liquids in general) are [not very compressible](http://en.wikipedia.org/wiki/Compressed_fluid), especially when compared to gases.",null,2,cdjc7my,1r2z86,askscience,new,7
varodrig,"Since liquids as virtually incompressible, water has virtually the same density even at the bottom of the ocean. The density of copper is 8960 kg/m3. The density of water is 1000 kg/m3. Therefore a penny would sink all the way to the bottom without much noticeable slowing.",null,0,cdjk3iu,1r2z86,askscience,new,3
jofwu,"Sounds to me like you're mixing up pressure and density. While a gas's density is directly related to pressure, this is not typically the case for liquids (including water).

Buoyancy is the force responsible for making things float, and it is a function of the relative densities of the fluid and the object. Water pressure is not involved.",null,0,cdjz5t6,1r2z86,askscience,new,1
datums,"This is a politically loaded question, but in general, the answer is no. Most of the significant gains have come as a result of regulation by governments. Two examples of this are CFCs (chlorofluorocarbons), and PCBs (polychlorinated biphenyls). Though they were both highly damaging, and demonized in the press, they were both widely utilized in markets that had not banned them.      
  
A potential reason for this is that their use is not always easily tied to particular consumer products. For example, PCBs were used mainly in the operation of heavy machinery. It would be hard to say that a product was 'PCB free', given the number of mining, refining, and manufacturing steps involved in making most consumer products. One might be able to say that a pencil is PCB free, but it would be difficult to say that none of the machinery involved in any of the steps required to make a pencil were PCB free. 
  
",null,0,cdja8ra,1r2yus,askscience,new,1
adamsolomon,"The Milky Way is quite a standard spiral galaxy. Nothing especially out of the ordinary. The Milky Way, by the way, is filled to the brim with stars with planets, and it's quite likely that some of those have life. Already we're beginning to find planets almost like Earth around other stars, and those are generally hard to find - not to mention we can only look in our solar neighborhood. The same goes for any other decently-sized galaxy. So it's not as if life on Earth is necessarily a rare feature in itself, either.",null,0,cdj5orq,1r2xuh,askscience,new,6
wwarnout,"For stars whose axis of rotation is not pointing toward us, we can measure the light (spectra) of the left and right sides of the stars.  Since one side is moving away from us, and the other is moving toward us, there is a shift in the spectra (similar to the doppler shift that causes a car coming toward us to sound different than when it goes away from us).

By observing this difference, astronomers can calculate the rotational speed.",null,0,cdj6zlq,1r2xu4,askscience,new,4
selfification,"So you know how we write decimal numbers?  What exactly are we doing?  We're trying to represent numbers as digits that we ascribe a meaning to based on their place in the sequence.  If our number is in base b and our digits are a0,a1,a2...  Then the number we are trying to represent is a0 + a1*b + a2*b^2 + ...  Power series generalize this concept.  In a power series, your coefficients can take on any value and you can use an arbitrary base.",null,1,cdjg3jo,1r2u62,askscience,new,4
DarylHannahMontana,"Conceptually, I think of power series (and Fourier series) like I think of a chemical compound having a ""recipe"" in terms of its component elements. For instance, the chemical aluminum silicate is 

   Al_2 O_7 Si_2

i.e. to make an aluminum silicate molecule, you need 2 aluminum, 7 oxygen and 2 silicon. 

In a similar way, we can decompose a function in terms of simpler functions (polynomials or sine and cosine), though there are some obvious differences, mainly that you can have any amount of each component (unlike compounds/elements, where only integer multiples happen), and there are an infinite (but [countable](http://en.wikipedia.org/wiki/Countable_set)) number of ""elements"" to consider. For instance, the function e^x and its Taylor series

   e^x = 1 + 1 x + 1/2 x^2 + 1/6 x^3 + 1/24 x^4 + ...

give you a ""recipe"" for e^(x): you need a 1, an x, half of an x^(2), one-sixth of an x^(3), etc.

Alternatively, if you already know some linear algebra, a more directly parallel idea is that of a vector space, and the idea of a basis. Recall that a basis for an n-dimensional vector space is a collection of n vectors {v_1, v_2, ... v_n} that ""span"" the entire vector space. That is, every other vector can be written

   w = c_1 v_1 + c_2 v_2 + ... + c_n v_n

with *some* choice of numbers c_1, ..., c_n.

This analogy is especially apt for Fourier series, where there is a notion of orthogonality; i.e. you can rigorously define what it means for two functions to be ""perpendicular"" and if you know some linear algebra, you know that this can make bases much easier to work with.

There are also ""series expansions"" of functions that are much more general than power series or Fourier series; depending on the setting and application, there are reasons that make some of these more ""exotic"" series better suited to the task at hand. If you're interested, you might look up [wavelets](http://en.wikipedia.org/wiki/Wavelet).",null,2,cdjop6k,1r2u62,askscience,new,4
aczelthrow,"Power series are used in two major ways.

Often in analysis (calculus and such) power series are simply types of functions that behave like polynomials of infinite degree. Sometimes they only converge (or make sense) for particular values of the variable. Sometimes we write other functions in terms of power series, such as exp(x) = sum(n = 0 to infinity) x^(n)/(n!). There are methods to figure out how to write a given function as a power series. In that sense, a power series is way to represent a function that is often more convenient. For instance, if you had paper and pencil only, how would you compute exp(2)? The best way is to use the power series. (Many calculators and mathematics software internally use power series, or something related, to compute exp, sin, cos, etc.)

In algebra, by contrast, we sometimes use ""formal"" power series. A formal power series looks just like a usual power series but it's regarded as its own object divorced from any notion of values of x for which it converges or not. The coefficients of the formal power series may be chosen from a given ring or field (generalizations of real numbers). We can add, multiply, even divide formal series using the usual rules, and doing so we define a ring or field of power series that are of interest to algebraists. This kind of power series is less encountered in applications.",null,1,cdjhknw,1r2u62,askscience,new,2
Spiralofourdiv,"Which part is confusing, the ""power"", or the ""series"" part?

It's important you understand what a series representation of a function is fairly well before you can understand what makes a power series special. A power series is just a clever way of rewriting a function as a, perhaps infinite, polynomial using its derivatives and a center. How it works takes a bit of calculus background. If you have a calculus background and are still mystified, then your hang up probably will be answered sometime throughout a real analysis course. 

However, I have a feeling exploring series' in general as representations of functions might answer part of your question. A great example is a Fourier Series, which is a way to describe **any** periodic function as a sum of sines and cosines. First, consider and understand [this](http://en.wikipedia.org/wiki/File:Sine_curve_drawing_animation.gif) illustration of sine. Now look at what a [Fourier Series](http://i.imgur.com/w1IuPKJ.gif) looks like in the same fashion. If you were to add an infinite number of circles, you'd have a square wave, even though it's simply a sum of the very-non-square trig functions. Each additional circle is just another sine or cosine term in the series, and this makes sense in the picture! A little circle is added on that behaves the same way. The specific radii and angular frequency will depend on the series, but study this gif and it will make sense how the series approaches a square wave.

Most series work in a really similar way: they start out as a broad approximation and then, as more terms are added, get closer and closer to the function. Sometimes you can describe the function in a finite number of terms, and sometimes you need an infinite amount. A power series is no different: each term in the sequence of partial sums is a more accurate approximation of the function than the proceeding element. With an infinite amount of elements, we get infinite accuracy and we're done.

If you're asking WHY an infinite series is easier to deal with than the original, nice looking function, all I can say is there are too many reasons to count, or at least applications to count. Most of the time it has to do with certain operations being significantly easier to do on the series representation.",null,0,cdkx38e,1r2u62,askscience,new,1
iorgfeflkd,"There really isn't much antimatter in the known universe. Why, is a mystery. Positrons are created fairly frequently and they often annihilate with electrons. If you look for positron-electron annihilation in the sky you see it [coming from the galactic center](http://www.cesr.fr/~jurgen/spi/spi511map-pr-hres.jpeg).",null,2,cdj006a,1r2pq1,askscience,new,16
miczajkj,"It mostly does. When for example an positron is produced in the ß^+ Radiation (what happens rather often), it ionizes the medium it is moving in and slows down until it finally annihilates with an electron.

The biggest part of antimatter in our universe consists of anti-neutrinos: just like neutrinos they just don't interact with other matter/anti-matter very often and are therefore very likely to have a long lifespan. They are for example produced in the ß^- radiation. 

As we know so far, there is no particular difference between matter and anti-matter that can explain, why there is much more matter, so much more, that anti-matter can't really enjoy it's existence until it annihilates with regular matter. This is one of the unsolved mysteries of particle physics. ",null,0,cdj09t7,1r2pq1,askscience,new,3
atomfullerene,"The fundamental continental arrangements which lead to an ice age (namely the formation of the Isthumus of Panama, and, for Antarctic ice, the Drake Passage)  have not changed in the past few thousand years, so barring anthropogenic climate change, we'd be heading into another glacial period as soon as we swing back around the proper point in the [Milankovitch cycle](http://en.wikipedia.org/wiki/Milankovitch_cycles).",null,0,cdjpewm,1r2ppd,askscience,new,3
ModernTarantula,"Just saw on TV. The Earth wobbles on its axis. At a certain wobble winters would be shorter The orbit also changes from more elliptical to less. Less elliptical the winters are warmer. Both those conditions are current.  However we are closer to the next glacial period, due to be in 1500 years.",null,2,cdjh7qp,1r2ppd,askscience,new,2
blacksheep998,"It has more calories in the sense that hot things have more energy than cold ones. But it wouldn't have more usable calories. Not directly at least since the body cannot directly convert heat into a usable calorie source for metabolic action.

Indirectly though it might give us a little more energy. When you eat something it enters your stomach and rapidly changes to body temperature. In the case of something cold this means it absorbs heat from your body. This heat has to be replaced and the body does so by burning calories.

If the food you ate was warm then the heat energy would be absorbed by your body which would not need to burn as much to keep warm, for a few minutes anyway.

I'm on my phone so I can't do any calculations right now to see what exactly the difference is, but its not a lot. Not what you'd consider a 'significant amount.'",null,10,cdiym89,1r2n1j,askscience,new,56
Gibonius,"Most foods have specific heats around 0.6 to 0.8 kcal/kg*C.  A steak has around 2000 kcals/kg in chemical energy.  If we look at a +40C change in temperature (reasonable), we're looking at 26 kcal/kg (for steak).

That's 1.3% more energy, which I'd say counts as ""not significant."" 

That's neglecting any consideration of how the body uses thermal energy.  I'd be reasonably comfortable assuming it's far less than 100% efficient.  ",null,4,cdiz4o9,1r2n1j,askscience,new,24
Oznog99,"Only in one specific case of hypothermia.

The body produces about 100W of heat, and in normal operation regulates internal temperature to ~98.6F by regulating blood flow to the skin and perspiration.

If you drink 1L (1KG) of icewater at 0C (hopefully this takes awhile), this will steal 37,000 ""small calories"" (the proper chemistry term) , which would be 37 ""food calories""/""large Calories"", to warm it up to body temp.  This is confusing, but the calories for food you know are actually ""kilocalories"".

But the body doesn't work that way.  As you drink icewater, the body senses a small temperature drop and reduces blood flow to the skin.  If you were sweating before, you'll stop sweating now. The skin is not as warm and stops losing as much heat to the environment.  

Soon the same generation of heat in the body, with a *reduction* in heat loss through the skin, restores temp to normal and everything goes back to how it was before.

In general the body does NOT increase metabolism to restore heat, so the cold water (or food) does not count against your dietary calories.  

There is an exception, and that's ""shivering"", an automatic emergency defense.  Shivering can consume 252 kcal/hr!  IF you were shivering, and drank hot unsweetened tea with no dietary calories, the added heat will reduce the shivering condition and thus decrease the amount of calories you burn during that day.  

It is an unusual case, though.  And a terrible concept for a diet plan.  As noted, simply turning down the AC to 58F will not cause your metabolism to increase at all.  You must be *shivering*, which is very uncomfortable, and wreaks havoc on body processes because a hypothermia is an emergency condition the body is responding to.  You can't think clearly, digestion may slow down or speed up, your muscles may cramp.  Greater calorie consumption can be had simply with moderate exercise, and going for a walk is far more enjoyable- and effective- than trying to take a bath in icewater.

",null,1,cdjd0b7,1r2n1j,askscience,new,6
CanadianSnow,"There are two things to consider, when both dishes have the same amount of potential calories
1) the amount of body heat required/expended to heat the cold food as it enters your body so it can be digested. Our bodies do not digest cold food, even ice cream is first heated to body temperature before it can be digested fully.
2) The thermogenic effect of food relates largely to spiciness of food, however a very hot dish will cause the body to expend calories via sweating to cool the body down from the heat/spiciness of the food.",null,0,cdj4g7x,1r2n1j,askscience,new,2
rightwaydown,"Actually food cooling can undergo chemical reactions changing it's exact extractable energy. For instance potatoes left to go cold have a higher percentage of resistant starch. 

I'm can't think of many examples though. In general I think they would be very similar. Of course there are many examples of food gaining calories from being cooked, but that's not your question.",null,0,cdjnarv,1r2n1j,askscience,new,2
ModernTarantula,The Calories of food is that which is generated by burning it completely in a closed stove. Our metabolism is the breakdown  and rebuilding of our body. The calories needed for that is only chmical not thermal. If we wanted to save energy used to maintain body temperature we should use the stove to heat the air not the soup.,null,0,cdjgo5l,1r2n1j,askscience,new,1
puma721,"The idea of a fan isn't that it makes your room cooler (although ceiling fans can move cold air from the ground upward, or hot air off the ceiling)
The benefit a fan gives you is that you feel cooler because the air is more efficient at pulling moisture off your skin.",null,0,cdj2jp7,1r2n13,askscience,new,14
LogicForDummies,"Depends on three factors mainly. 1) The insulating value of the ""walls"" 2) the temperature difference between the ""inside"" and the ""outside"" of the sealed room and 3) The amount of heat produced by the fan motor.

If the outside temp is lower AND the insulating value allows enough heat to pass through the walls at a rate higher than the heat added from the motor AND the rate of thermal transfer is higher with the air moving over the ""walls"" (think heatsink) versus static air after factoring the added motor heat, then it will cool the room. Otherwise it will warm the room. But if you were in the room (which also adds heat) you would still ""feel"" cooler with the fan on even though you could be heating the room, up to a point.",null,0,cdiygfh,1r2n13,askscience,new,10
S7R4nG3,"This question is highly dependent on what you have inside the room ""measuring"" the temperature?

If you are taking a person inside and using their sense of temperature as a ""measurement"" then yes, they would feel cooler as a result of normal sweat evaporation on the skin.

If you are taking a thermometer in the sealed room, then it will always read the ambient temperature of the room as it is uninhibited by the effects of [wind chill](http://en.wikipedia.org/wiki/Wind_chill#Explanation).

So, with a dry thermometer at the stable room temperature, the effect of adding a ceiling fan to the room would not change its reading. 

However, because you state that the room would be warmer than the ambient temperature around the room, the effect would be just as you specify, it would increase the rate by which it cooled to the ambient temperature.",null,0,cdiyfx5,1r2n13,askscience,new,4
InternalEnergy,"Chemical engineer here--lots of training in thermodynamic analysis.

Let's make some assumptions. First, the room is completely sealed off to flux of matter. Not a single atom passes through the doors, walls, etc.

Second: the room is well-insulated, so an adiabatic assumption is valid. No heat is transferred through the boundaries of the room.

Third: the boundaries/walls are completely rigid. They will not move at all even if the pressure inside the room increases.

Ok. Now let's do some thermodynamics. First law of thermo states that matter and energy is conserved: neither created nor destroyed. With no heat flux (adiabatic), matter-material flux, or pressure-volume work (rigid boundary), the only input to the room is the electrical energy required to power the fan. Moving stuff takes energy--air is no exception, and certainly the blades of the fan have mass too. So we have energy coming in, in the form of electricity.

Ever feel the heat coming off of your computer or smart phone during use? Electrical energy converts easily to heat.

But we insulated the room, remember? And we sealed it too, so no hot air leaves, to take that energy away. So the end result is the room gets warmer. 

In fact, the only reason why a fan might make you perceive the room as cooler is because you sweat. The moving air evaporates that sweat, which requires heat energy, which your body supplies. 

Yay, thermodynamics!",null,0,cdj4a4m,1r2n13,askscience,new,3
do_od,"A full water bottle would not be crushed because water is very incompressible. [This experiment](http://earth.geology.yale.edu/~ajs/1969/ajs_267A_11.pdf/70.pdf) shows that in order to press a liter of water into a 9 deciliter bottle you have to subject it to a pressure equivalent to that on the bottom of a 33 km ocean, three times deeper than the Mariana trench which is the deepest known on Earth. Salt water and oil is similarily incompressible. ",null,1,cdizgkk,1r2lsg,askscience,new,18
blackhawk0093,"Deep-sea ecologist here. When we send down our collection chambers we ALWAYS fill them with surface water. As mentioned earlier on this thread, water isn't very compressible so going down 1,000 meters or so isn't going to cause a huge effect. On the other hand, there's been a few times when people forgot and sent them down with air, and they literally imploded on the side of the sub. This is (partly) also why all of the electronics on the sub are encased in oil- it won't short everything out like water or implode like air, and you can actually increase the pressure as the sub descends so it is always under a slightly higher pressure than the surrounding seawater (so small leaks will leak out, not in - better to lose some oil than gain some seawater). ",null,0,cdjbeer,1r2lsg,askscience,new,8
topher-dot-com,"The bottle would shrink but it wouldn't be crushed, because liquids aren't very compressible. [Here] (http://docs.engineeringtoolbox.com/documents/309/water-density-temperature-pressure_2.png) is a graph of the density of water as a function of temperature and pressure. even at 200 times ambient pressure the density only increases by about 1%. 

It doesn't really matter what is in the bottle as long at it is completely filled with liquid then it won't be crushed because liquids aren't very compressible.

Bathyscaphes, use gasoline to trim buoyancy because it is a liquid (so it won't compress at those depths) but it is less dense than water. So it is used to help it return to the surface since the ballast tanks that have air in them on the surface now have water, which can't be pushed out because the pressure is too great.",null,2,cdizvs2,1r2lsg,askscience,new,6
Thew_Nell,"This is probably a bit off topic, but you might be interested in the movie ""The Abyss.""  One of the characters is able to withstand incredible deep oceanic pressure because of this idea.  He 'inhales' a liquid that replaces the gasses in his body to reduce compression.  However,  this isn't entirely accurate, because it is a movie, but I thought I would bring it up regardless.
",null,1,cdj52ec,1r2lsg,askscience,new,3
S7R4nG3,"Ok, from what I'm remembering when going through dive training, if you take a full perfectly water-tight waterbottle that allows no water to escape regardless of pressure and that imparts no bouyancy, then yes it would crush as you moved it further down the water column. 

When you fill the water bottle at the surface, you are sealing in the water at the ambient sea-level pressure, thus as you moved it down the water column, the pressure around it increases crushing the bottle to compress the contents to ambient pressure. 

I specify contents here because water at sea-level generally has more dissolved gases that would allow the bottle to compress somewhat.

When considering similar effects of different liquids, its quite interesting to look at older [bathyscaphes](http://en.wikipedia.org/wiki/Bathyscaphe) that used gasoline to trim their bouyancy due to its higher density than water. You can also read up on [Henry's Law](http://en.wikipedia.org/wiki/Henry's_law) that defines the solubility of gases in liquids at pressure.",null,10,cdiz9wf,1r2lsg,askscience,new,3
fishify,"In the first several minutes after the Big Bang, the first nuclei formed.  At this time, this matter consisted of about 75% hydrogen and 25% helium; there was a little bit of lithium and beryllium formed, too.  (Hydrogen nuclei are the easiest to form -- quarks bind to form nucleons, free neutrons are unstable, and protons are single particles, whereas nuclear fusion processes are needed to generate even the slightly heavier nuclei.) To this day, we see that the matter in the universe is around 3/4 hydrogen and nearly 1/4 helium.

Atoms, rather than nuclei, did not become stable till the universe was about 370,000 years old; prior to this, the universe was so hot that electrons and nuclei did not reliably bind.

The nuclei heavier than the ones made right after the Big Bang are primarily formed via the nuclear fusion processes that go on in stars.  The first stars formed when the universe was around 200 million years old.  Over time, stars convert some of their matter to heavier nuclei: first helium, and then heavier nuclei (especially carbon, nitrogen, oxygen, neon, magnesium, silicon, sulfur, and iron, but other nuclei up to iron as well), and then nuclei beyond iron arising during supernova explosions.

So the short answer answer is that a lot of hydrogen was formed in the early universe, and all the subsequent nuclear processes have only converted a small fraction of that.",null,0,cdiy1qz,1r2kqn,askscience,new,8
f4hy,"Hydrogen is the simplest atom. It is just an electron orbiting a proton. In the early universe, the aftermath of the big bang, there were processes which could create protons, but protons repel each other unless you can get them really close together. So in the aftermath of the big bang, we were left with a bunch of protons, and electrons were able to find them, but it was not until stars formed and nuclear fusion at their cores to fuse the protons together into heavier atoms.",null,0,cdixjmh,1r2kqn,askscience,new,4
meaningless_name,"Does coarsely ground coffee make a stronger cup? I don't think it does. The higher surface area of finely ground coffee maximizes surface area exposed to water, as you pointed out. This definitely means fine grinds will produce stronger coffee in a given amount of time.",null,3,cdiz3ra,1r2jdi,askscience,new,24
2dwgs,"Coarsely ground coffee is usually exposed to water for more time (french press, for example) which leads to a bold cup of coffee. But finer ground coffee has more surface area exposed, which leads to a stronger flavor when comparing coarse vs. fine *given everything else is the same* (brew time, water amount, origin, roast, etc.)",null,0,cdizqhy,1r2jdi,askscience,new,8
endocytosis,"Not a barista, but increasing the grinding time on something has an effect on surface area.  Here's an example:

4 coffee beans, 16 x 16 x 16, total volume = 16384 units (4096 units each, assume for simplicity the coffee is cubic in shape)
SA = 6*L^2 = 6*256 = 1536 x 4 = 6144 units total surface area

The coffee goes through a special grinder and the grinds are perfectly halved in size to be Coarsely ground coffee:
8 grounds of coffee, 16 x 16 x 8 = 16384 total volume (2048 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*16) + 2(16*8) + 2(16*8) = 8*(512 + 
256 + 256) = 8192 units total surface area

This makes sense.  You don't put coffee beans into a coffee maker, you grind them up.

The coffee again goes through the special grinder and the grinds are perfectly halved in size to be Finely ground coffee:
16 grounds of coffee, 16 x 8 x 8 = 16384 total volume (1024 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*8) + 2(8*8) + 2(8*8) = 16*(256 + 
128 + 128) = 8192 units total surface area

No Change.  So we keep on grinding...

The coffee once again goes through the special grinder and the grinds are perfectly halved in size to be Super Finely ground coffee:
32 grounds of coffee, 8 x 8 x 8 = 16384 total volume (512 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(8*8) + 2(8*8) + 2(8*8) = 32*(128 + 
128 + 128) = 12288 units total surface area

Now the surface area is greater, but it is worth noting that the relationship between surface area and grind time isn't exactly linear.  However, the more finely ground something is, *usually* the slower it is that the coffee passes through.  More coarse grinds are bigger and don't fit together as well, so there's more room for water to pass around the bits and pieces.  Finer grinds will pack together more tightly and slow the passage of water, probably increasing flavor and caffeine extraction.

TL;DR: Grinding coffee more increases surface area, but you need to fine-tune your grinding to get the maximal effectiveness.  Also the finer the grind, the longer the water will sit.",null,2,cdj21cy,1r2jdi,askscience,new,3
null,null,null,13,cdize84,1r2jdi,askscience,new,9
PraecorLoth970,"I attended a lecture that addressed exactly this issue. It was by Dr. Gerald H. Pollack who has a lot of studies published on water. I've found a video lecture which looks similar to the one I watched, so I recommend you watch it in its entirety. It was fascinating. So many things that I took for granted and he, with compelling evidence, showed how wrong many of my assumptions were.

The video lecture is [here](http://www.youtube.com/watch?v=XVBEwn6iWOo). The part about clouds is at about [48:30](http://youtu.be/XVBEwn6iWOo?t=47m22s) but in order to understand the explanation, you need to watch the entire thing.

If I may try a TL;DW: It's because on many interfaces, water molecules aggregate on a liquid crystalline structure which he proposed as being similar to that of ice, which is negatively charged (this is called the exclusion zone). This would give water droplets's surface a negative charge, and the atmosphere is positively charged (ground is negative, atmosphere is positive). This results in the negatively charged water droplets to attract each other (like likes like. Weird, huh?), because between two negatively charged surfaces of droplets there would be a bigger concentration of positive atmospheric charges, and the droplets would be attracted to this, and would move towards each other, and thus coalesce into a cloud. There is a [TEDxTalk](http://www.youtube.com/watch?v=i-T7tCMUDXU) which is shorter. Youtube has lots of other videos with him.

Edit: Made my TL;DW a bit more clearer and more accurate.

Edit 2: Some images from his articles and videos.

* [Abstract 1](http://i.imgur.com/XLXVqHT.png)
* [Abstract 2](http://i.imgur.com/c2PrPxg.png)
* [Abstract 3](http://i.imgur.com/XkpMnkv.png)
* [Abstract 4](http://i.imgur.com/nncKR1w.png)
* [Exclusion zone](http://i.imgur.com/tUUF8nq.png)
* [Charge in the exclusion zone](http://i.imgur.com/aOlwvlT.png)
* [Tube with EZ; Produces flow without pressure difference. Energy source is ambient radiation, infrared](http://i.imgur.com/euZ0PvO.png)
* [Positive region created due to negative spheres.](http://i.imgur.com/ckAQYVq.png)
* [Negative region created due to positive spheres](http://i.imgur.com/vzz3DZH.png)
* [Like attracts like](http://faculty.washington.edu/ghp/images/stories/likelike.png)
* [Formation of the exclusion zone (Video)](http://vimeo.com/7294988)
* [Water droplets on water surface (Video)](http://vimeo.com/7279153)
* [Pollack laboratory webpage](http://faculty.washington.edu/ghp/)
",null,8,cdj5gbr,1r2j3i,askscience,new,17
SpicyBuffaloFeather,"Based on my understanding of meteorology: air with differing humidity levels and temperature do not readily mix, especially in fairly calm weather. Instead, a parcel (bubble) of warm air will rise if surrounded by cooler air until it cools adiabaticaly due to expansion. When this happens, the relative humidity of said parcel approaches 100% (relative humidity is a measure of how much water vapor a given unit of air is holding vs. how much water vapor it can hold before it is totally saturated. When the parcel cools sufficiently, it reaches the saturation point and water vapor begins to condense on condensation nuclei (fancy science terminology for bits of dust floating around in the air). If enough water vapor condenses on said nuclei, it will become heavy enough to fall. When it falls a process known as ""collision and coalescence"" occurs which basically means the water droplet hits other water droplets and combines to make a bigger droplet, thus increasing the speed at which it falls. However, a raindrop can only get so big before friction with the air causes it to break apart and the process starts anew. 

Fun fact: rain drops do not actually look like like [this](http://www.psdgraphics.com/file/water-droplet-icon.jpg) but infact look like more like [this](http://www.sailingissues.com/raindrop-shape-evolution.png)

Edit: I realize I kind of digressed from your question there. To answer your question, differential heating of the earth's surface is why water vapor does not evenly distribute itself throughout the atmosphere. Warm air can hold more water vapor than cold air and in general, warm air at the equator moves toward the poles while cold air at the poles moves to the equator in a futile attempt to achieve equilibrium. This overly simplistic model is based on the [hadley cell](http://serc.carleton.edu/images/eslabs/hurricanes/3d_hadley_md.v3.jpg). ",null,1,cdjeyox,1r2j3i,askscience,new,6
__Pers,[This article](http://www.scientificamerican.com/article.cfm?id=why-do-clouds-always-appe) in *Scientific American* addresses your question in some detail. ,null,2,cdiz70k,1r2j3i,askscience,new,9
rocketsocks,"Water vapor is transparent, you literally cannot see it.

When you have a pot of water on the stove boiling and you see the ""water vapor"" rising above it's not the vapor you see it's tiny water droplets recondensing as it cools off in the air. Those tiny droplets are still hot (nearly 100 deg. C) and they are easily blown around by air currents, which is why they continue to rise.

Clouds are visible because they are composed of water droplets, not just vapor. ",null,1,cdjbhmp,1r2j3i,askscience,new,2
Surf_Science,"A tentative contig should be just what it sounds, a draft contig. 

For those who don't know what a contig is. 

DNA is sequenced in very small bits, usually 75 to ~800 base pairs. This bits are lined up to make a longer sequence. A contig is one of those alignments and contigs are usually millions and millions of base pairs. A contig of the mouse genome for example could be more than 50 million base pairs. ",null,0,cdizwc9,1r2isv,askscience,new,5
PricaCells,"These are known as sun-sensitizing drugs. This is because they cause certain side effects only when in direct sunlight, such as photoallergy, or phototoxicity. The latter is more common, and it occurs when the skin is exposed to sunlight after certain medications are injected, taken orally, or applied to the skin. The drug absorbs the UV light, then releases it into the skin, causing cell death. So, that's what I can tell you. ",null,1,cdixufz,1r2id5,askscience,new,18
null,null,null,0,cdjhvqy,1r2id5,askscience,new,1
iorgfeflkd,"The extreme example is [Anatoly Bugorski](http://en.wikipedia.org/wiki/Anatoli_Bugorski) who had a high energy proton beam go through his head. He was injured, but survived.",null,96,cdiwr26,1r2gwb,askscience,new,489
DAlder_HardlyKnowHer,"Alpha particles are just that, they are equivalent to a He2+ atom. Which consists of 2 protons, 2 neutrons, and no electrons. Very few things are smaller than alpha particles including isotopes of hydrogen ( H ). Alpha particles are already too large to pass through your body. However, if the source which emits the alpha particle is already inside of you ( ingested or inhaled ) it will cause extreme amounts of damage, pending the amount.",null,21,cdizx62,1r2gwb,askscience,new,113
50bmg,"1 atom won't do much unless it contains extraordinary amounts of energy (see: http://en.wikipedia.org/wiki/Oh-My-God_particle). 


A stream of protons (essentially hydrogen nuclei) would do exactly this if carefully controlled:
http://en.wikipedia.org/wiki/Proton_therapy


A more energetic beam would do this:
http://en.wikipedia.org/wiki/Anatoli_Bugorski


Essentially what happens is the proton will likely collide with another atom (although many will make it all the way through without hitting anything!). Depending on the energy of the proton, and the type of atom it collides with - several types of atomic reactions could occur, most of them altering the molecule it collided with quite dramatically, or releasing heat and various types of radiation. In the worst case - it would break or alter a DNA strand and potentially cause cancer, however in most cases the cell with the damaged DNA would just be unable to replicate and therefore die. ",null,7,cdj40hn,1r2gwb,askscience,new,42
fillterfood,"Astronauts in space report seeing flashes in their vision when they close their eyes. It is thought that these are caused by cosmic rays (nuclei of atoms) shooting through them and hitting their retinas. While I imagine some level of damage is happening, it doesn't seem to be too dangerous.

The most obvious danger is an increased risk of cancer. If the atoms collide with DNA, they can break it apart. While a single killed cell here or there isn't much of a problem in our bodies. It's when the cell gets reprogrammed to start growing endlessly that it becomes a real threat to our health.

So I think the answer to your question is a bit of a yes and no. Physically, there seems to be no risk, biologically however, combined with a bit of bad luck, it can kill.",null,11,cdizsy1,1r2gwb,askscience,new,40
EdPeggJr,"The [Oh My God particle](http://en.wikipedia.org/wiki/Oh-My-God_particle) was a single particle, likely a proton, which hit with a force ""equal to that of 50 Joules, or a 5-ounce (142 g) baseball traveling at about 100 kilometers per hour (60 mph)."" Detectors have confirmed 15 similar events.",null,4,cdj41fd,1r2gwb,askscience,new,15
bertrussell,"This happens to everyone, all the time. There are cosmic radiation sources and also terrestrial radiation sources. Sure, radiation has its issues, but the body can actually handle a fair amount of radiation throughout a person's lifetime.

http://video.mit.edu/watch/cloud-chamber-4058/",null,2,cdj7wu3,1r2gwb,askscience,new,12
dddm,"A single atom or particle (or photon) passing through a human body would never be able to cause much damage, although a beam containing many individual particles certainly could.  There is a range of energies that are most damaging and it's not accurate to say that higher energy radiation sources always produce greater radiation damage.

If the energy is too low, below about 5-100 eV depending on which [definition](http://en.wikipedia.org/wiki/Ionizing_radiation#Ionization_and_the_definition_problem) is used, the radiation is not energetic enough to ionize the target material.  This type of non-ionizing radiation is generally not much of a concern (sitting near a normal light bulb probably won't cause much long term damage), but there still are some [damaging effects](http://en.wikipedia.org/wiki/Non-ionizing_radiation#Health_risks) mostly due to heating in the target material.

Above the ionizing threshold, radiation damage generally increases with energy up to a certain point.  This is because radiation damage is generally proportional to the total energy deposited.  If the energy of the incident particle isn't too high, it will stop within the target material, indicating that all of its kinetic energy was deposited into the target.  In this regime, higher energy particles will always lead to a larger radiation dose.

However, the situation isn't as clear when the incident particle has enough initial energy to pass through the target.  Even if a particle passes through the target, much of its initial energy may be deposited in the target material along its path.  The rate of energy deposition (called the [stopping power](http://pdg.lbl.gov/2006/reviews/passagerpp.pdf)) as a function of depth within the target is described by the [Bragg curve](http://en.wikipedia.org/wiki/Bragg_peak).  As an approximation, the shape of the Bragg curve depends mostly on the species and initial energy of the incident particle, and on the density of the target.

The rate of energy deposition by a particle generally decreases for shallow depths as the particle initial energy increases.  Thunderf00t has an excellent video describing this effect (the pertinent discussion is towards the end of the video, but the whole video is relevant):

https://www.youtube.com/watch?v=oj6v8MtuVdU

For very high energy, the thickness of a human body may be a very small portion of the maximum radiation depth, and the stopping power in this depth range would approach zero as the particle energy increases.  So there exists a threshold energy above which the particle actually does less radiation damage compared to (comparatively) lower energies.

Estimating this threshold energy for simple geometries, such as a human hand in a large vacuum, can be done by integrating the stopping power up to the target thickness and comparing the result for different energies.  For more complicated geometries, for example those involving non-vaccum materials near the target that may produce secondary particles when exposed to the initial radiation, the system generally needs to be simulated using particle tracking toolkits such as Geant4, FLUKA, or MCNP.

In any case, a single particle cannot impart an infinite amount of energy into a target material.  The maximum deposited energy would be on the order of 100's of MeV for protons, yielding a single proton maximum effective dose of maybe 100 nSv.  This is an negligible dose compared to [other common exposures](http://en.wikipedia.org/wiki/Orders_of_magnitude_(radiation\)), such as the daily background dose of about 10,000 nSv.

A string of particles, which would be a model for a particle beam, can certainly lead to intense radiation damage, since a typical particle beam may contain on the order of 10^10 individual particles.",null,2,cdjaj1n,1r2gwb,askscience,new,7
madscientistuk,"Sixty Symbols (a video series from the University of Nottingham about Physics, Astronomy and Maths) answered a similar question which I think is relevant.

As part of the video series they have a number of videos where they asked physicists, astronomers and maths lecturers questions submitted by the sixty symbols viewers. The first question of the 2nd video about viewers questions was:

""If I put my hand in front of the beam at the Large Hadron Collider, what would happen to my hand.""

The responses are great in my opinion :) [YouTube video - Putting your hand in the Large Hadron Collider...](http://www.youtube.com/watch?v=_NMqPT6oKJ8) question answered 0.00-3.54.

This was so popular they then had a second video where they asked scientists working at the Large Hadron Collider the same question. Again I think the answers are great.

[YouTube video - Hand back into the Large Hadron Collider - Sixty Symbols](http://www.youtube.com/watch?v=lVefgfmFg9o)

Edit - hopefully fixed the links",null,1,cdjemqa,1r2gwb,askscience,new,5
yinz_n-at,"Depends. Lost of Probability involved. The higher the energy the more damaging but also the more likely it'll just go straight through you (gamma ray) . Low energy particles are less damaging but more likely to collide with a nucleus in your body (atoms are vastly empty).

A neutron from radioactive decay is pretty damaging because it has a high probability of colliding with a human nucleus. 

And one nuclei isn't all that damaging because the probability of one particle causing damage is very small. Now a flux or beam of particles will do some serious damage. 

Side Note: Ingesting a radioactive material is pretty dangerous because every decay product will be absorbed by your body. Alpha particles are pretty easy to shield against but if theyre not shielded then they'll definitely knock some nuclei around (worst case is knocking a DNA nuclei out causing mutations.) 

Hope that helps!

Source: Nuclear Engineer",null,1,cdjawm3,1r2gwb,askscience,new,5
KillerInYourCloset,"Sorry, this might sound *very* unscientific- but wasn't there an example of someone who put their hand in front a particle collider? I believe they had really no adverse affects. Sorry, I can't look into it much I'm on a sev A call, but not really listening unless I hear my name :)",null,0,cdjb6yn,1r2gwb,askscience,new,3
OnlyOneWithThisName,"I believe this question is a very important obstacle to consider in regard to interplanetary space travel within the immediate solar system, and most likely long distance interstellar travel as well. This article about the [health threat of cosmic rays](http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays ""en.wikipedia.org"") will only partially answer your question, but I believe it is a worthwhile read.",null,0,cdj9o6m,1r2gwb,askscience,new,2
antpuncher,"You have trillions of neutrinos passing through you every second, and you're doing just grand.

The important piece is how much energy the particle can deposit in your body.  This depends on how much energy the particle has, and what the ""interaction cross section"" is.  Like it sounds, the cross section is kind of like how big it is, but for quantum mechanical things it's not really a property of the _size_ of the object, and has to do with things like the electrical force.   

Then it also depends on where it goes.  You could probably take a packet from the LHC through your ear lobe and only lose some skin, but a few wrongly placed alpha particles and you have brain cancer.",null,6,cdj9rbr,1r2gwb,askscience,new,8
websterandy42,"What you are talking about is a very rare scientific concept known as radiation. Haha,  one of the most common types of radiation (and the first to decompose after a nuclear reaction) is literally a helium nuclei. While your cells are tough enough not to be hurt by small amounts. What it really affects is DNA, DNA does not heal so damaged DNA can will work but not properly, the cells will begin reproducing at an increased rate. This is how tumors form.",null,0,cdjmbsd,1r2gwb,askscience,new,2
recycled_ideas,"It depends if it hits anything,  and if so what. At the subatomic scale there's a lot empty space and any given particle could fly right through you doing no damage quite easily. Of course it could also hit something and damage it.  Enough particles and enough hits could kill you. A few hits could cause cancer and kill you over time. Or your body could repair the damage and nothing happens. 

You're actually being hit by particles all day. ",null,0,cdjotgd,1r2gwb,askscience,new,1
denchpotench,"Charged particles would cause a lot more damage than uncharged ones. For example beta radiation is just fast moving electrons or positrons. These have charge and can ionize atoms, if these are ionised at certain points in strand of DNA it can cause the parent cell to die or become cancerous. If an uncharged atom flew through you it might take a few atoms out on the way but would affect far fewer than a positron stripping electrons from many atoms.",null,0,cdk0oc3,1r2gwb,askscience,new,1
Needless-To-Say,"Typically the signals are separated within the common medium by frequency shifting. Each individual endpoint would have a dedicated frequency range within the common medium. Copper cables have a much lower threshold of channels than Cable which in turn have a much lower threshold than Fiber.

When the signal is digital the data is typically encrypted to prevent unathorized access. Anyone trying to Sniff the data would first need to decrypt the data. With older, analog technology, it can be a much simpler matter to ""sniff other peoples signals""

When you get into wireless tech, things get far more interesting with multiple simultaneous security measures in play to prevent eavesdropping but nothing is completely secure.",null,2,cdj234i,1r2esb,askscience,new,5
chrisbaird,"There are two basic ways to stabilize a projectile: 1) passive control, such as fixed fins or spinning the object, and 2) active control, such as tilting the exhaust cones in the appropriate direction, having small position-correcting thrusters, or moving wing flaps. A lot of rockets have both because using passive controls means that your active controls have an easier job to do. But you can get by with no passive controls such as fins if your active controls are advanced enough. 

It depends a lot on the application. Many rockets, such as missiles, spend a large portion of their trajectory in ballistic motion (i.e. rocket engines turned off). In such a case, you can't use gimballing the rocket nozzles to stabilize flight, so fins become more important.",null,1,cdix4y7,1r2e54,askscience,new,20
DUFFYSTE98,"It's all about the exhausts. They can change the direction of the exhausts which changes the path of the rocket.

Hope this answers your question, if you want to read more into it http://inventors.about.com/library/inventors/blrocketcontrol.htm",null,1,cdiw9st,1r2e54,askscience,new,4
rupert1920,"They have a very good idea on what it's _supposed_ to do. This data is available from early research _in vitro_.

In terms of side effects, this is gleamed in toxicological studies and [clinical trials](http://en.wikipedia.org/wiki/Clinical_trials) - specifically Phase I, where the drug is administered to healthy volunteers. To move onto later phases of the trials, safety and efficacy must be demonstrated to the relevant regulating agencies in the country.

Especially now in the era of [rational drug design](http://en.wikipedia.org/wiki/Rational_drug_design), a drug is synthesized with the explicit purpose of interfering with some known biochemical pathway. So it's really not like drug companies are just randomly administering chemicals just to see what they do.",null,1,cdivgvd,1r2dfy,askscience,new,10
snusmumrikan,"Starting a new comment because I'm directly answering your question but that is not to take away from some valid points made by Rupert and botanist. 

I have experience in early R&amp;D at the UKs largest pharma (devilishly difficult clue there). 

For a new drug to have even the slimmest chance of getting to first-time in human (FTIH) it has to have gone through extensive and rigorous tests. Initially almost all drug design begins with the development of assays (repeatable standardised test experiments) which are used to screen a company's vast library (millions and millions) of compounds. These assays are becoming more extensive and phenotypicaly relevant over time as the biggest problem in drug development is attrition, and for every step further down the chain you take a compound before finding out it is not fit for use as a medicine it costs exponentially more - you want to be almost completely sure you're compound is viable before progressing it. A shift away from standard model cell lines and towards disease-relevant tissue samples derived from patients is currently underway as this gives significantly more confidence that the 'hits' are actually good, whilst doing away with the confidence, experience and reliability of those extensively studied model cell lines. If you're interested I can send you sections of my Masters research project which was in this area and has the appropriate references.

Once you have your hits from your initial screen these are then run against standard assays for things like cardiac liability (will it mess up ion channels in your heart?) or general toxicity to know whether it shouldn't be allowed in your body at all, you'd be surprised just how often certain molecules come up as great hits (elevating the protein which is deficient in a disease, for example) but turn out to be the old red herrings like HDAc inhibitors.

After this the compound will be carried forwards into animal trials, and this is where there is too much variation to try and cover it all but it will usually start in something small with a shorter lifespan like mice, potentially recombinant mice which have had a disease phenotype replicated within them, along with healthy controls and so on. After this other animals may be used, for example for respiratory and cardiac models, dogs are used as they represent the most human-like model.

Once the animal testing has been done extensively and for a long period to identify any and all side effects, if there are none which would indicate toxicity or severe side effects in humans the company will spend several months compiling the data, checking it, analysing it extensively before submitting it for Phase 0 clinical trials. These are before phase 1 trials and involve extremely low doses in relatively very few people in order to assess at an early stage the pharmacokinetic (where it goes inside you) and pharmacodynamic (what it does) implications of the compound. 

As you can see it takes a lot to get there and companies want to be incredibly sure of their compound before going near a human because A: it costs an armada full of money and B: no one wants another thalidomide. 

This is obviously a huge subject and i have left out many of the initial stages and assumed the compound came from a small molecule library (as opposed to chemical design, structural rational design or biologics such as antibodies) but the general principles remain true. 

Hope this helps, happy to answer anything else as long as I feel comfortable with my knowledge. 
",null,0,cdj1lcx,1r2dfy,askscience,new,8
czyivn,"All the rules of pharma are written in blood.  Past experiences with bad drugs have shaped our current practice to prevent patient deaths.  Here are the major things that prevent human deaths during drug trials:

1. Test it before it gets to animals.  Lots of general mechanisms where drugs kill people are already known.  Inhibition of hERG channels causes cardiac deaths, so they have an assay where all drugs are tested for hERG inhibition.  Liver damage is another mechanism, so all drugs are tested for toxicity to hepatocytes. 

2. Test it extensively in animals.  Before it ever sees a human, the drug has usually been tested in at least rats and at least one larger animal (dogs, monkeys, pigs).  We aren't smart enough to predict animal toxicity in advance (and anyone who says otherwise is a liar), so the only way to be safe in humans is to test it in other animals first.

2. Dose escalation.  They start out in humans at doses that are usually much less than where they expect serious side effects.  They gradually increase the dosage until side effects start to appear, like elevated liver enzymes.  If any of these symptoms are troubling, they stop the dose escalation immediately, and typically remove that patient from the trial.",null,0,cdizkvg,1r2dfy,askscience,new,3
chrisbaird,"I think you meant to ask ""Is a gravitational pull fundamental"". Of course gravitational pulls exist, they are just not fundamental. Fundamentally, gravitational effects are caused by the warping of spacetime by mass and energy, and not by a Newton-style force field stretching out from massive bodies. But just because a concept is not fundamental, does not make it wrong, imaginary, or useless. Most of the things we talk about in science (e.g. friction, centrifugal force, Van der Walls force, buoyancy, etc.) are not fundamental. The only fundamental things in the universe are the ones in this chart: 

http://en.wikipedia.org/wiki/File:Standard_Model_of_Elementary_Particles.svg 

But describing the motion of a child on a swing strictly in terms of quarks, electrons, and photons would be unnecessary and difficult.

 ",null,1,cdiylvn,1r2dfo,askscience,new,8
iorgfeflkd,"If you drop something, it falls. That is a manifestation of the existence of gravitational pull.",null,2,cdiwi9z,1r2dfo,askscience,new,4
fishify,"Via Einstein, we learned that we can understand gravity as a manifestation of space and time being curved.

Alternatively, we can not take into account this curvature, and then we must define a force to account for gravity.  Said force is attractive and pulls on objects.",null,0,cdiy566,1r2dfo,askscience,new,3
pucklermuskau,"its a trade off of investment. Some species invest little in their offspring, in terms of the amount of energy contained in individual eggs, and in terms of the amount of parental investment afterwards. That means individual offspring have little chance of survival. Other species reproduce less, and later, but invest more in the individual, so that particular individual has greater chance of survival... reproduction per se isnt the goal: successful reproduction is the goal: your offspring need to themselves reproduce!",null,1,cdiu7cw,1r2b9n,askscience,new,10
Unidan,"I think this question isn't so much asking about the trade-offs between being a long-lived organism versus a short-lived one, but rather *why* long-lived organisms would evolve in the first place, correct?

If that's the question, the current idea is that within very *stable* environments, being able to exploit scarce resources can be advantageous.  With this, longer-lived organisms with more experience or learning may be selected for!  

If you're a very short-lived organism in a stable environment, but not necessarily very competitive, you most likely *won't* be favored if you're going up against an organism that has experience in exploiting a particular resource.  So, eventually, evolution has produced organisms that may require large amounts of parental investment, long periods of growth to maturity and more, but these will give an advantage over those that are only equipped with that with which they are born.

Does that make more sense?",null,5,cdj14hm,1r2b9n,askscience,new,9
polistes,"In addition to the answers already given, I'd like to also illustrate this issue with the difference between short-lived plants and long-lived plants (I like using plants because they cannot run away from their problems). Some plants live for many years, while others only live one season. There are advantages for both strategies. Short lived plants use resources quickly and don't have to invest too much in adaptations for surviving hard conditions, like a dry period or winter. They simply make lots of seeds, which can survive these difficult periods. On the other hand, if there is a year in which the growing season is very tough as well (prolonged drought, insect pests like locusts), the population of single-season plants gets a large blow, because many plants won't make it to seed production and you end up with less seeds being deposited.

Now if you are a long-lived plant, you invest a lot in these survival skills and grow slower. If you endure a particulary hard year, you may end up with not having enough energy to make flowers and seeds. However, this is not that bad, because you'll have a new chance next year! For example, there are a lot of plants in remote areas that struggle to attract pollinators for their flowers, so they end up not having seeds in many consecutive years. However, because they are long-lived, they still have many opportunities to pass their genes. If you were a short-lived plant, your population would die of pretty soon.  

So, there are both advantages and disadvantages of these strategies, and neither is particulary better.",null,0,cdjkefq,1r2b9n,askscience,new,1
Truck43,"It may cause a conventional explosion and release of nuclear material, but modern nuclear weapons (older designs are less safe) include safeties to prevent nuclear detonation in event of fire or accident. To have a nuclear detonation requires precise timing of the detonation of the explosive shell, unlikely in a lightning strike. ",null,3,cdiwbg3,1r2adb,askscience,new,11
Sannish,"The fuel tanks of the nuclear warhead would explode, creating a radioactive plume (e.g. a dirty bomb).  I would guess that the currents from the lightning strike would not trigger the nuclear fission process as I assume the warhead design has some basic safety/grounding features in the circuitry.",null,3,cdiwer1,1r2adb,askscience,new,5
redditor5690,"I've seen 60's and 70's nuclear warheads up close many times.

The casings will not allow electrical flow from outside.  Think of what happens when a car or plane is struck my lightning, then imagine how much better it would be if the engineers actually chose that capability as a design requirement.

I would be more concerned with the heat that might be generated in the case if sufficient current flowed through it on the way to electrical earth ground. That heat might be enough to trigger an explosion of the conventional explosives which act as the detonator for the weapon, but those explosive charges, arranged around the U-235/P-239 core, must explode with extremely precise timing to produce a nuclear explosion. But, setting one off would be enough to cause a ""dirty bomb"" explosion which would contaminate a large area.

But, there's always Murphy to consider.",null,0,cdjhktw,1r2adb,askscience,new,2
SimpleBen,"Graying is dependent on the balance of two opposing processes. The first is the loss of pigment, which is dependent on inflammatory factors, natural catalase levels, and H2O2 levels in the epithelium. The second is the restoration of pigment, which is UV exposure dependent.  
  
The scalp above the ears doesn't get a whole lot of UV exposure. ",null,0,cdiyfc4,1r275u,askscience,new,6
bearsnchairs,Without knowing more specifics it could be due to the mixing in your container. Even when doing a simple acid base titration with an indicator you can get transient color change whee you have a local high concentration of titrant. If you increase the stirring rate it becomes harder to form these areas of high concentration so you don't get patches of color change.,null,0,cdiuncm,1r265d,askscience,new,4
PricaCells,"Social grooming is not exclusive to just apes. In fact, its quite common in animal behaviour - and is also present in Humans. However, this may not be in the same context as chimpanzees. Keep in mind that different primates have different patterns of grooming as well.

Amongst people, we see this explained in studies by Nelson, H. (2006), 'Human mutual grooming: an ethological perspective on its form and function' and Nelson, Holly and Geher, Glenn. (2007-09-15) 'Mutual Grooming in Human Dyadic Relationships: An Ethological Perspective'. 

To summarize, there is grooming, but mostly in romantic couples. These are studies that have been done on western societies, however, and from an ethnographic perspective I believe that there are many more examples of grooming in different relationships as well. Social grooming as a form of touch communication is exceptionally important in many mammalian species, and I'd answer your question with a yes. The same is happening with humans, but in different was, and through different means., ",null,2,cdixqbb,1r23q6,askscience,new,7
snickeringshadow,"Tentatively yes, but it's complicated. I'm sure if you look hard enough you could find examples of humans grooming each other as part of social bonding. It's a little cliché but think about your 'typical' teenage girl slumber party where they paint each other's nails. That certainly qualifies. The problem is it's rather difficult to come up with universal explanations of how we do this because human behavior is patterned by culture as well as biology. Humans like to create formalized, idiosyncratic rituals that govern social interactions. All humans are social, but *how* we are social varies between populations and changes over time. This makes it really difficult to get at the biological basis for behavior - as they are phenotypically inseparable from the environmental or 'cultural' components. ",null,1,cdiy215,1r23q6,askscience,new,7
LXZY,It would depend on what you set as zero.  ,null,0,cdizk4y,1r21m7,askscience,new,10
MayContainNugat,"The value of the potential energy isn't observable. Only changes of energy matter. So the value itself is arbitrary, up to a constant. ",null,1,cdj0cth,1r21m7,askscience,new,6
rlee89,"Gravitational potential energies are usually expressed as negative values.  Under such a representation, 0 is the energy when you are infinitely far from the object and at the top of its gravity well, and the potential energy becomes more negative the further down you are in the gravity well.

There are a few reasons why one might do this.  Picking zero energy at infinite distance natural leads to all potentials being negative and it is the least arbitrary point for zero to be chosen, since the potential energy goes to minus infinity at the object and any given distance would be an arbitrary choice.",null,0,cdj2sav,1r21m7,askscience,new,5
MCMXCII,"As others have said, potential energy scales are arbitrary. However it's common to represent bound states with negative potential energies.",null,0,cdj13cv,1r21m7,askscience,new,2
LoyalSol,"As other's have mentioned,  it depends on what you define your reference energy as.   A positive value means you are above that reference energy and a negative value means you are below that.

",null,0,cdj68ch,1r21m7,askscience,new,2
euneirophrenia,You can make the surface of an object as complex as you'd like to make the surface area huge and the volume tiny. At the extreme you can have  fractal curve like the [Menger sponge](http://en.wikipedia.org/wiki/Menger_sponge) which has infinite surface area and zero volume (at infinite iterations),null,0,cdjripe,1r215u,askscience,new,2
patchgrabber,"Molecular Clock Hypothesis tries to estimate how far apart organisms are evolutionarily by means of using specific proteins. Some proteins, such as cytochrome c (present in almost all organisms) seem to have a fairly consistent time between neutral mutations, meaning that if most mutations are neutral (have no effect on fitness), and if they occur at more or less regular intervals, you can estimate how many new mutations you should see in a generation. 

Thus, by measuring the number of mutations in that protein from the time when two now distinct species had the same or very similar versions of these proteins, one can theoretically estimate the time these species diverged. There are several limitations of this process, like fossil prevalence, generation time and metabolic rate, among others. So while it may not be a perfect process, it's not without its uses.",null,73,cdiruch,1r1z4w,askscience,new,382
oliverisyourdaddy,"I'm an evolutionary anthropologist!

They compared the genomes of humans and chimps, estimating the total number of divergences (changes).  Then they calculated the average number of mutations (changes) in one generation (by comparing the genes of parents and children).  

Then they performed the following calculation: 
[(Number of total divergences)/2]/(mutations per generation)
to determine how many generations have passed since the divergence of humans and chimps.  (They divide the total number by two because the divergences represent changes accumulated in both the chimp genome AND the human genome, whereas you want the number of generations for just one species, since they're happening simultaneously.)

Now that they have the number of generations, they convert that to a time by multiplying that number by the average generation time - that is, the age at which a parent has a child (the average child, not first or last). 

So basically, find out how different the genomes are, find out how many mutations happen per generation, and calculate how many generations have passed.  Then multiply by the number of years a generation is.

Finally, they corroborate it with fossil evidence.  We can date fossils using isotope dating, so if we have fossils for all the ""intermediate"" species dating back to a common ancestor for two species, we can get a good timeframe for their divergence.  The problem with fossil evidence is that it's actually very limited for non-human apes.  We have a good fossil record for the human lineage, but not for the chimp, gorilla, or orangutan lineage.  The next closest primate that has a really good fossil record is actually macaques (a type of monkey), so calculations are often checked against the macaque record.  For a long time, our ape calculations actually didn't jive so well with the macaque record.

Something interesting happened in 2012 (I could be misremembering the year).  Scholars named Scally and Durbin proposed that the calculations had all been incorrect because they had used generation time for *current apes*.  Larger animals tend to have larger generation times (bigger animals have kids later, take longer to mature), and extant modern apes are generally larger than their ancestors.  Therefore the ""generation time"" variable was decreased a little, and these guys' new calculations fit better with the macaque evidence. 

Edit: wording",null,14,cdiub9d,1r1z4w,askscience,new,82
skadefryd,"I'm gonna stick out like a sore thumb in this thread, because I have a very different picture of how these estimates are obtained. I look forward to being proven wrong, though, because there are some people in this thread who know a lot more about the subject than I do.

My understanding has always been that divergence time estimates are generally obtained based on fossil calibration. These estimates are then compared to the number of (purportedly neutral) substitutions to obtain a neutral substitution rate and hence mutation rate, *not the other way around*. Measuring mutation rates in vivo is really hard, and we've only just recently been able to do it with any degree of precision, and a variety of factors can cause it not to agree precisely with mutation rates estimated phylogenetically (though they typically agree to within fifty per cent or so).

Any of the above might be completely wrong. Maybe /u/patchgrabber or /u/jjberg2 can set me right.",null,3,cdj46ol,1r1z4w,askscience,new,12
null,null,null,0,cdiyvny,1r1z4w,askscience,new,3
nedved777,"How do we figure out the ""number of substitutions per base pair per generation for a given piece of DNA?""  Is this something we can find using, for example, only two or three generations of chimpanzee DNA, or is it something requiring us to count the number of substitutions over thousands of years in a fossil sample whose age was determined by another method (e.g. radioactive dating)?

Since we are talking about specific proteins, some of which (cytochrome c) are present in almost all organisms, is there any reason we can't monitor rate of mutations per generation in some species of bacteria or something to find the rate?",null,1,cdiu46g,1r1z4w,askscience,new,2
null,null,null,1,cdiytjz,1r1z4w,askscience,new,2
null,null,null,13,cdirtxz,1r1z4w,askscience,new,4
YoYoDingDongYo,"Here's one: http://link.springer.com/article/10.1007/BF01055264#page-1

Here's another: http://www.ncbi.nlm.nih.gov/pubmed/1175391",null,0,cdir0dx,1r1xxt,askscience,new,8
My_Nipples_AreOnFire,"Your question actually touches on a great deal of pharmacological principles. Classical pharmacology assumes that a drug's therapeutic/toxic effects are directly related to concentration achieved at the site of action (ie. the tissue/cells that the drugs are effecting). This concentration is directly correlated to the concentration achieved in the blood, since this is essentially how much drug will be distributed to the site of action. This assumption leads to the conclusion that taking more of a drug will ellicit a greater effect:

* Greater drug concentration administered -&gt; greater blood concentration achieved -&gt; greater distribution to site of action -&gt; stronger elicited effect

This principle has essentially dictated pharmacology for a long time and has had good results.

However, I think this is what others are getting at, this does not mean that greater dose necessarily means more **therapeutic** effect. This simply means greater dose produces greater effect. After a point, this effect leads to a **toxic** effect rather than a therapeutic one. As you increase concentration you will reach a point where 100% of respondents are experiencing the drug effect, but you enter a concentration where toxic effects are produced. So to answer your question, classical pharmacology (actually pharmacokinetics, to be specific) would say, in terms of effect, no. Greater concentration of drug will elicit greater effect up to a point, but this does not necessarily equal the desired therapeutic effect. Just the drug's chemical effect.

This is the distinction that must be made. Because some drugs (such as those listed by /u/YoYoDingDongYo) will have some idea therapeutic dose, but prescribing a dose greater than that could lead to a greater *drug* effect that inhibits it's *therapeutic* effect.",null,2,cdj4v6k,1r1xxt,askscience,new,4
mrdeath5493,"So basically I think you would want to look into dose-response curves.  [This one](http://www.softchalk.com/lessonchallenge09/lesson/Pharmacology/dose_response.png), for morphine, shows that at some point increasing the dose  no longer increases its analgesic effects (pain relief).  But if you keep going, you start getting way way too much respiratory depression.  So if you were giving a dose that was too high for a particular patient, then you could decrease the dose and it would be just as effective at pain relief but with very little of the undesirable effect.  In a way, a decrease in this manner is making better use of the drug.  even though I don't think this is what you were looking for, it applies to most drugs and is a very good example to start with.

From here it is important to note that it is all relative.  Drugs have many effects, but are mostly used for a single effect.  I'll try to focus on the single therapeutic uses instead of the side effects.  In trying to answer your question, I found myself at a theoretical conundrum.  The smallest dose of a drug would be 1 individual particle.  Is there a drug out there that has its greatest effect at 1 particle and then decreases in effect as you increase the dose.  I wasn't sure that was possible, [however /u/YoYoDingDongYo provided an excellent example](http://www.reddit.com/r/askscience/comments/1r1xxt/chemistrypharmacology_are_there_any_drugs_that/cdir0dx) where a drug was more effective at *1 part per billion* than at higher doses which actually kinda blew my mind.

Why is this?...The following is educated guessing:  Well, the drug in that study exerts an effect a 1ppb, and at some point past that it must breech a threshold where it activates another biological process that works against the desired effect.  Either a detection mechanism is triggered or as it becomes more concentrated the cell might start to actively eliminate it.  I would like to see studies to see if there was an induction of elimination and if it was permanent.  Also, it might be helpful to look at 1,2,3,4, etc. ppb to see if there might actually be a threshold and perhaps an increasing effect on its way to 1,000 ppb.",null,2,cdiwwtj,1r1xxt,askscience,new,3
ModernTarantula,"Here are a couple of concepts: In immunology prophylaxis is the action of vaccine wherein the first exposure has a small immune response but subsequent exposure has a magnified immune response. Anaphylaxis wherein prior exposure leads to a magnified detrimental immune effect. The last of these is not from immunology--tachyphylaxis, repeated exposure depletes the response until absent. This is seen amongst tweekers who use meth until they deplete all neurotransmitters in their brains.",null,0,cdjhidk,1r1xxt,askscience,new,1
eclarep,"One example of a drug that has the effect you mention is Trazdone, an antidepressant that is much more commonly prescribed for sleep problems.
At a lower dose, much too low for antidepressant effects, trazdone has sedative effect but it takes a much higher dose to have antidepressant properties.
So, as a prescription sleep aid, the effect is dependent on the dose in a somewhat inverse manner. Above a certain threshold and up to a limit, it causes sedation, outside that range it can help with depression.
This is not medical advice, just a case that is an example of a drug of the kind you asked about.
[source](http://www.medscape.org/viewarticle/508820)",null,0,cdkulfh,1r1xxt,askscience,new,1
ringboard,"If something causes a increase in one effect, then it is by default causing a decrease in another effect. For example the Non-steroidal anti-inflammatory drug, COX-2 inhibitor, will decrease the activity of Cox-2, but by doing this it is increasing anti-inflammation effects. So higher doses means less Cox-2 activity but more anti-inflammatory activity.",null,4,cdiu26c,1r1xxt,askscience,new,2
Rangi42,"First of all, inventing a way to describe how stuff works in terms of atoms, gravity, spacetime, or other such models requires the same kind of creativity as inventing a way to depict your senses using [geometric shapes](https://en.wikipedia.org/wiki/Nude_Descending_a_Staircase,_No._2) or [colored points](https://en.wikipedia.org/wiki/Pointillism), if not more. To quote Voltaire, ""There is far more imagination in the head of [Archimedes](https://en.wikipedia.org/wiki/Archimedes%27_principle) than in that of [Homer](https://en.wikipedia.org/wiki/Iliad)."" The difference is, artists use their intuitive inspiration to fuel their works, whereas scientists have to define [their theories](https://en.wikipedia.org/wiki/Atomic_theory) in concrete enough terms that they can be understood and tested by others.

You bring up the example of atomic theory. Democritus somehow had the inspiration of matter being made up of atoms 2400 years ago, but at the time his idea remained just an interesting concept. [John Dalton's theory](https://en.wikipedia.org/wiki/Dalton%27s_atomic_theory#Atomic_theory) in 1800 was more than just a neat idea: he took the time to deduce what consequences would follow from the initial idea, such as different types of atoms having particular weights, and used his theory to [explain his observations](https://en.wikipedia.org/wiki/Law_of_multiple_proportions).

The thing is, 100% certainty is impossible. Alternative theories can always account for the observed evidence if you twist them enough. The geocentric model of the solar system was bolstered by adding more and more [epicycles](https://en.wikipedia.org/wiki/Epicycle) to fit the planets' observed orbits, and even though the heliocentric model is much simpler, the geocentric one isn't actually ruled out.

A rule of thumb to decide between two models is [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), which basically says that you should prefer the simpler theory as long as it explains the same data. Before Einstein came up with his theory of relativity, Newton's law of universal gravitation seemed like a good enough model for how mass moves through space. The [precession of Mercury's orbit](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury) could be dismissed as observational error, or a result of some unknown celestial body influencing it, or other explanations. When relativity's predictions not only matched Mercury's observed orbit more precisely, but also [explained other phenomena](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Deflection_of_light_by_the_Sun), Occam's razor would suggest adopting relativity over Newtonian gravitation.

As for how scientists gather data to test their theories and invent new ones: it's hard. Measuring [tiny](https://en.wikipedia.org/wiki/Quark), or [huge](https://en.wikipedia.org/wiki/Supercluster), or [distant](https://en.wikipedia.org/wiki/Pulsar), or other extreme things requires technological breakthroughs, which rely on the same sort of insight as artistic or scientific breakthroughs. There are [theories](https://en.wikipedia.org/wiki/Insight#Theories) about how people come up with new ideas, and attempts to [program computers](https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms) with some of our abilities, but we're nowhere near understanding or reproducing it yet. One technique is to predict the side-effects something will have and look for those instead. Black holes, for instance, can't be observed directly, but [the way they bend starlight](https://en.wikipedia.org/wiki/Gravitational_lensing) can be predicted, and telescopes check if the actual sky matches the predictions.",null,27,cdir7gf,1r1xxj,askscience,new,161
__Pers,"Atomic theory is one of the most well established and experimentally validated theories in science. Are we 100% sure of anything in science? No, but in this case, we're very confident we have the essential theory correct. 

Like the rest of science, it came about through application of the scientific method: proposing hypotheses, testing said hypotheses, and refining one's models as needed.

As astonishing as it may seem, while atoms are indeed small, [we've even been able to trap and study individual atoms in isolation](http://www.opticsinfobase.org/ol/abstract.cfm?uri=ol-35-13-2164). ",null,7,cdir6fh,1r1xxj,askscience,new,41
yeast_problem,"I think the easiest way is to learn the history of the discoveries. The greeks believed that elements could be divided into smaller and smaller pieces until an ""atom"" remained, but they didnt even know about electrons or the periodic table.

Try looking at the [Thomson Model](http://en.wikipedia.org/wiki/Plum_pudding_model) to get a feel for some of the intermediate stages of discovery.

You can calculate the mass of charged particles by firing them through a magnetic field and working out the force the magnetic field applied from the radius of the curve they make.",null,0,cdirad3,1r1xxj,askscience,new,5
RainmakerUK,"It is the best model we have -right now- to describe our observations of matter. 

In the world of Science, we are never sure. There is no ""sure"" in Science. We only have best guesses, which are replaced by better guesses over time, often as a result of observations which can't be explained by our current model. 

Always remember, ""the map is not the territory"". Our equations and models are simply our best descriptors of reality. They are a functional tool. They are not the roadmarkers of reality itself. ",null,1,cdiyswc,1r1xxj,askscience,new,6
HoopyHobo,"CERN is 99.999999999% percent sure that they have found the Higgs Boson, and that absolutely pales in comparison to how certain we are about protons, electrons and neutrons. Scientists will probably always tell you (correctly) that they can never be 100% certain about anything, but the infinitesimal percentage that isn't accounted for shouldn't really matter to anyone, especially those of us who aren't physicists.",null,1,cdj6vr7,1r1xxj,askscience,new,5
TurboCommander,"From Albert Einstein and Leopold Infeld, in their co-written book “The Evolution of Physics.” 

“In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears the ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations.”

“He will never be able to compare his picture with the real mechanism and he cannot even imagine the possibility or meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of sensuous impressions.”",null,2,cdiyefe,1r1xxj,askscience,new,4
TheGoodMachine,"There is no such thing as “100% sure”. All we know is that until now all our observations agreed on that, and not a single observation disagreed, and hence it is ***useful*** (in predicting the world), which is exactly all that matters. And we did a shitload of observation (usually as a result of experiments).

So while this is a valid layman question, it’s an invalid type of question if you really think about it. A very common one though, which we all asked at least once in our lives.

Hence there will be no answer that will make you happy. You have to change your question. :)",null,0,cdj00dm,1r1xxj,askscience,new,2
Hypothesis_Null,"Yes we are 100% certain.

We are not 100% certain what makes up protons, neutrons, and electrons, and even less certain of what makes up what we think makes them up... but we are not wrong that things we call protons neutrons and electrons exist and act approximately as we've modeled them.

There is too much evidence and consistency in the model we have.  The model cannot be proven 'wrong' at this point.  It can be proven to be a constrained version of a more general model.  We could discover them doing something we didn't notice before if we put them under extreme conditions we havn't observed them under before.  But that will only result in a deeper understanding of them, not a different one.  Any new rules we learn will have the old rules as emergent approximations when put under the more common conditions we're used to. 

The common analogy is Newtonian Physics.  Newton wasn't wrong.  It's just when you things get extremely massive and/or fast, or are massless, the equations no longer accurately describes their behavior.  But if you use the more general equations Einstein and others have developed since, the equations reduce back to the Newtonian approximations when used on everything from large molecules to stars going slower than 1% lightspeed.

**TL;DR** We are certain, because they have been observed behaving as we believe them to behave too consistently and too much.  Anything at this point will not disprove what we know, but simply shrink the error bars on what we do not.",null,0,cdj5r5l,1r1xxj,askscience,new,2
dracho,"On almost any level, yes, we are sure.  

On the other hand, nothing is ever 100%.  Nothing.  Sorry to get philosophical, but there's always an exception to the rule.  Different universe?  Inside a black hole?  Dimension X?  Who knows?  Nobody, with 100% certainty.",null,3,cdiwu3h,1r1xxj,askscience,new,4
CoolStoryJohn,"The atomic model can be viewed as just that: a model.  We've had scientific models in the past that have been outdated/scrapped, and  inherently, any legitimate scientific model will be open to the same treatment given sufficient evidence.  That being said, the atomic model has been incredibly successful in both explaining behavior and predicting it.  At its base, the atomic model is based on induction (i.e. experimental evidence and generalization of specific results).  We've used the atomic model to explain the results of various experiments, and used those results/explanations to predict future cases (which have largely been accurate).  So then, up until this point: so far so good.  ",null,2,cdixr0j,1r1xxj,askscience,new,3
coniform,"&gt; Are we 100 % sure that the matter is made of atoms , the atoms made of protons, electrons and neutrons ect... ?

Yes. There is overwhelming, irrefutable evidence supporting the existence of atoms, and the particular structure of atoms.

&gt; how did we even deduce such things?

Here is a very clever and historically groundbreaking approach: [brownian motion](http://en.wikipedia.org/wiki/Brownian_motion#Einstein.27s_theory).

You can see brownian motion yourself...get a cup of water and watch very closely that the particles will move very erratically.

&gt; And how could we calculate the number of electrons of each element? And most of all , how could we calculate each of these particles' mass if they are so small ?

This has to do with how we figured out how the elements were [ordered in the periodic table](http://www.rsc.org/education/teachers/resources/periodictable/pre16/order/atomicnumber.htm).

All of these problems were studied just over a century ago. Isn't it amazing? This was barely after the discovery of the electron, and before the development of quantum mechanics. All of these very basic ideas foreshadowed an entire century of amazing scientific ideas.",null,1,cdjbl8k,1r1xxj,askscience,new,2
logical,"The atomic theory of matter is a proven theory.  We are certain of it.  We did not deduce it, but we induced it.  

This theory, like all proven scientific theories, start with observations of reality and arrive at generalizations by integrating them with other observations.  Contradictions of observations in controlled experiments rule out possible explanations and confirmations lead to further validation.  

It is important to note that additional knowledge can further refine a theory, but that in the context of what was known the original theory is not incorrect, but limited.  

If the atomic theory of matter was false, atom bombs and nuclear reactors wouldn't work, the periodic table of elements would not be such an accurate predictor of what elements exists and what their properties were, molecular science wouldn't work (which is what is behind most of the chemicals in your life, from cleaning agents to medicines) and so much of what you depend on from science would simply have to obey different laws and possess different properties.  

You can find a history of the timeline of the discovery of the atomic theory of matter [here](http://atomictimeline.net/index.php).

There are some good courses available that summarize the history of this theory if you look for them.",null,0,cdjcfij,1r1xxj,askscience,new,1
HighPriestofShiloh,"No.  Only 99.999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999%",null,2,cdjdtst,1r1xxj,askscience,new,3
dopsi,"Actually we can't be sure that these elements even exist, this is a model (which means an approximation) of the universe, build from our knowledge. For now this is the most accurate model we have, but it has its limits, like every model.

If you have a look back, the Greek astronomers believed the Earth was in the middle of the universe and the Sun and planets turned around it in circle. Their model gave them accurate enough results for their purpose and they didn't need another model. We know it isn't the case, but this was discovered when scientists got new and more accurate methods of measuring the world, that's why the current model could be seen as senseless is several centuries.

That's why we can't say we're 100% sure, because it's a model.",null,9,cditw52,1r1xxj,askscience,new,8
ChocoThornton,"I recently graduated with a degree in the History and Philosophy of Science, and this is a popular [realism/anti-realism debate](http://plato.stanford.edu/entries/realism/). Are scientific theories what actually happens, or are they just tools used to understand the Universe? It's a vast philosophical concept with no answer I'm afraid!",null,6,cdiw8bc,1r1xxj,askscience,new,6
miczajkj,"I can choose an coordinate system, where the Origin and the trajectory of the object lie in the same plane and make this plane the xy-plane. 
The origin is now (0,0). Now if the particle is moving with constant velocity v (constant velocity also means always the same direction) I can choose t=0 in a way, that d(0) = (d_0, 0) and v(0) = (0, v). Than the position of the object at an arbitrary t is d(t) = (d_0, vt). 

This is only an coordinate displacement, so the formula is completely non-relativistic. The relativistic effects only arise in the objects reference frame.

Okay, now let's have a look on what we think about the particles position. At the time t we see the particle at the position b(t). As the light from the particle has taken the time t' = |b(t)|/c this is equals:

b(t) = d(t-t') = d(t-|b(t)|/c)

So now we have an implicit equation to calculate b(t) from a given d(t)!
Let's substitute it into our equation. Notice, that the x-coordinate doesn't change (it's always d_0) so we only need to investigate the y-coordinate: 

y = v*(t-sqrt(d_0² + y²)/c)

(always read y as y(t).)

We can easily solve this equation vor y: 

First we introduce ß = v/c and write the equation as

y - ßct = -ß sqrt(d_0² + y²)  
y² - 2 ßct y + ß²c²t² = ß² (d_0² + y²)
y²*(1-ß²) - 2ßct y + ß² (c²t² - d_0²) = 0  
y² - 2 ß/(1-ß²) ct y + ß²/(1-ß²) (c²t² - d_0²) = 0  

y = ß/(1-ß²) ct +- sqrt(ß²/(1-ß²) c²t² - ß²/(1-ß²) (c²t² - d_0²))  
y = ß/(1-ß²) (ct +- sqrt(c²t² - (1-ß²)(c²t²-d_0²))  
y = ß/(1-ß²) (ct +- sqrt( ß²c²t² + d_0²(1-ß²))

We can now think about what y(0) should be: because the particle is moving towards positive y-values, y(0) should be negative - it is a position the particle has been before t=0. So we need to take the negative root. (The other root belongs to the so called advanced particle [in comparison to the retarded particle] that is no physical solution to this situation). If we choose to substitute ß=v/c again, we get: 

y(t) = 1/(1-v²/c²) (vt - v/c sqrt( v²t² + d_0²(1-v²/c²))

Now the current position of the particle is (d_0, vt) = (u, w) so the observed position as a function of current position and the velocity is

(x(u, w, v), y(u, w, v)) = (u, 1/(1-v²/c²) (w - v/c sqrt(w² + u²(1-v²/c²)))

What can be expressed more nicely by using ß=v/c and  γ = 1/sqrt(1-v²/c²) as

(x, y) = (u, γ²(w-ß sqrt(w²+u²/γ²)))

To find the solution in other coordinate systems you can now make simple coordinate transformations. (Note that it get's more complicated, as v -&gt; v(cos ϕ, sin ϕ) because you need to know the direction too.)

It's a funny thing, that all those relativistic factors show up, even if we didn't choose to use any features of special relativity. 

[Can you think of the reason?](#s ""It's because the whole formalism of relativity is inspired from the asumption of retarded potentials. It needs to reproduce the feature of delay between the emission of a signal and the measurement of this signal at another position. Therefore these factors are inherent if you choose to consider finite speed of information."")
",null,0,cdiznjj,1r1xvr,askscience,new,3
shamdalar,"As stated, your problem seems to be overdetermined. Let's leave light out of it, and assume we get a message traveling at speed c in a straight line from the object traveling at velocity v in a straight line, but we don't know the direction it came from. We know the current position and distance d from the object. If the angle at A is theta, the angle at our position is arcsin(sin(theta)*(v/c)) by the law of sines, and we can use law of sines again to find the distance from A to B.",null,1,cdixxsq,1r1xvr,askscience,new,2
hikaruzero,"&gt;What is the formula for finding this observed location given the current location and velocity of the object?

It's the simple formula for displacement with a constant velocity.

d = d*_0_* + vt

You should remember this formula from high school physics class.  It was pretty much the easiest one.  :)

&gt;Since it's for a little relativity game, it would be nice if the formula/algorithm were expressed in matrix/vector manipulation (eg cross product, dot product, matrix multiplication etc) and simple operators such (eg multiplication, addition subtraction, square root, etc.)

Just copy the formula above three times (one for each spatial axis) and put a subscript labelling the axis on every variable except t.  Then put them vertically in square brackets.  The only operation is simple vector addition -- you can add the components independently of eachother.  You can also combine them into a single magnitude of the vector with the Pythagorean theorem, which you should have learned in geometry class in jr. high school:

d = √(d*_x_*^(2) + d*_y_*^(2) + d*_z_*^(2))

Not sure what that has to do with relativity though.  Indeed it really doesn't have anything to do with relativity -- the relativistic formula is identical, it's just that the variables take on different values in different reference frames, and those variables are related to eachother in different frames by way of a [Lorentz transformation](http://en.wikipedia.org/wiki/Lorentz_transformation).

&gt;Trigonometric functions such as sine, arcsin cosine are tolerable but not so good. Solutions that require calculus probably can't be used for performance reasons alas.

You don't need any of this -- the problem's not *that* hard.",null,1,cdivtu1,1r1xvr,askscience,new,1
xenneract,"Odor depends on the release of small volatile molecules known as odorants. Inside orange peels is a small organic molecule known as [limonene](https://en.wikipedia.org/wiki/Limonene), which is responsible for the smell. Limonene has a vapor pressure of about 1 mmHg at room temperature, which means that it will exist almost entirely in the vapor phase. So as soon as you peel your orange, all of the limonene in the peel rapidly vaporizes and diffuses through the room.",null,0,cdivrnt,1r1xaf,askscience,new,8
sporclesam,"Even without forward facing vision, animals such as Horses &amp; other herbivores (majorly [Monocular Vision] (http://en.wikipedia.org/wiki/Monocular_vision)) have great field of vision but poor depth of field (Look up [Stereopsis](http://en.wikipedia.org/wiki/Stereopsis) for more on DoF). 
Other links: [1] (http://en.wikipedia.org/wiki/Binocular_vision), [2 (sorry for the garish blue!)] (http://www.artinarch.com/vp05.html)

SO navigation is no issue. Its interesting how there is a distinct link when it comes to predator-prey relations (with predators more frontal while prey being more lateral in their vision range).
",null,0,cdiy4hq,1r1wof,askscience,new,1
MarineLife42,"Not a dumb question at all.  
Rule of thumb: If you can see the eyes looking at the front of the animal, it can see you. So yes, they can see forwards but have poor depth perception. Also, there is usually a small area in front of their heads where they cannot see. Further forward, the angles of view overlap.  
Animals like this don't necessarily use their eyes for navigation, though they certainly use them to look for obstacles close to them. Primarily their eyes are there to warn them of predator's movements, while navigation is often done based on smell and sound.  
So a horse might wander to a farther place that smells good, while its eyes tell it about objects such as trees or the position of other horses in the immediate vicinity. ",null,0,cdj74p7,1r1wof,askscience,new,1
skorps,"With our eyes, an animal can not see. But vision is not a requirement for navigation. There are many other examples of ways animals navigate. Bats have echolocation in the dark, many bugs use smells, and many animals can detect vibrations. Some  environments do not require eyes (too dark, undergroubd) other animals just can invest the energy in evolving eyes when other less expensive methods work ",null,3,cdiramq,1r1wof,askscience,new,2
ddubspecial,"With today's understanding of the standard model, we have no real answer if what gives particles charge or spin. Before Higgs, we didn't have a definite answer for mass either.  
There are theories that attempt to explain these fundamental properties. In string theory, tiny 1 dimensional strings of energy vibrate around in 10 dimensions if space. It is thought that the shape of these dimensions and the way the strings vibrate through them gives a particle it's properties.  Just a theory though. It's all math at this point. ",null,3,cdistj9,1r1uhx,askscience,new,15
fishify,"Charges arise in physics due to the ways particles and fields are modified by certain transformations known as symmetry transformations.  Things that transform differently will have different charges.

In our understanding of physics today, one of the fundamental elements of a physical theory is the set of transformations that leave the equations of motion unchanged.  Such transformations are called the symmetries of the theory, and each particle and field will transform in some specific way under such a transformation.  (By the way, particles and antiparticles differ with respect to a variety of charges, not just electric charge.)  Antiparticles and particles will transform in opposite ways under a given symmetry transformation associated with their charge.

Note that your idea that there is some constituent that carries one or another charge to distinguish particles and antiparticles doesn't really change anything; why should *those* two constituents have opposite charges?  ",null,0,cdixkwe,1r1uhx,askscience,new,6
zeug,"Antiparticles are predicted by the Dirac equation, which was an attempt to get a quantum mechanical equation for the electron which satisfied special relativity and accounted for the two spin states for the electron.

Dirac was able to do this by writing down an equation similar to the wave equation, but that does not have any solution for real or complex numbers. The trick was to reinterpret it as a 4x4 matrix equation operating on 4 component vectors.

The first two components naturally appeared to be the two spin states of the electron, and the last two were also spin up and down, but had a difference in sign that caused quite a bit of a puzzle. One consequence of this change is sign is that these last two components would correspond to particles of opposite charge when electromagnetic interactions are added.

So basically, if you want to satisfy what is observed about quantum mechanics and special relativity, and write down an equation to describe a particle with two spin states, such as the electron, the simplest way to go is the Dirac equation, which predicts antimatter components.


So you don't need charge or an electromagnetic field to have antimatter, it is simply there. The neutrinos have no electric charge, and there are corresponding antineutrinos also without charge. 

But it is true that if you add a term to the equation that gives the particles an interaction with the electromagnetic field, the antiparticles have to have an equal and opposite charge to the particles.

",null,0,cditj9k,1r1uhx,askscience,new,5
iorgfeflkd,"No, charge is just an inherent property of those particles.",null,4,cdirxwh,1r1uhx,askscience,new,5
endocytosis,"Probably not.  It depends on how shortly after death (seconds, minutes, hours, days), and also on the recoverability of eggs and sperm.  Generally sperm need the semen that is produced in the ejaculatory ducts and seminiferous vesicles to remain viable and healthy, but it is [possible](http://www.advancedfertility.com/testicsperm.htm) to collect sperm with a syringe by literally (gently) draining off the top layer of sperm that collects in the epididymis with a needle.  This would need to be done very quickly if a person died.   
Eggs can be collected from the follicles [directly](http://www.genea.com.au/Library/Fertility-Treatments/Assisted-conception/IVF), but usually the woman is given the hormone FSH first to maximize the chances that the follicles will produce an egg that can be collected.  If this has not occurred, the chances would be negligible, because with at most minutes to spare, *and* the hope that the woman happened to be ovulating at the time, the surgeon would have to find the follicle that was about to release an egg, and then successfully harvest it, which is not a simple procedure.

TL;DR Since cells in the body start to die within minutes of no oxygen, and the timing would have to be exactly correct to collect eggs with no prior preparation, eggs and sperm would not be viable after death.",null,0,cditqux,1r1r5j,askscience,new,3
fladam,"I believe that 'funny feeling' you are referring to is actually the effect of inertia. 
Things which are moving in a certain trajectory tend to 'want' to continue moving in that same trajectory, or things which remain at rest tend to 'want' to remain at rest.. 

Let me explain, imagine being on a roller coaster which is travelling along a straight horizontal path, and then suddenly the path shoots up at some angle, say 45 degrees for arguments sake, then inertia is that force which pushes you down into your seat. In the same way, if you were to feel a sudden drop on the roller coaster, you feel yourself coming out of your seat and you get that butterfly feeling. So if you were at rest in zero-gravity and you were to propel yourself downward then your body will feel a force in the direction opposite to the direction you propel yourself due to inertia which is independant to gravity in this case. This is because the force will be provided by your own propulsion. ",null,0,cdiv1w5,1r1qqi,askscience,new,2
ignotos,"The predicted trajectory already takes the sun into account - it's already feeling the gravitational effect of the sun - getting closer to it doesn't really change this.

Also, the mass of the comet doesn't make much difference - it is still inconsequentially small/low-mass compared to the object it is orbiting, and often the mass of the orbiting object isn't even considered in cases like these.",null,0,cdjrz4b,1r1q27,askscience,new,1
Jobediah,"Problems with a whale farm range from being extremely impractical, to being biologically unrealistic, to not knowing what these animals need to survive to being uneconomical.

Impractical- You'd need to cordon off a huge area that included all the resources whales need.

Unrealistic- Many whales (most? all?) range widely, some thousands of miles from their wintering and birthing grounds to their feeding grounds.

Mysterious- We don't know all the things whales do or need and this would make it likely the experiment failed.

Uneconomical- yeah, this would fail because whales generally take a long time to mature and reproduce. Farming generally requires fast reproduction and growth to be sustainable. And see other ways this would fail above. 

These are all general problems. There might be individual species for which some of these issues could be avoided but I high doubt there are any for which all of these problems could be solved.",null,0,cdirg85,1r1o8p,askscience,new,9
atomfullerene,"Arthur C. Clarke one wrote a story about herding whales [The Deep Range](http://books.google.com/books?id=-fYpAAAAQBAJ&amp;lpg=PT152&amp;ots=xV-sRgT5v6&amp;dq=arthur%20c%20clarke%20whale%20herding&amp;pg=PT152#v=onepage&amp;q=arthur%20c%20clarke%20whale%20herding&amp;f=false)

It's not likely it will ever be done, but if you were going to domesticate whales you'd pretty much have to herd them rather than try and pen them up somewhere.  Guide pods of females around the antarctic ocean between zones of rich plankton, protect them from predators, harvest most of the offspring every year.  It would still be quite impractical, though.  The expense of just keeping ships out there to keep an eye on them....

",null,0,cdj31w8,1r1o8p,askscience,new,2
null,null,moderator,1,cdioent,1r1o8p,askscience,new,2
marley88,"There are plans for this. So yes, it is theoretically possible.
[Here.](http://www.theguardian.com/world/2002/jan/14/highereducation.research)",null,2,cdiqrkh,1r1o8p,askscience,new,2
desig_nate,"There's actually very little contribution of current oil prices to current gas prices, it's about a four-to-eight week lag, depending on who you ask. Much of the market pricing is dominated by futures prices (i.e. price speculation) on various commodity exchanges due to hedging or pure speculation.

[This](http://money.howstuffworks.com/oil-speculation-raise-gas-price.htm) article is a good primer on speculation. [This one](http://useconomy.about.com/od/supply/p/oil_gas_prices.htm) talks a little more about the supply-and-demand aspect of it.

Unfortunately it's hard to point to one or two direct determinants of gas prices. If you look nationwide, much of it is determined by access to oil and gas pipelines (check out [this map](http://www.eia.gov/pub/oil_gas/natural_gas/analysis_publications/ngpipeline/images/ngpipelines_map.jpg)). In local markets, pricing strategies are more dominant, since everyone has roughly the same access to pipelines.

Hope that helps somewhat.

EDIT: Wording.",null,0,cdj1zck,1r1n6h,askscience,new,2
SqueakyGate,"Yes, humans have always been omnivores. 

The hominin line and the pan line diverged about 6 million years ago from a common ancestor. The hominin line includes humans and all of our extinct relatives, like the neanderthals. The pan line includes the living chimpanzee and bonobo as well as all of their extinct relatives. This common ancestor was not a chimpanzee/bonobo and it was not a human. It was its own ape species. Based on some commonalities in both human and chimpanzee/bonobo diets (as well as inferences from other ape diets) we can surmize that this last common ancestor was also an omnivore. Chimpanzees and bonobos hunt for raw meat and it makes up a small portion of their diet. The rest of their diet consists of raw plant material, like fruits, leaves, roots etc.

There are a few major dietary milestones in the hominin lineage. 

1. The very early hominins, like [ardi](http://en.wikipedia.org/wiki/Ardi) are not associated with stone tools and they are not associated with fire. From these two observations we suppose that Ardi's diet was a lot like a modern chimps. Mostly plants and some scavenged meat (birds eggs, recently dead animals etc.).

2. Around 2.6 million years ago we see the first evidence of [stone tool use](http://en.wikipedia.org/wiki/Stone_tool) this may not have changed the way [australopithecines](http://en.wikipedia.org/wiki/Australopithecine) hunted for meat, they were probably still mostly scavengers. Australopithecine species lived from about 2-4 million years ago. 

3. With [H. erectus](http://en.wikipedia.org/wiki/Homo_erectus) we begin to see the first evidence of the control of fire. First we see changes in the morphology and dentition of H. erectus which some authors (listed below) link to a change in the diet. Cooked food allows an individual to have a higher caloric gain than raw food because the cooking process essentially pre-digests the food for us. These authors among others postulate that the cooked food diet is what finally enabled brains to grow larger. Brains are an energetically expensive organ and so is digestion. A constricting caloric intake meant that the brain could not grow too big even if their were environmental or social pressures for it to do so. By increasing their caloric intake by converting to a cooked diet this released the constraints on brain size and so it began to grow. So a cooked diet explains *how* brain size grew, but not *why*. Physical evidence for fire in the forth of hearths etc. dates back about 750,000 - 300,000 years. Thus ancestral Homo species were cooking food for a lot longer than humans have ever been around. However, we still don't know how pervasive this cooked food diet was, did all species living at this time know how to control fire? did all populations within the species? In any case some of them did - so their diet changed from raw to cooked, but not in composition. It was still mostly plants (fruits, nuts, tubers...) and some meat. They also had refined hunting techniques as tool cultures also became more complex. However, they were probably hunting small game (e.g. rabbits) rather than large game. I'll get into why this is probably the case if you like.

4. Humans arrive on the scene about 200,000 years ago. At this point Neanderthals are living in Europe and H. erectus has managed to spread herself all the way to the far corners of Asia (China, Java, India...). These early humans had the same tool cultures as these other species and they very likely also new how to control fire. Humans migrated out of Africa about 100,000 years ago and then things really began to get interesting. Tool cultures improved, humans kept expanding into new environments...we were innovators. We exploited and adapted. We became hunters. Our tools began to become so sophisticated that we could hunt from a distance (e.g. throwing a spear). In contrast it appears as if Neanderthal tool technology was stagnant at the time, and their spears could not be thrown - they were close ""combat"" spears. This is important because a lot of the neanderthal bones we have are broken, they have fractures etc. Humans on the other hand have less of these and this is attributed to the idea that we could hunt at a distance lowering the risk of injury. That being said big game still probably didn't make up a large portion of our diet. Modern hunter-gather societies have diets that consist mostly of plants, and some meat which is typically communally shared. This meat is more often than not small game. The success of small game hunting is on the order of every 1-3 days, whereas large game hunting is more on the order of once a month. You cannot reliably feed a population meat with large game using these early hunting techniques. So those depictions of early humans or neanderthals taking down a mammoth or a lion are very very unlikely. Try a fuzzy little rabbit, or collecting eggs from a nest.

5. 10-12,000 years ago humans begin domesticating plants and animals. Almost everything you find in the grocery today is a domesticated plant or animal, many of which date back to these first domestication events. Peppers, cauliflower, broccoli, corn, potato, carrot, wheat, barley, oats, rice, quinoa, lemons, grapefruit, oranges, apples, pears, olives, plums, millet, spelt, sunflowers, beans... all domesticated plants. Some of whose wild counterparts you would not even recognize - [teosinte](http://nrm101-summer2010.community.uaf.edu/files/2010/07/corn-and-teosinte_h1.jpg) or [wild potato](http://www.ars-grin.gov/nr6/bapotwso.jpg). So I often find those who argue that certain plants are not ""good"" for humans to eat because they have only recently been introduced to the diet (~10,000 years ago) often don't understand that the vast majority of our diet consists of plants and animals that never existed in their current form over 10,000 years ago. So humans around this time still had diets that consisted mostly of plants and some meat. However as populations began growing their own food and housing their own animals the types of things we ate changed. Moreover some populations began to acquire unique adaptations. For example, [lactase pesistence](http://en.wikipedia.org/wiki/Lactase_persistence) is a gene mutation which occurred about 10,000 years ago in certain populations. It allows an individual to continue to digest milk *naturally* into adulthood by keeping the production of the enzyme lactase turned on. Lactase digests lactose, the sugar which many people are intolerant too. Lactase persistence is more common in cultures/populations where dairying is also common. 

6. But what does this mean for our modern diets? Well as omnivores we can eat pretty much whatever we want (baring plant products which have a lot of cellulose, a sugar we cannot digest) without too much trouble. As long as we get all the nutrients we need we are fine. Today, many people live in places were access to food is easy. Moreover you can access a wide variety of foods from around the world that would otherwise be inaccessible. **We have the unique opportunity and privilege of living in a society and time where we can tailor our individual diets to reflect our preferences, our intolerances, our allergies and our moral or ethical obligations.** For example, being a vegan or vegetarian is not impossible but rather very much attainable in today's society because we have access to many different food resources which can make up the difference in terms of nutrients and caloric intake.",null,1,cdiu8do,1r1n2n,askscience,new,4
NAG3LT,"In part because it reflects light in the same way as white paper does - it scatters it. This scattering means that the reflected light is distributed over a larger area and thus is less intense. You usually don't feel blinded by a white matte paper in the sunlight either. 


Another important thing to mention is the fact that not every surface reflects all the light, but only some smaller amount of light gets reflect. Your mirror may reflect over 90% of light falling on it, but if it gets dirty the refelection will be dim. On average, Moon reflects just 1/10 of the light falling on it. However, Moon's surface has retro reflective properties, so its brightness at full Moon is more than twice the brightness at half Moon. Still, it never becomes a perfect reflector and the scattering of reflected light still lowers the intensity a lot. ",null,0,cdioqh4,1r1mf7,askscience,new,5
Madau,"Mutations involving albinism are related to pigmentation.  Any organism that produces a pigment can have a mutation in some part of the pathway of deployment, production, or structure that causes some form of ""albinism"" although I'm not saying all organisms can become white.

Fruit flies are a very well understood organism in genetics.  There are many variations of fruit flies that have different colors due to albino-like mutations.",null,0,cdivk5t,1r1m8u,askscience,new,2
ModernTarantula,First hydrogen peroxide is [not a good](http://www.webmd.com/a-to-z-guides/wound-care-10/slideshow-wound-care-dos-and-donts) for open wounds. Then is it good as a surface antiseptic? Probably [ok](http://apb.tbzmed.ac.ir/Portals/0/Archive/Vol2No1/8.Ghotaslou.pdf). Then does catalase help the bugs. Looks like it is [so.](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC301784/) not all aerobes make catalase,null,0,cdji36m,1r1lh6,askscience,new,3
Javi2639,"The idea is that these bacteria would break down the hydrogen peroxide with catalase into water and oxygen gas, which bubbles to the surface of the wound. These gas bubbles would also bring most of the bacteria to the surface, which then be washes away. The immune system can then take care of the smaller amount of bacteria left. There's actually a debate about whether or not this is effective, but that's the theory. ",null,0,cdj89vu,1r1lh6,askscience,new,1
lazlokovax,"When you say or hear a word, certain neural pathways related to that word's meaning fire in your brain. There is a self-inhibitory aspect to this activity: i.e. when they fire repeatedly, the firing becomes less intense with each repetition. You could think of it as the part of your brain that understands that word being overloaded, and so the volume is temporarily turned down.

It's called [semantic satiation](http://en.wikipedia.org/wiki/Semantic_satiation).",null,6,cdipge9,1r1lgt,askscience,new,28
syvelior,"It's called semantic satiation, and neuroimaging results suggest that it has to do with how people process semantics rather than sensory satiation / adaptation (Kounios, Kotz, &amp; Holcomb, 2000).

**References**

Kounios, J., Kotz, S. A., &amp; Holcomb, P. J. (2000). On the locus of the semantic satiation effect: Evidence from event-related brain potentials. *Memory &amp; Cognition*, 28(8), 1366-1377.",null,4,cdirb65,1r1lgt,askscience,new,13
albasri,"This question has been asked several times before. Here are a few of the previous posts:

http://www.reddit.com/r/askscience/comments/mwrzz/why_is_it_that_when_you_repeat_a_word_over_and/
http://www.reddit.com/r/askscience/comments/1i0x5z/can_the_effect_of_semantic_satiation_occur_in/
http://www.reddit.com/r/askscience/comments/scz2a/why_do_words_written_or_spoken_in_the_language/
http://www.reddit.com/r/askscience/comments/m9pji/why_do_words_become_meaningless_when_you_say_them/

Unfortunately, because of the high volume, a lot of really interesting questions and great answers can get buried. The searchbar can be a great way to explore /r/askscience, although it can be a bit difficult to get just the right question. I found the above links by searching for ""repeat word"" and ""say word again""

",null,1,cdizmb4,1r1lgt,askscience,new,7
arble,"Infinitesimal means smaller than any real value and yet nonzero. Saying that the difference between two values is infinitesimal is therefore meaningless because there is no real quantity that this difference could correspond to. For entropy change to be zero in a reversible process, the starting and ending temperatures must be the same. If they're the same (not infinitesimally close but actually the same), you can't extract any work from the engine no matter how long you run it.",null,2,cdirhbw,1r1kuj,askscience,new,6
Guanglais_disciple,"You mean the temperature difference between reservoir and heat exchanger is approaching zero, I'm sure (subtle but important distinction). Usually, a power cycle operating at Carnot efficiency does zero work, although I'm struggling to justify this. Maybe someone else can expand on my answer. 
",null,1,cdiuh75,1r1kuj,askscience,new,3
rocketgolfer,"The point of the Carnot cycle is not that there is no entropy increase, but that there is no wasted entropy increase. In the Carnot cycle, all energy involved in the temperature change from hot to cold is converted to work. In fact, peak efficiency is possible only with an infinitely large difference in temperature between the hot and cold resevoirs. If you go through a truly isentropic process, no net work will be done.",null,1,cdixb1v,1r1kuj,askscience,new,3
xxx_yyy,"&gt; ... the temperature differential between the hot and cold reservoir is infinitesimal ...

This not an accurate description of a Carnot cycle.  There is a finite temperature difference between the hot and cold reservoirs.  The working fluid cycles between the two temperatures in such a way that there is never any heat flow between finite temperature differences.

In an ideal Carnot cycle, the total entropy does not increase - it remains constant.  That's why Carnot cycles are reversible (can be operated in rather direction around the heat cycle).  One way gives you a heat engine, and the other gives you a refrigerator.

One needs to have a finite T(hot) - T(cold).  The efficiency of a Carnot heat engine increases as this temperature difference increases: 

Efficiency = [T(hot) - T(cold)] / T(hot)",null,0,cdjdm7b,1r1kuj,askscience,new,2
varodrig,"Not really, not in any significant way.   
Heat is the lowest/simplest form of energy (that we know of). This means that all forms of energy eventually degrade into heat. Let's take an example of two electric heaters. 'A' is just a heater. 'B' is a heater that also has a fan, a few LEDs and a speaker that reads out the time every hour.  
For both A and B, the electricity used by the machine's coils will all be converted to heat by the resistance. But the interesting thing about B is that even the air movement (fan), light (LEDs) and sound waves (speaker) will all degrade into heat as well.  
The only way A will be more efficient than B is via the MINISCULE energy lost by the sound waves and vibrations leaving your sealed room through the wall. ",null,0,cdj06vt,1r1jp1,askscience,new,2
DevinTheGrand,"The menstrual cycle is a primate only means of female reproduction.  The reason why female primates have a period is because they shed the lining of the uterus when pregnancy does not occur.  Most other mammals use an estrus cycle, which results in the re-absorption of the uterine lining by the uterus.  (As an aside, rats can actually re-absorb miscarried young through their uterus as well, which is pretty awesome)

Animals that use estrus also are usually only sexually active during their period of ovulation.  This is called ""being in heat"" which I'm sure you've heard before, even if you didn't know the context.  Occasionally bloody vaginal discharge will occur, but it is not necessary, and it does not contain uterine lining.",null,2,cditwrk,1r1jah,askscience,new,11
MarineLife42,"Apart from /u/DevinTheGrand's excellent explanation, please note that what we call ""period"" in women is only a very recent phenomenon. Up to about 100 years ago, a woman of fertile age would have maybe four or five menstrual discharges throughout her life; the rest of the time she would have been either pregnant or lactating. My own great-grandmother had fourteen children of which ten survived into adulthood - totally normal then.  
Consider that at that time, girls/women began puberty much later than now, and you can see that there was hardly time for her to get anything approaching a 'period' going.  
In fact, menstrual bleeding is a kind of emergency backup program that the body runs when no egg fertilization occurs, which explains why it causes so much discomfort for many women. As far as the body is concerned, an egg going unfertilized is not supposed to happen.  
With mammals not under human supervision, including marine mammals, it is like the olden days with humans - as soon as a female is in estrus, sex will occur and she will end up pregnant. In the wild his is the norm, not being fertilized is the exception. ",null,8,cdj7il5,1r1jah,askscience,new,0
trebuday,"This is still a topic of hot debate in climatology.  There are several ideas regarding the cause of the Mid-Pleistocene Transition (MPT), and a quick google search brought up these two papers:

[Clark, et.al., 2006](http://geosci.uchicago.edu/~archer/reprints/clark.2006.MPT.pdf)

[Martinez-Garcia, et. al., 2006](http://adsabs.harvard.edu/abs/2006AGUFMPP21B1688M)

Both of these papers attempt to address inconsistencies in various theories regarding the MPT, and both agree that the cause of the MPT was probably a very complex set of interactions between many or all of the various climate change processes (silicate weathering, deglaciation, changes in insolation, changes in ocean circulation, etc.). 
",null,0,cdj2kl9,1r1j7m,askscience,new,1
chaseoc,"A thin sheet of aluminum foil would offer very little protection against cosmic rays unless it were extremely magnetized. Cosmic rays are extremely high energy particles emitted from black holes and stars... they travel very very close to the speed of light. Some carry as much energy in one tiny little atom as a thrown tennis ball. Because they are so small, there is a very small chance they will actually impact any atoms in that sheet of aluminum unless it were very very thick... and even if they do impact, they usually create a shower of particles that will continue on if the material is not thick enough from the point of collision to absorb them.

The same is true for normal radiation. You need thick lead containers to fully absorb radioactivity.... and normal radiation is not nearly as powerful as a cosmic ray. On earth it would be easy to protect, but in space wrapping a spacecraft with lead plating is not really feasible. Magnetic fields are an option, but to be effective with such high energy particles it takes a lot of energy... and to generate a lot of energy usually involves a very weighty reactor. 

http://en.wikipedia.org/wiki/Cosmic_ray",null,0,cdizta3,1r1j7j,askscience,new,5
endocytosis,"[Chlorophyll](http://en.wikipedia.org/wiki/Chlorophyll), what plants use to make energy, absorbs most light in the UV or near-UV range of light, although they can absorb some light in the yellow-red range (they reflect green, so plants have greenish leaves).  An incandescent lamp won't provide enough light in this range, and usually most other light sources won't either.  Artificial sun-lamps basically just provide shorter wavelength, ""blue-purple"" color, or possibly UV, although that is a bit more dangerous to your eyes, so the plants can use that for photosynthesis.  Sunlight has all of visible light, in addition to other wavelengths.",null,0,cditxys,1r1icj,askscience,new,2
LoyalSol,"Yup,  even in mixtures like corn starch and water you can still sink if you stay still long enough and of course you can't breath it. ",null,0,cditifq,1r1i97,askscience,new,3
steeeeve,"Non-newtonian fluids can actually be more dangerous than newtonian ones, because as a person attempts to swim or tread water, the fluid increases in viscosity around their limbs. 

[Quicksand](http://en.wikipedia.org/wiki/Quicksand#Properties) is an example of a non-newtonian fluid, and though its dangers have been exaggerated in pop culture, it's certainly still possible to drown in it.",null,0,cdj0w3k,1r1i97,askscience,new,3
mrmayo26,"If I remember correctly, the trauma to the head causes a great deal of force and potential injury to the brain. Therefore in order to protect itself (although I'm unclear how this protects it or helps it exactly) it stops doing non-essential functions and reduces its activity. Although I'm sure someone else knows more about this subject than myself

",null,0,cdjaena,1r1hma,askscience,new,2
ericgdc,"There is a torsional force that takes place when you receive a concussive blow. This torsion 'stretches' various midline structures within the brainstem, areas that control breathing, alertness, etc. (think of wringing a wet towel, the center structures being 'twisted' the most). This is thought to play a role in the sudden loss of consciousness that takes place with certain blows to the head.



",null,0,cdjankh,1r1hma,askscience,new,2
GProteins,"Short answer: No.

Your tears are more than just water. This is unfortunate for your no-blink plan, but fortunate for your eyes! They contain things like lysozymes, which help prevent eye infections by killing bacteria. It also contains various lubricating oils and some salts, which may just be left over from being drawn from blood plasma.

But either way, if you lived in an environment humid enough not to blink, it's humid enough to have a LOT of bacteria around that you don't want in your various mucous membranes.",null,0,cdn9i8i,1r1fvd,askscience,new,1
DanielSank,"A relative of mine does this stuff for a living so I can tell you what I've learned by asking the same question. Note that this is a combination of *what I've heard* and what makes sense.

Issues that you have to worry about when sending electronics to space:

* Radiation. Without the atmosphere as a shield there is more radiation in space. When a circuit gets hit by this radiation a variety of things can go wrong. A digital memory element may change state. This could lead to errors in CPU or FPGA controller chips. This is particularly a problem if the physical features are really small, so you have to get ""old"" parts that don't use current lithography scales. Analog parts may have problems too but I'm not sure. Parts that are qualified to not have errors when exposed to space levels of radiation are called ""radiation hard"" or ""rad hard"".
* Vibration. During launch the payload is subjected to vibrations. Why is this a problem? I think it's because the connections to the chip can break. Microchips are usually fabricated on a substrate of silicon, and the contact pads for wires going in and out are pretty small in cases where many wires are needed. You can't just solder to that. A common way to connect to the pads is with [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding), essentially very thin threads of metal that are sort of mashed onto the contact pads.
* Packaging. Consumer electronics are packaged in plastic. If you look inside your keyboard you'll see a variety of black square or rectangular things with metal pins sticking out. The black is plastic and the silicon die is inside. Plastic absorbs water from the air. When you send that into space it gets cold and I think the problem is that the water freezes, expands, and cracks the package. This would probably rip off some of the wire bonds. Now, having said that a guy in my lab put a consumer FPGA into a 4 Kelvin cryostat and it worked so I could be completely wrong about this.

As for the cost, there are two factors. The first is that making something with the right packaging and robustness against radiation means you aren't using your standard fabrication line. That means the price is higher just because of supply/demand. That aside, an enormous cost is *testing*. When NASA builds a satellite they require not only that the parts in principle deal with the things listed above, but also that they are actually tested by

* Vibrating them
* Operating them under irradiation
* Thermal cycling
* ...and more.

That means you need extra test equipment, people, and time. when you buy a mouse for your computer that's just a part off of a fab line that makes a million of 'em a day. Maybe a small number of parts from each batch are tested for quality control. When you build a satellite, you test every single part.

EDIT: I think a lot of contemporary parts use degenerately doped silicon, so I guess carrier freeze-out isn't a problem. Does anyone know if this was a problem ~30 years ago?",null,0,cdilxiv,1r1bka,askscience,new,11
humanino,"We first should acknowledge that we can only speculate about answers to those question. The final stages of black-hole evaporation are not well approximated by quantum field theory in curved spacetime, one would have to use a full non-perturbative theory of quantum gravity. 

Although very slow at first, the process eventually appears explosive. Because of this, (in my understanding) most people believe nothing is left behind. That is still explicit in Hawking's latest papers for instance. The total lifetime, estimated from the power emitted by Hawking's radiation (a formula which may not apply at late stages, but this stage should be short), varies like the cube of the mass of the black hole. For a black hole of one solar mass (10^30 kg), the lifetime would be 10^67 years. One should pause to contemplate such a number. The lifetime of the universe is a mere 13.7 billion years in comparison. Most black holes are (much) more massive than the Sun.

That is the reason people hoped microscopic black holes would have formed in the early universe, whose explosion we could possibly detect today. ",null,0,cdikbx3,1r17mc,askscience,new,4
Quantumfizzix,"All that /u/humanino  said is true, so keep that in mind.

There is no such thing as ""not enough mass"" for an event horizon to form. All that's required for a black hole is density. Obviously, it needs SOME mass, but that amount of mass can be as small as necessary so long that the volume is proportionally small. Once something becomes a black hole, it stays that way until completely evaporated. There is no remnant.

",null,2,cdiknps,1r17mc,askscience,new,4
5k3k73k,"There are at least two hypotheses:

Hawking radiation increases the smaller the black hole becomes. So much radiation is lost in the last few moments that it looks like the black hole explodes.

There is a threshold at which a black hole becomes too small to interact with other particles, halting Hawking radiation, and it becomes a stable WIMP.",null,0,cdj40qf,1r17mc,askscience,new,2
MayContainNugat,The C-14 scale is corrected for variations in atmospheric C-14 by calibrating with tree rings of definite known ages. ,null,0,cdipwgt,1r17l3,askscience,new,5
rupert1920,"Molecular weight doesn't quite matter - the filter doesn't operate base on [size exclusion](http://en.wikipedia.org/wiki/Size-exclusion_chromatography).

Your Brita filter removes most ions by [ion-exchange](http://en.wikipedia.org/wiki/Ion_exchange_resin) - these are stationary phases with pre-existing slots for ions to bind to. The ""exchange"" occurs when your impurity replaces the existing ions in binding to the stationary phase.

The [activated charcoal](http://en.wikipedia.org/wiki/Activated_charcoal) (the black powder that sometimes leaks out) is good for removing a large range of other impurities, and they work by non-specific [adsorption](http://en.wikipedia.org/wiki/Adsorption). In that sense, they _are_ able to remove dissolved organic compounds. However, if you're talking about something like dissolved sugars, you're often looking at concentrations way beyond trace amounts.",null,2,cdij2gr,1r16yk,askscience,new,11
myarlak,"yes, if it is a water reactive substance like calcium carbide water will react exothermically and ignite the gases produced",null,0,cdij0ig,1r15t3,askscience,new,11
FatSquirrels,"Besides a chemical reaction with water itself, anything that has an [autoignition temperature](http://en.wikipedia.org/wiki/Autoignition_temperature) below 100 C could theoretically be ignited by heating it with boiling water.  However, if the material did not react with water and you poured the water on top of it you might simply smother any flames that would be normally produced.",null,0,cdix6p7,1r15t3,askscience,new,3
null,null,null,6,cdij52z,1r15t3,askscience,new,1
Das_Mime,"Sound is a compression wave, it travels by massive (which in this context just means ""having mass"") particles like molecules bumping into each other and transmitting that bump forward. Imagine being in a dense crowd, and then a group of people charge into one edge of the crowd. Their momentum will be tranferred to the next person, and the next, and so on, propagating a wave outward through the crowd.

Radio waves are electromagnetic radiation, which occurs when you have an accelerating charge. The electric and magnetic fields (which can exist in empty space) keep oscillating and propagating themselves through space. They don't require a medium (other than spacetime and electromagnetic fields) because they aren't transmitted via massive particles.",null,2,cdij03b,1r15bm,askscience,new,36
BoxAMu,"This had 19th century physicists stumped too!  It used to be believed that a medium for EM wave propagation called the luminiferous ether pervaded all space.  However, this should change the properties of EM interactions (namely wave speed) for observers moving relative to the ether, just as motion with respect to still air changes the effective properties of sound.

Long story short, this change of wave properties is not seen.  Light always travels at c = 3 x 10^8 m/s in free space regardless of the relative motion of observer and source.  Moreover, based on the theory of relativity it is meaningless to even try to define some absolute space in which the ether could be located.  No medium for the propagation of EM waves exists.

So what IS an EM wave?  The question is hard to pin down.  The laws of physics correctly explain all observed EM phenomena, so it seems that this question is not directly relevant to experimental science.  But indeed, the idea of a wave without a medium is hard to visualize.",null,1,cdijd5s,1r15bm,askscience,new,15
Qazerowl,"I kind of get the feeling you don't know what radio waves are (if you do, ignore this).

Radio waves aren't actually sound, they're actually a type of light. Radio antennas can detect radio waves, and are programmed to make noise based on what signals they get. So, sending radio waves through space is more like Morse code with lasers than yelling into a megaphone.",null,4,cdikn7r,1r15bm,askscience,new,9
50bmg,"As i understand it, EM radiation is a wave created through *self propogating* electric and magnetic fields. That means there is no medium necessary for the wave to move through it - just the capability for empty space to have a quantifiable magnetic/electric field (which can be zero at any point, so it's not exactly a medium like water or air). The quantized form of this wave is a particle (the photon). Sometimes it is easier to picture radiation as photons traveling through space. ",null,0,cditcon,1r15bm,askscience,new,3
Roar_of_Time,"Sound can be recorded into a microphone, where it is converted into electrical energy and stored in a computer hard drive. The electrical energy can then be converted into a radio wave using an antenna. Radio waves are really small particles/waves called photons, and are a form of electromagnetic radiation. Visible light is exactly the same thing, just at a smaller wavelength, as are X-Rays, Gamma Rays, Ultraviolet Rays, etc. Photons require no atomic medium to travel, so they can travel through space. They travel at the ""Speed of Light"" through space. Photons can be picked up by other antennas and converted into electrical energy again, and then be converted into mechanical energy by a vibrating magnet, or a speaker. This mechanical energy is transferred through the air by the atoms in the air colliding with each other in a wave pattern. I suppose you could see them as a sort of atom sized photon, some people call them phonons. The colliding atom chain reaction enters your ear canal, where it is converted back into electrical energy and processed by your brain, which you perceive as sound. By changing the electrical energy that vibrates the magnet, the vibration can be faster or slower, making different sounds by changing the force applied to the atoms. 

Here's a nifty chart for the electromagnetic radiation spectrum if you're interested: 

http://mynasadata.larc.nasa.gov/images/EM_Spectrum3-new.jpg

And here's a rather cool video I found that shows that sound waves are simply moving atomic matter.

http://www.youtube.com/watch?v=wvJAgrUBF4w",null,0,cdinc4m,1r15bm,askscience,new,1
CoolStoryJohn,"A very simplistic answer is that sound is essentially a sequence of vibrations that travel through a region (molecules in the environment being the vehicles for vibration, i.e. molecules in the environment are responsible for the transportation of the sound) whereas a radio wave can be viewed as its own transport vehicle (""self-sufficient"").  Space has no atmosphere (and thus no molecules), so sound cannot travel.  However, as mentioned, radio waves aren't dependent on their environment in that way, so they are able to travel in space.  ",null,1,cdiy9vv,1r15bm,askscience,new,2
GlorifiedTapeOp,"Sound is a compression wave,  which means that energy is transferred between particles.  If there's no medium in space for the sound compression wave to transfer it's energy through, then the sound becomes void. 

Radio waves are essentially light (electromagnetic radiation) which don't require a medium like particles with mass for sound. 

Edit: Das_Mime answered this question better",null,0,cdiyezb,1r15bm,askscience,new,1
RationalAnimal,"
This is a good question.   And the posters above are not wrong, as far as their answers go, but there is a sense in which the original question is not answered. 

As others have pointed out, contemporary physics has it that light exhibits a wave/particle duality.  

The sense in which light is a wave is, in modern physics, only a metaphorical sense. In the literal sense, a wave is a higher-order property of some physical substrate.   Waves in water are the more voluminous sections of water in a pattern of more (wave) and less (trough) voluminous sections of water.   Waves of sound are patterns of sections of air with greater and then lesser air pressure.   Light ""waves"", however, are not now believed to be real waves in any other physical substrate.  

For a very long time, it has been known that light exhibits behaviors that waves exhibit.  Light diffracts through thin slits, for example, into fan-shaped patterns of different colors of light.  Light exhibits interference behaviors, like water waves do.  This evidence led scientists to believe, for a very long time, that light, too, was a wave.  So, they reasoned, if light were a wave there must be some physical substrate of which it is a wave.   Aether was posited as the substance patterned variations in which were light waves. 

It turns out, however, that there is no other evidence for aether other than the fact that light seems wave-like.   There is no detectable drag on moving objects in the vacuum of space.  There are no relative-motion effects exhibited by light relative to the direction of earth's motion around the sun, as one would expect if earth were traveling with respect to aether.  And so on.  

So, the notion that there is aether was dropped, but we were still left with the awkward exhibition of wave-like behavior of light.   That's pretty much where we are now, as far as a comprehensible physical model of light is concerned.  The claims that light is a ""self-propogating"" wave, or that light is the kind of wave that is not a wave in anything else, are not really meant to *explain* the puzzling sounding claim that light is a wave of nothing.  That is, these claims are not meant as ways of thinking of waves in a literal sense as a *physical model* of what light really is.  

They are just ways of saying that light exhibits wave-like behaviors sometimes, and that a representation of light as a wave permits one to predict the behaviors of real light accurately.   Even the claim sometimes made that we can think of light waves as ""probability waves"" isn't meant to offer a description of a physical model of light.  It is a shorthand way of saying that there are higher probabilities of detecting photons at certain places in beams of light and that the pattern of these probabilities has a formal resemblance to physical waves. 

So, the question of *how* light can be a wave without being a wave of something isn't really answered by modern physics.  It's better to think of modern physics as telling us that that macro-level, real waves are not a physical model of real light, but that light nevertheless exhibits wave-like properties.   What light *really* is is something, modern physics tells us, that has no analogue in our direct, macro-level experience.  But we know it travels through a vacuum.  So, there you go.  ",null,0,cdjj6kf,1r15bm,askscience,new,1
DanielSank,"&gt; What are the parts of the D-Wave quantum computer?

~Sigh~

What D-Wave is selling has not been proven to be a quantum computer.

&gt; what general parts are there in a quantum computer?

I have [this saved from when /u/whittlemedownz explained it a while back](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp) but I'll give me own spin.

A quantum computer has a bunch of individual information storage elements analogous to the bits in a classical computer CPU or memory. The physical incarnation of the bits in a quantum computer must be build so as to exhibit quantum mechanical behavior. As such they are called ""quantum bits"" or ""qubits"". There are many possible physical elements that can be used: atoms, electron spins, photon polarization, superconducting circuits, and more. The D-Wave machine uses superconducting circuits. Each one is a loop of superconducting wire. The current in the wire can flow clockwise or counterclockwise. Because it's quantum the qubit can also be in a state that is a superposition of both directions of current. The bits interact with one another via the magnetic dipole of each loop of current: each bit feels its neighbors' magnetic fields.

The computer also has wires that carry signals into the bits to control them in various ways. In the case of D-Wave this is by applying external magnetic flux to the loops. In fact this control circuitry is extremely difficult to get right and in my opinion is the most impressive thing about D-Wave's machine.

&gt; but what exactly am I looking at - some sort of scintillating CPU?

The thing in the center is the chip with the superconducting qubits. The rest of it is control wiring and a cryogenic mount. Look in the dead center. See the rainbow colors in the black square? That's the chip. The rainbow color happens because of diffraction off of the tiny lithographically defined features. Those sets of itty bitty parallel lines at the border of the black chip are [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding). The wire bonds connect wires on the chip to wires on the green circuit board mount. The circuit board has its own wires (the thin lines in the green board) which fan out and connect to those bundles of what look like braided copper wires (the brownish things with the white labels on them). The gold colored metal just a mounting apparatus. It's probably gold plated oxygen free copper, a commonly used material in cryogenic applications.

Source - I work in a quantum computing lab",null,0,cdina0c,1r140w,askscience,new,9
spirit_of_loneliness,"It's hard to explain 'quantumness' in quantum computers without refering to quantum physics, but in extremely simplified version it's about 'more than 2 states', as 'regular' computers work only on two states ('voltage' or 'no voltage', so 0 or 1) and everything is built on this. Now, what makes quantum computer 'quantum' is the fact, that it can work on those 'usual' 2 states (0 and 1) and everything BETWEEN them (let's say a current state can have 40% of '0' and 60% of '1' ), this is based on fundamental physical principle called [superposition](http://en.wikipedia.org/wiki/Quantum_superposition)  

Basically, as you probably know, ""quantum computer"" works on qubits (named this way to differentiate from regular bits, reason above), so it boils down to the question how those qubits are implemented in given machine.  
Scientists already managed to invent more than one implementation, just take a look [here](http://en.wikipedia.org/wiki/Qubit#Physical_representation)   Of course, leaving all the 'standard' electronics, quantum computer is usually built around hardware, that can interface with physical materials, for example. laser and photon detectors (when qubits are implemented with photons), in general 'something' that can read the current state of our physical implementation (yet another simplification - read how much % of a '0' or '1' is there currently)",null,7,cdineb8,1r140w,askscience,new,7
mutatron,"Right now it's Spring in the southern hemisphere, and it's just business as usual in the vast tropical regions. Here in Texas the leaves are only just changing, but we also have evergreens. I suspect you have evergreens in Massachusetts too. Not only that, but around 50% of the oxygen on Earth comes from plankton, and another 20% comes from algae.

There's a seasonal variation in local oxygen levels described [here](http://answers.google.com/answers/threadview/id/769958.html):

&gt; The levels of O2 are altered by the fall/winter in the northern
hemisphere, but not to a detectable level. Plants (both deciduous
(leafy) trees and many bushes and grass) do not perform photosynthesis
during the fall and winter months. This results in a cyclical
variation in Carbon Dioxide (CO2) levels. This season variation is
~5-6 ppmv: parts per million by volume. The total amount of CO2 is
approximately 380 ppmv. So the CO2 level cycles by ~1.5% annually.

&gt; O2 should change for the same reason, but the fraction of O2 to CO2 in
the atmosphere is 549:1 (by volume). Or O2 is 209,460 ppmv to CO2 ~380
ppmv. So the percentage variation of O2 is &gt; 0.002%.",null,1,cdiioni,1r13vn,askscience,new,10
Jeeebs,"There is a condition called Bromism (Not something from How I Met Your Mother), where someone ingests too much bromide, and it can react with things in the body causing pretty serious problems. Typically this is from bromide salts, but it's also handy to note that BVO is not a salt.

HOWEVER... Bromine atoms are great leaving groups, so in much of the harsh conditions of the body, it is very possible the bromides are leaving the fatty acid chain and causing the same havoc.",null,0,cdinsbq,1r12y1,askscience,new,2
MarineLife42,"Depends where in the body, and whether the heat can dissipate. There are many people now who have a brain implant that does just what you said - deliver a very low voltage of a specific point in the brain, helping with such things as Parkinson's, depression and other neurological issues. So when used correctly, it is even beneficial.  
Our entire nervous system relies on low voltages running as signals, controlling voluntary and involuntary movement of muscles. Here, a low voltage would interfere. ",null,1,cdj7oqo,1r1272,askscience,new,3
iorgfeflkd,We are near the centre of our observable universe.,null,0,cdii8uj,1r11s1,askscience,new,5
Lirkmor,"Everything is moving away from everything else (broadly, not taking into account galaxy collisions and such), so even if we were observing from another planet light years away, we would still see the same effect. This is because objects aren't moving away from each other *through* space, but rather space *itself* is expanding.",null,0,cdiin49,1r11s1,askscience,new,4
e_t_,"Space itself is expanding, so everything is getting further from everything else. It's not like inflating a balloon. One method of enlarging an image is to divide the original into a grid, then recreate each square but bigger. Every point on the big copy will be further away from its neighbors than on the small picture, but that doesn't give you any information about where the center is.",null,1,cdiit62,1r11s1,askscience,new,1
BoxAMu,"This is a general question about light, regardless of the source producing it (sun, candle, laser, etc.)  Light spreads out as it propagates, becoming weaker in intensity because the same energy is spread over larger and larger areas.  Light doesn't actually disappear unless it is absorbed by matter, in which case the energy it carries is converted into a different form (motion of particles for example).

It's definitely possible to trap light in a box or other type of resonator, just like sound can be contained in a resonant chamber (like an acoustic guitar body).  In a resonator, waves propagate around the structure continuously.  They don't decay unless they leak out of the resonator or are absorbed in the walls.  For light, this has been achieved with a variety of different structures.  Some of the best are made from micro-sized (~10^-6 - 10^-5 m) spheres in which the light is confined to circulate near the walls.  However, these can still only confine the light for a short period of time (micro or nanoseconds).  ",null,0,cdiiimb,1r1184,askscience,new,2
Mazetron,"First of all, wormholes are only theoretical at this point.  We have no evidence of real wormholes existing and we are no sure whether or not they could exist theoretically.

However, they are fun to think about.  [Here is a rendering of what a real wormhole might look like](http://en.m.wikipedia.org/wiki/File:Wurmloch.jpg)

Go check out the Wikipedia page, it's got some good stuff.",null,2,cdilx53,1r10y1,askscience,new,12
rupert1920,"There is no reason it won't be possible. You'll get a hydrogen ion - or a [hydron](http://en.wikipedia.org/wiki/Hydron_(chemistry\)), which is highly reactive.",null,0,cdiia6d,1r0y6n,askscience,new,9
myarlak,"you would make a proton, happens all the time
",null,0,cdij1oa,1r0y6n,askscience,new,8
un7ucky,"hi production horticultrlist here. 
no the exocarp (peel) of a fruit will not grow back, but it will (plant depending) attept to form a scab to protect undeveloped fruiting bodies. most mature fruit will likely rot. 

any specific plant you had in mind when you asked?",null,2,cdil5ru,1r0xqx,askscience,new,8
Truck43,No. The skin is part of the flower that becomes the structure on the fruit. It cannot grow back. ,null,3,cdijcjb,1r0xqx,askscience,new,3
BoxAMu,"First: the state of a system encompasses the whole trajectory of its particles consistent with a given energy.  One section of that trajectory moving to another part is not a change of thermodynamic state.

Second: entropy is a statistical concept.  The fundamental laws of particle interaction are time-reversible, and the second law of thermodynamics does not apply until one considers an ensemble of many systems.  Or, one could consider the interacting particles to have some internal structure.

What would happen if two macroscopic charged bodies were released and allowed to attract one another?  Eventually they would collide and bounce back, only to re-collide and bounce back again and again.  Either the system would continue to oscillate like this, or more realistically, some energy would be dissipated in each collision until the bodies stick together.  The conversion of lost energy to heat represents an increase in entropy.",null,1,cdiiymq,1r0xju,askscience,new,6
cephsdiablo,"""When two oppositely charged particles attract towards one another via the electromagnetic force, does the decreasing volume of the system""

If you isolate to opposite charges those charges will attract towards each other with one given equation (I'm sure you know which equation this is). This equation shows that they are interacting only with each other not the entire system.

""result in fewer microstates per given macrostate of that system (and therefore an evolution towards decreasing entropy)?""

You can't assume that two microstates coming together creating one macrostate creates a decrease in entropy. Here is an easy example. Hydrogen and Oxygen in two smaller ""microstates"" create a much larger explosive macrostate when combined. 

If we could combine an electron and proton together to the point where they could create a reaction, I'm not sure what would happen and I'm not even sure if that is physically possible. Has anyone done it before??

A decrease in volume does not necessarily dictate entropy decrease.",null,0,cdip0ww,1r0xju,askscience,new,1
Rocker232,"It really depends on what animal you are talking about, many adaptations have evolved. Some animals such as penguins use huddling as one technique, some frogs completely shut down their bodies and freeze. Other animals have a substance in their blood that prevents freezing. The amount of adaptations to combat against cold weather/temperatures could go on and on.",null,1,cdilly0,1r0x4h,askscience,new,4
sporclesam,"When its extreme, most [ectotherms](http://en.wikipedia.org/wiki/Ectotherm) go into stasis/torpor. Endotherms generally consume more nutrition &amp; conserve body heat through physical means such as those Rocker232 mentioned. Animals such as some bears will hibernate during extremely cold weather conditions. Many smaller animals such as hares can burrow in and conserve body heat in such fashion. Deer generally migrate in extreme conditions. 
You are underestimating the impact of fur, it is quite useful (along with subcutaneous fat) in such conditions. 

All these are physical means; there are numerous [bio-chemical ways](http://www.nature.com/scitable/knowledge/library/extreme-cold-hardiness-in-ectotherms-24286275) in which animals (and plants) fend themselves in the cold. ",null,0,cdiq2j6,1r0x4h,askscience,new,2
baloo_the_bear,"Not really. Calories are a measure of the energy stored in the food, and eating calorie-dense food does increase the sensation of satiety, but the food you eat gets broken down into sugars, fats, and amino acids and is absorbed. The food causes an initial spike in blood sugar levels, but it is quickly converted into long term stores if it isn't used pretty quickly. The body likes to maintain a relatively narrow range of parameters; staying alive requires very specific set points for things like serum glucose concentrations, pH, temperature, etc. 

The feeling of hunger has a few proposed causes, from emptying of the stomach to dips in blood sugar. We do know that there are hormones which can affect the feeling of hunger and satiety, and they are ghrelin and leptin, respectively.

Hunger is odd in the sense that a person can survive for days without eating, yet we experience hunger several times a day, and whether we need food to survive or not. 

Hormone states are constantly in flux, and insulin, glucagon, ghrelin, and leptin are some of the hormones that play roles in the feeling of hunger.",null,0,cdiio5d,1r0wnc,askscience,new,9
iorgfeflkd,"In that specific scenario, the source of the gravitational field would cease to be due to the rest mass of the star, but rather to the energy density of all the gamma radiation from where that star used to be. As it spread out, the gravitational field would change accordingly. This is because in general relativity, the source of gravity is not just mass but a more complex quantity called the stress-energy tensor, which is primarily dependent on energy density and pressure.",null,1,cdiiv2u,1r0tsg,askscience,new,4
armrha,"Have some aspirations of super villainy, eh?

Well, unfortunately, 'mind control' implies some kind of agency to it that they don't really have. They don't 'know' what they are doing, and the mechanisms they do it aren't super-complicated machines they install in the host's brain or anything like that. They're just little tricks that ended up working in their favor. It's evolution in action -- they never 'figure out' the brain of their host, they brute force it with the evolutionary algorithm.

Take the commonly dragged out example of Dicrocoelium dendriticum. It's a trematode which 'mind controls' infected ants to go hang out on top of blades of grass in order to get eaten by cattle and such. At some point, they were just like many other non-behavior affecting parasites, but eventually some kind of small change to their genome resulted in some very slight chemical imbalance inside the ant hosts, that resulted in slightly more of the fluke's ancestors reproducing than they otherwise would.

This could have been any number of things. A chemical that makes the ant work harder, giving it more time to get eaten. Maybe it secreted something that slowed the ant down, just making it an easier target. It's impossible to say exactly.

After the first change, generations go by which more success, and slowly tiny changes that result in more reproductive success start to take hold. One generation moves a little closer to the nerve center, things get more efficient. Whenever a change is not beneficial to the host, it ends up failing to reproduce (or at least not outpacing reproduction). So the flukes that had their ants hang out on grass in the blazing sun didn't make grandchildren. Selection slowly weeds out the most beneficial effects.

Viewed as a whole as they exist today, they almost seem to be aware of what they are doing, like little pilots steering an ant ship. But that's just the culmination of thousands of tiny little brain hacks they've discovered along the way, automatic parts of their life cycle that interact with the host in an interesting way.

Could such a thing happen to humans? Absolutely, but again, it'd be on an evolutionary timescale. I mean, parasites exploit what is available to them. Parasites piggyback on your biology, altering it somewhat, and anytime things are altered there's always the potential for the brain being effected. There's conjecture that Toxoplasmosis infection makes people more prone to risk taking, for example. 

There's a constant arms race between complex, cooperating groups of cells like ourselves and the smaller complex cooperating groups of cells that want to get in there and defect, taking all the free resources they can get their hands on. Ultimately, that's what the entire parasite game is about. So the answer is yes, but probably not in the way you were thinking. Especially as far as not being able to cause any specific, meaningful actions as far as we think of them as persons.",null,3,cdipevt,1r0qvp,askscience,new,9
wretched_beasties,"This isn't just with insects. Parasites are masters at manipulating host behavior, even in humans. For example, the guinea worm (Dracunculus medinensis) drives infected humans to soak their feet in water, facilitating the release of their eggs into a water source and continuing their life cycle.
",null,0,cdjelnm,1r0qvp,askscience,new,3
glarn48,"Great question! Not in my wheelhouse but I was intrigued. Any time you ask a ""why"" question about evolution, there's always some degree of speculation involved, but that doesn't mean we can't think of logical explanations regarding evolutionary pressure, and it turns out many researchers seem to have asked this very question. I'm going to summarize the findings of one paper (http://www.tandfonline.com/doi/abs/10.1080/1357650X.2013.824461#.UoxJv8Rwq6V), though they cite many other competing hypotheses.

Their theory is built around issues of mobility, given that mobility is crucial for survival and reproduction in all species, predator and prey alike. If part of the body is injured moderately or even fatigued, the brain can monitor the body via feedback loops and compensate for injuries by making dynamic adjustments to movement. However if the brain and body are both compromised by an injury, this compensatory ability is diminished. The authors posit that because a non-lethal injury that damages the head and body is likely to be unilateral, having contralateral control by the brain can preserve these compensatory effects. 

The paper does not discuss the issues surrounding sensory processing that you raise in your question though, but the explanation holds to a certain effect here too. A truly severe injury to the right side of the head might knock out the right eye completely AND the right occipital lobe leaving the left eye ineffective; but such a serious injury would not fall under evolutionary pressures because animals who sustained such a serious injury wouldn't live regardless of whether sensory control was ipsilateral or contralateral.
",null,1,cdimdq9,1r0qp0,askscience,new,3
lastsynapse,"The short answer is that we don't really know, but the evolutionary process occurred very early in the history of vertebrates.

[These folks](http://www.ncbi.nlm.nih.gov/pubmed/18780298) suggest it is because decussation is less prone to errors.  There's some more references on the [quora](http://www.quora.com/Why-are-so-many-brain-pathways-crossed-and-when-did-decussation-develop-in-phylogeny) question, too.  Others suggest it has to do with injury and injury prevention, that with crossed representations you can defend yourself on the side of attack, but that's probably hooey.

The weird thing is that you have ipsilateral cerebellar representations which are contralateral on the cerebral cortex.  Eyes are mixed, projecting both ipislaterally and contralaterally.  Everything crosses in such weird ways.  If there is a reason evolution selected for crossed representation, it has a whole lot of explaining to do, that won't be reduced down to one simple sentence.",null,0,cdmktyb,1r0qp0,askscience,new,2
SqueakyGate,"We can't know for sure but here are some interesting dates:

1. ~60,000-50,000 years ago it is postulated that humans reached a point known as [behavioural modernity](http://en.wikipedia.org/wiki/Behavioral_modernity). ""One [hypothesis] holds that behavioral modernity occurred as a sudden event some 50 kya (50,000 years ago) in prehistory, possibly as a result of a major genetic mutation or as a result of a biological reorganization of the brain that led to the emergence of modern human natural languages"" These humans were like us in every way, so it is very very likely that you could mate with and produce viable offspring with these humans. Moreover these humans could be brought up in a modern society and would be able to blend in perfectly from both a physical and behavioural perspective. 

2. Anatomically modern humans arose about 200,000 years ago. It is very likely that you could mate and produce viable offspring with these humans as well. Although the evidence is less clear that these individuals behaved like us. For example we are unsure if they could have developed the same level of language complexity as ourselves. These anatomically modern humans could be brought up in a modern society and would be able to blend in perfectly from a physical perspective, however they may not be able to blend in from a behavioural perspective. It is worth noting that ""he second [hypothesis] holds that there was never any single technological or cognitive revolution. Proponents of this view argue that modern human behavior is the result of the gradual accumulation of knowledge, skills and culture occurring over hundreds of thousands of years of human evolution"". Therefore the dates for ~60 to 50 kya may not be so hard and fast.

3. Other homo species. We know that about 1-5% of human DNA is [Neanderthal](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens) in origin. We know that early humans and neanderthals could interbreed and produce viable offspring. Since it is very likely that modern humans could reproduce with ancient humans living 200,000 years ago than it stands to reason that modern humans and neanderthals could interbreed and produce viable offspring. A few important caveats are worth mentioning:

* 1-5% DNA can be explained by a small number of interbreeding events. This means that interbreeding events were probably uncommon.

* Human and Neanderthal populations did not overlap completely, therefore not all humans encountered neanderthals or vice versa. This means any interbreeding that occurred was not widespread.

* Neanderthals evolved about ~600,000 years ago in Europe, conversely humans evolved about ~200,000 years ago in Africa. Humans migrated out of Africa and into Asia about 100,000 years ago. Humans and neanderthals only encountered each other in Europe from about 50,000-25,000 years ago. Thus neanderthals and humans had been evolving independently for thousands of years. Behaviourally Neanderthals and Humans were very different, even more so around the time when they first began to encounter one another. Humans were better communicators, better innovators, engaged in long distance trade, and were expanding into new environments as well as exploiting them in novel ways. In contrast Neanderthals were somewhat limited: they had a relatively stagnant tool culture and and their cultural expressions were not as complex as humans. Humans at the time were beginning to express themselves symbolically, whereas neanderthals seemed only to be able to make jewellery, possibly some cave art and possibly engaged in symbolic burials.

* We know nothing about the context in which the exchange of DNA took place...we don't know if it was consensual or not. This has important implications for how the two populations viewed each other. Were they hostile? friendly? did they compete over resources (archeological evidence suggests yes, they did) or did they cooperate with each other? These questions help us better understand the complexity of their social worlds and how we might delineate these two populations (separate species, subspecies or the same species?)

* We know that flow of DNA was in one direction. ""While modern humans share some nuclear DNA with the extinct Neanderthals, the two species do not share any mitochondrial DNA, which in primates is always maternally transmitted. This observation has prompted the hypothesis that whereas female humans interbreeding with male Neanderthals were able to generate fertile offspring, the progeny of female Neanderthals who mated with male humans were either rare, absent or sterile"". This raises questions about hybrid vigour, and questions the assumption that we were the same species. At the very least it appears as if we were speciating, given the level of behavioural separation which had already taken place.

What does this mean for other hominin species like *H. erectus*? We can't know with 100% certainty if modern humans or even ancient humans could mate with these other extinct species. However, given the low levels of breeding with Neanderthals, the extensive evidence which suggests we were very different behaviourally and the lack of support for strong hybrid vigour I would come to the conclusion that it is unlikely that humans, ancient or modern, could produce viable offspring with any other hominin species other than neanderthals.

[More on our coexistence with Neanderthals](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens)

[More information on the Neanderthal Admixture Hypothesis](http://en.wikipedia.org/wiki/Neanderthal_admixture_theory#Neanderthals)",null,3,cdiio7h,1r0k55,askscience,new,19
TwizzlyHashtag,"There is also the issue of the theorized hormonal arms race between the fetus and the mother/host, wherein the fetus produces hormones that attempt to take over the host's resources, and the mother produces hormones to counter that.

However, a human woman from, say, 50,000 years ago would theoretically be too far behind in the evolution of said hormone production, leaving her to be overwhelmed and killed by a modern fetus, because the modern fetus will have been evolved to produce said hormones more aggressively.  Maybe. 

Some example discussions of this phenomenon:
http://books.google.com/books?id=FJixmoT5olEC&amp;pg=PA177&amp;lpg=PA177&amp;dq=mother+fetus+hormone+arms+race&amp;source=bl&amp;ots=OdcOT29uu1&amp;sig=xI2MNYMVlQpyp7M6gfucTVSBQGQ&amp;hl=en&amp;sa=X&amp;ei=GpCOUo79H-igiQKlyYGQDg&amp;ved=0CDQQ6AEwAQ

http://www.ulm.edu/~palmer/Mother.htm",null,0,cdk0nup,1r0k55,askscience,new,1
NassT,"In addition to the oxygen that makes up water (the O in H2O) there can also be oxygen (O2) dissolved in water, just like you can dissolve sugar or salt in water.  That's what people are talking about in this case.",null,0,cdic7sc,1r0iwf,askscience,new,5
hatsune_aru,"Gas can dissolve in water.

Fizzy drinks like Coke, Sprite, Beer and fizzy Champaign all have Carbon Dioxide dissolved in it. Oxygenated water, as the manufacturer claims, has more oxygen dissolved in it than just normal water.

Fun fact: most water-soluble compounds, like sugar and salt, are more soluble at higher temperatures. However, most gases, such as Carbon Dioxide and Oxygen, are more soluble at lower temperatures. That's why warm coke seems to have less fizz! ",null,0,cditnyo,1r0iwf,askscience,new,3
pucklermuskau,"not necessarily limited per se, but definitely a major constraint. Nairboi, Kenya comes to mind, as a city that has definitely expanded, but its road infrastructure is causing major issues, and its hard to fix because the roads that need the most work are the ones that are used day in and day out. China is helping them build a new highway system to take the load off, but yeah: historical decisions are often headaches...",null,0,cdida95,1r0gty,askscience,new,1
JimmyGroove,"While existing infrastructure can certainly limit a city's growth (and your case is a good example of that) there is always the possibilty of removing and replacing it.  That becomes more and more expensive as time goes on, but whether or not it ever becomes prohibilitively so depends on a myriad of economic and government factors that would be really to complex to answer simply.  For instance, a city that is wealthy enough might be able to afford huge infrastructure changes, but if the government of that city has no way to secure rights from current property owners growth might never happen, while a poorer city with a government that has more power might actually have a chance to replace limiting infrastructure.",null,0,cdigpd7,1r0gty,askscience,new,1
joca63,"Unless I'm mistaken it should decrease net calories, but increase the calories you can digest. When something is cooked there will always be partial oxidation somewhere which decreases the energy contained in the food (think, if you over cook you get charcoal, if you really over cook you get carbon dioxide). However cooking makes most nutrients more easially digested. This is particularly true with fruits and veggies. Chewing raw veggies breaks surprisingly few cell walls to allow access the nurtients. Cooking breaks them on a large scale.

In short, cooking is almost a pre-digestion, it does use some calories in the food, but it allows access to more than it uses.",null,0,cdigq6w,1r0fjt,askscience,new,4
lasserith,"The calories themselves are derived from how your body naturally processes the chemicals included in them. Look at this guide for an example: http://www.nutristrategy.com/nutrition/calories.htm. As you can see each class of chemical in your food is associated with a certain energy density. Fats as a rule produce 9 calories when broken down by your body. As your body breaks down all chemicals of a similar class in the same way we are able to accept this 9 calories per gram of fat measurement and apply it to all sorts of foods. What this means is that as long as you don't drastically alter the chemical structure during cooking such as by burning it you can assume that calories in = calories out. Only a few chemicals used in cooking actually substantially change their dietary impact during the process, such as baking powder/baking soda but these are already negligible. 

TL,DR: Cookies are still bad for you but so so delicious.

Edit: As to the question of cooking adding or removing calories this is I suppose possible but I can't off the top of my head think of any thing commonly cooked with that would undergo a dramatic change. What little alcohol is cooked off when you cook with wine would help to decrease calories. There is I guess the possibility of degrading fiber if you use a high enough temperature to produce molecules which your body could possibly break down thus increasing the calorie cost but that would be far outside the range of temperatures at which you cook at.",null,0,cdie19g,1r0fjt,askscience,new,2
quantummonkey25,"Viral genetic replication, especially in the case of retroviruses, have few, if any, error-checking  mechanisms, which results in a high rate of mutation. Combined with the high numbers of viral particles that can spawn from a single cell, this results in a much more rapid rate of evolution.

As for the second one, I have not personally heard of such a case. I do know viruses are often implicatied in horizontal gene transfer, a property that is routinely exploited in genetics labs to deliver particular genetic information to a target organism.",null,0,cdicdvj,1r0dir,askscience,new,13
Farnswirth,"I asked something along the lines of the second part of this question a while ago.  Here's what /u/schu06 (a virologist) responded with:

&gt;Life is thought to have begun as RNA in the primordial soup - chemicals coming together and forming RNA (just a hypothesis as have yet to properly prove it). 

&gt;It has been shown that there are RNA molecules that can catalyse reactions such as self-splicing and self-cleaving (take [Hammerhead](http://en.wikipedia.org/wiki/Hammerhead_ribozyme) as an example). 

&gt;If an RNA can be be enzymatic it's not beyond the realms of possibility that it can develop function as an RNA-dependent RNA-polymerase and self-replicate. This self-replicating entity would just produce more and more of itself allowing evolution to kick in. Mutation would occur giving other ""species"" of the same RNA. Interestingly, it has also been shown that the same sequence of RNA can fold in two different ways to carry out two different functions (so will be able to do more than just replicate)...

&gt;It would get to a point where becoming compartmentalised from the outside world is advantageous to an individual RNA strand, so other molecules don't steal your enzymatic actions. This compartmentalisation could also be controlled if the RNA molecule has other enzymatic activities (as mentioned in the previous paragraph). Compartmentalisation gave the first ""cell"". Since this cell can replicate itself independently of anything else, it isn't a virus by any modern definition. So it then becomes a matter of semantics, would you call this self-replicating cell a bacteria? It isn't a virus because of self-replication

Basically, the answer to the second part, from what I've gathered is a matter of definitions and semantics.  As soon as something ""evolves"" from an RNA-based structure, it is no longer considered a virus, because by definition a virus cannot replicate without first infecting an existing cell.  

*tl;dr - At least for the second part of your question: we don't really know because no one has ever observed it.  But it is not outside the realm of possibility.*",null,1,cdigwl1,1r0dir,askscience,new,5
JeremyJBarr,"Ill chime in here with a slightly different angle. Viruses and ""living"" cells shouldn't be considered as completely separate entities. There is lots of new and exciting research suggesting much more of a symbiotic interaction between the two then we ever realized or thought possible. 

Two recent [papers](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001626) show that a bacteriophage (virus that only infects bacteria) tail spike and a human transcription factor that regulates the insulation of our nervous system (myelination), share the same protein coding sequence. This raises the evolutionary question of where did this transcription factor, that shares such homology with a viral protein, originate? 

It is possible that the gene was similar horizontally transferred. But another interpretation may hint at an ancient interaction between a phage particle and a single cell eukaryote, which may fit in your definition of something 'living evolving from a virus'.
",null,0,cdihu6s,1r0dir,askscience,new,5
zmil,"/u/quantummonkey25 answered the first question quite well. As for the second, it depends on what you mean by ""living."" Plenty of biologists consider viruses themselves to be alive, in which case the answer to your question becomes trivially yes. If you want to limit your definition of ""alive"" to cellular life, well, it's still a little tricky. It has been hypothesized that the eukaryotic nucleus evolved from a virus, this is known as the [viral eukaryogenesis hypothesis.](http://en.wikipedia.org/wiki/Viral_eukaryogenesis) I don't consider it a particularly *likely* hypothesis, however. There are many other cases of organisms co-opting a virus or part of a virus for its own purposes, but generally this is just a case of a few viral genes getting incorporated into an organisms genome; are the genes ""alive""? They're part of something living, certainly, but I don't think that's what you meant by evolving from a virus. This is known as horizontal gene transfer, as alluded to by /u/quantummonkey25. ",null,0,cdih1z7,1r0dir,askscience,new,2
owaisofspades,"because viruses have RNA, when they replicate their genetic material the RNA polymerase can replicate the entire strand, whereas DNA replication cuts a tiny bit off the end of the strand every time, therefore they can replicate their RNA a virtually unlimited number of times. Because of this unrestrained replication, the probability of mistakes arising during replication increases, and these mistakes can occasionally lead to beneficial mutations. As mentioned earlier, other organisms can only replicate a limited number of times in most cells, so the probability of having a mutation let  alone a beneficial one is relatively low.

The second question is beyond the scope of my knowledge",null,3,cdicdri,1r0dir,askscience,new,1
wishfulthinkin,"Sleep and fatigue, along with many other functions, are controlled by the hypothalamus, and the sleep cycle is regulated by neurotransmitters such as adenosine, dopamine, GABA, histamine, and hypocretin, but not by inhibition.


For example, caffeine is a stimulant because it is an Adenosine A1 receptor antagonist, meaning it binds to receptors that adenosine would otherwise bind to, preventing the effects of the adenosine.


Dopamine interacts with norepinephrine receptors, inhibiting its effects - which means a decrease in the production and release of melatonin. Interestingly, the researchers found that these dopamine receptors only appear in the pineal gland towards the end of the night, as the dark period closes. Therefore, the researchers conclude, the formation of these heteromers is an effective mechanism to stop melatonin production when the day begins and to 'wake up' the brain.


Each of the neurotransmitters I mentioned serves a difference purpose in regulating the sleep cycle.  Here's a good paper describing the effects of different neurotransmitters on sleep: http://med.stanford.edu/psychiatry/narcolepsy/articles/natureneuro5.pdf",null,4,cdigdbq,1r0dd7,askscience,new,10
null,null,null,2,cdigblo,1r0dd7,askscience,new,7
MCMXCII,"&gt;And the closer something does travel to the speed of light, the slower time goes relative to that object. Is this correct?

If you're moving at the speed of light *relative to me* **I** see **your** time passing slower.

&gt;So theoretically, if something travels faster than the speed of light, this object would travel back in time.

Something moving faster than the speed of light would violate causality.

&gt;I was hoping someone could explain to me why light and time are directly related.

The speed of light is sort of a ""conversion factor"" between space and time. If you take a time coordinate and multiply it by the speed of light, you make it into a distance in meters, even though it represents a time in seconds (or whatever other units of distance and time you choose).",null,5,cdiccso,1r0cxy,askscience,new,9
The_Duck1,"&gt; I was hoping someone could explain to me why light and time are directly related.

Light isn't special. There is a special speed, called c. Nothing can travel faster than this speed. All massless particles travel at this speed. Light consists of massless particles called photons, and thus travels at c. Light happened to be the first phenomenon discovered that propagates at this speed, so we call c the ""speed of light"" for historical reasons.",null,3,cdil5lv,1r0cxy,askscience,new,4
sloggz,"&gt; Why is it different for light? Using a similar example, if a star explodes, it takes time before that light reaches you. I would assume that if you outran the light, it would only mean that you wouldn't see that happen for a bit, and not that you were outrunning the event itself.

It's really just about your frame of reference. In almost all practical purposes, light IS the event. As far as we understand physics, you can't outrun the light at all. No matter how fast you run away from light, light will still (from your point of view) be moving at the speed of light. 

If a supernova goes off 100 light-years away, when does it make sense to say that the event ""happened"" here. When the first signs of that event become observable? If so, than that's the light. That's the whole ""Relative"" part of relativity. Whether or not you say ""That 100 light-year away supernova is happening now!"" and ""I'm seeing the light from a 100 year old supernova!"" is all... relative.

",null,4,cdiggxs,1r0cxy,askscience,new,4
null,null,null,5,cdiky3p,1r0cxy,askscience,new,2
nimobo,"Objects cannot travel faster than the speed of light. Because if an object travels at the speed of light, its mass becomes infinite. Moving any object requires energy. Infinite mass will need infinite energy to keep moving. Since the universe has finite energy, travelling at speed of light becomes impossible. So you should not compare sound with respect to light.

As an object approaches the speed of light, time is just measured differently for the object and a stationary person observing it. 

Lets say, you are taking a trip, (from point A to point B and back to A), approaching the speed of light, and I am observing you in a stationary position. For me it would seem that you have taken 10 hrs to complete the journey. But for you it will seem you have completed the journey in 6 hrs. So it would appear to me that time has passed slowly for you in completing the journey. This is called time dilation.

",null,7,cdiiz71,1r0cxy,askscience,new,3
juliuszs,"That ""clear screen"" is a metal shield with holes smaller than the wavelenght of you microwave's radiation, so waves get stopped. The metal walls are not going to let you use your walkietalkies, but regular walls are not metal. You do get attenuation due to scattering and absorbtion, but the rf radiation is not stopped by typical walls.",null,1,cdic6gg,1r0bov,askscience,new,4
glarn48,"A pretty interesting question. Intuitively I see no reason why genetics would predetermine someone to like one form of music over another. It seems much more likely to be due to culture and exposure. For example, if your 
parents played Frank Sinatra all the time, it's likely you too will like Sinatra because it's familiar to you. (See http://link.springer.com/article/10.3758/BF03201171 for a short-term demonstration of familiarity effects in music preference)

Is there any evidence for heritability of music preference? This question doesn't seem to be well-studied. A twin study is one of the most commonly ways of looking at heritability. An older study suggests music preference is not heritable (http://journals.cambridge.org/action/displayAbstract;jsessionid=BF7D791BC14FD12F7D16CC48CCFD7C7E.journals?fromPage=online&amp;aid=1362580). I found a press release from Nokia about the results of a pretty broad twin study that suggests a high heritability of musical preferences (http://press.nokia.com/2009/11/12/nature-or-nurture-study-reveals-musical-genes/). I'm very dubious of this study for several reasons, not least of which is that it doesn't seem to be published anywhere peer-reviewed, but also because the listed heritability is so high (for reference, a ballpark estimate of IQ heritability might be 50%).

Heritability isn't necessarily intuitive though. For example, the heritability of IQ is dependent on SES (http://pss.sagepub.com/content/14/6/623.short). That just makes it even more important to have some information about their methods and population of that Nokia study before you can interpret their conclusions. I wish I had a more rigorously scientific study to share, but hopefully someone else will come along with an insight. ",null,0,cdicgay,1r0as7,askscience,new,1
latent_variableZ,"Palmer, Schloss, and Sammartino (2013) stated as introduction to their study on human preferences in harmony: “although empirical research on aesthetics has had some success in explaining the average preferences of groups of observers, relatively little is known about individual differences in preference.” This about as true of a statement as one could make in regards to aesthetics. Neurobiologically, we can observe there is activity in “such and such area” of the brain when “such and such stimuli” are perceived. However, the variation in the subjective experience and idiosyncratic responses related to aesthetics do not lend them self to a “this is fact” statement of why we have different tastes. The best I can do with my background (I am a doctoral candidate in clinical psychology with an emphasis in forensic and neuropsychological testing) is tell you socialization and a person’s cumulative learning history appear to have the largest effect size. The interaction between nature (biology) and nurture (the environment) is complex and most likely leads to the variations we observe in aesthetic taste, learning literally changes the brain.  However, learning appears to be a very powerful factor in determining aesthetics.  Palmer and Griscom (2013) provide a good review of the topic and give examples of studies that demonstrate aesthetic differences according to age, gender, and cultural; the take away while some preferences may be innate (“infants have a bias toward looking at dark-yellow and light-red and a bias against looking at light-blue and dark-green, nearly the opposite of [western] adults”) learning appears to be a powerful factor in determining preferences.  Learning is also implicated in the differences observed between gender preferences (girs tend to like pink/boy tend to like blue), theories tend to overwhelmingly postulate this happens due to socialization.  Historically, records show it was not until the mid-19th century the blue pink dichotomy emerged and became so prevalent.

References: 

Palmer, Stephen E. &amp; Griscom, William S. (2013) Accounting for taste: Individual differences in preference for harmony, Psychonomic Bulletin &amp; Review, 20(3), 453-461 DOI: 10.3758/s13423-012-0355-2  

Palmer, Schloss, &amp; Sammartino (2013) Visual Aesthetics and Human Preference, Annual Review of Psychology, 64, 77-107 DOI: 10.1146/annurev-psych-120710-100504",null,0,cdij457,1r0as7,askscience,new,1
albasri,"I don't have an answer, but there is an emerging field / group of researchers interested in [neuroaesthetics](http://en.wikipedia.org/wiki/Neuroesthetics). I'd start there.",null,0,cdizuds,1r0as7,askscience,new,1
Needless-To-Say,"Are you refering to the idiom of ""A sucker born every minute""?

If so we need to define the percentage of newborns that would be considered suckers.

Borrowing snusmumrikan's data of current birth rates and required births per year and making the (hugh) assumption that birth rates have been relatively constant and are linear with population. My best estimate would be when the world population was roughly 50,000,000 and according to my research this equates to about 1000BC",null,0,cdibryg,1r0agb,askscience,new,3
snusmumrikan,"Where are you getting the one every 8 seconds data? 

The data from the UN I've just looked up seems to suggest 2.5 births per second.

I tried to do some quick rough maths to answer your question but it means finding a year with half a million births, which will be so long ago there won't be any reliable records. ",null,0,cdibbop,1r0agb,askscience,new,2
iorgfeflkd,"Well, what is a magnetic field? A simple definition is that it's a region of space where moving charged objects experience torque. Light isn't charged, so it doesn't experience torque as it moves through a magnetic field.",null,2,cdibahb,1r09mw,askscience,new,9
SingleMonad,"There's not really a reason, at least not in terms of something more basic.

The electromagnetic field obeys the *principle of superposition*, which means that if you have the equivalent of two fields (either both electric or  both magnetic) in the same place, the resultant field is the (vectorial) sum of the two.  

For example, if you have two magnets, when you bring them close to each other you can imagine the two fields adding to each other in the space around them.  Depending on how you orient the magnets, you could make a stronger or weaker field in the common region.

Light is wave.  It propagates because as the electric field changes, it generates a changing magnetic field, which generates a changing electric field, and so on and so on.  This is the content of [Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell's_equations) #3 and #4.  If you aren't put off by an appeal to mathematics, really squint at those equations and you'll conclude that if the fields obey superposition (E_total = E_1 + E_2), then the formulas do too.  That's not an *explanation*, of course.  Maxwell's equations were deduced to agree with experiment.  There exist different physical media that *don't* obey the superposition principle, rubber, for example, if the amplitude is very large.

It would be a very weird universe if EM fields did *not* obey superposition.  I could be watching a movie, and if someone shined a flashlight across my view of the screen, I would see distortion.

At the risk of adding confusion, there is a deep connection between superposition of EM fields, the bosonic nature of the photon, and causality.  I wish I could explain it, but it's a result from quantum field theory, and I don't know a simpler explanation.

**TL;DL:**  That's just how it works.  The fancy name for it is [superposition](http://en.wikipedia.org/wiki/Superposition_principle).

Edit: Spleling",null,0,cdifs54,1r09mw,askscience,new,6
A_Mathematician,"Short answer is that light is not a charged particle and wouldn't be affected by any sort of H-field.  However check out the [Zeeman effect](http://en.wikipedia.org/wiki/Zeeman_effect) and the [Stark effect](http://en.wikipedia.org/wiki/Stark_effect) as an interesting result occurs.  In short, materials that are emitting light, if placed in a static magnetic field, will have their spectral lines (light waves) separate. This is how the magnetic field of stars and other astral bodies are measured.  If I find any of my experiments on it I will reply to my comment with photos in an edit.",null,0,cdimugr,1r09mw,askscience,new,2
chrisbaird,"Fundamentally, magnetic fields and light are just specific forms of electromagnetic fields. All electromagnetic fields are composed of photons. So, fundamentally, your question is really, ""why does one photon not bounce off another photon"". The reason is because they are bosons and they carry no electric charge.

All quantum particles can be classified as fermions or bosons. Fermions (such as electrons), obey the Pauli Exclusion Principle which states that no two fermions can occupy the exact same quantum state at the same location at the same time. So when two fermions approach each other, they must eventually ""bounce"" off each other to avoid ending up in the same quantum state. But bosons, such as photons, do not obey the Pauli Exclusion Principle and *can* be in the same quantum state at the same time. This leads to interesting effects such as lasers, superconductivity, and Bose-Einstein condensates. It also means that two photons cannot directly effect each other. Instead, they just pass through each other. What determines whether a particle is a boson or a fermion is how it spins. Integer-spin particles are bosons.

Also worth noting is that photons themselves carry no electric charge, so they are not effected by each other. Even if a particle is a boson, it can still interact with others of its kind if it carries charge. Such is the case with gluons, which carries color charge.",null,0,cdixln1,1r09mw,askscience,new,1
mc2222,"light is a *changing* Electromagnetic field.  If light travels through a region of space, we can use the analogy of a pond to describe the EM field.  Light is the ripple that travels on the pond after the surface is disturbed.  We can consider the depth of the pond to be how strong the *static* EM field is in this region.  since light is the ripple, the size of the ripple is completely unrelated to how deep the pond is.  So, if light passes through a region of free space with a magnetic field, it is unaffected.

Having said all that, magnetism in a *material* can effect how light travels through that medium.  Basically, the magnetic field effects the matter, and the organization of matter relates to how light propagates through it.",null,0,cdj3fhk,1r09mw,askscience,new,1
diazona,"It's not really so accurate to say microwaves have less energy than visible light. You can have any amount of energy in microwave radiation, just like you can have any amount in visible light. It just comes in smaller increments (i.e. photons) when you use microwaves.

In a nutshell, visible light doesn't work because it gets reflected, whereas microwaves tend to make it into the interior of the food. (Of course that's only the simplest version of the story.) See [here](http://en.wikipedia.org/wiki/Dielectric_heating) for more information.",null,1,cdigaqm,1r091o,askscience,new,9
baloo_the_bear,"Microwaves interact with water molecules and cause them to vibrate. On a molecular scale, vibration **is** heat energy. The heat from the vibration of water molecules heats the food being microwaved. The effect of microwaves only works on water, which may lead to rapid drying of the food, which is why when you over-microwave something it becomes brittle.",null,3,cdib7e7,1r091o,askscience,new,8
goatherder100,"Basically its like microwaves are pingpong balls (lower energy) and light would be basket balls (high energy). If I hit you with one ping pong ball, nothing. If I hit you with a basketball, you notice it. If I pelt you with 10million ping pong balls you will really notice it. Kinda the same thing. It is not the amount of energy EACH photon has to deposit, it is how MANY photons there are to deposit energy. A microwave creates a high density flux of a large number of lower energy photons that will each deposit all their energy into the substance causing the average kinetic energy of the substance to increase. That is what heats it up. ",null,0,cdii8a0,1r091o,askscience,new,5
myarlak,when molecules absorb microwaves it causes them to spin rapidly releasing heat due to the friction of the molecules rubbing against each other.  visible light cause electronic transitions which are not translated into motion thus do not cause friction.,null,2,cdij6qb,1r091o,askscience,new,3
king_of_the_universe,"&gt; Why can we use microwaves to heat our food, when microwaves have less energy than visible light?

But the light of your Zippo isn't as hot as the summer sun, while both are visible light. Yes, a microwave photon has less energy than a visible light photon. No, microwave light doesn't hence have less energy than visible light. Duration of exposure and amount of photons are important, too.",null,0,cdioxr2,1r091o,askscience,new,1
Weed_O_Whirler,"No. Instantaneous acceleration would require infinite force, and infinite force would cause all sorts of bad things to happen. 

So, what is actually happening? Well, you (and really, everyone) is a little squishy, so when you first get hit by the bus you slowly start to move, but moreso you start to deform- getting squished. Thus, you will ""slowly"" (slow being relative here, as it will still happen in a fraction of a second) move up to the speed of the bus. 

In fact, this speed is what matters. That is why if you were surrounded by a squishy ball, getting hit by a bus would not be as problematic, since that time it took you to speed up to the speed of the bus would be longer (and thus, you acceleration smaller)",null,0,cdiablh,1r086u,askscience,new,7
matts2,"You are a compressible being. So the force travels through you at different rates. Think of this: you are bruised on the side you are hit, not all through your body. What happens is that your are hit and there is force transference. Some of that force it too much for the local bonds and they break (broken blood vessels, broken bones, etc.) so moves though you and accelerates your body.",null,0,cdibs6x,1r086u,askscience,new,3
nonchalantkiwi,"No, not instantaneously. Someone getting hit by a bus is a great example of how Impulse works. Impulse is the force exerted on a particle (in this case, the bus exerting a force on a person) for a certain amount of time. Now with this collision, the time will be very small, but not instantaneous because if it were instantaneous then the impulse would be infinite, which is impossible.  ",null,0,cdiadfm,1r086u,askscience,new,2
Platypuskeeper,"Salt water is denser. As for how that affects buoyancy, that should be easy to figure out through [Archimedes' Principle](http://en.wikipedia.org/wiki/Archimedes%27_principle).
",null,2,cdib9j1,1r084w,askscience,new,6
Gargatua13013,"An subset of this problem is central to the study of the genesis of ore deposits associated to black smokers, namely what happens to the buoyancy of waters of grossly different salinities and temperatures when they interact. There is a seminal paper on this by Takeo Sato, (1972), ""*Behaviours of ore-forming solutions in seawater*"", Mining geology, **22**, 31-42.

Pressure is also an important variable. Turns out in the right circumstances, the brine plume will be denser than seawater and collapse to the seafloor where it may form a brine pool, which was observed in nature years after Sato's paper.",null,0,cdifh1g,1r084w,askscience,new,1
FizixPhun,"http://en.wikipedia.org/wiki/File:CNTnames.png

Basically, it gives you the direction in which you cut the graphene to get the end of the nanotube.  Think about m/n as a slope.  In Cartesian coordinates I can represent the slope in a similar way, (a,b).  This would mean that for every a you move along the x axis, move b along the y axis.  For instance, (1,2) would mean that if I moved x by 3, I would need to move my y by 6.  The only difference with the carbon nanotube notation is that the lattice vectors a1 and a2, the equivalent of the x and y axis, are not perpendicular.  This is illustrated nicely in the wikipedia picture.",null,0,cdif596,1r07ep,askscience,new,2
armour_de,"The indices n and m define a direction along the graphene sheet that the carbon nanotube could be considered to be rolled along.

This is done similar to how a cartesian vector can be written as (3,4) or v=3x+4y where x and y are the unit vectors defining the direction of the x and y axis.

The hexagonal structure of graphene has a natural set of directions related to the points on one of the hexagons in the perfect lattice.  [This](http://en.wikipedia.org/wiki/File:CNTnames.png) image from wikipedia shows a_1 and a_2 in the upper right.

The (n,m) values for a carbon nanotobue then makes a vector v=na_1 + ma_2 .

This choice of representation is used as it allows the indices n and m to be integers. Cartesian vectors (n^' ,m^' ) could be used to define identical angles but then n^' and m^' would rarely be integers.

The [carbon nanotube article](http://en.wikipedia.org/wiki/Carbon_nanotube) wikipedia has some more details on this near the start of the single walled section.",null,0,cdifqjh,1r07ep,askscience,new,1
homininet,"Sure, particularly in primates and great apes. There are several instances in which human viruses and diseases have been passed to chimpanzees or gorillas from humans, particularly researchers and ecotourists. These include things like respiratory viruses, polio, a disease called yaws, and potentially ebola. Many of the field sites where primatologists study gorillas and chimpanzees require masks whenever people get close to the animals to try to reduce the risk of spreading human-bourne diseases (check out the picture on this webpage: http://www.conservenature.org/learn_about_wildlife/chimpanzees/chimp_human_disease.htm)

Heres a link to a news article about a recently published study of this very issue: http://www.livescience.com/9565-human-viruses-kill-great-apes.html

",null,1,cdidd04,1r077s,askscience,new,7
abstrusey,"Veterinarian checking in: we call this ""reverse zoonosis"" or, less commonly, ""anthroponosis"". Just for clarity's sake, the term zoonosis should be sufficient, as it means transmission *between* humans and non-human creatures, without indicating directionality; however, the phrase has stuck.  Reverse zoonosis is incredibly common with a large number and variety of diseases. The purest examples come from infectious agents that do not cause disease in humans but do cause disease in animals; for example, there is a fungus called Batrachochytrium dendrobatidis that humans (who carry the agent without any disease symptoms) can transmit to frogs. The fungus is often deadly to the frog if no treatment is provided.

A more common example is found in the 2009 outbreak of the H1N1 influenza virus, poorly nicknamed ""swine flu"". It has had at least 13 confirmed case of reverse zoonosis to cats, 1 confirmed case to a dog, and a number of suspect cases to ferrets (between 2009-2012), according to the Veterinary Pathology journal. The actual number of cases was likely many fold higher. There were also many healthy pigs who caught H1N1 from a human.

Primates tend to be at a high risk of catching an infectious agent from humans, because of our similarities. Metapneumovirus is a common cause of human respiratory tract infections, and it has been implicated as a fatal respiratory disease in chimpanzees in captivity and the wild. Other examples can be devastatingly fatal, such as the Ebola virus.

There are many more examples, including: tuberculosis in elephants, the human gut microbe Serratia marcescens which causes white pox disease in elkhorn coral, Methicillin-resistant Staphylococcus aureus can be shared with household pets, Helicobacter pylori historically and bafflingly wiped out many research colonies of stripe-faced Dunnarts, herpes simplex virus affects marmosets and tamarins, human mumps causes canine parotiditis, human diptheria causes mastitis in cows, hepatitis can be passed to non-human primates... the list goes on, and on, and on...

As a fun fact to end it all: approximately 60% of all human pathogens are zoonotic, and greater than 75% of all new and emerging infectious diseases are zoonotic.",null,0,cdijjom,1r077s,askscience,new,5
xavier_505,"SMS message are effectively sent as a individual 'transaction' which requires paging, channel setup and message transmission in both directions. This is done in a similar way to how phone calls are connected, and it takes time to perform. 3G and 4G Internet connections are either always 'connected' (eg: UMTS) or are in low-latency standby modes. Additionally, SMS messages are not necessarily treated with great urgency by baseband processors or applications since the protocol was never intended to provide low-latency.

2G packet-switched data (eg: GPRS) also takes a bit of time to setup if they are in a disconnected state, though only one end (the user) has to do this channel setup. The network's GGSN is connected to the rest of the internet via low-latency means.",null,1,cdimzfa,1r0725,askscience,new,4
amnesiajune,"In short (well... as short as I can make it): SMS has to travel directly from you to the recipient through protocols that were defined and standardized in the 1980s (GSM and CDMA). These protocols can't be updated because they have to support any 2G cell phone. Any online message goes through a server at Facebook/Apple/BlackBerry/etc.. The way their messages are sent aren't standardized, so they can upgrade their servers and the apps that you and the recipient use to send/receive messages.

Even though your online message has to go through a third party, technology has advanced so far beyond what was conceivable in the 1980s that it's faster to go through a third party server than to go directly between phones using technology that was developed in the 80s and 90s",null,1,cdibc73,1r0725,askscience,new,2
gabesubdo,"All network devices, including wireless routers, have a unique ID called a MAC address. The MAC address of an access point is one of the things that gets broadcasted out in the open, so you can see the MAC addresses of all nearby access points even if you're not connected to any of them.

Several companies have, through different means, mapped out the geographical coordinates of as many access points as they can. For example, Google used to have a device in their Street View cars that would also record the addresses of all the access points they drove past (they might still do this, I'm not sure).

So when your iPhone has WiFi turned on, it can make a list of the access points nearby, send those MAC addresses to Apple (or another company providing this service), and get an approximate location back, if the company has seen one of those access points before. Your phone might even cache some of this data so it can do the lookup without sending data to Apple.

It's not the most accurate way of getting a location, but it's another method the phone can use if GPS isn't available. Otherwise, you'd have to go by the closest cell tower, which is very inaccurate.",null,0,cdibbne,1r05xd,askscience,new,3
lamboleap,"Cell towers are probably spread farther apart than the wireless access points at your school. Because there is greater distance between cell towers, your phone probably estimates the distance to your destination based on the closest tower (strongest signal). Now apply that idea to your school's routers. Because the internet at your school is affiliated with a specific location (your school), it can give you a closer estimate to the location on your map than the triangulated cell signal.",null,4,cdib51e,1r05xd,askscience,new,2
justin3003,"Two major elements allow for macrophage resistance. 

* Cell wall: The properties of mycobacterial cell walls make them very hearty organisms generally. Mycobacteria have mycolic acids in their cell walls, which make them resistant to many different types of degradation/destruction, including macrophages (by being resistant to lysosomal acidification and enzyme degradation) and many antibiotics. 
* Anti-ROS enzymes: Many mycobacteria, including M. tuberculosis, produce catalase (which turns hydrogen peroxide into water and oxygen) and superoxide dismutase (converts the superoxide radical (O2-) into hydrogen peroxide and oxygen). Without reactive oxygen species (ROS) activity, macrophages have few weapons left to use.",null,1,cdibm0j,1r02eg,askscience,new,9
chocolate_powder,The tape on the inside is magnetic. By altering the tape's magnetization it will produce an electromagnetic pulse when moving quickly past a playback head. The cassette will either play the music or rearrange the particles to reflect the music on the tape to record based on the orientation of the playback head.,null,1,cdieuny,1r028e,askscience,new,3
afcagroo,"/u/chocolate_powder answered this, but I found his wording a bit difficult to decipher. So here it is again, worded differently: 
 
The tape in an audio cassette is coated with a material which is susceptible to magnetization.  To record on such a tape, it is moved over a write head which contains an inductor (coil). The inductor creates a changing magnetic field as the electrical signal is passed through it, and that changing signal is recorded on the tape as it moves over the head. The coating becomes magnetized, with the direction and strength of the magnetism being an analog representation of the audio signal that was run through the write head's inductor.  (The audio signal could have come from a microphone, or a guitar pickup, or whatever.)
  
The playback process is the opposite. As the tape moves over a read inductor (coil), the varying magnetic orientations/strengths induce an electrical current.  That current is amplified and sent on its way, to be played back as an audio stream through a speaker. 
   
Note that a cassette recorder can have two separate heads, one for writing and one for reading, or they can be combined together.     ",null,0,cdj7lpo,1r028e,askscience,new,1
ScintillatingWit,"The effectiveness of silver compounds as an antiseptic is based on the ability of the biologically active silver ion (Ag+
) to irreversibly damage key enzyme systems in the cell membranes of pathogens. http://www.ncbi.nlm.nih.gov/pubmed/16766878",null,0,cdi71rf,1r00tq,askscience,new,8
SneakerofSneaks,"The highly reactive nature of the silver ion.

Because of the outer shell configuration, it reacts with most organic compounds.  It also has the ability to form numerous inorganic and organic compounds.  Because of this, it's toxic to bacteria, yeasts, and at larger quantities is toxic to us.",null,2,cdi7d2m,1r00tq,askscience,new,6
medikit,"Silver also appears to make antibiotics more effective:

http://www.nature.com/news/silver-makes-antibiotics-thousands-of-times-more-effective-1.13232
http://stm.sciencemag.org/content/5/190/190ra81",null,1,cdjr25i,1r00tq,askscience,new,3
nimobo,"Bulb flickering occurs due to either faulty connections or voltage fluctuations. Storms cause faults in the circuit resulting in power surges and fluctuations. This is why you see flickering. The bulb does not go out unless the circuit is broken, commonly happens when trees fall on the power lines, or the bulb itself burns out.",null,0,cdif7su,1qzx32,askscience,new,9
threegigs,"Big reason number 1: Titanium implants aren't rejected by the body.

2: A high strength to weight ratio, with ~~low~~ [edit] **high** modulus (not too flexible).

3: Not corroded or degraded by bodily fluids.

4: Requires no maintenance (fit and forget).

5: Osseointegration (will readily join with bone tissue).",null,3,cdi890y,1qzwpq,askscience,new,10
SneakerofSneaks,"It's a strong metal the resists corrosion very well.  It is slow to react with water and air.  It's generally non-reactive when it's temperature is in the range of the human body.  (Although it's considered highly reactive in the thousands of degrees range, the human body of course does not operate at those temperatures).  It is easily molded, plated and coated with as well.",null,3,cdi73qu,1qzwpq,askscience,new,2
botanist2,"Yes.  There are seasonal differences in oxygen production in the Northern and Southern Hemispheres because plants are dormant in the winter, but the extent of the fluctuations depend on scale.  At a local scale there might be a lot more seasonal variation than there is on a global scale, simply because there's still a lot of ocean and tropical areas where photosynthesis is still occurring, and that reduces the impact of dormancy in other regions. 

Edit: One of the most cited papers involving global variation in O2 and CO2 concentrations in the paper done by [Keeling and Schertz](http://132.239.121.69/publications/ralph/3_Seasonal.pdf) from 1992 that I thought you might find interesting",null,0,cdi5znl,1qzto7,askscience,new,6
darkness1685,"Absolutely. If you look closely at the famous CO2 graph from Mauna Loa, you can see atmospheric concentrations fluctuate on an annual cycle. This is due to global vegetation being more productive in warm seasons than cold. The same is true for Oxygen, but in the opposite manner of course.
http://www.skepticalscience.com/images/CO2_vs_oxygen.gif",null,1,cdia3nc,1qzto7,askscience,new,6
Blah2435,"While the amount of oxygen does drop a bit, around 70-80% of oxygen comes from plankton, algae, and other things that thrive in the ocean. So although it does change the amount of oxygen some, it definitely is not the biggest producer of oxygen so it doesn't effect the amount as much as you would think.",null,0,cdin7cd,1qzto7,askscience,new,2
iorgfeflkd,NGC 1277,null,1,cdi5527,1qzsxi,askscience,new,4
owaisofspades,"oh did you mean heartbreak as in getting dumped and/or emotional shock? My bad, I thought you meant a heart attack. In this case it might be Prinzmetal's angina, which is a stress-induced chest pain (similar to heart attack pain). This happens because stress can cause your coronary artery to spasm, causing ischemia (loss of blood flow) which induces pain.

As far as the stomach is concerned, the shock of having our heart broken could potentially overstimulate your vagus nerve causing a drop in blood pressure (which  can give you that sick in your stomach like your going to puke feeling) Not 100% sure is this is what you meant by stomach pain though",null,2,cdi9qws,1qzriw,askscience,new,4
KarlOskar12,"The discomfort is a result of stressing the Parasympathetic nervous system (PSNS). During high emotional stress the PSNS can get over-activated causing pain in the chest cavity and stomach. The PSNS is responsible for the digestive processes so it innervates the esophagus, stomach, intestines, etc.",null,0,cdixgnx,1qzriw,askscience,new,2
owaisofspades,"Your heart is innervated by the phrenic nerve, which has the spinal roots C3-C5. C5-T1 are the roots for cutaneous innervation of your upper limb. Because of the overlap in nerve roots, the pain in your heart radiates to other parts of your body innervated by the same nerve roots, in this case your pectoral region and arm",null,2,cdi9b5z,1qzriw,askscience,new,3
sfoglia301,"This would be ""referred pain"".  This is because the nerves that innervate the heart are ""visceral nerves"" and are not very precise as are sensory nerves in the skin . As a result, if your heart is ""hurting"" in a heart attack, you feel visceral pain throughout the dermatome (level of the nerve) where the heart's nerves arise in the spinal column.  In humans, that is in the chest and left arm.  ",null,3,cdi9251,1qzriw,askscience,new,2
snusmumrikan,"Tanning hasn't changed your phenotype because it has nothing do do with the alleles in your genes. It is an environmental change, just as with the giraffe an abundance of nutrients is more likely to be responsible for the growth spurt than anything else, nothing in the genes or allelic ratios has changed and therefore the 'phenotype' has not changed. Phenotype is the visible part of gene expression, it doesn't encompass 'absolutely everything you see'. A tattoo has not changed your phenotype. 

What you're hinting at is called Lemarckian evolution which was proposed by Jean-Baptiste Lemarck in the 18th century. He wondered if, say, chopping off a leg of an animal would lead to three-legged offspring. Obviously geneticists now see that as obviously wrong due to the nature of hereditary gene transfer to offspring. 

More recently this idea has been recalled with respect to epigenetics: the hereditary transfer of genetic activity which is not associated with the actual DNA sequence itself (you can inherit differently packaged/masked/expressed DNA which may be expressed in a different manner to someone else with the identical DNA sequence).

As a potentially interesting aside you might also want to look at DNA polymerase V. This DNA pol is part of the DNA SOS response (for double-stranded damage) and as a last resort may ignore damaged or slipped sections of DNA, essentially excising them and re-ligating the pieces at either end - which if carried through into the gametes would result in a heritable lack of removed DNA sections, a molecular version of exactly what Lemarck predicted over 200 years ago.

(Edited for spelling mistakes)",null,0,cdi4xmq,1qzqrc,askscience,new,6
KarlOskar12,"Let's take the giraffe example...If a group of giraffes lives in an area with a food source found very high off the ground only the giraffes with very long necks (which will only be giraffes with very long neck genotypes) will survive. They will have babies who also possess the very long neck genotype and over time the average neck length of giraffes in that area will increase. The variation in genotype doesn't come from one individual giraffe, but the genetic variation found amongst various giraffes.",null,1,cdi4605,1qzqrc,askscience,new,6
bwc6,Random variation combined with natural selection. [source](http://www.literature.org/authors/darwin-charles/the-origin-of-species/),null,0,cdj5qtt,1qzqrc,askscience,new,3
atomfullerene,"Basically, the stretching or tanning or what have you doesn't directly result in new adaptations.  Instead, think about the giraffe egg which mutates, eventually producing a giraffe with a 2cm longer neck.  That giraffe goes throughout life getting a bit of extra food, has a few extra babies (some of which have 2cm longer necks).  The longer necks result in more babies on average.  Rinse and repeat until all the giraffes have 2cm longer necks....and then wait for another mutation to occur.",null,0,cdi94w9,1qzqrc,askscience,new,2
darkness1685,"Natural selection can be viewed as a sorting mechanism. The individuals who have a genotype that results in the most well-adapted phenotype will have greater reproductive success (i.e., produce more offspring). Therefore, the favorable genotype will become more common in the population, and the less favorable genotype will be sorted out. Phenotype cannot influence the genotype of a particular organism, but it can influence the average genotype of the population as a whole through time. Avoid Lamarckianism! ",null,0,cdiabj2,1qzqrc,askscience,new,1
hikaruzero,"I don't think the discovery of the Huge-LQG has casted very much *doubt* on the cosmological principle, but it *has* called it into question for further analysis.

The identification of any structure large enough to violate the principle is definitely troubling, but the existance of such a large structure is not *impossible* assuming the cosmological principle, it is just very unlikely.  We could be looking at an anomaly that is just not very probable -- after all there's an entire observable universe for improbable things to occur in -- but we should also keep in mind that the Huge-LQG dataset is quite small overall (73 quasars).

The Arxiv.org paper you linked to (from which your second quote is from) also does indeed show that (paraphrased) the algorithm used to identify the Huge-LQG regularly produces false positives for identifying large structures, and that the Huge-LQG may actually be two or more smaller structures nearby, or may not even really be a ""structure"" at all.

It's definitely still too early to throw the cosmological principle out, considering the wealth of data that supports it.  Just because one counter-example is found doesn't mean the statistics are necessarily wrong.  What we'd really need are other independent observations that also show a violation of the cosmological principle.

Hope that helps.",null,0,cdi5yib,1qzqjp,askscience,new,2
Surf_Science,"1) Race is rather arbitrary, irrelevant, and somewhat silly. There are no hard lines between human ""races"" everything very much exists along a continuum. With some populations (which again will have muddled borders) with higher levels of homogeny (people are more genetically similar) you may see elevated levels of some genetic disease. This is particularly common with founder populations where a small group of people are the genetic ancestors of a large group (example 8 million people in Quebec are largely the progeny of 3,000 people). 

2) With respect to immunological diseases the answer may cut more ways. Higher levels of diversity may allow the MHC molecules of the individuals to respond to a higher diversity of antigens. Conversely infectious disease have a tendency to have reduced virulence in the populations they are derived from. Because of this it is possible that some individuals of a mix race background could loose the benefit of this attenuated virulence. With respect to this attenuation I believe that TB is a good example with Africans for example having reduced outcomes when infected with Asian derived TB. ",null,6,cdi6qrb,1qzqit,askscience,new,14
heresacorrection,"I would say **yes**. The more diverse an organism's background the more likely they are to be heterozygous for a mutation in any given gene or regulatory region. Knowing this and the fact that most genetically inherited diseases, both Mendelian and non-Mendelian (that we know of), are recessive rather than dominant would lead us to the conclusion that more diversity (assuming that is what differentiates between races) decreases one's chance of having those diseases. 

This would be further supported by the many studies showing that out-crossing helps avoid genetic bottlenecks and can help negate a loss of variation in a population (e.g. due to a founder effect).

EDIT: For this question to further define the diversity of having ""different"" races, I would push for a definition where we might say that different haplotypes could be thought of as derived from different races. I agree with *homininet* (below) who suggested we use ""ethnicity"" instead.",null,0,cdine0v,1qzqit,askscience,new,3
owaisofspades,"Are you asking whether mixed-descent people are more likely to have genetic disease, or whether they are more immune to certain disease?  The answer is different for each.

Surf_Science mentioned founder effects in Quebec, and founder effects are important to your question. For example, it's widely known that Ashekenazi Jewish populations have a higher prevalence of many genetic diseases because their population has alot of carriers for lots of different mutant alleles. So theoretically, if you are of half-ashekenazi jewish descent and half caucasian, you would most likely have less mutant alleles through inheritance than someone who is of full ashekenazi jewish descent and therefore you could say that half jewish half caucasian populations would theoretically have less prevalence of genetic disease than 100% ashekenazi jewish populations (but still more than 100% caucasian populations)

As far as resistance is concerned, the opposite would be the case. Two examples of alleles that increase resistance to malaria are sickle cell trait (common in africa) and G6PD (common in africa, middle east, and south asia). In this case if you are mixed african-south asian, you're more likely to carry one allele each of two different genes which both give you malaria resistance than someone who is just south-asian, but this increased resistance to malaria would come with the complications that go along with being a carrier for sickle cell. and just FYI having two alleles of G6PD and/or sickle cell is bad (although sickle cell is much worse)

hope this helps!",null,0,cdi8nrl,1qzqit,askscience,new,2
ModernTarantula,"the question can be separated. First race has to eliminated, rather refer to populations. Next genetically related diseases are not the same as immunity. Lastly hardiness in plants is not the same as immunity. So genetic disease are present much more in specific populations--thalassemia, Tay Sachs, Factor V Leiden, cystic fibrosis--plenty more. Immunity is more complex, there are specific inherited immune diseases (boy in the bubble). But subtleties in immunity is not a hard science.",null,0,cdio3cx,1qzqit,askscience,new,1
mousicle,"The chance of this happening are pretty slim.  Most of the man made material in space is not relatively far from the Earth.  Geostationary orbit, the farthest you would practically place a satelite, is only a little more then 10% the orbit of the moon.  At those distances the gravity of the Earth dominates the gravitational attraction of the debris by orders of magnitude.  So the chances of all those bits mutally attracting and forming one body before they are pulled back to the Earth is virtually zero. ",null,0,cdi7bdh,1qzp06,askscience,new,3
SimpleBen,"On the fingertip, you can sense the orthogonal displacement of your skin with a detection threshold of roughly 25 microns at 0.5 Hz (a 2 second smooth displacement). At 60 Hz  for a half second that threshold is under 10 microns, and it drops to 1-2 microns at 250-300 Hz.   
  
For movement parallel to the surface of the skin, stimuli a few orders of magnitude smaller (around 10 nm) can be detected simply because they drag the skin from side to side.    
http://jn.physiology.org/content/81/4/1548.short  
  
Of course, based on this 1999 study published in the Journal of Neurophysiology, scientists created another study to demonstrate the same level of sensitivity and published their results in Nature. And the reviewers must have been very ignorant not to notice this.",null,0,cdi6z76,1qzjrl,askscience,new,13
Mushucanbar,A very good article was published recently in Nature on the topic of the limit of our [tactile perception](http://www.nature.com/srep/2013/130912/srep02617/full/srep02617.html). Interestingly it was found that our sense of touch is sufficiently sensitive to detect nanoscale surface features as small as 10nm. ,null,0,cdim5u0,1qzjrl,askscience,new,2
yeast_problem,"I was thinking that the rain drop phenomenon was a perception of heat energy, i.e the small drops rapidly cooled a spot of skin and you are detecting the heat loss not the physical size of the object.

So I went to google smallest temperature difference skin can perceive and accidentally found [this](http://psycserver.psyc.queensu.ca/lederman/013.pdf) research showing how skin temperature affects the perception of roughness.",null,0,cdinthx,1qzjrl,askscience,new,2
null,null,null,2,cdi5cew,1qzjrl,askscience,new,2
patchgrabber,"From my experience with bacteria and phages, you are describing a lysogenic infection. The lambda phage is an example of a phage that inserts its DNA into *E. Coli* but does not kill the cell because of a repressor gene that keeps the phage from hijacking the bacterium's machinery.

The interesting part of some viruses is that they have an enzyme called a [site-specific recombinase](http://en.wikipedia.org/wiki/Site-specific_recombination), which targets a specific DNA sequence 20-300 bp in length. This recombination isn't random, so no important genes are affected. If it were a cell in a human body, and there were a virus that inserted itself into an important gene, the cell would likely just lyse, since it cannot remain viable. This insertion could also trigger oncogenes and cause cancer, so there are a variety of possibilities depending on which gene is disrupted. HIV inserts its DNA randomly, but acts much like a lysogenic bacteriophage otherwise.",null,1,cdi2rq8,1qzhuk,askscience,new,6
snusmumrikan,"Many viruses only transiently express their DNA instead of integrating into the host genome, and these are often used as vectors to shuttle DNA into cells by molecular biologists. It might seem like an inferior system for the virus as the DNA doesn't last as long as it would if properly integrated, but on the other hand it runs no risk of inserting into a vital gene region as you so rightly point out, so it has benefits and disadvantages (not that I'm saying viruses have thought about this).

As a virus which kills its host is a stupid and short-lived virus and as such they generally do not 'want' to kill off the cells they enter. Many rely on specific conserved sites, such as the AAVS1 site used by commonly used adenoviruses (PNAS. 1990 vol. 87 no. 6 2211-2215), which do not kill or inhibit the growth of the host cell. This helps maintain the propagation of the virus DNA and the insertion itself is not disease-causing.

Other viruses do cause disease by insertion but cause rapid cancerous growth, a method of quickly propagating the amount of viral DNA. These oncoviruses insert oncogenes which tend to push the cell into a proliferative stage of the cell cycle. For example HPV viruses, which you will know from medical advice can cause cancers, insert proteins E6 and E7 which inhibit tumour supressing host genes (Cell. 1990 Dec 21;63(6):1129-36.)

Other viruses will insert randomly and may or may not cause damage to the cell. To bring it back to your question, viruses often interfere with genes important to the host (and this includes inserting or disrupting the flanking regions which may be responsible for repression/promotion or other regulation of the gene), the thing you want to look for is whether this is beneficial for the propagation of new virus particles or not.",null,0,cdi5qxp,1qzhuk,askscience,new,3
sfoglia301,"Well, the best human example that I can think of is Hepatitis B Virus.  In this virus it is possible to get liver cancer without any liver damage/cirrhosis.  This is because HBV is a DNA virus that integrates into the genome.  If it integrates into an important tumor suppressor, boom, loss of function and you get liver cancer.  ",null,0,cdi90my,1qzhuk,askscience,new,2
gettingoldernotwiser,"Lots of examples of genome insertions which are non-lethal (to the host cell).  Virus-induced cancer is an example of a mutation which actually gives the host cell an advantage in replication.  Good for the host cell and the inserted viral genome, though bad for the organism in general.

If a viral genome were to have evolved which inserted itself into a gene which is REQUIRED for host cell survival, then the host would die and the viral DNA would not be propagated.  Not a good survival strategy for the virus, so it's not seen in nature.",null,0,cdiezu6,1qzhuk,askscience,new,2
bwc6,"&gt; Does the E coli just die?

Yes, that cell would probably die from the lack of mRNA.

&gt; What if it was a human instead of an E coli?

The individual cell that was infected would die. This would probably prevent the virus from spreading throughout the person. Viruses can cause problems for people if they interrupt important genes. Others in the thread have mentioned cancer as a good example.

&gt; how do virus genes pick where they're inserted into the host genome?

There is a ton of variation here based on the type of virus. For some viruses the insertion site is random. Others have very specific sites depending on site-specific recombinase, mentioned by patchgrabber. ",null,0,cdj4zsn,1qzhuk,askscience,new,1
Claclink,"the sonic boom created by a bullet is a strong enough shock wave to kill bacteria.

http://business.highbeam.com/137753/article-1G1-94870619/boom-youre-dead",null,465,cdi9p7l,1qzhu7,askscience,new,1823
ArmyOrtho,"Army orthopaedic surgeon here.  This is a very good question, and one I get asked all the time.

Short answer: no.  Bacteria on the end of a bullet is still infectious in a wound.  You'd think that because it's going fast enough and it gets hot enough that bullets are sterile.  This has been proven time and time again to be false.  

COL Louis A. LaGarde in 1903 performed a study where he took .30 calibre rifle bullets (rifle = high velocity, supersonic), dipped them in anthrax and shot cows.  The cows lived and contracted Anthrax.

I've taken countless numbers of bullets and fragments out of bodies and as a rule, each of them are treated as if they are infected.  The treatment is different for low velocity versus high velocity, but the principle remains = they are all treated as dirty wounds.

Long story short - bullets aren't sterile.

http://archive.org/stream/gunshotinjuriesh00lagauoft/gunshotinjuriesh00lagauoft_djvu.txt",null,52,cdi5772,1qzhu7,askscience,new,435
pthors,"You can transform yeast via microprojectile bombardment.
http://www.ncbi.nlm.nih.gov/pubmed/2836954
Basically, it's a shotgun blast with DNA coated tungsten or gold nanoparticles impacting a paste of yeast cells spread out on an agar containg petri plate.  There's a massive kill zone at the center of the plate where the particles hit, with survivors who receive DNA being away from the center of the blast.  So, at least in that circumstance it's pretty easy to kill a lot of microogranisms with a ""bullet"".  Might not be exactly relevant to the question, but interesting all the same.",null,34,cdi5u7o,1qzhu7,askscience,new,183
CocaineSmellsSoGood,"Former ballistic technician at one of the worlds leading labs here. There are too many variables to be calculated without running a serious simulation. 

1. Millions of organisms and their individual traits to consider. 

2.The projectile speed and design. A superheated area of gases exist around most projectiles in flight, as well as a very strong shockwave coming from the leading edge of the projectile. Possibly keeping anything from coming in contact with it. 

3. Centrifugal force. All bullets spin violently for stabilization in flight. It would take a lot for new airborne organisms to overcome the centrifugal force AFTER penetrating and surviving the shockwave. I would ponder the slower the bullets velocity the better the chances are of it happening. A musket ball out of a muzzleloader traveling 450-850fps? Probably. Anything else traveling faster? Doubtful. An exception may be shotgun loads. Lots of spaces to hide, tons more surface area, and relatively slow speeds in the ballistics world. Edit: words.",null,10,cdi4wur,1qzhu7,askscience,new,59
borthuria,"Many information would be needed : First  what is the pressure around the bullet trajectory : since the bullet is travelling at high speed, there is a high pressure in from of it and low pressure behing it. Something like [this](http://youtruth.weebly.com/uploads/1/3/1/8/1318459/504309298_orig.jpg?164) :

Second, we would need to know the pressure needed to ""break"" the microorganism : depending on the microorganism, they all have their own characteristic and they don't react to pressure differently, since they are airborne, I think we COULD assume they all ""break"" at the same pressure.  (I would need to confirm this from a biologist, since airborne microorganisms could be fungus, bacteria or Viral)

Third, it would depend on the bullet you fire and it's velocity.

It is more a question to ask a biologist then a phycisit, the phycisist in me ask the bioligist this one :

""what would happen if a microorganism would be put in a pressure gradient""",null,13,cdiawrp,1qzhu7,askscience,new,51
edge000,"If we are talking about the temperature of the bullet being the mechanism for microbial inactivation then we are talking about a process similar to pasteurization. 

The answer is definitely yes. The heat generated from a bullet being fired will kill microorganisms. This isn't a terribly interesting question from a microbiology standpoint, it is easy to kill some microorganisms.

The real question you want to ask is how many you are killing. That fact of the matter is that even killing 90% of microorganisms in the trajectory of a bullet isn't terribly effective, due to the rate at which microorganisms can multiply. 

When you talk about inactivating microorganisms, it is measured in terms of logarithmic reduction - 90% =1 log vs. 99.9% = 3 log vs. 99.9999% = 5 log; etc. 

The other concept to keep in mind when we are talking about heat is that any temperature over ~ 50C can kill microorganisms, the question becomes *how long are they exposed to it.* 

Going back to [pasteurization](https://en.wikipedia.org/wiki/Pasteurization) - this was designed to be a 5 log reduction of viable microorganisms in milk. There are different mechanisms, but they basically balance time of exposure and temperature. The lower the temperature, the longer the product needs to be exposed to it, and vice versa. The most common type, [UHT](https://en.wikipedia.org/wiki/Ultra-high_temperature_processing) utilizes 135C for about 1-2 seconds. 

I looked around some to try and find the temperature of fired bullets and landed on what amounts to an [advertisement for thermal camera.](http://www.advancedimagingpro.com/print/Advanced-Imaging-Magazine/Infrared-Camera-Measures-Bullet-Heating/1$180). The image they had showed temperatures ranging from 170C to &gt;500C at different points on bullet. To determine what extent organisms are destroyed you need to know the organism, the temperature and how long it is exposed to it. 

If I had to guess for the length of time a bullet is in the air, 170C may not achieve 5 log reductions whereas the 500C probably would, at least. This doesn't take into account the bullet cooling along its trajectory. ",null,3,cdi4w47,1qzhu7,askscience,new,28
realised,"I cannot locate any published sources regarding microorganisms in the bullet's trajectory but regarding microorganisms present on the bullet itself can be found [here](http://www.ncbi.nlm.nih.gov/pubmed/621766).

It only looks at one specific type of microorganism as well as only low-velocity bullets (unsure how they differ as not a gun person myself).  So, other microorganisms may be impacted differently.",null,7,cdi4lcu,1qzhu7,askscience,new,17
null,null,null,30,cdi8mks,1qzhu7,askscience,new,36
Yannnn,"Your question is akin to 'Do human bullets kill people'. The answer is yes, but not always.. it depends. My educated guess would be that larger organisms will break apart from the shock. But smaller bacteria will probably be rather untouched: the bullet will go too fast to properly transmit any heat and their small bodies will have relatively little mechanic stress.

It may surprise you, but guns are actually used in genetic engineering. [Have a look here for a 'Gene gun'.](http://www.youtube.com/watch?feature=player_detailpage&amp;v=8kS5TOLRKdc#t=44) If my memory serves me correctly, a large percentage of organisms die during the bombardment, approximately 50~70%. And of the surviving cells only a small percentage have the 'new' gene, approximately 0.1%. But this usually is more than enough.

edit: If you down vote me it would be nice to know why. I'd like not to make the same mistake twice. Thanks!",null,5,cdiiycv,1qzhu7,askscience,new,12
locosenor,"Odds are that bullets under the speed of sound, the micro-organism will simply be pushed away by the force of the bullet just like a small bird being pushed out the way by the force of a truck. The reasoning behind that would be that when something cuts through air like a car or bullet there is still a tiny layer of air around the tip http://i.dailymail.co.uk/i/pix/2009/03/26/article-0-041778A6000005DC-509_960x564.jpg (best exemplified by that).
That small layer of air will usually push substance away from the speeding organism or being (due to the low speed of the object) before it actually hits the speeding object often saving it's life. 
But if the bullet would break the speed of sound (guns that have a fifty-caliber bullet like a Barret M107) the micro-organism will be killed. The shock alone would kill it and that little bubble won't be enough to stop it as the speeding bullet would be too fast.

Edit: Forgot to mention that the heat caused by air resistance and the gunpowder igniting the bullet would instantly sterilize the bullet (killing everything in it's way)",null,0,cdi6e4u,1qzhu7,askscience,new,5
shaffer620,"OK, so the bullet can kill the microbe if it can create a sonic boom. The bullet can also kill the microbe if it heats up warm enough and allows for a long enough exposure to transfer the heat. However, I believe the main question that activeNeuron wants answered is if the bullet can kill the microbe just by impact. like if the bullet was shoot and while traveling through the air it hit a bunch a microbes. Would that be enough to kill it? for that question I Have no idea.",null,3,cdiftr2,1qzhu7,askscience,new,6
knobtwiddler,"this can only be answered properly by experimentation. because it depends on the type of microorganism (virus, bacteiria, amoeba?), and where in the bullet trajectory the organism finds itself.

... but, it's going to end up being some proportion killed, damaged, and unharmed based on proximity to the muzzle, characteristics of the gun and bullet that might heat it up; many factors that will determine whether an individual microbe would survive or not.  

At the end of the trajectory, the bullet velocity will be a fraction of the muzzle velocity, and the bullet may have cooled down, so some microbes might survive any surface heating from being stuck in the microscopic texture of the bullet, or the pressure wave from the imact.  

As a thought experiment, imagine a wall traveling 3000 feet per second (approximate muzzle velocity of a rifle round) smashes into an amoeba.  Amoeba accelerates from 0-3000 fps instantly, squeezing the water out the side and rupturing its membrane, while heating it. That could kill a single cell, depending on the velocity and temperature of the round, and what type of microorganism.  

With a virus, it's hard to say. They involve less complex machinery that might be more impact resistant than larger water-filled cells.  

I wrote this all without reading prior comments. I'm sure some physicists or biochemists can give a better explanation than me but anyway... very interesting question. 

tl;dr  yes",null,0,cdir3ng,1qzhu7,askscience,new,2
Ph30n1x5000,"Theoretically it can only move or change them.  I'm sure you already know matter can't be created or destroyed, therefore it would push these so-called microorganisms out of its way and/or make some sort of molecular bond thus changing the atmosphere around it.",null,0,cdiosyi,1qzhu7,askscience,new,2
emodestroyer,"I once loaded a carpenter ant inside a 7.62x54r shell, the pressure and powder flash alone probably disintegrated it, we found no trace of bug guts or remains of the bug. If something is inside your bore (like a bug) and a bullet comes out travelling between 1000-2800fps (depends on caliber) it would be microscopically crushed and reduced to its smallest particles, unless it obstructs the bore with enough mass to interrupt the bullets path through the rifling. Basically inside the rifle is a combustion chamber until the bullet exits toward its target.",null,0,cdis4p9,1qzhu7,askscience,new,1
Saint_Oliver,It seems to me that most of the posts on this thread deal with microorganisms that are already on the bullet when it is fired or are on the target. What about MO's that are in the path of the bullet? i.e. what is the effect of a large impulse on a microorganism? Would fluid dynamics simply move the MOs out of the path of the bullet as it flies by?,null,0,cdisi7f,1qzhu7,askscience,new,1
3rdopinion,"Emergency doctor here, and while I don't have a particularly advanced understanding of ballistics, this is an important concept in traumatic .  Large caliber bullets (I'm actually not sure precisely at what point a bullet is considered large caliber), are significantly less likely to cause an infection than smaller ones, though any projectile increases the risk. The majority of infections caused by bullets tend to be from organisms usually present on the skin, suggesting that the bullet has dragged in small bits of skin and superficial tissue. Experiments have demonstrated that, when applied to a bullet prior to firing into live tissue, certain bacteria have a propensity to cause infections. Bacteria that form spores are more likely to remain viable after firing. ",null,0,cdj98ua,1qzhu7,askscience,new,1
TheGloriousHole,"Think about the difference in impact between punching a fly in midair and punching a standing person.

Bacteria has an extremely small inertial mass. It's an extremely oversimplified guess but I would assume based on that, that no it wouldn't kill them.

To accurately work that out, there are a lot more calculations you'd have to do but you know... Guessing is easier.",null,0,cdi3rj7,1qzhu7,askscience,new,1
Platypuskeeper,"Only if the air had the same density as the feather, which would take compressing the air to a liquid state, if not more.
",null,1,cdi2jw6,1qzhot,askscience,new,4
crashd5,"John Hunt, from http://www.ccmr.cornell.edu/education/ask/?quid=1308

Astronomical calculations, measurements of seismic waves and observations (about meteorites and magnetic fields) have led to our understanding of the interior of the Earth.

Once Isaac Newton determined that the density of the Earth was about twice the density of most rocks, he concluded that the interior, which we cannot sample directly, must consist of much denser material.

When a powerful earthquake shakes the Earth at a certain point, some of the energy radiates as seismic waves from that point and these waves can be detected around the planet. If the source and the detector (seismograph) are on opposite sides of the globe then the waves that arrive first at the detector are the ones which travelled through the center of the Earth! By studying the nature of these waves geophysicists have determined that the outer core of the Earth is liquid (because certain types of waves cannot propagate through a liquid) and that the inner core of the Earth is solid (because it allows a different type of seismic wave to propagate through it).

Meteorites are considered to be remnants of material that either did not consolidate into something large enough to be a planet, or are fragments of planets that exploded far away from the Earth. Many meteorites are composed primarily of iron which is a strong piece of evidence about the composition of the Earth's core. The density of Iron is close to 8 grams per cc, whereas most surface rocks are closer to 2.5 grams per cc. This higher density also agrees with Newton's calculations.

In addition, the Earth has a magnetic field which is always present and whose origin must be related to magnetic material inside the planet. Although the solid iron in the inner core is too hot to be magnetic, it is believed that the movement of the molten iron in the outer core is responsible for the magnetic field.

",null,0,cdi3x2n,1qzgs5,askscience,new,12
The_Serious_Account,"I assume you're talking about a [particle in a box?](http://www.lightandmatter.com/html_books/lm/ch35/figs/particle-in-a-box.png) I don't think there's any good answer as to 'why'. It's a standing wave in a box and that's how the equations of a standing wave fall out. You fix the end points by an infinite potential, so the wave function has to disappear at those points. I don't know what else to tell you. 

edit: Maybe your problem is that you're thinking about it as an actual particle in a box. It's not a particle, it's a wave. If you probe it with a measurement there's a certain probability you'll have an interaction with it. That probability is given by the square of the amplitude which happen to be zero in the middle for the first excited state.",null,3,cdi23wb,1qzfqb,askscience,new,6
null,null,null,1,cdi17ok,1qzfqb,askscience,new,2
Ingolfisntmyrealname,"As other have mentioned, I'm assuming you're talking about the ""infinite square well potential"" or what's also called the ""particle in a box"".

This question is technically the same as asking: ""What's the probability that the particle will be found at any point x0 inside the box?"". The answer to this questions is ""zero probability"". Why? Because technically there're an infinite number of points between x0=0 and x0=a. This applies to every point in the box, not just the middle

What you want to ask, then, is: ""What's the probability that the particle will be found at some interval, x0+dx?"" (where dx is some small interval). When you ask the question like this, you will get a real, nonzero probability. This includes finding the particle ""in a small interval centered around the middle of the box"". This probability is, as you can see from the wave-function (squared), rather low, but it is not zero.",null,0,cdi539t,1qzfqb,askscience,new,1
selfification,"Because you're using an intuitive framework where like dots or shiny spherical balls needs to move from one place to another.  You're using a mental framework where particles are described as items have:
a) an outside and an inside.
b) a center (and possibly spherical symmetry around it).
c) a fixed/rigid shape.
d) equations of motion that consist of continuous translations/rotations.

These are all wrong.  The ""particle"" in a quantum world is so different from the particle you are used to that it no longer does it any justice to call it a particle.  It's called that for historical reasons and because some of its mathematical properties carry over from Newtonian mechanics which dealt with particles.  It would be like trying to describe a iPhone to someone from the 30s and starting off with ""well, it's a phone which...""  No!  That mental model is simply not useful or helpful.

If you saw everything as waves, your sanity has a much better chance of coming through a QM class intact.  When you strum a guitar, it's strange that the 2nd normal mode of vibration of string has a node in the center, but it isn't earth shatteringly confusing because nobody calls that vibration a particle (well...  I guess you could call them phonons if you really want to stretch your credibility).  Electrons are similarly just waves.  They follow the rules of wave mechanics.  They reflect off of potential wells.  They can form superpositions.  They form standing waves or travelling waves depending on the situation involved.  They can spread over large volumes.  They do all the things that your regular intuition about waves tells you they do.  There is an interesting kink known as the measurement problem that you'd still need to surmount, but you'd need to do that whether you think of it as a particle or as a wave, so it's just more sensible to think of it as a wave from the get go.",null,0,cdi94ao,1qzfqb,askscience,new,1
Whisket,"To start, there is a distinction between temperature and heat. In a given amount of water, the temperature of water is the *average* energy level amongst all of the molecules of water. However, heat is the *quantity* of energy that the amount of water has in total. As an example, the ocean has a lot more heat than a boiling cup of tea, but the cup of tea has a higher temperature. The quantity of heat in the ocean is greater than the tea, but the average temperature in the cup of tea is greater than the ocean.

At room temperature, the average energy levels in the water is not great enough to cause it to evaporate. However, individual water molecules do not all have the same quantity of heat, and some can gain enough energy to evaporate. This amount of energy is then lost from the source of water (evaporative cooling), but it will soon be replaced from the ambient air (at room temperature). Thus, the average temperature will remain constant, but individual water molecules are capable of evaporating if they gain more energy.

This is also why things will dry faster with higher temperature, even below boiling. The average temperature of the water molecules is higher, and a molecule takes less deviation from the average to gain enough energy to evaporate.",null,9,cdi3e43,1qzdx1,askscience,new,37
Cassiel23,"Water has energy.  At any temperature, molecules of water move.  When the movement of water molecules is sufficiently fast to break their attraction to other water molecules, the molecules are released into the atmosphere as vapor. 

This occurs at a temperature of 100 degrees F for water molecules, but at lower temperatures there is still some energy transfer between molecules sufficient to break molecular attraction and release the water as vapor (i.e.; ""drying"" the wet object), although this process is much slower and less efficient than at higher temperatures.",null,0,cdi3c3a,1qzdx1,askscience,new,8
BigPapaTyrannax,"Consider a closed container half full (or half empty) with water. There is a equilibrium between the two phases of water, the liquid phase that we can see and the vapor phase that we cannot. The amount of water molecules in either phase is dictated by thermodynamics. Without getting into the complex math behind it, at higher temperatures, the liquid water has a higher chemical potential and will be more likely to enter the gas phase. The bigger the area for the vapor phase to expand into, the more molecules will enter the vapor phase (once again, tons of math that you probably don't want to see). Basically, you can get more of the liquid to turn into gas by heating it up or by giving it more room to expand into. 

How does this apply to your question? Well, things ""dry"" at room temperature because there is an equilibrium between the liquid phase of the wet object, and the vapor phase of the room around you. The liquid phase and the vapor phase must come to an equilibrium. Since the room is so large, and the wet object has relatively small amounts of water, the equilibrium will be such that the small amount of liquid is converted entirely to vapor. 

Source: Undergrad bioengineering student, currently taking advnaced physical chemistry courses, please feel free to add to or correct my understanding.

",null,1,cdi3h4h,1qzdx1,askscience,new,8
rupert1920,Check out [some of these past threads](http://www.reddit.com/r/askscience/search?q=water+boil+dries&amp;restrict_sr=on).,null,1,cdi3vz4,1qzdx1,askscience,new,6
NastyEbilPiwate,"Temperature is a measure of average energy. In room temperature water, some molecules will have a lot of energy and some will only have a little. The total energy averages out and the water will be 'room temperature'. Some of those molecules with a lot of energy will end up having enough that they are able to escape from the liquid and float off, having evaporated.

The remaining molecules will absorb energy from the room (the loss of the high-energy molecules reduces the average energy and thus temperature of the water so that energy will flow from the room into the water), and so some of them will be sped up enough that *they* can then escape.

Eventually, all the molecules will have gained enough energy to break free of the remaining liquid, and there'll be nothing left.

Edit: The distribution of energy will look like this: http://i.imgur.com/STMeQDi.png",null,2,cdi3fs7,1qzdx1,askscience,new,4
florinandrei,"The boiling point is not a magical temperature where water just starts to evaporate. Evaporation happens at all temperatures, it's just much stronger when water is boiling.

Things get dry at room temperature because some evaporation happens even at that low temperature.",null,1,cdi5409,1qzdx1,askscience,new,3
sexdrugsandsam,"If you think of temperature, think of it as the average speed the molecules are moving. So if the water is an average of 30 degrees Celsius, that means that there are some molecules that have very little movement, some that move closer to the average and some moving exceptionally faster. The molecules which are moving exceptionally faster (i.e. the temperature of the single molecule would be 100+ degrees Celcius) would themselves 'evaporate' and leave the liquid and taking on the properties of a gas. ",null,1,cdi5nym,1qzdx1,askscience,new,3
rocketsocks,"Evaporation.

OK, I'll try to explain as simply as possible... go:

Temperature is, essentially, average kinetic energy of molecules. In a liquid the molecules will have a range of kinetic energies around that average. Some molecules will have enough energy to move into gas form if they are near the surface. Because this is a molecule with above average KE escaping it lowers the temperature of the remaining liquid.

However, assuming the liquid is in thermal equilibrium it will return to the same temperature as its surroundings, and the evaporation process will continue. This is a ratchet effect, which can cause all of the liquid to evaporate, but how long that takes depends on the amount of liquid, temperature, vapore pressure, etc.",null,1,cdibwmc,1qzdx1,askscience,new,3
MayContainNugat,"When sugar dissolves in water, it isn't melting.

When water dissolves in air, it isn't boiling.",null,9,cdi59z4,1qzdx1,askscience,new,6
iorgfeflkd,"The unit we use to describe time is arbitrary, and the age of the universe is consistent under proper conversion between them.

Humans use Earth years when discussing long periods of time to communicate with other humans, because that's what humans are familiar with.",null,1,cdi0xva,1qzc7u,askscience,new,4
heyamipeeing,"What you should be asking is if every where in the universe experiences this moment in time simultaneously. Even though the universal constant (c) prevents us from witnessing events in ""real-time"", it is interesting to wonder whether every place in the universe has experienced the same amount of elapsed time since 0 (the Big Bang) or if some unknown effects cause time to speed up or slow down. If someone could shed some light on this that would be awesome.",null,0,cdi18pt,1qzc7u,askscience,new,2
loctopode,"It differs between animals, but in general:
Sleep is a regular reduction in activity. There is a reduction in body temperature and oxygen consumption. The metabolic rate in sheep is reduced by up to 13% while sleeping, humans reduced by up to about 12%. The body can still respond to external stimuli and arousal (""waking up"") can happen quite quickly.

'Torpor' is similar to sleeping, but can last for a few days. The metabolic reduction is greater, and can be down to about 30% basal metabolic rate. Body temperature can lower, but remains above ambient. It's used especially by some small mammals who undergo torpor daily to avoid times when food is scarce. Arousal can take up to an hour.

Hibernation lasts for much longer (several months). Possible triggers can be reduction in temperature, food availability, light level etc (and will vary between species). Heart rate decreases. Metabolic rate is lower than in torpor, potentially going down to about 5%. Body temperature drops even further, coming very close to ambient temperature. ""Respiratory acidosis"" occurs, where the intra/extra-cellular pH decreases. This inhibits the activity of some enzymes and helps reduce metabolic rate. Normal bodily reactions to a reduce in core temperature are stopped. Response to external stimuli reduced in comparison to sleeping. Arousal can take a few hours and happens periodically (number of theories why they wake up, some suggest it's to restore energy, others say it's to initiate an immunse response, with some evidence indicating animals are susceptible to parasites while hibernating). 

The cause of a coma varies, but when in a coma the body doesn't reduce temperature or metabolic rate like hibernation. The individual may wake from the coma within a few days, weeks etc or not at all, it can vary with the cause.

Death is a real reduction in temperature and metabolic rate. The heart stops beating (not just slows like in hibernating). The body then can eventually ambient temperature. However, unlike in hibernation, it's very diffcult to recover from this without (i.e. medical help).

So hibernation can be seen as a greater/deeper sleep, but there are important differences especially regarding metabolic/temperature rates.",null,0,cdidwqq,1qzb91,askscience,new,5
null,null,null,0,cdibpk8,1qzb91,askscience,new,2
ZeroCool1,"**The main factor is the Gibbs free energy of reaction occurring in the battery.**  Temperature plays a role as well.  


The Gibbs free energy of a reaction is a measure of the ""usable work"" available from the reaction.  In general for a reaction to occur, a negative number is required, indicating an excess energy.  If a reaction has a positive Gibbs, the reaction does not occur spontaneously, due to a lack of energy.  Sometimes energy can be given to the reaction through an increase in temperature, sometimes no amount of temperature increase will make it run.

In fact there is a relationship between the Gibbs free energy of reaction, the equilibrium constant of a reaction K (or Q if it is not in equilibrium), and the cell potential.  See slide 15 for the ""triangle"" of equations.

http://courses.washington.edu/bhrchem/c152/Lec14.pdf

The whole lecture explains the concept.

Here's a great picture:

http://chemwiki.ucdavis.edu/@api/deki/files/126/triangle_diagram.png?size=bestfit&amp;width=463&amp;height=298&amp;revision=1",null,0,cdi0msg,1qz6fm,askscience,new,3
BadDadWhy,"This [Equation](http://upload.wikimedia.org/math/d/3/b/d3bf57847ca22c9cee3a1865d3548eae.png) shows that most of it comes from the materials difference, then that is adjusted by the differences in concentration of the parts. They don't go into internal and external resistances but in real life they are part of the circuit and effect the cell voltage. 

You may notice that temp doesn't come into the equation but batteries work worse in cold weather. That is because the kinetics of transport are slower, you get less current not less voltage.

Me heap big electrochemical engineer.",null,1,cdhyp0u,1qz6fm,askscience,new,2
do_od,"Correct, you could build a x-ray camera with a pinhole lens just as you would build any other pinhole camera. You could also use a [zone plate](http://en.wikipedia.org/wiki/Zone_plate) which has the advantage of greater aperture for a given resolution, that is it makes sharper images than a pinhole of the same area. If you know the wavelength and focal lenght of your setup you can calculate the geometry for the zone plate using the formula found on wikipedia or just google zone plate generator, there are lots of online tools. Zone plates can also be approximated with an [array of pinholes like this](http://2.bp.blogspot.com/_9xmT5wmPOhY/Rx2TyQ19hxI/AAAAAAAABH8/3slGBsx1nF8/s400/pinsieve.jpg). 

Edit: Ofcourse, fabricating the plate is a different matter. For visible light it is as simple as photographing a template with a film camera and use the negative. I would suggest etching the pattern in a thin metallic film on a substrate that is transparent to x-rays, sort of like how [integrated circuits are made](http://en.wikipedia.org/wiki/Photolithography). ",null,2,cdhzqtr,1qz517,askscience,new,9
uzzors2k,"Here is a [forum thread](http://4hv.org/e107_plugins/forum/forum_viewtopic.php?106277.120#post_109615) by some amateurs using a single pin-hole camera to determine the x-ray emission points in a vacuum tube. You can see the actual [x-ray ""camera""](http://4hv.org/e107_plugins/forum/forum_viewtopic.php?106277.150#post_109830) here. The x-ray source is a high voltage rectifier tube, reverse biased and run at anode voltages far above it's specification. About creating a lens, I am unsure.",null,1,cdi0ead,1qz517,askscience,new,3
ron_leflore,"A single pin hole would work.  The problem is that the throughput is so small, that you would need super strong sources and/or very long exposure times.  So his type of lens is not used for x-rays.

Several x-ray telescopes have been built using grazing incidence techniques.  For more information on x-ray lenses, start here http://en.wikipedia.org/wiki/X-ray_optics",null,0,cdi8zfu,1qz517,askscience,new,1
Das_Mime,"[This diagram should hopefully help clear it up](http://www.moonconnection.com/images/moon_phases_diagram.jpg)

When the moon is up in the daytime, it's between you and the Sun, so you're mostly going to see the part of the Moon that is in shade at the time. If you want to see a full moon, then you (Earth) need to be between the Moon and Sun-- which happens at nighttime.",null,0,cdhyawx,1qz4pb,askscience,new,10
Queen-of-Hobo-Jungle,"It is called Lunar Cycles. The moon rotates around the earth every 30 days, approximately. When it is in front of the sun, we don't see it at night, but it occasionally casts shadows over the tropics as eclipses. Directly across from the sun at night, and it appears full for most of the night. Any time inbetween, the earth is casting a partial shadow, and as the moon moves from waning to new, it appears closer to dawn.",null,6,cdhzdkw,1qz4pb,askscience,new,4
baloo_the_bear,"Thermal burns don't just affect the most outer layers of skin. The heat energy that caused the burn will continue to dissipate into deeper layers of skin. Pouring cold water over a burn helps to give that heat energy somewhere to go besides your tissue and helps prevent the burn from going from a first degree to a second degree (or second to third, etc). ",null,1,cdi6elf,1qz402,askscience,new,3
brawnkowsky,"well, burns interfere with the epidermis' ability to prevent water loss via destruction of cells (these cells prevent most evaporation of water from plasma and interstitial fluid), and the large amount of energy transferred to the skin causes water to evaporate.  running water over a burn restores that lost water, which probably is the cause of the 'good feeling'.

http://www.anaesthesiamcq.com/FluidBook/fl3_2.php  ",null,2,cdhzow6,1qz402,askscience,new,2
OrbitalPete,"Mars' volcanic activity is very different to that on Earth. Earth volcanism is driven by plate tectonics, with the vast, vast majority of volcanoes being located at either constructive or destructive plate margins. In contrast, volcanism on Mars is believed to be far more analogous to hot spot volcanism (that which forms the hawaiian islands for example).  Mars has no active plate tectonics due to the fact it's cooled down. That same explanation is highly likely to explain why Martian volcanism has died down.

Martian volcanoes are basaltic in nature - the same as plume volcanoes here on earth. That suggests they are forming from a partial melt within the mantle, then rising buoyantly through the crust. because Mars never had the active tectonics on a scale similar to Earth the crust has never been able to get highly silicious, hence crustal contamination of melt as it rises has not seemingly varied the chemistry too much from the original basalt.

It's possible Mars is still volcanically active, we're just in a window where we have not observed any eruptions. It may be that the eruption rate has died down significantly such that thousands or even millions of years may pass between events.

One interesting thing to note - because of the low atmospheric pressure on Mars, it means that while the chemistry of the basalts is fairly similar, instead of flowing out leisurely as they do on earth it becomes possible for gas bubbles to inflate to many times what you would expect in a heavier atmosphere. That drives the ascending jet more powerfully, which in turn means you can get far more violent eruptions from what on earth would behave as a fairly low-energy effusive eruption.",null,14,cdhzrvz,1qz3mf,askscience,new,86
TheGreatFabsy,"Until an expert gets here, I'll offer some of my knowledge.

Simply put it's because Mars isn't active.
The core is ""shut down"", cold. That's why there isn't a magnetic field on Mars, also the reason Mars lost it's atmosphere. The Sun is bombarding the planet with high charged particles, effectively ""sand blasting"" the atmosphere. Good thing the Earth is active! It's magnetic field is repelling these particles, kinda like James Bond deflects that bullet with his magnetic watch! (MythBusters busted the Bond myth, tho.)",null,8,cdhzecc,1qz3mf,askscience,new,17
bellcrank,"This appears to be a frontal cyclone, or midlatitude cyclone, not a tropical cyclone.  Europeans often give names to these systems, though this practice is not typical in the United States.  I think The Weather Channel has started giving names to frontal cyclones in the US, but it is not an official practice.",null,0,cdi8rv7,1qz3jz,askscience,new,2
wazoheat,"Is it a real cyclone? Yes. ""Cyclone"" is a term in meteorology for *any* rotating cyclonic (counter-clockwise in the northern hemisphere, clockwise in the southern) storm of any size or origin, from dust devils and tornadoes to hurricanes and frontal storm systems.

Is it a *tropical cyclone*, akin to typhoons and hurricanes? No. It is a ""cold core"" cyclone with associated warm and cold fronts, just like every other average storm system in the mid-latitudes (between 30 and 60 degrees away from the equator). I'm not sure where the name came from, I can't seem to find any information on it.",null,0,cdif77e,1qz3jz,askscience,new,2
Pluckerpluck,"Before anything forms we have a cloud of dust. This will eventually become our solar system.

This cloud will almost certainly have some amount of spin around its center of mass. It could be minute, but as the dust contracts conservation of angular momentum will increase this spin. At the same time these particles will start smashing together an start forming planets.

At this point you can think of it in a few ways. One way is to realize that the system is basically a big ball of mass rotating around a central axis. Perpendicular to this central axis inertia (centrifugal force) is stopping the sphere collapsing. However, parallel to this axis this force/inertia doesn't exist. As such the sphere can collapse along this axis and thus forms a disk (this is my preferred way of explaining this). It's similar to how spinning pizza dough in the air becomes a disk.

Another way to look at this is to imagine two planets orbiting in planes at a slight angle off from each other. At this point it should be relatively easy to realize that these planets will attract each other and thus get closer together (and thus move into more similar planes of orbit).

The final way to look at this is in a more mathematical way and state that the system ""likes"" to be in it's lowest energy state.  If we conserve the total angular momentum of the planets, then a flat disc has the least total kinetic energy. As such when the system slowly loses energy (from collisions and gravitational effects) it will move towards a state of lower kinetic energy and thus move towards a disk shape.",null,1,cdhz44g,1qz13p,askscience,new,8
__Pers,"It's only strictly true that they can't if the surfaces of the waveguide are perfect conductors. 

Assuming that they are (or that the conductivity is high enough that they can be treated as such), then the surface of the conductor is an equipotential, meaning that the electric field must vanish inside the waveguide. The magnetic field of a TEM wave is proportional to the electric field, so the magnetic field also vanishes. This is why you need at least two topologically distinct conducting surfaces (e.g., a co-axial cable) to propagate a TEM wave. 

Edit: typo. ",null,0,cdhzrp3,1qyy8r,askscience,new,3
FizixPhun,"While not very mathematical, here is the intuitive way I understand it.  Think about what is happening microscopically when you transition from one phase to another.  Lets take ice melting into water as an example.  When you heat ice up to 0C, it begins to melt.  In solid ice, the molecules are well ordered and are hydrogen bonded with their neighbors.  When you melt it, you are breaking these nicely ordered bonds.  This takes energy in the form of heat.

Equivalently, you can also think about the fact that entropy is different in two different phases of a material.  Entropy can roughly be thought of as how disordered a system is.  The molecules in ice are nicely bonded into a lattice, much more ordered than the moving molecules in water.  Even though you are not changing the temperature when you melt the ice, you increase the entropy.  This increase in entropy costs heat as a change is entropy is defined as the amount of heat added divided by the temperature at which the heat is added.",null,0,cdifxam,1qyxr6,askscience,new,1
lmo2th,It does complete it's shell. Cyanides have a counter ion (usually Na or K) or a hydrogen atom also bonded to the carbon. It gets 3 extra electrons from the C-N triple bond (one from each bond) and one extra from the counter ion or hydrogen atom. That gives a total of 4 electrons. Add this to carbons 4 valence electrons and you get 8 - a complete octet.,null,1,cdhztdg,1qyupv,askscience,new,5
dragonnyxx,"Cyanide isn't a chemical compound. The word ""cyanide"" refers to chemicals which contain the cyano (CN) group, such as potassium cyanide or hydrogen cyanide. These neutral molecules *do* contain complete electron shells, whereas the CN ion by itself does not.

""Cyanide"" is used as a general term for such chemicals because the free CN ion is poisonous, and therefore it doesn't particularly matter whether it was potassium cyanide or sodium cyanide or hydrogen cyanide you were exposed to, you're going to be poisoned the exact same way from any of them.",null,8,cdhzx18,1qyupv,askscience,new,4
hooligan333,"Light attracts zooplankton, as it usually indicates the presence of bioluminescent bacteria which they feed upon. [Source, which also explains much below](http://www.alphagalileo.org/ViewItem.aspx?ItemId=117844&amp;CultureCode=en).   
These zooplankton in turn are a significant food source for many smaller fish. So these fish will readily approach such beacons (which conveniently also make their intended prey easier to spot) with the promise of an easy meal. As coming across a predator utilizing bioluminescent lures is significantly less frequent than coming across said bacterial colonies, this 'gamble' frequently pays off for the fish. So for these fish to develop an aversion to light sources would deprive them of a major food source and this would ultimately be far more detrimental to the survival of a species than losing the occasional individual to such deceptive predators. So natural selection, working as it does, continues to favor the fish that go towards the light...

Edit: Formatting",null,0,cdhx0zd,1qyt1z,askscience,new,16
Izawwlgood,"Think about it this way; those predators are mimicking something the prey fish want. The question shouldn't be 'why would prey fish not evolve to avoid that?' but 'what do the prey fish want that glows?'. 

You know how crocodiles stalk watering holes waiting for gazelle to come drink water? Gazelle can't evolve to not drink water.",null,1,cdhyo68,1qyt1z,askscience,new,5
KToff,"1. The mass of the earth is so incredibly large that it is impossible to accumulate any significant amount of garbage. The US produces roughly 250 million tons of garbage per year (2.5 *10^11 kg of solid waste). The mass of the earth is give or take 6*10^24 kg. This means that (at the current output) in a million years the US would not even produce a tenth of a million of the earth's mass.

2. Even if you were to launch significant chunks of the earth into space (and neglecting the energy requirements of doing that), removing a lot of mass would not change the days in a year (they almost exclusively depend on the mass of the sun and the distance between earth and sun), the length of the day (depends on the rotation period of earth which is not directly affected by removal of mass). Depending on where the mass goes it could have an effect on the tides because the moon earth system would change quite a bit, if one of the partners suddenly changed weight.

TL;DR; We could never throw enough stuff into orbit to have any significant impact (in the sense of changes in the behaviour of the celestial body earth)",null,0,cdhvxzj,1qysak,askscience,new,3
adamsolomon,"I'm not sure quite what you're referring to - maybe you could find an example from a popular exposition you've read.

Quantum field theory does have wiggle room. In fact, quantum field theory isn't a theory at all, but a framework, and you can define all sorts of quantum field theories. Some of them (like the ones making up the standard model of particle physics) correspond well with reality, and others are just toy models on paper. And these have some wiggle room.

What a good quantum field theory does have is some guiding principle, namely a symmetry. For example, pretty much any good quantum field theory should look the same no matter what speed you're travelling at, or how your lab is oriented. (Physicists call this Lorentz symmetry.) More complicated mathematical symmetries exist and are crucial to defining other field theories. For example, electromagnetism has a symmetry called U(1) gauge symmetry - don't mind the name, it just means that if you change the variables in a certain way, the changes cancel such that the equations of the theory stay the same.

These symmetries are crucial to modern physics, because they do restrict the kinds of terms you can write in your equations. Namely, once you choose the symmetry your theory has, your equations can only have terms which don't change under that symmetry.

So when you define a field theory, you tend to choose the symmetry, and then choose which allowed terms are there, what the constants are, etc. It's not as if the theory has a perfectly rigid structure - though you definitely don't have complete freedom.

VERY IMPORTANT, by the way - if your theory doesn't obey this symmetry or that, it's not as if it's logically invalid. It may have theoretical issues to overcome, it may very well disagree with experiment, but it's not a priori illogical.

Newtonian mechanics has symmetry too, even though I think it was rarely thought about in that way. It has, for example, spacetime translation symmetry - if I move around in space or wait a long time, F is still ma. F = ma + 1 N would maintain this symmetry, but something like F=ma + 1 N * (the year in AD) wouldn't.",null,1,cdhvfih,1qys8x,askscience,new,7
adamsolomon,"&gt;Since the universe itself moves faster than light, would it be hypothetically possible to see the light from an extremly old object in its Infancy, say a few billion light years away, while being next to that object in present day? If not, under what conditions, if any, would this be possible?

No. The Universe doesn't actually move faster than light - or move at all. It's expanding. This means that individual points - galaxies, say - move away from each other. Nearby ones move away from each other below the speed of light, faraway ones recede from each other faster than light. But light travels in straight lines, so once it's past you, it's past you. In order to see old light from something sitting next to you, the Universe itself would have to be curved like a sphere, so that light goes all the way around. The trouble with this is that a Universe curved like this will eventually recollapse in a Big Crunch. It turns out that light emitted at the Big Bang can only go around the whole Universe at most once before the Big Crunch happens.

(By the way, that's all hypothetical - there's no evidence to suggest that the Universe has that curvature or will ever have a Big Crunch.)

&gt;I saw a picture of the universe an extremely short time after the big bang and it made me wonder if that picture included the ingredients for our galaxy, solar system and even our planet. It has to, right? So I was wondering more specifically about individual objects as well.

The ingredients, definitely. Now, we wouldn't be able to see the light from the place that became *our* galaxy, since we emitted that light billions of years ago and we haven't moved (as I discussed before). But we can see the seeds of distant galaxies which today would look much like our own.

As for individual objects, you can't see those because they hadn't formed yet. The seeds you see in the images of the cosmic microwave background (the oldest light in the Universe) represent slightly overdense and underdense regions of space. The overdense parts collapsed under gravity, forming stars and galaxies in the process.",null,0,cdhvior,1qyq5k,askscience,new,2
null,"a) The universe isn't currently expanding faster than light.  It did so during it's inflationary period but it's expansion slowed drastically after that.  It's true that the expansion rate is accelerating but it still isn't anywhere close to c.

b) You could come up with some geometric configuration that would allow a light ray to travel in a closed path of a few billion light years so you could observe it from it's origin.  It's possible such a configuration exists in the universe but since we can only observe from Earth's neighborhood we wouldn't be aware of it.

c) The late big bang environment included hydrogen, a tiny bit of helium, and an even tinier bit of lithium.  That's it.  Everything else was manufactured by stellar processes.  Elements heavier than iron were manufactured in supernovae.",null,1,cdhv2y3,1qyq5k,askscience,new,2
adamhstevens,"&gt; How do they know these were rivers and oceans of specifically water and not some other liquid, or for that matter solid glaciers? Also how do they know the rivers entered oceans instead of simply evaporating?

We don't. There are theories that it could have been caused by 'fluidised' carbon dioxide (CO2 can't exist as a liquid under current martian pressures and temperatures, which of course may have been higher in the past, but it can exists as a weird mixture of gas and dust that forms a liquid-like fluid that can flow), but really, the only candidate for a liquid at the conditions that we expect could have occurred is water. There just aren't that many substances that exist as a liquid that at standard planetary conditions and are also abundant enough to form flows.

There are also people that think the features could have been caused by glaciers, since a lot of the morphology is similar. However, in depth studies show important features that can't really be caused by glaciers, e.g. http://www.wired.com/wiredscience/2011/06/teardrop-shaped-island/. There are a lot of different channels on Mars, though, so it could be there are some caused by both mechanisms.

In terms of the oceans, some of the channels (generally the shallower ones) do just stop and appear to have evaporated and/or infiltrated into the regolith. However, there are also some features that look like deltas, which would suggest a river joining an ocean e.g. http://photojournal.jpl.nasa.gov/catalog/PIA04869.",null,2,cdhxg1y,1qypmp,askscience,new,3
_wigga_,"Radioactive decay is a nuclear process not an atomic process. It also is a statistical process and one nucleus does not have an influence on another. Each nucleus decides independently when it decays. So the answer is no it does not matter which state your substance is in

If your substance is a solid, liquid, gas, frozen to almost 0 K, the radioactive decay rate would be the same.

Actually there was a paper once that showed the possibility of delayed radioactivity at lower temperatures but this is nonsense and was disproved by experiments.",null,3,cdhwqoo,1qypjq,askscience,new,4
FizixPhun,"I don't know the exact answer but here is my guess given my knowledge of Bose Einstein condensates and radioactive decay.

First, let’s talk about radioactive decay.  Basically, this is when an atom has a nucleus that is too large to be stable and a chunk flies off to make it more stable.   The result is an atom with a different atomic number, hence a different element from our original atom, and some other decay products.

Now, let’s discuss Bose Einstein condensates.  This is a phenomenon that occurs when many bosons occupy the lowest energy state, the ground state.  The wave function for our system must include information about all the particles.  Bosons are identical particles that are required to have a wave function that is even under the interchange of any two labels for our particles.  The key nugget is that changing the state of even one of the bosons changes the wave function for all particles and is therefore energetically unfavorable.

Now let’s combine these aspects.  To get a Bose Einstein condensate we must first have atoms that are bosons.  Now we also need these atoms to be radioactive which pretty much limits us to very heavy elements.  Now cool them to a sufficiently low temperature that they are all in the ground state and behave as a condensate.  If an atom radioactively decays, it is no longer the same type of identical particle and it can no longer be part of the condensate.  The Bose Einstein condensate only occurs when the bosons are indistinguishable.  Clearly, atoms of different elements are distinguishable.  This would cause the wave function to change as it goes from n identical bosons to n-1 identical bosons and one other atom.  Basically, we just have one less atom in our condensate.  I’m almost 100% confident that being in a condensate would not cause all atoms to radioactively decay if one does.  I’m less certain but suspect that being in a condensate would not prevent an atom from radioactively decaying.  In reality, this would be pretty difficult to measure as the critical temperature below which a condensate occurs scales as 1/mass.  As previously stated, radioactivity occurs mostly in very massive elements, which means that the condensates would be very hard to achieve because they would require exceptionally low temperatures.

Maybe some atomic and ultra-cold experts can comment on this? ",null,1,cdih7an,1qypjq,askscience,new,2
afcagroo,"It is really hard to address this without knowing what is meant by memory ""performance"". If it means the speed or bandwidth of the memory, then the answer is fairly simple. Most of the Moore's Law improvements in memory are used to put more bits onto a die, rather than to increase how fast bits can be accessed.  The former is simply more valuable in general than the latter, since (as someone else mentioned) the bandwidth is somewhat constrained by other factors such as the PCB traces and number of I/O's available in a given space. 
   
There is a tradeoff to be made.  You can keep the bit cell capacitance fairly high so that the charge transfer settles quickly and you can read the cell faster, or you can reduce the bit cell capacitance (and make a smaller cell) but take a penalty for longer settling times. 
  
It is possible to instead make the memory faster but not much more dense, and to some extent that has been done in the past on SRAMs. But at some point, you get two where making the chip faster is nonsensical, since you can't get the information in/out any faster in a reasonably practical system.  
  
If you look for charts of DRAM *density* as a function of time, I think you will find it improving at a similar rate to processor performance. At one time it was actually improving a bit faster, but I don't know if that's true today.   
  
Some of the manufacturing technologies are different, too. Processors tend to use straight CMOS logic. DRAMs need to maximize the capacitance of the bit cell, so they have gone to some trench technologies and vertical stacking that require unusual processes.  ",null,0,cdi1els,1qypga,askscience,new,4
spPad,"RAMs are interconnect dominated - so the larger you make each bank, the slower it will get. Each bit-cell in RAMs have been getting much faster, but the problem is that the interconnects get worse at smaller technology nodes. This is bad for interconnect dominated systems.

Secondly, your off chip memory (DRAM) is usually not limited by the speed of the RAM, but the speed of the bus. The wires on the PCB are *huge* compared to the wires within the chip.

Lastly, you do see the impact of smaller device sizes on the capacity of the RAM, not necessarily on its speed. Back in 2007 I was running a laptop with 64MB DDR RAM and was perfectly satisfied. Today, my phone has 2GB. ",null,0,cdhz0sw,1qypga,askscience,new,3
Luminarie,EDIT: They guy below is actually right :D.,null,1,cdhvl6u,1qypga,askscience,new,2
mzellers,"For the processor, having more transistors allows for increased parallelism.  That allows you to do more work in each cycle. Some processors will even proceed down both paths of a conditional branch and throw away the results of the wrong path!  Today, multiple core CPUs are quite common.  For a memory chip, other than allowing for a wider data path, there is no similar gain.",null,0,cditmwv,1qypga,askscience,new,1
teramut,"No, different regions expand at different rates depending on how much matter there is. Underdense regions expand faster, and overdense regions slower. There are also stable structures, galaxies, where the expansion has first turned into contraction, and contraction has been balanced by internal pressure and rotation. 

The expansion rate is constant only if you take averages over large enough regions. There are some attempts at evaluating this homogeneity scale, above which the universe looks homogeneous, and it's roughly 300 million light-years. So in regions larger than this, the expansion rates should be equal on average.",null,1,cdhzv2b,1qymma,askscience,new,2
null,null,null,4,cdhxpdz,1qymma,askscience,new,1
coloroftheskye,"There are particles which we call Force carriers. These particles are used to explain how forces act on a distance. For instance in the sense of magnetic attraction and repulsion we explain it by an exchange of ""virtual"" photons. As you may know photons are massless and thus the distance they can travel is infinite. other forces such as the strong and weak nuclear force are mediated by other particles which do have mass, That is why these forces get much more weaker over a distance than the electromagnetic force. On very small distance scales these forces are stronger than the electromagnetic force and thus protons can exist (protons are positively charged and thus repel each other but on the small scale in the nucleus the nuclear force is stronger). For the very same reason fusion is so hard to achieve. We first need to overcome the electromagnetic repulsion from the electrons and nuclei before the nuclear force can take over.

wikipedia is quite helpful with this: http://en.wikipedia.org/wiki/Force_carrier",null,1,cdhtyt4,1qymea,askscience,new,3
wazoheat,"""Radar"" maps like that on Intellicast, at least back in early 2011, were not actually detecting the precipitation type. The radars were ""dumb"" in that they could only estimate the intensity of the ""echo"" reflecting back from what the radar beam hit in the air, whether it be rain or snow or dust or birds. Maps like the one you show above were made by a process of stitching radar images from all the individual sites together, then using an algorithm based on actual surface observations to estimate what the precipitation type is throughout the US. So the ""circle"" of snow probably didn't exist at all, but was probably due to a bad observation or two.

Though is probably different now because, with the national [upgrade to dual-polarization radar](http://www.nws.noaa.gov/com/weatherreadynation/news/130425_dualpol.html), radars can now directly estimate the precipitation type.",null,0,cdhv0be,1qylwu,askscience,new,2
rupert1920,"It is because the two are not the same.

Your example of a coin on a turntable has constant angular velocity, regardless of distance from the center. In other words, no matter how far away it is, it makes one revolution in the same period of time. This is _forcibly_ imposed on the coin by the friction of the turntable.

If you look at the equation for [centripetal force](http://en.wikipedia.org/wiki/Centripetal_force#Formula), you'll find that for constant angular velocity, centripetal force increases as a function of distance from the center. At some large distance, friction will not be able to apply enough centripetal force - and your coin will slip and be flung off.

In the case of gravity, you can find this is not the case. The inward force [does not increase as a function of distance](http://en.wikipedia.org/wiki/Law_of_gravitation) - in fact, it _decreases_ as a square of distance - so it can never maintain the same angular velocity as you change the distance.",null,0,cdht8uy,1qykuu,askscience,new,7
Astromike23,"Unlike solid-body rotation, gravity decreases in force as the distance squared. Two coins on a turntable make a full ""orbit"" in exactly the same amount of time. Two planets around the Sun do not.

In order to keep an object moving in a circle, there must be an acceleration acting on that object. (By acceleration, I don't mean that it's speeding up, but rather that its velocity is constantly changing direction.) There will be a balance between the tangential velocity always trying to propel the object away from the circle, and the acceleration keeping it on the circle. This careful balance can be written in equation form as:

a = v^(2)/r        (1)

...where a is the acceleration, v is the velocity, and r is the radius of the circle. In the case of gravitational acceleration, Newton taught us that's it's equal to:

a = GM/r^2           (2)

...where G is the gravitational constant, M is the mass of the central body, and r is again the radius. We can set these two things equal and solve for v:

v^(2)/r = GM/r^2

v = sqrt(GM/r)    (3)

So, the velocity of an orbit decreases as  as one over the square root of the size of the circle. In other words, if a planet is 4 times as far from the Sun as Earth, its velocity will be half that of Earth's to stay in a circular orbit. This is simply because gravity weakens as one gets further from the source, and so the velocity must also decrease to stay in a circle - otherwise it would go flying off.

Now in the case of a turntable, the velocity increases with increasing distance. The coin at the edge of the turntable traces out a larger circle than the coin near the center even though they both must make a full circle in the same amount of time, so the edge-coin must be moving more quickly. In fact, the equation for how quickly a coin must move will be:

v=ωr

where ω is how fast the turntable is spinning. We can substitute that v into equation (1) to find the acceleration:

a = (ωr)^(2)/r = ω^(2)r^(2)/r = ω^(2)r   (4)

So, in the turntable case, the acceleration to stay in a circle actually *increases* with increasing distance, very different from gravity. This makes sense - imagine you were sitting on a giant rotating turntable the size of city block. It would be easy to sit still if your were close to the center...but at the edge you'd have to hold on for dear life.

**TL;DR:** Because gravity decreases with increasing distance. ",null,1,cdhtklg,1qykuu,askscience,new,2
chrisbaird,"Fundamentally, it depends on the strength of the interaction between the objects that are orbiting. For instance, the limbs of our galaxy orbit like a collection of independent bodies (like planets in our solar system). But in the nucleus of our galaxy, the interaction between stars is strong enough that they orbit more like a solid sphere (like your turn table). The interaction or atoms in the turntable at different radii are strong enough that the object is rigid so that they must all rotate with the same angular frequency. The different atoms lock each other into this type of motion. In a rough sense, the atoms at smaller radii pull the atoms at large radii faster then they would normally go if they were left to orbit independently. In the same way, in the nucleus of our galaxy, the attraction between stars is strong enough that the inner stars pull the outer stars faster than they would normally go if they orbited independently.",null,0,cdixxy4,1qykuu,askscience,new,1
Goeatabagofdicks,"Believe it or not, gravity is actually considered a weak force.  With response to the question you asked, like the others have said, you are referring to cetripetal force.  With gravity, you are creating a warp in space time depending on the mass of the object.  So, when astronaughts are ""weightless"" in space, truly they are not.  The are constantly falling towards the earth.  Idk if you are from America, but here, in various malls and such, we have a concave coin donation object where you place a quarter in and it spins around and around going faster and faster until its ultimately deposited into the hole in the center.  This is a great representation of space time.  The more concave it is, the greater the mass of the object.  Size really doesn't matter, it's mass.  If I had a cotton ball the size of the earth, it would warp space time significantly less than a lead ball the size of the earth.  Further, the ""falling"" of the object is independant of the rotation of the earth.  Now lets speak of centripetal force.  Lets say I attach a pole of infinite strength to Pluto.  It takes something like 248 years for Pluto to have one rotation around the sun.  If its orbit in the solar system was directly related and fixed to the rotation of the earth, it would be hauling butt around the sun.  Much like if I put you on rollerscates in a parking lot.  I'm in the center holding a pole that you are whipping you around.  Depending on the length of the pole, the faster you go.  Gravity is a warp in time space, what you suggest is centripetal force.",null,3,cdhuhyx,1qykuu,askscience,new,1
Diracdeltafunct,"Well they dont really ""orbit"". The word orbit is kind of an artifact of the original model of the atom.  Now its often still erroneously kept around because each eigenstate of an electron carries some angular momentum (similar to an orbiting or spinning body) and we thus call the quanta of angular momentum ""orbitals."" The shape of these orbitals are entirely dependent on this angular momentum.

Electrons are negatively charged and are thus attracted to the nucleus. The electron does not just get pulled into the nucleus for  reasons: 1.  It has a large amount of translational energy to keep it moving  2. There is an eventual strong force repulsion that creates a barrier. ",null,1,cdhym9u,1qyjyy,askscience,new,6
MayContainNugat,"Electrons are attracted to the nuclei of atoms because of the electric force, not gravitation. But like gravitation, the attraction (in this case, between positive and negative charge, as opposed to masses) prevents the electron from flying off and keeps it close to the nucleus.

The shapes of orbitals, labeled S, P, D, F, etc., are due to the electron not really being a hard sphere like a planet, but more like a wave. When electrons are free, not in atoms, their waves take on the familiar wavy shapes of sines and cosines that you might find on vibrating strings. But when held inside an atom, they take on the shapes of vibrating spherical membranes (like soap bubbles or the surface of the Sun). Spheres can vibrate all as one (the whole bubble going in, out, in, out, etc) and that would be the S wave. Or... two poles of the sphere can move inwards while the rest of the sphere moves outwards... and then the other way around. That would be the P vibration... and so on.",null,1,cdi54ru,1qyjyy,askscience,new,5
chrisbaird,"The force binding electrons to atomic nuclei is the electromagnetic force, pure and simple. Gravity is far too weak at this scale to play any role. The electromagnetic force is fundamental, so there is no deeper force causing it. The simple picture of an electrically negatively charged electron being attracted to an electrically positively charged nucleus is fundamentally true and is not some over-simplified picture taught to beginners. The electromagnetic force is carried by photons, so photons are exchanged between objects that have electric charge as they interact with each other.

The electrostatic force is a central force, similar to gravity, but when you add in magnetic effects, it gets more complicated. Also, electrons are not solid balls, but are quantized wave functions that spread out to fill the atom. The ""orbitals"" of an atom are not averaged locations of the electrons. The orbitals *are* the electrons. At a single instant of time, the entire orbital exists in its spread out form.

The shape of electrons when in the form of atomic orbitals is caused by an interaction of electromagnetic attraction and angular momentum, and by the fundamental quantum nature of an electron as a smooth, three-dimensional vibrating wavefunction.",null,0,cdiyd15,1qyjyy,askscience,new,2
Olog,"In short, because Earth moves in its orbit. This is really best explained with pictures. NASA has a tool to view orbits of various small bodies, [here](http://ssd.jpl.nasa.gov/sbdb.cgi?sstr=ison&amp;orb=1). It's a bit clunky to use so I took some screenshots, [here](http://imgur.com/a/ixdii).

The first image is Jan 1st 2012, the comet is beyond Jupiter's orbit and Earth is roughly between the comet and the Sun. The second picture is Apr 1st, Earth has moved to the left side of its orbit while the comet has moved fairly little. It should be fairly easy to see why the comet appears to move sideways. It stays mostly in the same direction as seen from the Sun but Earth is moving sideways.

Up until now, Earth has been moving left, but now it starts moving right, so naturally the apparent motion of the comet will also change direction, and you can see this in the picture you linked.

The reason why the apparent motion doesn't change direction again in October is that by now the comet starts getting close to Earth's orbit, and inside it by November.

Edit: I just realised that your website actually has a video which might better visualise this than my pictures. The orientation is just different. And also note that the time slows down the further it goes. The date is Jan 1st just as the picture fades in and Earth is between the comet and the Sun. April is when Earth has done quarter of an orbit, only a couple of seconds into the video, and starts moving up again.",null,1,cdhv4j1,1qyip9,askscience,new,4
OrbitalPete,Image here http://www.nasa.gov/images/content/738138main_ISON_track.jpg,null,1,cdhuqr9,1qyip9,askscience,new,2
OrbitalPete,"Yes. In fact, per unit area, the UK gets more (reported) tornadoes, whirlwinds and waterspouts than ~~anywhere else~~ any other country in the world*. The difference is that we don't have the big open plains environment that allows truly massive storms to develop, so what we get are usually no more than an F0.

In the US you have a conflagration of factors which produce very strong tornadoes - you have a very wide flat basin plain across the centre of the country, over which cold polar air can interact with warm tropical air.  In Europe and Asia these air masses have a very much harder time coming together because we have the Alps and the Himalaya respectively. 

*See wazoheat response below",null,1,cdhujc6,1qyihv,askscience,new,7
OrbitalPete,"You start with a river passing over a relatively flat geography.

Now you add some tectonic uplift, or a sea level retreat. You've now added a height difference between source and ocean, so there is more potential energy available. The river now has to cut down to reach the ocean. As it cuts down you can experience more uplift, leading to an gradual but persistant vertical carving of the canyon system.  That's about as complicated as it gets. IF the gradient of the river is low enough you can generate nice wide meanders, or if it's carving down quickly at a steep gradient you tend to get straighter channels. All of this can be effected by the local geology, so if for example the river passes first over a hard rock, then a soft one it will erode the soft rock preferentially, leading to waterfalls.",null,0,cdhuq3r,1qyi8i,askscience,new,4
grikgrok,"Someone can probably write up a more comprehensive explanation but, first of all the bases bond Adenine to Thymine and Guanine to Cytosine, this results in a ""coding"" strand and a ""template"" strand in gene transcription. 
     Genes have information stored in codons, groups of three nitrogenous bases. Each possible sequence codes for an amino acid, the start of a gene, or the end of a gene. Here's a table showing the codes. http://www.uic.edu/classes/phys/phys461/phys450/ANJUM02/codon_table.jpg . The genes are ""transcribed"" to RNA molecules which leave the nucleus of the cell and ""translated"" into proteins.
     This translation takes place in the ribosomes of the cell, tRNA with complementary ""anti-codons"" bind to the RNA and add their amino acids to the growing protein.
     As not all genes are expressed all the time, you don't have hair growing out of your eyeballs, gene expression is regulated. Eukaryotic gene regulation is rather complicated, hopefully someone can give a good explanation of it here.
*the U in the table refers to Uracil, the RNA equivalent of Thymine, there is no thymine in RNA",null,0,cdhspea,1qyi5k,askscience,new,2
baloo_the_bear,"The DNA base pairs are grouped by threes. Let's say there is a length of DNA that reads ATGTGTCACATGACA. Now, RNA polymerase can come along and create the appropriate complementary mRNA for this strand: UACACAGUGUACUCU. In this strand, Thymine is replaced by Uracil. 

Now, the base pairs are grouped by threes: UAC-ACA-GUG-UAC-UCU. Each of these groups, or **codons** is matched by a specific, complementary tRNA. The tRNA that matches UAC will be ATG, and will correspond to a specific amino acid. As the codons get read, tRNA will bring in amino acids that are specific for each codon and eventually a protein strand will be made. ",null,0,cdhyagj,1qyi5k,askscience,new,2
itsokaybyme,"A,T,C and G's are on a strand of DNA, certain sequences of bases act as a trigger/flag for RNA polymerase and say, ""hey I'm a sequence of genes, I make a protein, copy me."" This flag is called a promoter region. This region is highly regulated and different subunits that attach to RNA polymerase ""see"" different promoter regions (think of a screwdriver with interchangable heads). 

The sequence that the RNA polymerase copies is usually pretty long and becomes mRNA (messanger RNA) which goes to be translated into a protein by a ribosome. However mRNA can be cut up and rearranged by various enzymes and RNA to alter its message, hence why we can get a lot of information from our DNA even through it is only 4 bases.

This whole process is also tightly regulated. Different hormones and chemicals affect how and when genes are transcribed. It's a very complicated process! Hopefully this helps to give you a brief into to the subject.

Source: Molecular Biology major. ",null,0,cdj5meb,1qyi5k,askscience,new,2
ssjsonic1,"The effect that /u/veganakos mentioned is not a strong effect.  The dominating mechanism is as follows:

As you turn up the temperature, the composition in the atmosphere changes.  Cool stars have many transition lines because the atmosphere has molecules and they have many ways of absorbing photons.  Heat up the atmosphere, and the molecules break into single atoms.  Single atoms also have lines due to the transition between electron states.  However, turn up the heat more and the electrons begin to be stripped away from the atom.  More heat means less electrons bound to atoms and less possible transitions.  Hydrogen and Helium transitions require a good chunk of energy, so hotter stars will produce deeper absorption features here.  However, for the hottest stars, the fraction of ionized H and He increases and the lines become weaker once again.

Here are some ionization energies ordered from the outermost electron to the innermost electron (remember H and He only have 1 and 2 electrons):
http://dept.astro.lsa.umich.edu/~cowley/ionen.htm

Edit: Notice that most elements have a small requirement to remove the first electron, but a much larger requirement for the second electron (H and He are an exception to this where the first electron requires a medium amount of energy).",null,0,cdia12v,1qygpc,askscience,new,2
veganakos,"Due to the strong gravitational forces at the surface of a star, heavier elements tend to settle down at deeper layers. However, if the stellar envelope is sufficiently cold, [convection](http://en.wikipedia.org/wiki/Convection_zone) becomes the dominant mechanism with which energy is transported from the interior to the surface. This allows heavier elements from the deep layers to move at the surface.    ",null,0,cdhuwwi,1qygpc,askscience,new,1
polymercury,"*Layman's speculation as a placeholder until actual scientists arrive:*

Realistically?  A big enough terrestrial planet in a developing solar system will usually acquire an extremely massive gas atmosphere, i.e. become a gas giant.  Jupiter's rocky core is predicted to be somewhere between 12 and 45 times the mass of Earth.  Now where that threshold mass is numerically, I don't know.",null,0,cdhrn3r,1qyfli,askscience,new,3
jswhitten,"For much of Earth's history the temperature at the poles was much warmer than it is today. We just happen to be in an ice age right now.

If the CO2 level in the atmosphere became high enough, the greenhouse effect would again result in less temperature difference between the poles and lower latitudes.",null,1,cdi7llr,1qydow,askscience,new,5
Gargatua13013,"A - ""Comfortably"" is a very relative term - The Inuit live in the high arctic and consider their territory an eminently comfortable place to inhabit.

B - If by comfortably, you mean comparable to conditions in the seasonal temperate zone, there were elaborate forests, with redwoods and mixed hardwood-conifer assemblages growing on Ellesmere Island during the Eocene, about 30 My ago (see:http://www.thecanadianencyclopedia.com/articles/fossil-plants). As you can see on this map, Ellesmere was in an Arctic position at the time: http://eas.unl.edu/~tfrank/History%20on%20the%20Rocks/Nebraska%20Geology/Cenozoic/cenozoic%20web/2/Timescale.html#6 .

C - Worth remembering: the effect of oceanic currents such as the Gulf Stream on actual climate is tremendous. As an example, consider the respective positions of Kuujjuaq and, say, Thurso in northern Scotland. Kuujjuaq is at the edge of the treeline with a definitely arctic climate, while the vicinity of Thurso is surrounded by fields denoting agricultural activity which would be unimaginable in Kuujjuak.

As far as your vision of aurorae above cityscapes, we see some quite regularly in southern Canada, around Montréal for instance provided you can get far away from the city lights and its light pollution. You might want to have a look in northern cities such as Oslo, St-Petersburg, Murmansk, Arkhangelsk and Edinburgh.

Also, keep in mind that aurorae do not form at the pole *per se*, but in a ring centered on the poles. ",null,0,cdi3o0d,1qydow,askscience,new,2
disconnectedLUT,"I bet even the poles on Mercury are hotter than a comfortable temperature...

I think it is always possible for the poles to be hotter, but I'm not sure if something large enough to be a planet could have a comfortable temperature at the poles and the equator. Perhaps a planet that has less/different land mass than earth could have better ocean currents to smooth out the temperature more effectively...",null,0,cdhsjds,1qydow,askscience,new,1
EdwardDeathBlack,"From what I have been taught, pure evolutionary snafu, just like [the laryngeal nerve](http://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve#Evidence_of_evolution). 

The octopus, which evolved its eye structure separately from our evolution path, has it built the [""right way""](http://en.wikipedia.org/wiki/Cephalopod_eye) . So it is perfectly possible to do it the ""right way"", but we are in a metastable trap caused by evolution. ",null,0,cdhtiaj,1qydk6,askscience,new,3
opticreason,"Yes it's just evolution. Evolution thinks up a good idea and even if it's not perfect, it runs with it. 

Cephalapods, which  are thought to have evolved vision in parallel to vertebrates, actually have this [""better"" design](http://en.wikipedia.org/wiki/Cephalopod_eye)

",null,0,cdhrq4d,1qydk6,askscience,new,1
OrbitalPete,"While we may be able to work out how the composition of Mars' atmosphere changed over time, we certainly do not currently have the means to replenish it, and there's definitely no way we're giving it a magnetic field.

Kim Stanley Robinson's 'Mars' trilogy gives a great account of how we might go about terraforming Mars, with the best suggestion for generating an atmospehre being to use comets. Basically, keep towing comets into Mars approaches so they burn up in the existing atmosphere, adding lots more water etc. But this is way beyond our capabilities at the moment, and would require a phenomenal amount of cometary material.

Starting up the geodynamo that would maintain a magnetic field is even further off the scale of possibility. The amount of energy you would have to inject into the mantle and core of Mars is just staggering, and we have no method for doing so even if we were able to generate that amount.",null,0,cdhuobt,1qyd2k,askscience,new,2
lasserith,"Most clothes are made of polymers. These polymers are actually semi-crystalline liquids at room temperature. If these polymers are heated too hot they lose their crystallinity and will melt together. The lower the temperature these polymers form a 'melt' the cheaper it is to process as the factory that produces them needs to use less heat. Thus most synthetic fibers clothes are cheap because they use polymers with a low melt temperature. On the other hand because cotton is a natural product it doesn't need to be produced/melted but merely spun. Thus it doesn't matter the temperature at which it it softens. This means that in general you need a far lower temperature to soften out synthetic clothes then cotton clothes. 

Synopsis: Temperature required to iron is based off the temperature required to soften the polymers that make up the clothes. 

Edit: Nice example: Think of two extension cords tangled up. When they get all tangled it requires a certain amount of energy to untangle them (Shaking or moving the cords through knots etc). If you apply too much energy say by securing one cord to your truck and the other to a post and driving off you don't solve the knot problem but rather are more likely to break the cord.",null,0,cdied90,1qyc3x,askscience,new,1
FujiKitakyusho,"Hydrogen combusts according to the equation


2H2 + O2 --&gt; 2H2O (-286 kJ/mol)

This reaction is exothermic, but it is not spontaneous.  It requires a minimum amount of energy to initiate, known as the activation energy of the reaction.  When you light hydrogen on fire, the spark or other source of ignition you use provides the initial activation energy. After that, the energy released by the reaction itself sustains the reaction by providing the activation energy for more hydrogen to react, and this continues until the energy released by the combusting hydrogen is insufficient to provide the activation energy for more reactions, or until one or more of the reactants is consumed. When you cool a reaction, you remove energy by conduction, convection or radiation, and reduce the amount of available energy to activate more reactions.  Thus, it is not the absolute temperature that determines whether a reaction will proceed, but rather the balance between energy produced and energy lost. As soon as more energy is being lost through cooling than is being produced by combustion, the amount of available energy decreases until it falls below the activation energy, at which point the reaction stops.
",null,0,cdhxe90,1qybou,askscience,new,5
The_Serious_Account,Hydrogen ignites at 500 degrees Celsius.,null,2,cdhyd1m,1qybou,askscience,new,1
null,null,null,0,cdhykph,1qybkw,askscience,new,2
jadiusatreu,"Insects that are more closely related can indeed hybridize. However, most hybrid suffer from sterility or at the least have very poor fitness: Meaning they wont survive very long or might not be able to reproduce.

The Concept of a Species is really whats gets you into trouble because, what we label as two species may still be able to reproduce with one another.  We classify species by letting nature do the work: If they don't recognize each other as mates in nature, we call them separate species.  But there are cases where two species are still closely related (Genetically similar enough) to reproduce. This hybrid rarely (if ever) goes on to become a separate species (as far as we know, there is some evidence of that possibility).

But as mentioned before, you can't get two very different insects to mate, genetics are two different. Plus they may not love one another.",null,0,cdi25r2,1qybkw,askscience,new,2
Urgullibl,"Eventually, you will be capable of understanding the language as-is. Languages differ in grammar and syntax, which makes this sort of simultaneous in-head translation impractical for your brain.

Source: I speak 5 languages. English isn't my first, and I wrote this comment without thinking about its content in my native language.",null,2,cdhsj8m,1qybcz,askscience,new,15
null,null,null,1,cdhxul6,1qybcz,askscience,new,2
sargonkid,"I speak three languages very fluently (native level) - The first one was from birth - the second I started when I was about 10 - (moved to another country).  It took me a long time to not have to translate at all - decades of constant use of the second langauge - in fact - I did not speak the first langauge after moving.  About 15 years ago (age 40), I started a third langauge (immersed in it)- and it too, has been taking a long time.

It seems the older we get, the harder it is to make another langauge native to us.  I am in no way an expert - just know what I have gone through - and I am guessing the only way to become truley multilingual is to start from birth and learn all at once and at the same time - equally.

Funny thing though, I still dream mostly in my first langauge.

EDIT - Understanding a language has many levels.  To get by day to day is a lot lower level of understanding than communicating the abstract - such as politics or religion.  The true test of fully understanding (for me, not necessarily for others) was when I could carry on a deep, emotionally charged, controversial conversation.",null,0,cdj2bhz,1qybcz,askscience,new,1
Stanage,"Yes, you *can* control the oxidation state of the product, although like most things, they will come in an equilibrium.

Although it is a completely different and unrelated process, think of protonation of a product.  Think of PO43-, which can occur as H3PO4, NaH2PO4, Na2HPO4, and so on.  You manipulate the product of these reactions by altering the pH of the reaction, with lower pH values favoring the more protonated state of PO43- since there is a higher abundance of available protons.

The same thing occurs with oxidation state.  In order to oxidate chemical A, you *must* reduce chemical B (B is the *oxidant* here, and A is the *reductant*).  In addition, you must be in favorable conditions so that once you oxidize A using B to make C, that chemical C is in the presence of enough of another oxidant (one strong enough) D to oxidize C to chemical E, and so on.  

Typically, when you react two species in a redox reaction, you will only be in favorable conditions for the production of one product in a given oxidation state, **assuming you do not have an excess amount of the oxidant**.  If you do have an excess amount, and say your oxidant is oxygen gas (O2), then you can keep oxidizing a compound until O2 is no longer strong enough to oxidize the resulting species to a higher oxidation state.",null,0,cdhs7s7,1qyb7u,askscience,new,2
LemurianLemurLad,"This question is very awkwardly worded.  I'm going to assume you mean most mechanoreceptors (cells that sense pressure), in which case it's definitely the fingertips.  Following that, it's probably the lips or the surface of the eye, but I am less certain on those two.

If you'd like a better answer, you'll probably need to be more specific about what you're looking for.  Some areas are very good at sensing heat, while others will respond better to other stimuli.",null,1,cdhrtwq,1qyb5l,askscience,new,2
homininet,"One rough way to look at this is by looking at the relative contribution of the sensory cortex of the brain to certain body parts. There is a famous figure called a [homunculus](http://sciencedefined.files.wordpress.com/2012/01/sensory-maps.jpg) which shows this. You can see from the figure that places like the face and lips and fingers and hands are incredibly sensitive relative to other areas.

Also google cortical homunculus if you want to see some other werid pictures of this representation.",null,0,cdhxf6m,1qyb5l,askscience,new,1
OrbitalPete,"Like this
http://www.space.com/12633-perseid-meteor-shower-space-photo.html",null,1,cdhulmm,1qy87w,askscience,new,3
O_Zenobia,"The methodology you're looking for is called ""reverse genetics"", but you don't need to do anything that crazy. 

Check out this paper:

Watanabe, Shinji, et al. ""Production of novel Ebola virus-like particles from cDNAs: an alternative to Ebola virus generation by reverse genetics."" Journal of virology 78.2 (2004): 999-1005.

Essentially, they took apart the Ebola genome and cloned genes for each capsid protein into plasmids, which they transfected into cells in culture. The virus-like particles assembled from their components. 


Or you could make a replication-incompetent virus and observe cell entry that way.",null,0,cdhtc6z,1qy845,askscience,new,3
sporclesam,"Additionally , there is the interesting/scary issue of [aggravated human virulence as well] (http://healthland.time.com/2011/12/21/bioterror-should-scientists-describe-how-to-make-a-man-made-killer-flu/). But viral infection studies can be done by suitably modifying certain aspects of the genome (the [HIV1 Protease] (http://www.rcsb.org/pdb/101/motm.do?momID=6) comes to mind as an achilles heel) in trying to ""dumb down"" the virus. Attenuation is necessary, but it also depends on what is being studied. 
However, replacing the genome would be counterproductive as you would lose the machinery which dictates cell entry &amp; replication. ",null,0,cdhtgo3,1qy845,askscience,new,2
Rhioms,"Mostly No, there is a lot of planning that goes into making the parks the way they are, and this involves a lot of landscaping. ",null,0,cdhtng4,1qy6rg,askscience,new,3
Astromike23,"While the technique of using a single telescope over 6 months works well for [parallax](http://en.wikipedia.org/wiki/Parallax) of nearby stars, it won't work for interferometry.

In order for interferometry to work, you must be capturing the same wavefront at the same time with at least two distant telescopes. This is because the separate telescopes must act as though they are part of the same telescope - you can think of it essentially as two reflective patches on an enormous, otherwise dark mirror. Interferometry allows the wavefronts from different telescopes to be in superposition and interfere, just as they would in a hypothetical enormous telescope. 

In order to properly allow these wavefronts to interfere on separated telescopes, that also means knowing the distance between your two telescopes to within one-quarter of a wavelength. This addresses your second question - for visible light, that means getting down to a precision around 100 nanometers. Knowing that kind of separation between orbiting telescopes, while difficult, is still technically possible...but it's still a bit outside our grasp both in terms of technology and funding.

There certainly have been proposals to create a series of orbiting telescopes as a space inferometer - [TPF-I](http://en.wikipedia.org/wiki/Terrestrial_Planet_Finder) stands out among them. As it is, however that project was cancelled as the funding for both the technology required as well as the telescopes themselves is currently lacking.

**TL;DR**: Aperture synthesis requires observing the exact same wavefront at different locations, so that's a ""no"" to your first question. For the second, it's technically feasible, but still slightly outside our technological capabilities and no one has been willing to fund it.",null,0,cdhsvxl,1qy62g,askscience,new,2
Staus,"The Standard Hydrogen Electrode is relatively easy to set up and, unlike the ones you mention, pretty close to the middle of the electrochemical series.  The chemicals in question are not too expensive nor too reactive and can be acquired in high purity, and can be stored for a long time with no large loss of quality.  For these reasons it was a good thing to use historically and the scale hasn't been changed.  No one actually uses them that often, though, these days.  

In reality most people measure these things versus a calomel electrode (mercury-mercury chloride) since that's even easier to set up (no gas!) or, even better, a standard silver electrode (silver-silver chloride) which is thankfully much less toxic.  Both of those (and many others) can be purchased cheaply in a self-contained package.  Just gotta keep them wet and topped up with KCl solution.  

http://en.wikipedia.org/wiki/Silver_chloride_electrode",null,1,cdhtt7z,1qy5rp,askscience,new,3
bjos144,"It's basically history.  People did a lot of early work with Normal Hydrogen Electrodes (NHE) because they could understand them, they're stable, and well defined.  These are a Pt wire in a solution of 1M HCl with Hydrogen gas bubbling around the electrode.  They have good properties for a stable zero potentail.  (If you need more on choosing a reference I refer you to Bard and Faulkner.)  

The problem is they're a huge bitch to work with, and eventually people found other electrodes to use.  But just like in a physics 2 class where you can set a potential anywhere as long as you dont change it, electrochemisty was stuck with the problem of choosing a zero.  They eventually extrapolated on the NHE in a theoretical fashion and defined the SHE (Standard Hydrogen Electrode) which has a defined potential of zero at all temperatures.  This is because it's defined in a theoretically infinitesimal morality acid solution (If memory serves).  After that it's just a matter of calculating/measuring the potential difference of your new reference (often Ag/AgCl) with respect to the community accepted zero.

TL;DR:  History, like most conventions.  ",null,0,cdhtmq7,1qy5rp,askscience,new,1
ididnoteatyourcat,"You are removing the fuel. A candle flame consists of a pocket of vaporized fuel (wax) whose surface burns, fed by oxygen. When you blow out a candle you are blowing away that pocket of vaporized fuel before it has a chance to be replenished. You have blown out the candle. The candle remains hot enough for a while to continue issuing fuel vapor, but you have blown away the burning surface that previously provided enough heat to ignite the vapor. ",null,0,cdhys9x,1qy5pe,askscience,new,5
Andannius,"You're more or less right that the volume of oxygen available for burning is increased when you blow on a candle. What you've missed is that the temperature of the oxygen you're blowing on the candle is actually much, much lower than the temperature of the air (which is slightly depleted of oxygen at this point) surrounding the candle while it's burning. You're essentially then reducing the temperature at the wick to the point where it can't sustain a flame and goes out. ",null,3,cdhqj9w,1qy5pe,askscience,new,2
cuweathernerd,"~~[This is a map of all officially recorded tornadoes in MI.](http://i.imgur.com/HpRoc5F.png)~~ - I'd say the more obvious trend is fewer tornadoes to the north.   There are tornadoes in the western part of the state.

That being said, the lake does modify the air above it.  Supercells and strong storms are *surface based* - they get their energy from parcels of air that start out near the ground.  So how the air close to the surface is modified will impact the storm.

In the case of severe weather months, the lake is cooler than the equivalent land. This means the low level temperatures are also cooler.  That has some benefit to tornadoes because it'll lower the cloud base a bit.

However, weaker ""low level lapse rates"" will also take away some of the storm's energy, called *convective available potential energy"" - which is very dependent on surface temperature.  Those weaker surface lapse rates will also weaken near surface updrafts (less change in temperature/density with height). 

That added stability from the cooler surface air is going to be the biggest impact.

(in this most recent storm, the system was long since linear when it got to you: discrete storms are much more tornadic.  Had the overall conditions remained favorable for individual storms, your tornado threat would have been more significant)",null,0,cdht76x,1qy5nn,askscience,new,2
owaisofspades,"Basically you've got two main muscles in your throat for producing sound. Your posterior cricoarytenoid which you use to produce sound, and your vocalis, which you use to modulate your pitch. People with better voices are just able to control their vocalis better, allowing for greater and better controlled pitch ranges",null,0,cdicmp1,1qy5mz,askscience,new,1
arumbar,"The substance that you overdosed on makes all the difference - there is no general 'overdose reaction' that occurs.  Remember what Paracelsus said: ""All things are poison, and nothing is without poison; only the dose permits something not to be poisonous.""  Overdosing on simply water (water intoxication) can lead to death through changes in electrolyte compositions leading to brain damage.  

Some other examples: overdosing on opiates or benzodiazepines/barbituates can lead to death through respiratory depression (you simply stop breathing).  Overdosing on iron can lead to severe metabolic derangements and shock through its effects on cellular respiration and other processes.  Similarly, aspirin poisoning can lead to metabolic acidosis and arrhythmias.  Acetaminophen poisoning can irreparably damage the liver, a vital organ.  The list goes on and on, but almost each substance has its own mechanism of injury and causes death in a different way.",null,1,cdhp5c3,1qy4xr,askscience,new,7
MCMXCII,"Photons and gluons both have zero mass. However systems of photons and/or gluons can have mass. Mass is equivalent to (with a conversion factor of c^2 ) to the energy something has in its rest frame. So baryons, which are bound together by the strong force, which can be represented as the exchange of virtual gluons, has that strong binding energy in its own rest frame. So that energy contributes to its mass, even though on-shell (real) gluons don't actually have any mass.

&gt;Does this mean gluons are 'moving' within the nucleus at close to the speed of light and gain mass?

Real gluons are alway traveling at the speed of light, and they can never have any mass, by definition. Virtual gluons can have whatever mass you want, but it's okay because virtual particles aren't actually real.",null,0,cdhox24,1qy17k,askscience,new,8
molliebatmit,"There are enough nucleated cells in blood (that is, not red blood cells) that DNA can be extracted from whole blood.

Prior to the availability of DNA amplification (PCR), it would be possible to take whole blood and determine its blood type (A, B, AB, O), which could exclude people and therefore possibly suggest that the blood belonged to a specific person in a group.",null,1,cdhp6zk,1qy0jh,askscience,new,5
chidgeon,"A lot of the times, drug abuse/addiction causes your body to create a new standard of homeostasis which revolves around taking the drug. So your new ""normal"" state is dependent on the substance and tolerance begins to develop to certain effects. Once you stop taking the drug abruptly, that's when withdrawal occurs and why symptoms are usually so severe. Basically, your body doesn't know what to do with itself because it's missing something it once considered normal to function. ",null,0,cdhq3t3,1qxzmb,askscience,new,4
derpalist,"With opiates, what happens when you become addicted is this:

Opiates(Morpine, heroin, oxycodone etc) mimic natural endorphins. Endorphins are the chemicals your body uses to combat pain(Both mental and physical), to cause sensations of pleasure and happiness. Opiate drugs are basically the same as these natural endorphins, except they do the job a lot better(Have a more potent effect). When you introduce opiate drugs into your system over a long period of time, your body stops the production of its own endorphins. It does this because when it is receiving endorphins from outside, there is no reason to waste energy producing the stuff. It is this deregulation of the natural production of endorphins that is the physical addiction: If you suddenly stop consuming the opiates that your body is now dependent on, you will find yourself in a situation in which you are not receiving opiates from outside, nor producing your own; this is withdrawal. Over a period of time(About 3-7 days in the case of heroin addiction) your body will begin its own production of endorphins again, but during those 3-7 days, your life will be absolute torture, as the chemicals that combat pain and cause pleasure, are simply absent from your body. The only thing you feel during this withdrawal period is pain.

Source: I'm an opiate addict",null,0,cdhyaoi,1qxzmb,askscience,new,1
KarlOskar12,"Addition is frequently given two criteria: physiological addiction and tolerance (There is mental addiction, but that's a completely different field of study). Tolerance is built up when receptors are exposed to a stimulus enough to desensitize them (which can happen by down regulating presentation of the receptor, conformational changes in the receptors, by killing off the cells, etc.). Physiological addiction sort of goes hand-in-hand with tolerance. It occurs when the body adapts to the presence of exogenous molecules stimulating receptors in the body by ceasing production of that molecule. This is why withdrawals cause paradoxical effects. Opiats for example cause constipation, withdrawals cause diarrhea.

Source: various textbooks and articles",null,0,cdi14rk,1qxzmb,askscience,new,1
AutoModerator,"Thank you for your submission! Unfortunately, your submission has been automatically removed because it contains the phrase ""**ELI5**"", so it is possible you are looking for /r/explainlikeimfive. If you would like scientific answers, you can repost your question to /r/AskScience though! Experts will always simplify and explain, so that even difficult concepts are easy to understand. Thanks for understanding. :)

*[I am a bot](/r/AutoModerator/comments/q11pu/what_is_automoderator/), and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose?to=%2Fr%2Faskscience) if you have any questions or concerns.*",moderator,4,cdhmc3k,1qxyx3,askscience,new,3
Helen___Keller,"Hi! What you're getting confused about is what it means for something to be a ""larger"" infinity.

In naive, common-sense terms, yes, we are mapping a smaller infinity to a larger one, but in strict mathematical terms, the two infinities are exactly the same. Let me explain:

In mathematics, we measure the size of a set using what is called the ""Cardinality"" of the set.

We say set A and set B have the same cardinality if and only if there is some bijective (one-to-one and onto) mapping between sets A and B. For example:

There is a bijective mapping between the natural numbers {0,1,...} and the integers {...,-1,0,1,...}. It is constructed easily: simply have the even natural numbers map to non-negative integers, and the odd natural numbers map to negative integers. Thus, mathematicians will say the Natural numbers and the Integers have the same cardinality, or the same size.

To answer your question:

&gt; Is every point actually represented?

Yup! This can be verified by proving your transformation is one-to-one and onto. If it is, then each element of your 2x2 square is mapped to by one and ONLY one element of your unit square.

Finally, let me wrap up by saying that not all infinities are the same size, even though we have seen that some (1x1 vs 2x2 squares, integers vs naturals) are the same size. It has been shown (via Cantor's famed diagonalization argument) that there is an infinite set that is *strictly* larger than the natural numbers (for example, the real numbers). There does not exist a bijection between the natural numbers and the real numbers.",null,2,cdhp90o,1qxwuy,askscience,new,12
control1757,"'Measure' isn't the same thing as cardinality. You can map [0, 1] to [0,2] using f(x) = 2x and the cardinality of sets is different but it's still a well defined bijection. 

But clearly it makes sense to say that one of these sets is larger than the other. Naively it's a pretty straight forward process and works exactly how you would think, [0,2] has a 'measure' of  2 with respect to the fact that we are in 1d. Or that [0,2] X [0,2] has measure 4 with respect to the fact that we are in the 2nd dimension. 

But a wrench was thrown into this with the Banarch Tarski Paradox http://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox which basically just says that using this naive construction  of measures, ZF set theory and the axiom of choice they were able to construct 2 spheres from 1 of the same size without doing anything like stretching the boundary. Because of this measure theory/integration is now one of the first math courses you'll take if you go onto graduate study or in your senior year. ",null,0,cdn8pf1,1qxwuy,askscience,new,1
iorgfeflkd,"We can measure how far away things are, and we can measure how fast things are going. Hubble in the 1920s made observations of the speed and distance of supernovae, and found that the farther away they are the faster they appear to be receding from us. There are two conclusions that can be drawn: the Earth is in the centre of the Universe and everything is moving away from it, or everything is moving away from everything else. The latter implies the expansion of the universe.",null,2,cdhls0q,1qxw1j,askscience,new,5
The_Duck1,"As far as we know, there *isn't* an edge to the universe. It may well be infinite. [However, since it has only been a finite amount of time since the Big Bang, light has only be able to travel a certain distance and so we can only see out a certain distance; the region we can see constitutes the ""observable universe.""]

When we say that the universe is expanding what we mean is that on very large scales, all the galaxies are spreading apart. We know this because when we observe distant galaxies we see that they are all receding from us. We can tell that they are receding because the light we receive from them is [redshifted](http://en.wikipedia.org/wiki/Redshift) by their velocity.",null,0,cdhqzss,1qxw1j,askscience,new,2
null,null,null,2,cdhlxvg,1qxw1j,askscience,new,1
BoxAMu,"Power delivered by the source is P = VI.  The question is what the power is going to be used for.  For a resistive element in a circuit, V=I/R, so P_res = I^2 R.  This shows that the power lost to resistance goes like the square of the current, so to cut down on resistive loses one wants to minimize the current.  

If voltage is made very high and current is made very low, then the total power P=VI can stay the same, but less of it will be wasted on resistance (which simply heats the wires).  This means that for high voltage transmission, more of the power is spent on doing useful work.",null,0,cdho6z2,1qxtyn,askscience,new,5
selfification,"/u/BoxAMu has a relevant answer but it in my opinion, it doesn't quite address why there is different amounts of power loss when you vary voltage.

I'll try:  Power = voltage * amperage.  Correct.  Consider for a moment a power generator that produces a certain constant amount of power (from burning coal or splitting atoms or whatever).  You can't change this amount.  You goal is to get as much power to your customers (desired power loss) as possible and to reduce as power loss in your circuit (undesired power loss) as possible.  After all - all the power must be consumed/lost at some point.

Let's consider a simple series circuits of 2 resistors.  Pretend that one of these is the load resistor (the customer's toaster) and the other is the resistance of the power line.  From the above constraints, you can't actually change the voltage at all.  The voltage you can apply is entirely determined by the max power you can dissipate.  The only way you can change the amount of power on the load is by varying the resistance of the power line.  If you can reduce the resistance of the power line to 0, then all power will be delivered to the load resistance.  In a purely resistive circuit, you can't vary voltage while keeping the total power output constant.

So clearly all this voltage up/down conversion has to do with circuits that aren't purely resistive.  Power transformers are coupled inductive circuits.  It uses Faraday's law to exploit the fact that a changing magnetic field inside a closed loop with generate a non-zero ""voltage"" (more technically, a non-vanishing line integral of the electric field along the closed loop).  You can read about this at http://en.wikipedia.org/wiki/Transformer.  The trick here is as follows - there are 2 circuits: the circuit from the power source through one side of the transformer windings and the circuit with the load through the other side of the windings.  If these two windings were isolated from each other, no power would flow from one to the other.  The power supply (i'm assuming an AC power source), would see some self-inductance from the windings that would act like an antenna, but that's about it.

If we coupled the two circuits, things change.  The impedance seen by the power supply now changes (impedance is the fancy word for resistance with capacitance and inductance worked into it).  The power supply now sees the load resistor, but doesn't directly see it.  Because of the way the transformer is structured, it sees the load resistor as a multiple of what the load's impedance actually is.  This ratio of what the power source sees and what the impedance actually is is determined by the square of the ratio of windings on either side of the transformer.  You can see the trick we've done here.  Instead of simply lowering the resistance of the wire, we instead found a way to cheat and increase the impedance of the load, helping us proportionally deliver more power to it (not necessarily true when dealing with impedances but it happens to be so in this case).

The voltage balance on either side of the windings is an outcome of this ""cheat"" that we performed.  Since the actual load expects a certain voltage to satisfy its power requirement, the apparent load that the power source sees through the coupled circuit now requires a much much higher voltages (based on the square of the ratio of windings) to deliver the same power.  Therefore the power source increases its voltage to account for this, decreasing the total current in the circuit to remain at the same power level.  This circuit *does not* obey Ohm's law - at least not the naive formulation of it.  The apparent load seen by the circuit is *not* a resistor but has inductive elements to it.  More advanced treatment of this has to deal with the impedance matching of the circuits so that power actually makes it to the other side of a transformer.  In a circuit is not correctly impedance matched, power never actually makes it through to the coupled circuit and is returned to the source which is very very bad.",null,0,cdi0f5w,1qxtyn,askscience,new,2
O_Zenobia,"The other issue is that viral loads (and so infectiousness) are not level throughout the ""latent"" period -- a misnomer anyway, as the virus is setting up shop in various tissues throughout that time.

People are most infectious in the acute phase, during the burst of early replication a month or so after infection. The risk of transmission in the acute phase is more like 1/50. They probably have short periods of being highly infectious after the acute phase, too. During an outbreak of herpes or a syphilis infection, for example, viral loads go up and open sores facilitate blood-blood or blood-mucous membrane contact. 


Pilcher, Christopher D., et al. ""Brief but efficient: acute HIV infection and the sexual transmission of HIV."" Journal of Infectious Diseases 189.10 (2004): 1785-1792.

Cohen, Myron S., and Christopher D. Pilcher. ""Amplified HIV transmission and new approaches to HIV prevention."" Journal of Infectious Diseases 191.9 (2005): 1391-1393.",null,0,cdht4nv,1qxqmw,askscience,new,4
disconnectedLUT,"That rate is per sexual encounter. So a woman who has vaginal sex with an infected male partner twice a week for a year has a 1-(1-.0008)^(2*52)~=8% chance of infection. Over 5 years this becomes 34% or over ten years 56%.

The point is that even though the 'per encounter' rate is very low, the overall chance of infection can be very high for people who either don't know that their partner is infected or have frequent sex with many partners.",null,0,cdhp80o,1qxqmw,askscience,new,3
pheel23,"So many ways to answer you questions. I'll try and be brief.

The numbers are correct. Female to male transmission is quite low. However people are having sex. Billions and millions of people are having sex. Given data on condom usage there is very high rates of unprotected sex. Low risk over repeated risk taking results in a significant amount of HIV. This is for the U.S. and mostly true for the west coast. ",null,1,cdhoy4y,1qxqmw,askscience,new,2
medikit,"NSFW

People aren't unlucky, they just have a lot of unprotected sex. Especially those who are most at risk.

The anus and vagina have epithelium that are not keratinized so the virus can pass through them more easily. They can also develop micro-tears through which the HIV virus can travel. The penis including the glans (head) is keratinized and it is not as easy for the virus to cause infection in. The foreskin (present if uncircumcised) is not-keratinized and there is an increase risk of infection in uncircumcised individuals. Concurrent genital herpes and other ulcerative lesions can increase risk for all partners. Using a condom profoundly reduces your risk of HIV acquisition. Adherent HIV treatment (HAART) of the infected partner or prophylactic use anti-HIV therapy (PrEP) has also been shown to reduce risk.

Even so the rate is not 100%, virus needs to survive and make it into the blood. The virus that survives needs to find a receptive cell and successfully infect that cell. There are a number of places where this can go wrong and the virus will fail.

Risk of infection is as follows:

* Receptive anal intercourse: 1 transmission/200 sex acts
* Insertive anal intercourse: 6 transmissions/10,000 sex acts
* Receptive oral sex: 1 transmission/10,000 sex acts
* Receptive vaginal intercourse: 1 transmission/1000 sex acts
* Insertive vaginal intercourse: 1 transmission/10,000 sex acts

1. http://www.uptodate.com/contents/nonoccupational-exposure-to-hiv-in-adults?source=search_result&amp;search=hiv+infection+risk&amp;selectedTitle=1~150
2. http://www.ncbi.nlm.nih.gov/pubmed?term=10430236
3. http://www.ncbi.nlm.nih.gov/pubmed?term=18684670
4. http://www.ncbi.nlm.nih.gov/pubmed?term=20397962",null,1,cdhoz85,1qxqmw,askscience,new,2
redditor5690,"Bootstrapping is the term I think you're looking for.

The first electronic computers were programming with wires. The presence or absence of a wire at a decoded junction represented each data bit.

There were no ""languages"" then, just simple machine coded instructions.

After that, it's one small step after the next.",null,1,cdhrya2,1qxqf8,askscience,new,9
disconnectedLUT,"You could program the memory with switches (a set of data switches, a set of address switches, and a write switch). Then you could have a reset switch to set the processor to start executing code that you would have manually entered.

Deciding what data to write to memory is a simple matter of looking up byte codes for the assembly instructions you want to execute. This wouldn't be too difficult because early computers didn't have very many or very complicated instructions.",null,1,cdhotwc,1qxqf8,askscience,new,3
Chilangosta,"I thought that [this explanation](http://www.reddit.com/r/AskReddit/comments/1qogor/what_is_one_thing_you_cannot_understand_despite/cdezhe1?context=1) by /u/topupdown was very good and very easy to understand. He basically explains the evolution of programming, starting from the point you're discussing, with the very first computers. ",null,0,cdi7os2,1qxqf8,askscience,new,2
robogen,"Initially it started as hardcoded commands on circuitry. Literally, binary code. If electricity is coming through, then true. If no electricity, than false. Even modern day circuits work this way; the CPU in your computer can only communicate with binary digits. There was then developed a way to 'program' the binary digits, and save the newly created software. The code was incredibly hard to read, as the code was simply ones and zeros. And when I say 'read', this was still in the days where you had to write your code with punch cards.

Eventually there came assembly code. A compiler was created, which can read in assembly code and convert it to binary. Assembly code is much more human readable, and makes it easier to write a program. Still though, it was a complex language, and difficult to learn. So, a new compiler was written for a language called Pascal (or it was Fortran by this point, I get it mixed up). Literally, Pascal was written because the creator felt lazy and didn't want to work with such a complex coding language. 

Most languages are written on top of another one, and everytime this happens, the code becomes easier to read and work with, but much more resource intensive unfortunately. 

For examples sake, look at Java. Java is a fairly easy to read language, and decent to work with. But the code of the Java compiler is written in C++, which is built on top of C, which requires a compiler of its own to send instructions to the CPU in Assembly, which then get interpreted into Binary code which the CPU uses to actually run the program.

Sometimes I get into explanations and tangently get away from my original point, does this answer your question?",null,0,cdhswwy,1qxqf8,askscience,new,1
bc87,"http://en.wikipedia.org/wiki/ENIAC#Programming

The first computer was huge. It took up a whole room and used a huge quantity of electricity(compared to today's computers). They literally programmed using cables, switches, and plugs.

There's a documentary about the first-generation programmers. They were literally first-generation programmers, as there were no others before them (The scientist that designed the machine didn't count themselves as programmers).

http://www.imdb.com/title/tt1587359/

It's available on netflix.",null,0,cdlrno4,1qxqf8,askscience,new,1
iorgfeflkd,"It would look the same over large scales, there would just be other parts to observe.",null,38,cdhiwos,1qxn8n,askscience,new,155
lsdkdlsdk,"The universe is literally infinite in every direction.  Light travels at a finite speed and the universe has only existed for a finite amount of time though, so it's only possible to see the universe to a certain ever-expanding distance from your present location.  The parts of the universe that are close enough to see are part of the observable universe, the parts that are too far to see are not.  

The parts of the universe outside of the observable universe shouldn't be different from the parts of the universe inside of the observable universe.  It should have galaxies, stars, planets, etc also.  There's no edge anywhere out there.  If you were to travel to what is presently the edge of our observable universe, you would find yourself in the center of a different observable universe that looks pretty similar to our own.  

I'm guessing that you were asking this question from the perspective of time, though, and how the further out you look, the older the thing you're looking at gets.  

One important thing to keep in mind is that the universe wasn't always transparent to light.  Prior to the cosmic microwave background emission, it was opaque.  If you look out as far as is physically possible, you'd eventually see nothing beyond a certain point due to this.  Let's assume you've got some way around this.  Maybe a neutrino telescope or something?  I dunno.  

Sometimes hypothetical scenarios are useful, and provide insight to certain situations.  This isn't one of those scenarios.  A telescope like you described, that lets you see parts of the universe that haven't yet had enough time to radiate light our way, isn't possible to construct.  I'm sure you knew that, but the reason this isn't a good hypothetical scenario is because even if you did have such a magical telescope, there's no meaningful answer to what it would see.  As you observed parts of the universe farther and farther you'd eventually get to the point where you were viewing things happening mere moments after the big bang, then eventually to the exact moment of.  Looking any further would be completely meaningless.  There isn't a physical answer to what you'd see, or a theoretical answer, or any kind of satisfying answer at all.  ",null,19,cdhn3fa,1qxn8n,askscience,new,43
dont_read_this_plz,"Well the observable universe is what we can see of the universe from a fixed point. However changing that point also changes  the perspective of the ""observable universe"" we we can see. Its like climbing up a tree looking around for a bit then climbing up a tree a mile away, you're seeing different part of the observable forest, and there's more of the forest you haven't seen. Did this help?",null,9,cdhkbh8,1qxn8n,askscience,new,31
TwirlySocrates,"Most scientists think that the universe is the same everywhere. So if I were somehow living 13.7 billion light years away form here, it would be the same as here, only different galaxys.

If you're wondering what the universe was like 13.7 billion years ago, that's a totally different question.

Also, there's no such thing as a ""past the border where the laws of physics don't exist yet"". The universe is infinite (we think) and the laws of physics are the same everywhere (we think).",null,0,cdhtbh9,1qxn8n,askscience,new,13
pmreddick,"I won't claim that I actually know what I'm talking about here, but from what I can understand the universe doesn't have ""an edge."" It's kind of like if we lived on the inside of the Earth's surface rather than on top of it; so gravity pulled us outward rather than toward the center. Look as far as you want in any direction and 1. you're always in the center and 2. you will never find an edge.

On top of that, there's a weird theory that the [entire universe could be smaller than the observable universe](https://www.khanacademy.org/science/cosmology-and-astronomy/universe-scale-topic/big-bang-expansion-topic/v/a-universe-smaller-than-the-observable).",null,4,cdhn1jd,1qxn8n,askscience,new,11
ComplainyGuy,"Meta, when we dont know;

Can we just say ""sorry op, your question can only be answered artistically""

ESPECIALLY when it comes to things like ""why did humans evolve...an enjoyment to music"" or ""what does light look like if you are going faster than it and shine a light on it and then enter the 6th dimension""",null,3,cdhwpw4,1qxn8n,askscience,new,5
lekjart,"The best way I have found to understand an expanding 3D universe is the following:

Imagine you are working in a 3D authoring tool such as Maya or Google Sketchup. Now imagine that you define a regular cubic grid that goes on to infinity in all directions, where each cell has a unit length, say 1 m.

Now finally, assume that you can input a scaling factor, let's call it S, that can scale the length of the unit length.

So if S equals 1, then the unit length of your grid is 1.

If S equals 0, then the unit length of you grid is 0, which basically means it is undefined. You can't really picture a cubic cell with zero length sides.

The transition from S=0 to S &gt; 0 is basically your Big Bang: you go from a completely undefined (and non-existing) grid to a grid that extends infinitely in all directions with a unit length of S.

There is no center for the Big Bang. The whole grid comes into existence everywhere in space simultaneously in all directions.

Now finally, imagine that you increase the value of S smoothly with time. Your grid is basically expanding uniformly in all directions. If you position yourself at any point in the grid and measure how fast other points in the grid are moving, you will observe that all points seem to be moving away from you. Furthermore, the farther away a point is, the faster it goes away from you. For some grid distance (that depends on the how fast S is changing) the points beyond will be travelling at a speed faster than light as measure from your point. This means that information from those points in the future will not be able to reach you. This effectively defines an event horizon and that is the effective size of the measureable universe from your point of view. The Universe still goes on to infinity (except for some weird topology), but for all practical purposes it has a finite observable size for you.

PS: This is not a completely mathematically accurate description, but if you can wrap your mind around the above, you are more than halfway in understanding this.

Here are more gory details if you are interested:

http://en.wikipedia.org/wiki/Metric_expansion_of_space


",null,2,cdhykx6,1qxn8n,askscience,new,5
Rastiln,"I've seen a lot of speculation in this thread, and as a layman it's only made me more confused.  Let me ask some specific followup questions, and I hope that somebody qualified can explain this to me.

1) I've heard different people in here say that the universe is infinite and that the universe has an edge.  Do we know with a reasonable (let's say p&lt;.01) amount of certainty that it really is infinite?  Is most of our knowledge here based on factual data, or theoretical extrapolation, or pure speculation?

2) If the universe IS infinite, how is is ""constantly expanding""?  Does that simply mean that all objects are continuously moving further from one central origin (potentially the Big Bang), or am I off-base there?  If I am correct and all objects are moving away from some point of origin, then are they all moving to areas that are, or at least used to be, completely empty?",null,0,cdhwyjd,1qxn8n,askscience,new,2
logic_card,"The universe is expanding, if you could travel fast enough to reach the ""edge of the observable universe"" you would have to travel faster than the speed of light.

You could speculate that this will result in you traveling backwards in time, in which case the ""edge of the observable universe"" wouldn't be a place but a time, around about the time of the big bang.",null,1,cdi29hu,1qxn8n,askscience,new,2
Anhanguera,I believe the edge of the observable universe is as far from us in light years as the universe is old in years. We can observe that far because that's the farthest light could have travelled since the universe started. So I think there is no way to see past this 'barrier' but physics wise the 'laws of nature' are pretty universal by definition aren't they?,null,20,cdhjiy9,1qxn8n,askscience,new,6
null,null,null,13,cdhqevb,1qxn8n,askscience,new,1
null,null,null,26,cdhmeqh,1qxn8n,askscience,new,5
DrPeavey,"Snowflakes form in the atmosphere by a process known as the [Bergeron Process](http://weather.cod.edu/sirvatka/bergeron.html). 

[*(Instructional video here)*](http://www.youtube.com/watch?v=WByQ3yKxj1s)

Snow and ice crystals form macroscopic, hexagonal shapes due to the angles at which hydrogen bonds form between water molecules during the deposition process.",null,2,cdhk2m4,1qxfng,askscience,new,4
Gargatua13013,"Basic crystallography: there are a number of symmetry classes for crystals based on the shape of the crystal lattice: cubic, rhobohedric, orthorhombic, monoclinic and hexagonal.

The form of ice crystal stable at normal temperature and pressure is hexagonal.",null,0,cdhplc2,1qxfng,askscience,new,1
TangentialThreat,"This is the sort of thing people who breed plants and animals do all the time.

Similar arrangements where A breeds with B and B breeds with C but A doesn't breed with C are pretty common in nature; they are called [ring species](http://en.wikipedia.org/wiki/Ring_species). Speciation is often less than complete and nonlinear.

",null,2,cdhjbim,1qx9rg,askscience,new,12
FizixPhun,"The heart of your question lies in solid state physics.  This is a subset of quantum mechanics aimed at understanding why solids behave the way they do.  On a quantum level, current is still just the amount of charge that moves through some space in a given time.  The only difference is that the charge is now packaged up into discrete units (electrons, protons and other charged fundamental particles).

To understand how current flows in a material you first have to understand electrons behave in a material.  The key feature of solid state physics is that many materials are crystals.  This means that the atoms are spaced periodically.  As you mention, band structures are the way that we summarize the effect of this periodic potential.  Basically, a band structure just relates an electrons momentum (p=mv=hbar k) to its energy.  The momentum can be positive or negative, the sign only denotes direction.  In free space this is very boring, Energy=(m v^2 )/2 = p^2 /2m=(hbar k)^2 /2m.  When you throw in a periodic potential, this becomes modified and results in bands.   Actually calculating band structures is quite difficult.  The key idea is that there are ranges of energy where the electron can live and ranges of energy where the electron cannot live.  

The electrons in a crystal live in the band structure.  Each atom of the crystal brings a certain number of electrons with it.  They fill the states in the bands starting from the lowest energy.  Each of these states has a specific momentum associated with it.  When a band is filled, the next electron has to be placed in a state in the next highest band.   Applying a voltage to a material is the same as applying an electric field to the material (E=V/l where l is the length of the material).  In the semiclassical picture, electrons with charge -e, feel a force F=-eE in the applied electric field.  This force accelerates the electrons from lower voltage to higher voltage (they are negatively charged so lower voltage is actually higher energy for them as Energy=V*q where q is the charge, including the sign).  These moving electrons constitute your current.  A caveat to this is that electrons really live in quantum states and no two electrons can live in the same state(Pauli exclusion principle as electrons are Fermions).  The electric field really moves electrons from states with one momentum to states with a momentum that is in the direction of the electric field.  If the band is full, all the states are full and the electric field cannot change the electron’s state so no current flows.  This is an insulator.  When a band is partway filled, there are states that the electric field can move the electrons to.  This allows a current to flow.

Transistors are a little more complicated.  The main thing you have to understand is p doping and n doping semiconductors.  Imagine you have a crystal of silicon.  If you take out a silicon atom and put a phosphorus atom in its place, you suddenly have an extra electron.  A single phosphorus atom won’t change your band structure as you still have 10^23 silicon atoms so it’s like you just added an extra electron to your system.  Semiconductors have a filled band with another band with only slightly more energy (.5ish eV).  This extra electron from the phosphorus can’t live in our filled band, called the valence band, because there are no more states.  It must live in the next band, the conduction band.  If you apply an electric field, this electron in the conduction band can flow because pretty much all the states in its band are empty.  This is called n doping because we added an extra negative charge, the extra electron.  If instead of a phosphorus atom we add an aluminum atom, we have one less electron.  If the aluminum steals an electron from a neighbor, this neighbor now is missing an electron.  Instead, of thinking of the aluminum as stealing an electron, you can think of the aluminum as giving the neighboring atom an empty state.  This empty state is called a hole in solid state physics.  A hole is basically a missing electron and it behaves like a particle with charge +e.  If you apply an electric field to it, it can move around by trading places with an electron.  Again, you get a current.  We call this p doping a material as it is now missing an electron or you can think of it as having positively charged particles, holes.  Transistors are semiconductors with a p doped region surrounded on both sides by an n doped region or vice versa.  Honestly, I study physics and not material science or electrical engineering so I’m not super familiar with the details of how a transistor works.  I hope this helps.  Sorry it’s so long winded.

edit:I explain how to find band structures in two limits down in the comments.

edit 2:Paragraphs",null,61,cdho5j8,1qx8zd,askscience,new,368
akanthos,"Conduction band electrons aren't treated discretely, and aren't part of any particular orbital in general. Rather they are said to be 'delocalized,' with probability distributions that are spread out over the entire lattice instead of atomic or molecular orbitals. This is usually modeled as a 'sea' of electrons around cations. One model if this you can look up is the Drude model. 

A more quantum phenomenon is the promotion of electrons from the valence band into the conduction band. Electrons can tunnel across this potential barrier.

",null,20,cdhg6x3,1qx8zd,askscience,new,100
mcmad,"The picture of conduction you're describing is actually that used in the Hubbard model - used to study graphene nanoribbons for example. In these materials electrons are localised (or located) on individual atoms and this is very much the way to think about it.

But metals are very different. Some of the electrons in metals are actually spread over the whole material. This is why metals conduct electricity so well, as these electrons are free to move through the whole of the metal.

These two types of ""orbitals"" come up all over quantum mechanics. If you want to read more about this, the localised states are called bound states,  where as the delocalised states in metals behave like free electrons.
",null,2,cdhjdt9,1qx8zd,askscience,new,16
wbeaty,"&gt; would current moving through a regular metal be like each electron appearing in the closest neighboring orbital, as a chain reaction, when a voltage is applied to the metal?

It might help you to visualize the Classical Physics version first.  Then add in all the QM weird stuff on top.   Start with Newton and Maxwell.   There's lots of justification for this since electric currents certainly aren't inherently QM.   The QM becomes significant when the carriers are very low mass and in periodic potential wells: electrons in crystals.   But this doesn't apply to electric current in oceans and electrophorisis and sparks and in human tissue, where the moving carriers are enormous ions and in some cases are even directly visible.  To get a visual gut-level feel for the physics behind electronic components, assume that conductors are hoses full of salt water rather than metals.
___

OK, first, all conductors are always full of positive and negative charged particles, with one or both being mobile.  Whenever a conductive material is disconnected or sitting on a shelf unpowered, its charge carriers will be in constant high-speed ""thermal"" motion, same as with any liquid or gas.  This motion is required, since without it, all the opposite particles would fall together and become electrically immobile.  For example in sea water, if the positive sodium and negative chloride atoms all paired up and stuck together, then conductivity would vanish.  Any applied voltage wouldn't cause any carrier flow.   Thermal motion keeps the carriers free to respond to an e-field.

So, a conductor is like a container of gas, with the gas composed of two populations of particles:  positive charges and negative charges.  Overall the conductor starts out neutral, yet it's still extremely electrical, since the positives and negatives can flow along separately in different directions.  That's a Classical model of electrolytic conductors.

In Classical metals the positive particles are much more massive than the negative, and the positives are connected to the ""container,"" and can only move as it moves.  So, a Classical Physics model of a metal would be a positively-charged sponge wetted with a fluid of mobile negative particles.

What happens when we apply a difference in potential to the ends of a long conductor?   Well, all conductors are electromagnetic shields.  If we try to create an internal e-field along the length of a long conductor, this field will not instantly appear inside the material.  Instead, mobile charges at the surface of the conductor will try to flow so as to produce zero field inside the conductor.   For a perfect, zero-ohms wire, the applied e-field would only cause the mobile charges on the surface to start flowing.   So, close a switch, and all the mobile charges within the surface of the wire all suddenly begin flowing at about the same time.  They flow quite slowly.   But they all start up at once, like turning on a conveyor belt.  Next, quite rapidly the outer layer of flowing charges interacts with inner layers of movable carriers, and the deeper layers begin flowing as well.  The surface current ""sags inside"" the wire, and within a fraction of a second the entire charge-cloud inside the conductor is in slow motion.  In metals the positive stays still and the negative cloud moves along.   During currents in electrolytes (and in unrefined semiconductors, and in intrinsic semis) there'd be two populations of carriers moving slow in opposite directions; interpenetrating clouds of positives and negatives both flowing through each other.

Note that this slow avearage motion is added to the constant high-speed ""thermal vibration"" of all the charge-carriers.  The random ""dance"" of carriers is always there, and the momentary velocity of individual carriers is, on average, immensely fast.  (In salt water the charges are wiggling at the speed of sound; in metals the electrons fly around at nearly the speed of light.)   An electric current happens when the entire ""dance floor"" then moves along very slowly.   I like to visualize this as a screen full of television white-noise inside the conductor, where electric currents exist whenever the fast-sparkling screen starts moving along at about a tenth of a MPH.  This velocity is proportional to amperes: double the drift velocity and you double the value of current.

How about Classic Physics bandgaps?

:)

That's easy.   In our above model of metal as ""postive sponge containing a negative fluid,""  let's imagine that the negative particles can either be stuck to the sponge surface, or they can be flowing freely around inside the pockets of the sponge.   Let those stuck particles move along the sponge surface without breaking free.   That gives us some low-energy particles on the sponge.  ""Low energy"" because they've been attracted in and trapped.  It takes electrical energy to pull them away against the electrical attraction between positive sponge versus negative particle.   Also, we then have a population of ""high energy"" particles which haven't fallen down to the sponge surface, and they remain wandering around in the material at a ""higher"" level.   If a free high-energy particle should fall down and crash into the positive sponge, this gives out energy: it produces a significant EMP, a ripple of EM waves.   And, large impulses of EM waves are able to occasionally free one of the negative particles sliding around against the positive sponge surface.  If a particle breaks free, it momentairly casts a shadow and absorbs the EM waves which knocked it loose.

That's raw silicon with heavy p++ n-- doping.   Should I do intrinsic semis?   PN junctions?

:)


",null,1,cdhsltp,1qx8zd,askscience,new,4
novaya_zemlya,"At its core, the existence of a band gap in a semiconductor is a quantum phenomenon. Let's say you have a semiconductor with a large band gap (1 electron Volt) at 4 Kelvin. You apply a voltage difference of 0.5 electron Volts and still, the electrons don't go anywhere because there isn't enough voltage or thermal energy to excite the electrons across the band gap. But what IS a band gap? It's not a vacuum, it's just a bunch of quantum states that our would-be conduction electrons cannot occupy due to selection rules. Other things (bonded electrons, etc.) are occupying those states, so our Valence band electrons can't go there.

There are some phenomena where current is carried in discreet quanta (e.g. single electrons) that can be measured. However, this does not really apply to normal conductors (such as metals) under normal conditions (such as ambient temperature and pressure.) As others have said, current in a normal wire can be described by a ""sea"" of delocalized electrons which are free to move with an external electric field without being constrained by the localized electric potential of the individual atoms in the wire.

BUT, when you start looking at nanoscaled, low dimensional systems (such as carbon nanotubes, graphene lattice, or quantum dots, especially at low temperatures) conduction becomes quantized very quickly. In these special cases, instead of a sea of free electrons participating in conduction, you can have just a few, sometimes just ONE electron that is able to move from its current energy state to another available energy state. 

You start observing things like [Coulomb Blockade](http://en.wikipedia.org/wiki/Coulomb_blockade) and measuring [discreet quanta of conductance](http://en.wikipedia.org/wiki/Conductance_quantum).",null,1,cdhqie4,1qx8zd,askscience,new,4
drwho9437,"You can end up with band structure via tight binding or nearly free models of electrons in a solid. I feel like you are asking are the electrons really bound tightly or are they free?

I think you could generalize your question even further, what does anything look like at the quantum level. We pick wave and particle models because those classical systems share properties of the quantum world but neither is really correct. 

Transport in a solid can take place in all sorts of ways. There are electrons which are highly localized in some materials. These often create bands with low mobilities. In simple metals (say Na) the nearly free picture works well. 

But what does it look like? Well you have your normal uncertainty principle trade-offs but you really are asking I feel is what is the coherence length of the wavefunction. Over how many lattice points is the wavefunction spread out. Vanilla quantum statics you learn on the undergraduate level does not treat dissipation or inelastic scattering. If you presume atoms are localized and you have some scattering and you have a voltage applied the big spread out wavefunction occasionally (actually all the time) should collapse and localize at an atomic site. If it scatters without collapse it would be just entangled with the state of the atom and that process is hard to get (see quantum computing) so we can pretty safely say in the transistors you are studying that it is very particle-y at each scattering and wave-y between them. 

This is all very hand-wavy but they are reasonably useful toy models I think.

Some real models of transport in metals:

* http://en.wikipedia.org/wiki/Drude_model
* http://en.wikipedia.org/wiki/Free_electron_model
* http://en.wikipedia.org/wiki/Fermi_liquid_theory
* http://en.wikipedia.org/wiki/BCS_theory

Overall transport is often modeled with the Boltzmann equation:

* http://en.wikipedia.org/wiki/Boltzmann_equation

Most of this stuff is graduate level solid-state physics. ",null,1,cdht6ni,1qx8zd,askscience,new,4
majornoon,"You can't really think of it like a chain reaction, when you have a bunch of atoms next to each other in a solid they give rise to energy channels that the electrons can move in.  It's a joint thing, atoms further away have less impact, but the energies the electron can travel at are determined by the structure as a whole.  A cool fact is that if there weren't imperfections (i.e. missing atoms, etc) in the solid, there would be no resistance!  Drude model treated electrons moving as if they were colliding with atoms, but this fails for many phenomena.",null,0,cdhlq8j,1qx8zd,askscience,new,2
apocryphite,"Like /u/akanthos said, electrons are delocalized. At least the conduction ones electrons - most electrons in a metal are still bound to the atoms, but one or two electrons (typically, depends on element) are donated to the ""electron sea"". Keep in mind the particle/wave duality. Conduction electrons are more like smeared out waves.

A current through a wire is a very slow drift of all the conduction electrons (the [drift velocity](http://en.wikipedia.org/wiki/Drift_velocity) is something like centimeters per hour!).

About applying a voltage: The voltage relates to the potential energy of the electrons. Imagine I roll a ball up a hill. I'm working against gravity, so when on the top of the hill the ball has some potential energy. The ball can roll down the hill, you can even make it roll through stuff and do work.

Now, I take an electron and I push it towards a negative charge. I'm fighting the Coulomb force, so pushing the charges closer is a bit analogous to rolling the ball higher up the hill.

Say I take two boxes and put electrons in them. In one box, they are tightly squeezed together. The other box is less crowded. Now I connect a tube between the two boxes - electrons will drift from the box with higher potential energy towards the one with lower energy. This is like putting a battery in an electrical circuit, ie. applying a voltage :)

If someone asks really nicely I can talk about scattering, resistance and ballistic conduction, but it will have to be later.",null,0,cdhossz,1qx8zd,askscience,new,2
CoolStoryJohn,"A less detailed/nuanced answer for those curious about the basics of these questions:

P1: Current is merely the aggregate motion of electrons.  So, if one were to ""watch"" current, one would be viewing the displacement of a large batch of electrons from point to point in an electrical circuit.  In a standard metal, remember that electrons can be viewed as ""pooled"" together (almost like a special type of bonding mechanism).  That pool would be the group of electrons that compose the current.  Naturally, though, a circular or rectangular bar (i.e. a complete circuit) doesn't just inherently have a current.  A voltage needs to be applied to the circuit in order to ""excite"" (if you will) the group of electrons into motion...thus producing the current.  

P2: Voltage is really just the electric analog to gravity's potential energy.  You have an object raised some distance ""h"" off of the ground (which we'll define as our reference point--h = 0m) and that object has a potential energy of mgh ((mass)(gravitational acceleration)(h)).  There is no physical indication of the object having that potential energy, but it merely contains it as a result of being displaced from the reference point (the ground in this case) while in the presence of a gravitational field.  Voltage can be viewed almost exactly the same, except the ""object"" is a point in an active electrical circuit and the physical ground is the electrical ground (defined as 0 Volts).  ",null,1,cdif07s,1qx8zd,askscience,new,3
imsowitty,"Of course there are many ways to think of this sort of thing, but one that may be useful is simple electrostatic attraction/repulsion.  If you can convince yourself that like charges repel, then flowing current ends up as positive (or negative) charges trying to get away from each other.  An electric potential is set up by pumping extra charge into one section on a conductor, and letting electrostatics push the remaining charges away.  

If you're getting into transistors, you may be dealing with depletion regions and work functions, but the same idea applies.  A lower work function material simply has less affinity for its electrons, so it's more likely to give them up to another material with more affinity for electrons (call it a lower energy state), until enough charges build up on the high work function material that the built up charge prevents further current flow.  ",null,0,cdhoh83,1qx8zd,askscience,new,1
venikk,"A voltage is defined to be the potential of a coulumb of charge in an electric field. This is synonymous to gravitational potential of an object. More mass means more gravitational potential, and more charge means more voltage. Likewise the closer two charges are the higher the voltage, and the closer two massive objects are the more they attract. They also are both attracted by the inverse square law relationship. The attraction increases exponentially as they approach eachother.

Voltage is a man-made abstract quantity, that is anywhere and everywhere there is a difference in charge. 

An Ampere is defined as one coulumb of charge passing through one Ohm of resistance per second. So current is literally electrons passing through a conductor. It should be noted that these electrons behave like wind or gases. Not all electrons are going in the direction of the current, but on a per charge basis most are. It should also be noted that the electrons aren't necessarily traveling at the speed of light or at the speed of the current. In a circuit electrons are pushing on eachother, and it is the speed of which the first and last electrons push on eachother that gives us the lightning fast speed of electricity. The physical drift velocity of a electron is generally less than a cm per second.",null,0,cdhpelk,1qx8zd,askscience,new,1
MaterialsScientist,"Great question. It's difficult to imagine the true multi-particle wavefunction for a gazillion-particle crystal, so we have to imagine pictures that we know are wrong but are still useful. Band theory is one of these pictures.

According to band theory, an electron is simultaneously present in every unit cell of the crystal, like a wave. As the wave moves, it's like the electron moving.

(In reality though, the electron is a point particle.)",null,0,cdhpvqm,1qx8zd,askscience,new,1
Kegnaught,"Lactase persistence is the ""official"" title given to our ability to continually produce lactase (the enzyme responsible for the breakdown of lactose), and this persistence appears to be unique to humans. Right off the bat, this suggests that our long ago ancestors did not have this trait, as no other mammalian species share it. While mammals are capable of producing lactase while they are young, this ability does not last past infancy.

It's believed that it arose within the last 10,000 years due to some aspect of animal husbandry, and our ingestion of dairy products, though the exact mechanisms for its selection are unknown. Furthermore, lactose intolerance is actually correlated with race, and we know that lactose intolerance is actually more common than lactase persistence among the global population, so it appears to be more common in certain ethnic groups than others, providing further evidence that it arose independently among certain ethnic groups at some point in our (evolutionarily) recent past.",null,1,cdhjcyx,1qx8oq,askscience,new,22
DNAthrowaway1234,"Genetic mutations occur at a relatively predictable rate. By comparing the individual DNA sequences of lactose tolerant and lactose intolerant individuals, the minimum number of mutations required to go from one to the other can be estimated, and converted into a measure of time. 

Sources:
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182075/

http://en.wikipedia.org/wiki/Coalescent_theory",null,2,cdhoxuu,1qx8oq,askscience,new,11
beetlesaurus,"See (Bersaglieri et al. Am J Hum Genet 74:1111-1120, 2004) for one of the first papers to come up with the 5k-10k ya figure.

It's just math. Population-level genetic events like this are analyzed with statistics that detect how ""coalesced"" (or similar) a population is, how frequent the allele is across a few generation, how well it's associated with other alleles that mark selection, and use that information to estimate the time it took for the genetic pattern to get that way. 

I think a statistician should take it from here and explain the coefficient of selection, etc., because they'll do a better job than me... ",null,0,cdhr04k,1qx8oq,askscience,new,4
Clack082,"Hey I'm just a lowly microbio student but I might be able to help out some until someone more educated comes along. Just don't take anything I say as verified truth.

So a big part of figuring out when specific traits came into existence is looking at the genetic code of different populations. We can look at different populations and see ok this trait only developed after this group became separate from this other group. The ability to digest lactase actually had multiple origins we have determined because there are multiple genes which allow this which occur in different places in the genome. 

So for example, and this is just me talking, say we found the gene allowing for lactase digestion on chromosome four in  northern Europeans but a different gene allowing for it on chromosome ten in Mongolian people. And say Southern Europeans didn't have either gene. This tells us the trait developed in Northern Europeans after they split off from the Southern Europeans. And that Mongolians probably evolved their gene separately and we could look at related populations to determine when that came about. We can tell when these splits occurred by looking at other genes and by comparing what we see genetically with historical and archeological data.

I recommend Dawkin's book The Selfish Gene, where he does a much better job of explaining how gene's are actually the driving force behind evolution and how events like this happen.


Wikipedia has a decent article to serve as a starting point, if you're interested in learning more about the spread of lactase tolerance.",null,2,cdhg2tv,1qx8oq,askscience,new,7
vegetablehater,"As evolution proceeds certain mutations are selected for or against, depending if they are helpful or hurtful to the survival of an organism in the current environment.  The individuals who have the ""good"" mutations will do a better job of accumulating resources throughout their lifetime, or simply not die, thus have more offspring than the individuals without the beneficial mutation, and thus be a larger percentage of the next generation.  

The thing is that, evolution doesn't just pick out a winner and make sure everyone in a population has that adaptation.  Evolution doesn't engineer a solution, it has to work with the variation that already exists in nature and optimize it.  That means that not everyone has exactly the same DNA sequence and you can make comparisons between individuals to see what is different.  

If you look for the specific mutation that expands the expression of lactase to the entire life vs. just as a baby.  You can see that it is centered in Europe, and populations outside Europe have a much smaller percentage of the population with that specific mutation that allows them to do that.  

There are some complicated statistical tests that you can do based on how much of an advantage it provides to be able to drink milk, and the generation time of humans to try and get an idea about when a mutation like that might have arisen, given how prevalent in a population it is now.  In this case they also combine it with some archeological data about how long ago they know humans invented dairy production and kind of put the two together to get that 10,000 year old estimate.  

There are also other ways to calculate how old a mutation is based on specific signatures that evolution leaves in DNA sequence as time passes.  Not every mutation is an important one.  Unimportant changes often get accumulated at a standard rate and you can kind of count up those around a change you care about up to get an idea about how long selection has been acting on a specific gene or region of the chromosome.  This can also tell you if a mutation has been selected for (positive selection), against (negative selection), removed from a population (purifying selection) or a bunch of other stuff depending on how crazy you want to get with your modeling and statistics.  

It is kinda cool that DNA is both the blueprint for building every thing that is alive on earth, as well as a history of what has happened to arrive at that specific version of an organism all at the same time.  

The primary article about the milk drinking in Europe is here and open access (free to read) in case you want to read it for yourself:

http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1000491


TLDR:  They combined archeological data, population genetic data and some statistical models to get the 7,500 to 10,000 year estimate.  ",null,1,cdhodwl,1qx8oq,askscience,new,3
rupert1920,"You should note that receptor-ligand interactions are never limited to _one_ ligand per receptor, so it's not like there are only 5 molecules taste receptors respond to. Likewise, it's not like there are several thousand different receptors, each for the unique chemical your nose is sensitive to.

What part of the post you linked to doesn't answer your question? [This comment](http://www.reddit.com/r/askscience/comments/11385w/what_is_the_mechanism_by_which_an_atom_or/c6iy6yt) and the resultant discussion gives a good overview of the study of olfaction - not to mention that the [Wikipedia article](http://en.wikipedia.org/wiki/Olfaction#Study_of_olfaction) lists the competing theories on how the receptors interact with the ligand.

So knowing all this, your question:

&gt; ... is there a more comprehensive source of images of all such chemicals humans can sense?

is akin to asking if there is a comprehensive list of images of all chemicals humans can see, or touch. If such a list exists, it'll be a _very big_, if not limitless, list, and there isn't much purpose in making such a list. The closest you can get is any [MSDS](http://en.wikipedia.org/wiki/Msds) database - just look up any chemical and you'll find, under physical characteristics, whether it has an odor.",null,1,cdhgpwh,1qx7t3,askscience,new,5
lilitryan,"The olfactory system is very complicated actually. You don't smell different organic molecules because each organic molecule can have more than one functional group, and olfactory receptors are based on functional groups of the of the odorant. When you smell something and the molecules get in your nasal cavity, the receptors on the olfactory cells will detect all the different functional groups on those molecules and each type of receptor will send the information to the corresponding glomeruli, and glomeruli in turn will send the information they receive to the thalamus where all the different functional group signals will be put together to create a distinct smell perception. So each olfactory receptor recognizes different functional groups on an odorant molecule. The main point is different odorant molecules will trigger different combinations of responses from the olfactory cells. That is why we can smell so many different variations of odors.",null,1,cdj5g42,1qx7t3,askscience,new,2
do_od,"Spacecrafts like this usually have instruments and components that are sensitive to contamination, and really expensive too. The overall scale of investment make it logical to do everything practically possible to limit the risk of failure in every stage of spacecraft manufacture. The clothes help limit airborne particles (dandruff, vapour from respiration, coughs and so on) and are only part of a grand scheme of keeping spacecraft assembly as clean as possible. See [clean room](http://en.wikipedia.org/wiki/Cleanroom). Also it looks awesome. ",null,2,cdhgw3p,1qx6ad,askscience,new,9
Schmetlappio,"SWCNTs of appropriate diameter and folding angle have been demonstrated to have such a high potential for hydrogen storage that there are groups researching their application in fuel cells for hydrogen powered vehicles. We haven't been able to match this storage experimentally, but it should eventually work. As for ""shooting protons down them"" I'm not sure what you mean. ",null,1,cdhtusa,1qx46i,askscience,new,2
__Pers,"It is possible to do so. [Here](http://prst-ab.aps.org/abstract/PRSTAB/v6/i3/e033502) is one recent article. 

Edit: Here's a [press release](http://www.osaka-u.ac.jp/en/news/ResearchRelease/2013/04/20130409_2) for another, recent article that includes a graphic of how protons are accelerated. ",null,0,cdhwopm,1qx46i,askscience,new,1
Doener_wa,"Carbon nanotubes (CNT) may have different diameters. According to [this paper](http://cmliris.harvard.edu/assets/JPCB_106_2429.pdf) the diameter depends on your reactants. It may be quiet large (up to 12 nm), so yeah I think you could to this since protons are realy small, way below 1 A (10^-10 m). ",null,5,cdhgu1m,1qx46i,askscience,new,2
snusmumrikan,"Herpes viruses have a latent phase and an active phase. As double stranded DNA viruses they incubate (can't think of a better word) in the host cell nucleus, particularly the nerve cells. As these cells stay alive for your entire life, once you get the virus in them it's very hard to get rid of and is out of view of the immune system

When the virus enters its active stage the dsDNA is transcribed to mRNA by the host cell machinery and the viral proteins produced and more virus particles are assembled in the cell, resulting in the blisters and sores associated with many types of herpes. 

We have no treatment for viral infections, we merely manage symptoms and try to aid the immune system in tracking down and killing the virus-infected cells, which is difficult with herpes viruses as they go through extended periods of latency in the host. 

There are so many forms because herpes viruses is a term which encompases all members of the herpesvirales order. There are lots of viruses in this order: '3 families, 3 subfamilies plus 1 unassigned, 17 genera, 90 species and plus 48 as yet unassigned viruses'",null,2,cdhh8s1,1qx2vq,askscience,new,24
atomfullerene,"I can't think of a specific example, but it's pretty likely, for two reasons.  First, experimental design can be flawed, leading to people being really certain of wrong things happening.  Second, there are a _lot_ of papers out there.  Enough that even 10^-4 is likely to pop up once in a while",null,1,cdhj68q,1qx255,askscience,new,4
iorgfeflkd,"It was actually pretty low tech, they just used a really long rope with a weight at the bottom and saw how much rope was pulled before it stopped.",null,1,cdhiolz,1qx1qy,askscience,new,6
stuthulhu,"You are correct. Your shove would essentially propagate through the material at the speed of sound (in that material).

This question often comes up in terms of a ""perfectly rigid"" object being suggested as a means of communication faster than light, the problem being nothing is perfectly rigid.

Here are some related posts that may provide you more information:
http://www.reddit.com/r/sciencefaqs/comments/fj1qd/if_i_had_an_infinitely_stiff_rod_could_i_push_and/",null,20,cdhg3rz,1qx01x,askscience,new,172
opsomath,"Well, your 2x4 would weigh about 65,000 pounds (30,000 kg)So, your shove would do very little indeed.

The vibrations from your shove would travel at the speed of sound. Assuming you pushed with enough force to move it 1 meter in a short time (say a second) that's an acceleration of around 30,000 kg*m*s^(-2), 30,000N. The buckling strength of a 2x4 is about 2400N. So, the 2x4 would be shredded by the force involved.

tl;dr You'd break your board.",null,1,cdhyhfe,1qx01x,askscience,new,9
null,null,null,61,cdhnqcu,1qx01x,askscience,new,12
GeoGeoGeoGeo,"It depends entirely on the type of glacial ice that covers the region as to how much the landscape is altered. Temperate or warm (wet) based glaciers are very erosive though the debate between what contributes more to topography (the melt water or the ice itself) is still debated. As the degree of sliding is largest within these types of bodies of ice, they tend to alter the landscape quite dramatically, forming glacial valleys (the classic U shape depicted in text books). They also form horns, arretes, cirques closer too their formation point within the accumulation zone. Large bodies of water can form in the glacier (englacial), below (subglacial) on top (supraglacial) in the front (proglacial) or on the sides (lateral), such as lakes, braided river systems and during deposition can create massive deltas, kames, morraines, eskers, drumlins, crag-and-tails, etc. 

Summary of temperate (warm or wet) based:

- ice is at the pressure melting point throughout the glacier
- thus the mean annual temperature of the ice is about 0o
- geothermal heat and melt are concentrated at the glacier bed
- abundant meltwater and high glacier velocities from sliding 


With cold based glaciers, also known as polar, the body of ice is below the pressure melting temperature from surface to base and so there is little movement (only internal creep or deformation of the ice - no sliding on along the base). These types in fact *[preserve](http://www.livescience.com/38696-greenland-ghost-glaciers-prevent-erosion.html)* the landscape beneath them! Revealing ancient forests and [hardy plant life](http://www.nature.com/news/wild-flower-blooms-again-after-30-000-years-on-ice-1.10069) 10's of thousands of years old.

Summary of cold-based (polar) glaciers:

- the ice is below the pressure melting point throughout the glacier
- that is the glacier is frozen to its bed (permafrost) and thus can move only by internal creep ",null,0,cdio9zr,1qwzqt,askscience,new,1
JDL523,"The landscape changes dramatically through glaciation. As glaciers move over the landscape, they gouge and scrape away large amounts of materials. As the glaciers retreat, all the material they picked up gets left behind, and forms a large variety of landforms (hummocky terrain, undulating terrain, drumlins, and eskers to name a few). Further, you'll get glacial lakes forming, like Lake Agassiz, which, when they let go, can carve out large valleys, as millons of cubic meters of water are released from them. ",null,1,cdi8sg2,1qwzqt,askscience,new,1
fishify,"While there is no verified unification of those three forces, here is the way it works mathematically (depending on your background, this may or may not be sufficiently non-technical).  The electromagnetic, weak, and strong forces are associated with a number, a 2x2 matrix, and a 3x3 matrix, respectively.  Grand unified theories that unify these three forces embed these matrices in a single larger matrix.",null,1,cdhm357,1qwz5t,askscience,new,7
MCMXCII,"The idea of unifying the strong, weak, and EM forces is known as a Grand Unified Theory (GUT). So far, there isn't one. It's still a work in progress. The weak and EM forces have been unified, in that they behave the same way at high energies. But so far no one has been able to add the strong force into the mix. And gravity is a whole other beast in and of itself.",null,4,cdhig1s,1qwz5t,askscience,new,5
baloo_the_bear,"Both have their advantages. Laying eggs saves the mother from needing to carry the fetuses for an extended period of time during gestation, and is 'cheaper' in a metabolic sense. Giving birth to live young is more expensive metabolically (meaning the mother will need more food) but the offspring are less vulnerable (and more mobile) than their shelled counterparts. 

One of the major things that has affected the evolution of live birth is head size. One of the reasons human babies are so helpless when born while a deer can plop out and start walking around immediately is that the head size required to fit a human brain is way too big for a human female pelvis to birth. In contrast, however, a deer does not require such a complex brain and therefore it can develop to a higher degree *in utero*. This is also why babies' skulls are not completely developed at birth, because the skull literally needs to be able to squeeze through the birth canal.",null,23,cdhc8q4,1qww57,askscience,new,168
r_n_b,"All those referring to head size and bipedal movement as reasons for early human births are mentioning an outdated theory.

In reality, the fetus is born when the maximum level of metabolism is reached in the mother i.e. the point at which the mother is unable to produce any more energy or metabolize fuel, regardless of the number of calories consumed.

This value is so accurate in predicting births that you can track the metabolism of a mother via her breathing or urine (can't remember) and estimate how long she has been pregnant. 

Found it:
""My metabolic rate was about double what you'd expect – for a non-pregnant woman. But I was five months pregnant: I was providing energy for the developing foetus too. When my metabolic rate rose to 2.1 times the normal rate, I would go into labour – my body would no longer be able to provide the energy being demanded by the foetus.""

[Source](http://www.theguardian.com/science/2013/jun/30/childbirth-metabolic-rate-obstetric-dilemma)

",null,6,cdhjmbb,1qww57,askscience,new,44
floppylobster,"Live births are quick events and can be done on the move. Eggs are susceptible to attacks from small mammals. And once you have a lot of small mammals eating eggs, while live-birthing their own offspring, the balance soon shifts to live births being more common.

Also, 'from an evolutionary stand point', what works, works. What is, is. Just because something is better does not mean evolution selects it. Evolution is survival of the fittest. As in ""survival of what fits"", not what is strongest or best. Just what works best in the current environment.",null,5,cdhiotd,1qww57,askscience,new,23
Fix_Lag,"Eggs can only contain so much nutrition which the fetus can use to develop before an organism has to hatch.  There is a limit to egg gestation periods based on shell strength because the more nutrition material you try to cram into the egg, the stronger the shell has to be to support the weight.

Live births, obviously, do not have this problem, and as such most organisms with complex brains have live births.  Live birthing does, however, have the problem of size--an organism that grows too large while gestating will die during birth (possibly killing the mother) and if it is too small it will be underdeveloped and not survive.  In humans the size issue is often that a baby's head is too large to fit through the hole in the pelvis.",null,0,cdhk0vi,1qww57,askscience,new,3
ragingclit,"Squamate reptiles (i.e., lizards and snakes) have evolved viviparity more times than any other other vertebrate radiation. The predominant hypothesis for the evolution of viviparity in squamates is that viviparity evolves in repsonse to colder temperatures. This is supported by a number of studies, including [this recent large-scale analysis across Squamata](http://onlinelibrary.wiley.com/doi/10.1111/ele.12168/abstract?deniedAccessCustomisedMessage=&amp;userIsAuthenticated=false). I'm wary of some of the estimated speciation and extinction rates and ancestral state reconstructions, but the correlation between temperature and parity mode is solid.",null,1,cdho3jw,1qww57,askscience,new,4
joe12321,"To expand and stress a little bit of what floppylobster said...

From an evolutionary standpoint, If xxx-trait is better, why does this species do/have yyy-trait? is not a great question.  

(To play a little fast and loose with anthropomorphizing the concepts...) At no point does evolution get to look through all the possible solutions to a problem and choose the best.  Rather, when a change in a species occurs, if it enhances reproduction, it may stick. 

So let's say (oversimplifyingly) a species starts giving live birth, and it works well.  It's gonna go on and keep working!  And all further evolution will be around that behavior.  Even if laying eggs works 20% better, there's no intelligent mechanism that will wait around for or design that behavior instead.",null,1,cdhs3yd,1qww57,askscience,new,3
dawgfan64,"In a sort of related question; why did either laying eggs or live birth arise when reproducing asexually was much more efficient? To put it another way, how/when did organisms make the jump from reproducing asexually to reproducing with sexual organs?",null,0,cdhpp2f,1qww57,askscience,new,1
M4rkusD,"It's got something to do with r-K selection. r-selection happens when you're a species that gives birth to a large number of offspring (sometimes in the millions) but don't invest a lot of energy in parental care. K-selection happens when you only get a limited number of offspring but invest a lot of energy in parental care. Viviparous animals (like most mammals) are generally K-selectors. So with only a limited amount of offspring they can carry their young around inside of them for an extended period. Egg-laying animals (Reptiles, Amphibians &amp; lower) are generally r-selectors. Fish can lay millions of eggs (so they don't have the room to gestate them inside of their body, purely due to the numbers) and don't bother with parental care.",null,0,cdhvmdw,1qww57,askscience,new,1
Diamond_Jared,"Quality and quantity. If you're going to have a lot of kids, eggs are probably a good idea since you don't waste as much time and energy. If you're just going to have one kid, you need it to be ready, to be crass, right when it gets out, and a lot of live birth animals are more or less.",null,1,cdi1dig,1qww57,askscience,new,1
patchgrabber,"There are a variety of reasons, but they all revolve around an increase in the number of nutrients needed for algal growth. All lakes sit somewhere on the [trophic state index](http://en.wikipedia.org/wiki/Trophic_state_index), which is a measure of the productivity of a body of water. Virtually all lakes (left to their own devices) become more eutrophic over time, even though many may start as oligotrophic. This is a natural process of life; as organisms die their biomass sinks to the bottom becoming sediment and the nutrients from their dead bodies/cells/etc. are recycled back into the water for other organisms to use. 

Over enough time (think centuries), the sediment builds up and begins to fill a lake from the ground up, but more nutrients keep getting added, so there is more available for organisms like algae to use. The most important of these nutrients are nitrogen and phosphorous. Now people, by agricultural runoff or dumping of industrial waste or other processes add even more nitrogen and phosphorous to the water, speeding up eutrophication of that body of water. Lake Eerie had a big problem with this a while back, and Eerie is further along on the trophic index than the other Great Lakes. So as time goes on, unless we actively counter the eutrophication of these lakes, they will eventually all become like Eerie, and this will happen sooner because we are actively adding to it.",null,0,cdhbukh,1qwv8i,askscience,new,30
shiningPate,"One of the reasons algal blooms are increasing in recent years is the increased application of animal waste directly onto farm fields, and the phosphorous that subsequently runs off from them. Phosphorus was at one time widely used in household detergents and is included in farm fertilizer. During those times, there was a great deal of phosphorus flowing into the streams and lakes. With the creation of the EPA and passage of the clean water act, much of that phosphorus was eliminated from urban effluent and farm runoff but... it could be regulated because industrially produced fertilizer has a defined content. Its application could be regulated. The increasing practice of using animal waste direct on the fields is in effect an end run around environment regulations on chemical fertilizer. Animal waste ""is just shit"". There isn't a factory producing it that certifies it contains X pounds (or Kg) of phosphorus per ton of fertilizer applied. There have been attempts to begin regulating the use of animal waste as fertilizer, but the farm lobbying groups having previously had their ability to put unlimited phosphorus on their fields taken away have stymied the EPA's attempts to regulate it. It is currently a bone of contention between the states whose runoff enters the Chesapeak Bay. There are lots of dairy farms in NY and PA whose cow pastures drain into the bay. Similarly in Maryland and Virginia, farmers routinely apply chicken manure on their fields. Even though the bay is choking under the nutrient loads in the run off water, the farm groups have so far managed to fight off most regulation of the  application of manure and farm field run off into the bay. ",null,0,cdhdsnv,1qwv8i,askscience,new,8
un-scared,"One of the grad students a couple offices down from mine just gave a talk on the lake Eerie algae blooms recently. From what I remember it's simply a case of nutrient inputs (particularly phosphorus) increasing again. I say again because his isn't the first time algae blooms have been a huge problem for lake Eerie, what makes these notable is that phosphorus levels were reduced in the 2000s and the lake cleared up significantly. For some reason (I can't actually remember why, seem to recall most of the blame was on agricultural runoff) the phosphorus inputs have increased again in the last couple years causing these large blooms.

If the nutrient inputs are decreased again the lake should clear up significantly but that's much easier said than done.

",null,0,cdhpghl,1qwv8i,askscience,new,1
null,null,null,1,cdhemhf,1qwv8i,askscience,new,1
Mackerie,"Memory research is such a hot topic in Psychology/Neuroscience right now. There's so much research out there because it's so easily testable.  

There's 3 stages of memory, encoding, storage and retrieval and changes in any of these stages will affect how a memory is stored and subsequently how it is recalled. For normal people (ie: people without lesions in their brains), the encoding phase is, I think, usually the most important in memory consolidation. 

There is a theory that elaborative rehearsal will aid memory. For example, when you see a stimulus, you can shallowly process it (ie: just look at it) or you can make more connections with other systems in your brain (ie: I saw this yesterday too or this other time, it was blue etc...). Elaborative rehearsal will always help encode memories better. This is why teachers and profs suggest finding a way to personalize the information you study or new ways to synthesize information. 

Another factor is what type of memory you're trying to recall. Episodic memories (memory of personal events - what we usually consider 'memory') will always be easier to recall than semantic (facts) and 'feel' more poignant because there's a lot of details associated. One theory that might explain why is that semantic memories are more generalized and abstract version of episodic memories to allow for application across different contexts and thus less details.  

Emotional impact is also a large factor as well. Events that are more emotionally salient will be remembered better than non-emotional events. There are various studies of this and if you want, I can suggest a few.  

Also you can't forget about learned associations (this is more into the realm of implicit memory (ie: skill memory and priming etc...). For example, if I always associate a certain classroom with a time when I failed a test, then I will always activate certain negative stereotypes when entering the room (a context) and end up activating other associated memories and thought about failing tests or times when I've failed.  If you have strong associations between a context and a certain stimulus, you will have stronger activation scripts for activation of memories of that stimulus.  

Hopefully this will satiate your interest in memory research slightly until someone else can come along with a more detailed answer! c:

TLDR: the brain is mf'ing confusing. ",null,0,cdhe2fe,1qwuy1,askscience,new,3
tishtok,"In addition to /u/Mackerie's good summary, when you're remembering a memory you've remembered in the past, you're often recalling a memory of your memory, not the original memory. So if you've recalled something recently, even if the original episode happened a longer time ago than a more recent episode, it may be easier to recall. ",null,0,cdhf3eo,1qwuy1,askscience,new,3
Hekatoncheir,"There are some studies regarding the relationship between lower metabolic rates and longevity, like this one http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881137/

Basically, the idea is that the slower the rate of everything happening in an organism, the longer it can live for relative to the same organism in a normal condition.",null,6,cdhdjye,1qwu5y,askscience,new,48
iorgfeflkd,"It's better to think of momentum as something that moving objects have, rather than an explicit product of mass and velocity. p=mv just applies to slow moving objects, as an approximation.

The full relationship between energy, mass, and momentum is E^(2)=(pc)^(2) + (mc^(2))^2",null,42,cdhaiv4,1qwtsc,askscience,new,314
floydos,"E = mc^2 is an approximation of:

**E^2 = m^2 c^4 + p^2 c^2** 

Since photons have no mass (m = 0), the equation becomes:

E^2 = P^2 c^2

*This is quite easily rearranged to* 

p = E / c

*and since E = hf*

**p = hf / c**

*and since Wavelength = c / f*

p = h / Wavelength
",null,10,cdhhrvp,1qwtsc,askscience,new,60
rupert1920,"You can also check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/g3qlc/is_light_massless_why_is_it_affected_by_gravity/) in /r/sciencefaqs, for common questions.",null,3,cdhd8ph,1qwtsc,askscience,new,13
cdstephens,"p = mv is not true in a modern context. The momentum of light is written as p = hf/c. p = mv is just an approximation. 

Depending on who you ask, you can also say photons have a relativistic mass, where the relativistic mass is defined as E/c^2, and for an object with a nonzero rest mass, m0 * gamma (the same gamma used in special relativity). This in part gives rise to the suggestion that energy and mass are directly connected; that an increase in energy in something should increase its apparent mass. I think there's some minor controversy though about whether it should be taught.",null,2,cdhd8ii,1qwtsc,askscience,new,9
brwbck,"I figure I'll add to this by explaining why light can impart momentum even under the classical EM wave theory.

It has to do with the fact that light has both electrical and magnetic components. When a light wave interacts with a charged particle, the electric field exerts a force on the particle, and the particle moves. Now that the particle is moving, the magnetic component of the wave will affect it. The force of a magnetic field on a moving particle is perpendicular to both the motion of the particle and the direction of the magnetic field. This produces a force in the same direction as the wave is traveling. Thus the light pushes on the object.

The question of where the momentum is ""stored"" before the wave interacts with something is hard to explain, and is one of the weaknesses of the pure wave theory.",null,3,cdhlqfj,1qwtsc,askscience,new,7
Gimlis_Axes,"Photons have no mass? I thought that a black hole is 'black' because it's gravitational attraction is stronger than the speed of light and does not allow any light out. So I figured light must be subject to gravity and therefore have mass. Or am I, as usual, making an ass of myself. ",null,3,cdhktqb,1qwtsc,askscience,new,6
mc2222,"A good rule of thumb is that *light travels as a wave but interacts with matter as a particle*. This means that any interaction with matter - absorption and emission - must occur in discrete chunks of energy  which we call photons.  For all other cases, it's often simpler to think of light in terms of EM waves.

Light is an electromagnetic wave - it is a changing electric and magnetic field that move through space together.  Waves are fundamentally a way that *energy* moves through the universe.  Waves are often (always?) associated with some type of field (like the electric field).  A wave has the ability to move objects that come in its path, for example, a water wave will move a boat up and down, or a wave in the electric field will cause an electron to move.  Since waves can exert a force on an object and move that object, this means they must also have momentum.",null,3,cdhdtow,1qwtsc,askscience,new,5
circleget,"you are correct in both assumptions of energy and momentum in relation to its mass. However, through experiments regarding the photoelectric effect one has to consider the particle property of light whose energy is no longer associated with the intensity of the light source, and instead is proportional to it's frequency (E=hf). when you substitute this into E=pc you find that the momentum of light is now P= (hf)/c, no longer requiring mass to define momentum.",null,0,cdhhued,1qwtsc,askscience,new,3
carlinco,"Another way to look at it is through pure wave mechanics. When something creates light, it basically stirs up vibrations, not unlike a stone thrown into a pond. Those vibrations spread out, not through momentum, but through exciting whatever is around them. The next excited place around will again excite everything around it, in any direction - so no real momentum, only an apparent momentum at the circular front of the ""excited"" area. And when those waves hit something, that thing might get moved around - like a stone lying in the water. Which is pretty close to what happens when light is absorbed for instance by a solar cell, and moves electrons.

This is also why laser rays gets weaker the farther they go - more and more light goes off in different directions, due to lack of momentum.

However, the word momentum can still be used in a meaningful way with waves - to describe the intensity of the ""wave making"" in certain contexts, or the amount of momentum which they can transfer to matter.

The water analogy does not go too far, however, as there is no real equivalent in space, like an ether.",null,1,cdhka14,1qwtsc,askscience,new,3
mrbeanbag,"The bottom line is that if you hit something with a photon, it will move. We can either choose to say that photons have momentum, or we can throw conservation of momentum out (not such a great idea). So we say photons have momentum, and then derive equations that accurately describe their momentum.

While it may feel more intuitive, there is no good reason that massive objects should carry momentum either, except that we would like *something* to have momentum so that we can actually do physics.",null,2,cdhkdhk,1qwtsc,askscience,new,6
CBunneh,"Photons have a momentum given by De Broglie's equations which are derived from E = mc² ____ m=E/c² _________ As E=hf, therefore we can state that: m=hf/² ______ p=mv ______ p=(hf/c²).c ______ p=hf/c _______________ As c here is the velocity of light in a vacuum, then this is also v and as _______ v= fλ ________ Therefore: p=hf/fλ _____ and thus we have found that as the frequencies cancel out, we can say that: p=h/λ     From this derivation, we can say that we can that p can occur without a need for mass.",null,3,cdi9im8,1qwtsc,askscience,new,4
TheBB,"You can't define medians of infinite sets^(1). As you say, there's no reason to prefer zero to five or negative five.

^(1)Well, I guess it could be generalized to finite measure spaces.",null,4,cdhbdw5,1qwsmr,askscience,new,23
DarylHannahMontana,"Well, yes and no.

I mean, if you are just asking ""what is the median of the set of all integers"", then there isn't one, because, as you argue, there are the ""same amount"" of other numbers below and above any given number, and so there's no good reason to pick one number over another.

On the other hand, if your set is ""all real numbers between 0 and 4"" there are an infinite amount of numbers in that set, so given any number x between 0 and 4, there are an infinite amount of numbers below x and an infinite amount of numbers above x.

However, in this case, you can make a good argument for why 2 should be the median of this set, it's pretty intuitive that it should be, and this can be defined in a mathematically rigorous way also.

Likewise, with a more complicated probability distribution, say the weight of all people ever born, we have to consider every real number between 0 and, say, 1500 lbs as possible (since a person's weight changes over their life, let's consider their maximum weight). Thus we have an infinite amount of weights to consider (we're pretending we have a scale that's accurate to an infinite precision). Nonetheless, we can still talk about the median of this set; it's just the number such that exactly half the people to ever live weighed less than this, and the other half weighed more than this.",null,0,cdhfxf0,1qwsmr,askscience,new,8
rlee89,"&gt;Basically if there are an infinite amount of numbers, then isn't any number technically the median of that infinite set, since there are is an infinite amount of numbers above and below that number?

I believe that the issue with that lies more in that you are attempting to run statistics on an unbounded uniform probability set than an issue with the definition of median.  Such a set breaks *several* statistical measure, primarily as a consequence of the probability measure of any bounded subset of elements being [measure zero](http://en.wikipedia.org/wiki/Null_set).

If we wish to generalize the notion of a median to an infinite set, the most intuitive way to do so would be to call the median the point at which the integral of the probability density of the regions below and above that point are equal.  This works well for any set in which the elements can be assigned nonzero probabilities or probability densities.  There are some minor technical complications with integration in the case of discrete sets, because you end up integrating over Dirac deltas, but this issue is not that hard to handle.",null,0,cdhfpeb,1qwsmr,askscience,new,4
protocol_7,"The notion of a ""median"" is a property of [probability distributions](https://en.wikipedia.org/wiki/Probability_distribution), not of sets of real numbers. (This gets conflated with finite sets because there's a natural choice of probability distribution, namely the uniform distribution.) There's no uniform probability distribution on the set of integers, so the integers don't have a natural choice of probability distribution.

As a result, it doesn't even make sense to talk about ""the median of the set of integers"". To elaborate, what if you were to ask for ""the sum of a set of five integers""? Well, the question doesn't quite make sense, because some key information — which five integers are being summed — is missing. Likewise, given any probability measure on the real numbers, you can talk about the median, but it doesn't make sense to talk about the median if there's no probability measure.",null,0,cdhu9ik,1qwsmr,askscience,new,2
king_of_the_universe,"That's like saying ""If the universe is infinite, isn't every point the center?""

But that's not true. ""Infinite"" means that there is no end. Hence, in the universe case, the exact size of the stretch *is undefined*. You can't say ""? divided by 2 = undefined, hence I'll just say that *every* value is the correct result"".",null,0,cdhue65,1qwsmr,askscience,new,2
Spiralofourdiv,"I believe you are knocking on the door of [measure theory](http://en.wikipedia.org/wiki/Measure_(mathematics).

If you are defining the median to be the element on an ordered set such that there an an equal number of elements on each side of it, then no, a median cannot be defined on an infinite set.

However, that's not necessarily what ""median"" means in what we call a ""metric space"". All a metric space is is a space that has a ""notion of distance"". What that means can be ignored for the most part aside from exactly what you think it means: you can take two points in the space (elements in the set) and there exists the idea of a ""unit"" and that we can count how many units their are between two points. That is, a definition of distance exists, and thus we can use numbers as ""measurement"" of this distance.

The real line is a metric space. Take the set [0,2]. It's an infinite, ordered set, right? But we know that the median is 1. How can that be? The reason is because we've kinda redefined ""median"" within a metric space to mean something different: now a median is the value such that there is equal **distance** between it and the endpoints of the set. A median now describes equal distance and NOT equal number of terms. 0.5 isn't the median even though there are infinitely many elements between it and either of the endpoints of the (ordered) set, the metric is NOT the same, it's 0.5 on one side and 1.5 on the other. If the set isn't infinite, we care not whether it exists in a metric space, and as long as the set is ordered we can use the element definition of median.

Now consider (-inf, +inf). Well, that's a metric space too, but all points are equally distant from the endpoints, so no median! We thus have one more stipulation:

The metric space must be finite. That is, since we have a metric, the endpoints of the set must have a finite measure between them, if they don't, then the measure between a point and at least one endpoint will be infinite and thus meaningless.


So finally: Yes, you can have a median on an infinite set if it's a finite, ordered metric space, and perhaps other structures, but you are right that the notion of infinity does mess up defining a median for certain sets.",null,0,cdkxla1,1qwsmr,askscience,new,2
null,null,null,5,cdhgztd,1qwsmr,askscience,new,2
_NW_,"[Resonance](https://www.google.com/#q=resonance) is when you excite something at its natural frequency.  Think of a tuning fork, and then apply that idea to just about anything.  Pushing somebody on a swing, bouncing on a diving board, a flag waiving, a bell ringing, a guitar string.  Basically, anything that can sustain a periodic motion has a preferred or natural frequency.  Constructively encouraging that natureal frequency is resonance.",null,0,cdhcm4o,1qws23,askscience,new,1
quarked,"Resonance is a phenomenon that can occur for any system with a natural frequency. If you *drive* a system at the same frequency as its natural frequency, you will pump energy into the system.

Try taking some object attached to a string to make a simple pendulum. Pendulums have a natural frequency they will oscillate at if you let them swing (f~(g/l)^(1/2)). You can see the natural frequency for yourself by letting the pendulum swing freely. Now try *driving* the pendulum by holding the string and moving it back-and-forth. 

If you move the string very slowly, the pendulum won't swing (it will just move with the string). Similarly, if you move the string very quickly to and fro, the bob won't move (it doesn't have time to ""respond"" to the string's rapid movements). The pendulum doesn't ""see"" oscillations very far from its natural frequency. But, if you move the string back-and-forth around the pendulum's natural frequency, it will resonate and the pendulum will swing higher and higher (until the oscillations become too big and it no longer behaves regular). At *resonance*, you are driving energy into the system. You might say that at resonance, the system most easily ""accepts"" the energy driving it.

",null,0,cdhdh2l,1qws23,askscience,new,1
SwedishBoatlover,"Another way to look at resonance is that you are in resonance when you can keep the amplitude of an oscillation constant with the least possible effort.

Think of pushing somebody on a swing. You want to keep the amplitude (i.e. the swing goes to the same height every time). If you push right after the swing has reached it's peak, you don't have to push very hard to keep the swing going. You are in resonance with the swing. Now instead start pushing when the swing is at it lowest point. You will have to push really hard, first absorbing the energy currently in the swing, then you need to push it back up. You are out of resonance with the swing. ",null,0,cdhgjz3,1qws23,askscience,new,1
I_am_Bob,Most of the earths water probably came from comets and asteroids after the surface had cooled a bit from planetary creation. Oxygen is produced by stars during fusion. So when a star supernovas it ejects the oxygen as well as unused hydrogen. After they cool off a few million degrees they may end up colliding and bonding to form water ice crystals that eventually amass in to comets. ,null,0,cdhborc,1qwrrm,askscience,new,3
Osymandius,"Yes and no. 

Yes: Horizontal gene transfer has previously occurred between mitochondria and chloroplasts to the eukaryotic nucleus. This falls in line with their having been prokaryotes once upon a time. 

We see eukaryotic signalling stubs in bacteriophages (viruses). [Snazzy paper](http://www.pnas.org/content/early/2012/10/17/1216635109.abstract)

Aphids have ""managed"" to acquire their own biosynthesis pathways for carotenoids rather than from a dietary source. [Another snazzy paper](http://www.sciencemag.org/content/328/5978/624)

No (or at least no in the way I suspect you're asking): The problem with horizontal gene transfer is it's from one cell to one cell. This isn't a problem if your whole organism is one cell. But if you happen to have 5x10^10 or so cells, then the significance of lateral gene transfer in a single cell is somewhat abrogated. However, it does happen on an evolutionary time scale, but not in the way that bacteria have lateral gene transfer and immediately gain antibiotic resistance in 1 generation.

",null,1,cdhe0s6,1qwqr1,askscience,new,6
SqrrlBait,"There's a new paper out by Coelho on HGT in fungal species with data showing that Aspergillus has genes from Candida species.  So, yes, it occurs.

Coelho, M. A., Goncalves, C., Sampaio, J. P. and Goncalves, P. (2013) 'Extensive intra-kingdom horizontal gene transfer converging on a fungal fructose transporter gene', PLoS Genet, 9(6), e1003587.",null,0,cdhml32,1qwqr1,askscience,new,2
iorgfeflkd,"Consider 1/2+1/4+1/8+1/16... each term is smaller than the last and each additional term brings the series closer to 2. This is a convergent series, each additional term brings it closer to converging on a finite value.

Consider 1/2+1/3+1/4+1/5+1/6+1/7... this series does not converge. It reaches infinity with an infinite number of terms, or an arbitrarily large number with a very large number of terms.",null,5,cdh9rsp,1qwqfu,askscience,new,16
Weed_O_Whirler,"Your first impression may be that if you add up an infinite number of anything, that their sum must be infinite. However, this isn't necessarily the case. 

The most common example is the series 1/n^(2), where n is integers. Writing out the first couple of terms you get 1/1 + 1/2 + 1/4 + 1/16 +... You can try it, keep typing as many of those as you want into your calculator, and you'll see that while the sum is (of course) always increasing, no matter how many you punch in, it will never get larger than 1.7 (in fact, it goes to ""pi squared over six""). 

If this is still confusing, imagine an even simpler series: 0.9 + 0.09 + 0.009 + 0.0009... you can see that each term of this series will get you a little closer to 1, but you'll never be larger than 1. So from these examples you can start to see how series convergence works. You are adding up an infinite number of things, but if the series gets ""smaller"" quickly enough, the sum can be contained. ",null,7,cdhcdk7,1qwqfu,askscience,new,19
TheBB,"A series converges to a value *L* if you can get a finite part of the series to have a sum as close to *L* as you like by picking enough terms. (This is called a partial sum. By ""enough terms"" I mean that the partial sum should obey the closeness criterion for any number of terms larger than a lower bound.)

A series converges if a series converges to *L* for some *L*.

Maybe easier is to just think in terms of partial sums. A series converges if the sequence determined by its partial sums converges in the sense of sequences.",null,1,cdhak5m,1qwqfu,askscience,new,4
__Pers,"Convergence means that the sequence of partial sums of the series (the nth partial sum just means adding up the first n terms of the series) approaches a limiting value h. This means that for each positive ""error"" value that you might define, call it epsilon, you can always find some finite number of terms N such that when you sum at least N terms in the series, the result is within epsilon of the sum of the infinite series. 

Incidentally, there are special and useful kinds of divergent series called asymptotic series that don't converge in the sense above, but yet often yield a very good approximation to a function in the neighborhood of a reference point, provided you only take the first few terms. ",null,1,cdhanph,1qwqfu,askscience,new,4
rlee89,"It means that there exists some number, such that you can arbitrarily close to it, merely by adding enough terms in the sequence.  Put another way, no matter how close you want to get to the limit, there is some sufficiently large number of terms you can add to get that close.

One caveat to that is that the series needs to stay near that limit even if you add more terms.  In other words, after you add that sufficient number of terms, it won't get further away than that arbitrarily picked closeness (and as a caveat to that caveat, the needed number of terms so that it stays close could be more than the first time the series gets that close to the limit).

Contrasting with divergent series (which diverge to infinity in a monotonic fashion), a divergent series is one in which you can reach any arbitrarily large number by simply adding enough terms.  There can be no finite number as the limit, because it will exceed any arbitrary number by any arbitrary amount you care to pick.",null,0,cdhg0rt,1qwqfu,askscience,new,3
Ag3ntD,"If you are down with integrals (area under the curve) converging or diverging, then you will be fine with series!  As you said, if a definite integral (of a positive function) converges, then there is finite area under the graph of that function.  Likewise if a series converges then we mean that we can ascribe a finite value to that sum.",null,0,cdhs702,1qwqfu,askscience,new,1
Apollo_Felix,"Think of a switch where if the input is high (5V) then the two pins are short circuited. However if the input is low (0V), then the two pins are disconnected. The following [circuit](http://imgur.com/5OLiNdk) is then a NAND gate. If only one switch is low, the output voltage is always 5V (high). If both are low, the output is also high (5V). However if both are high, then the output is low.
The switches can be made using a FET transistor where the inputs are connected to the [gate](http://imgur.com/rpdF5R2).
An [XOR](https://en.wikipedia.org/wiki/NAND_logic), or some other gate, can be made using this same circuit. An XOR gate would look like [this](http://imgur.com/8uWDZ5O).",null,3,cdhdl4m,1qwqb7,askscience,new,34
afcagroo,"I think that most of the [Wikipedia articles](http://en.wikipedia.org/wiki/AND_gate#Implementations) are pretty clear.  Although the AND gate implementation only shows an NMOS circuit, not [CMOS](http://www.c-jump.com/CIS77/images/figure_3_7_AND_gate.png) such as is typically used. (CMOS circuits tend to be more power efficient, although they use more transistors than simple NMOS only circuits.)  
  
Keep in mind that an NMOS device turns on with the gate presented with a HIGH signal, and a PMOS turns on when the gate is presented with a LOW signal. ",null,8,cdhcjb8,1qwqb7,askscience,new,29
dirtpirate,"Others have given more detailed answers, I'll just pitch in with a flash game that challenges you to build circuits including logic gates and things like RAM by drawing n-type and p-type silicon as well as metal paths (simplified versions through). [Engineer of the people](http://www.zachtronics.com/play-kohctpyktop/). Quite fun, but it has a steep initial curve and gets incredibly complex towards the end.  ",null,3,cdhfr7o,1qwqb7,askscience,new,13
bisbyx,"**TRANSISTOR PART -- MAKING SWITCHES**

First, transistors. Imagine you have an upside down shaped T tube. In the vertical part, you have a magnet that blocks the horizontal part. Water will not flow from the left to the right. Now imagine if you put a magnet above the vertical part, the blocker will get magnetized up, and liquid can flow through.

This is a gross simplification, but this is basically how a MOS transistor works. You have a large positive pool of positively charged silicon. You have 2 ""gates"" of smaller negative charge silicon sitting in the positive charge. Electricity can't flow from gate to gate, because the positive charge is blocking the flow. So between the gates, we apply a capacitive oxide layer with metal on top of it. When we apply positive charge to the metal, like charges repel each other - the positive charge is repelled away from the surface of the large positive silicon pool. This then allows the charge to flow from the 2 gates. ""MOS"" means Metal, Oxide, Semiconductor. This is a great picture that explains it. http://en.wikipedia.org/wiki/File:Mosfet_saturation.svg (the diagonal line section is the ""neutral"" section that can contain current. once enough charge is appplied to the metal gate, the diagonal line will connect the 2 sides.

**GATES PART -- USING SWITCHES TO MAKE LOGIC**

Now the gates themselves are a bit trickier (lol, not really) and I dont have my ECE110 book with me. But basically, ANY logic can be built from a NAND gate. Based on DeMorgan's law (!A + !B = !(A*B)) and the fact that A NAND A = !A.

This http://en.wikipedia.org/wiki/File:NMOS_NAND.png is how you make a NAND gate. Remembering back to how transistors work, output F is connected to the high voltage V_cc (if there is no current, then the resistor has no voltage, so F = V_cc - see Ohms Law : V = iR). However, if A and B are BOTH positive voltage (1), then current can flow across them. If either one is not positive (0), then the flow is broken from V_cc to ground and the entire thing dies.  If current can flow, then we have a connection from V_cc to ground, which means current is flowing across the resistor, forming a voltage. Which means that F is no longer equal to V_cc. Depending on the voltage drop across the transistor, the voltage wont actually be 0V, but it will be a logical 0 (not enough to trigger another transistor).

Now we have a NAND. quite simple to make.

**BUILDING LOGIC -- THE REST OF THE GATES**

There are simpler ways of building the other gates, but lets just pretend we ONLY have NAND gates. We can still make all the other basic logic gates.

* NOT A = A NAND A   (this is drawn as !A)
* A AND B = !(A NAND B)   (we're using !, because we made it in the previous step)
* A NOR B = !(A OR B) = !A AND !B (demorgans law)
* A OR B = !!(A OR B) = !(!A AND !B) (run a nor through a not gate)
* A XOR B = (A AND !B) OR (B AND !A)

source: i have a degree in electrical engineering.",null,2,cdhr26o,1qwqb7,askscience,new,11
null,null,null,9,cdhik2v,1qwqb7,askscience,new,15
Kwibbian_Kel,"Not a direct answer, but here's something that might show you it's not as funky as it sounds:

Do you have any lights in your home that are controlled by two separate wall switches?

If so,  they probably work like this:  

 * both switches up or both switches down,  the light is off.  

 * when one switch is up and one is down,  the light is on.

You know what this is?  It's an XOR circuit!
",null,2,cdhgifz,1qwqb7,askscience,new,5
manofoar,"Folks have covered the theory behind the development pretty well. In a modern context, gates are built up through assembling transistors to work in concert to create the functionality.  In modern chips, these transistors are created by layering semiconductive materials with conductive ones, or by ""doping"" silicon with a thin layer of conductive material in a pattern.  The process of doing this is known as lithography.  

In the lithographic process, transistors are assembled upon a silicon substrate (with appropriate infusions of conductive materials) layers in such a way as to create the conductive and semi-condictive regions that can then be used as a transistor. The sale of these operations is on the order of nanometers. When you hear about chips that have "".1"" nanometer transistors, that means that the size of a transistor is 1nm in width, as measured from conductive region that ""collects"" the electrons, to the other region that ""emits"" the electrons, and with a semiconductive region between that also has in contact with it a ""base"".

In more laymans terms, you can consider the ""emitter"" to be just that - electrons flow OUT of the emitter. the ""base"" is the control, and the collector is where the electrons go ""to"".  the base controls this flow of electrons by means of a voltage applied to it.

These terms actually originate from back in the vacuum tube era of electronics, albeit with modifications. In the old vacuum tube days, you had your cathode, anode, and a grid. These functioned identically with that transistor - your anode emitted electrons (hence why the emitter is called that"", the cathode ""collected"" them, and the grid was what controlled the current flow between the two.
",null,1,cdhjliy,1qwqb7,askscience,new,4
mbizzle88,"The basic idea:

* AND: If you have two switches in a series (like [this](http://www.renesas.eu/media/edge_ol/engineer/04/img_02.gif)) then the current can only pass through if both switches are closed.

* OR: If you have a circuit with two switches in parallel (like [this](http://www.renesas.eu/media/edge_ol/engineer/04/img_03.gif)) then current can pass through it if at least one switch is closed.

* NOT: Electrical current will always take the path of least resistance. So if you have a switch that, when closed completes a ""short circuit"", current will travel through the short circuit rather than to the output (like [this](http://www.bottomlayer.com/bottom/banks/images/batt_NOT_gate.jpg)).

With these three kinds of circuits you can construct any other logical gate (such as XOR, NAND, and NOR).",null,0,cdho4sw,1qwqb7,askscience,new,3
VennDiaphragm,"Can you imagine a NOT gate (an inverter)?  If you know how a transistor works (sort of like a relay), this gate is very simple to imagine.  Put voltage on the base, and it opens the circuit.  If that circuit opens to ground and you have a pullup resistor, it's a NOT gate.  

How about a NOR gate?  That's like an inverter but it has 2 inputs, and if either of them are ON, the output is OFF.  Imagine connecting both of the inputs to the NOR gate to the base/gate of a transistor with appropriate resistors.  Basically, if either line has voltage, it will turn on the transistor.  

Once you have a NOR gate, all other gates can be made from it.  If you don't understand this, I'll quickly explain.  First, you can tie the inputs and you get an inverter.  Second, accept this formula: !(A|B) = (!A &amp; !B).  The left side of that equation is a NOR gate, so if you look at the right side it's telling us that if we invert the two inputs to a NOR gate, we have an AND gate.  And so on.

Of course, modern gates are designed with special properties for power reduction, speed, impedance, etc. ",null,1,cdhgktq,1qwqb7,askscience,new,3
deadlywoodlouse,"I'm a Computing Science student who has studied electronics, and I am really interested in their fundamentals.

Modern computing relies on electronics, mainly because of the advantages of (lack of) size and (abundance of) speed. However, you don't need to use electronics, or electricity at all for that matter. In fact, the first computers were purely mechanical, using gears and levers in all sort of cool ways. The US Navy used analogue mechanical computers for calculating where to target enemy ships, for example ([This](https://www.youtube.com/watch?v=s1i-dnAH9Y4) video explains how they worked, if you're at all interested). You could, if you wanted to, build an entire computer out of sticks if you want, but I don't recommend it. It would be very big, very slow and also very loud.

If you want to understand how modern electronic logic gates work, see some of the other responses here. But if you want to understand how they can work in general, then carry on reading this.

The thing main thing about logic gates (and computing in general) is encoding. That's all there is to it. For example, we could say that a lit light corresponds with logic 1, and an off one with logic 0. We could just as easily do the opposite, with a lit bulb corresponding with logic 0. The point is that it doesn't matter what you use, so long as you are consistent with it.

I've made an [album of diagrams](http://imgur.com/a/HCHME#0 ""Sorry for the quality"") for you, to help explain using (wait for it) sticks. I've assumed you're familiar with symbols for logic gates; if you aren't, [here](https://en.wikipedia.org/wiki/Logic_gate#Symbols)'s Wikipedia's entry for logic gates. My apologies for the crappy MS Paint quality, I wanted to get it ready for you as soon as possible.

As TheBlueShrike said [here](http://www.reddit.com/r/askscience/comments/1qwqb7/how_are_logic_gates_andorxor_etc_built/cdhik2v), redstone in Minecraft can help a lot with visualising logic. As an example of just how far Minecraft can go, someone [built an ALU](http://www.youtube.com/watch?v=LGkkyKZVzug&amp;feature=player_embedded ""Arithmetic and Logic Unit"") out of redstone. (The circuit was based on a [computer you can build yourself](http://nand2tetris.org/ ""Nand2Tetris""). It's semi-related to what you've asked: you build a computer from NAND gates up).",null,1,cdhk1di,1qwqb7,askscience,new,2
rocketsocks,"Transistors can be used natively as NAND gates. The rules of logic show that you can build any other logical operations using just NAND gates, so all you have to do is wire up transistors in different ways to achieve any possible logic circuit.

Consider NOT, for example, that's just ""NAND(x, 1)"" or ""NAND(x, x)"". AND is then just ""NOT(NAND(x, y))"". OR is just ""NAND(NOT(X), NOT(y))"". NOR is just an inverted OR. XOR is a bit more complicated but I think you get the idea.",null,0,cdhn4mr,1qwqb7,askscience,new,1
null,null,null,6,cdhk32u,1qwqb7,askscience,new,6
gfpumpkins,"Red. If you didn't have at least some oxygen in your blood at all times, you'd be in serious trouble. ",null,0,cdhbc3s,1qwpio,askscience,new,2
baloo_the_bear,"Arterial blood is bright red while venous blood is a much deeper, nearly purple, red. The reason for this is the oxygen content of the blood. If you nick a vein, however, the blood will contact the air and grab oxygen out of it, giving it a bright red appearance.",null,0,cdhcbrz,1qwpio,askscience,new,1
Daniel_Oss,"Blood is red all through out the body, however venous blood (blood with less oxygen on its way back to the heart) is a much darker red than oxygen rich blood. The reason for this is due to hemoglobin in your blood tends to give off a red spectrum but other animals contain other molecules to help them carry around oxygen such as hemocyanin which gives off a purpleish spectrum. In other words, there is purple blood around but it is NEVER present in humans.",null,0,cdhqxly,1qwpio,askscience,new,1
Manhigh,"Theres a relatively long term launch window that is governed by the alignment of the Earth and the target planet.  The Earth and another planet will have the same relative alignment around the sun after one synodic period.  For Earth and Mars this is approximately every 730 days.

Think of efficient ways to get to Mars.  If you wanted to travel the shortest distance, you would aim to hit Mars at its close-approach to Earth, flying radially away from the Sun.  This is incredibly costly from a propellant standpoint.

Generally, the most efficient way to get between two circular coplanar orbits is the Hohmann transfer.  You would leave Earth tangential to it's path, and arrive at Mars tangential to its path, travelling 180 degrees around the Sun.  The problem is, Earth and Mars aren't quite coplanar, so it's actually impossible to do this with a 180 degree transfer unless you utilize a midcourse correction, and even then its pretty inefficient.

Instead, we generally take somewhat shorter or longer trips.  If you look at page 33 of http://www.ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20100037210_2010040782.pdf you'll see an example of a ""porkchop plot"" which shows how much energy the launch vehicle has to put into launching the spacecraft to leave and arrive on the specified dates.  There are two local minima, one for shorter trips, and one for longer trips.  This is generally the opening of the launch window.  Launch early and you pay a penalty, and launch late and you'll pay a penalty.  That's the basics of the launch window as you mentioned it.

Other factors that go into this are:


- budgets:  Operating a spacecraft requires an expensive team of smart people.  Launching when you can afford to pay them sometimes effects timing.


- lighting conditions:  After columbia the shuttle was required to launch in daylight.  For robotic missions like MAVEN this usually isn't the case.

",null,0,cdhf3hj,1qwphe,askscience,new,17
monkeyfett8,"You plan a transfer orbit between planets such that you pick your launch time and your time of arrival at the other body.  By picking the start and end times you can calculate how much fuel you need to get from one orbit to the other.  You do this for many start and end positions and you can [map out the energy, and thus propellant, needed](http://en.wikipedia.org/wiki/Porkchop_plot).  With this you can say, if I launch November 18th, 2013 and arrive September 22nd, 2014 you will need X amount of delta V which in turn defines that you need Z kg of fuel.   It all comes down to the alignment of Earth at launch and Mars when you plan to get there.

Typically in this map of start and end times, you pick the point which requires the minimum amount of fuel.  You can also pick based on wanting a shorter flight, etc.  Once you design your spacecraft you can oversize your fuel tanks just in case you need some more at any point.  This margin lets you do extra maneuvers as needed.  (Alternately if your launch vehicle is not maxed out for lifting mass, you might be able to get some extra delta V there, without using your spacecraft itself)

Now, since you picked your orbit to be the minimum fuel needed, and you now have a bit more fuel than the minimum, you can freedom around this optimal point.   So this extra fuel might mean you have enough to launch on November 19th, 20th ... through December 7th.  However you might not have enough after the 7th.  You probably wouldn't try to launch early, so your window will start with the optimal time and end with the last date you would have sufficient fuel.  If you miss the end of your window you have to wait until the planets are in a good position again to allow for the fuel you sized to get you there.

**TL:DR:** MAVEN's most fuel efficient launch point to transfer to Mars is probably Today, but each day after requires more fuel. There's enough fuel to postpone the launch until the 7th of December.",null,3,cdhfh6i,1qwphe,askscience,new,6
MayContainNugat,"Between updates, the computer would keep rough track of position, velocity, and orientation using a system of accelerometers and gyros. It would be periodically updated by the navigator taking sextant sightings of stars and measuring their angles against landmarks on the Earth or Moon (The positions and orientations of Earth and Moon were hardwired into ROM memory already). The spacecraft was also being tracked by stations on Earth, and these readings would be uploaded to the computer by telemetry. There were additional other systems providing position data. For instance, during lunar landing, it would receive altitude data from a radar altimeter.

Lots of people were asking this exact same question in 1969. NASA produced a cool [popular but technical video](http://www.youtube.com/watch?v=YIBhPsyYCiM) describing exactly how the computer was built and operated. [This one](http://www.youtube.com/watch?v=fsyDQex0c-M) demonstrates how it was actually used in flight. Pretty fascinating stuff.",null,4,cdhcq9k,1qwmjb,askscience,new,28
vjnexus,"The source code for the apollo 11 mission was released as open source a few years back for the 40th anniversary of the moon landing. http://googlecode.blogspot.com/2009/07/apollo-11-missions-40th-anniversary-one.html

There is an emulator available to run it as well. If you dig through the code you can find some pretty interesting comments in it.",null,0,cdhg8ja,1qwmjb,askscience,new,9
midsprat123,"While there were on board instruments, most data was calculated on Earth then sent to the ship. Important thing to remember is that the on board computers just received data and acted on it because the craft would have been to cumbersome for it to all take place on board. A really, really good video on the guidance system can be found [here](http://www.youtube.com/watch?v=vU5G9VsoER8)",null,0,cdhe3w4,1qwmjb,askscience,new,3
wazoheat,"It's not just metals, it's all materials. [Melting and boiling require energy](http://hyperphysics.phy-astr.gsu.edu/hbase/thermo/phase.html), since it involves breaking inter-molecular bonds. Conversely, condensing and freezing release the same amount of energy due to the re-formation of these bonds.",null,0,cdh8tps,1qwlnc,askscience,new,7
TangentialThreat,"It is very challenging (by which I mean impossible) to focus a laser such that the beam does not disperse. Beams tend to be very slightly cone-shaped instead of perfect cylinders.

It is possible to, say, [hit a specific region of the moon](http://en.wikipedia.org/wiki/Lunar_Laser_Ranging_experiment) with a laserbeam for rangefinding experiments. The spot size is many kilometers across by the time it gets there and is no longer useful for burning things because the energy is so spread out.

If you aimed a laser at interstellar space, the light would keep going forever but would be spread over a larger and larger area. The universe has some dust and gas to absorb the beam and reradiate the energy as heat, but interstellar matter is also rather sparse and not a significant source of loss.

It should be noted that Star Wars physics does not match up very well with real-world physics. For example, ""laser"" beams often move visibly across the screen and look like somebody shot a glowstick out of an anemic crossbow. Real lasers move at the speed of light.",null,0,cdhf2gl,1qwlju,askscience,new,16
EagleFalconn,"You may find [this old, old thread](http://www.reddit.com/r/askscience/comments/1anyst/in_the_famous_doubleslit_experiment_how_do_we/c8z9q5g?context=3) useful. Happy to answer follow up questions.",null,1,cdhe0w3,1qwlju,askscience,new,8
sloan_wall,it will continue 'flying through space' dissipating very very slowly.,null,1,cdhd5js,1qwlju,askscience,new,4
thephoton,"We often say that the output of a laser is a [collimated beam](http://en.wikipedia.org/wiki/Collimated_light) --- that is a beam whose rays all travel parallel indefinitely.

But in fact the output is more accurately described as a [Gaussian beam](http://en.wikipedia.org/wiki/Gaussian_beam). A Gaussian beam has a *waist* which is a region where it approximates a collimated beam. However there is a trade-off: the narrower the waist, the shorter its length. 

Beyond the waist, the beam diverges, and at very far distances the power hitting a given area will fall with an inverse-square relationship to the distance from the source, just like light emanating from a point source.

The beam waist/length trade-off can also be seen as a consequence of [diffraction](http://en.wikipedia.org/wiki/Diffraction) -- a light beam passed through a narrower aperture (or otherwise confined to a narrow cross-section, like at the beam waist) will diffract at a larger angle.",null,0,cdhs0b6,1qwlju,askscience,new,2
Phoenix1989,"This is known as the [solar cycle](http://en.wikipedia.org/wiki/Solar_cycle). 

As for the objects in the pictures I can only guess that they are sun spots or related to the temperature differential of the sun (Brighter spots being hotter and darker spots being cooler than the surrounding area).",null,0,cdhhyg4,1qwkvy,askscience,new,1
Cheesecake_Tiramisu," 

Lipinski's Rule of 5 is the general method to determine whether a drug will pass through the BBB, here are the points:

- Not more than 5 hydrogen bond donors (nitrogen or oxygen atoms with one or more hydrogen atoms)
- Not more than 10 hydrogen bond acceptors (nitrogen or oxygen atoms)
- A molecular mass less than 500 daltons
- An octanol-water partition coefficient[5] log P not greater than 5",null,0,cdhb7zi,1qwhwq,askscience,new,3
iorgfeflkd,"x^(2)+y^(2)=R^(2) where R is the radius.

If you want it as a function, y=(R^(2)-x^(2))^(1/2) for half of a circle, and minus that for the other half. The function is only real for |x|&lt;=R.",null,1,cdh7ewa,1qwhkl,askscience,new,11
DarylHannahMontana,"&gt; infinitely small points

Mathematically, a point doesn't take up any space - that's the way it's defined to begin with - so there's no cause for concern. Thus, the usual formula x^2 + y^2 = R^(2) is still the right formula. 

In reality, if you try and draw a circle, you're only going to get an approximation. Even assuming that we could get it perfectly circular, the pen stroke is going to have some width (this sounds like the concern stated in your question). In this case, assuming the pen width is constant (a big assumption), the equation for the figure drawn would be something more like 

|x^2 + y^2 - R^(2)| &lt; ε

where ε a constant.

If the stroke width varies, it would be something more like 

|x^2 + y^2 - R^(2)| &lt; ε(x,y)

i.e. ε can vary with x and y.

And if it's not perfectly circular, it could be something like 

|x^2 + y^2 - R^(2)(x,y)| &lt; ε(x,y)",null,1,cdh8pm1,1qwhkl,askscience,new,6
__Pers,"One formula for a perfect circle (a la Dido's Problem, named after the Queen of Carthage in Virgil's *Aeneid*) is that the circle is the extremal shape maximizing enclosed area A for a closed curve of perimeter L. More specifically, the isoperimetric inequality 4 pi A &lt;= L^2 holds, with equality in the case of a circle in Euclidian space. Generalizations to curve space (e.g., the surface of a sphere) are possible. ",null,0,cdh8obx,1qwhkl,askscience,new,4
-Ignotus-,"x^2 +y^2 =r^2 with r as the radius.
How do you get there?
Let's start with the definition of a circle: a circle is a collection of points that all have the same distance to a certain point (the center). (Might be using the wrong terms here, as I learned math in Dutch, but you should get the point)
Let's start with point P(a, b). The distance from P to (0, 0) is then sqrt(a^2 +b^2 ).
Basically because of Pythagoras we have a^2 +b^2 =distance^2 . So for a distance of 5 it would be a^2 +b^2 =5^2. If you were to enter this into wolframalpha you'd get a nice circle, because every point (a, b) that fulfills the equation has a distance of 5 to (0, 0). 

Note: my terminology might be completely off, but I think you get the point.
",null,0,cdhh55a,1qwhkl,askscience,new,1
quantummonkey25,"In the case of lions, social groups are typically one or two males and a pride of females. Infanticide usually occurs when a new male takes control of the group, thus any offspring born after an event are likely to be that of the new male. Other species, such as chimpanzees, which have beta males present in the groups, have much less established paternities. In fact, prior to genetic testing, all family lineages in chimpanzees, such as those described by Dr. Jane Goodall, were documented following only mother-child connections.",null,0,cdhmcyi,1qwfl3,askscience,new,3
MCMXCII,"When using Ampere's law, you draw Amperian loops around currents to find the magnetic field generated by these currents. But the current must flow into and out of the loop.

No imagine you have a wire carrying current into a capacitor. Now draw your Amperian loop so that the current pierces through it on one end, but the other end is in between the two capacitor plates. You have an Amperian loop where current goes in but it never comes out. So using the old version of Ampere's law, you'll find that there is no magnetic field. But that's nonsense because the wire is carrying current, so clearly it must have a magnetic field. 

Once the displacement current term is added to Ampere's law, you find that the changing electric field between the capacitor plates generates a magnetic field, and all of physics is restored to how it should be.",null,1,cdhie33,1qwe28,askscience,new,4
fishify,"Are you familiar Faraday's Law, that there is electric field generated from a time-changing magnetic flux?  The displacement is the analogous term in the other direction, providing the generation of a magnetic field from a time-changing electric flux.",null,0,cdhm1d2,1qwe28,askscience,new,1
wazoheat,"**tl;dr: Using some very rough estimates, it seems that cigarettes do likely have a small but discernible effect on air pollution**

This calculation was a bit harder than I thought it would be at first, but I think we can make some solid conclusions from the following estimates.

There are [about 5 trillion cigarettes](http://www.longwood.edu/cleanva/cigbutthowmany.htm) produced each year. Let's assume that each one that is produced is smoked. To look at the impact on pollution, let's look at one specific type of harmful pollution: [PM_2.5](http://www.nasa.gov/topics/earth/features/health-sapping.html), or small particles less than 2.5 micrometers across. This type of pollution is one of the most important for impacts on human health, and is mostly attributable to combustion sources (fires, power plants, cars), so probably most relevant for this calculation. 

Each cigarette weighs [0.8 grams](http://viewknowdo.blogspot.com/2012/01/how-much-does-cigarette-weigh.html), so assuming that each cigarette is entirely burned and turned into particulate matter, this will add 4 billion kilograms of this type of pollution to the atmosphere each year. The whole cigarette is not burned and released in to the air, as we can see from ashtrays and cigarette butts, so the number is probably closer to 1 billion kg.

Now this might seem like a big number, but we need to compare it to other emissions to be sure: The worldwide emissions of PM_2.5 are [41 million tons per year](http://www.worldenergyoutlook.org/media/weowebsite/energymodel/IIASA_Emissions_Impacts_WEO2011.pdf), or about 37 billion kg.

So if we assume that every manufactured cigarette is smoked, and 25% of the smoked cigarette is released into the atmosphere, that means that 2-3% of particulate pollution worldwide can be attributed to cigarette smoke. Now, this is a very rough figure, but even if it's off by a factor of ten, it would still be significant enough to be measured. Additionally these calculations don't take into account that cigarette smoke is concentrated around where people are, whereas other sources (such as power plants and wildfires) might not be.

This is all just educated estimates, but I think this is the best answer I can give. It also agrees with some related studies [such as this one](http://tobaccocontrol.bmj.com/content/13/3/219.full).",null,1,cdhiftx,1qwe0g,askscience,new,3
homininet,"Very complicated-ly....

So I might do a little bit of a cop-out and give you the short version and a link to a youtube video if you're more interested. It is actually very complex, and I could dedicate a chapters worth of explanation to it, and many have.

So here is a great video, detailing in painstaking detail how it happens (http://www.youtube.com/watch?v=5DIUk9IXUaI). 

But essential the heart is original a tube. With a venous end, and an arterial end. This tube ends up undergoing some folding, which basically separates it into an ventricular half (in front), and an atrial half (behind). However it is still a tube, and there is a single opening between the ventricular and atrial halves. Then it gets tricky. A series of septa sweep down towards the center of the developing heart, and partition both the atrium and ventricle into left and right halves. However, this whole time blood is flowing through the heart, so a series of holes are gained, lost and divided to ensure that blood can still get where its going. Eventually the septum dived the heart into something that we would recognize in shape as a human heart. But the final problem is that in a fetus, oxygenated blood comes from the mother, not the lungs. So there actually exists a little shunt in the heart thats used to allow oxygenated blood into the heart in such a way that it can get to the aorta and bypass the lungs. When the baby is born, the shunt is closed, and blood flows the normal way.

So thats what happens in a nutshell. It is very complicated, so let me know if you want clarification on any of the stages. ",null,0,cdhkum4,1qwdzu,askscience,new,3
Henipah,"The first step is to form a tube that runs from up to down through your chest. This is basically a large blood vessel. It then [folds](http://php.med.unsw.edu.au/embryology/images/4/4b/Heart_Looping_Sequence.jpg) to form a more familiar shape. On the interior the heart cells remodel using the *turbulence of flowing blood* to tell them what to do. They form the septa that divide up the chambers. 

[More info](http://php.med.unsw.edu.au/embryology/index.php?title=Cardiac_Embryology)",null,0,cdhkqzn,1qwdzu,askscience,new,2
uberhobo,"Yes, it's called a match.  It's even simpler than electrolysis, actually.  

But, if you want a method that's about as complicated as electrolysis, you're looking for fuel cells.",null,3,cdh7ldc,1qwdlb,askscience,new,12
therationalpi,"The system, as you defined it, isn't closed. For a resonating chamber to amplify sound, there has to be acoustic energy getting pumped into the resonator. The trick is simply that the acoustic energy isn't getting dissipated, so the total energy in the system is increasing linearly with time until it reaches equilibrium with the losses.

The best way to think about this is that the resonator is a device that takes acoustic energy at a given frequency and holds on to it with low dissipation. The analogy I always hear is water getting poured into a bucket. If I am pouring water into a bucket with a small hole, the water level in the bucket will increase. As the water level gets higher, the pressure will increase, and water coming out of the hole will increase. Eventually the volume of water pouring out of the hole is equal to the water getting poured in. In this case, the bucket is the resonator, the hole is the dissipative loss in the resonator, and the water is energy. Note that, at any given time, the water inside the bucket is greater than the volume coming out, but if you stopped pouring water in, eventually the bucket would totally drain.

Likewise, with the resonator, the acoustic energy is getting dissipated at a rate proportional to the energy stored in the resonator. Eventually the losses will match the input energy, and your resonator will reach equilibrium. The amplitude of the wave at this equilibrium is higher that the input wave, but we can still account for all the energy if we look back in time to when the source started going.

Hopefully that answered your question. Cheers!",null,4,cdhhdrr,1qwdkp,askscience,new,11
SingleMonad,"I'm going to assume you really are asking this question:  ""If I tie a guitar string between two nails on a post, and pluck it, I don't hear much sound.  How does the body of a guitar make the sound louder?""

First:  Why is the sound so soft to begin with?  Because, the string doesn't move much air.  The string is so small that it just doesn't make good 'contact' with the medium of the air, and the wave energy can't move off of the string.  There is an analogy to this in electronics called [impedance matching](http://en.wikipedia.org/wiki/Impedance_matching).

The body (or resonator) is an *impedance matching* device.  The string can couple more efficiently to body via the guitar bridge.  The walls of body have more area of contact with the air, and can move more air.

What /u/therationalpi said about energy is correct.  In equilibrium, the same amount of energy moves from string to resonator as moves from resonator to air.  It's the coupling efficiency that makes it work.",null,1,cdihccf,1qwdkp,askscience,new,3
klenow,"The proteins, fats, and carbohydrates are (for the most part) still there. They are not volatile enough to boil out. Remember that your food, just like, is mostly water. ",null,1,cdhjo08,1qw9xs,askscience,new,2
afcagroo,"There are several different technologies for constructing security ""holograms"" (they usually aren't literally holograms).  None of them can be replicated with a standard offset press or something like a laser printer; they require specialized and generally expensive equipment. For one thing, they generally require feature sizes that are smaller and require more accurate registration; some as low as 100nm. For some types, the ""hologram"" is constructed in layers, with the reflective material angled in a specific direction.  
  
Certainly it is possible for a counterfeiter to buy/build the equipment to make security holograms, but the capital outlay would be significant.  ",null,0,cdjqryq,1qw788,askscience,new,1
Hiddencamper,"The differences are in the way they are designed, and the enrichment of the fuel.

Lets talk about bombs first. Nuclear bombs utilize very high enrichment fuel (&gt;90%). Nuclear bombs are designed such that when they are assembled into a critical mass, the goal is for them to go prompt critical, then release as much energy as possible before the bomb blows itself apart. Nuclear bombs have no control systems, and all feedback mechanisms are designed to try and make the bomb as effective/efficient at releasing energy as possible.

Nuclear reactors are very different. They generally utilize low enerichment fuel (&lt;5%), (exception: naval reactors). Nuclear power plants are designed and operated such that they cannot maintain full core criticality on prompt neutrons alone, they rely on delayed neutrons. Prompt neutrons are generated in about 10^-4 to 10^-7 seconds, while delayed neutrons take several seconds to be generated. Because power reactors rely on delayed neutrons, their power changes take several seconds to occur, and this allows time for active control systems or passive feedback mechanisms to control reactor power. (Passive feedback mechanisms include the Doppler effect, voids/boiling, temperature effects, liquid density effects, etc) In a worst case scenario, it buys enough time for reactor protection system to automatically scram the reactor on high flux signals. Nuclear reactors utilize sufficient negative reactivity coefficients that they cannot undergo rapid out of control power loops (some exceptions, like when Chernoby's RBMK reactor was operated inappropriately. Part of this was the particular design of this plant, part of it was the way it was inappropriately operated). 

Both ""reactors"" (bomb and power) generate heat. The difference is how fast you get to that peak power output. Bombs get there in moments, while nuclear reactors take quite a while. A nuclear bomb will go from completely subcritical to an explosion (several orders of magnitude) in moments. The worst a nuclear power reactor can do, is about a 2-3 times power increase, before negative reactivity coefficients pull this down, and/or a reactor scram terminates the power increase.

Just an interesting anecdotal data point, the worst case reactivity increase that I've seen in a commercial BWR happened when a reactor cooling valve opened at the maximum possible rate due to a control system error during plant startup testing, which caused the valve open signal to bypass the valve motion limiter. The sudden increase in cooling reduced the steam voids in the reactor, which led to a prompt neutron reactivity spike of just over 300% neutron flux, which was terminated by a reactor scram in about 1.1 seconds. No fuel damage occurred because 2 seconds of a flux spike does not allow enough time for heat to transfer to the fuel cladding and cause damage. The plant was a ~3000 MWth BWR.",null,0,cdhixmx,1qw712,askscience,new,2
wwarnout,"Uranium is usually used for power generation, and is mined from Uranium ore, which is mostly U-238 with a small amount of U-235.  The latter is the fissionable component (i.e., the radioactive part).

In low concentrations, U-235 will decay in a fission reaction (it breaks apart), with heat as the by-product.  Power plants use Uranium that has been enriched to about 5% U-235, which produces a lot of heat, but not quickly enough to cause an explosion.

Weapons-grade Uranium is enriched to about 90+% U-235.  It also produces heat, but in this case, the heat is produced so quickly that an explosion occurs.",null,1,cdhg2mj,1qw712,askscience,new,2
iorgfeflkd,"The amplitude of an electromagnetic field is the strengths of its transverse electric and magnetic fields, measured in Volts per meter and Teslas respectively. If you want to look at it in terms of photons, the intensity is related to the number of photons as well as the square of the amplitude, so the field strength is related to the square root of the number of photons.",null,1,cdhhgny,1qw6dq,askscience,new,8
tliff,"Maybe nitpicking but those two don't really hold. 2 is even and is prime. 5 ends with a 5 and is prime.

What you're listing are [divisibility rules](https://en.wikipedia.org/wiki/Divisibility_rule) which are simple ways to tell if a certain number is divisible by another. You can use those rules to find the divisors for a number and if you only find them to be 1 and the number itself the number is prime but simply fulfilling one of these rules is not enough.",null,0,cdhg84f,1qw5gv,askscience,new,8
iorgfeflkd,The number of prime numbers below x is approximately x/ln(x). This is not the most precise known approximation.,null,1,cdhhmue,1qw5gv,askscience,new,4
GOD_Over_Djinn,"Here's one that isn't just a divisibility rule. If p is prime, then for any natural number n, n^(p)-n is divisible by p. This is called [Fermat's little theorem](http://en.wikipedia.org/wiki/Fermat%27s_little_theorem). This gives us a nice way to test if numbers *aren't* prime (note that the implication doesn't go both ways, a fact that gives rise to what are called [pseudoprimes](http://en.wikipedia.org/wiki/Fermat_pseudoprime). ",null,0,cdhiklt,1qw5gv,askscience,new,6
rlee89,"Any number (other than 3) whose digits sum to a multiple of 3, must have 3 as a factor, and thus not be prime.

&gt;If it's even it cannot possibly be prime.

(other than 2)

Of course, 'even' is defined as 'divisible by 2' so that one's rather trivial.

&gt;If it ends in a 5 it cannot possibly be prime.

(other than 5)

That one is really just an artifact of the base 10 of decimal representation.  You can for similar reasons say that any number other than 2 ending in 2, 4, 6, 8, or 0 cannot be prime (though that does overlap with 'cannot be even').",null,0,cdhg9bp,1qw5gv,askscience,new,2
IncongruentModulo1,"The rules you described are actually division rules. If a number is even, that means that it is divisible by 2. If a number ends in 5 or 0, then it is divisible by 5. Clearly, the only primes divisible by 2 or 5 are 2 and 5, respectively.

More interesting is the fact that every real natural number has a unique prime factorization. 

This is not true for something like the complex natural numbers, for example. 20 + 0i = (4 + 0i) * (5 + 0i) = (2 + 0i) * (2 + 0i) * (2 + i) * (2 - i)

The fact that every real natural number has a unique prime factorization is quite useful in number theory since there are a good number of arithmetic functions that are additive and multiplicative. From Wikipedia:

`Then an arithmetic function a is`

    additive if a(mn) = a(m) + a(n) for all coprime natural numbers m and n;

    multiplicative if a(mn) = a(m)a(n) for all coprime natural numbers m and n.

Maybe you can see that if we have the prime factorization of a number, say n = {p1}^{a1} * {p2}^{a2} * ... * {pk}^{ak}, then to compute f(n), you just need to do f({p1}^{a1}) * f({p2}^{a2}) * ... * f({pk}^{ak}) or f({p1}^{a1}) + f({p2}^{a2}) + ... + f({pk}^{ak}). This will come up a lot more than you might expect. Here are some examples of [multiplicative](http://en.wikipedia.org/wiki/Multiplicative_function) and [additive](http://en.wikipedia.org/wiki/Additive_function).
",null,0,cdhgfd5,1qw5gv,askscience,new,1
SwedishBoatlover,"It is hypothesized that gravity is mediated through elementary particles called ""Gravitons"". This is a hypothesis, not a theory, which means that scientists are far from sure. It has not been proven whether gravitons actually exist or not, but it would make sense since the other three forces are mediated by elementary particles (electromagnetism through photons, strong interaction by gluons and weak interaction by W and Z bosons). ",null,0,cdhfz90,1qw4kk,askscience,new,5
iorgfeflkd,It's part of the universe we live in. Any answer you get will just reduce to that.,null,4,cdhhh7j,1qw4kk,askscience,new,6
binkabi,"The size of Andre the Giant is inlarge due to a syndrome called acromegaly. Acromegalycauses overproduction of growth horomone, resulting in the large size of the patient. Several studies including [Leon-Carrion et al, 2010](http://jcem.endojournals.org/content/95/9/4367.full) supports the hypothesis that the overprodustion of GH (and IGF-I) leads to impairment of the brain, especially the memory (LTM and STM).

EDIT: Please take note of the correction /u/Coldhardt made to this post
",null,303,cdh4dcy,1qvy7q,askscience,new,1276
TillyGalore,"Craniofacial development research has shown that the way the skull grows is in part due to the brain. The skull is not resting on or compressing the brain, in fact the brain can move around inside the skull with some freedom, yet it is attached to the skull through many dural attachments. With that being said, the increased skull density will not push on the brain, because in conditions like gigantism, the fontanelles do not close prematurely and therefor will grow along with the brain to a size supportive of the brain tissue. ",null,11,cdh5mcq,1qvy7q,askscience,new,61
null,null,null,16,cdh4s43,1qvy7q,askscience,new,40
sporclesam,"Usually caused by [pituitary adenomas] (http://www.cancer.org/cancer/pituitarytumors/detailedguide/pituitary-tumors-what-is-pituitary-tumor), acromegaly such as Andre's could cause vision loss and severe headaches (symptoms not due to increased bone mass, but similar to stroke) apart from other body dysfunctions [detailed here] (http://www.nlm.nih.gov/medlineplus/ency/article/000321.htm). ",null,5,cdh4es8,1qvy7q,askscience,new,15
DJ_the_Greek,"In the case of gigantism, chronic heart failure begins to take place do to the extreme demand for Oxygen ad blood to the disproportionately large muscles, organs, and tissues.  Typically, the cardiac muscle itself compensates by growing larger and stronger (hypertrophy), which eventually leads to the heart muscle functioning in a progressively less elastic fashion.  Basically, the heart gets to a state where it is not able to stretch and contract adequately to perform its function.  
I realize this is a question about the brain, but I thought the description of heart failure might draw a distinction between the effects of gigantism on the heart vs the brain, i.e. There is not an increase of demand on the brain like there is for the heart.  I'm seeing some threads here talking about how the brain is affected, but I personally haven't done the research.",null,0,cdhh0y0,1qvy7q,askscience,new,7
null,null,null,5,cdhaou5,1qvy7q,askscience,new,7
Manilow,"Endothelial dysfunction (pathalogic changes to the lining of your blood vessels) is common in people with acromegaly.  This leads to a much higher risk of cardiac events like heart attack and stroke, when combined with the changes seen in the cardiac muscle.

Endothelial degeneration is associated with cognitive impairment, both long term and acute impairments from ischemic events.

So yes, it does effect the brain eventually.  Anything that effects cerebral perfusion will effect it in the long run. ",null,0,cdhdaqw,1qvy7q,askscience,new,1
null,null,null,11,cdh63cq,1qvy7q,askscience,new,6
null,null,null,72,cdh4e5i,1qvy7q,askscience,new,12
thatool,"&gt;I was told that crystals grow by magnetic direction.

Do you have any sources for that? I'm not aware of any natural crystals having magnetic poles. Well except for magnetite. Are you referring only to magnetite? Even in that case I find it hard to imagine that it is affected by local magnetic fields. Natural magnetite tends to be formed of multiple domains that oppose each other and form a net zero total magnetism.

",null,2,cdh3af9,1qvwsv,askscience,new,7
UpsidedownGround,"The land did not rise from the water, per se. The land masses had already begun to form by the time there were oceans on earth's surface. 


Very close to earth's formation, it was a giant ball of undifferentiated molten material. As it began to cool, that material started to separate based on its density, with the heaviest stuff sinking to the planet's center and the lighter stuff bobbing to the top. The lightest stuff is what makes up the continental landmasses. 


Those landmasses aren't static, however. Earth's crust is fragmented into a bunch of tectonic plates that sit atop the outer mantle. This mantle is somewhat plasticky (think a wad of silly putty), and so it flows in a manner of speaking, dragging the tectonic plates around with it. Sometimes the plates collide, scraping off bits of continental crust onto each other.  When all of the continental crust just happens to be accreted together at one time, we call the landmass a ""supercontinent."" We have good evidence that there have been seven such supercontinents in earth's history. Pangaea was the most recent, occurring about 300 million years ago.  It split apart as the tectonic plates underneath it started flowing away from each other, ripping the continent asunder and creating the Atlantic Ocean.

Source: Second year geology student.",null,0,cdh45il,1qvw5f,askscience,new,46
null,null,null,18,cdh4tfy,1qvw5f,askscience,new,1
barc0de,"If you are an astronaut, in a space suit, firing a gun, then yes - you will hear something as sound will be transmitted through your glove into the suit. It wont sound anything like a normal gun firing, as most of that noise is generated by the shockwave of the bullet going through the air.

The astronaut floating a few feet away will hear nothing, unless the bullet hits them of course",null,2,cdh29ez,1qvvtd,askscience,new,16
Ruiner,"No, it won't. Sound needs a physical medium to propagate, so you won't hear anything in the vacuum.",null,4,cdh1fv9,1qvvtd,askscience,new,10
5p0ng3b0b,"Here is a wikipedia article and 2 discovery articles that will help you understand more about this topic. Although there are common rat-like ancestors, there are also better intermediate examples that were amphibious and maybe give a better resemblance.  
On the size thing, there are only speculations of course, but the most common theories involve the abundance of food and the skeleton adapting to water(less restrictions).  
I can't find a reference to the blow-holes but if you look at the images of the skeletons and digital reconstructions of intermediate species you will notice that their nostrils start from the edge of their long noses and gradually goes further back which would give them some advantage being fully submerged.

  
http://en.wikipedia.org/wiki/Evolution_of_cetaceans  
http://news.discovery.com/animals/whales-dolphins/blue-whale-larger-than-ever-120131.htm   
http://news.discovery.com/animals/dinosaurs/how-dinosaurs-got-so-big-120131.htm",null,25,cdh2q3u,1qvvpn,askscience,new,92
mehmattski,"Here are some non-Wikipedia webpages:

[The Panda's Thumb](http://www.pandasthumb.org/archives/2008/03/whale-evolution.html) explains how embryonic development of the blowhole in the modern dolphin is very similar to what we observe in fossil whales.

&gt; In most mammals, the nose opening is located near the tip of the snout. In modern dolphins, on the other hand, it is located on the top of the head, above the eyes. It is called the blowhole.

&gt; In development, the nose opening shifts from the tip of the snout (arrow in left embryo) to its position on top of the head.
&gt; 
&gt; Ancestral whales also have their nose opening near the tip of the snout, and the shift to the forehead is documented evolutionarily by fossils.

The page also notes a transition from nostril to blowhole evident in the series of fossils known as *Pakicetus*, *Ambulocetus*, and *Kutchicetus*. Each organism has a blowhole position between that of modern ungulates and that of modern whales. http://pandasthumb.org/nasal_drift.gif

A great deal more information about the evolution of whales can be found at [TalkOrigins](http://www.talkorigins.org/features/whales/).  
 ",null,1,cdh4fye,1qvvpn,askscience,new,19
atomicrobomonkey,long story short is their blow hole is a nostril that move to the top of the head.  Size is because they live in water.  A whales weight would crush it's own organs on land but they are almost neutrally buoyant when in the water so there was nothing stopping them from evolving to be bigger.,null,9,cdh30h3,1qvvpn,askscience,new,20
homininet,"You also might want to check out this website (http://www.pasttime.org/?p=476) and the associated podcast. One of the recent episodes was about whale evolution. Whales are actually more closely related to cows and deer and hippos (artiodactyls) than they are to rats, and what they probably evolved from was something totally unlike rats (and also something unlike any modern artiodactyls). 

As far as the body size increase, one thing to keep in mind is there are a lot of biomechanical constraints to body size in terrestrial animals. Its really hard to achieve a massive body size, and only a few things have done it (sauropod dinosaurs for example). But once you get into the ocean, the musculoskeletal system gets a little bit of a load off because of the buoyancy of the water. So its at least a little easier for animals to achieve huge body masses.
",null,5,cdh3ln5,1qvvpn,askscience,new,18
iamdelf,"This is sort of tangential to the original question, but I've always wondered about it.  Since whales, particularly baleen whales feed directly off of plankton and therefore skip the entire food chain in the ocean, was their evolution associated with a mass die-off or mass extinction event due to food stress in other ocean dwelling species of the time?",null,2,cdh6dat,1qvvpn,askscience,new,7
bbqrubbershoe,"Since the fossil record is not complete geneticists have the best evidence.  Richard Dawkins' book The Ancestor's Tale has a chapter called The Hippo's Tale that is very informative (and available for free on books.google).

""Hippo's Tale"": Greek root words: Hippos = horse, potamus = river. Zoologically however hippos are artiodactyl, meaning ""even-toed.""

Wikipedia: ""This group includes pigs, peccaries, hippopotamuses, camels, llamas, chevrotains (mouse deer), deer, giraffes, pronghorn, antelopes, sheep, goats, and cattle. The group excludes whales (Cetacea) even though DNA sequence data indicate that they share a common ancestor, making the group paraphyletic.""

""Paraphyletic"" means that artiodactyls shared a common ancestor, or concestor, with whales (Cetacea). The composite name is Cetariodactyla.  But even this is kind of wrong because ""according to molecular evidence, whales are deeply embedded within the even-toed ungulates. Hippos are closer cousins to whales than hippos are to anything else including other even-toed..."" (Dawkins p197)

""Gathering all this together, we can sketch a forward chronology as follows. Molecular evidence puts the split between camels (plus llamas) and the rest of the artidactyls at 65 million years, more or less exactly when the last dinosaurs died. Don't imagine, by the way, that the shared ancestor looked anything like a camel. In those days, all mammals looked more or less like shrews. But 65 million years ago, the 'shrews' that were going to give rise to camels split from the 'shrews' that were going to give rise to all the rest of the artidactyls. The split between pigs and the rest (mostly ruminants and hippos took place about  55 million years ago. Then the whale lineage split off from the hippo lineage not long afterwards, say about 54 million years ago, which gives time from primitive whales such as the semi-aquatic Pakicetus to have evolved by 50 million years ago. Toothed whales and baleen whales parted company much later, around 34 million years ago, around the time when the earliest baleen whale fossils are found."" (Dawkins p200)",null,1,cdh64is,1qvvpn,askscience,new,5
loveyourlies,"I saw a BBC Documentary that noted marine life can grow to colossal sizes underneath Antarctica (showing a colossal octopus as an example).  

Maybe whales have spent a much longer time (speaking in hundreds of thousands of years) closer to a colder climate?  This would differentiate them from other, more tropical, fish that managed to survive due to their incredible numbers and camouflage techniques.",null,0,cdh8xah,1qvvpn,askscience,new,2
drinkermoth,"tl;dr: The nostrils moved slowly to the end of the snout then back the head to mean less time and effort had to be spent on breathing and the increased size helps thermo regulation efficiency.

In the beginning the quadrepedal mammals took to the water.  Many species did this, as the water - lakes, rivers, the sea and such bodies - contained a plethora of resources for the mammals to use.  These included food, space (possibly the opportunity to escape predators), cooling, etc. 

This was good and many mammals were happy just paddling in the water, after all there were already specialised water animals (fish etc.) that did a great good job of exploiting the water's resources, and so the niches were already pretty saturated.  Evidently however, at least one species found it advantageous to its fitness (survival and reproductive capability for itself and future generations) to venture deeper and deeper into the water.  

This move occurred over many generations and over many more species ventured deeper into the water.  Now there is no way of discerning this with certainty, but if I can be allowed to offer a likely explanation it may have gone like this:

as you get deeper into the water you must occasionally come up for air and so you might rear up and lift your head to breath, over time and generation those individuals with nostrils higher on the nose did better than those with them lower on the head; they could spend less time breathing and more time looking around.  

Once the nostril got to the tip of the nose pressure then drove it back, allowing future generations to lie flat in the water, it is likely that eyes migrated downwards and to the sides (if they had not already) to allow better vision of the water below to watch for predators.  And that is how the whale got it's nose.

As for size, the buoyancy provided by the water around allows animals under the sea to grow in a much less restricted way.  The pressure of the water meant that less investment needed to be made in skeletal strength, muscle mass, blood pressure etc. 

also there is a pressure to grow in size as there is a direct correlation between size and an organisms efficiency at maintaining body temperature (especially important for species that regulate their own temperature like mammals), basically bigger is better for staying warm.  Also water is a more efficient thermal conductor than air and so there would be pressure to put on fat for that reason as well.  By no means do you have to be big to be a water dwelling mammal, but as the whales (and Pinnipeds) specialised into their roles it became important for most of them to keep warm, either due to their deep diving or geographical location.",null,0,cdh9s9g,1qvvpn,askscience,new,2
HughJorgens,"Nobody has pointed out that stress (from  environmental change or new predators) causes more mutations, which helps animals adapt quicker.  The other thing to consider is that animals who are well adapted to their environment (in a low stress environment) don't tend to change.  It's called punctuated equilibrium.  When they change, it happens all at once, in response to new stress.
TLDR  animals don't change unless they need to.",null,2,cdh500v,1qvvpn,askscience,new,2
null,null,null,18,cdh3opg,1qvvpn,askscience,new,7
SynbiosVyse,"What you're experiencing is just the increased blood flow to the area, obviously not a permanent increase in muscle mass. A few hours after your workout and they should be back to normal size, although with some tearing. Then they'll actually increase in mass within the week.

The pump has very little to do with, and is not necessary, to actually get a good workout in and improve mass. However Arnold and a lot of other body builders agree, if it helps you motivated.. and feels pleasurable... it doesn't really hurt you.",null,1,cdh4nx5,1qvv8c,askscience,new,8
Ruiner,"Not exactly. ""Mass"" itself doesn't increase once you increase velocity. The faster an object moves, the harder it is to accelerate it, though. This is only due to the properties of special relativity, an in order to interpret that in a classical context, objects are often said to have a ""relativistic mass"", which increases with velocity. 

Nevertheless, if you are inside an object moving fast with respect to another observer, this is your reference frame and from your point of view, your velocity is 0. Think about this: you are moving very fast with respect to the same, and the sun is moving very fast with respect to the center of the galaxy and so on...",null,1,cdh1hcd,1qvv6f,askscience,new,17
Fenring,"&gt;Centrifugal force

You can't really separate this one. The problem is that the question really has to be considered in terms of General Relativity, in which context the centrifugal force is treated in exactly the same way as gravity (in a sense, it's actually a form of gravity). The centrifugal force is going to be the dominant effect here, causing your overall weight to be reduced.",null,2,cdh1ltx,1qvv6f,askscience,new,3
KerSan,"I'm kind of surprised that none of the answers so far have pointed out that your question is ill-posed. Special relativity concerns itself only with *inertial* frames of reference, which means that you cannot have acceleration or gravity (though these are actually the same thing under Einstein's principle of equivalence). Weighing people involves moving beyond special relativity, at least slightly.

Nevertheless, as /u/Ruiner points out, the answer to your question is 'no'. That is because your weighing scale is also in the same frame of reference as you, so the principle of relativity tells us that your airplane may as well be stationary and the observer is the one speeding past you. Any relativistic effects observed by the moving observer cannot cause them to disagree about the reading of your scale. The effect of relativity on the physics of your body are the same on the physics of the scale itself, so everything cancels out.",null,1,cdh98yl,1qvv6f,askscience,new,1
papagayno,"""Acne occurs most commonly during adolescence, affecting an estimated 70-90% of teenagers.[4] In adolescence, acne is usually caused by an increase in testosterone, which occurs during puberty, regardless of sex.[5] For most people, acne diminishes over time and tends to disappear — or at the very least decreases — by age 25.""

http://en.wikipedia.org/wiki/Acne_vulgaris",null,16,cdh7a0w,1qvuw8,askscience,new,37
perusername,"NSAIDs (Non steroidal anti-inflammatory Drugs) can be absorbed through the skin. They are best used on joints on the body that are close to the surface of the skin.


They can be absorbed into the blood stream, however, this isn't common. The toxicities can lead to stomach ulcers and high blood pressure.

Edit: Missed a question - they often also have dosage recommendations or sizes of amounts to use in the instructions.",null,2,cdh3q97,1qvutl,askscience,new,18
Pest,"In osteoarthritis (where inflammation is relatively limited) I don't think there is much conclusive evidence showing efficacy of topical NSAIDs vs oral. The structure of the joint is such that penetrance of the drug into the joint (in sufficient concentrations to have a biological effect) is limited, as the drug would have to pass through the skin, muscle, joint capsule and synovium into the joint to have any effect on the cartilage and bone that make up the joint. [Citation](http://www.bmj.com/content/329/7461/324)

That being said, the case may be different in rheumatoid arthritis where  the tissues that surround the joint are much more involved in the disease. ",null,0,cdh4nf0,1qvutl,askscience,new,7
Ariadnepyanfar,"Correct me if I'm wrong, but little molecules that are fat soluble can be absorbed through the skin.  They are absorbed through the surface skin cells and released, and then taken up by cells further down and so on.  Some can penetrate down to the bone in this way.  By ""little molecules"" I mean molecules that have 420 protons or smaller (420 atomic weight or smaller).
",null,1,cdh3s4d,1qvutl,askscience,new,2
twentyone_21,"It depends what products you are talking about.

NSAIDS can somewhat be topically absorbed into the system, but produces less adverse reactions than its systemic counterparts. It must be absorbed somewhat to work, but it does not need to completely enter systemic circulation to work. Abstract: http://www.ncbi.nlm.nih.gov/pubmed/15871609

Counterirritants, on the other hand, give nearby neurons different irritation to counteract the actual pain from the joint. They don't have to be absorbed to work. ",null,0,cdhahg2,1qvutl,askscience,new,1
Bakkie,"Topical application of drugs is a well established method of administration. If you take an asthma inhaler you are performing a topical application to the ""skin"" /tissue inside your respiratory system.
Patch application of medicine such as nicotine, Medrol Dosepacks (pain relief and anti inflammatory) are a well establish part of an orthopedic /rheumatology arsenal.

In the US the data base which the doctors use is PubMed.com. It is maintained by the National Institute of Health (NIH) and aggregates peer reviewed journal articles. It is NOT internet medicine. It is free .  Since it collects all international journals I assume it can be accessed outside the US.

As of 3:00p.m. CST on November 18,2013, on PubMed, the terms topical capsaicin yields 1002 results. Here are a couple examples.

If you want to confirm the  process by which topical capsaicin works, you need only go through the list. (In the search term bar, capsaicin alone gave me roughly 25 ways to search)

1. Eur J Pain. 2013 Sep 24. doi: 10.1002/j.1532-2149.2013.00400.x. [Epub ahead of print]
Topical analgesics for neuropathic pain: Preclinical exploration, clinical validation, future development.
Sawynok J.

2.Mechanisms of topical analgesics in relieving pain in an animal model of muscular inflammation. Duan WR, Lu J, Xie YK.
Pain Med. 2013 Sep;14(9):1381-7

3.Eur J Pain. 2013 Nov;17(10):1491-501. doi: 10.1002/j.1532-2149.2013.00329.x. Epub 2013 May 6.
A novel approach to identify responder subgroups and predictors of response to low- and high-dose capsaicin patches in postherpetic neuralgia.
Martini CH, Yassen A, Krebs-Brown A, Passier P, Stoker M, Olofsen E, Dahan A.



The headnote are free. Abbreviations are explained in the headnotes. Some articles are available in full at no charge. All assume you have  expertise and understand technical terms. If you don't understand a term, do a quick Google search.

You will find the answer to your specific question by poking around in there.

Source: insurance defense attorney who takes multiple medical depositions",null,1,cdhetzx,1qvutl,askscience,new,1
murphyw_xyzzy,"There are a few questions here, unpacking them:

1) Can drugs effectively penetrate skin and have an effect? Certainly, this is a 'yes', look at DMSO on a door knobs.

2) Can the effect be local? Yep. Though if it goes through the skin, it will probably circulate in your whole system, there can be a larger effect near where it is put on the skin.

3) Without being toxic (or ""contaminating"")? Need to pin down the substance in question. Some stuff isn't toxic and is delivered this way.

4) Can it help with joint pain? Well, maybe. Is the pain really ""in the joint""? Would loosening the muscles or warming the muscles and tendons near the joint help? Would increasing the circulation to the area also possibly help? I'd lean towards ""yes"" for some of these. I am skeptical, like the submitter, about if it would really help the bone contact surfaces, but maybe these arrows are pointing 'near' the pain and the product is to help with the surrounding tissues.

Edit: Soo many typos!",null,5,cdh4aur,1qvutl,askscience,new,3
null,null,null,3,cdh6qnd,1qvutl,askscience,new,1
JacFloyd,"The Wiki suggests 0,1µSv dose for one banana. Making it 5µSv dose for 50 bananas. Average backgound dose for one day is 10µSv so you would only increase your dose by 50% for the day. You need 100mSv annual dose for increased cancer risk and 400mSv dose in short period time to receive symptoms of radiation poisoning, equaling to to about 2700 bananas every day for a year or 4 million bananas in short period of time, respectively. You get higher doses than 50 bananas by just taking a x-ray scan or taking a flight from LA to NYC. [Link w/ sources](http://xkcd.com/radiation/)

EDIT: These are comparisons of equivalent dose and ignore potassium homeostasis (In reality, excess potassium is quickly eliminated from the body). Also, due to activity of radioactive potassium, the actual dose is estimated as dose over longer period of time. (thanks /u/radioactivefallgrout).",null,20,cdh2ty0,1qvuk1,askscience,new,125
Osymandius,"Bananas are potassium rich, but if you aim to get some sort of radiation damage from them, you're much more likely to kill yourself by disrupting your K/Na balance which permits conduction of electrical impulses through nerve fibres and, more importantly, cardiac tissue. This used to be used as a [lethal injection](http://en.wikipedia.org/wiki/Lethal_injection#Potassium_chloride) method.

You have to appreciate the levels of radiation we're talking about here - you pick up more by going on a long haul flight or have an X-ray, let alone a CT scan.

Edit: Yes - the mother of all diarrheal experiences.",null,13,cdh2hya,1qvuk1,askscience,new,74
BCMM,"Bananas are only radioactive^1 because they contain potassium, and all naturally-occurring potassium contains radioactive isotopes. Incidentally, potassium is necessary for survival, so short of consuming expensive, artificially purified K39, there's no way to avoid a radioactive dose equivalent to eating a healthy number of bananas.

If you somehow managed to eat massive quantities of potassium-rich food, you'd [die of something else](http://en.wikipedia.org/wiki/Hyperkalemia) long before experiencing any radiation-related symptoms.

^1 Significantly more radioactive than other foodstuffs, that is. Almost everything is at least a bit radioactive.",null,0,cdh4nb2,1qvuk1,askscience,new,16
GimletOnTheRocks,"**No.**  Potassium levels in the body stay relatively constant via homeostasis.  Bananas are only radioactive in that they contain a lot of potassium, a portion of which is a radioactive isotope of potassium, K40.  Unless you ingest pure K40 rather than a mixture, you can't really increase the radioactivity in your body.  Any excess is excreted before the K40 can do damage with its long half-life of 1.25 billion years.

Other radioactive isotopes are much more damaging, particularly those with shorter half-lives and those which are not naturally occurring.  Cesium 137 and Strontium 90 are two examples.

Eat all the bananas you want without worry!",null,0,cdh7qta,1qvuk1,askscience,new,7
Hristix,"The banana equivalent dose (for some reason often confused with a Becquerel) was basically created as a bit of a joke to make fun of reporters and public officials that absolutely freaked out (usually to generate interest in whatever their goals were) over tiny amounts of radiation, and then completely ignore absolutely face meltingly huge amounts of radiation, like those occasional criticality accidents or lost sources that end up exposing people.  Rarely hear much about those in the news unless it has something to do with a nuclear power plant.",null,0,cdh9kjb,1qvuk1,askscience,new,2
Shadow14l,"The air and atmosphere are rotating with the Earth as well. Let's say you could suspend yourself in the air 1m above the ground. Assuming there's no wind but still air, you aren't going to start magically moving around because the earth is rotating below you. Unless something is actively holding you in place against the rotation or air currents, you'll stay exactly positioned above that spot of ground.",null,0,cdh2ivs,1qvt9i,askscience,new,11
stuthulhu,"The plane and the atmosphere are rotating with the earth. This is also why, given that the earth is rotating at ~1000 mph at the equator, if you jump in the air for 2 seconds you don't land half a mile away (or explode messily against any grounded obstacles hitting you at 1000mph). 

The airplane must exert thrust to change its movement along with the earth, which is essentially what it is doing while flying.",null,0,cdh7d06,1qvt9i,askscience,new,5
Ruiner,"You can answer that. When you measure the speed of an airplane, do you measure it with respect to what? Or in other words, what it is that propels an airplane?",null,1,cdh2fce,1qvt9i,askscience,new,6
drzowie,"Since drag scales as something between directly and quadratically proportional to speed, and work is force times distance, the faster you go the more work you will have to do.  So, *ceteris paribus*, you do better with uniform speed than with variable speed and the constant speed on the flat is the best way to go from the standpoint of energy required to move the car.

But you don't actually care about energy used to move the car, you care about how much fuel you have to burn to move the car from place to place.  Depending on the engine in the car it could be more efficient to drive the hills.  Some conventional normally-aspirated gasoline engines are much less efficient at low throttle settings than at high throttle settings - in vehicles with those engines you do significantly better by ""punching it"" up the hills and shifting to neutral going down the hills.

",null,1,cdh7dos,1qvsro,askscience,new,4
do_od,"In the ideal case (no friction or air resistance) a car travelling at a constant speed on a flat road will not use any energy at all. The same is true if there is a mountain or bulge in the way, as long as the car starts out with sufficient momentum to reach over the peak. If the car goes down a valley it will speed up and then slow down in a symmetrical fashion so that when it comes out of the valley it will be going as fast as it was when it started out. The velocity of the ideal car can be described at any point using this equation:

v = sqrt ( 2 \* g \* h + v_0^2)

*v* is velocity, *v_0* is velocity at the beginning, *g* is acceleration of gravity and *h* is the vertical height relative to where the car started from. The direction of *h* is down, so going up will make h negative. 

You see that the greatest height the car can climb to is h = -v_0^2 / 2g, and after that you will have to add energy somehow. 

As for real cars, the power required to maintain speed is higher the faster you go. You will have to supply more information to make your question precisely answerable, because it is possible to use less energy to go up a hill really slow than driving fast on a flat road. Are the cars required to arrive at the same time? How do you measure distances, horisontally or parallell to the ground? ",null,0,cdh29hh,1qvsro,askscience,new,1
samcobra,"Generally, the hormonal imbalances affect the self-regulation of hunger and satiety. People consume more calories and don't feel full. Other hormones such as lack if thyroid hormone can also lower the basal metabolic rate, or the rate at which your body burns calories at rest.",null,0,cdh4x4w,1qvsjq,askscience,new,3
ohnobananapeeeeeels,"let me just preface this by saying metabolism is a very complex thing -- we are STILL making new discoveries about how the body processes and stores the calories that we take in.

like /u/samcobra said, there's a component regarding hunger and satiety (i think he/she was referencing hormones like ghrelin and leptin), but that's not the whole picture. you were asking specifically about how, even if a person takes in less calories than their basal metabolic rate, they can still gain weight.

your thyroid is the main player in metabolism. if it's underactive, then you can gain weight no matter how little you eat. this is because the body kind of bypasses that step where it ""uses"" most of your calories and just places them directly into storage, i.e. fat. you probably will have enough energy to get by and keep your organs running okay, but you won't feel like a million bucks.

that's the basics of it, more or less. but i do want to spend a bit of time talking about the OPPOSITE, wherein you can take in a bunch of calories yet not gain weight. and i'm not talking about exercise, either. back in the day, there was this pesticide called 2,4-DNP. it had the particularly interesting effect of weight loss. it did this by messing with the system of ATP generation (also known as chemiosmosis or oxidative phosphorylation).

to boil down making ATP for you, your body pumps protons to one side of a membrane so there's a buildup on that side. since the forces of the universe want to achieve equilibrium across the membrane, the protons are shuttled through an enzyme called ATP synthase. the movement of the protons generates enough energy for a phosphate to be pinned on to ADP (adenosine diphosphate), making it become ATP (adenosine triphosphate). ATP is the main energy currency for our bodies.

now, if you take in 2,4-DNP then the protons don't go through ATP synthase. the chemical causes protons to pass back through the membrane without creating ATP. in essence, your body goes through the process of taking your food, breaking it down, and readying itself to extract energy from your food, but without actually making any energy whatsoever. it's really quite fascinating.

but anyway, don't take 2,4-DNP because it's a pesticide and i'm quite certain people have died from it.

source: [2,4-DNP](http://en.wikipedia.org/wiki/2,4-Dinitrophenol)

[thyroid](http://en.wikipedia.org/wiki/Thyroid#Hypothyroidism)",null,0,cdh6q2n,1qvsjq,askscience,new,1
homininet,"Howdy. In the video clip, there doesn't seem to be any shot of the pangolin actually running, just walking. There are many differences between walking and running, and from a human point of view the main difference is that running involves a short period of time when both the limbs are off the ground. So by that definition the pangolin isn't running, and I'd be surprised if they ever do. You're right that for many species speed is important in evading predators, but pangolins are covered in those keratinous scales (basically like fingernails), and when a predator gets close to them, they roll up into a little armored ball (like this: http://www.earthrangers.com/wildwire/omg_animals/pangolin-armour/). So moving fast isnt really a high priority for them.

What is a high priority though are those massive claws that they use to dig up ants and termites. And in evolving those giant claws, they've really sacrificed the ability to use their hands and forelimbs for locomotion. The claws just really arent meant for bearing weight and walking and running. So instead, most of the time they walk around on two legs (bipedally) just like us. But if you watch the video closely, they will put their hands on the ground for short periods of time, presumably for balance. So really the way they walk is just kind of a byproduct of the fact that they have those massive shovels on either hand. 

Also fun fact, pangolins are the only other mammals (I believe) that walk bipedally in the same way that humans do, by taking alternating steps (L,R,L etc). Most other bipedal mammals hop. ",null,0,cdhkjh2,1qvpyw,askscience,new,3
wwarnout,"Astronomers assume that the laws of physics are the same everywhere, primarily because there is no evidence to the contrary.  Therefore, they could predict a trajectory pretty well.  However, given the vast distances, there would have to be provisions for making minor course corrections as the spacecraft neared its destination.",null,0,cdhic6k,1qvp9m,askscience,new,1
Stanage,"So when you refer to soap as being ""slippery"" this may seem like it is a different phenomenon than say, when you feel ""slippery"" on ice.  Both stem from the same physical quality, however: a loss of friction.

Soaps and many other bases or hydrophobic materials are known as [surfactants](http://en.wikipedia.org/wiki/Surfactant).  This is due to them having bot a hydrophilic (water loving) and hydrophobic (water fearing) component.  When soap comes into contact with water, it forms a lather that forces the interface between the soapy water and whatever solid it is touching (your skin) to become very hydrophobic, since the hydrophilic portions of the soap molecules are dissolving the water.  Your skin is covered in hydrophobic oils, known as sebum, which are secreted by the [sebaceous glands](http://en.wikipedia.org/wiki/Sebaceous_gland).  Sebum is made up of primarily fatty acids and triglycerides, which are what soap molecules are comprised of.  This makes your skin to be largely water-resistant, and therefore quite hydrophobic.  The interface between your hydrophobic skin and the hydrophobic portions of the soap molecules causes the ""load"" of the soap to move easier across your skin, thus reducing the coefficient of friction, and acting as a [lubricant](http://en.wikipedia.org/wiki/Lubricant).  This is where the ""slippery"" feeling of soap comes from.

As far as the ""slippery"" ice feeling, that has to do with the ultra smooth surface of the ice interfacing with the relatively smooth surface of your shoes, much like the hydrophobic interface between your skin and the soap particles.  The coefficient of friction between the two materials is low, and slipping occurs.",null,0,cdhrz50,1qvp39,askscience,new,3
JimmyGroove,"When you actually hear sound, the nerve endings in your coclea in your ear is stimulated by waves trasmitted to it through the bones of the middle ear, the eardrum, and the air.  These nerve endings then transmit nerve impulses to your brain, where the primary auditory cortex analyzes them and produces the sensation of sound.

In the case of auditory hallucinations the auditory cortex is activated without imput from the ear due to any number of possible factors (physical damage to the cells, stimulation from another part of the brain, etc.)",null,2,cdh12vc,1qvoma,askscience,new,29
neuroPSYK,"A pharacological explanation is an N-methyl d-aspartate (NMDA) receptor hypofunction in corticostriatal and corticoaccumbens projections which leads to a sensory overload in the cortex:

NMDA receptor hypofunction in the thalamus (from the ventral tegmental area etc.) leads to a tonic inhibition of the thalamus. This, in turn, leads to less excitatory drive on gamma aminobutryric acid (GABA) neurons and therefore the thalamus cannot do its job of filtering information (sensory input). The result is that sensory information is allowed to run rampant to the cortex with little/no filter and positive symptoms (auditory hallucinations etc.) often result. 

Basically, the 'filter' that is used for auditory information isn't working, so any neural activity that is generated (from inside or out) is allowed to reach the cortex and the patient will 'hear' them. It is difficult to distinguish what is coming from *outside* and what is generated internally, so the auditory hallucinations are quite convincing. 

*Someone with more knowledge in this area can hopefully expand/criticize this post [not really my area]*",null,1,cdh402d,1qvoma,askscience,new,10
patchgrabber,"It seems to not be that necessary. [This](http://jdr.sagepub.com/content/early/2013/06/05/0022034513492336.full) study says that unless you're at risk for periodontal disease, it's not really a statistically significant increase in prevention.",null,2,cdh3eu1,1qvo2f,askscience,new,9
justin3003,"Whether the biannual timing of the cleaning itself is necessary, I cannot say for certain. But, there is no doubt that regular access to proper dental care is essential. Decaying teeth left untreated can and do become very dangerous abscesses ([1](http://www.webmd.com/oral-health/tc/abscessed-tooth-topic-overview)). Furthermore, published studies have shown that improper oral care can also increase your risk for inflammatory mediated diseases such as heart disease and stroke ([1](http://www.ncbi.nlm.nih.gov/pubmed/20882577), [2](http://www.sciencedirect.com/science/article/pii/S0002870307005418)). And finally, just like with the eyes, a good number of serious diseases manifest as oral symptoms observable to the trained eye ([1](http://emedicine.medscape.com/article/1081029-overview#showall)). It simply isn't enough to go to the dentist when things start hurting/bleeding/etc.; one should view dental visits as an essential part of the preventative medicine routine. ",null,2,cdh8nuk,1qvo2f,askscience,new,4
StringOfLights,"Marsupial pouches actually vary quite a lot in their morphology (the pouch is also called a [marsupium](http://en.wikipedia.org/wiki/Pouch_%28marsupial%29)). [*Dromiciops*](http://animaldiversity.ummz.umich.edu/accounts/Dromiciops_gliroides/) has a mere [indentation](http://www.mammalogy.org/uploads/imagecache/library_image/library/1629.jpg) instead of a full-on pouch. Some marsupials have the forward-facing pouches we're used to seeing, although they may be fairly small. Some have backwards-facing pouches, like wombats, koalas, and the [Tasmanian tiger](http://www.mnh.si.edu/onehundredyears/featured_objects/Thylacine/thylacine_painting_large.jpg).  The American opossum *Didelphis* has a [slit that runs up and down the body](http://wwwdelivery.superstock.com/WI/223/1990/PreviewComp/SuperStock_1990-18873.jpg) (hopefully that photo isn't too gross!). These different morphologies provide different advantages, so depending on what the species is doing ecologically a forward- or backward-facing pouch might be selected for.

However, it's hard to track the evolution of the marsupium in the fossil record because it's something that is only present in soft tissue. Marsupials do have a bone called an [epipubis](http://en.wikipedia.org/wiki/Epipubic_bone) that in theory supports the pouch. The problem is that the epipubis also [varies in morphology independent of the marsupium](http://books.google.com/books?id=HmQTTChULOMC&amp;lpg=PP1&amp;ots=9y1N5kv2XX&amp;dq=marsupium%20evolution%20marsupial&amp;lr&amp;pg=PA56#v=onepage&amp;q=marsupium%20evolution%20marsupial&amp;f=false), so the marsupium doesn't structurally depend on the epipubis being present. This means that we can't really infer what the pouch looked like based on the epipubis. It's also present in a bunch of early mammal groups, including the early eutherian [*Eomaia*](http://en.wikipedia.org/wiki/Eomaia), so it seems to be something that's ancestral and we in fact can't really assume it even indicates the presence of a pouch. It may just be an ancestral character that placental mammals lost.

There is somewhat of a [phylogenetic pattern to the morphology of the marsupium](https://www.researchgate.net/publication/250927274_The_marsupial_pouch_implications_for_reproductive_success_and_mammalian_evolution).  Two of the living lineages that lie more basally within marsupials, Didelphimorphia and Paucituberculata, both have taxa that have a vertical slit in the pouch. If we consider what the marsupium is for - protecting young that are clinging to their mothers' teats - it's fairly easy to see how an indentation and perhaps then a slit would be selected for, and depending on what each lineage did ecologically, such as climbing, burrowing, or running, the indentation or vertical slit would close either anteriorly or posteriorly and the different pouch morphologies themselves would be selected for. 
",null,0,cdh0xfs,1qvksp,askscience,new,15
null,null,null,1,cdh060v,1qvj38,askscience,new,6
tmatzz,"This is essentially how a condenser works.  If you simply put cold water into a condenser with hot gas/liquid flowing through it the cold water would heat up to the same temperature as the hot liquid.  Not much condensing would happen.
",null,1,cdgynq9,1qvj38,askscience,new,2
PostalPenguin,"&gt;what sort of detrimental affects would I have if I was distilling an alcoholic spirit WITHOUT having the condenser being cooled with the cool water in/out system?

A significant portion of your alcohol would remain in the vapor phase and simply dissipate rather than being condensed into liquid. Once the condenser was at equilibrium with the vapor temperature, no more alcohol would condense and as a result the alcohol would remain as a vapor until it was released into the air. 
",null,0,cdh9m7i,1qvj38,askscience,new,1
Karmic-Chameleon,"Also of note, your water flow should go in at the bottom of a condenser and come out of the top.

My understanding (though I may be wrong) is that as the cold water cools the hot vapour inside the condenser it heats up. Heated water is less dense than the cold water so tends to rise. Put the cold water in at the top, trying to force it downwards, and it has to compete against the in-rushing cold water.",null,2,cdh5kig,1qvj38,askscience,new,1
lazy_smurf,"This is complicated because your question isn't entirely clear. If by ""health"", you mean weight gain/loss, it is very hard to overeat truly healthy food. have you ever tried eating 3000 calories of meat and vegetation? your stomach and satiety hormones simply won't let you. healthier food also tends to impact insulin levels and sensitivity, controlling blood sugar and thus, hunger.

If ""health"" for you goes beyond just weight control, there are many vitamins and minerals that we must eat to maintain proper health, and junk food tends to be extremely low in these, as well as plant-based compounds that are very good for you (think fruit phytochemicals and leafy greens). 

while it is true that the sheer number of calories matters for weight loss, it is clearly easier and healthier to eat higher-quality foods for these calories than lower-quality ones.",null,1,cdh625v,1qvipm,askscience,new,10
pairyhenis,"It's definitely not a dumb question. 

Some things to consider/bear in mind:

- Eating ""healthily"" is a poorly defined concept, and the many items can be both healthy in one regard and unhealthy in another (for example, the association of certain types of fats, omega-3's, with both improved metabolic health and a small increased risk of prostate cancer)

- ""Healthy"" eating is invariably dependent on how much of a particular nutrient you consume; compounds in red wine improve cardiovascular (heart and circulation) health, but drinking 2 bottles a night won't make you healthy. 

- Certain classes of compounds worsen metabolic measures.  For example, a type of fat compound very similar to the omega-3 fats (which are highly beneficial), the omega-6 fats, are linked to increased inflammation, which drives many disease states including obesity, diabetes and cardiovascular disease.  Trans-fat consumption is even more strongly linked to these disease risks.

It's also worth saying that calorie content is not accurate for many foods, due to the method of estimation used.  This often fails to accurately account for inefficiencies in digestion, which can, for example, be altered by the bacteria in the intestines (the microbiome).  The [wiki article](http://en.wikipedia.org/wiki/Atwater_system#Theoretical_and_practical_considerations_relating_to_the_calculation_of_energy_values) discusses some issues.
",null,1,cdh6e7b,1qvipm,askscience,new,7
w0den,"being healthy is not maximized by minimizing weight, thats the first point it guess, also every kind of ""unhealthy"" is only a matter of dosage. For example protein is considered healthy. But too much protein is unhealthy. Sugar is considered unhealthy but without it(or it being split out of more complex carbs) you have no short term energy. The only thing thats is kind of out of that principle is water since it can't be essentially unhealthy when consumed in mass but its unhealthy to consume too little. Also too little diversity in food can lead to not having enough substances to process certain types of food. I can't really describe this any better since english is not my main language.",null,0,cdh6wsm,1qvipm,askscience,new,2
davbob,"Food that is considered bad for you, cakes sweets and soda and the like, are said to contain empty calories. This means that they provide calories with very little other nutrients. Eating nothing but these will lead to malnutrition. Your bidy knows when it is malnourished and makes you hungry so you want to eat more. When you eat more you add more calories. Even if you do keep your calorie count eaten below your calories expended, if you arent getting the correct nutrients from your food the body goes into starvation mode and burns muscle before fat.  It also converts any left overs into fat to lay down a store in cade of emergency.",null,6,cdh1lhb,1qvipm,askscience,new,7
KarlOskar12,"Actually, eating [healthier foods](http://www.mayoclinic.com/health/junk-food-diet/MY01589) isn't obviously better. That study looks at a 10-week period, and all signs pointed to increased health even though he was eating a junk-food diet. The long term consequences aren't known on such a diet because people who eat like that eat far too much of it so they have way too many calories, and consume far too much cholesterol. But it supports the theoretical principal that it doesn't matter where you get your calories from, a calorie is a calorie.",null,2,cdh8dam,1qvipm,askscience,new,1
Spaturno,"Your premise is wrong. It's not just a matter of calories, a lot of other things count: how much bad stuff you ingest, and can't be expelled, and make damages to your body. Bad quality fats and high quality ones are not digested in the same way, despite having the same caloric values, one will hurt you and the others won't.",null,13,cdh4big,1qvipm,askscience,new,3
readams,"Here is an article that discusses the common origins and subsequent independent evolution of multiple kinds of eyes:

http://www.nyas.org/publications/detail.aspx?cid=93b487b2-153a-4630-9fb2-5679a061fff7

There were some very simple common origins of eyes, but they mostly evolved independently.  Notably this tells us that eyes are actually not that hard to evolve.",null,0,cdgyafj,1qvio1,askscience,new,7
angry_squidward,"Some were convergent evolution. One example is the eye of a squid. They presume this because humans and our ancestors have a hole in the back of our eye that causes a blind spot while squids do not, so that leads to the speculation that they were formed differently and independently. Thats about all I remember from class.",null,0,cdgy2bp,1qvio1,askscience,new,2
null,null,null,1,cdgzgt6,1qvfzb,askscience,new,7
dampew,"Well, there are a lot of composite particles that can form bosons.  For instance, Helium 3 also forms a condensate (superfluid).  Superconducting electrons form cooper pairs.  There are also lattice vibrations (phonons), electron oscillations (plasmons), electron-hole pairs, and so on that don't necessarily form condensates but do obey bose statistics.",null,1,cdh168l,1qvfzb,askscience,new,6
Fenring,"I'll add to the other responses by pointing out that, even within the elementary particles, the Higgs boson is not a gauge boson.",null,1,cdh1msx,1qvfzb,askscience,new,6
Aeolitus,"As a general answer: It isnt. 

What you need to consider, is that a boson is composed of fermions (if it is not an elementary particle.) All fermions adhere to the Pauli Exclusion Principle - but if they form a boson together, then they do not need to do so anymore, since the pauli principle is valid only for fermions, and they form a boson now. Thus, there is no Problem with BECs. Dont think of the Pauli Principle as something that prevents fermions from ever coming close to each other. It is a mathematical Rule that describes the behaviour of fermions. Nothing more. As soon as we are not looking at fermions anymore, Pauli's Principle is of no importance anymore, and this is the case in Helium-4.

Another Example is btw. Lithium-7, which we can condensate just fine and is a Boson: 3 Proton + 4 Neutron + 3 Electron, all fermions on their own, form a Boson. 

Source: Physics Grad Student, did my Bachelorthesis on Condensates. 

EDIT: 
As /u/dampew pointed out, we can make condensates of fermions aswell. However, this works by pairing the fermions to something between bosonic molecules and cooper pairs by tuning via the feshbach resonance, and is thus in a way just a condensate of bosonic molecules. If you wish to take a further look into this topic, I recommend looking into the publications of Prof. Ketterle from MIT. Especially the Doctor Thesis of Martin Zwierlein is highly recommended.",null,0,cdh2kqn,1qvfzb,askscience,new,4
wyliee,"Yes.  Yes is the answer to your question technically, however let me start with some definitions to be sure I am answering the question you are really asking.  A gene is the common term for the part of the DNA that gets transcribed through the process of transcription.  What we most commonly think of are protein coding genes.  These are transcribed then translated to make a protein.  However, a gene doesn't have to be protein coding.  There are genomic regions that get transcribed, but don't get translated.  For example, there are many genes that encode entities that function as RNA molecules.  These are transcribed from DNA into RNA, but are not translated. They are still functional, they play a regulatory role as an RNA molecule.  Since they are never translated into a protein, they don't need stop (or a start ) codons.   Most of the ribosome itself, which translates the RNA into protein (where an RNA needs the start and stop codon to be properly processed) is made of these enzymatic RNAs.  So the technical answer to your question is yes, you can have a functioning gene without a stop codon.

If you are asking only about protein coding genes, these do not need to have a stop codon to be transcribed into RNA, but do to be their properly functioning protein.  It is possible for them to be processed by the ribosome. What typically happens is that the ribosome will just keep going along till it hits the next stop codon or runs out of RNA.  This usually results in a nonsensical protein and can cause problems for the cell.
",null,1,cdgxkly,1qvf2d,askscience,new,9
ModernTarantula,". the stop for transcription are the [termination bases](http://www.chemguide.co.uk/organicprops/aminoacids/dna3.html). mRNA are further modified before translation, removing introns. They wrap and fold and are subject to degradation. A very large mRNA would likely be unstable and not usable for the ribosome.  Now lets say teh revers the termination sequence is before any stop codon. the protien would be truncated-- short.",null,1,cdh0ezl,1qvf2d,askscience,new,3
Memeophile,"Transcription termination is usually not far past the stop codon, so stop-codon read through is often not a big deal. Obviously there are exceptions. Amazingly, however, global read-through of specific sets of stop codons are pretty well-tolerated in E. coli, as evidenced by [the famous amber, ochre, and opal suppressor mutants.](http://en.wikipedia.org/wiki/Stop_codon#Amber.2C_ochre.2C_and_opal_nomenclature)

",null,0,cdh649o,1qvf2d,askscience,new,1
ModernTarantula,We have a small muscle attached to the eardrum when it pulls tight it makes the eardrum vibrate less. That gives us its latin name the tensor tympani. It makes the tympanic membrane tense. It has a reflex to tighten with loud sound. It protects your ears from a sharp noise.  However if the loud noise continues it will stay tense long after.  Paralysis of that muscle makes people sensitive to loud noises (hyperacousis),null,14,cdh0iby,1qvdn1,askscience,new,44
maleslp,"Hello! Speech-Language Pathologist here. I believe what you're referring to is called temporary threshold shift (TTS). This is a phenomenon where the blood vessels which supply blood to the little hairs in the ear which, in turn, send a signal to the brain to interpret the energy into sound (they're called cilia) become constricted (vasoconstriction), thus inhibiting the signal and causing the brain to interpret the new ""pattern"" as a ringing. This can last anywhere from a day to a week, the former more common. Caution! Not giving the ears enough time to recover CAN result in PST, or permanent threshold shift (a.k.a. tinnitus). If the cilia don't have enough time to recover, they WILL die and do NOT grow back like normal ""hair"". As far as I know, we're not sure why the lack of energy coming from the cilia is interpreted as a ringing. 

That was a pretty quick explanation; I'm sure an audiologist would be able to provide more technical insight. 

Here's a source link: http://www.sfu.ca/sonic-studio/handbook/Threshold_Shift.html",null,3,cdh3yqz,1qvdn1,askscience,new,27
Anacanthros,"In terms of hearing loss, which may not be precisely what you're asking about:

Inner hair cells in the organ of Corti (the sound-detecting part of the cochlea) are being shorn away.  When sound reaches the cochlea, it moves segments of the basilar and tectorial membranes, which are connected by the kinocilia of inner hair cells.  This movement stretches the hair cells' cilia, and the stretching is what causes electrical signals to be generated and sent to the brain.  When the movement is too severe (a consequence of loud sounds), the cilia can break.  They don't grow back.",null,3,cdh3vj9,1qvdn1,askscience,new,7
null,null,null,7,cdgz10c,1qvdn1,askscience,new,7
therationalpi,"The answer to this may seem straightforward, but it's important to remember the very long history that records had. Over the years there were several ways that we converted the grooves in the record to sound. I'm gathering a lot of information from the [relevant Wikipedia article](http://en.wikipedia.org/wiki/Turntable#Pickup_systems), but I'll supplement it with my own knowledge as appropriate.

Before getting started, it's worth going over what the ""bumbs"" in a vinyl record are. The groove of a record is a time-record of the acoustic pressure for a musical recording. In general, these recordings are built from combining multiple recordings off microphones and mixing them into a song. The job of the recording engineer was thus to record these sound by picking the right mics and placing them in the right places, as well as balancing the levels of the many tracks and applying any effects necessary. An additional job was to be mindful of the limitations of the record itself, since vinyl records didn't have as large of dynamic range as modern media (Though admittedly we don't take advantage of that dynamic range anymore either).

The earliest record players were completely mechanical. A metal (or diamond tipped) stylus  would sit in a groove and follow them up and down as the record turned. In earliest devices, the stylus was directly connected to a stiff diaphragm. The change of surface area from the stylus tip to the diaphragm made this effectively a mechanical-acoustic transformer, changing the mechanical motion of the stylus directly into acoustic waves. The diaphragm would be augmented with an acoustic horn, which would help the waves from the diaphragm couple better to the air. This was, essentially, a pure acoustic transformer meant to match the mechanical impedance of the diaphragm to the acoustic impedance of the air.

Later phonographs used electric transduction. At this point, the tone-arm comes into play. Essentially, the tone-arm is a mechanical lever attached to the stylus. When the stylus moves, the tone arm transfers the stress to a cartridge containing the pickup. The second generation of record players relied on piezoelectric materials for pickups: materials that respond to mechanical stress by generating an electric potential (voltage). This voltage could then be amplified and sent to a regular loudspeaker. The first ones used free floating piezoelectric crystals (like a sand), but had very poor quality because of non-linearity of the transduction. Later loadspeakers used piezoelectrics suspended in a ceramic material, which performed much better.

The next generation of pickups are very similar to the standard moving-coil loudspeaker, but in reverse. The tone are moves a magnet in a magnetic field, which generates an electric current that can be amplified. Alternatively, the tonearm moves the coil around a magnet, to the safe effect. The only difference here is wether the magnet or coil weighs more, which essentially depends on how many wraps of wire the coil is made out of. If you are curious about this type of pickup, just look up loudspeakers to get a general idea.

Between magnetic and piezoelectric charges, you've probably got most of the record players that people used. Some later models used strain gauges, where an electric resistance is altered by the strain put on the device. This resistance could be placed in a network of resistors with a constant current (like a Wheatstone Bridge), and the differential current could be used to measure sound.

More interesting are the optical methods that developed late in the history of records. These are called ""no-touch"" sensors, and they are still the preferred method for anyone who doesn't want to risk damaging their rare records, like collectors or archives. A classic no-touch sensor uses a laser reflecting off the grooves to measure their depth. These are used in place of a standard stylus on a tone-arm, and the record player looks quite similar to a standard record player. Alternatively, with enough resolution, you can simply scan the record player like you would a document. The grooves are then read directly from the image and converted to sound. This is the preferred method for archives, like the Library of Congress, because the sound of the recording can still be enjoyed without risking the physical record itself getting damaged. It does, however, require very high quality scans to be effective.

Please feel free to ask any other follow-up questions, since most of this info was just the basic stuff from Wikipedia!",null,2,cdh512s,1qvamu,askscience,new,5
tea-earlgray-hot,"When discussing porous materials surface areas, we normally speak of gas-accessible areas. This is the ""real"" surface area, that stuff can actually get to and interact with.

More formally, we measure the amount of gas adsorbing to surface of the sample, then back-calculate the area using the size of the gas molecule and the mass/volume of the material. This is normally measured as an isotherm, and modeled with Langmuir or BET theory.

Edit: It's probably relevant to mention that if your surface is electrochemically active, your can use this to determine surface area instead. It's more practical for very small samples (&lt;25mg), but is somewhat poorly defined.",null,0,cdh2g06,1qvacl,askscience,new,3
iorgfeflkd,"I don't know if it's the case for this specific device, but capacitors often have many parallel layers of material inside them. If you add up the surface area of each layer, it is much greater than the cross sectional area of the device. For example, if there are 10,000 sheets of graphene each of which are a square centimeter (just making these numbers up, don't know the exact details), it will have a surface area of a square meter.",null,1,cdgy707,1qvacl,askscience,new,2
dampew,"In terms of ""how is this possible"", remember that there are mathematical objects like fractals with infinite surface area and finite volume -- if the surface is complex enough, the surface area can be huge.

In this case, the surfaces are bunches of graphene sheets.  The basic idea, I think, is that they take graphite or thick graphene and oxidize it -- the lattice is filled with oxygen everywhere.  When they remove the oxygen the whole thing flakes apart and forms pores at the nanoscale.",null,1,cdh1fvh,1qvacl,askscience,new,1
iorgfeflkd,"That terminology is generally used to refer to nuclear reactions like in bombs and power plants. Basically, a large nucleus, like uranium, breaks apart into several smaller nuclei and neutrons, all of which are moving very fast. This can either happen spontaneously or can be triggered, like when Uranium is hit with a neutron. [Here](https://en.wikipedia.org/wiki/File:Nuclear_fission.svg) is a schematic.",null,0,cdgvu0k,1qva0j,askscience,new,3
TheMac394,"To go into a tad more detail than others here, let's step back and look at how atoms are put together.

The nucleus of an atom - what we're concerned with - is made of protons and neutrons. Now protons are positively charged; as you might now, positive things repel other positive things. If you get a bunch of protons and neutrons together in a nucleus, they'll all repel each other - this is the electromagnetic force; if this was the end of the story, well, we wouldn't have any atoms. Luckily, there's something called the *strong nuclear force*. The strong force is, as the name suggests, very strong, but acts only over a very short distance. If you get protons close enough together, though, the strong force will become enough to hold them all together.

Having a nucleus, then, is a matter of balancing out the strong force and the electromagnetic force: if the two are equal, you'll be stable. To make some nuclei more stable, you can also add neutrons. Neutrons also feel the strong force, but don't feel the electromagnetic force, so they generally act to increase the force holding things together.

Now, if an atom doesn't have the right ratio of protons to neutrons, it won't be stable. In this case, it will be radioactive, and will decay, emitting some radiation in the process to get rid of energy and become more stable.

What if you're right on the brink of stability, though? You've got the right ratio to hold the nucleus together, but any small change would throw that ratio off. In this case, you have the possibility for *fission* - splitting the nucleus.

Fission works like this: a borderline-stable nucleus will get hit with a neutron and absorb that neutron. The neutron's energy causes the nucleus to bulge out, like the second picture [here](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Stdef2.png/150px-Stdef2.png). Now, remember that the strong force - the one holding things together - acts over a very short distance. When the nucleus bulges out, the strong force can't reach all the way out to the edges of the bulge to pull them in; since our nucleus was very close to unstable to begin with, this turns out to be the final push it needed. The new nucleus can't hold itself together, and will basically break in half, as depicted in the picture above.

It's worth noting that this can - theoretically - happen to pretty much any nucleus; it's just a matter of getting enough energy in your neutron to cause fission. Practically, however, there only a few nuclides for which this is possible; specifically, we usually deal with Uranium and Plutonium.

Now, in the case of Uranium and Plutonium, since they're heavy and not super-stable, they release energy when they break apart; the theory behind this is complicated, just know that ""stable"" is more-or-less equivalent to ""low-energy"", and if a nucleus becomes more stable, that energy has to go somewhere. This is where the gamma radiation that we associate with nuclear reactions comes from - it's essentially pure energy being carried away from the nucleus as it splits.

Fission also produces a few free neutrons, which don't become a part of either of the halves of the splitting atom. Remember that fission was initially caused by a neutron hitting our atom - if we've got a bunch of fissionable atoms, and each fission produces a neutron, then those neutrons can go on to cause other fissions, causing a chain reaction. This is how nuclear reactors and nuclear bombs work: you cause a lot of fissions over a very short time, which releases a *lot* of energy. That's why most people associate ""splitting an atom"" with explosions; in reality, splitting only a single atom will produce some energy and radiation, but won't immediately result in a massive mushroom cloud.",null,1,cdhbzh0,1qva0j,askscience,new,4
wesTRONcorp,"I like all of the comments so far.  Simply though and stepping back to ""what is an explosion?""... a rapid increase in volume and energy.  This brings us to why nuclear fission is so energetic.  When an atom splits, what was previously held ever-so-tightly together--the atoms nucleus--by the strong force is now suddenly released.  The nucleus must now expand immensely and take up more volume than a tightly bound nucleus.  Furthermore, all those protons/neutrons are able move independently with more possible motion when broken apart.  Also, the energy (strong force) holding the nucleus together must be released....etc...",null,0,cdh1utd,1qva0j,askscience,new,2
datums,"Slightly besides the point, but not all IMAX cameras use film. There have been digital IMAX cameras for about 5 years, and newer digital IMAX cameras can shoot in 4k resolution.    
  
As for how digital effects are added to analog film - the film is scanned, frame by frame, into the digital domain. Once in the digital domain, all the computer effects are added. Then a digital print of the film is produced. This is analogous to scanning a picture of an old family picture, photoshopping an alligator in, and then printing it on your inkjet printer.  ",null,1,cdh8e9b,1qv9oz,askscience,new,5
D_I_S_D,"You seem to be under the impression that a cinema has separate projectors for the movie and for the CGI, this is not the case.

Normally CGI effects are added to film in the post production phase of the movie, after the physical shot has been taken. The script may call for ""Panning shot with giant robot"" so the director will film the ""Panning shot"" and the ""giant robot"" will be added later during the editing phase essentially ""frame by frame"" using a digital image manipulation program.

Once the final cut of the film is decided upon the cut is then printed onto film to be distributed to the cinemas. The production houses can produce films compatible with the cinema's projector, so for an IMAX movie they just make an IMAX compatible print.

Due to their cost/size IMAX cameras are not used too often, even ""Dark Knight Rises"" is only approximately 1/3 genuine IMAX shot footage, the rest being regular footage that is later re-cut and printed to fit the IMAX format for the IMAX cut of the film.",null,3,cdh1s1d,1qv9oz,askscience,new,6
iorgfeflkd,"They also rotate, and the rotation causes them to bulge out at the equator. That's why large bodies are oblate spheroids.

Objects smaller than about 200 km in radius don't have enough mass for gravity to overcome the inherent rigidity of the materials, so they stay aspherical.",null,0,cdgy59u,1qv88y,askscience,new,17
FewRevelations,"Gravity from the planet's star only pulls on one face of the planet at a time, which, combined with an elliptical (rather than round) orbit, makes the planet more oblong (in more extreme gravitational conditions this can cause tidal heating, like on Io).",null,5,cdgyh7c,1qv88y,askscience,new,4
datums,"The consensus seems to be that prosody, the melody of the word, is more important to a dog than the complex series of sounds that actually make up the word. It follows from that that the language is not particularly important. What matters is which words are chosen, and how they are pronounced. People tend to talk to their dogs in a particular tone, with exaggerated emphasis, and that it what they are able to recognize. ",null,0,cdh9fqs,1qv865,askscience,new,2
quantum_lotus,"To add to what others have said here, plastids (mitochondria and chloroplasts) once had complete genomes, but over time most of the genes that started out in the plastids have been moved into the nuclear genome.  For a sense of scale, mitochondria have about 1,000 proteins (gene products) working inside of them, but human mitochondria only encode 13 genes in their genomes.",null,0,cdh5yfb,1qv58e,askscience,new,6
DutchmanIII,"No, that wouldn't work. As you say, it is theorized that mitochondria and chloroblasts were individual organisms and that they formed a symbiosis with other cells. The fact that they still contain fragments of their own DNA supports this theory.

Through evolution, these organisms have come to specialize in a certain task to their host cell (e.g. production of ATP) and have given up other functions essential to survival.",null,0,cdh2pj2,1qv58e,askscience,new,4
sporclesam,"The [endosymbiotic theory] (http://evolution.berkeley.edu/evolibrary/article/history_24) states this. As DutchmanIII mentioned, these organelles have given up their extraneous functions and evolved into single-track organelles, yet hang on to bacterial ancestry such as its genome &amp; ribosomal machinery. As an endosymbiont evolved into an organelle, most of its genome [transferred to the host cell] (http://www.ncbi.nlm.nih.gov/pubmed/18430636).",null,0,cdh56on,1qv58e,askscience,new,3
wazoheat,"Clouds are made of very small particles of water and/or ice, typically around [20 micrometers (0.00079 inches) across](http://apollo.lsc.vsc.edu/classes/met130/notes/chapter7/ccn_drop_prec.html). At those particle sizes, light doesn't interact the same as it does with a single, uniform object. Light traveling through clouds undergoes a process known as [Mie scattering](http://hyperphysics.phy-astr.gsu.edu/hbase/atmos/blusky.html#c3). Light is scattered in all directions by this process, though it is primarily *forward-scattered*, meaning the light generally continues in the same direction it came from, though at an altered angle. Each photon of light reaching your eyes through clouds has likely been ""bounced around"" several times before it reached your eyes, which is why you can see light coming through the clouds but you can't see what is behind the clouds.

The thicker (in depth) a cloud is, the more the light is scattered before it can make it through. This is why ""thin"" clouds seem quite wispy and translucent, while thick clouds appear dark from below.",null,1,cdh0adf,1qv0zy,askscience,new,4
thewetness,"From a mechanical engineer's perspective, this statement isn't totally true.   It assumes that the instrument is right at capacity, meaning that it is right about to break due to the tension. Most things have what we call a factor of safety incorporated into their design. The factor of safety is basically the ratio of how strong it actually is (the maximum load it could survive) divided by how strong it needs to be (the maximum load it will actually experience in its use). So when it is at capacity, it has a factor of safety of 1. So if it experiences any load higher than intended, it will fail. However, when you have a factor of safety greater than 1, say 2, then increasing the load (tension) by 25% will not cause it to break.

Also, it says every part of the instrument needs to be stronger, it ignores the possibility that one part of the instrument is ""weaker"" than the rest. To get an idea of this, consider a tensile testing ""dogbone"" specimen: http://www.faimaterialstesting.com/site/images/physical/comp_tens/dogbone_sample_caliper_2.jpg

Basically, the ends are much thicker than the middle so we know where the specimen should break when we pull on the ends. The instrument could have a section that is the weakest link, and even that section would likely have a factor of safety.

From a general engineering standpoint, if something is going to experience 25% more load, then if we want to keep the same factor of safety, then we should increase strength by 25%, but usually, we have the factor of safety because we simply can't be sure what the maximum load is going to be. So we don't always have to worry about making things 25% stronger for most applications. High performance products like those in the aerospace industry have low factors of safety, near 1, because higher factors of safety mean higher weight. For musical instruments, worrying about weight isn't our primary concern, so I would expect to see this logic in instrument design.",null,0,cdh029z,1qv0py,askscience,new,4
datums,"The problem here is that Double basses do not have adjustable truss rods.   
  
With a normal guitar or electric bass, the neck is rather flexible. The tension of the strings want to make the neck to bend forward, becoming concave, while the neck wants to bend backward, becoming convex. When properly adjusting such an instrument, one must balance these forces, using a *truss rod*. The truss rod runs through the neck and allows the curvature to be finely adjusted. Properly adjusted, the neck is slightly concave, rather than straight. This gives the strings room to vibrate without hitting the fretboard.   
  
If the truss rod is adjusted improperly, two things can happen. If it is *too* concave, the strings will be high off of the fretboard, making the instrument difficult to play, though it may sound great. If the neck is too flat, or even convex, the strings will strike the fretboard when plucked, resulting in a horrible sounding and unplayable instrument.   
  
As I said above, the problem with a double bass is that they do not have adjustable truss rods. The instrument has been made to have a certain curvature given a certain tension, and this cannot be altered. Though damage may not occur, the playability of the instrument will be compromised if string tension is significantly altered.    
  
Having read your post again, I see that I am hopelessly off topic. ",null,0,cdh97er,1qv0py,askscience,new,5
do_od,"There are many factors you have to consider when taking digital cameras into space. First, you have the radiation environment that can be damaging to electronics. There is also a phenomenon called outgassing that can occur in the vaccuum of space. What happens is that gasses, solvents and moisture evaporate from materials, in particular from electronics, adhesives and such, which can form [deposits on the optics of your camera](http://en.wikipedia.org/wiki/Outgassing). Most batteries, and in particular the commonplace LiIon type, have bad performance in low temperatures and that can also be a problem in space. 

Cameras intended for space duty are built with special [radiation-hardened](http://en.wikipedia.org/wiki/Radiation_hardening) circuits and even metal radiation shielding of critical parts. Outgassing can be mitigated by pre-treating components by heating them under low pressure, ""boiling off"" any solvents and moisture trapped in the material. To get reliable operation from the batteries it might be necessary to have special equipment to keep the batteries at acceptable temperatures, or use other power sources.

That being said, a standard camera might work to snap a few selfies on the moon, but it is also possible that it wouldn't even survive for that long. Even if you can take a few pictures, the quality may be reduced due to condensed moisture on the optics and data corruption from radiation damage. Reliable operation is very improbable because the same technology that make electronics so cheap also make them less radiation resistant. 

A film camera on the other hand will usually work. NASA used medium format [Hasselblad cameras](http://en.wikipedia.org/wiki/Hasselblad#Hasselblad_cameras_in_space) for the Apollo missions and they performed very well. Eliminating the electronic parts also eliminates most of the problems with taking pictures in space. 

e: Added some wikipedia references.",null,0,cdh14ai,1quzsj,askscience,new,9
aussiekinga,"I can't see there being any problem. Digital cameras work by taking not of where light hits on an electronic plate. As light still travels in space, being a wave, it is still going to hit on the plate in the same manner. The camera would then experience the same thing.

A film camera, I imagine would be similar. It has chemicals that react when light fits the film. Again, as the nature of light isn't different the way it interacts with the film wouldnt be different. 

I guess exposure of the film or the camera itself to vacuum could cause an issue, but a clear air tight case (like for underwater photos) should be enough to fix that.

The only way it would not work is if the nature of light is different. While it can't be effected by physical objects (refractions etc.) or gravity I'm not sure that there would be any major different in space. I guess you would guess less solar flares on the lens?",null,1,cdh0wgr,1quzsj,askscience,new,1
Owl_,"Higgs bosons don't maintain the Higgs field, nor do photons maintain the electromagnetic field, etc. These particles are excitations of their respective fields. The fields exist whether there's a particle there or not. 

Not really. We have a pretty solid idea of what the Higgs boson is like. We could, of course, be wrong about something or have yet to discover something new about it, but that's unlikely. 

The Higgs field is an all-permeating field. That description was redundant, though; a field, by definition, exists in all space and time. There's no escaping it. ",null,2,cdgzde9,1quzsd,askscience,new,10
quarked,"Be careful when you say the Higgs boson ""maintains"" the Higgs field. This isn't really a good description of what the Higgs is or does.

Like others have mentioned, the Higgs boson is just an *excitation* of the Higgs field. Similarly, there are other gauge bosons which mediate their respective forces. However, and this is the important part, particles do not need to interact with Higgs boson to interact with the Higgs *field itself*.

In the standard model, all the fundamental forces are *mediated* by the gauge bosons which ""carry"" their respective force, but that is different from the Higgs interaction, which gives most particles mass because the Higgs field has a non-zero expectation value. ",null,1,cdh0x0p,1quzsd,askscience,new,7
LazinCajun,"To add to owl's comment, The Higgs boson is unstable.  It decays *very* quickly compared to human detectable timescales.

Broadly speaking, the LHC deduces the existence of the Higgs (and many other unstable particles) by examining the decay products very carefully over a large number of collisions.",null,1,cdgzisl,1quzsd,askscience,new,5
MCMXCII,"The Higgs boson is an excitation of the Higgs field just like the photon is an excitation of the EM field. But unlike the massless photon, the Higgs boson is *very* heavy, which means its a lot harder to make than a photon. That's why we need very energetic collisions in the LHC to find Higgs bosons.",null,0,cdh3wom,1quzsd,askscience,new,2
redappless,"In a recent study they had suggested that it was more probable that carnivores had originally consisted of a omnivorous diet. Shifting from a carnivore to a herbivore over time seemed impossible due to the dramatic structural changes a species had to take over time. http://www.sciencedaily.com/releases/2012/04/120416154417.htm

Survival of the fittest due to environmental changes and competition would definitely have a role in the diet of an animal. As carnivores originally had an omnivorous diet, the type of food they had more access to (less predators, more abundance) probably influenced overtime the adaption of their digestive system to that type of diet. As they had an omnivorous digestive system, adapting and developing specialised anatomical and physiological characteristics to either a carnivore or herbivore overtime wouldn't be so hard.",null,0,cdgrok2,1qusb3,askscience,new,8
Platypuskeeper,"Well, if you start with the [RNA world hypothesis](http://en.wikipedia.org/wiki/RNA_world_hypothesis), you only had RNA in the beginning. Note that DNA is synthesized from RNA, more specifically Ribonucleotide Reductase turns ribonucleotides into deoxyribonucleotides, which is not an very easy chemical reaction.

So why not just continue to use RNA for everything? Well one of the key differences here is that DNA is more stable, especially if the chains are long. So it makes sense that it'd be worth the energetically-expensive of producing DNA in return for this increased stability, and increased ability to form more complex life forms. 

So you might flip the question around here: Using DNA for storing the genome makes sense, then. But is it really necessary for transcription and all that? Since they use shorter strands (and the empirical fact this doesn't happen), it seems there wasn't any benefit in using DNA for more than what it's used for. 

Those are just the broads strokes though, I'm sure a biochemist can fill in more details.
",null,3,cdgqpyq,1qunlm,askscience,new,6
KarlOskar12,"You have a lot of great questions here so I'll try to answer a couple them as best I can. My organic chemistry is pretty terrible so I'll try to avoid it as much as possible. DNA is an extremely stable molecule which makes it perfect for storing genetic information. mRNA is a very good messenger because it is degradable. This is optimal because not all proteins need to be made all the time. The longer the mRNA is around, the more protein will be made. If it were mDNA being made then cancer would be much more prevalent as a result.",null,2,cdgqtcp,1qunlm,askscience,new,3
KarlOskar12,"Depending on the textbook or other source you look at the number varies typically between 28 and 32 ATP. The low 28 takes into account the amount of energy required to move the NADH from the CAC into the mitochondria. Some also attribute varying amounts of ATP production to the proton gradient during the ETC. Nothing else is happening to make it produce less ATP, some people just crunch the numbers differently.",null,0,cdgqipx,1qumwp,askscience,new,2
Osymandius,"Fairly crucial point:

3 ATP synthesised per rotation of ATP synthase. Number of protons required to rotate ATP synthase through 360o depends on the number of c subnunits (varies between 8 and 12 from organism to organism). This means in 1 organism if you've moved the same number of protons you might not get the same number of ATP as another organism with **exactly** the same number of protons in the intermembrane space.",null,0,cdh2kol,1qumwp,askscience,new,2
codyish,Mitochondria can vary in efficiency by losing (leaking) electrons that travel through the electron transport chain or by having protons leak through the membrane. I know that there are several factors that can cause this but the one I've seen studied the most is variation in membrane composition. ,null,0,cdh7bip,1qumwp,askscience,new,2
bellcrank,"While there is some minor justification for believing that urbanization can cause storm-splitting, the more likely scenario is that you are simply paying attention to Chicago, so you notice every time it doesn't get hit.  If you paid close attention to a particular (arbitrary) location on a dart-board, you'd notice it never seems to get hit.  It's just an observation bias.",null,9,cdgn3jk,1quhk6,askscience,new,44
ACuteMonkeysUncle,"http://www.isws.illinois.edu/atmos/statecli/tornado/NewMaps/IL-Tornadoes-13.png

Here's a map showing every tornado in Illinois from 1950 to 2010. As you can see, Cook County and the city seem a bit ""underrepresented"" tornado wise. There are others as well, though, such as DeKalb county don't have too many tornadoes either. 

So, part of it is confirmation bias on your part. But still, it seems that just to the south in Will County there are a whole bunch of tornadoes, including that pink one, which must be the Plainfield tornado of 1990. That, I cannot explain. It might just be a fluke, it might not. I will defer to the expertise of others, here. ",null,1,cdgq8xv,1quhk6,askscience,new,13
snusmumrikan,"Venemous =/= poisonous. 

Venom is injected, whereas poison can be ingested/absorbed etc. So by eating the animal, the honey badger swallows the venom sacs or whatever the venom is in and it is processed, diluted, broken down or excreted like all other food. 

The venom of a snake would be dangerous to it if it injected it into the honey badger's blood stream (this actually happens, but the honey badgers are also really tough and usually pass out, sleep and then wake up to eat the snake afterwards!).",null,2,cdgnfus,1qufwl,askscience,new,19
Javi2639,"Hybrid orbitals don't actually exist. At the time of their postulation, it was the best theory to explain what was going on. The current accepted theory is called molecular orbital theory, which is much more complicated, but explains what we observe a little closer than hybridization theory. However, since hybrid orbitals are easy to understand and explain, we use them for most purposes. It's like with Newton's and Einstein's theories of gravity. Technically, Newton's equations are incorrect, and Einstein's are closer to what we observe, but since Newton's are so close, we use them for most practical purposes and only use Einstein's when we have to.",null,4,cdgmb7j,1qufpd,askscience,new,11
bohr_exciton,"The orbitals you are describing are called hydrogen-like orbitals. These orbitals are only the exact solution to the simplest system to which they can be applied- a centrosymmetric system of two oppositely charged particles, such as the hydrogen atom. Even in similar system, such as heavier atoms, these orbitals are no longer exact because of interactions between multiple electrons. In molecules  there are even more complications from the presence of additional nuclei in addition to the extra atoms.

However, even in these more complicated systems, we still use the language of hydrogen-like (or atomic) orbitals. The reason for this is twofold, 1) this practice can greatly help our intuition and 2) these orbitals are a convenient starting point for calculations. In regards to the first point, the case of the ""hybrid"" orbitals is a good illustration of the convenience of using the hydrogen-like orbitals. It is fairly straightforward to think of an sp2 center, say in a carbon double bond, as consisting of a mixture of one s and 2 p orbitals. While this description is definitely not exact, it nonetheless give quite a bit of intuition regarding the resulting physical and chemical properties of that center, from its geometry to its reactivity. 

As for the second point, it turns out that in many systems, the atomic orbitals turn out to be very good building blocks (more technically a good basis set) for describing the electronic structure of more complicated systems. The idea here is similar to hybridization, but more general and systematic. For example the electronic density at some point in space could be described as 0.5C_2pz + 0.2O_2s +..., in this case showing that it's a superposition of electronic density of a carbon 2pz orbital and an oxygen 2s orbital and so on weighted by certain numerical coefficients. This picture is especially convenient when atoms interact rather weakly with one another, however it is always valid. In principle one can create a complete basis set out of specific combinations of these atomic orbitals, and could thus exactly describe any physical system.",null,0,cdgnebw,1qufpd,askscience,new,9
defrandymoss,"Hybrid orbitals are useful for explaining bond geometries, but to get geometries of these we must know the shapes of the original atomic orbitals.
A more current and accurate view of bonding interactions is molecular orbital theory, where a linear combination of atomic orbitals gives the properties (including geometry) of the bonding molecular orbitals.
Both of these theories are built on first knowing the properties of the atomic orbitals, which you get from solving Schrodinger's equation in spherical coordinates. The atomic orbitals are the more fundamental entities, so we must ""go through"" them before moving on to the more complicated bonding interactions between atoms",null,0,cdgmuos,1qufpd,askscience,new,5
endocytosis,"Yes.  You can see them with a light microscope but need to use high magnification.  A video is [here](http://www.youtube.com/watch?v=vvnEsOaKxuw).
As far as fertility goes, there's a general formula for sperm motility (%motile/%nonmotile)/field, but I don't know it off the top of my head. Other factors like Calcium and other ion content of semen, and DNA integrity of the sperm are [important](http://humrep.oxfordjournals.org/content/28/1/274.short) indicators, not just motility.

EDIT: Would not recommend trying to ejaculate into a slide and viewing under a microscope.  See a doctor if you're worried or curious about your health.",null,0,cdglku2,1quet2,askscience,new,17
Eldritter,"You don't need to ejaculate onto a slide.  You need about 1 microliter of semen on a slide, probably along with a very small drop of water or saline to observe the motile cells via microscopy.  

For context this means that you could just touch your finger to the slide w/ semen on it and it would have thousands of sperm.  You can observe the cells for sure at 400-600x mag quite well.  Probably at 100x you could see spots moving.",null,0,cdgpmoa,1quet2,askscience,new,5
prooveit1701,"Depends on the power of the microscope but probably yes. You can predict fertility based on the mobility of the sperm cells however, other factors effect the quality of sperm that are not necessarily visible - such as genetic defects etc",null,1,cdglex8,1quet2,askscience,new,5
swollennode,"Yes, actually. This is how they do male fertility test. They ask the men to masturbate into a cup, then a lab tech take a drop of the semen, put it on a slide, then cover it with a cover slip, put it under a microscope and examine for motility. They also look for sperm morphology. Like is the head normal? Is the tail normal? is it swimming normally?",null,0,cdh2qto,1quet2,askscience,new,1
7LeagueBoots,"Having two nostrils may be more a product of the evolutionary parsimony of the bi-laterally symmetrical body-plan than of any advantage of two over one.  With a very few exceptions we (and other animals) exhibit a bi-lateral body-plan because it is easier to code for in an evolutionary sense.

The nostrils are close enough together that it is unlikely that smell directionality is a factor for most species, especially considering the way scents diffuse and how slowly they spread, unlike sounds which maintain their directional nature due to the speed at which they travel.",null,1,cdgr71l,1qubtr,askscience,new,8
ragingclit,"Having two nostrils allows animals to assign directionality to perceived smells. Rather than just detecting the presence of a smell, animals can identify which nostril the smell is stronger in, and by how much, and use this to determine the direction that smells are coming from. The same basic thing applies to ears and sounds.",null,7,cdgmyo7,1qubtr,askscience,new,8
ShazbotSimulator2012,"Each nostril functions differently. At any time one nostril tends to be narrower than the other due to increased blood flow, making it function better for detecting certain odorants that are aborbed more slowly, that would pass through the high-flow nostril too quickly. Interestingly, which nostril has more airflow tends to change throughout the day.

http://news.stanford.edu/pr/99/991103smell.html

[and a video from Vsauce that summarizes the above study](http://www.youtube.com/watch?feature=player_detailpage&amp;v=eiAx2kqmUpQ)",null,0,cdh0lvu,1qubtr,askscience,new,2
loctopode,"Two nostrils have a larger surface area than one. The function of nasal hair has been suggested to prevent particles entering and so a larger surface area would be more efficient.
Some people suggest that the two nostrils are better at detecting different odours, or that it helps with detecting odour direction.",null,0,cdiedwy,1qubtr,askscience,new,2
bellcrank,"A hypothetically gigantic raindrop would experience stress during descent that would split it into smaller drops.  There's a tendency for drops to be large enough to fall, but not so large that they are forced to split, narrowing their size to a specific range.",null,2,cdgn57s,1qubr8,askscience,new,23
null,null,null,3,cdgly7l,1qubr8,askscience,new,4
dbaker102194,"The polar forces of water are what hold water together and give it surface tension and all that observable stuff. But eventually the mass becomes too great, and there are too many air molecules buffeting it, for the hydrophillic force of water to hold itself together. Water droplets appear to be around the optimum size for water to travel through the air at the air density near ground. ",null,2,cdgndqt,1qubr8,askscience,new,3
Saelyre,"Because when they reach a certain size they can't stay suspended in the air and start falling.

Source: http://ga.water.usgs.gov/edu/raindropsizes.html",null,4,cdglgm4,1qubr8,askscience,new,4
KarlOskar12,"That depends entirely on what is eaten. A candy bar, a milk shake, a protein bar, etc. Normal [insulin](http://www.healthcentral.com/ency/408/003701res.html?ic=506048) levels are between 0.5 and 2.0 ng/mL.",null,0,cdgkv5j,1quag2,askscience,new,3
eekabomb,"the amount of insulin released post-meal depends on what you've eaten. a meal high in carbs will illicit a higher amount of insulin to deal with it.


it really depends on the person, their weight/gender/other factors. everybody will have their own basal rate of insulin production in addition to producing insulin in response to meals. normal insulin levels aren't all that useful without context from other lab values like fasting blood glucose or hemoglobin a1c...so I am not surprised that every source you have found has talked about both together.


relevant to your question, in the case of insulin dependent diabetics, we sometimes calculate an insulin to carb ratio and use that to estimate how much insulin to give a patient post-meal. in some people one unit of insulin may cover 10g of carbs, in others it may cover 20g...so if a diabetic has a meal worth 10g and their particular ratio is 10g/u, they'll need to use 1u of insulin after the meal, if that person had a ratio of 20g/1u they'd only need to have 0.5u   
",null,0,cdh0avc,1quag2,askscience,new,1
snusmumrikan,"It depends what you mean by stable. 

A folded protein is stable, and therefore the more correctly folded a protein is, the more stable it is. However this is not applicable to enzymes which often require conformational shifts in order to carry out their role; for example disordered peptide loops are often ordered when substrate binds or vice versa. Therefore from a biological standpoint, that enzyme, whilst not tightly bound with hydrogen cross links at every possible position, is in it's 'stable' and active form.

Although from an entropic viewpoint protein folding is generally favourable, and equilibrium of unfolding exists in which some of the protein will be folded and some unfolded depending upon environmental variables. 

There's a simple equation for this applied small molecules which can also be used for very small protein domains which undergo a one-step folding process. However it is a bit tricky to calculate it for larger proteins as they go through pre-defined intermediate folded states, which were proposed in response to the Levinthal paradox (for a protein to fold entirely correctly at random it would take an impossibly long time to get it right, whereas in reality it is very quick).

So proteins are supposed to be stable in living cells, although this stability includes a stable level of folded/unfolded, a stable level of expression in response to the needs of the cell coupled to a stable level of degredation, and a structure which is active in its defined role - even if this requires domains or sections to be disordered.

",null,0,cdgkwjb,1qu9z2,askscience,new,2
KarlOskar12,"That depends entirely on what you mean by [metastable](http://www.thefreedictionary.com/metastable). Many of the proteins in the body only function in a relatively narrow range of conditions (temperature and pH) and will degrade if they are subjected to conditions far outside their optimal levels. If we take the more physics-related definition then I think the [sodium-potassium pump](http://puu.sh/5loy4.jpg) is a good example (sorry the source is from a powerpoint on my computer, the original source is from a textbook that I don't have). The pump is a protein and it is phosphorylated/de-phosphorylated constantly changing its conformation and changing it from an unstable state to a more stable one and vice versa.",null,0,cdgl37w,1qu9z2,askscience,new,2
Mxlexrd,"Yes, if the Earth were to stop rotating suddenly, people would fly forwards at 1000 mph at the equator and slower speeds at higher latitudes.


Yes, gravity would seem slightly stronger due to lack of centrifugal force, but the difference is only a small amount. You can calculate the total centrifugal force experienced at the equator using a = v^2 /r, which gives an acceleration of about 0.03 m/s^2 which is less than 1% of the acceleration due to gravity.


So, we do experience the centrifugal force, it is just quite small.",null,4,cdgm5uu,1qu92o,askscience,new,17
Mazetron,"It's not that they would ""go faster or slower"", the speed is controlled by the driver.  The car would be able to accelerate faster, though.  That means if you were to slam on your brakes, the car would come to a stop more quickly.  Also, you would be able to slam on the gas harder before your tires start slipping.",null,0,cdgth0x,1qu8j5,askscience,new,4
Jobediah,"Respiration in water is more energetically costly that breathing air for two main reasons. First, water has way less oxygen in it than a similar volume of air. This means that an organism has to move more fluids around. Second, water is heavy and viscous (or sticky). Therefore, it requires more energy to move a given volume of water than air. Water must be sucked or pumped through gills which typically have tiny slits and the resistance is very high. So aquatic organisms must allocate a larger percentage of their overall energy budget to respiration than terrestrial organisms.",null,0,cdgth91,1qu7b1,askscience,new,4
Osymandius,"So by and large the majority of ""substances"" which break down other molecules are enzymes. Enzymes are complex proteins that are very difficult to ""design"". 

Lifted from wiki, the active site of ethanol dehydrogenase:

The active site consists of a zinc atom, His-67, Cys-174, Cys-46, Ser-48, His-51, Ile-269, Val-292, Ala-317, and Phe-319. 

The zinc coordinates the substrate (alcohol). The zinc is coordinated by Cys-46, Cys-174, and His-67. 

Phe-319, Ala-317, His-51, Ile-269 and Val-292 stabilize NAD+ by forming hydrogen bonds. 

His-51 and Ile-269 form hydrogen bonds with the alcohols on nicotinamide ribose. 

Phe-319, Ala-317 and Val-292 form hydrogen bonds with the amide on NAD+.

So - we have a coordinated metal ion centre which acts as a redox centre. We have 9 amino acids which are by no mean consecutive; they're scattered across the protein structure and only brought together in 3D space by the complex tertiary folding procedure. This is very difficult to predict for cytosolic proteins. We also need to create a site which accepts both ethanol and our electron donor (NADH) in close enough proximity for one to attack the other. Once (hypothetically!) we've managed to do all of this, we need to add control mechanisms to integrate this new enzyme into metabolic pathways. It needs to be turned on and off as required and it needs to be transcribed and translated as required. This means we need to integrate its stimulus into a receptor's function and subsequent signalling pathways.

I've skipped over a great many steps, but this should give you a rough idea of how complex our systems are and how difficult tinkering with them is. If you're interested - have a look at the bioengineering attempts at modifying Rubisco, a key enzyme in CO2 fixation in plants. It's a very slow enzyme and the idea is if we speed it up/decrease the back reaction we could dramatically increase plant yield and solve the food shortage we will shortly be facing.

If you'd like to know more, reply and I'll see what I can do.",null,3,cdgki64,1qu7az,askscience,new,8
Quantumfizzix,"It didn't start out that way, Pangea was just the last supercontinent that formed. The continents on the Earth roughly follow something called the supercontinent cycle, gradually changing from continents, to a supercontinent, back, and forth, over and over again. There were many supercontinents before Pangea, and there will be conceivably more in the future.",null,4,cdgm2kz,1qu6b0,askscience,new,54
homininet,"So, the world didn't quite start out as one big land mass. The supercontinent Pangea actually existed during the Permian period (around 275 million years ago). But of course the world is much older than this, and the continents were moving around before this, but the farther into the past you try to reconstruct continental positions, the harder it gets. [Here](http://www.scotese.com/earth.htm) is great website that has beautiful maps of the positions of the continents back to about 650 million years ago. And if you dig around a little, you can find proposed continental positions even earlier than this. ",null,5,cdgm5kl,1qu6b0,askscience,new,32
iorgfeflkd,"Orbits emit gravitational radiation and decay because of it. However, the timescale of the decay is generally much longer than any other astronomical or cosmological process that it's pretty much irrelevant. Even in systems like double pulsars, the time-scale is still in the trillions of years.",null,1,cdghxn3,1qu50c,askscience,new,8
Ingolfisntmyrealname,"The Schwarzschild solution of the Einstein Field Equations definitely allow for stable planetary orbits. The Schwarzschild solution describes how spacetime curvature is outside a spherically symmetric object like a star or black hole which is most often very much approximately spherically symmetric. It also describes the geodesics paths in this spacetime curvature, which is to say ""the orbits of freely falling test particles"". Planets around a much heavier star is not exactly, but often close to a test particle. Light rays (photons) also follow geodesic paths. In this solution, massive test particles with large enough angular momentum L can have both a stable and an unstable circular orbit. I've illustrated it in [this](http://i.imgur.com/m8Gk8G7.jpg) plot where the gravitational potential is plotted against the distance from test particle to object (e.g. planet to star). At the bottom of the bowl there're stable orbits. It will be a circular orbit if the test particle is at the exact minimum in the bowl. It will be a [precessing ellipse](http://upload.wikimedia.org/wikipedia/commons/2/2d/Precessing_ellipse.png) if it's not exactly at the bottom. A real example of this is the [Precession of Mercury's perihilion around the Sun](http://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury).

Tl;dr: Yes there are stable orbits of massive particles like planets revolving around a Star in General Relativity. We have to make a lot of approximations and assumptions like absolute spherical symmetry and more, but these are often very accurate.",null,1,cdgjz2g,1qu50c,askscience,new,2
iorgfeflkd,They do. Stable nuclei exist.,null,0,cdghy2y,1qu4yx,askscience,new,8
restricteddata,"What makes something radioactive is that, by definition, it does not yet have a stable nucleus. If you look at a decay chain diagrammed (i.e. [the uranium-238 decay chain](http://upload.wikimedia.org/wikipedia/commons/a/a1/Decay_chain(4n%2B2,_Uranium_series\).PNG)) you can see how it goes all the way down until it hits something stable. None of those intermediary steps are stable. As for why it goes through them, it is because there are only a limited number of types of [decay modes](http://ie.lbl.gov/education/decmode.html), and they only let isotopes ""move"" around the periodic table in very limited ways (basically: a neutron can become a proton; a proton can become a neutron; two protons can become two neutrons; two protons and two neutrons can be ejected; and for very heavy isotopes there is spontaneous fission — most isotopes have an overwhelming probability of just one decay mode at a time). The number of stable isotopes is much, much smaller than the number of stable ones. So most decays are not going to lead to a stable isotope.",null,1,cdgiia5,1qu4yx,askscience,new,4
iorgfeflkd,"You are correct. Things like bowling balls are rough on scales larger than the atomic size, but the [spherical mirrors in gravity probe B](https://en.wikipedia.org/wiki/File:Einstein_gyro_gravity_probe_b.jpg) are the closest things to perfect spheres that we have made. If you want to go down to the atomic level, you an look at the structure of [buckyballs](https://en.wikipedia.org/wiki/Buckminsterfullerene), which are the closest things to molecular spheres but still have 32 faces.",null,0,cdgi0m7,1qu3mf,askscience,new,1
uberhobo,"A helium atom is spherical, for all intents and purposes.",null,2,cdgr77h,1qu3mf,askscience,new,1
Chemomechanics,You can use [chemical vapor deposition](http://en.wikipedia.org/wiki/Tungsten_hexafluoride)---flowing a tunsten-containing gas over the wafer and allowing it to react with silicon (at relatively low temperatures) to form metal and a gaseous byproduct.,null,1,cdgpeet,1qu2jp,askscience,new,7
LukeSkyWRx,"You can also electroplate with tungsten, typically takes place at even lower temps than CVD.",null,2,cdgvvq2,1qu2jp,askscience,new,2
Pennsylvania_Fatts,"Yes. Separations science is devoted to creating pure chemicals from mixtures. Many different techniques can be used depending on the mixture.  Some common methods are chromatography, centrifugation, filtration, distillation, extraction, and recrystallization.


Besides separation, sometimes components can be detected from the mixture if they have properties that are unique. The physical phenomena that must be present for a type of analysis to detect a component are called the selection rules. Since you mention foods, you can also use your mouth and nose as to analyze, because your senses of taste and smell select for certain components of the mixture.",null,0,cdgwyr4,1qu1w8,askscience,new,3
aggasalk,"it seems that a good olfactory system should work [in stereo] (http://stke.sciencemag.org/cgi/content/abstract/sci;311/5761/666). so, the animal should sense the local gradient of a scent in just one sniff through two nostrils (or flick of the forked tongue), and follow that gradient to the source by sniffing repeatedly.

look at [this paper] (http://www.nature.com/ncomms/journal/v4/n2/full/ncomms2444.html). they describe an experiment with moles to test this idea; basically you just plug one nostril and see if it causes the right kind of heading error as the animal seeks out a bit of food. seems to work (with caveats)!

i wouldn't think of this as '3d' perception, but that's a loaded term anyways. the world is 3d at any given moment, and if you're perceiving the world through any sense, the percept will be 3d to the extent that it's accurate. i guess you could say olfaction is 3d, sure.",null,1,cdgtax2,1qu1v3,askscience,new,14
Mazetron,"Well for starters you have to take into account air resistance and the decreasing force of gravity as you gain altitude.  You could calculate a fairly accurate angle to launch your rocket, but if you need more precision than that, your rocket may need to have smaller rockets on it to make adjustments.",null,0,cdgtkwx,1qtzfl,askscience,new,1
TheMacPhisto,"Rockets are guided based on planned trajectories that are planned prior to launch, or shortly after. There are formulas out there used, in tandem with guidance software, that map the most effective route out, even before the projectile is launched.

Using this pre-planned map of the ballistic trajectory (from launch to target) cross referenced with real time GPS data, RADAR data, Millimeter Wave Scanning, or any other type of guidance aid, the control surface software in the projectile will know what adjustments to make to return the projectile back to it's pre-determined path, and thus to it's target should any unforeseen factors alter it's course.

The ideal path is pretty ordnance specific. It also depends on the type of target you are looking to impact. The combinations of which are too numerous to mention in detail here. But an example would be the difference between an impact-fused warhead, compared to a proximity fused warhead, or the difference between striking a hard target like a bunker, or fortified above ground installation, compared to a soft target like a house or other standard building.

Most guidance systems these days operate the way I described above. this is known as ""passive"" guidance or ""semi-active."" As this type of guidance is incredibly difficult to interrupt or throw-off while at the same time being incredibly accurate.",null,0,cdhtbbz,1qtzfl,askscience,new,1
bellcrank,"Solar radiance, or solar insolation, is a typical measurement, though it may be difficult to search for specific data (like 'cloudy days').  It's a statistic very important to the solar power industry.",null,1,cdgn9gi,1qty2t,askscience,new,3
wazoheat,"If you aren't opposed to number crunching, you can check out the [National Solar Radiation Data Base](http://rredc.nrel.gov/solar/old_data/nsrdb/), which contains hourly data on the amount of solar radiation reaching the ground since 1961. [There are a lot of stations nation-wide](http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2010/images/NSRDB_Stations_revised.png).",null,0,cdgu90b,1qty2t,askscience,new,1
squishy_fishy,"In snakes, venom evolved 60-80 million years ago, derived from other proteins.

The following is from the abstract of [this paper](http://genome.cshlp.org/content/15/3/403.long):

*The snake toxins were shown to have arisen from recruitment events of genes from within the following protein families: acetylcholinesterase, ADAM (disintegrin/metalloproteinase), AVIT, complement C3, crotasin/β defensin, cystatin, endothelin, factor V, factor X, kallikrein, kunitz-type proteinase inhibitor, LYNX/SLUR, L-amino oxidase, lectin, natriuretic peptide, βnerve growth factor, phospholipase A2, SPla/Ryanodine, vascular endothelial growth factor, and whey acidic protein/secretory leukoproteinase inhibitor. Toxin recruitment events were found to have occurred at least 24 times in the evolution of snake venom. Two of these toxin derivations (CRISP and kallikrein toxins) appear to have been actually the result of modifications of existing salivary proteins rather than gene recruitment events. One snake toxin type, the waglerin peptides from Tropidolaemus wagleri (Wagler's Viper), did not have a match with known proteins and may be derived from a uniquely reptilian peptide. All of the snake toxin types still possess the bioactivity of the ancestral proteins in at least some of the toxin isoforms.*

According to [this paper](http://mbe.oxfordjournals.org/content/21/5/870.long), venom evolved once in snakes and was subsequently modified, though some others have the view that venom evolved independently several times.",null,66,cdghayl,1qtwue,askscience,new,380
null,null,null,22,cdghxkh,1qtwue,askscience,new,67
Toptomcat,"Since [animals are generally considered to have evolved from a unicellular flagellated eukaryote](http://en.wikipedia.org/wiki/Urmetazoan), and such organisms use cytotoxins in attack and defense all the time, the answer is a bit fuzzy. Usually venoms have to be mechanically injected into the target to qualify as such, but the line between chemical and mechanical actions gets very fuzzy at the cellular level: if there's a poisonous payload encased in a nontoxic protein that opens up the cell wall and delivers the poison, does it count as a toxin? At any rate, some of the earliest known animal fossils are [of the phylum Cnidaria, coral](http://en.wikipedia.org/wiki/Cnidaria#Evolutionary_history), and [the distinguishing factor of that phylum is its use of venom.](http://en.wikipedia.org/wiki/Nematocyst)

So the answer to your question is that animals *didn't* evolve to use venoms- venomous organisms evolved to become animals.",null,10,cdgkumw,1qtwue,askscience,new,48
MonkeyDeathCar,"Here's a related question, if nobody minds me asking: is the reason why some venomous snakes and other animals (that tiny blue-ringed octopus springs to mind) have toxins that are so incredibly out-of-proportion to any potential threats in their current environment due to them evolving when threats of much larger body mass existed and then there being no evolutionary impetus to scale back down? (a dead threat is a dead threat, evolutionarily, as long as there is minimal difference in the metabolic cost to produce said toxins, a venom that kills a threat in thirty seconds should be the same thing as one that kills in thirty minutes as long as the threat feels pain and moves off in the meantime, right?)",null,0,cdgve1o,1qtwue,askscience,new,15
Xythan,"This paper - [The birdlike raptor *Sinornithosaurus* was venomous](http://www.pnas.org/content/107/2/766.full) - and the two following papers were incredibly interesting. I studied and wrote on it for my Palaeontology degree and in doing so learned a lot about venom and the many convergent evolutionary events across multiple taxa.

PS - The three papers were:

Gong, E., L.D. Martin, D.E. Burnham, and A.R. Falk. (2009). ""The birdlike raptor *Sinornithosaurus* was venomous."" *Proceedings of the National Academy of Sciences*.

Gianechini, F.A., Agnolín, F.L. and Ezcurra, M.D. (2010). ""A reassessment of the purported venom delivery system of the bird-like raptor *Sinornithosaurus*."" *Paläontologische Zeitschrift*.

Gong, E., Martin, L.D., Burnham, D.A. and Falk, A.R. (2010). ""Evidence for a venomous *Sinornithosaurus*."" *Paläontologische Zeitschrift*.

PPS - For the record I came to the conclusion that it was entirely possible that *Sinornithosaurus* was venomous, and perhaps additional studies might find further evidence supporting this for both this species and even others.",null,0,cdh3dgb,1qtwue,askscience,new,3
Loud-Mouthed-Sloth,"An interesting note is that the Komodo Dragon shows what could be looked at as a archaic form of venom. The lizard itself if none-venomous, however, it produces a large amount of saliva, which has properties that support a large amount of bacterial growth. The Komodo must simply bite its prey and wait for the infection to spread and immobilize the target, should it not take it down on the first strike.

I'm not really a scientist, but I do love biology, and I just thought it's interesting.",null,31,cdghonq,1qtwue,askscience,new,31
null,null,null,10,cdgma9f,1qtwue,askscience,new,4
SqueakyGate,"The current data is really really questionable. It really depends on who you ask, because personal biases often play a subtle but important role in the way people interpret these psychological tests. 

I would suggest reading up on the [delusions of gender by Cordelia Fine](http://en.wikipedia.org/wiki/Delusions_of_Gender): ""The author criticizes claimed evidence of the existence of innate biological differences between men and women's minds, as being faulty and exaggerated, and argues that cultural and societal beliefs contribute to commonly perceived sex differences....""The thesis of my book (no veils required) is that while social effects on sex differences are well-established, spurious results, poor methodologies and untested assumptions mean we don’t yet know whether, on average, males and females are born differently predisposed to systemizing versus empathizing.""

However, some criticism of her book have been made - although I don't know of any substantial rebuttals to her work. Given the level of debate from both sides, the lack of strong evidence one way or the other I have to say that overall psychological and mental differences in sex (e.g. men vs. women) is not strongly supported at this time. 

Gender is of course a social and cultural construction of the perceived way the different sexes should act, behave, dress, conduct themselves etc. In western societies, there is a two-sex two-gender system, that is male = masculine and female = feminine. But this is not the case for all cultures, some have multiple genders which overlap with the two sexes. Therefore it would be great to have a broader characterization and experimentation of people who come from cultures with more than two gender systems. It would be interesting to see how the two sexes perform when they are not held too the western standards of gender. 

What I am trying to get at is any test or examination conducted today has a really difficult time separating biological sex and imposed cultural gender stereotypes. 

",null,0,cdgwehk,1qtuwc,askscience,new,2
CompMolNeuro,All mammals at the very minimum.  I would posit that any animal capable of having a psychological profile will show gender specific behavior.,null,3,cdgkydf,1qtuwc,askscience,new,3
homininet,"Good question. Most mammals are what are called dichromats. What this means is that they have two types of cone cells in their retina that are optimized to see blue/violet light, and red light. Certain primates are unique in that they possess a duplication of gene which has given them 3 cones instead of two (blue/violet, red, and green). These primates are catarrhines (old world monkeys, apes, and humans). Certain other primates have also evolved color vision but in different ways. Howler monkeys (new world monkey) have all evolved trichromacy, and in most other new world monkeys, only females are trichromats, and the males are all dichromats because the gene duplication responsible for this rides on the X gene. Some lemurs are like this as well.

Certainly this has also played a role in evolution. Most early mammals were nocturnal, and didnt really have a huge need for trichromatic vision. They did however, have great senses of smell. Primates have kind of done a double take, and foresaken their great sense of smell for a more keen sense of vision, and in the process, certain lineages became diurnal. When you walk around in the daytime, trichromatic vision is a huge plus, and catarrhines were able seize on this advantage due to this early gene duplication which cause them to be trichromatic. 

",null,0,cdglv7i,1qtuty,askscience,new,5
nicorivas,"Yep, colour is an advantage just by the possibilities of distinction. And also yes, there are animals that have the same receptors as us, although not all of them. The article on wikipedia on the [evolution of colour in primates](http://en.wikipedia.org/wiki/Evolution_of_color_vision_in_primates) is very informative.",null,0,cdgkv7e,1qtuty,askscience,new,1
Osymandius,"I think you're coming at this from the wrong angle. There's no reason one signalling molecule does any particular function, bar that that's the function that its receptor effects.

When you say ""why"" testosterone - because that's the molecule that binds to its receptor. It wasn't that testosterone existed beforehand, and happened to produce a male phenotype upon binding to its cognant receptor. 

Any molecule which triggered that set of downstream pathways would do, but its vital that we have the molecular specificity such that only one molecule does. Imagine if blood glucose or a dietary fat bound to sex hormone receptors - it would be chaos!

Does that clear it up? I appreciate it's not an answer of molecular basis, but it's a question of terminology - their suitability has nothing to do with their downstream effects, only their specificity. ",null,2,cdgkow2,1qtuit,askscience,new,3
sapolism,"Its thought that nuclear steroid receptors sites all diverged from a common ancestor. The whimsy of mutation and the success thereof is what would have driven higher specificity in these receptors to bind specific molecules. Discussion of the cause of success here would get very complex, and I find it unlikely that we have enough DNA evidence to make a good argument for what actually happened.

I don't know what is likely to have happened on the ligand end of things either...
",null,0,cdgm2p1,1qtuit,askscience,new,1
wcrrqy,"Current cannot instantaneously change in an inductor.  If there is no current, once it starts flowing it has to also induce that corresponding magnetic field the Faraday and Maxwell were all on about.  As the magnetic field increases, it opposes the current flow a bit.  After a period of time (some constant that is a property of the entire circuit) the magnetic field is fully established and current can flow at its full rate.

If current is already flowing and it stops, the induced magnetic field collapses and induces current back into the wire.  And this back induced current opposes the collapse of the magnetic field a bit.

If we're pushing a constant DC current, you can see how things reach equilibrium.  There are some start-up costs and a bit of trail-off when shut down, but otherwise works as expected.

If we slowly cycle the current it will kind of act like constant DC.  As we increase the frequency of the cycling, all those induced magnetic fields and collapse induced currents start to all interfere with one another and cancel out and the result on the other side of the inductor is (as frequency increases) just the underlying DC component of the signal (if it has some).",null,1,cdghqe4,1qtr3o,askscience,new,13
Death_Star,"The ferrite itself is not causing the frequency response. It increases the inductance since it has a high magnetic permeability and can concentrate the magnetic flux in the core. Then the inductor will respond more strongly to rapid changes in current though it, due to a stronger changing magnetic filed though the loop, per Faraday's Law.

Depending on the circuit configuration this could create a low pass filter with attenuated high frequncy output. For example if you are monitoring a voltage that is **NOT** the voltage on the inductor, then it will be low, since the high frequncy voltage is all dropping across the  high impedance inductor. Just analyze the circuit using the impedance value of the inductor, and remember that this impedance at different frequency is directly related to the core material.",null,2,cdgipa5,1qtr3o,askscience,new,4
AverageEverydayGuy,"There are actually multiple physical mechanisms for how the beads attenuate high frequencies.  The most predominant is inductance (most of the other comments reference this).  Other losses are caused by other effects that happen within the ferrite material - for example eddie currents become more predominant at higher frequencies and cause losses as high frequency signals get converted to heat.  There are other frequency dependent loss mechanisms as well - Magnetic material science is a very complex subject and what would appear to be a simple matter of losses in simple bead of material could be the subject of multiple PhD level research efforts.

Wikipedia's Ferrite bead article describes these effects as follows: ""First, the ferrite will concentrate the magnetic field, increase the inductance and therefore the reactance which will impede, or ‘filter out’ this noise. Second, if the ferrite is so designed, it can also add an additional loss in the form of resistance that occurs because of loss in the ferrite itself"".

http://en.wikipedia.org/wiki/Ferrite_bead
",null,1,cdglhuk,1qtr3o,askscience,new,4
grayman19,"

Ferrite beads are used in inductors to form a passive low-pass filter. The geometry and electromagnetic properties of coiled wire over the ferrite bead result in an impedance for high-frequency signals, attenuating high frequency EMI/RFI electronic noise. The energy is either reflected back up the cable, or dissipated as low level heat. Only in extreme cases will the heat be noticeable.

Copy pasted directly from wikipedia. ",null,5,cdgfmjj,1qtr3o,askscience,new,6
FW190,"Sorry for hijacking the thread, I have related question. What is the best place in the following chain to place ferrite bead to maximise its EMI/RFI blocking effect: mains -&gt; transformer -&gt; rectifier -&gt; consumer. I usually see it before transformer but transformer as I understand it can pick up some radiation too so wouldn't it be better to place it after transformer?",null,1,cdgoxl9,1qtr3o,askscience,new,2
The_Serious_Account,"David Christian of Big History suggests that it was a result of Earth coming out of the last ice age about 10.000 years ago. Humans for a large part appears to have had all the technologies necessary for agriculture, but simply didn't need to. Increasing in population put pressure on humans to increase the amount of energy extracted from the same area of land.  In fact it appears things like living standards and life expectancy went *down*. But those who did go into agriculture started to simply out compete surrounding hunter gathers in shear numbers. When your population density is more than an order of magnitude higher, war becomes almost trivial.

The rest is, as we say, history. 
",null,12,cdgb3dg,1qtfqk,askscience,new,48
snickeringshadow,"Very good question. Unfortunately, the answer is rather complicated. (***TL&amp;DR***: There were a lot of things happening at the same time. Agriculture is kind of like a symbiotic relationship between plants and humans, and the conditions that allowed this to develop first appear during the Holocene. How this relates to the origins of complex society is even more complicated.)

First, you have to understand that when you're talking the origin of 'civilization,' you're really talking about four major changes:

1. A shift from a hunger-gatherer to agricultural subsistence pattern
2. A shift to permanent settlements.
3. The growth of cities
4. The development of ranked and stratified societies (that is, a society with dedicated rulers and class distinctions). 

You are correct in pointing out that this seems to happen at about 6 different places at the same point in time. (The classical six, in chronological order, are Mesopotamia, Egypt, Peru, India, China, and Mexico.) Unfortunately, these things don't always happen in the same *order* everywhere. For example, cultures on the coast of Peru c. 3000 - 2,000 BC developed cities with central planning without any agricultural food source. So while we can identify general trends that seem to occur, we can't really reduce it to some kind of formula to produce a civilization. 

**Permanent Settlements**

Not all hunter-gatherers are nomadic. Foragers that make use of maritime or lacustrine resources, for example, may live in permanent settlements along shore lines. Other hunter-gathers are semi-nomadic and have specific places they go to depending on the time of year. Others have migratory patterns that follow specific prey (the cliche example of this is the Lakota of the US great plains who followed American Bison.) All of these patterns can lead to domestication of the food resource in question, but complex society requires that at least some of the population lives in permanent residence at one location. 

It is likely not a coincidence that many (although not all) early settled agriculturalists lived in areas where there were lots of resources concentrated in one location (rivers, coastlines, valleys, etc.). It makes sense if you think about it from a cost-benefit framework. If it's easy for you to get all of your resources in one place, it makes sense for you to stay there regularly or exclusively. 

**Domestication**

Agriculture is about more than just planting. Domestication is an evolutionary change that takes place in plants and animals as a result of humans interfering in the reproductive cycle of the organism. However, this isn't necessarily a conscious process and in some cases seems to happen by accident. The wild precursor of maize (corn), for example, is widely believed to be teosinte. However, teosinte is barely edible in its wild form. It does not appear that the Archaic Period inhabitants of Mexico were planting it as a food source. Instead, its possible the grass simply grew in disturbed environments near campsites and people began collecting it. The other possibility is that people were growing it intentionally but as a supplemental food (i.e., to make beer). Over time people consciously or unconsciously selected those cobs which had larger kernels and more easily removed seed husks. Over time, the plant evolved into maize. 

Although we often phrase domestication as something we do to plants, its important to realize that we were also changing in the process. As maize became larger and had a higher caloric yield, people began producing it more. Population went up, and the intensity of farming went up with it. Humans were now interfering in the plant's reproductive cycle to the point where it was dependent on us to reproduce. As a result, it evolved to be more attractive to us. And in response, humans adjusted their culture to make heavier use of it. Since all of this took place over thousands of years, this was not likely something humans set out to do. It just sort of developed as a symbiotic relationship beneficial to both organisms. 

Now, as to why this seemed to happen at the same time in multiple places, this *likely* has something to do with the ecological changes that occurred during the Holocene. Warmer climates and the growth of grasslands meant that the conditions necessary for this symbiotic relationship to develop were more common. *However*, we have to be cautious with this interpretation. Its easy for archaeologists to correlate changes in human society with paleoecological changes, but correlation does not imply causation. Some of this may be bias introduced by the fact that paleoecological changes are easier to see than more ephemeral changes in culture, society, and politics. There is likely more to the picture that we're still missing.

**Urbanism and Social Complexity**

Now the hard part: how does this tie in to the development of 'civilization?' Most of the ""Old World"" civilizations (like Egypt and Mesopotamia) developed agriculture first, then cities, then social complexity. However, it doesn't look like that pattern is universal. Andean civilizations, for example, developed cities and social complexity first, then agriculture. In Mesoamerica agriculture gets started early, but the domestication process takes a long time. Cities and complex societies develop before the process finishes.

Unfortunately, there really *isn't* a good explanation for why complex societies developed. We've identified a number of key factors, but a lot of research needs to be done before we can be conclusive about this. Most archaeologists are split between two camps: managerial efficiency (or *top-down*) models and agency (or *bottom-up*) models.

*Managerial Efficiency Models:*

These models attempt to explain complex society as part of a system that people buy into. The most famous example of this kind of model is Karl Wittfogel's Hydraulic Hypothesis. Wittfogel hypothesized that complex societies developed in Egypt and Mesopotamia because people needed a central authority to manage irrigation systems. With population increases in these regions there was not enough water for every farmer to use whatever he wanted for his own fields. A kind of 'tragedy of commons' developed, and to combat this people developed centralized institutions that could regulate water consumption. It's fairly well established that ancient Mesopotamian city governments and the Egyptian state did in fact do this, but it's causal importance in their origin has not been established.

*Agency-Based Models:*

These models developed as a critique of the top-down character of managerial efficiency models, which don't really explain why an *individual* would want to buy into this. Think about it: if you're John Q. Farmer and a guy comes up to you one day and says you're only allowed to use *x* amount of water, why would you listen to him? What reason would you have in even acknowledging his authority over you? Agency based models seek to address this by looking at economic and ideological factors in the origin of complex society. These have been gaining lots of ground recently in archaeology, but they currently lack unity.

So basically, bottom line: we don't know the exact reasons complex societies developed yet. There are two major theoretical camps, but there are about as many different ideas on this topic as there are archaeologists. ",null,2,cdgijoy,1qtfqk,askscience,new,21
null,null,null,2,cdghl8y,1qtfqk,askscience,new,8
docious,"Hunter-gatherers did not have the ability to store food for long periods of time and hence had to constantly follow the herds they hunted and move on from pastures that they had exhausted of resources. Despite the fact that wild grain grew plentifully in areas like the Fertile Crescent the grain could not be stored due to the fact that the husks were so thin and weak that they would crack and break as a result of being piled up (as if for storage).

Then our environment changed; specifically the genetics of the grains that were growing wildly in the cradle of civilization. A hybridization between two or more grains occurred that resulted in a grain with a stronger husk which allowed it to be stored. Once humans discovered that the grains that grew in there area could be harvested and stored for long periods of time there was no longer a need to remain living a nomadic/hunter gatherer lifestyle.

",null,4,cdgfyjv,1qtfqk,askscience,new,6
Zeakk1,"
 I haven't seen this potential theory presented yet but it was published relatively recently in the Journal of Archeological Method and Theory.

Basically, the authors of the study suggest that the demand for cereal grains increased due to the interest in having the cereal grains to produce beer because we like alcohol. They support their claims by noting that beer appears to have been the primary use of cereals before using it to bake bread was. 

Their study also implies that the role of consuming alcohol became important in some societies, and to paraphrase, acted as a social lubricant to allow for folks to be able to begin breaking patterns of social structure that were crucial in hunter/gatherer societies and less helpful for an urbanized setting.

I think my favorite part about your question, though, is that having spent some time working on archeological digs and studying classical history is that basically everything related to the answer of our question is speculation. Speculation based off of data, but still speculation. I personally enjoy the 'to have beer' answer to your question because it takes it from anything lofty regarding rational understandings of improved survival chances, or understanding of economic benefits and turns it into something simple that can be true to anyone in any society with any level of education: Humans like getting drunk. We grew cereals because we figured out how they could help us get drunk.

The actual article
http://link.springer.com/article/10.1007/s10816-011-9127-y
A free and wonderfully written layman's account from the New York Times
www.nytimes.com/2013/03/17/opinion/sunday/how-beer-gave-us-civilization.html",null,1,cdgjk5a,1qtfqk,askscience,new,4
TangentialThreat,"Many antibacterial consumer products use triclosan, which does [several different things](http://jac.oxfordjournals.org/content/53/5/693).

Humans are also massive and covered in an outer toughened layer of dead watertight cells (skin), which means we are relatively unharmed by high concentrations of alcohol and many other forms of chemical attack. Most hand sanitizers are basically alcohol and a thickening agent; it burns only if you get it in a cut.

I am also somewhat skeptical of statistics in marketing, and I've never felt really comfortable with the idea of leaving 0.01% of bacteria alive.",null,14,cdga2hy,1qtc5y,askscience,new,52
Orckilla,"Not all soaps actually kill bacteria. The anti bacterial ones have  triclosan. http://en.m.wikipedia.org/wiki/Triclosan triclosan is a biocide, which is something attacks cellular membranes. At low doses it seems to attack bacteria's fatty acid synthesis. Fatty acids are required for the building of cell membranes. Usually soap works by creating a surface that is too ""slippery"" for bacteria to adhere to. This causes them to fall off while being washed, not doing much damage to your skin. ",null,3,cdgdnl8,1qtc5y,askscience,new,6
get_awkward,"Actually, to say it is harmless is not necessarily true.  It inhibits fatty acid synthesis, which can kill bacteria.  Whether or not it has adverse effects in humans is still up for debate.  That's why many companies no longer use it, and I've seen it advertised in toothpastes that 'triclosan free'.  

Triclosan inhibits enoyl-acyl carrier reductases, an enzyme required for FA synthesis in bacteria.  Though humans do not have this enzyme, but it is suspected of being a carcinogen.
",null,0,cdgetyt,1qtc5y,askscience,new,3
miloMILK,"As a general rule, antimicrobials tend to target structures and enzymes found only in bacteria or fungi. 

Two classic examples are lysozyme and penicillin. These both affect the peptidoglycan cell wall only found in certain bacteria. This is basically an outer layer made of a sugar polymer. Lysozyme is found naturally in tears and saliva. It breaks the bonds between the sugar molecules of the cell wall, while penicillin blocks the enzyme that forms cell wall.

There are many different possible targets for antimicrobials. Bacteria have different ribosomes than us and many different metabolic pathways. Blocking any of these will kill the bacteria and leave us unharmed.",null,0,cdgejld,1qtc5y,askscience,new,2
repmack,"Mainly the composition of Bacteria or viruses is somewhat different than Eukaryota cells, which is what our body is made out of. Also skin. If you put that stuff inside your body it could do some damage. ",null,3,cdgalcz,1qtc5y,askscience,new,4
Eulerslist,"Most simple soaps are effective against a large percentage of bacteria simply because they lyse, (dissolve or damage) the cells membrane which contains an outer shell of lipids.
They would not be ""harmless to most of our own cells' were the cells directly 
exposed to it instead in a 'hand-washing' concentration instead of being protected by the insoluble keratin in our skins.
Try letting some soap get into the next small cut you get and see how it feels.",null,0,cdgibwq,1qtc5y,askscience,new,1
zombiecheesus,"A lot of people are talking about the antibiotic soaps.

The bacterial and viral membranes are not resistant to detergents, being a single cell organism they lack a complex extracellular matrix. Those that are not killed are surrounded by detergents and washed away. 

Additionally, the organisms that evoled with us are more ""hardy"" and have better mechanisms to attach to our surface and resist our antibiotics (the intrinsic ones produced by people); these are not so readily washed away. Whereas pathogens are less ""fit"" in a general sense and are more prone to loose initial attachments and detergent lysis. Therefor, using non-antibiotic soap will not significantly disrupt your flora, where antibiotic soap over use will kill your flora leaving a fresh unchallenged surface for a pathogen. ",null,0,cdgwdd5,1qtc5y,askscience,new,1
ModernTarantula,"Putting it all together. detergents penetrate and disrupt cell membranes but there hs to be a lot compared to the amount of membrane so the tiny viruses and bacteria are at risk. Human cells would not do well if exposed, so keeping skin around to protect is good. Alcohol evaporates and dries out the area. that is how it sanitizes.  But would be back for intact cells as well.  Llimited problem with intact skin, but could be after repeated exposure.  Most of it is size",null,0,cdh0qua,1qtc5y,askscience,new,1
none_shall_pass,"OCD Handwasher Alert:

You can now stop using hand cleaner like it's Kryptonite. Because it isn't.

Alcohol isn't actually 100% effective at killing all germs. In fact there was a recent [recall on alcohol pads because they were contaminated](http://www.nbcnews.com/id/41914778/ns/health-infectious_diseases/t/colo-hospital-blew-whistle-contaminated-wipes/?ns/health-infectious_diseases/t/colo-hospital-blew-whistle-contaminated-wipes/#.Uoj9eeI0hT8) and infecting people.

",null,1,cdgfxpt,1qtc5y,askscience,new,1
null,null,null,5,cdgg6o2,1qtc5y,askscience,new,4
neha_is_sitting_down,"They target chemicals or processes which are unique to bacteria and/or viruses.

Bacterial cells and viruses are very different from animal cells, and so something that breaks down a process or part in a bacteria or virus would not necessarily affect you.",null,11,cdg9kcp,1qtc5y,askscience,new,4
Xhaf,"When you add salt and lower the freezing temperature of the solution, ice will begin to melt because it's then above the freezing temperature. However, ice requires a lot of energy when it melts, which it will take from it's surroundings and therefor decrease the overall temperature.",null,1,cdgbv3m,1qt7qr,askscience,new,15
Platypuskeeper,"The ice cools it. It takes quite a bit of energy to melt ice. 

But if you have a mixture of pure water and ice at 0 C, then it'll stay that way, the proportion of ice/water won't change - some water will freeze and some ice will melt, but it's at the same rate. 

If you mix water above 0 C with ice, then there will be more melting than freezing, and the energy for that is coming from the heat of the water, which drops until it has melted enough ice to bring them both down to 0 C. 

Same thing if they're both at 0 C and you add salt and depress the freezing point. You shift that balance. The water is now above the freezing point (for that solution, not for pure water), so the water doesn't freeze at the same rate as the ice is melting anymore. So you have a net melting of the ice and the temperature of the water solution will drop until it reaches the new, depressed, freezing point.
",null,5,cdg849k,1qt7qr,askscience,new,12
rupert1920,"Melting is an endothermic process, which means it takes energy from the environment to melt.

When you add salt to ice, the ice melts, and the temperature of the system lowers. This is how ice baths - down to about -20 C - can be prepared in the laboratory with ice, water and salt alone.",null,1,cdgcnsm,1qt7qr,askscience,new,7
null,null,null,1,cdg825r,1qt7qr,askscience,new,1
hipstercouver,"You put a 0C after ice... the ice in your freezer is likely at -20C

When you have salt water, it's freezing point is lowered, ie, -5C 

When you have Ice cubes in salt water, the system will go towards an equilibrium point which means your 3C water will cool down, the -20C cubes will warm up.  

The surface temperature of the ice cubes will be at -5C ",null,4,cdgamyl,1qt7qr,askscience,new,5
Eldritter,"The question you are asking has a lot of words, but it's still vague.  One possible solution to this is that you could type out the homework question you are trying to answer at full length to make it un-ambiguous.

One possible rephrasing of your question would be as follows: ""How does mixing salt and water both at the same temperature result in a lower temperature of the combined substances?""  One possible answer for this is that the property of a material determines how much temperature change that material undergoes when given a specific amount of thermal energy.  The classic example is that the specific heat capacity of water is fairly high so that it takes a lot of heat to raise the temperature compared an identical amount of copper metal which requires less thermal energy to rise in temperature.  Thus, by mixing salt with water you changed the specific heat capacity meaning that the water still has the same amount of thermal energy, but does not have the same temperature.  

You are right, the crystal lattice of cold hydrogen bonding is disrupted by the salt.  In some cases, solutes will disrupt the lattice so that the ""brine water"" will be in liquid form despite being below the freezing temperature of water with no-ions dissolved.  Energy input is required for the salt to dissolve.  However, the reason it is possible for the brine solution to be at a lower in temperature after salt dissolution is that after after the reaction is over, many water molecules are organized to shield the ions instead of participating in hydrogen bonds.  This shielding in the case of common table salt would result in less water molecule movement meaning a thermometer would see a lower temperature lower than 0 degrees C.  The reason the water will be below 0C and will still be liquid is because the water is shielding ions instead of participating in hydrogen bonding that forms water crystal lattices.  
",null,3,cdgpj53,1qt7qr,askscience,new,1
JohnSmith1800,"The same way all your peripheral cells get oxygen.

The arteries from the heart divide and shrink as they progress from the aorta. From these, capillary beds spring, which are a maze of small vessels slightly smaller than an RBC. Across the walls of capillaries oxygen/CO2/nutrient exchange occurs, and these then diffuse into surrounding tissue.

Capillaries run in all living tissue, and peripheral nerves receive oxygen by diffusion from them.",null,0,cdg8f1i,1qt76r,askscience,new,2
counterfriction,"Technically, it is possible if the body closer to the sun was orbiting at the L1 [Lagrange Point](https://en.wikipedia.org/wiki/Lagrangian_point) of the outer planet. In fact, we have satellites orbiting around this point. However, this is an unstable orbit and without active stabilization (hard to do for a whole planet) the inner body would fall into orbit around either the other planet or the Sun.  
  
As neha points out, this is not possible for basic Keplerian orbits, as bodies orbiting the Sun at different distances would necessarily get out of phase since they have different orbital periods.",null,5,cdg9kh0,1qt68p,askscience,new,31
michaelrohansmith,"Thats the Lagrange 2 point: http://en.wikipedia.org/wiki/Lagrangian_point

But note that L2 is not stable. Without some station keeping you will eventually drop in to a normal orbit around the planet.",null,3,cdg7l2d,1qt68p,askscience,new,10
Drendude,"This is the second Lagrangian point (L2). It is possible, and we have satellites that are there (for example, the James Webb Space Telescope is planned for that, and the Planck Surveyor and Herschel are already there)

However, the Earth does not fully eclipse the sun at this point. So the radial size of the sun is larger than the radial size of Earth.

You can solve this by going behind Jupiter. The density of the planet is less than earth, on average, so the radial size of Jupiter is much larger relative to the distance of its Lagrangian point.",null,0,cdgj213,1qt68p,askscience,new,2
beginner-,"Branching off of OP's question:

I'm in an Intro to Space Dynamics class and two or so months ago we went over satellites that have orbital characteristics so that the satellite is always looking at the sun-lit half of the Earth. I think the orbital period was the main aspect of this orbit but I can't remember. Why can't we do it just on the other side of the Earth with a circular orbit so that it fits OP's question?

By the way we haven't gone over Lagrange points or anything like that yet and it doesn't look like we will this semester.",null,0,cdgg5mf,1qt68p,askscience,new,1
neha_is_sitting_down,"How fast you go around a planet depends on how close you are to the planet and the mass of the planet. Every orbit distance from a planet has an orbit period (how long it takes to go around). This increases as you get farther from the planet. 

To keep the planet between you and the star, you would need your orbital period to equal the planet's orbital period. Meaning, for every time the planet orbits the star, you orbit the planet.

The equation for period is here http://en.wikipedia.org/wiki/Orbital_period#Small_body_orbiting_a_central_body.

I think you could do it, but you would need to have a very big star and a very small planet. Otherwise your orbit would be too large and you would be pulled by the star... I'm not sure.",null,6,cdg7uw5,1qt68p,askscience,new,5
Thatsnotwhatthatsfor,Nope. You could orbit the star at a point that always kept the other planet between you and the star though - which several others seem to have explained but not quite pointed out that it wouldn't be an orbit of the planet itself. ,null,3,cdggwpu,1qt68p,askscience,new,2
hiimsadako,"Too acidic - demineralization of the enamel which results in cavities

Too alkaline - mineralization of the bacterial plaque's matrix resulting accumulation of tartar which irritates the gingiva. 

The pH of your mouth has to be just about right, not too acidic, not too alkaline, but somewhere around 7. That's not always possible given our diets and not brushing or flossing after a more acidic meal or using chewing gum. 

Brushing your teeth with baking soda is terrible for the enamel (I have seen people do this), it could be used as a mouthwash (the ratio is 1/2 teaspoon dissolved in 200ml of lukewarm water) to buffer the acidity. 

The direction we should be heading towards is not making our mouths more alkaline but rather reducing the acidity so the pH doesn't drop below 5.5 which is the critical pH of enamel demineralization. ",null,17,cdgcnzx,1qt5q9,askscience,new,123
Nepene,"I know from popular culture a well known folk tooth protector is baking soda, based on this reasoning.

http://www.sciencedaily.com/releases/2009/10/091027132424.htm

But alkaline substances can damage your teeth too.

I'd definitely appreciate a scientific perspective on the effects of more mildly alkaline substances.",null,11,cdgaiat,1qt5q9,askscience,new,21
briansanasshole,"Too alkaline is no good for your oral environment either. Like many things in human physiology, optimal tends to be at some point of homeostasis. And in the case of the mouth a narrow range of pH. When you change the pH in either direction outside this range, you run the risk of caries or rampant growth of oral fauna. There may be other conditions whose growth could be altered by a change in pH (fungal diseases like thrush, and hundreds more… btw, no idea how pH affects thrush, I just pulled its name out of the air). I'm not sure which pathologies would be affected by having too basic of a mouth, but I'm confident that a dentist here could name some. 

As for baking soda, I suspect that the grit of the paste is as beneficial (as a mechanical remover) as it is a basic solution/chemical ""protectant"". ",null,4,cdgbdyy,1qt5q9,askscience,new,11
CptMellow17,"I only have a bit of a neurological background, and I don't have a source, but I would assume that your brain gets accustomed to the fact of wearing something, and ignores the sensation of wearing it so it wouldn't bother you.

It's like how your brain ignores your nose. It's actually really in the way of your vision, and if we'd notice it all the time, it'd be really annoying. The same thing with your watch, or with glasses. Sometimes, I can feel my glasses on my face and it's really annoying if I think about it, so my brain ignores that feeling.

Anyways, to answer your question. Your brain spends the whole day ignoring the sensation of your watch on your wrist or your glasses on your face (therefore your brain acknowledges it's there). If this happens often, your brain gets into the habit of making you ""forget your watch is there"". Therefore, even when you're not wearing a watch, your brain thinks you are (out of habit), so it's basically convincing another part of your brain that you're wearing a watch. That's why you get the sensation.

It's kind of like the placebo effect. Basically, your brain is so used to you wearing a watch, even when you're not wearing one, it manages to convince you that you're wearing one, which is where the sensation comes from (""If I am wearing a watch, then I must feel it on my wrist"").

No idea if that was well explained or not.",null,1,cdghujw,1qt5ce,askscience,new,4
iorgfeflkd,"It can be made by surrounding a nuclear reactor with lithium-6, which produces tritium. Tritium then decays to helium-3. This is cheaper than going to the moon.",null,0,cdg5q7c,1qsxjq,askscience,new,3
cfulla94,"This is an example of constructive or destructive interference. If the waves are in-phase, this is constructive interference. The intensities of the two waves will add up and create one much more intense wave.

If the two waves are out of phase, then destructive interference will be experienced. If we have destructive interference, the waves will create a lower intensity wave, but as mentioned before, it depends on where the sound is measured because it will be louder in some places, and much quieter in others.",null,3,cdg4etq,1qstrz,askscience,new,30
neha_is_sitting_down,"Do you mean if I take two speakers and play the same sound, will it create sound louder than either speaker?

If so, then the answer is yes, but it depends where you measure, since in some places the sound will add up and in others it will cancel out.",null,2,cdg3oan,1qstrz,askscience,new,24
zimmermanstudios,"What happens when two sounds combine depends on the phase relationship between the sounds, what the source of the sound is, and what the sound is.

Two different speakers playing the exact same sound at the EXACT same time will result in a sound field with different volumes depending on where you measure, as others have noted. If you are equidistant from the two speakers, the sound will be 6 decibels louder than the original sound at the same location. If, say, you are playing a single tone, and you measure at a location closer to one speaker than the other by half a wavelength of that tone, you get silence. If you are playing anything other than a single tone, it gets far more complex and the question is impossible to answer without knowing more information.

Two different speakers playing the exact same sound at even slightly different times (milliseconds off, even) will result in a very complex field of constructive and destructive interference.

Two separate but ""identical"" sounds, such as two trumpets playing the same note at the same volume, same time, same everything, results in a sound that is 3 decibels louder than either individually at all measurement locations.",null,1,cdg6kh3,1qstrz,askscience,new,12
GN8,"A little more detail:

Sounds that are directly phase correlated (same time, same wave shape, truly exactly the same) will sum together and give exactly a 6 dB increase in Sound Pressure Level. A 10 dB increase in Sound Pressure Level is generally considered to correspond to a perceived doubling of volume.  So, assuming you could place them in all exactly the same spot of existence, every time you doubled the amount of speakers, you would gain 6 dB.  

When summing highly complex but not phase correlated signals, the result is a 3 dB increase (highly complex means containing material from all sound frequencies, think music, or the two trumpets picture used by zimmermanstudios).  If one were to tune two boom boxes that occupied the exact same space to two different channels, both were playing loud rock music but two different songs, it would be 3 dB louder in Sound Pressure Level to have both on when compared to just having one.",null,3,cdgav28,1qstrz,askscience,new,6
farhan_maulana,"Yes, you can have multiple sources of sound to create much louder decibel output.

For example, if you want to increase a 90 dB sound to a 110 dB output, you must have 100 source exact same sound played over each other.

The formula is like this;

New intensity = current intensity + 10 * log n, where *n* is the number of source.

But again, you must have *exact* same sounds which is in-phase. If the sounds are out of phase, the equation above cannot be applied.",null,2,cdg8wu1,1qstrz,askscience,new,4
null,null,null,0,cdgarjp,1qstrz,askscience,new,1
mrsv,"In short:

Yes!

In practice:

As soon as you have one or more sound sources that are not an infinitely small point and in exactly the same position, it will change directionality of the sound field. So it is not the same sound pressure at every point of the room. This is used a lot in PA where you can have speaker arrays to exactly shape the sound ""beam""  [Line Array](http://en.wikipedia.org/wiki/Line_array)",null,0,cdgexml,1qstrz,askscience,new,1
slickbackllamar,"Radiolab did an interesting podcast on this about the biblical story 'The Walls of Jericho'. In the story, a bunch of trumpeters play a noise so loud it brings the wall down, and it asks if this is possible. It covers the answer to your question and a bunch of other stuff about diminishing returns on sound.

http://www.radiolab.org/story/96854-walls-jericho/",null,0,cdghvsz,1qstrz,askscience,new,1
CookiesRiot,"Shape and density are the biggest factors (in wires, tension and length are the most important). The pitches you get when you hit something are due to a natural instability in the object.

The natural vibration frequencies can be calculated, and are called ""modes"". The first (fundamental) mode is the lowest frequency and also the one that comes across the loudest. The other modes are all considered overtones. If you take shapes that aren't simple (like aircraft wings), these can be frustratingly difficult to determine. But strings are pretty straightforward.

So the fundamental frequency is ""the note"" that you hear. If you play a middle C on a trombone and a piano, the pitches are identical but they sound different. The difference in sound (timbre) is due to the difference in overtones. The higher frequencies of vibration make the instruments seem different even though the fundamental is the same.

The actual nature of the different vibrations is really interesting; in a metal bar, you hear several main virbations. If you put your hand on the end of the bar after you hit it, one of the pitches will go away because you stopped it from expanding and shrinking lengthwise.

Percussive sounds are not necessarily like pitches; they don't have a set frequency or timbre and they don't sustain at all. Instead of a vibration from a natural instability, the're actually single pressure waves caused by a collision. If you hit two sticks together really hard, you'll hear a click sound and two extremely short pitches. The click is a single bump from the collision, and the pitches are the very short-lived vibrations of the sticks. Cymbals actually have pitches, but the modes are really similar in magnitude and are extremely dissonant, so it sounds like a bunch of noise, and the initial percussive sound is really important.",null,2,cdg8w6b,1qst72,askscience,new,4
therationalpi,"There are a lot of things about a material that lead to the sounds it can make. The two biggest are wave speed and geometry of the object. The geometry determines the wave shapes that the object can support (often called modes). For circular geometries you can expect modes that look like bessel functions, for rectangular geometries the modes look like sine waves. Regardless of what the modes look like, though, the object itself will oscillate sinusoidally creating a tone.

The wave speed will determine the frequency of those modes. An object might have multiple wave speeds, each corresponding to a different form of motion. A solid metal rod, for example, will have one wave speed associated with longitudinal waves (the bar compressing), another wave speed associated with shear waves (the bar bending), and another associated with rotation. Putting mode shapes and wave speeds together, you can find all of the frequencies associated with an object.

That only answers part of your question, however, since knowing the frequencies of harmonics only tells you so much about the sound. The initial amplitude and rate of decay of the modes will make a big difference for how the object sounds.

Initial amplitudes for the harmonics is relatively simple, since it's really associated with how you are creating the sound in the first place. For example, if you hit a xylophone key, where you hit the key is going to determine which modes are initially excited. If you are playing a reed instrument, the frequency of vibration for the reed is going to make a difference.

To understand the rate of decay for the modes you really need to start looking into the chemistry and the structure of the materials. The decay essentially comes in when processes turn sound into heat, and the differences for each harmonic either have to do with the time scales or length scales of those processes. Viscosity plays a part, thermal transport plays a role, molecular relaxation is involved, etc. For example, the crystal dinnerware rings more than an analogous piece of unleaded glass because the crystal has a more regular molecular structure. A metal key from a glockenspiel rings ""truer"" than a wooden xylophone key because metal can deform with less loss to heat.",null,0,cdgkmik,1qst72,askscience,new,2
ISkipLegDayAMA,"It's deceiving because a typical heating coil on, say, a stove top typically has three layers - the inner metal coil (to conduct electricity and generate heat), a layer of ceramic insulation around that (electrical insulation), and then a metal sheath around the insulation. So even though what you see is a metal coil on your stove top, it doesn't pose an electrical risk.

Interestingly enough, even if you were to touch a current-conducting coil, it wouldn't necessarily shock you. The reason for this is because the electrical resistance of the coil is much lower than your body, so electricity still prefers the metal wire to your hand. In order to short the current, it would have to go through your body, down through your rubber shoes and into the ground - a much higher resistance path than the metal wire. Now if you were standing barefoot on a metal plate when you touched it, you might feel a shock... but then I would probably ask what you were doing barefoot with live electrical wire. 

Edit - As a couple others have mentioned, there's a bit more to handling voltage sources that I've left out, such as how close you are to the hot (higher potential) end when you touch the wire. In a wall outlet, for example, the two connections are at 120 V and ground. If these are connected by a simple wire then the voltage is dropped evenly along the length of the wire. So if you touch the wire near the 120 V end, then 120 V is what your body will feel (which will hurt, a lot). If you touch the wire closer to the 0 V end then you likely won't feel anything.

I should have been more clear in my initial explanation, I just didn't want to get too sidetracked talking about voltage sources. Please don't handle exposed electrical parts unless you know what you're doing.",null,21,cdg4it0,1qss3e,askscience,new,65
thefourbees,"Generally, the heat is produced by a conductor encased in an insulator (like ceramic). Since the external part is not a conductor (well, not a good one) it doesn't shock you when you touch it. Ceramic retains heat well, but doesn't conduct electricity.",null,4,cdg38z2,1qss3e,askscience,new,13
classicsat,"As said, stoves and such have the heat element in a clay/ceramic, which is in a tube. 

Ideally the tube grounded so it is no electric hazard, should the clay insulation fail.

On the other hand, hair dryers have an open element which can be an electric hazard, so have shields and whatnot to protect against electrocution, and sometimes have a GFCI as well.
",null,0,cdge19c,1qss3e,askscience,new,2
tauneutrino9,"It is possible in select circumstances.  These are in decays that go by internal conversion.  Since the decay depends on electrons, changes to the electronic environment can change the half life.  This has been seen in numerous isotopes.  U-235m is an example.  

The reason why this is not true for most decays is because the decays depend on characteristics of the nucleus.  It is very hard to change aspects of the nucleus that matters for decay because the energy levels involved are usually in the keV to MeV region.  Those are massive shifts.  That is unlike shifting electronic shells around, which have energies in the eV region.  So intense magnetic or electric fields can easily change the shell structure and thus the rates of electronic decays.",null,44,cdg4bwl,1qsrho,askscience,new,268
xxx_yyy,"Decay rates depend on:

* The strength of the process (the ""forces"") producing the decay.
* The number of states available to the decay products. This is largely determined by the energy released in the decay (more energy -&gt; more available states).

For nuclear processes, we can't control the former, and it is very unusual to be able to control the latter.  There are a few exceptions, such as [Dysprosium-163](http://prl.aps.org/abstract/PRL/v69/i15/p2164_1),where ionizing the atom has a dramatic effect on its decay rate.",null,2,cdg48px,1qsrho,askscience,new,19
SingleMonad,"For nuclear decays which proceed electromagnetically, can't you stimulate them with EM radiation at the transition frequency?  It would be next to impossible to do in practice, of course, but in principle at least...?

Nuclear's not my field, so it would be nice to hear from a specialist on this.",null,1,cdg6ybc,1qsrho,askscience,new,6
ehj,"It is, according to theory, possible to alter the speed of radioactive decay. One can induce beta decay if the nucleus is in an electric field or magnetic field which approaches the Schwinger limit. The Schwinger limit is the electric field strength at which one must take corrections from Quantum Electrodynamics into account in Electrodynamics. It is a huge field strength. For a magnetic field it is 4.4 billion Teslas. Compare this to for instance an MRI which has between 0.5 and 3 Teslas. Such fields, although, are present at for instance Magnetars or in high energy processes in an accelerator.
The specific process of beta induced decay due to a strong electric field has not yet been measured, due to the difficulty of reaching the neccesary field strengths, but it is not impossible that we can do this in an experiment in the future. See for instance
http://www.jetp.ac.ru/cgi-bin/dn/e_058_05_0883.pdf
For the theory explaining this phenomenon.",null,1,cdgamjo,1qsrho,askscience,new,3
skadefryd,"It's actually quite possible in very specific circumstances. For example, rhenium-187 has a beta decay energy of about 2.6 keV. Normal rhenium-187 has a half life of about 42 billion years, but in the lab, fully ionized rhenium-187 has a half life of about 33 years ([sauce](http://prl.aps.org/abstract/PRL/v77/i26/p5190_1)). Of course, good luck finding any fully ionized rhenium-187 anywhere on Earth outside of a physics lab.

There have also been claims that decay rates can vary due to solar activity or throughout the year. These fluctuations are typically on the order of less than a per cent.",null,0,cdgbur5,1qsrho,askscience,new,2
Vod372,"As someone else mentioned it appears that solar activity may influence radioactive decay rates. Now one hypothesis is that neutrinos may be the means by which this takes place, but given how weakly they interact with matter that seems difficult to imagine. 

Regardless of the mechanism by which it takes place though it would nonetheless be a potentially revolutionary area of research that could allow for the safe elimination of all nuclear waste. ",null,0,cdgfl4m,1qsrho,askscience,new,1
nexusheli,"Related question; isn't sped-up decay what is essentially a nuclear bomb?  I've always understood it that way, with particles naturally decaying being deflected back through other radioactive particles knocking them free ad infinitum until *boom*.  ",null,3,cdgbln8,1qsrho,askscience,new,3
null,null,null,13,cdg5pzk,1qsrho,askscience,new,4
charliedarwinsfather,"So we know what part of our body is injured. If I had a gash on my back but didn't feel my back as the source for that pain, I wouldn't know about it. Once I do, I can take proper precautions to protect the wound. 

I'm guessing this is a trait we've developed through evolution but I'm certain someone else will have a better answer for you.",null,1,cdg2y6g,1qsqn5,askscience,new,6
CarbonWeAre,"Basically because while pain is interpreted in the brain, it's a response to a specific input from a specific nerve. The peripheral nervous system extends out from the central nervous system like branches on a tree, dividing into ever finer strands. The end of each sensory nerve is a receptor of some type (like pressure or temperature differential) and when an input excites the receptor then it signals the central nervous system, and your brain knows which nerves correspond to which body parts. ",null,0,cdg55ba,1qsqn5,askscience,new,2
aggasalk,"you're perception of ""place"" (including [all the places of your body] (http://en.wikipedia.org/wiki/Somatosensory_cortex)) is also in your mind, so it all works out.",null,0,cdhmxs0,1qsqn5,askscience,new,1
TevashSzat,"The skin is made up of cells just like the rest of your body. The epidermis can be divided into 5 layers though and are as follows from superficial to deep - stratum corneum, stratum lucidum (but only in thicker parts of your skin such as the soles of your feet), stratum granulosum, stratum spinosum, and stratum basale. Further deeper than that you have the dermis followed by the hypodermis.

Part of the reason that the skin is tough is due to the secretion of keratin proteins and lipids mainly within the spinosum and granulosum layers. The keratin undergoes alot of processing and eventually aggregates, giving the corneum layer its tough nature.",null,2,cdg3w2n,1qsqe7,askscience,new,12
KarlOskar12,The volume. Stretch receptors in the stomach tell the hypothalamus via the vagus nerve that it is full and that you should stop eating. But there are many other factors influencing [hunger](http://en.wikipedia.org/wiki/Hunger_%28motivational_state%29) and appetite. That link is a good place to start if you would like to know about the other factors.,null,2,cdg58cf,1qsowa,askscience,new,14
auraseer,"When you have a hematoma, the thing ""the body is trying to do"" is already over. 

A hematoma is just an area of blood that leaked out of a broken blood vessel. Once clotting occurs and the vessel has healed itself, you still have this blob of blood sitting there in the tissue. That's what a hematoma is. It doesn't have a purpose. It's wasted material, and the body will eventually break it down and resorb it.

I suppose heparin could somewhat speed up the process of breaking down that blood. However, I've never seen heparin given for a hematoma. I would think that its risks would usually outweigh the benefits. 

A superficial hematoma is a minor problem that doesn't need this heavy-duty drug. A hematoma in a critical area like the brain needs surgical intervention. Either way heparin would increase the risk of further bleeding without giving any very large benefit. ",null,0,cdg6vd9,1qsoh8,askscience,new,6
mobilehypo,"Hey Guys! Friendly neighborhood mod here. This is a great question, but let's not focus on the whole ""Homeopathic remedies"" part of this question. We're not here to debate that. We're here to talk about heparin!

Cheers!",moderator,1,cdg2uso,1qsoh8,askscience,new,6
TevashSzat,"I believe the heparin found in gel forms are usually low molecular weight forms so they should penetrate the skin more easily.

Heparin is capable of binding to histamine which sequesters it from triggering the inflammatory response. Among other things, histamine causes vasodilation and increases the permeability of capillaries near the site of injury. This leads to edema at the site which is the swelling that you would see. ",null,1,cdg8i37,1qsoh8,askscience,new,5
clessa,Heparin applied topically is usually an anti-inflammatory in the setting of sports injuries. I would like to see a study that says it does anything for hematoma resorption because I certainly can't find any.,null,0,cdgydzi,1qsoh8,askscience,new,2
6c1,"Edit: My answer was wrong, I was mixing up two vaguely similar properties. See the replies to this comment for the real answer.

~~Number theory student here.~~

~~The property you're referring to is called normality --- so, a number where you can find any combination of digits in its digital expansion is called a normal number.~~

~~The basic definition of normality is that, for an arbitrary positive integer i, the probability of a particular digital sequence of length i appearing in the first n digits of the digital expansion of x is the same no matter which digital sequence you picked.~~

~~Pi has not been proven to be either normal or nonnormal. It is transcendental, but normality is a stronger condition.~~

~~However, there are plenty of other normal numbers out there, and even plenty of ways of generating normal numbers. For example, if you concatenate the primes (like 0.235711131719232931...), that's normal.~~

~~The type of alternative memory storage you describe is actually a project I've been working on (on and off) for a few years. Here's the project page on Github: http://6c1.github.io/NormMem/~~

~~You would basically need to store three things:~~

* ~~A reference to a normal number~~
* ~~The starting index for the data in that number~~
* ~~The length of the data~~

~~The two primary difficulties are~~

* ~~That second number gets big extremely fast~~
* ~~Generating arbitrary digits of a normal number via indexing is difficult.~~
",null,10,cdg301q,1qsnvr,askscience,new,34
ShittyEverything,"http://www.askamathematician.com/2009/11/since-pi-is-infinite-can-i-draw-any-random-number-sequence-and-be-certain-that-it-exists-somewhere-in-the-digits-of-pi/

It hasn't been proven whether it contains all possible finite sequences or not.",null,3,cdg1zp8,1qsnvr,askscience,new,17
Helen___Keller,"&gt;In theory, if we had enough of pi, instead of storing individual files, could we have references to indexes of pi? As in, this song starts at decimal 28k and ends at 280k?

To answer this specific part of the question, if we assumed pi is normal, this would be possible but not feasible. It is definitely possible to store a file with two extremely large numbers (the length of the file and a pi index) and then run an algorithm that calculates pi between those decimal places.

 There are two main reasons this is not feasible, though:

First, and most obvious, is the time required to both store the file into pi. It's an absolutely massive search problem to find a specific sequence of that length in pi. In fact, the search would be exponential in the length of the file you are storing. To give you an idea of what that means, each additional bit of data you need to store could as much as double the search time.

As a result, storing even a JPG picture in pi would probably be beyond all of humanity's processing power.

As demonstration, you can actually search for digits on this website http://www.subidiom.com/pi/

I tried searching ""12345678"" then ""123456789"" then ""1234567890"". The first one took 1.3 seconds, the second took 3.8 seconds, and the third took 14.7 seconds. (of course, this isn't rigorous since it's kind of random, but you get the point.)

The second issue, which is even a bigger problem, is that we wouldn't gain anything (space-wise) from storing these numbers. That is, if we found the index of pi that stored our movie, the amount of data to store this index is, in expectation, the same as the amount of data to store the movie itself.

If you're interested in why I can make a sketch of a proof that uses basic probability (and assumes the digits of pi are random, which is clearly not the case, but it may as well be if pi is normal). I could probably make a more formal proof without this assumption, but I haven't tried before.",null,0,cdg7nh9,1qsnvr,askscience,new,7
UniversalSnip,"I'll try not to get into specifics so that more knowledgeable people can fill in details instead of correcting my errors: there are many, many numbers that possess the property you are driving at, which is, in a more carefully delineated form, called normality. Unfortunately, we currently have no way to demonstrate that specific numbers are actually normal, so whether pi is or not, we can't 100% say.

Assuming it is normal: a difficulty with the scheme you describe is that the more characters you want to 'shorthand', the deeper into pi you'll have to look to find your particular sequence: each additional digit makes the sequence rarer and rarer and more likely to be far past the decimal. So as your sequence gets longer, so do the numbers you're using to mark it out.",null,1,cdg2c3r,1qsnvr,askscience,new,4
muntoo,"Let's say we have a number consisting of 0's and 1's. However, our number is special and follows a particular pattern:

    0.1001000100001000001...

(1 0, 1 1, 2 0s, 1 1, 3 0s, 1 1, 4 0s, 1 1, 5 0s, 1 1, ...)

This number will never contain the sequence `11` and yet it will never repeat either.",null,8,cdg92wi,1qsnvr,askscience,new,1
thilly1,"In quantum mechanics conservation of information is phrased as ""unitary evolution"" or you might say the conservation of probability. I think typically you'd consider this a postulate of the theory rather than a consequence of some symmetry, but it can be related to (for example) the ""phase symmetry"" of the wave function: only the magnitude of the state vector matters, not its phase in the complex plane. ",null,1,cdg2i60,1qsmys,askscience,new,8
fehilz,"Not really, not in the typical sense of Noether's Theorem being used to derive conservation laws. This is because [information](http://physics.stackexchange.com/questions/2685/what-is-information) is not really the same kind of thing as time and location which do produce conservation laws (of energy and linear momentum) when the Lagrangian is symmetric (read: invariant) in changes of either of them.

Funnily enough, there's a post on the physics stackexchange that is almost word for word your question with a really good answer:
&gt;1) If you want a Noether theorem for information, there is no such thing.

&gt;Trying to obtain it from a symmetry law, by Noether's theorem can't work, simply because information is not a quantity that can be obtained for instance by the derivative of the Lagrangian with respect to some variable. Information is not scalar, vector, tensor, spinor etc.

&gt;2) Another way to obtain conservation laws can be found in quantum mechanics. The observables that commute with the Hamiltonian are conserved. Again, you don't have an observable, in the sense of quantum mechanic, for information.

&gt;Trying to obtain conservation of information from commutation with Hamiltonian can't work, because there is no observable (hermitian operator on the Hilbert space) associated to information. Information is not the eigenvalue of such an operator.

&gt;3) The only way, which also is the simplest and the most direct, is the following: to have information conservation, when you reverse the evolution laws, you have to obtain evolution laws that are deterministic. This ensures conservation of information, in fact, they are equivalent. In particular, most classical laws are deterministic and reversible. Also, in quantum mechanics, unitary evolution is reversible, giving you the conservation of information.&gt;

By Cristi Stoica from [here](http://physics.stackexchange.com/questions/41765/is-there-a-symmetry-associated-to-the-conservation-of-information).",null,0,cdgcxgu,1qsmys,askscience,new,5
ididnoteatyourcat,This question was asked on stack exchange and the top voted answer is quite good. See [here](http://physics.stackexchange.com/questions/41765/is-there-a-symmetry-associated-to-the-conservation-of-information).,null,0,cdgcqfy,1qsmys,askscience,new,2
abstrusey,"Veterinarian perspective here: we have bred this into chickens, AND eye laying rate is related to nutritional intake. So, we're selecting for the layingest of layers, and feeding them the perfectly formulated feed for maximum production. The average layer hen is actually VERY skinny, because all of their resources are going toward eggs. If they can't keep up with the production, their production levels usually go way down before their health does.",null,68,cdg5smd,1qsmuy,askscience,new,295
atomfullerene,"For chickens, the main reason is because humans have bred them to lay large numbers of eggs whether or not fertilization occurs.  Many egg laying reptiles and birds will occasionally lay unfertilized eggs.  The unfertilized egg has to be produced regardless, so it will be available to _be_ fertilized.  Sometimes these eggs get reabsorbed if they are not fertilized, sometimes they are just laid whether or not they are fertilized.  

You may ask ""why not just absorb all unfertilized eggs"" but remember that this carries some risk.. a mechanism to do this could accidentally absorb _fertilized_ eggs, which in some cases may be worse than passing the rare (in the wild) unfertilized egg.  After all, passing an unfertilized egg just costs some energy, absorbing a fertilized one directly reduces your reproduction rate.",null,70,cdg66ev,1qsmuy,askscience,new,276
null,null,null,12,cdg5epx,1qsmuy,askscience,new,47
westminsterabby,Think of an egg as an encapsulated menstrual cycle. Human females have one a month and no one expects them to 'hold on' to that investment until it's fertilized. Chickens have been selectively bred to have a menstrual cycle once a day. It gets encapsulated and delivered to your breakfast plate thanks to encouraged evolution. ,null,11,cdg5fvz,1qsmuy,askscience,new,41
KrunchyKale,"Evolution is a lazy force, and it almost never gets things done at peak efficiency. 

Since the chance of laying an unfertilized egg in the wild is pretty low, there's no reason to not make an egg whenever it's the appropriate egg-laying time. ",null,14,cdg544c,1qsmuy,askscience,new,31
rohrspatz,"&gt;Come to think of it, don't humans do the same thing every month?

This is true, but for us, a single egg isn't a big investment. It's only a single cell. For mammals, the big energy/nutrient investment is gestation, so menstruation isn't a big enough deal to have been eliminated by evolution.

(Also, in the wild, most mammals of childbearing age are pregnant like.. all the time, whenever possible. The same goes for humans up until maybe the last few centuries. So I'm bettting that part of the reason is simply that menstruation doesn't/didn't really ever happen frequently enough to become a selective pressure.)",null,1,cdgep3i,1qsmuy,askscience,new,2
null,null,null,11,cdge1kw,1qsmuy,askscience,new,4
squirrelandpeanut,"Both moths and butterflies belong to the order Lepidoptera. Butterflies actually evolved from within the moths, they're like a specialized group of moths, making the moths a paraphyletic grouping while all butterflies share a common ancestor. Morphologically I usually use the antennae to tell them apart, it's the most obvious and definitive visual characteristic. Moths have variety in antenna shape, but are usually filamentous, or sometimes feathery in males which is used for pheromone detection. Butterflies always have a club at the end of the antennae. Size and colour is really variable in both groups.
 
So basically the difference is evolutionary, and morphological. Behavior is really variable, there might be some trends but have a look at the moths around your outdoor lights at night and you might find some butterflies, and tons of moths visit flowers during the day.",null,0,cdg6yod,1qsm5i,askscience,new,6
TaslemGuy,"[Wikipedia](http://en.wikipedia.org/wiki/Comparison_of_butterflies_and_moths) has an article overviewing the differences between them.

There isn't universal agreement on what the distinction is, but there are a few morphological ways to identify them. 

Their antennae:

&gt;Most butterflies have thin slender filamentous antennae which are club-shaped at the end. Moths, on the other hand, often have comb-like or feathery antennae, or filamentous and unclubbed. This distinction is the basis for the earliest taxonomic divisions in the Lepidoptera: the Rhopalocera (""clubbed horn"", the butterflies) and the Heterocera (""varied horn"", the moths).

Their wings:

&gt;Many moths have a frenulum which is a filament arising from the hindwing and coupling (matching up) with barbs on the forewing. The frenulum can be observed only when a specimen is in hand. Some moths have a lobe on the forewing called a jugum that helps in coupling with the hindwing. Butterflies, however, lack these structures.

Their pupae:

&gt;Most moth caterpillars spin a cocoon made of silk within which they metamorphose into the pupal stage. Most butterfly caterpillars, on the other hand, form an exposed pupa, also termed a chrysalis.

However, there are exceptions to these, so they're not perfect. 
",null,0,cdg47c0,1qsm5i,askscience,new,1
neha_is_sitting_down,"adding to the other answer, it will also feel colder than room temperature even if it is at room temperature.  This is because glass, metal, and water have higher specific heats than air (they hold more heat energy). Because of this, they will stick more heat out of you when you touch then than air and so they will feel colder. Think about a slightly chilly day. The air might be a bit cold, but it's not too bad. However, you definitely wouldn't want to go swimming in an unheated outdoor pool because the water would feel very cold. The water would actually be near the same temperature as the air, but it would feel colder. ",null,3,cdg3yov,1qsm05,askscience,new,14
TazDevil5,"Because there hasn't been enough time to warm it up. Heat transfers through convection, conduction and radiation. Assuming no major sources of radioactive heat in your room, convection and conduction are what's going to move the cold outta that there bottle. 

Conduction - you set the bottle on a table in the room, at room temperature. The conduction occurs where the bottle meets the table - relatively small area. Unless the table is really hot, there won't be a whole lot of heat moving into the bottle that way. Additionally, as the area the bottle is resting on cools, the heat transfer slows because heat transfer increases with increased temperature differential. as the surfaces get closer to the same temp, the rate of temp change slows.  

Convection is the big one here - it's the air around the bottle. In your case, we can assume the air is relatively still because if you'd have aimed a fan at that bottle a few hours would have probably evened out the temp. Similar to the table top, the air immediately around the bottle will cool down as it adds it's heat to the bottle. As it does so, the rate of transfer slows due to the lower delta T between the air and bottle. But if you blow a fan across it, there will always be the warmer, room temperature air contacting the bottle, transferring the heat faster.",null,3,cdg3cqw,1qsm05,askscience,new,9
ISkipLegDayAMA,"The rate of heat in (or out) of an object is proportional to the temperature *difference* between it and its environment. So as the water bottle warms up and gets closer to the temperature of the air around it, the temperature difference decreases and the rate at which its warming up actually slows down. 

In addition, what neha_is_sitting_down said is very true. Some objects will feel colder even though they're the same temperature. This has to do with how the human body ""feels"" temperature and heat, and may be a factor in  determining how cold the bottle of water is. ",null,0,cdg8j6r,1qsm05,askscience,new,5
yeast_problem,"I'll go with neha_is_sitting_down for the general perception of coldness. However if the beverage is exposed to air, water will evaporate from it reducing its temperature until it approaches the wet bulb temperature, which is always lower than the room temperature, how close depends on the humidity of the air and to some extent the rate of air movement at the liquid surface.",null,0,cdgj6sc,1qsm05,askscience,new,1
aggasalk,"1. best possible human [visual acuity] (http://en.wikipedia.org/wiki/Visual_acuity) is around 20/10 (or 6/3); with this acuity you can discriminate between details or spots that are just one half arcminute of visual angle across (and separated by the same distance, so the limit of resolution is 1 discrimination per arcminute). you're going to need two good eyes to get such good resolution (basically for binocular noise correction).

2. the closest distance to your face you can converge both eyes and keep focused images on your retinas (assuming you're young and can still accommodate and have normal binocular vision) is (this is an educated guess) about 5±1cm.

3. with a little trigonometry (tangent of the angle of minimum resolution, times the speck distance, or 5cm*tan{1arcmin}), the tinest speck you can discriminate from other specks is 0.0015 cm wide, or about 15±3 microns. this is the size of a big [cell] (http://en.wikipedia.org/wiki/Cell_%28biology%29) (e.g. human eggs are much bigger than this), and a little bigger than your typical bacterium.

**this is lots of atoms, i don't know how many**.

(this might sound amazing, and it would be dependent on perfect lighting and other sorts of conditions, but if you change the viewing distance, it's a little more believable. for example, under these perfect conditions, someone with 20/10 vision could see, at 1 meter, the spaces between the millimeter marks on a ruler. impressive, but not unbelievable..)",null,8,cdg4szq,1qskht,askscience,new,19
dogofsteel,"OP did not indicate what the ""object"" needed to be made out of. Because we don't actually see little bits of objects themselves, only reflections from those objects. I think a better fundamental question for OP should be:

""Whats the smallest quantum of photons that are perceivable by the human eye"" 

As OP's other question is bogged down in difficulties such as calculating net flux off of a variety of different materials in different lighting conditions, at different angles and wavelengths, and a multitude of other problems that the_european, and aggasalk have outlined. 

Turns out the answer to the question I suggested, is 1. Under ideal viewing conditions, the smallest object a human eye can detect and report to the brain to be noticed, is 1 photon of light. 

For a brief primer read this wiki article: [Absolute threshold](http://en.wikipedia.org/wiki/Absolute_threshold).",null,2,cdgd9xt,1qskht,askscience,new,4
airbornemint,"It depends on what you mean by ""winter"". The trigger for most annual cycles (including shedding of leaves) is the daily cycle of sunlight.

So, if you had a season in which days got shorter, but temperatures didn't get cold, then yes, trees would lose leaves. This is what happens when you have a mild winter — trees lose leaves on schedule.

If you had a season in which days  didn't get shorter, but weather got colder, then trees wouldn't lose leaves; at some point the leaves might freeze and die off though, depending on the temperatures. This is what happens during a cold snap that precedes onset of winter — plants don't start to lose leaves, but the leaves are often damaged.",null,1,cdg9yns,1qsji8,askscience,new,3
No_Kids_for_Dads,"All things being equal, shouldn't make a difference. The same surface area should hold the same weight regardless of distribution.

But if the thing you're hanging has an unequal distribution of mass, you're better of redistributing the adhesive to the heavier area.

The most effective thing you can do is clean both surfaces very well, finish with isopropyl alcohol and let evaporate",null,1,cdg53cy,1qsj3x,askscience,new,5
Flea0,"smaller chunks have the added benefit that as the corner of one piece of tape comes off with time and wear (especially in humid places), the detached tape won't be able to provide leverage for more tape to be detached. basically, an interruption in the length of tape makes it more difficult for the detachment to travel along the tape.",null,0,cdgb75q,1qsj3x,askscience,new,4
spinningspinning,"If you ignite it, yes (or heat it enough to autoignite).  Just sitting around at room temperature there's not enough energy for it to overcome the activation barrier necessary to become water.  ",null,3,cdg3d20,1qsiwf,askscience,new,21
chuck10470,"Water is made every day. Just look up at the jet contrails in the sky. Condensation trails are water vapor made from jet engines burning fuel and making water (among a lot of other gasses), which leaves the jet engines as stream and then condenses in the thin, cold atmosphere 27,000 feet up. One of the byproducts of nearly every fire is water vapor. 

In your scenario, with the room full of hydrogen and oxygen, you would create water, but only after you struck a match (providing the activation energy) causing a tremendous explosion. The walls of the room (if it's still standing) would, however, be covered with water droplets. ",null,2,cdg4qzn,1qsiwf,askscience,new,7
JohnSmith1800,"Muscles are first stimulated by a motor neuron. This in turn is triggered by the central nervous system. I wont go into too much detail here, but if you want more, feel free to ask.

*How Muscle Fibres Are Grouped*

The motor neuron meets the muscle fibre (an elongated cell with multiple nuclei) at what is known as a ""motor endplate"", this a region of the muscle fibre with greater sensitivity to neurotransmitters. A single motor neuron can stimulate a handful of muscle fibres, or hundreds, together this is called a motor unit. In turn, a muscle such as your bicep is composed of many motor units. This separation into smaller subunits allows you to have greater control of force, as a muscle fibre contraction is only somewhat graded.

*Muscle Fibre Contraction*

A muscle fibre contains several long strands of ""myofibirils"", each of which is composed of subunits called ""sacromeres"" stacked on top of each other. Each sacromere contains strands of actin (thin filament) and myosin (thick filament), these filaments are composed of repeating protein subunits. Each thick filament has several thin filaments running parallel. When contraction occurs, what are known as ""myosin heads"" grab on to the actin strands. ATP (the cell's power molecule) is then hydrolised, and the head bends, pulling the actin filament along. Another myosin head then grabs on, the original releases and the process continues. This is happening at multiple points in each sacromere, and in each sacromere along each myofibril. So although the individual force from a myosin head power stroke is small, the overall effect is significant. The myosin head binding/release process is called cross bridge cycling.

*What Triggers Contraction*

When the motor neuron undergoes an action potential, neurotransmitters are released. These cross a small gap (synapse) between the end of the motor neuron, and the motor plate on the muscle fibre. On the motor plate are receptors which the motor neuron binds to. These receptors are in turn linked to calcium ion channels inside the cell. These channels are on what is called the sarcoplasmic reticulum, which has a much higher concentration of calcium than the rest of the cell. When the channels open, this rushes out into the cell proper. It binds to other proteins which are normally running alongside the actin filaments (troponin complex, and specifically troponin c). When calcium binds to troponin c, the complex changes shape slightly, and myosin heads are able to bind.

Calcium is constantly being pumped back into the sacroplasmic reticulum, and the channels don't stay open long. This means that even though enough Ca^2+ is released to bind all troponin c complexes, a single fire of the neuron doesn't trigger a full contraction. Instead, it is what is called a ""twitch"" and only occurs in the blinking of the eyelid (as far as I'm aware). What usually happens is that the motor neuron is stimulated repeatedly, which keeps calcium levels high, and produces a greater muscle fibre contraction.",null,1,cdg55a0,1qsiik,askscience,new,10
JohnSmith1800,"One way mirrors don't actually exist, it's a common fallacy. What you see in crime shows (if they don't just fake it), is a mirror which is partially transparent and partially reflective (more reflective than your regular window, but not completely reflective like a mirror).

To create the one-way effect, the closest you can get is if you have a dark room on one side of the glass and a well lit room on the other side. The well lit side will mostly see their reflection in the pane of glass, but will be able to make out dim outlines of people in the other room. Conversely the people in the dark room will mostly see into the other room, but will have a faint reflection of themselves visible.

Given this, you can't have a ""one way mirror"" ball. If you placed a ball of partially reflective glass in a room, the light levels inside and out will be equal, and you'll see reflections (of other parts of the room from inside the ball) and a reflection of yourself looking at it.",null,0,cdg4ozg,1qsi38,askscience,new,11
rupert1920,You'll find that [this question is very common](http://www.reddit.com/r/askscience/search?q=sphere+mirror&amp;restrict_sr=on&amp;sort=relevance&amp;t=all). You'll also notice that there are tips on how to look for repeat questions in the sidebar to the right - though in this case it seems it was ignored.,null,0,cdgdiel,1qsi38,askscience,new,1
DrBenedict,"I'm downvoting you because of the ""probably gonna get ignored tho"" bit.  But here's the answer.

There is no such thing as a ""one way mirrors material"" -- the way ""one way mirrors"" work is by having a surface that reflects half of the light hitting it and lets the other half through.

If there's a brightly-lit criminal on one side, and a poorly-lit cop on the other, the poorly-lit cop sees a brightly-lit criminal, overwhelming the reflection of the poorly-lit cop.

The brightly-lit criminal, on the other hand, sees a bright reflexion overwhelming the poorly-lit cop.  The point is, there's no difference between the two sides of the glass -- if you took it out and turned it around, the criminal would not be able to see the cop.

The cube you're referencing ( http://imgur.com/a/FSDai ) has bright lights inside it and is in an otherwise dimly-lit room, so the reflections of those lights get reflected many times.

The sphere you're hypothesizing would look like a part-mirror, part-see-through, funky thing, depending on how bright the room was.",null,1,cdg4qvb,1qsi38,askscience,new,1
tauneutrino9,"When you buy stuff that is ""anti-rad"" like potassium iodide pills they are not protecting you from radiation.  What they are doing is they are giving you a high dose of iodine in order to prevent your body from up taking any radioactive iodine in the environment.  So in the case of KI pills, they only protect you from a very specific set of isotopes.  ",null,1,cdg47zo,1qsgxm,askscience,new,6
The_Write_Stuff,"You don't ""metabolize"" ionizing radiation, even if you ingest a source (there have actually been several people who have done that).  Your cells have mechanisms to fix the damage done by radiation, but they're not unique to damage done by radiation, they'll fix damage done by anything.  

I'm not aware of anything you can eat that shields you from most types of radiation.  ",null,1,cdg3c6e,1qsgxm,askscience,new,3
TangentialThreat,"Radaway is purely fictional.

There's ionizing radiation (x-rays, beta particles etc) that damages cells directly like shooting them with a tiny bullet, and then there's radionuclides which are unstable atoms that are likely to emit ionizing radiation at some point in the future.

There's nothing you can do about a blast of ionizing radiation except put something dense like lead or water between you and the source. Anything badly irradiated just stops living and treatment is mostly palliative. A nuclear bomb will also scatter a fair amount of radioactive dust and ash that's full of radionuclides, aka fallout.

The best approach to fallout is to never allow any of it to get onto or into your body where it will be much more dangerous. Shower thoroughly, keep your shelter free of dust, wear an airtight suit, try not to eat or drink anything contaminated etc. 

If it does get in then your body will try to incorporate it. You really don't want radioactive calcium (or radium, which mimics calcium) merged into your bones, or radioactive iodine in your thyroid. Radionuclides in your body will cause long-term damage.

Potassium iodide is given to keep your body from feeling the need to incorporate any more iodine if you are expecting iodine-131. There are also [chelating agents](http://www.ncbi.nlm.nih.gov/m/pubmed/19492929/) that try to grab on to loose radionuclides in your blood so they can be disposed of through the kidneys; this is not super-efficient and failed to save Alexander Litvienko.

Future vault-boys should shower every day, take their vitamins, keep a clean house. I would also recommend watching all the old Civil Defense filmstrips which are full of helpful advice for surviving the unsurvivable.
",null,0,cdgka29,1qsgxm,askscience,new,1
Count_Spatula,"Any particular radioactive isotope of any given element can exist as part of a molecule your body might want to absorb.  For example, PET studies are done with FDG, which your body thinks is glucose (sugar).  It has a radioactive atom of fluorine that spits out a positron, which is detected when it decays.  It's used when an image is needed of something that uses a lot of glucose, like your brain.

http://en.wikipedia.org/wiki/Fludeoxyglucose_(18F)

Some elements, such as radium, act like others that are similar to it (in this case, calcium).  This means that your body may shuttle them to particular places, such as your bones.  There it sits, and since it's decaying from inside your body, it's much more likely that the radiation will hit your body and cause some damage.

http://en.wikipedia.org/wiki/Radium_jaw

Alpha particles, for example, cannot penetrate your skin.  However, if you ingest an alpha source, you now have it floating around inside your body, where the massive alpha particles can do quite a bit of damage in the very short range they travel.

http://en.wikipedia.org/wiki/Poisoning_of_Alexander_Litvinenko

The speed at which your body ""gets rid of"" whatever depends on how fast it biologically clears whatever form it is in and how fast the isotope decays.  In the case of long-lived ""bone seekers""  this can be quite a long time, indeed.

http://en.wikipedia.org/wiki/Biological_half-life",null,0,cdgl8ie,1qsgxm,askscience,new,1
THE_SKULK,"Itching, also known as pruritus, starts with some kind of external stimuli, including bugs, dust, clothing fibers and hair. Like tickling, itching is a built-in defense mechanism that alerts your body to the potential of being harmed. In this case, it might be the potential of being bit by a bug.

more: http://health.howstuffworks.com/skin-care/information/anatomy/question600.htm",null,1,cdg60hn,1qsgmn,askscience,new,17
S7R4nG3,"The kick would make you fall faster for a short period of time before returning back to terminal velocity. You have to remember that velocity and acceleration are independent of one another. 

So, assuming that were just in the atmosphere with no other forces acting on you other than air resistance and you are falling at your terminal velocity, then your acceleration before the kick is 0, ie you aren't speeding up since you're terminal. After the kick, you would gain an acceleration for a small period of time, slightly increasing your velocity. However, this acceleration will be met by a higher force of air resistance, decelerating you to your terminal velocity.

So yes, you would speed up for a second before the air in front of you slowed you back down to your original terminal speed. (In before crap about magnitudes and whatnot with speed vs velocity)",null,0,cdfs8n9,1qrrw3,askscience,new,10
-Rookery-,The kick would speed you up for a brief moment and you would hit terminal velocity again. A good example of this is when a space vehicle is re-entering the atmosphere. It will slow down to terminal velocity at the given height that it is at no matter what pushes it.,null,0,cdg8pyd,1qrrw3,askscience,new,3
dampew,"Your terminal velocity depends on your weight and surface area, so even if you just tie two people together they're likely to fall faster than a single person.

Anyhow, the answer is yes, that would help, and the top person would probably fall faster anyhow due to the bottom person pushing the wind.  Drafting in cycling works that way -- the person at the back of the peloton is doing the least work, and on descents the person in the back can give the person in front a push.  For instance: http://www.youtube.com/watch?v=0tFpNsZXWgc",null,0,cdh1hm5,1qrrw3,askscience,new,1
Smoothened,"They are actually in separate clades, so not really that closely related. Toucans are in the same order as woodpeckers and are more closely related to eagles, while parrots form their own order and are thought to be more closely related to songbirds. Still, both clades belong to the group of often tree dwelling birds that are related to true passerines. Their last common ancestor likely lived around 90 million years ago, before many of the ""near passerine"" orders seem to have emerged. But you have to bear in mind that the taxonomy of birds is still controversial.

The similarities you observed are likely due to [convergent evolution](http://en.wikipedia.org/wiki/Convergent_evolution), meaning they evolved similarly because they share niches as tropical/sub-tropical fruit-eating medium-sized birds that often live on trees. The zygotactyl feet, for example, provide an advantage to birds that live on tree trunks. It could also be a retention of an ancient trait that was lost in other related birds. ",null,0,cdfsu7o,1qrqss,askscience,new,4
bjornostman,"Luckily there is an excellent online tool for exactly this sort of question: Timetree.org.

Parrot and toucan share an ancestor about 89 million years ago. Quite a long time ago, no?

http://timetree.org/index.php?taxon_a=parrot&amp;taxon_b=toucan&amp;submit=Search",null,1,cdfsqfc,1qrqss,askscience,new,3
norsoulnet,"This question is base on a false premise, specifically that nuclear weapons are tested every year.  This is not true, and in fact there are very few tests performed since most of the world's countries ratified the [comprehensive nuclear test ban treaty](http://en.wikipedia.org/wiki/Comprehensive_Test_Ban_Treaty).  The only major world actors that still test do so underground and consist primarily of North Korea, who is also party to the comprehensive nuclear test ban treaty but has tested 3 times in a many years deep underground.  There are a few other tests commutes since 1998.  See [this article](http://en.wikipedia.org/wiki/List_of_nuclear_weapons_tests) for a list of tests.",null,0,cdfss7d,1qrpla,askscience,new,13
tauneutrino9,"They used to do a lot of testing by actually detonating the device and getting lots of data.  They also would test out devices that would just barely go critical.  However, we no longer test in that way.  Mostly because we voluntarily said we would stop after Russia did.  We signed the CTBT, however it is not ratified.  

Currently, testing is done using both sub critical tests and computer simulations.  The sub critical tests are mostly done at the site formally known as the Nevada Test Site.  Computer simulations are done at LANL and LLNL.  ",null,2,cdfywwe,1qrpla,askscience,new,4
Calkhas,"This is a very important question. You may be surprised to know that the physics of nuclear detonations is rather poorly understood; yet several nations maintain large stockpiles of devices, and military and political requirements are that these devices should act predictably if they are needed.

One key problem is in the opacity of heavy metals. Basically, how much radiation can get through ultra-high temperature uranium? How much is reflected back, how much passes through to the outside, and how much is absorbed to heat the next layer of fuel? You can solve this problem on paper with hydrogen, because hydrogen is a very simple system. You can solve this problem with a laptop computer for light elements like helium or nitrogen. But as you go up to heavy elements, there are too many electrons involved to compute these figures from first principles.

Instead we can use approximations in our computer simulations, but these are not necessarily accurate. The temperatures and pressures involved are almost never experienced on Earth, so we would not expect our rules of physics, which are mostly formulated under normal conditions, to hold at such extremes.

Until the comprehensive test ban treaty one was able to perform detonations on a regular basis to advance this science. The United States occasionally conducts sub-critical tests, where there is insufficient ""fuel"" to produce a big blast, but enough of a reaction takes place to access some of the relevant physics. Even this is politically troublesome.

At the moment the best way to validate our models of nuclear detonations is by using ultra-high-power lasers to heat up and squeeze materials to the conditions we believe are experienced in nuclear detonations. We can then probe these materials with xrays (for example) to see if we get what we expect. Then we can go back and tune computer simulations.

This kind of science is performed at the National Ignition Facility at Lawrence Livermore National Laboratory in the US; at the newly built Orion laser at the Atomic Weapons Establishment in England; and (in the future) at Laser Mégajoule at le Commissariat à l'énergie atomique in France. All these nations have long standing laser science programmes precisely because of the need to understand the physics relevant to their nuclear stockpiles.

Of course these facilities have important peace-time uses as well.

TL;DR: These days mostly computer simulations tuned by laser experiments.",null,1,cdfz0va,1qrpla,askscience,new,3
null,null,null,7,cdfrusa,1qrpla,askscience,new,2
KarlOskar12,"Various animals have different dietary needs in terms of protein (amongst other things). [Cats](http://www.drsfostersmith.com/pic/article.cfm?dept_id=0&amp;siteid=1&amp;acatid=297&amp;aid=547) for example need 22 amino acids and 11 of them are essential. [Humans](http://en.wikipedia.org/wiki/Essential_amino_acid) need 21 and 9 are essential. And we need them in different amounts than cats. So it would make sense that the muscles of human meat (lol) vs. cat meat (or any other animal) would taste differently based on that alone. However, there are many other factors. If you take an animal that isn't very active versus an animal that roams through a large forest the roaming animal will have varying muscle content whether it's more/less slow and fast twitch muscles. Wild turkeys, for example, tend to be a lot tougher if you were to cook them. However, they are also more likely to malnourished than a farm raised turkey which is another factor in the flavor of their meat. Basically, it's not just about what they eat, but their life styles because activity/inactivity will alter the specific content of their muscle (as well as the amount of fat that is present).",null,0,cdfyqyb,1qrp17,askscience,new,2
Dzugavili,"The short answer is no, our nuclear testing has had no effect on the global climate. 

Most of the nuclear tests were in deserts, which typically don't contribute much to the global climate, while only several occurred at sea, which is a giant thermal bath: in short, a nuclear detonation really isn't that big, when compared to the events that would register on this scale, such as an astrological impact, such as a large meteorite or, worst case, a comet.

There was pretty much no concern for the animals, but then again, most of them were doing fine/already being driven into extinction, just as they are now. So, if there was an effect, we never saw anything and it has been washed out.

The long answer, going beyond what you asked, is yes.

The nuclear testing released isotopes of various common elements into the environment. This will complicate various forms of radiometric dating when they try to determine the age of organic objects after this era.",null,3,cdftt4c,1qro8w,askscience,new,19
theansweris7,Not much effect on weather but there are some radioactive elements still hanging out in the stratosphere as a result of atmospheric nuclear testing. http://www.americanscientist.org/issues/issue.aspx?id=982&amp;y=0&amp;no=&amp;content=true&amp;page=6&amp;css=print . Fun times,null,2,cdfu245,1qro8w,askscience,new,7
Bbrhuft,"The reason past nuclear test did not effect climate is because weapons were detonated in regions with little burnable vegetation, so there was no soot generation.

Modern climate modelling shows that, a full scale nuclear conflict would have a devastating effect on climate by generating vast quantities (150 million tons) of fine smoke particles and lofting this soot high into the stratosphere; a global heat reflecting vale envelopes the planet, reducing the heat from the sun, temperatures plummet by as much as 35 degrees C in some interior continental regions.

Summer temperatures in crop growing regions of Ukraine, Russia, the Great Planes of North America may never exceed 0 Celsius (32 degrees F) for over 10 years or more.

And even in a very limited regional conflict, say India v's Pakistan where 0.03% of the worlds global nuclear arsenal is detonated, where ""only"" 5 million tons of soot is generated, global temperatures may drop by ~1.5 degrees C - shortening the growing season, causing regional droughts, increasing food prices and causing severe famines (btw, they say one of the top factors for the Arab Spring was high food prices). So even a small nuclear war is bad news for everyone.

References:

Robock, A. &amp; Toon, O.B., 2012. Self-assured destruction: The climate impacts of nuclear war. Bulletin of the Atomic Scientists, 68, 66–74. http://thewe.cc/thewe_/_/pdf/climate-impact-of-nuclear-war.pdf

Turco, R.P., Toon, O.B., Ackerman, T.P., Pollack, J.B. &amp; Sagan, C., 1990. Climate and smoke: An appraisal of nuclear winter. Science, 247, 166–176. http://www.atmos.washington.edu/~ackerman/Articles/Turco_Nuclear_Winter_90.pdf

Sagan, C., 1983. Nuclear War and Climatic Catastrophe: Some Policy Implications. Foreign Affairs, 62, 257.",null,0,cdg32hz,1qro8w,askscience,new,1
Platypuskeeper,"Well, for starters [we've observed them more or less directly](http://physicsworld.com/cws/article/news/2009/aug/27/molecules-revealed-in-all-their-glory-by-microscope) in electron microscopes. And the amount of indirect evidence is overwhelming, from almost all of chemistry to everyday observations like the sky being blue. (caused by Rayleigh scattering that wouldn't be there if it was some homogenous medium)





",null,0,cdfs0x0,1qro1p,askscience,new,8
spPad,"The easiest way to convince you is to probably show you an image. [This](http://www.ixbt.com/cpu/semiconductor/intel-65nm/gate_oxide.jpg) shows a SEM image of Intel's transistor at a 90nm node from several years ago. the ""silicon substrate"" is pure crystalline silicon. You can clearly see its lattice structure along with the atoms.",null,1,cdfrus3,1qro1p,askscience,new,5
MasterPatricko,"The idea of the atom (discrete units of matter) has existed since the ancient times but it wasn't a proper scientific theory until the 1600's.  The discovery of all the different elements and the periodic table gave indirect evidence for the atom's existence, but the theory only became testable in the early 1900's. There is loads of indirect evidence -- for example, [Brownian motion](http://en.wikipedia.org/wiki/Brownian_motion) was shown by Einstein to be a result of atoms moving. Then the discovery of beta and alpha radiation allowed us to aim beams at materials and see how they bounced (scattered) off. The reflected pattern indicated that materials were made up of atoms, and eventually to work out that even atoms were made up of subatomic particles.

It's not possible to see atoms directly using optical microscopes as they are much smaller than the wavelength of visible light. But in the last half-century we have developed microscopes that work without EM radiation -- like [atomic force microscopes](https://en.wikipedia.org/wiki/Atomic_force_microscopy), [scanning tunnelling microscopes](https://en.wikipedia.org/wiki/Scanning_tunneling_microscope), and [electron microscopes](https://en.wikipedia.org/wiki/Electron_microscope) we can just about see the boundaries of the electron clouds of individual atoms.

Today we can even manipulate structures atom-by-atom -- check out something IBM created last year:

http://www.research.ibm.com/articles/madewithatoms.shtml",null,0,cdfskan,1qro1p,askscience,new,4
SpaceBearon,"First I want to draw your attention to something that you are familiar with. Do you know how an MRI machine works? The intuitive way to understand it--a way that I think everyone understands is that it creates a perturbation in your body, and we measure those perturbations. Apply a magnetic field, and see the perturbation. From this perturbation, we can resolve an image of various body parts. It's pretty cool, really.

We can do (and have done) the same thing with atoms. We take a very fine needle. How fine? Thousands of times finer than needles you see every day (or hundreds, I can't recall the exact amount). Either way. It's damn tiny. And what we do is apply a current through it. We then move this needle around a surface, and pick up a signal from the surface. Similar to an MRI machine, roughly, we perturb the system and analyze it. Using, as others have said, electron microscopes (or scanning tunneling microscopes), we can perturb a very tiny system (on the order of nanometers since the needle is so small), and resolve an image with a similar resolution. Atoms exist on the order of angstroms, or 0.1 nanometers, which is within the resolution range of the microscopes.

In that sense, we can directly observe the atoms. There are so many other experiments that verify the existence of atoms, aside from direct observation as well. I just feel this analog may be the most intuitive. ",null,1,cdg1img,1qro1p,askscience,new,4
Ingolfisntmyrealname,"There are a lot of ways to indirectly showing that atoms exist. One of the more famous examples is the [Brownian motion](http://upload.wikimedia.org/wikipedia/commons/thumb/c/c2/Brownian_motion_large.gif/220px-Brownian_motion_large.gif) of particles, e.g. the seemingly random wiggles of small particles when they are suspended in a fluid like water. In 1905 a certain someone, Albert Einstein whom you might have heard of, showed that this random wiggle is caused by the particle bumping into even smaller particles, which is to say the molecules of the liquid.",null,0,cdfz8gq,1qro1p,askscience,new,2
Keereegs,"You're on the right track:

""When oil prices got high enough five or six years ago, it became profitable to drill and hydraulically fracture tight oil formations in North Dakota and Texas.

What made fracking feasible was that oil prices in the last decade rose from $20 a barrel to circa $100, making the drilling of very expensive fracked wells with very short productive lifetimes feasible...""

http://fcnp.com/2013/11/12/the-peak-oil-crisis-so-why-is-gasoline-so-cheap/",null,1,cdfuzx7,1qrnoi,askscience,new,6
mutatron,"It's complicated. Fracking is costly, but so many people are doing it the [cost of fracking has come down](http://www.ft.com/cms/s/0/ec1a6ba2-34e0-11e3-8148-00144feab7de.html#axzz2kppzEemj), so it still makes sense to continue at current prices.

&gt; PacWest, a consultancy, says that capacity utilisation for pressure pumping equipment dropped to just 74 per cent at the end of last year. As a result, prices for pumping services dropped an estimated 22 per cent between the first quarter of 2012 and the third quarter of 2013.

I assume you're talking about _gasoline_ prices and not _gas_ prices. Gasoline prices have been [more or less level for 3 years](http://www.gasbuddy.com/gb_retail_price_chart.aspx?time=3).",null,0,cdfsmz6,1qrnoi,askscience,new,1
ThePetPsychic,"Forgive me if this is too simple of an idea, but does this also mean that oil prices for the foreseeable future should remain relatively stable? As the shale oil supply is only tapped when prices reach a certain threshold, doesn't that automatically keep it around the same price as long as the amount of supply is the same?",null,0,cdggx1g,1qrnoi,askscience,new,1
chuck10470,"Shale oil is expensive to recover. You're lucky to get 10% oil by weight, or 200 lbs of oil for every ton of shale mined. Then the thick, dirty shale oil has to be cleaned and pre-treated prior to being piped to a refinery. Because it's so thick at ambient temperature,  it is typically diluted with benzene so it will flow through the pipeline. Cracking the long chain molecules in the refining process takes more energy and is, thus, more expensive.

No refining company was evergoing to invest in this technology and infrastructure before oil prices rose high enough to make it profitable. And then it takes time to get it all set up. The availability of shale oil and tar sands is part of the reason gas prices stabilized. But since gas prices aren't strictly a supply and demand marketplace,  it's difficult to determine exactly how much either source provides to overall price stabilization. ",null,1,cdg5bs4,1qrnoi,askscience,new,1
snusmumrikan,"A combination of things.

Firstly, you will inhale some of what you exhale, the reasons you don't re-inhale it all are (but probably are not limited to):

Exhaling gas with momentum drives it away from your mouth, this creates a slight vacuum which air from around your mouth will move in to fill. (interestingly this process is used to fill a hot-air balloon; placing the fan about a meter from the balloon fills it up many times faster than placing the fan directly at the mouth of the balloon, the gap creates a significant vacuum which allows surrounding air to rush in).

The air is hotter once it has been inside you and will rise away from you quickly.

The composition of the air has changed due to the gas exchange processes in your lungs and as such the 'new' gas you exhale will rapidly disperse to reach equilibrium across the environment.",null,0,cdgi2xi,1qrjo6,askscience,new,2
BillyBuckets,"The answers provided here are good, but I can add one more thing: we do re-breathe some of our ""old"" air. Our airways have what is termed ""dead space"", meaning that it does not exchange gasses and cannot contract to push out the air remaining inside of it. The trachea, that big air tube running from your head to your chest, is rigid from C-shaped rings of cartilage. At the end of each breath, it stays open and the air inside it slows to a stop. Inhale again and the air moves back into your lungs.

Turbulent flow causes this old air to mix with the fresh stuff from outside, so the old air reaches equilibrium with fresh air and fills your alveoli. Fortunately, the old air still has about 15% oxygen and only about 5% CO2, so in the end it really doesn't affect you much.

Smoke a lot of cigarettes and you might get COPD, specifically emphysema. This disease is characterized by the inability to empty your lungs fully after each breath, so you start to ""rebreathe"" more and more of your own exhaled breath. This is why people with emphysema get so winded so easily: they retain a lot of CO2-rich, O2-poor air with each exhalation.",null,0,cdgk97b,1qrjo6,askscience,new,1
TangentialThreat,"Your fingerprints are determined by your [dermal papillae](http://en.m.wikipedia.org/wiki/Dermal_papillae) which are deeper than your epidermis. This is why fingerprints come back after you burn your fingertip.

Damage that goes deep enough to destroy the dermis is perfectly possible, and speaking from experience it is no fun to lose a fingerprint. One of mine was partially replaced by shiny, smooth scar tissue after I cut off the entire pad of a finger while slicing sweet potatoes.",null,4,cdfy6h1,1qrihw,askscience,new,20
Kegnaught,"HIV, the causative agent of AIDS, is a retrovirus. Part of the retroviral life cycle is the synthesis of DNA from its viral genome and subsequent incorporation into your own genome. From here, it can go two ways depending on the activation state of the cell: it can either just lay dormant (or quiescent), or its genome can be actively transcribed/translated by the host cell machinery and produce more virus.

One major problem is that quiescent cells form a reservoir for the virus in our bodies. Our immune systems cannot recognize these cells as being infected because they're not actively producing virus and presentation of viral antigens is either turned off or significantly reduced. These quiescent cells can persist for years, if not decades, and subsequent infections or other events can activate them, allowing the virus to spread once again.

Current drugs prevent certain steps in the viral life cycle, so by keeping HIV+ patients on these antiretrovirals, their viral load is suppressed and spread of the virus will not occur, however we have no real way of flushing out the viral reservoirs from our body (the cells that have had the HIV genome incorporated into that of the cell and remain quiescent).

So essentially, we can prevent the virus from spreading, but we can't get rid of all the cells that are already infected. This is why when patients are either taken off antiretroviral therapy or who adhere poorly to the daily medication requirements, viral loads will quickly rebound due to random activation of quiescent infected cells, and the infection of new ones. Not to mention the fact that HIV reverse transcriptase (the enzyme responsible for turning its RNA genome into DNA) has no proofreading capability and its genome is highly mutable, leading to drug resistance and immune system evasion.",null,1,cdfvh3q,1qrgip,askscience,new,10
medikit,"Interesting question. I did not know and looked this up, I have edited my original response significantly. 

Basically there are necessary mycobacteria cellular components which need to be present in addition to the tuberculins to stimulate the delayed type-IV immune response:

From CDC: http://www.cdc.gov/mmwr/PDF/rr/rr4906.pdf
&gt;&gt;Immunologic basis for the tubeculin reaction: Infection with M. tuberculosis produces a delayed-type hypersensitivity reaction to certain antigenic components (tuberculins) that are contained in extract of culture filtrate of the organism. Purified protein derivative (PPD) tuberculin, which is used for most skin testing, is isolated from culture filtrate by protein preciptation.

Infection with tuberculosis is the best way to develop type IV sensitivity to tuberculin. NTM (Non-TB Mycobacteria) infection and vaccination with BCG can provoke type IV sensitivity but not nearly as reliably. It should be noted that there is no gold standard for latent tuberculosis infection which significantly limits are knowledge regarding this matter.",null,1,cdg1ugx,1qrft8,askscience,new,4
endocytosis,"It can happen, [actually](http://www.atsjournals.org/doi/full/10.1164/ajrccm.159.1.9801120#.UoermaXk3_M).  From the article, ""boosting"" is: ""The phenomenon of increased tuberculin reactions upon retesting in the absence of new infection, is believed to result from recall of waned cell-mediated immunity, akin to the anamnestic serologic response.""  If you take too frequent skin TB tests, less than 1 month apart, you can develop a reaction at the site and show a false positive.  ",null,0,cdfr6op,1qrft8,askscience,new,2
nemom,"Radiation.  We can't shield a craft well-enough to get something living to Mars without it getting an almost-lethal dose of radiation from the Sun.  Once a ship leaves the Earth's magnetic field, it will be bombarded by highly-charged particles.  It currently takes 5-10 months to get to Mars.",null,0,cdft6cp,1qrfkr,askscience,new,2
adamhstevens,"I assume you mean on the surface of Mars?

The pressure and temperature on the surface are too low. As well as the surface environment being hostile in terms of radiation and oxidising chemicals. If you could put them in their own rad-tolerant chamber you would never be able to build one big enough to hold the amount plants you would need. 

In fact, cyanobacteria would be a much better bet to take. However, even if you could produce the oxygen it would soon escape, probably at a rate comparable to how fast you would be able to produce it.",null,1,cdfuh15,1qrfkr,askscience,new,3
Urgullibl,"The problem with Mars is that its gravity is insufficient to hold on to much of an atmosphere for extended periods of time. This means that even if you could produce large quantities of oxygen, most of it would just escape into space.",null,0,cdfw3ea,1qrfkr,askscience,new,1
homininet,"The major cause of what most people call dizziness is when you spin at a constant angular velocity for a while, and then stop (ie you spin in an office chair for a while). The reason, which is what people have already mentioned, is that there are 3 gyroscopes in your ear called the semicircular canals. They are basically donut-shaped and filled with fluid, and have a little membrane at one end. When you spin, the fluid lags behind and moves the membrane, and your body senses that you're moving. When you spin at a constant angular velocity, eventually the fluid will catch up with the rest of your body (this doesnt take long). However, when you stop, the fluid is still pushing the membrane, and it does take a while for the membrane to finally ""push"" the fluid to rest (this does take a while, somewhere between 10-40 seconds, but I cant quite remember). This readjusting is what you sense as dizziness. Your inner ears are telling your brain you're moving, and to correct your vision, but you're not actually moving. 

So like others said below, this would still happen in space, because the motion of the fluid is angular, and would be unaffected by gravity. 

Now there are two other bits of your inner ear that might be affected, and these are little pendula that detect the direction of gravity and linear velocity. While these organs might be a little confused in space, I don't think they would cause you to be dizzy in the sense you are thinking of.",null,2,cdfxxpp,1qrcna,askscience,new,9
theansweris7,"Depends what you are spinning relative to. Visual field of view will probably matter a lot. Also, fluids in your ear (vestibular sense) sloshing around might make you dizzy as you accelerate or decelerate your spin, but while spinning at a constant angular velocity with eyes closed, probably no dizziness.",null,3,cdfu4hz,1qrcna,askscience,new,4
JoeScientist,"You will create centrifugal forces by spinning in space. The centrifugal force will be detected by various accelerometers in your head (inner ear). Unlike gravity, the centrifugal force isn't pointing in the same direction (or even with the same magnitude) all over your body. This should cause dizziness as your brain is trying to measure gravity and orientation to figure out which direction is down.

However, you will be a disoriented just by being weightless in space. Similarly, you're brain won't be able to figure out which direction is down. So I'm not sure how the effects would compound.
",null,2,cdfupd9,1qrcna,askscience,new,3
hans_useless,"Let's look at the math. The surface of a sphere with the same radius the orbit of a satelite (I assume about r=7000km) is 4*pi*r^2, so 6.15e8 km^2 = 6.15e14m^2. Assuming 10.000 satelites in orbit and one having a cross section of about 10m^2, we end up at an overall sattelite cross section of 100.000m^2 =1e5 m^2, so the chances are 1e5/(6.15 e14) = 1/6.15 e-9, so less than one in a billion. 

That's per meteorite entering the orbit of a satellite. 

Edit: Some additional thoughts. 

First, that assumes an infinitesimal asteroid. 

Second, an asteroid passing through earth orbit should penetrate the mentioned sphere twice. 

However, if a non-inifitesimal meteorite penetrates that sphere only once, we have bigger problems than a destroyed satellite.",null,0,cdft2gp,1qrbwe,askscience,new,3
wazoheat,"In addition to the other answer, I should say that Mars *does* have an atmosphere, and a quite significant one: about 0.6% of Earth's surface pressure. This might not seem like a lot, but in the grand scheme of things it's not much thinner than Earth's; most meteors will burn up before making it to the ground.",null,0,cdg9ey9,1qrbwe,askscience,new,1
xnihil0zer0,"Just because something is efficient, doesn't mean it will manifest, only that it will tend to be passed on if it does manifest. 

While winking is a gesture, [tortoises](http://www.youtube.com/watch?v=fBOFjbwh-Bs), [hamsters](http://www.youtube.com/watch?v=Shen6KxxH7U), and [geckos of the family Eublepharidae](http://www.youtube.com/watch?v=JDl3_MfXN6w) can blink their eyes independently. ",null,0,cdgjvrd,1qraab,askscience,new,1
99trumpets,"First let's consider why we don't all wet the bed our whole lives. Normally the kidneys have a strong circadian cycle in which they sharply reduce urine production at night. This occurs because during sleep, the pituitary gland increases output of a hormone called AVP (arginine vasopressin; aka antidiuretic hormone or ADH). AVP's main physiological role is to conserve water. It has a variety of actions around the body, but at the kidneys it has 1 main job: it sharply reduces urine output. So the bladder just doesn't get full. 

OK now, bedwetters. In many kids, the pituitary just doesn't have this circadian cycle fully in gear yet. (also they sometimes haven't learned control of the voluntary sphincter yet, but that's really not the key issue. The key issue is, is the bladder getting full in the middle of the night or not.) Older bedwetters may also just not have that AVP cycle going yet; and some older bedwetters turn out to have a related problem in which they do have the AVP increase at night, but their kidneys turn out to be unusually insensitive to AVP. 

Often bedwetting is treated with a drug called desmopressin. It's just an AVP analog, meaning, it binds to (and activates) AVP receptors on the kidney cells. Basically desmopressin can get the circadian cycle of urine output back to normal.

warning: I am a bit out of my field here (I know the hormone side, but not the diagnosis/treatment side) so anyone with more information, please step in. Here are some relevant papers: [source](http://ajprenal.physiology.org/content/256/4/F664), [source](http://informahealthcare.com/doi/abs/10.1080/003655999750169420), [source (pdf)](http://www.ciperj.org/imagens/evaluationandtreatmentenureses.pdf)",null,121,cdfn5d0,1qra44,askscience,new,699
medstudent22,Why children wet the bed is still an area of controversy. Some believe it is due to a lack of mature neurologic control over the bladder others believe it is due to a disconnect between the amount of urine produced and bladder capacity either due to a small bladder or increased urine output. Small bladder capacity is intuitive. Increased urine output may be due to a lack of ADH (hormone that makes you absorb water). Bladder size and neurologic control change with age for obvious reasons. ,null,94,cdfs784,1qra44,askscience,new,481
99trumpets,"Hi folks, just a reminder: this is AskScience, so please avoid personal anecdotes and speculation. (see sidebar) And we especially love citations to peer-reviewed research. Thank you!

edit: also - no requests for medical advice - we really cannot ever break this rule, thanks for understanding.",null,24,cdfqzgw,1qra44,askscience,new,95
null,null,null,6,cdfw7gi,1qra44,askscience,new,33
raff_riff,"Follow up question:

Do other mammals urinate while sleeping? I'm curious what evolutionary reason mammals have for holding bowel and bladder movements until awake / conscious (if there's any reason at all). ",null,4,cdfrgb8,1qra44,askscience,new,25
null,null,null,5,cdfuhgd,1qra44,askscience,new,22
wildcard5,"The nerve in control of the sphincter in the urethra which controls the outflow of the urine is not myelinated which is why we cannot control the muscle (the sphincter) which it innervates. As we grow older, the myelination of the nerve is completed and now we can control the sphincter with much more ease.",null,3,cdfrtbq,1qra44,askscience,new,18
null,null,null,4,cdfp1mp,1qra44,askscience,new,19
null,null,null,10,cdfpyba,1qra44,askscience,new,22
null,null,null,5,cdfqado,1qra44,askscience,new,18
Warle,"The gravitational pull of Jupiter is sufficient enough to disrupt the formation of anything that gets too big and simply rips it apart.

And if you think about it, there's actually not that much material in the asteroid belt. [Ceres](https://en.wikipedia.org/wiki/Ceres), a object that has not even 2% of the Moon's mass, is 25% of the total mass of the whole belt. Coupled with the fact that this mass is so spread out (a 'close encounter' could be a couple of hundred thousand kilometers away from any two asteroid, with collisions really unlikely) planet formation is physically impossible.

[More sources](https://en.wikipedia.org/wiki/Asteroid_belt)",null,5,cdfn6lw,1qr9q6,askscience,new,26
atomfullerene,The total mass of the asteroid belt is tiny...it's like 4% of the moon.  So there's really just not enough stuff present there to form a planet today.,null,1,cdfnl7m,1qr9q6,askscience,new,10
sq_ftw,"Others have addressed your main question. For the side question, the short answer is yes, it is theoretically possible to have multiple planets with the same orbital period. And it could be a stable configuration (though for some masses, dynamical interactions would no doubt cause instability). The easiest way would be to put a 2nd planet at the L4/L5 Lagrange point (60 degrees leading or trailing the 1st planet, rather than your 180 degree proposal). 

In fact, there are many asteroids that share Jupiter's orbit, one group at the L4 and one group at the L5 (the so-called ""Greeks"" and ""Trojans""). [Check out the wiki](http://en.wikipedia.org/wiki/Jupiter_Trojan) for this and related reading.

EDIT: [Here's an example of a paper](http://arxiv.org/abs/1307.7161) discussing ""Trojan planets"", and describing a search of public *Kepler* data. Spoiler: none have been found yet.",null,1,cdg6qnc,1qr9q6,askscience,new,4
aeschenkarnos,"Regarding your side question, it's highly unlikely that a planet could persist in a ""counter-Earth"" position for any significant length of time. Planetary orbits are slightly elliptical and the planets orbit the centre of gravity of the solar system, which is inside of the Sun, and moving around as the planets themselves orbit.

Another planet that started on the opposite side from Earth would not follow the exact same orbit. Its orbit would gradually diverge, and eventually interfere and perhaps even collide with Earth.",null,2,cdg4yhe,1qr9q6,askscience,new,3
medstudent22,"From the urinary side, there is an accumulation of risk factors that lead to incontinence is one factor. In men, prostate size increases with age leading to benign prostatic hyperplasia. This results in overflow incontinence and all of the lower urinary tract symptoms (LUTS) associated with an enlarged prostate. Over-aggressively treating this enlargement or removing the prostate can result in leaking as well. In women, the risk of stress incontinence increases with childbirth. Pelvic organ prolapse can result in incontinence as well and the risk of this increases with age.  

Even in the absence of these other factors, the lower urinary tract changes with age. The sphincters can weaken, urethral length can decrease, and bladder sensation can dull. Elderly people can also have involuntary bladder contractions at a higher rate.",null,0,cdfn2bt,1qr0zn,askscience,new,3
J4k0b42,"[Here's](http://www.doublexscience.org/why-are-snowflakes-always-six-sided/) a good article, basically water forms hydrogen bonds in a hexagonal shape (tetrahedral for each atom) and the crystal becomes symmetrical along the crystalline planes.",null,0,cdflq92,1qr0n2,askscience,new,7
sfurbo,"Each of the six branches start out identical, due to the crystal structure of ice. How a branch grows at a certain time is a function of the state of the branch and some external factors, such as temperature and humidity. As the snowflake is quite small, the branches experience the same environment, so they growing the same way, and end up looking similar.",null,0,cdfpdzi,1qr0n2,askscience,new,6
Caajk,"My understanding of it (Unfortunately I don't have any quotable sources) is it is due to the way particles act. Gas and liquid particles move around on their own in random directions. When there is a denser area of particles the particles bounce off each other. If there is a space devoid of the particles they will gradually spread out through random movements that result in the gradual spacing of the particles to evenly fill what ever container. There is no energy behind it it is just something that gradually happens because of the random movements.

A good analogy is people coming into a party. They all come in through the same door, but they do not stay there. They gradually space out and the net movement is towards the empty space until the people are fairly evenly spread out.",null,1,cdfm5up,1qqyyu,askscience,new,5
PipettesByMouth,"First, remember:  ΔG = ΔH - TΔS , and for the process to be spontaneous ΔG must be negative.

Osmosis is ""energy driven,"" in the sense that it has negative ΔG, but it is not driven by being coupled to some chemical reaction, so it is not *active* transport.

In active transport, you're coupling an unfavorable process (ΔG &gt; 0) to a favorable one like ATP hydrolysis (ΔG &lt; 0) to give a negative value of ΔG.  A big contribution for this process comes from the negative ΔH of ATP hydrolysis.

Osmosis is a bit different.  The enthalpy change is negligible.  To make ΔG negative, you need a positive value of ΔS.  You can think of entropy as a quantity that increases with the disorder of the system.  Two solutions of different concentrations separated by a semipermeable membrane are more ordered than the same two solutions at the same concentration.  So osmosis has positive ΔS.

TL;DR: Entropy.",null,2,cdfswkm,1qqyyu,askscience,new,6
nanopoop,"Osmosis is driven by a chemical potential gradient across a membrane. Or, put slightly differently, it is driven by the system attempting to reach an equilibrium state. Since it is driven by a chemical potential gradient, both entropy and energy play a role.",null,1,cdfnbtm,1qqyyu,askscience,new,5
Karmic-Chameleon,"Whilst the other two answerers have nailed the answer to this question, I'll chime in and point out that Osmosis is something biologists talk about, real Chemists just say diffusion as it covers a whole host of situations whilst osmosis is specifically for water travelling through a semi-permeable membrane.",null,2,cdfmfoa,1qqyyu,askscience,new,7
South_park_fan,"I would just also like to add that entropy and the second law of thermodynamics can be used to explain diffusion. Essentially, It happens because the greatest number of ways that something can be arranged is the most likely way that it will be arranged; the state with the greatest number of micro states is the most probable.",null,1,cdfmvzr,1qqyyu,askscience,new,3
CompMolNeuro,"Here's the paper reporting electrophysiological recordings (the first) in humans:

http://www.ncbi.nlm.nih.gov/pubmed/20381353

Be cautious about overarching conclusions based on such limited samples.  Just because they have a special name does not mean they are a unique class of neurons.  The naming of them is based purely on their behavior rather than any special anatomical differences.  What I consider far more likely is feedback between the various sensory cortices through the higher motor cortices.  Essentially, you're just imagining the motion prior to, or without actually performing the task.  No special neurons needed.  Parsimony.  ",null,1,cdfn7b2,1qqxin,askscience,new,6
glarn48,"Regarding your question on the potential connection between empathy and mirror neuron activation: it does seem like there is a link between mirror neuron activity and empathy. If you show someone a video of someone being hurt and the motor system lights up, does that qualify as evidence for this link? No not really. It only shows the classic demonstration that the motor system is active in understanding the actions of others. Rather, you have to manipulate empathy while controlling for the visual stimulus. 

That's precisely what a recent study did. They showed videos of a painful procedure being done, but in one condition they told subjects that the person in the video had a neurological disorder that stopped them from feeling pain. No pain, no empathic pain. The subjects who thought the person in the video was experiencing pain had more self-reported empathic concern and more activity in the mirror neuron system as indexed by mu/alpha suppression over fronto-parietal areas (a commonly used electrophysiological index of mirror neuron activity). (Source: http://www.ncbi.nlm.nih.gov/pubmed/21098810)

I'd like to caution you against taking the mirror neuron theory as hard facts though. There's still quite a bit of contention about its veracity. The proponent camp led by scientists like Marco Iacoboni (who did much of the initial research) suggest that there's something special about the motor system in its dual role as a perceptual and sensory system. There's quite a bit of evidence that most sensory systems play a role in understanding as well, and so many people criticize the limited/hyperspecific view (especially neurolinguists like Greg Kickok). They aren't arguing that motor systems don't help understanding the world, rather that the motor system isn't unique. ",null,0,cdfr5v5,1qqxin,askscience,new,2
homininet,"To answer the second question first, yes, the left has two lobes to make room for the heart. The aborted ""third"" lobe of the left lung is actually still present as a structure called the lingula. 

To answer the second question, its hard to say why humans have 2 to 3 lobes per lung, but I would disagree with the previous comment and say that it most likely doesn't have anything to oxygen consumption. When you look across animals, and across primates (which is what Im most familiar with), you see that the number of lobes per lung has both a phylogenetic signal (ie animals more closely related to each other tend to have the same number of lobes), as well as a functional signal (ie things that move in weird ways have weird numbers of lobes). 

So across primates, lemurs tend to have 3 or 4 lobes per lung (ie 7 total), tarsiers (leaping primates from asia) have 3,4,5, or 6 lobes per lung. Apes tend to have 2,3, or 4 lobes per lung. Interestingly, Orangutans usually only have 1 lobe per lung. This is also similar to sloths, koala bears, and some cetaceans (dolphins and whales). So it seems that having 1 non-lobate lung occurs in animals that use forms of locomotion that entail really slow climbing (and cetaceans for some reason). Whereas tarsiers may have evolved more lobes because of their specialized form of locomotion (they use whats called vertical clinging and leaping, youtube it, its pretty cool). 

Because how your body moves plays a big part in timing your inhalations, and controlling which lobes can readily fill with air, there is most likely a connection with what type of locomotion an animal uses, and how many lobes it has, the relationship though is likely complex, and definitely understudied.

There are other interesting examples of this relationship in non-primates as well.
 
Sources:
http://www.jstor.org/discover/10.2307/984708?uid=3739832&amp;uid=2&amp;uid=4&amp;uid=3739256&amp;sid=21102933878347
various comparative anatomy professors",null,0,cdfx7mb,1qqx2x,askscience,new,5
AerodynamicsEngineer,"No.

The reason that a soccer ball curves in air is that when it is spinning the velocity of the air relative to the surface on each side of the ball is different.  This causes a different pressure build up on each side creating a force perpendicular to it's original path of motion.

In a vacuum this wouldn't happen.

With regards to the inside of the ball, a spin would modify the pressure distribution on all sides equally, so it wouldn't change the direction of the ball.
  
",null,0,cdfnkds,1qqvup,askscience,new,10
rogerammjet,"I'm guessing you're ignoring gravity? Otherwise...

Pretty sure there's no way it can have a curved trajectory in a vaccum. Draw a free body diagram and try to find an external force.",null,2,cdfkpz3,1qqvup,askscience,new,6
Karmic-Chameleon,"There are no known compounds of Helium or Neon, as far as I'm aware, so sadly not.

As you may have learned in High School Chemistry, the elements on the far right of the periodic table are called the Noble Gases. They have unusually stability/un reactivity due to their electronic configurations and so don't normally form compounds. Why then, have I singled out Neon and Helium? Well, Xenon and Argon can both be 'convinced' to form compounds by bonding with Fluorine, the party animal of the periodic table. Xenon has also been made into a compound bonding with Oxygen through a Fluoride intermediate. These compounds are highly unstable and only exist under extreme conditions, if I remember rightly, very low temperatures.

The other major problem you'd have is that some of the elements are highly unstable and might not exist for long enough to actually form a compound. Some of them exist only in the order of nano-seconds though I guess if you were quick off the mark and got them bonded with something supremely reactive (most likely our old friend Fluorine) you might be able to manage it.

TL;DR No, it wouldn't be possible - sorry.",null,2,cdfmhst,1qquy6,askscience,new,9
PipettesByMouth,"Yes and no.  It depends on how you frame the question.

Every atom in everything you can see, touch, taste, or feel was either created shortly after the Big Bang, or in a supernova.  Every compound in existence *was* ultimately formed from its constituent elements.

It's not feasible for humans to reproduce that process in every case, though.  It does make for an interesting thought experiment.  Pick a compound... how would you synthesize it from the elements given limitless resources?

If you mean ""could we make any arbitrary grouping of atoms,"" ... then absolutely not.  Certain things (He2 is a simple example) cannot exist.",null,0,cdfthfk,1qquy6,askscience,new,5
hiimsadako,"T3 is the active form of the hormone but there are quite a few reasons why patients with hypothyroidism are treated with T4 instead of T3 and they all stem from the fact that T4 is converted into T3 in the liver and kidneys through deiodination. 

This means that the replacement of T4 alone provides a long lasting store of thyroid hormone that is gradually converted to T3 (T4 has a longer half life than T3 - T4 7 days and T3 1 day) resulting in stable plasma levels of both T3 and T4. 

Being gradually replaced means that depending on how much the dose is when you get blood work done in order to measure TSH titers, you will already have both T3 and T4 readily available for conversion which simulates normal thyroid hormone production. So normal doesn't necessarily mean optimal but it really depends on the dose and how good patients are at following instructions about their treatment.  ",null,0,cdfnl1o,1qqt8q,askscience,new,4
Simon_Riley,"As far as I know, T3 is used sometimes alongside T4 but I don't think I've heard if it used alone.  T4 is used alone alot as it is metabolized slower so you just need it once a day.  T4 alone is usually the first line of treatment and it is fairly effective most of the time.  Now let's look at TSH.  It is a hormone secreted by your brain to tell your thyroid to make more hormones if there are not enough.  High TSH means you have lower T3/T4 levels, generally speaking OR that you simply need a bit more right now for whatever reason!  Now, the physiological range of TSH can be very large and it is influenced by many things.  what's ""normal"" all depends on what guideline you are following.  To enroll in clinical studies, anything outside 1-4 ish is usually not accepted but my boss says he wouldn't look too much into it in a clnical setting unless it gets closer to 10 (if all other things looking good).  TSH is not a good marker honestly and usually ppeople need to look at T3 and T4 levels specifically.  Now if he says the TSH is normal, of course it doesn't mean optimal.  It means the brain is satisfied right now to not make the thyroid work harder.  But then optimal is also difficult to define with these things!  ",null,0,cdfnebb,1qqt8q,askscience,new,2
Pays4Porn,"Hypothyroidism is treated with a combination of T3 and T4  fairly regularly, in fact combo pills are one of the most prescribed drugs in the US.  PDT had 4.1 million prescriptions in the US last year, Synthroid(T4) had 23.4 million [prescriptions written](http://www.medscape.com/viewarticle/813571).  As you can see T4 treatment is much more common, but combo treatment is not all that rare. 

There are some very good studies that show some patients prefer pure t4 while others prefer a combo.  

[Effect of combination therapy with thyroxine (T
4
) and
3,5,3
0
-triiodothyronine versus T
4
monotherapy in patients with
hypothyroidism, a double-blind, randomised cross-over study](http://www.eje-online.org/content/161/6/895.full.pdf+html)

Plain T4 is easier on the patient, one pill a day vs multiple pills.  Much cheaper for the drugs.  Less blood tests etc.  Less heart problems.  This makes T4 a reasonable place to start, but if the patient is still having problems many doctors will continue on to combo therapy.",null,0,cdfsfwg,1qqt8q,askscience,new,2
Simon_Riley,"not ALL cells make direct contact with blood vessels of course!  Generally speaking a cell is bathing in interstitial fluid.  Let me show you a picture.  http://what-when-how.com/wp-content/uploads/2012/08/tmpe02651.png

As you can see, stuff in blood goes to the IF which is what the cell make exchange with and experiences.  Blood vessels are lined with specific cells that pump stuff out of blood vessels and into IF when appropriate and vice versa.  ",null,0,cdfnjva,1qqruk,askscience,new,3
CompMolNeuro,Neurons don't.  Their nutrients are shuttled from blood vessels by glial cells.  ,null,0,cdfna86,1qqruk,askscience,new,2
homininet,"Well the way my anatomy professor puts it is that we always think of the flight or fight response being sympathetic. But never confuse flight or fight with fright. Apparently when you're really scared, both the parasympathetic and sympathetic systems contribute to visceral motor innervation. Hence, the tendency to urinate, as well as some of the more traditional sympathetic responses. Unfortunately, I cant quickly find a ton of articles about it, but here is one about dual heart innervation during fear stimuli in rats (http://www.ncbi.nlm.nih.gov/pubmed/17184510)",null,1,cdfpgup,1qqorl,askscience,new,3
endocytosis,"Here's a (really informative, with images) press [release](http://www.sony.net/SonyInfo/News/Press/200708/07-074E/index.html) about a new BioBattery Sony developed.  Based on Sony's specs, it's has 62.5 mA output *1 min* after turning it on, and you need 4 to power a walkman.  They don't give any type of indication to hold that charge (so we get a mAh rating) but for comparison, the Li-ion polymer [batteries](http://www.ifixit.com/Teardown/iPod+Nano+7th+Generation+Teardown/10826) in an iPod nano have a 0.8 Wh, 220mAh capacity.  Not all portable devices have replaceable batteries, so I'd imagine the size, charge holding capacity, and the feasibility to replace/refill is a limiting factor.

It's a really cool idea though, and hopefully something comes from it.",null,0,cdfqtry,1qqllb,askscience,new,3
rocketsocks,"First, some background. EPA regulations mandate limits on the ""Reid vapor pressure"" (RVP) of gasoline, and these limits vary by the season.

The reasons for seasonal changes in gasoline composition are several. It's worth noting that RVP varies by temperature, which is why the composition is changed seasonally to hit the same targets. Low-RVP fuels may be more difficult to ignite, but this is not a reason for the EPA regulations. The biggest reason is that high RVP gasoline can lead to gas tanks that become pressurized by vapors. And, more importantly from an environmental aspect, gas vapors are a significant contributor to pollution. Gas makers take advantage of the higher RVP allowed in fall/winter to add more butane to gasoline, which is less expensive than the gas itself. That's why ""summer gas"" is more expensive, it contains more gasoline, while fall/winter gas has been blended with other hydrocarbons.

In terms of the automobile drive train summer gas doesn't make much difference any more. Modern engines will run just fine on summer gas in winter or winter gas in summer, it makes little difference, and the emissions from the tailpipe won't change significantly. But the pollution risk of winter gas being sold during summer comes from *handling* gas, not in burning it. Modern gasoline handling systems have also improved greatly though, so it's hard to say exactly how much difference seasonal gas composition still makes.",null,1,cdfm7k4,1qqka2,askscience,new,3
recycled_ideas,"Sort of. 

It's not possible to win a race of the same distance with a slower average velocity because average velocity is distance over time and to win you have to cover more distance than your opponent in the same time. 

The thing is though is that lots of races don't actually have all opponents travelling equal distances.  The Olympics usually do, but for example horse racing doesn't. This would mean you actually could win a horse race with lower average velocity.  

In essence if two racers have to travel different distances d1 and d2 where d2&gt;d1 the racer going d2 distance could travel a greater distance than their opponent and still lose allowing the racer with the lower average velocity to actually win. 

TL;DR In the mathematical sense no,  in real life yes. ",null,6,cdfkqv4,1qqj3v,askscience,new,19
RelativisticMechanic,"Technically yes, but practically no. In particular, if one assumes that any two runners of the race will start at the same point and end at the same point, then the answer is no, but in a real race the answer is yes.

The average speed (which is what you really mean if you want to talk about ""lower"" and ""higher"") is the magnitude of the net displacement divided by the elapsed time, or possibly the distance traveled divided by the time, depending on how you interpret the statement. In either case, if we assume that each runner either has the same net displacement or travels the same distance, respectively, then we can conclude that the runner with the shortest elapsed time will have the highest average speed.

However, in a real race there will be minor variations in the net displacement and the net distance traveled between runners. In this case, if the winner and the second-place runner have times that are close enough, it's possible that the winner will have traveled a shorter distance (or experienced a smaller net displacement) by a sufficient amount that their average speed is actually lower. For example, consider two runners on a circular track of radius 50 meters. They run sequentially, so that they can use the same lane. The first runner comes to a stop *precisely* at the starting location after precisely 39.95 seconds. The second runner comes to a stop 10 centimeters radially outward from the starting point after 40 seconds. The second runner lost the race, but had an average speed that was greater than the first runner's (in the magnitude of net displacement sense, this is true regardless of the paths they took; in the distance traveled sense, it depends on the path, but works out to this answer if we assume simple paths like a perfect circle and a simple spiral).",null,1,cdfktam,1qqj3v,askscience,new,11
APPALLING_USERNAME,"Velocity, measured in distance over time, is always somewhat of an average - if you are traveling ten meters per second, chances are in different parts of that second you are traveling faster or slower than that speed due to acceleration (force) or deceleration (friction). The average velocity of a racing contestant can be expressed by the total length of track divided by the amount of time it took to complete the race, then simplified. 

If all contestants are racing in a straightaway, the person who completes it in the least amount of time will necessarily have the greatest average velocity. 

    Loser : 1km/6m    = 10.0 km/h
    Winner: 1km/5m30s = 10.9 km/h

If the contestants are racing in a circuit (think Indy 500,) someone hugging the inside of the circuit will travel a lesser total distance than a contestant forced to the outside. 

    Winner: 60km/20m    = 180.0 km/h
    Loser : 64km/20m30s = 184.4 km/h

This is why you will see track &amp; field starting lines staggered back toward the inside, if the race length necessitates a turn in the track. In circuitous vehicle racing, however, by strategically forcing your opponents to the outside through maneuvering, you can win the race by traveling a lesser distance even if someone else has a higher average velocity.

Source: I won a statewide math competition in sixth grade &amp; got a plaque.",null,2,cdfkuux,1qqj3v,askscience,new,7
Krankite,"This is a more interesting question if you look at something like car racing the act of overtaking will disadvantage a racer such that the additional distance they need to travel will force them to require a higher average velocity then otherwise required to win, you can see this from time to time with the various team tactics.",null,0,cdgy8xa,1qqj3v,askscience,new,1
infinitooples,"You're hearing the sum of all the voices in the chorus, so like the average but multiplied by the number of singers.  This is because the amplitude of sound waves simply add in air, which is an example of a superposition principle.  So if one person out of twenty is off-key they'll only be 1/20 of the sound, and their mistake will get glossed over to some extent.

That being said, if everyone was a spot on copy of one singer, they would produce a louder version of that one singer.  The distinct sound of a chorus is actually caused by the fact that not everyone can sing exactly the same, they are a little out of time and a little out of tune and have slightly different timbres to their voices.  Electronic effects try to emulate this by adding a copy of a sound that has been phase or pitch shifted to itself.  It works surprisingly well, but it's hard to fake the complexity of the real thing. ",null,5,cdffbwj,1qqhl0,askscience,new,24
mirror_egami,"Even still water isn't actually still. You'd need veerry cold ice to get actual still water. So your scenario will still have very tiny changes because of how the electrons bump around in the water. I don't know the exact effect these moving electrons might have on the splash, but it is a factor that might be of importance. Also, I think, given enough time, the water's splash effect will repeat.

The only problem is that in the beginning when the water is pouring, it will have splashed harder on the surface than later on, because later on the surface will have risen because of the added water.

I can say that it's an interesting question that also gets me thinking about chaos theory a bit...",null,2,cdfm77t,1qqhkb,askscience,new,3
theansweris7,"There would probably be very high symmetry initially though as 'mirror_egami' suggests there are some perturbations that would add up over time. Asymmetry would be emergent, but over what timescale i'm not sure. Would need a better understanding of the properties of liquid water to address this.",null,1,cdfv3wa,1qqhkb,askscience,new,2
Sterlz,"The losses can be determined by applying the conservation of energy form of Bernoulli's equation I believe.

Here's a picture of the equation:
http://content.myodesie.com/images/techtransfer.com/wiki/fluidflow7.png

If laminar flow is assumed, then the only real losses will be from the roughness of the interior of the nozzle. The only other losses I could think of would be from vorticies forming where the nozzle re-expands, but those losses only become significant if the flow is turbulent ",null,0,cdffkx6,1qqhbm,askscience,new,8
Jasfss,"This depends on a few factors. Let's just assume that we're dealing with a low speed wind tunnel for an example (and that the flow never reaches a velocity above Mach1 because of this). Since we're in low speeds, we'll also assume incompressibility  You have three stages: the reservoir and converging section, the throat, and the diverging section. We know the pressure at the reservoir, P0, and that due to incompressibility, the density of air is ρ at all points. The proper formula to find the pressure at the exit (P1) is (P1/P0)=(ρ1/ρ0)^γ. In this case we know ρ1=ρ0, so (ρ1/ρ0)=1 and γ for air is 1.4. 1^1.4 =1, and 1*P0=P0, therefore P0=P1. There's more calculations you can do and be more specific about (I could go step by step from each section), but for this general example, the answer is that the pressure differences are effectively 0.
",null,2,cdfg3xl,1qqhbm,askscience,new,5
Randomaway,"You always have some energy loss in compressing and decompressing a fluid.  Think of your compressed air can getting cold as the air inside expands.  Even though your nozzle returns to the same diameter, there is some energy being lost from the system.",null,0,cdgf1e3,1qqhbm,askscience,new,1
ColHoraceGentleman,"An ordinary plane is going to run into lots of factors that will prevent this.  E.g., insufficient air/fuel mix getting too lean, insufficient lift, lack of response on control surfaces.  There are also human factors: beyond Armstrong's Line (https://en.wikipedia.org/wiki/Armstrong_limit) your body fluids will boil without a pressurized space suit.

In a specially-designed aircraft (or aerospace craft in this case), you'd need a LOT of thrust and large surfaces to allow a nominally atmospheric aircraft to truly leave the atmosphere, at which point it would drift on in a single direction unless equipped with some form of directional thrust.

A couple of military aircraft that do not truly enter space, but still get high enough to require spacesuits include the U-2 and SR-71 (no longer in service).  Some of the old X-plane pilots were rumored to have their astronaut wings as well.",null,1,cdfeowm,1qqeld,askscience,new,6
ryannayr140,"I assume that you're talking about commercial aircraft.  The air in the atmosphere gets thinner and thinner as the plane gets higher, at 40,000-50,000 feet the plane will simply stop climbing because the lift generated by the wings is equal to the force of gravity pulling the plane down.  The speed indicated to the pilots is lower than the speed they're actually traveling at, which is helpful to the pilot so they know whether or not they are going fast enough to generate lift given how thick the air is.  At some point, with the engines at full throttle, the plane will not be able to go any faster but the indicated air speed will be near the stall speed because the aircraft is in thin air.  If the pilot tried to pitch the nose up to climb higher near this speed the plane would stall, which would be very easy to recover from at such a high altitude.  ",null,1,cdfgrop,1qqeld,askscience,new,4
knflrpn,"It works the same way as a center punch.  Check out the ""Operation"" section of the Wikipedia page for a nice diagram:

http://en.wikipedia.org/wiki/Automatic_center_punch#Operation",null,0,cdg90n3,1qqegs,askscience,new,1
Platypuskeeper,"You have what's essentially a [ratchet mechanism](http://en.wikipedia.org/wiki/Ratchet_%28device%29). On the kind where you turn a knob it's pretty much directly a ratchet with a single 'tooth' on the round gear, compressing the spring and releasing it. 

I haven't taken apart a button-press one, but I'd imagine the ratchet is then on a rotating cylinder, a bit like the [mechanism of a ballpoint pen](http://upload.wikimedia.org/wikipedia/commons/f/fd/Ballpoint-pen-parts.jpg). 

",null,3,cdffpqr,1qqegs,askscience,new,3
DarthToothbrush,"I think this may have to do with the seasonal rains in the Ethiopian highlands, which experience heavy rains in the summer.  These wetlands, a major source of the Blue Nile, get their first rains in June, and they continue through September.  They are, like the nile flooding, variable in degree but consistent in their timing.
The phrase ""Dog days of Summer"" gets its root from the appearance of Sirius, the dog star, in the morning sky during the hottest part of the year for the ancient Romans.  This was pretty consistent for anyone in the same part of the globe, and Sirius is also one of the brightest objects in the sky.
It is the kind of coincidence that ancient people tended to imbue with the power of causation.  To many people, the star was bringing with it the heat of summer.
So the first of the big rains happens at the source of the Nile at a relatively consistent time of year, and eventually floods the river downstream at a relatively consistent time of year that happens to correspond with the brief appearance of a big, bright, very visible star.

",null,1,cdfiyvq,1qqedz,askscience,new,4
Das_Mime," Not on any reasonable timescale, no. They do contract somewhat, but it takes an extremely long time for them to collapse. Exact timescales are hard to pin down because it's an n-body problem, but the estimate for passive relaxation (just a globular cluster by itself, chilling, with stars occasionally causing slight deflections in each others' orbits) is at the end of this post. 

First off, for bound groups of stars, you have two basic types: dispersion-supported (like globular clusters and elliptical galaxies, where orbits are elliptical and go in every direction) and rotation-supported (like spiral galaxy disks, where the stars are proceeding in relatively circular orbits, mostly in the same direction). In either system, to get it to collapse, you have to get that angular momentum to go somewhere. In a spiral disk, that's fairly difficult to do, although the [bars which are present in some spiral galaxies](http://imgsrc.hubblesite.org/hu/db/images/hs-2005-01-a-print.jpg) create an uneven gravitational field which can draw stars and gas down toward the center.

In a globular cluster, the stars are densely packed enough that they have some 2-body interactions with each other, which can shed angular momentum and help the stars settle toward the center. The practical effect of this is that the globular cluster's core gets denser and shrinks. Eventually, if the globular cluster has a central black hole, [as a number of them do](http://www.astro.uni-bonn.de/~holger/preprints/imbh3.pdf), the stars will fall into that black hole. However, as the globular cluster loses more and more stars, it becomes harder and harder to shed angular momentum via 2-body interactions.

Globular clusters can also be dynamically cooled (dynamical cooling basically means that the system loses angular momentum) via gravitational interactions with other objects (for example, many globular clusters orbit the Milky Way and often pass through the dense disk of the galaxy). The tidal effects cause the stars with the highest velocities (the dynamically ""hottest"" stars) to be lost from the globular cluster, which causes the globular cluster to have less angular momentum and thus to contract.


t_*relax* = (1.8 x 10^10 yr) x &amp;sigma;^(3) m^-1 &amp;rho;^(-3)  / ln(N)

&amp;sigma; is the velocity dispersion in km/s

m is the characteristic mass of a star in the system

&amp;rho; is the star number density in pc^(-3)

and N is the total number of stars.",null,1,cdfh7gj,1qqe9b,askscience,new,8
peytong67,"Also, from our current understanding of gravity: galaxies should theoretically not exist because the gravity is simply too weak. According to the laws of physics galaxies should rip themselves apart and sling matter everywhere, but some unknown force helps to add gravity to keep galxaxies together. We call this unknown force dark matter.",null,3,cdfz6p9,1qqe9b,askscience,new,3
Simon_Riley,"So I think for your game, you need to make certain things developmentally lethal meaning if they have a mutation in a certain important gene then this embryo simply will die and not develop into adult to pass it on which means that mutation dies with the individual and the population as a whole stays healthy. ",null,0,cdff0or,1qqdrc,askscience,new,4
snusmumrikan,"The genes which lead to giving you two functioning eyeballs are part of a very complex and essential series of differentiation genes which will be involved in many other essential functions like limb formation and organ differentiation, and any disruption or loss of these genes in the evolutionary history of our species will probably have had significant effects on the offspring - likely resulting in termination of the pregnancy at the earliest stages. This means that those offspring would never have reached pro-creating age and therefore never gave rise to the potential for different eye-number phenotypes. Although if you are interested; looking into the effects of rubella on live-births will show that occasionally, sadly, children are born without eyes amongst other equally terrible disfigurements. Testament to the importance of the developmental gene networks. 

That's just looking quite recently at an evolutionary history covering land mammals. Looking further back, the evolutionary survival benefit of having stereoscopic vision is huge (comedian Dara O'Brien has a great sketch on one- and two-eyed monkeys), and those organisms which developed it would have quickly become dominant, embedding those genes as essential for survival to an age capable of procreation (the only driving force of evolution). 

Eye colour on the other hand is nowhere near as important from a survival, procreation or evolutionary perspective and is not determined by such a vast network of gene expression. This means those genes responsible are not 'set-in-stone' or part of a large and essential network of early developmental genes and allows for phenotypic variation.

My expertise is molecular biology, not game-development, so I can't be much help with your problem, although perhaps the problem may lie in all of your offspring being viable? If you tagged some genes as 'essential' such as those which give you the ability to digest food, you could have those offspring never make it to birth (which is also much more realistic). ",null,0,cdgioxc,1qqdrc,askscience,new,1
samcobra,"There is such a thing as cyclopism, when babies are born with one eye. Unfortunately, it's always a sign of a horrendous developmental problem that affects other things like the brain as well. (Fun fact , the eye is basically a part of the brain) These kids always end up dying very soon after birth if they survive to be born in the first place.",null,0,cdh5k9g,1qqdrc,askscience,new,1
Simon_Riley,"1) Correct, this is essentially blood doping.  
2) carboxyhemoglobin is CO bound with Hgb not CO2.  I'm assuming by 30% carboxyhemoglobin you mean 30% of your available Hgb are now bound by CO while the other 70% HgB are fine?  The ones with a CO bound to one of its 4 O2 binding sites are less likely to release the other three O2 as binding of one CO to HgB increases its affinity for oxygen at other sites and not release them when they are suppose to under normal physiological circumstances.  Therefore, for a given pO2, the %O2 saturation will be higher as the O2 are not being released into the tissues.  This shifts the graph to the left.  
3) The amount of oxygen in the blood won't be falling by 30% straight up it's more likely that only 70% of the blood oxygen are useful to other tissues.  Remember, since the curve shift to the left, it means for a given pO2, the amount of oxygen in your blood is higher.  At Max pO2 then the one with 30% CO-Hgb will have less as 100% HgB will have essentially all oxygen binding to all sites while the 30% CO-Hgb one will have oxygen binding on all sites, except those with one bound with CO.  However at a lower pO2, the one with 30% CO-Hgb will retain more oxygen while the 100% Hgb one will more likely to give off oxygen to the tissues.  ",null,0,cdffq9o,1qqb23,askscience,new,2
hopffiber,"Well, a more precise statement about photons would be that they do not have any internal frame of reference. If you try to do a Lorentz transform to some frame moving with c, you just get divergences. So there simply isn't any reference frame moving with the photon; so your first statement is quite meaningless. Also, photons obviously do not move between two different times instantly: you cannot use a photon to magically connect two different times. So I have no idea what exactly your idea of a particle only ""experiencing"" some of the spatial dimension would mean, it just sounds very strange. 

More generally on the concept of an action at a distance, in modern science we really like the concept of locality, which exactly means that we do not have any ""spooky action at a distance"" anywhere. This is also something that kind of follows from relativity. In popular science expositions of quantum mechanics, this might not be clear and people speak about weird non-local stuff like entanglement etc., but for most (and the most popular) interpretations of quantum mechanics, there just isn't any kind of ""action of a distance"". Some phenomena like entanglement seems non-local, but if one looks closer there really isn't any non-local interactions anywhere. There are some interpretations, like Bohmian mechanics, which have non-locality, but personally those are very weird. 
",null,2,cdffus1,1qqar4,askscience,new,5
ignirtoq,"""Spooky action at a distance"" is what Einstein called a phenomenon now called quantum entanglement.  Depending on who you ask, entanglement isn't really a poorly-understood phenomenon anymore.

When we say two or more quantum particles are entangled we mean their collective state is described by a single wave-function.  The ""action"" part comes when we make an observation and collapse the wave-function.  At that moment the state of the system projects onto an eigenstate, which means all particles involved are simultaneously projected into an eigenstate of the observed quantum operator (e.g. position, momentum, spin, etc.).  This process is what is not so well understood, but it's not unique to entanglement.

When any quantum system is observed, it is projected into an eigenstate.  This projection is just more pronounced (or ""less intuitive"" may be a better way to say it) when it happens with exotic systems like entangled particles.

",null,0,cdfnowv,1qqar4,askscience,new,4
xxx_yyy,"&gt; I would expect that such a particle could interact with others at any location instantaneously regardless of distance.

You're thinking about a particle that can move infinitely fast.  This has nothing to do with photons, which always move at a finite speed.

These particles (called tachyons) have been considered and searched for (to no avail).  Their existence would wreak havoc with our current understanding of causality.  For example, one could use them to communicate with the past.
",null,0,cdfzuuz,1qqar4,askscience,new,1
dapwnsauce,"Salt when introduced to our digestive system will eventually disassociate into Na+ and Cl-.  Both ions can potentially enter cells via ion channels. Now the reason why it draws water out is more towards the Cl- ion.  Due to the negative charge it osmotically causes water to flow outside the cells to balance the chloride concentration. Na+ also is absorbed in your small intestines. For every glucose molecule absorbed one Na+ is absorbed as well(talk about sweet and salty eh?). 

Though this is not my area of expertise. I would ask that if there is any mistake in y explanation, please correct. 

My formatting is poor replying from my iPhone. Sorry. ",null,0,cdfds4k,1qq9vb,askscience,new,3
Simon_Riley,"Salt just entering our cells would be bad of course.  Cells normally have high potassium count and lower sodium count and this is maintained by the NA/K ATPase as you must know.  This is a tightly regulated thing to maintain resting membrane potential and cell size.  That being said, when you drink salt water, your kidney will work very hard to eliminate the excess sodium in your urine to maintain your ion balance and of course the water come from your circulation which come from cells. ",null,0,cdfdtzz,1qq9vb,askscience,new,1
elcoopadero,"What nobody is talking about here is water potential. You must understand two things: first, water is pretty much the only molecule that can travel freely between cells and membranes in your body, and second, water travels to the place with the highest concentration of solutes (dissolved solids, like salt). So, when you drink saltwater, a highly concentrated solution, the water in your body moves through the membranes in your cells so that it can make the concentration equal between your cells and your bloodstream. 

Salt, on the other hand, dissociates into ions with respective charges, so it can't go into cells as easily. And yes, it is this way because if too much salt went into your cells they would die.",null,0,cdfj9m3,1qq9vb,askscience,new,1
medikit,"I have two questions for /u/th7buhs or anyone without a medical background: 

1. I once saw a patient in the hospital who was overdosed with a medication called lactulose. They received over 10x the recommended dosage due to a combination of pharmacy and nursing error. Their serum sodium was in the 180s. This is extremely high, normal is around 139. How did this happen?

2. What happens when the salt water you are drinking is Potassium Chloride instead of Sodium Chloride?",null,0,cdg2nzl,1qq9vb,askscience,new,1
Platypuskeeper,"[Forced convection](http://en.wikipedia.org/wiki/Forced_convection). Same thing as with a fan. You force colder air towards a hot thing and that cools it off faster than the normal circulation of hot air (natural convection) would have.
",null,1,cdfev6k,1qq7y1,askscience,new,3
NAG3LT,"I think there are two things at play here.


As your mirror is foggy, the reflection becomes very diffuse. That can be seen in your photo, as the reflected beam is much larger than the one on the first mirror. The wider beam means less intensity and the scattered light from it becomes harder to notice. 


That alone would not give an effect as big as you see, however. Another important part is the scattering intensity dependence on angle. You see the original beam from laser via its scattered light. The closer the angle between your line of sight and the direction of beam is to 0 deg or 180 deg, the more intense the scattered light will be. However, if beam and your line of sight are close to 90 deg, there will be very little scattered light to see. The beam from laser in your photo seems to be very close to your camera's line of sight and can be seen clearly, while the reflected one is likely viewed from its side. 


In short, the beam is still there, diverging a lot after reflection. You do not see the scattered light from the reflected beam, due to the amount of scattered light you can see being dependent on an angle you're viewing it at. ",null,2,cdfec0t,1qq6gw,askscience,new,15
apples-are-rubbish,"The lumen of your GI tract is considered ""the outside"" to your immune system. When you eat something, you introduce tons of potential antigens and you put yourself at risk of infection

Goblet cells in your epithelial layer are specialised cells that produce mucus.. It's said that our bodies generate up to 4 litres of mucus per day and there's a turnover ~20 hours! the mucus helps to trap pathogens and antigens, and then that mucus is excreted along with the nasty pathogens.

When that isn't enough and you are infected by a bacterium, there's your mucosa-associated lymphoid tissues (Malt) and your gut-associated lymphoid tissues at the ready to mount a defence. You have Peyer's patches which are secondary lymphoid organs. These tissues contain B cells and T cells and macrophages to fight these microbes that have penetrated your barriers. You also have Paneth cells in your epithelium which secrete antimicrobial peptides.",null,0,cdfebwy,1qq0y5,askscience,new,5
Keereegs,"There are parts of our body that need to be sterile, and remain sterile, but other parts of our body that need bacteria to function and develop normally.  Our immune system keeps everyone in their place with the help of our ""microbiota"" - the microbes, like bacteria, that normally live within us.  Our microbiota help us process food, and help ward-off ""harmfull"" bacteria.

Our bodies have more up to 10x more bacterial cells than human cells.

Healthy poo contains mostly helpful bacteria and other microbes.  Healthy poo is 10-20% bacteria by-weight (and they are good for your septic system after they leave your colon!).  

Healthy poo can be used in Fecal Transplants to treat and usually cure many intestinal problems in humans (and other animals).

Old veterinarians used ""poo- tea,"" which they made using dung from a healthy cow, to treat colitis in sick cows.

When you use Antibiotics your body's bacterial allies become ""collateral damage"" - they can be killed off.  If that happens, other ""harmful"" bacteria have a better chance of infecting you.

Treat your inner microbes with respect ;)",null,0,cdfrouv,1qq0y5,askscience,new,2
ReplaceSelect,"Someone will give you a more detailed answer, but it easy to think of the digestive tract as a tube that goes through your body. It's basically outside of your body as far as most of your immune system is concerned.",null,1,cdfd9ce,1qq0y5,askscience,new,2
Larry_Boy,"Not only is it possible, it is necessary for life! Thymine is just uracil with a methyl group on it, and is usually called [5-methyl-uracil or ribothymidine](http://en.wikipedia.org/wiki/Ribothymidine) when it is incorporated in RNA. 5-methyl-uracil is used in both rRNA (the RNA which helps join amino-acids together) and tRNA (the RNA which carries and labels amino-acids for use by ribosomes). Both of these forms of RNA have a number of unique nucleobases in addition to 5-methyl uracil. [Modified base in tRNA](http://www.ias.ac.in/jarch/jbiosci/6/757-770.pdf) are important for creating the 'wobble' code that allows a single tRNA anti-codon to recognize a number of different mRNA codons.  

To be continued...",null,1,cdfs46t,1qpyw6,askscience,new,5
InsaneAI,"RNA polymerases only include ribonucleotides in the RNA synthesis during transcription. Thymine only exists as dTTP, i.e. the deoxyribose-version of the nucleotide. The two sugars differ by the 2' -OH group, which deoxyribose lacks. This article goes into detail about the discrimination: http://www.ncbi.nlm.nih.gov/pubmed/15262972 . DNA polymerases, on the other hand, discriminate against ribonucleotides, preventing uracil incorporation (which is vital due to the deamination of cytosine and associated need for removal of uracil).

edit: typo",null,3,cdfeb6b,1qpyw6,askscience,new,4
michmach,"Are you talking about a single base pair, or a whole gene? if it is for a whole gene, it would not be RNA at all it would just be a strand of DNA because that is the major difference between DNA and RNA else the fact that it is a single strand.",null,8,cdfe71v,1qpyw6,askscience,new,2
YoYoDingDongYo,"Google changed it with [PageRank](https://en.wikipedia.org/wiki/PageRank), which tries to get an honest, objective view of how relevant a page is to the search terms.  

Before that search engines would do brain-damaged things like order results by how many times your search terms appeared on a page, which led to all kinds of web stupidity.",null,0,cdfefet,1qpwzh,askscience,new,8
c_programmer,"There was no binary change. As processors got faster they could run better algorithms, as network speed increased they could run distributed algorithms better and as hard drive space increased they had the capacity to store far more data. All the while Google has spend lots of money on some of the best R&amp;D teams in the world making sure their results are really good. Google has always been at the forefront but other major search engines have been doing similar things. ",null,0,cdfvfn7,1qpwzh,askscience,new,4
amnesiajune,"They never ""became optimized"" - it's been a gradual process. Lots of people point to PageRank as the point when search engines ""became"" optimized, but the reality is that we've come a long way since then too.

The goal of a search engine is to display the best result. The goal of search engine optimization (What websites do when they want more hits) is to trick a search engine into thinking that one website the best result. Search Engine ranking algorithms are being constantly improved to outsmart SEO techniques, as well as to show more user-relevant results. The algorithms that Google uses today are much more complex than PageRank",null,0,cdmyx3m,1qpwzh,askscience,new,1
Platypuskeeper,"According to [Mathworld](http://mathworld.wolfram.com/FibonacciPrime.html) (first sentence) this is so.

Searching is not really how you prove anything in number theory. There's an infinite number of Fibonacci numbers, so you're never any closer to proving things no matter how many you try. Quite a few counterexamples to conjectures have been found where brute-force would never have found them. E.g. for x^2 - 991y^2 = 1, the lowest-valued solution is:

 x = 379516400906811930638014896080, y = 12055735790331359447442538767.

",null,1,cdfebe1,1qpwg7,askscience,new,6
KfoipRfged,"I haven't done much with Fibonacci numbers recently, but I did remember reading the answer to your question here: http://en.wikipedia.org/wiki/Fibonacci_number#Primes_and_divisibility

I think what would really seal the deal for you is a proof of the fact on wikipedia of ""Every kth number in the sequence is a multiple of F_k""",null,1,cdfe13y,1qpwg7,askscience,new,4
lithiumdeuteride,"I don't know about any proof, but I (or rather, my computer) checked the first 10,000 Fibonacci numbers, and apart from 3, all of the prime Fibonacci numbers F(i) occurred for prime i.

In case anyone is wondering, the 10,000th Fibonacci number is 33644764876431783266621612005107543310302148460680063906564769974680081442166662368155595513633734025582065332680836159373734790483865268263040892463056431887354544369559827491606602099884183933864652731300088830269235673613135117579297437854413752130520504347701602264758318906527890855154366159582987279682987510631200575428783453215515103870818298969791613127856265033195487140214287532698187962046936097879900350962302291026368131493195275630227837628441540360584402572114334961180023091208287046088923962328835461505776583271252546093591128203925285393434620904245248929403901706233888991085841065183173360437470737908552631764325733993712871937587746897479926305837065742830161637408969178426378624212835258112820516370298089332099905707920064367426202389783111470054074998459250360633560933883831923386783056136435351892133279732908133732642652633989763922723407882928177953580570993691049175470808931841056146322338217465637321248226383092103297701648054726243842374862411453093812206564914032751086643394517512161526545361333111314042436854805106765843493523836959653428071768775328348234345557366719731392746273629108210679280784718035329131176778924659089938635459327894523777674406192240337638674004021330343297496902028328145933418826817683893072003634795623117103101291953169794607632737589253530772552375943788434504067715555779056450443016640119462580972216729758615026968443146952034614932291105970676243268515992834709891284706740862008587135016260312071903172086094081298321581077282076353186624611278245537208532365305775956430072517744315051539600905168603220349163222640885248852433158051534849622434848299380905070483482449327453732624567755879089187190803662058009594743150052402532709746995318770724376825907419939632265984147498193609285223945039707165443156421328157688908058783183404917434556270520223564846495196112460268313970975069382648706613264507665074611512677522748621598642530711298441182622661057163515069260029861704945425047491378115154139941550671256271197133252763631939606902895650288268608362241082050562430701794976171121233066073310059947366875",null,4,cdfe9yf,1qpwg7,askscience,new,3
_Momotsuki,"Having constant adrenaline isn't a good thing. You yet palpitations, diaphoresis, anxiety and prolonged exposure leads to cachexia. There are people who have tumours that secrete adrenaline/noradrenaline. These tumours are called pheochromocytomas. 

Source: medical student ",null,0,cdfeqf8,1qpvw6,askscience,new,7
wildcard5,"The classic symptoms would be, headache, sweating, and heart palpitations (a fast heart beat) in association with markedly elevated blood pressure (hypertension). Other conditions that may accompany these classic symptoms are anxiety, nausea, tremors, weakness, abdominal pain, and weight loss.

In some cases, the high blood pressure comes and goes and may be difficult to document. In other cases, the blood pressure is consistently elevated 

For you this may have been a hypothetical question but pheochromocytomas can actually cause this. These signs and symptoms develop because this type of tumor produces an excess of chemical compounds called catecholamines. Excessive secretion of catecholamines — the hormones adrenaline (epinephrine) and noradrenaline (norepinephrine) — can lead to persistent high blood pressure or wild fluctuations in your blood pressure, depending on whether the catecholamines are released continuously or in shorter bursts. The intermittent release of these hormones can cause other symptoms to occur from time to time as well.",null,0,cdg9oga,1qpvw6,askscience,new,1
asterliberi,"I believe deforestation is increasing globally. Looking at my ""Principles of Conservation Biology"" textbook from 2009. It reads:

""In 25 countries, no forest remains, more than 90% of forests have been lost (Millennium Ecosystem Assessment 2005b). Worldwide, between 1990 and 2000, the proportion of land area covered by forest decreased from 30.4% to 29.7% (from 38.79 million km^2 to 37.85 million km^2; UN Environment Programme 2002). Forest losses in the 1990s were greatest in Africa, which lost 8% of its forested area during that decade. More than 60% of annual forest losses worldwide have occurred in the tropics, with a further 2.3 million ha of tropical forest sufficiently degraded to be detectable via satellite imagery (Achard et al. 2002)."" 

So it does not mention Boreal forests explicitly but you can see an overall trend worldwide with loss of forests and therefore decreasing biomass.

",null,1,cdfd0px,1qpvtv,askscience,new,3
darkness1685,"To answer your first question, yes, there is definitely a decrease in net forest biomass. The abstract of the article clearly mentions that since 2000, 2.3 M km^2 of forest have been lost, and just 0.8 km^2 have been gained. I don't think the fact that more was lost than gained is surprising, although the difference between them did strike me considering that this was a global-scale analysis. I would need to see the reference that you are referring to about the taiga/boreal regions, but I will point out that this new study is by the far the best estimate that we have now on forest extent, gain, and loss (at least for the past decade). These results are sure to be much more accurate than whatever you read in an old biology book (although keep in mind that this study just covers 2000-2012).

To answer your second question, no, the pattern is not consistent across the globe, and the article points out that it is predominately due to tropical forest loss (this is also not surprising at all). They also point out that the tropics are the only area where the results were statistically significant. If you look at the US and Europe for example, forest losses are mostly counteracted by subsequent gains over the same period. This is largely due to responsible forestry practices in these areas. In contrast, deforestation in the tropics is very far from responsible, meaning that forests are simply clearcut without any plans for reforestation or silviculture. One thing that did surprise me was that I would have expected to see a larger net gain in forest cover in the Northeastern United States. The forestry industry in this area has experienced significant declines, as demand for timber has declined and exports from overseas have increased. It seems that the net forest cover has remained pretty much the same, although I expect this is partly due to the spatial grain used in the study and the fact that cover in general is so high in that region. For boreal forests, the authors point out that loss is due mostly to fires.

I would definitely recommend looking at the high resolution map that is linked in the article (not the actual figure reported). It is very easy to explore all of the different data that was mapped for this study. Really cool stuff! ",null,1,cdfo1p1,1qpvtv,askscience,new,1
nofriggingway,"Layman here. I think there are other factors which are important for both the surface and the tyre material.

For example, the road surface must be able to drain away rain quickly. Tyres, since they wear off into small particles in the air, must be non-toxic. I have sometimes contemplated what would make a great road surface, and it is quite a challenging set of requirements, some in opposition. It must be flexible and somewhat self-healing, able to be easily repaired, but durable and difficult to damage. Current road surfaces with stone particles held together with bitumen seem like a good (and inexpensive) solution.",null,0,cdg0qed,1qpvml,askscience,new,3
OneBigBug,"It's liquid cooled with ammonia.

The ISS has aluminum radiators mounted to it. Convection is obviously a very potent form of heat transfer, and isn't an option in space, but heat is still radiated into space in the infrared part of the spectrum without it.

If you look [here](http://apod.nasa.gov/apod/image/0102/iss_sts98a.jpg) you can see them beside the bottom right of the bottom left solar panel. 

[You can read more here](http://science.nasa.gov/science-news/science-at-nasa/2001/ast21mar_1/)",null,45,cdf8dw8,1qpp16,askscience,new,223
Robamaton,"For the spacewalking part:

[Here](http://www.youtube.com/watch?v=VsdoJy8rzZg) is an interview with a manager at NASA showing off the EMU (the suit astronauts use on spacewalks). There is a cold-water cooling system inside the suit which draws heat into the life support unit on the back, where it is either transferred to the vehicle via an umbilical, or is fed into a sheet of ice which then sublimates and is vented.",null,3,cdf8jru,1qpp16,askscience,new,24
lumberjackninja,"According to [this wikipedia article](http://en.wikipedia.org/wiki/Thermal_Control_Subsystem), it basically amounts to trying to maximize the surface area that emits excess heat.

You can do a crude estimate of how large the panels need to be by treating them as a [black body](http://en.wikipedia.org/wiki/Black_body_radiation) and using the Stefan-Boltzman law to calculate how much power is radiated for a given panel temperature. Of course, this ignores the emissivity of the panel material, the Earth's albedo, and other things I'm sure I'm forgetting.

For example, in order to lose one kilowatt of heat with a panel temperature of 30 degrees Celsius (about 86 degrees Fahrenheit), you need to solve the equation A \* sigma \* T^4 = 1000 W, where A is the area (m^2 ), sigma is the Stefan-Boltzman constant (5.67E-8 W \* m^-2 \* K^-4) and T is the temperature of the panel in Kelvin. A = 1000 W / (sigma \* T^4) where T is 303.15K, which gives us A = 2.08 m^2 .",null,5,cdf8obs,1qpp16,askscience,new,22
ppphhh,"It doesn't directly answer your question, but I saw an AMA a few months ago from Chris Hadfield (real life spaceman) who said that often they sleep with fans pointed at them, since with no convection, a little bubble of hot air (and CO2) builds up around you if you're not moving about. 

As for actually cooling the station itself, as others have said, obviously convection and conduction aren't options in orbit, but you can still radiate heat via infra-red. Pumping the heat into a broad panel will do an ok job of this. If you've ever stood near a black stone wall/floor on a sunny day, you know how much heat can be put out this way. ",null,3,cdfa9fh,1qpp16,askscience,new,13
DubiousDoctor,As they can't use conduction or convection they use radiation.,null,4,cdf8839,1qpp16,askscience,new,12
Wheeler_Dealer,"Radiation from the Sun and Earth (Earth's Albedo and it's infrared radiation) are the main sources that can cause heating on the ISS, mainly the the Sun. 

Radiation comes from the ability of electromagnetic waves (or light) to transfer energy. It is important to note that electromagnetic waves do not require matter to transfer the energy, which is why its the main concern in space applications. With radiation, the important quantities that effect the transfer of heat are temperature, surface area, and material specific constants such as emissivity and absorption.

Radiators take advantage of having large 'fins', with high emissivity, which emit excess heat. Special coatings on the outside of the space station, such as gold and silver (and believe it or not, white colored paint), manipulate the electromagnetic energy to be emitted faster than it can be absorbed (effectively reflecting the energy).

So the more efficient technique would be to reflect the electromagnetic radiation with the different types of coatings, followed up by radiators for the energy that is actually absorbed. Both techniques are considered as passive heat controls, which are more frequently used in spacecraft than active heat controls - they use power. ",null,1,cdfbq8k,1qpp16,askscience,new,3
lordlicorice,"In the science fiction novel Ringworld, the cabin of the ship is cooled by transferring all of the heat to a white-hot point of mass inside the engines. I've always wondered (and this seems like a good thread to ask) if this is thermodynamically possible. Would this work for the space station: fly up a chunk of tungsten, heat it to thousands of degrees K, and then eject it out into space?",null,2,cdfjmbr,1qpp16,askscience,new,3
fauxscot,"The only method available in space is radiation.  Unless, of course, you periodically shipped in cold matter from earth as a consumable supply of ""negative energy"", e.g. tanks of liquid helium or such.  The ammonia coolant only MOVES heat from a source to a destination.  It order to eliminate it, in space the only method is radiation.    Radiation works.  Stand a few feet from a radiant heater and see.  Hold up a piece of aluminum foil close to a bonfire as a heat shield and see.  Probably as good a terrestrial illustration as you can find quickly.   ",null,1,cdfm86w,1qpp16,askscience,new,2
colorimetry,"The [citric acid cycle](http://www.ncbi.nlm.nih.gov/books/NBK22340/) requires thiamine to generate ATP. Lack of thiamine causes a deficiency of the enzyme pyruvate dehydrogenase, since this enzyme uses thiamine as a cofactor; this leads to high levels of pyruvate, since, without pyruvate dehydrogenase, glucose cannot be broken down by the citric acid cycle. Lacking the ability to use glucose as a fuel results in neurological symptoms because the nervous system cannot use fuels other than glucose.",null,1,cdf7zuk,1qpnvf,askscience,new,11
mutatron,"They use rocket motors, not aerocapture. Once the spacecraft gets in position near a planet, they orient the engine to point more or less opposite the direction of motion. Then they do an orbital insertion burn to get down to orbital velocity. Usually this leaves the craft in a highly elliptical orbit, so they keep doing burns at the apsids until they get the orbit they want.

This is part of why they're not going to orbit Pluto, getting out there in a reasonable time is going to take a lot of fuel and a couple of gravity assists. It will be going very fast when it gets there, which would require a lot of fuel to burn off. The other part of the reason is because they want to go to the Kuiper Belt.",null,2,cdf6eoq,1qplni,askscience,new,13
this_or_this,"This is a very broad question and that makes it difficult to answer in detail. Interplanetary probes destined for orbits around planets must necessarily have a way of changing their orbits. Typically this is done with rocket engines. Cassini and Juno are both good examples of this. 

Aerobraking has been used by a handful of missions. See this link: http://en.wikipedia.org/wiki/Aerobraking#Spacecraft_missions
",null,0,cdf73is,1qplni,askscience,new,9
iorgfeflkd,"A black hole has the maximum possible entropy that can fit into a given region of space (the Bekenstein bound). If you think about all the possible configurations of the star prior to the black hole, and then that a black hole only has three properties (mass, charge, and spin), you can see that a black hole is the macrostate with the most possible microstates.",null,5,cdf4tr1,1qpidc,askscience,new,53
AnUglyMind,"The surface area of a black hole contains information so to speak, so as a black hole gets larger, it's entropy increases. In fact, black holes obey almost identical laws to the laws of thermodynamics.

Heat death means that the universe no longer has energy available to do work- all existing energy has diffused, to the point where collecting it would require more energy than you would bring in. This has to do with Hawking Radiation, in which black holes very slowly radiate particles until they either reach a very small state, collapse, or explode (we don't know yet). This form of heat death would take around 10^100 years.",null,5,cdf8gtw,1qpidc,askscience,new,20
Another_Penguin,"I scanned through the comments and didn't see any mention of the evaporation of black holes: http://en.wikipedia.org/wiki/Hawking_radiation

A black hole is only one step on the way from high entropy to ""heat death"". Given enough time, the black hole will be gone and the universe will be that much more uniform.",null,1,cdfh011,1qpidc,askscience,new,7
v3rsatile,"Please excuse me if this sounds like a dumb question as I'm not as educated as some of you, but I've recently reinvigorated my interest in the various studies of the universe and my question is, is it possible that black holes may suck up all the mass in the universe given trillions of years? ",null,2,cdfek7d,1qpidc,askscience,new,3
iorgfeflkd,"It's a quantity that represents how strong the gravitational interaction is. It was first measured by Henry Cavendish, who observed the rotation of a torsional pendulum between two giant lead spheres.",null,11,cdf4ukz,1qpfzu,askscience,new,72
fishsupreme,"Like a lot of physical constants, the gravitational constant G doesn't have its value because of something in reality, it has its value because of the units we use.

We know that F=G(m1*m2)/r^2.  That relationship comes from reality, from the universe -- the force of gravity is proportional to the masses involved and the distance between them.  But we measure force in newtons, mass in kilograms, and distance in meters -- none of which came from the universe.  The constant G is used to make the relationship work in the units we've chosen.

If you express length, mass, and time in Planck units -- natural units derived from the universe -- the gravitational constant is not needed and we can just say F=(m1*m2)/r^2.

Another way of demonstrating that G is all about units and not about gravity: G is exactly the cube of the Planck length in meters, divided by the product of the Planck mass in kilograms and the square of Planck time in seconds.  No gravity involved; just redefinition of units.",null,14,cdf93wb,1qpfzu,askscience,new,61
akunin,"All of what everyone is saying here is true, but I think you are kind of looking for a deeper meaning to the number.

The long and short of it is this: it's just a number that was derived emperically, as /u/iorgfeflkd said. It doesn't really describe anything in the universe except for what we call the gravitational force interaction between masses.

It's sort of like asking what the speed of light is. The speed of light is the speed at which light^1 goes. We only know what number that is because we measured it.

^1 other important things go this fast, too, but that's complicated...",null,9,cdf7kvx,1qpfzu,askscience,new,31
TheCastleOfArggg,"I have a small addition just because no one has pointed it out yet.  The gravitational constant is based on the units being used.  It is usually given  as ~6.67384 × 10^(-11), but that is only for when you use newtons, kilograms, meters, and seconds.  If you used other units the gravitational constant would be other numbers.  It is simply the number used to make the equation (G(mass1)(mass2) / radius^2 = Force) balanced.  Other people have already described how that equation was derived.",null,3,cdf7o7e,1qpfzu,askscience,new,22
667384," G is the gravitational constant. If you know the mass of two objects, the distance between their centers of mass and G you can calculate the force of gravity between them by multiplying the masses together, multiplying by G and dividing by the square of the distance between them. So G(m1)(m2) / r^2 = force of gravity. 


It was first suggested by Isaac Newton but measured by Henry Cavendish. He measured it with a beam that had a lead ball at each end. He hung the beam with a single wire at its center point. The beam started oscillating (moving back and forth) at a specific rate. He then moved two lead balls near the ends of the beam and measured how the oscillation changed. From this he was able to calculate the force that the balls exerted on eachother. His paper doesn't explicitly contain the value of G but it is trivial to calculate it from his data.",null,2,cdf6ta4,1qpfzu,askscience,new,9
sawowner,"No, the smooth muscles in your esophagus will push the food down into your stomach independent of gravity. This is why you can take a gulp of water upside down.

As for the stomach and intestines, they also have layers of smooth muscle called tunica muscularis that maintain peristalsis in order to keep the food going in the right direction.",null,5,cdf6n89,1qpegr,askscience,new,36
FerociousSalmon,"Your digestive system works by peristalsis not gravity. Peristalsis is the process which causes your muscles in your digestive system to contract and relax, in doing this it can push food throughout your digestive system.",null,0,cdf8wc1,1qpegr,askscience,new,9
SnowDogger,"You can actually do some self-experimentation with this. Stand on your head (lean against a wall if you have to), and bite into an apple. Chew and swallow. You'll feel the bolus of food rise up toward your feet.",null,0,cdfarf1,1qpegr,askscience,new,3
Platypuskeeper,"'Hitting' an electron isn't really a thing, neither the photon or electron are small hard spheres. 

The photon is a change in the electrical (and magnetic) field and the electron is a charged particle that responds to it. The end result can be that the photon just passes without any change, that the electron is excited if the photon has sufficient energy, or that the photon gets scattered (the scattered photon may also have higher or lower energy than the original photon).
 
",null,3,cdf6jlh,1qpedy,askscience,new,18
bleedingLance,"The electron needs to absorb a very specific amount of energy for it to be excited. If a photon doesn't have this energy, the electron can't absorb it. So nothing will happen, and the photon will continue unaffected. It can't be re-emitted since the electron can't absorb it in the first place.",null,2,cdf6kkh,1qpedy,askscience,new,2
chrisbaird,"The photon is its own antiparticle. Antiparticles are formed mathematically by taking certain properties such as the charge and flipping them. For instance an electron has a charge of negative one, so an anti-electron (a positron) has a charge of positive one. The photon has a charge of zero, and the negative of zero is still zero, so the photon is its own antiparticle.

This makes sense if you think in terms of time. Mathematically, antiparticles can be thought of as regular particles traveling backwards in time (this ""backwards-in-time"" nature can't be used to do anything interesting as antiparticles obey all the conservation laws and therefore do not violate causality). So an antielectron is just an electron that has been knocked backwards in time by emitting a energentic enough photon according to the symmetry of the Feynman diagram. But a photon going backwards in time is the same as a photon going forwards in time because photons are really outside of time. Photons travel at the universal speed limit, and at that speed, time ceases to have meaning. ",null,21,cdf4ge4,1qpccj,askscience,new,110
iorgfeflkd,The photon is its own antiparticle.,null,10,cdf3ngs,1qpccj,askscience,new,44
Crushnaut,"Although the photon does not have an anti-particle as others have already said, it MAY have a [superpartner](http://en.wikipedia.org/wiki/Superpartner) (aka a sparticle) called the [Photino](http://en.wikipedia.org/wiki/Photino) which would be an example of a [Gaugino](http://en.wikipedia.org/wiki/Gaugino).

This is predicted by the theory of [Supersymmetry](http://en.wikipedia.org/wiki/Supersymmetry) which is untested.
",null,0,cdfd4yn,1qpccj,askscience,new,4
aWetNoodle,Then why is there a distinguishment (sp) between neutrinos and antineutrinos? Wouldn't the neutrino be its own antiparticle because it is electrically neutral?,null,0,cdf6nz1,1qpccj,askscience,new,2
elderbio,"Geotechnical Engineer here.  Holy hell, I'm relevant on reddit?  

As some have pointed out previously, soil has a saturation limit.  Varying types of soil may have greater infiltration rates than others.  Such as sand versus clay.  Sand is a much larger particle than clay and as the particles settle, sand will create greater voids per unit volume than clay.  These voids allow for water to filter through more readily, increasing the hydraulic conductivity of the soil type.  So the larger the particle = the easier water can flow through the soil.

In the case of a stream bed, it becomes a bit more complicated.  The stream bed is the physical representation of water collecting from a massive watershed.  For instance, the Mississippi River's watershed collects from the majority of the eastern United States.  At the starting points of these watersheds, typically the larger particles; cobbles and gravels, and sands are located here.  It is likely that in these areas the infiltration rates are higher allowing more water to drain into the ground, but because the water is continually being fed from melting snow pack and rain and other sources, the stream bed is continually being saturated beyond its maximum infiltration rate and capacity.  Thus standing water will begin to form, and if the water eventually finds a way to move downhill, streams begin to form.

Now follow these streams downhill until you get to a resevoir or a lake.  As the water finds flat terrain and builds up, it creates a stilling basin that allows even the smallest clay particles that have been previously suspended in the stream's current to settle to the bottom. These smaller clay particles can create a barrier that makes it more difficult for the water to seep into the ground.  As a matter of fact, clay is commonly used as a barrier to keep contaminated water or leachate from spreading, such as a landfill.

**TL;DR  The water stays above ground because soils can only allow so much water filter through depending on size.  If the water is coming in faster than it is draining through the soils, water will build up above the surface.**

Edit: Thanks for the gold! 

Edit 2: There has been quite some hubbub about the void per unit volume statement of sand versus clay and the ability of a clay to hold water.  Clay is a peculiar material in that it has varying degrees of arrangements.  In its under-consolidated arrangement it is likely that clay will have more voids per unit volume than sand.  However, in it's highly over-consolidated arrangement it is likely not so.  Imagine you have a deck of cards and you build a house of cards by placing cards in a sort of ""T"" fashion.  This is how a clay is arranged in its under-consolidated arrangement. A lot of voids between very thin small plates.  Now take that house of cards and collapse it down so they are all laying on top of each other.  This is the over-consolidated arrangement.  Now keep pressing these cards together with an insane amount of pressure to the point they nearly start to make a brick.  This is the highly over-consolidated formation.  A clay shale is the result of over-consolidation. 

This is not the only reason water is restricted from moving through clays.  Because of the small particle size of a clay, the geometry of a clay particle, and what clay particles are made of, clay particles has a tendency to attract water molecules.  They will attract water molecules and these water molecules will hold on to the clay particle to some degree, filling what voids are left.

The point is not about permeability or porosity but [hydraulic conductivity](http://en.wikipedia.org/wiki/Hydraulic_conductivity).  Hydraulic conductivity is simply how fast can the water move through the clay versus the sand.  Sand will have a much higher hydraulic conductivity than under-consolidated clay (~10^-4 cm/s) and many orders of magnitude more than a over-consolidated clay (~10^-7 cm/s).",null,618,cdf6r0g,1qpbb6,askscience,new,2441
null,null,null,87,cdf74vc,1qpbb6,askscience,new,474
null,null,null,46,cdfbuzv,1qpbb6,askscience,new,268
Mookie5,"a question about my discipline! huzzah! I'm a certified Hydrogeologist in the State of California with 13 years of experience working with groundwater as a water resources consultant. 

The short answer is that it *does* seep in, but at different rates, that vary wildly and depend on the ground conditions beneath the water. Often, this rate is so low, that the water cannot seep in at a measurable rate compared to how much water is present. Further in some locations water has completely saturated the subsurface below the body of water,and no further water can seep in. 

I will be updating this with detailed descriptions and links to diagrams throughout the day. 

I'll start by talking about permeability. Permeability is the measure of how fast water can move through subsurface materials (detailed Wikipedia defition here: http://en.wikipedia.org/wiki/Permeability_(earth_sciences)). Different types of materials have different permeabilities. Ever pour water on a pile of gravel? It goes right in through the holes between chunks of gravel right? water on sand, like at the beach? still pretty fast, but a little slower, pour it on clay, like the stuff you make ceramics out of? it doesn't flow into that does it? but you know when that clay is dry, so it must have had water soak into it somehow, right? We generally order these materials in ranks of permeability, from More permeable to Less permeable:


*Gravel

*Sand

*Silt

*Clay

*(Photo http://i.imgur.com/gc5fmEd.gif)

Most subsurface is a mixture of these grain size types and/or bedrock. Without getting into too much detail, generally the more gravel/sand an area has, the more permeable it is, and the more water that subsurface can hold (storativity). Stuff with more silt and clay is less permeable. Lastly, there is 'bedrock' which is a term I'll use to describe the hardrock that isn't sediments, and it usually has very little permeability, as it's very solid (there are exceptions). 

So, I'll talk about what this looks like in a lake and a river example for now. 

In a lake, generally, water flows to the lake from the lakes watershed (a watershed is the geographic area that when rain falls on it, that water will move downhill due to gravity to thing at lowest point in the watershed, in this case, a lake). This water will work it's way to the lake and pool up. Some portion of that water will seep into the ground below the lake based on the material below's permeability, and over time, will completely saturate the pore spaces (storage) in the sediments below the lake. Water will seep in, and spread outward as well, until it hits a space to discharge to, or an impermable sediment like bedrock, or very slowly permeable sediment like clay.  

Rivers and Streams are more complicated. When it rains on a hillside, some of that water seeps into the ground, and some runs off. The water that runs off will collect where hillsides connect, and in rills, gullys, etc, and flow downhill and collect into bigger groupings and form streams and so on to rivers. The water that seeps into the ground will move into the ground, and then move *downhill* underground (this is generally called the gradient). Sometimes, after moving downhill underground, it will come out of the ground and join a stream (making it a gaining stream, as it gains water as it moves from the subsurface). If these rivers and stream flow to a lake, some of the groundwater will also flow into the lake. Gaining stream diagram (http://i.imgur.com/B1guIKD.gif)

Sometimes it will keep moving underground until it reaches the lowest point it can, and still not have reached a stream. This difference is dependant on how far down into the ground the water table is (the top of the water table is the first depth that you would encounter water if you dug a hole). In areas where the water table is far below the ground surface, when it rains, the water that seeps into the ground goes to the water table, and does not reach the stream. Any streams that form due to runoff (remember water can only seep in as fast as the permeability of the ground materials) will flow over the ground, and slowly seep into the ground in these areas, and are known as losing streams. In the San Joaquin valley in California, there are ENTIRE RIVERS like the Cosumnes river, that rarely reach the delta/ocean because of this phenomenon. Losing stream diagram (http://i.imgur.com/3kEy5CO.jpg)

That's the simplest basics I can put together, there's lots of exceptions and special cases to talk about as well I'll try to elaborate (with hopfully more/better pictures) in a bit.

",null,24,cdf7608,1qpbb6,askscience,new,140
ramk13,"What I wrote in a [previous post](http://www.reddit.com/r/askscience/comments/1ezt1s/why_does_the_water_in_a_lake_or_pond_not_absorb/ca5etm1):
&gt;It does absorb (infiltrate) into the ground below, but at some point that soil is saturated. What stops the water from continuously going down is that there are different types of soil/rock and some are less permeable than others. It usually goes along the line of gravel -&gt; sand -&gt; clay -&gt; rock. Below the permeable soil at some depth (depends on the location) there is usually a practically impermeable bedrock. The water 'pools' in the ground above those impermeable formations.

&gt;Since it can't go straight down any further, the water in the saturated soil flows downhill just like water on the surface. It looks for places with lower groundwater elevation (or a lower water table). In some cases those places of lower groundwater elevation will intersect with the ground surface and you'll have a spring where water flows downhill and out of the ground (sounds counter intuitive).

&gt;[Here's a cartoon like diagram/animation that's pretty easy to follow](http://earthguide.ucsd.edu/earthguide/diagrams/groundwater/). In this example you can substitute the river in the picture for a lake or other surface water body.
 
&gt;As you mentioned in your post, when you dig a hole and fill it with water, the water infiltrates. If you keep digging in most places you'll dig down to the water table like in the diagram above and you'll have a well.

&gt;You can find plenty more basic info at the wiki page for [groundwater](http://en.wikipedia.org/wiki/Groundwater) or by googling some of the terms I used above.

To add to that, the places where the 'pools' of water I mention above meet the surface are rivers, lakes and streams.",null,3,cdfdg8i,1qpbb6,askscience,new,37
Trecus,"different from all the other answers and slightly offtopic:

there are cases where water seeps in. A very prominent one in germany is the ""[Donauversickerung](http://en.wikipedia.org/wiki/Danube_Sinkhole)"" where the Donau - yes, [the Donau](http://en.wikipedia.org/wiki/Danube), the second longest River in Europe - disappears completely for a few kilometers.

I will probably get it wrong trying to explain it, so the wiki page on this will have to do...",null,0,cdf76u5,1qpbb6,askscience,new,8
tyhopkin,"Rivers are usually just exposed areas of the water table.  This in ""valleys"", where as mountain streams are from runoff (rain/melting snow).  Lakes are essentially just low points in the area where a large amount of water drains to and will recharge the groundwater, though if the rate of recharge + evaporation + surface water uses is less than what's coming in, then a lake forms.  ",null,4,cdfbvu9,1qpbb6,askscience,new,11
gabbagabba777,"It does!  You are correct!  

There are two kinds of streams, influent (gaining) and effluent (losing) streams.  Effluent rivers lose part of the flow, either from evaporation or to groundwater.  Influent streams are the opposite, groundwater supports part of the flow of the river.  In New England for example we have an extremely high water table, and during dry months our rivers still run because the water table is high enough to support these streams.  

",null,2,cdfa4lj,1qpbb6,askscience,new,7
null,null,null,2,cdfoe2v,1qpbb6,askscience,new,8
Riceboi89,"Hydrogeologist here. Rivers and lakes water surfaces are really just the groundwater table when they are not in the ground. Generally groundwater which is the saturated soil in the ground will kind of form a gradient into rivers and lakes and you will often find a gradual ""drop"" in groundwater table as it meets rivers and streams. Keep this connection in mind, rivers and lakes are always connected to the surrounding groundwater! The golden rule in hydrogeology is that water always flow from high hydraulic head to low hydraulic head (water flow downhill) so most of the year rivers and lakes is actually being recharged by the groundwater. However groundwater table fluctuates seasonally, generally summer being high while water level drops during the winter. Because of this, in fact some time rivers and lakes are actually seeping into the ground below to recharge groundwater, other time groundwater is seeping into rivers and lakes. The rate of these water flow depends on type of soil and where littoral zone (fully saturated) is located, generally groundwater flow is pretty damn slow)
I hope this answer your question",null,1,cdf9b32,1qpbb6,askscience,new,6
CreativelyChallenged,"The water table around a river, stream, or lake is actually at the height of the water and goes deeper with distance from the source of water. The water table height when not immediately by a source of standing water is at the depth in the ground at which the hydrostatic pressure from the water below is the same as the atmospheric pressure from the air above. Immediately above this line is the capillary fringe followed by a permeable zone in which water infiltrates both downward and horizontally as well. When enough precipitation occurs without water being able to drain away, the water table rises and can reach the surface. This is called return overland flow and can be a major source of runoff when an area experiences consistent rain.",null,1,cdfbu93,1qpbb6,askscience,new,5
Copse_Of_Trees,"As a geology student I struggled with this for YEARS. It's hard to visualize. The main to realize is under the surface of Earth's there's soil, then ""weathered"" rock, then layers of unweathered rock all the way down to the mantle.

As many have mentioned, soil is part air. When you pour water on it, water flows into the voids.

Rocks can ALSO be part air. Some rocks are full of air holes. Others have lots of cracks running through them. Go look at a rock some day. There are cracks in all sorts of rocks.

With that in mind, let's go back to that idea of the subsurface: soil - weathered rock - unweathered rock. 

When water enters that system (I.e. Rainfall) it first flows through soil. It hits a rock layer and start to seep into cracks. This is weathered rock. Here the water is slowly eroding that bedrock, which is part of why soil forms. Soil is mostly just weathered down tiny bits of rock.

Now, at some point that weathered rock can't hold more water. The cracks all fill up. After the rock fills up, the soil starts to fill up. That level of rock + filled soil is where the ""water table"" is.

The other important idea to keep in mind is that our world is 3-dimensional. So this subsurface soil-rock layer is higher is some places than others, and that water will always flow downhill.

This wasn't the most technical and glossed over all sorts of minor important concepts and exceptions and such, but works as a general idea.

Tl;dr: The ground is porous to a certain depth (filled with holes). Water flows through the ground, filling the porous space from the bottom up.",null,1,cdfbcim,1qpbb6,askscience,new,4
useless-member,"empty the bowl and fill it with gravel and sand and make a depression to represent your lake...starting by flowing water into the lake, no water will be retained in the depression until the surrounding voids between the gravel and sand have been filled this is a losing stream. once the voids are full, ponding will occur, this is called a gaining stream. the bedrock can have large cavities like in basalt flows and limestone, it's called karst if found in limestone, smaller inter-granular voids found in clastics like sandstone, some volcanics, and conglomerates, or no voids found in clay, shale, ash, and other impermeable layers.
generally a surface system will have impermeable layers underneath, slowing drainage or the near surface aquifer will be ""full""",null,0,cdfcakg,1qpbb6,askscience,new,3
LordofKleenex,"In some cases there may be what is called a clay pan, which is a layer of clay in the soil, which prevents water from draining any further into the ground.  Water moves through clay very slowly.

For example, flooding in areas with soils that have a very high clay content near the surface is common simply because the water cannot be absorbed into the clay.",null,0,cdfbjen,1qpbb6,askscience,new,3
froggy_baby04,"As I'm sure you know, water does seep into the ground. But some stays on top. Why? There are three reasons:

(1) If you dig deep enough into the ground you will likely hit one of two things: Either practically impenetrable rock, or practically impenetrable clay. Both of these keep water from sinking into the earth and getting vapourized by the heat from the earth's core. 

(2) sometimes it does get vapourized by the heat from the earth's core. We call this geothermally heated ground water. AKA hot springs, geysers, etc. 

(3) the soil underneath the water is saturated with water so that it can no longer hold water any more water. Any extra water has to sit on top, or slide down a hill, only to succombe another one of these three fates.

Source: did an undergrad degree in civil engineering
",null,0,cdfhaq5,1qpbb6,askscience,new,2
LaPoderosa,"I actually have an answer that is relevant and hasn't been posted?? 

So the top post about saturation is correct but is not the whole story. Under the dirt at the bottom of the lake is some kind of rock layer with other rock layers under it. The water saturates the soil on top of the rocks until it reaches the rock, where it will flow downhill. Eventually it will pool up somewhere and form a lake. ",null,0,cdfbty2,1qpbb6,askscience,new,2
shiningPate,"Some of it does go into the ground but there's more water in the flow than can be absorbed and flow into the ground. Watched a movie last year about a guy who kayaked from one of the sources of the Colorado river to the gulf of California, except he didn't make it. The river reduces to a trickle before disappearing altogether about 20 miles from the sea. At a certain point too much is being absorbed into the sand (mainly because too much has been taken out and sent to LA) ",null,1,cdfh5ir,1qpbb6,askscience,new,2
rupert1920,"Their aversion to forming a miscible solution is due to differences in intermolecular forces and the entropy of the molecules at the interface, not gravity.",null,7,cdf1gxn,1qp2p0,askscience,new,26
misunderstandgap,"Gases always mix; liquids sometimes mix. In liquids, intermolecular forces are strong enough to control mixing. In a gas, they aren't. If you took normal oil and normal water and put them under vacuum, they would both boil. The liquid phases would not mix (except for on a large scale, due to agitation by the boiling), while the gas phases would mix.

By a large scale, I mean things like emulsions: bubbles of oil in water, or vice-versa. This is not stable for liquids that really don't want to mix, but can form temporarily, and on a large scale. We'll ignore emulsifiers.",null,1,cdfcaoi,1qp2p0,askscience,new,3
Tetrakis,"For a physical chemistry perspective: It depends on the oil, and it depends on the temperature.

Short answer, yes. You can do it with most oils, as long as you raise the temperature enough. You can do it at vacuum and room temperature, as long as you have a small amount of either oil or water.

So in ""normal space,"" (which I take to mean ""at standard conditions,"") water and most ""oils"" are not perfectly immiscible. A very small amount of one is soluble in the other, and vice-versa. When we're talking about miscibility, we're really talking about the *degree* of miscibility, which is a numerical factor (""partition coefficient."")

The degree of miscibility depends on the temperature, pressure (at vacuum, pressure = 0), and the molar fraction of each component. (Do we have a LOT of oil? A LOT of water? A roughly equal amount of each?) Each of these factors determines whether you have one phase (homogenous, fully miscible) or two (two layers, heterogenous, partially miscible.) 

As far as the degree of this miscibility and how it responds to changes in temperature, pressure, and molar fraction? That depends purely on the two substances you're looking at. There's no generic ""oil;"" nonpolar substances like hexane, benzene, or decane partition into water in ways that are very different. If you want to see how they partition differently, check out some phase diagrams. :) Phase diagrams will illustrate how either temperature or pressure (y axis) affect miscibility of variable amounts of oil in water (x axis, molar fraction). This has a good section on the topic: http://en.wikipedia.org/wiki/Partition_coefficient


http://casey.brown.edu/research/crp/Edu/Documents/00_Chem201/images/Phase_diagram_of_a_binary_mixt.gif

Here is an example of a phase diagram. As you increase temperature (go up on y axis,) at a given molar fraction (x value), you will eventually hit a point of homogeneity. As you increase the molar fraction (go up on x axis) at a given temperature (y value,) you go from full miscibility to some miscibility, to full miscibility again. 

What is happening here is this: When x = 0, you have no oil. Water is just one phase. As you increase the amount of oil, it is soluble in water up until the water is saturated with it. This is the leftmost edge of the parabola. When you continue to add oil, the oil will partition into a second layer, and you will have two immiscible phases. However, recall that water and oil are soluble *in each other!* This means that as you add oil, some of the added oil ""dissolves"" the original water. When you've added *enough* oil, you hit the rightmost edge of the parabola. This is the point where you have enough oil to dissolve all the water, and you have only one phase again. Adding oil after this point, just increases the amount of ""unsaturated"" oil. This continues until you reach 100% oil, where you have full miscibility for the same reason that you did when you had only water.",null,0,cdhshxi,1qp2p0,askscience,new,1
7LeagueBoots,"Ice cores, sea-floor sediment cores, pollen cores, and tree ring records are all key indicators of past climate.

One of the most common and reliable ways of measure past thermal conditions is to measure the 18O and 16O (different isotopes of oxygen) found in the shells of animals that build with calcium carbonate. In colder time periods the shells contain more 18O and in warmer periods the ratio is more skewed towards 16O.  This is then correlated with the oxygen isotope ratios from ice cores with have an inverse 18O and 16O proportion.   This doesn't measure the temperature directly, but measures the change in temperature that the wet air moved through.

Within the ocean the temperature affects how much dissolved oxygen the water holds (warmer water, less oxygen) and, to a certain amount, acidity, although the latter is more closely associated with CO2 content. 

Each of these things leaves a very distinct record in the ocean floor sediments that can be read like any other stratigraphic sequence, or like tree rings.  These records show up in coral banding, plankton remains in shallow sea floor sediment (like that found between the Channel Islands and Santa Barbara in California, and spottily in the fossil record going back many millions of years.

Pollen cores provide species specific information about what plants grew in an area and allow past environments to be partially reconstructed.  We can compare the current range of those species with the past range and gain an idea of what the climate range was like at the time the pollen was laid down.

These records, and tree rings, all need to be looked at in the local context which adds a great deal of complexity to climate reconstructions.

Using these methods and looking at both ocean and land based lines of evidence we can build up a pretty long record of temperatures. 

Look at the RealClimate blog for some good articles on this sort of thing: http://www.realclimate.org/ ",null,1,cdf1fhg,1qovsz,askscience,new,12
darkness1685,"Plant biology PhD student here. Scientists rely on proxy measures to estimate things like prehistoric temperatures, which of course are impossible to measure directly. Ice cores are indeed one of the more reliable means of measuring past temperatures.

However, ice core data only dates back to about 500,000 years, so we must rely on other measures to get estimates from further back. One very interesting method is to use plant fossils. There is a rather good correlation between leaf morphological traits (such as leaf size, leaf margin, and number of teeth on the edge of the leaf) and temperature. This is due to the fact that leaves are best adapted to the climate that they evolved in. For example, large thin leaves are good for absorbing maximum sunlight in warm environments, but would lead to water loss in arid environments and freezing in cold environments. Therefore, paleobotanists can determine what the climate of the region was like based on the average trait values from fossils that they collect in a particular area. 

There are likely other ways that this is done as well, but these are the two methods that I am most familiar with. I should also point out that the leaf method is typically used to estimate climate at a localized scale. I'm not sure if it is used to estimate regional or global temperatures averages. I would need to look into this.",null,3,cdezc7y,1qovsz,askscience,new,9
chrisbaird,"You don't see the moon in its ordinary colors. Due to Rayleigh scattering, the moon viewed on earth's surface is slightly less blue than when viewed from space. The difference is so small though that you wouldn't notice it with your naked eye. However, sensitive instruments can detect the difference. 

It is the same with the sun. The sun when viewed from earth's surface is slightly less blue due to Rayleigh scattering than when viewed from space. The difference is so small though that human eyes can only detect it at sunrise/sunset when there is enough Rayleigh scattering to make the atmospheric reddening of the sun noticeable. But even at high noon when we don't notice the difference, the sun is still slightly redder (but is still mostly a broad distribution of all colors, which is white) as viewed on earth compared to space. Take a look at these spectra:

http://en.wikipedia.org/wiki/File:Solar_Spectrum.png

Notice how the blue and violet part of the sun's spectrum (around 400 -500 nm) drops down a lot after going through the atmosphere.",null,3,cdf46ma,1qoua9,askscience,new,16
dakami,"The intensity of light coming off the sun is vastly higher than what's reflected off the moon.  So the differentials -- from both the yellowish tinge of the sun, and the cyanish tinge from scatterying -- are vastly less.  What remains is equal enough across our photoreceptors that we perceive white on black, instead of yellow on blue.",null,0,cdfdi59,1qoua9,askscience,new,1
arble,"We see the sun in its ordinary colours too. On a clear night the moon does actually cause the sky to appear slightly blue - we often don't notice it because at low light levels we see primarily with our rod cells, which don't perceive colour.",null,10,cdexttw,1qoua9,askscience,new,3
nemom,"The clay pots add a thermal mass to the system.  Without them, the candle heats the air around it, and the warm air rises up to the ceiling.  You feel very little of that heat unless you put your hand right over the candle.  With the pots, the warm air gets trapped inside and heats up the pots, which then re-radiate the heat.

On a clear, sunny, chilly day, a black-top road can soak up the sun and re-radiate the energy and feel a lot warmer than the grassy yard next to it.",null,0,cdey3x4,1qos8t,askscience,new,2
norsoulnet,"I drew a picture [here](http://imgur.com/r8lSHby) explaining all the heat transfer processes based on the design I saw in the video.

At point A on the diagram, there is significant radiative heat transfer from the candle flame to the bottom pot (dark red pot in the image).  In addition, hot gas rises from the top of the candle and into the pot, where convective heat transfer cools the hot air and further heats the bottom pot.  The cool air drops down and out the bottom of the pot, and into the channel between pots.

There should be an inductive effect of this gas rising into the between-pot channels whereby cool air is drawn from outside and into the channel accompanying the hot gas from the inner chamber.

At point B, the cool air from outside mixes with the hot waste gas from the inner chamber, as well as convective heat transfer from the inner pot to the now cooler mixed gas.  This heat transfer convects across the flow (normal to the flow of mixed gasses) and to the cooler outer pot.  In addition to the two-fold convective heat transfer and mixing, radiative heat transfer propagates from the inner pot to the outer pot (depicted by purple arrows).  Al this heat transfer results in a moderately warm outer pot and relatively hot gas flowing from the between-pot channel, into the cavity on top and out the exhaust port.  Most of this gas forms a hot gas channel straight to the top of the room and heats the room very little (just like with bare candles).

At point C the moderately warm pot radiates thermal radiation (similar to a water/oil radiant heater, depicted by purple arrows) which is the primary mode of heat transfer, as well as natural convection of gas on the outside surface of the outside pot.",null,0,cdf3jvt,1qos8t,askscience,new,2
zendium,"The last cardiomyocyte to depolarize is the first to repolarize. So it means that the endocardium is the last portion of ventricle thickness to depolarize, but it will be first portion to repolarize.
Think of repolariation as being the opposite of depolarization. Whereas a positive deflection will be seen as a depolarized wave approaches a positive electrode, a replorization will show a positive deflection when moving away from a positive electrode",null,0,cdf3vhw,1qoqw8,askscience,new,3
Macrophagemike,"I was a cardiology technician for several years and am now in school to be a PA.  The way I was taught and understand why the different leads have different morphology all depends on what lead you are looking at.  For example if you are looking at lead II (which is inline with the axis of the heart) you will have maximum amplitude and a mostly positive tracing because the wave of depolarization is traveling directly toward the lead.  In general EKG's are positive when a wave of depolarization is traveling toward the lead and negative when it is traveling away.  So in a normal EKG you will see a biphasic or roughly equal positive and negative QRS in lead I because it's perpendicular to the path the wave of depolarization is taking.  It will appear positive as the depolarization is approaching the lead and negative as it moves further away.  I hope this makes sense and let me know if you have any other questions.  Bored in class talking about diabetes.
 ",null,0,cdicb55,1qoqw8,askscience,new,1
drzowie,"If you pour honey so that it forms a thin column from the honeypot (heh) to, say, a plate, then it will form a little coil.  The coil is formed by the interplay of two forces - inertia/motion and viscosity. Consider the path of the honey in the absence of viscosity.  That's easy -- it's the path the honey follows for most of its descent, i.e. an approximately ballistic shape, with each piece of honey falling approximately independently of the others, so that the entire stream traces the path that a single drop of honey would follow.  Now, insert a plate or something.  When it impacts the plate, the stream has to deform -- it has to go from long and thin to short and flat (part of the spreading pool).  That is a severe deformation, and the viscosity opposes it.  How?

Well, viscosity is a quantity that relates *shear motion* to *shear reaction force*.  Shear motion is a style of motion in which the velocity (a vector quantity) varies in length, if you travel perpendicular to the velocity vector.  For example, on a freeway if traffic is slow in the right-hand lane and fast in the left-hand lane, there is a leftward shear in the forward velocity of traffic.  If freeways were viscous (they have effects that act a little like viscosity, but are more complex), then there would be a reaction force decelerating traffic in the left lane and accelerating traffic in the right lane.  The reaction force would be directly proportional to the *difference* in speed between those lanes, and inversely proportional to the width of the freeway.   The constant of proportionality is the viscosity.  Ahem.

So the viscosity prevents spread of the honey as it lands, which makes a little mountain of honey.  The weight of each little drop of honey then drives deformation by squeezing the drops under it (that fell a little earlier), so the whole mound is slowly falling down and squishing out sideways.  But it turns out that, for some rates of pour, a direct-pileup mound like that is rare.  Why?  

Well, if the honey is viscous enough, it piles up a pretty high mound.  The mound is unstable to tipping over.  As it tips, the viscous forces keep the stream from disconnecting (easily), and force the next few droplets of honey off to the side.  Once they are off to the side, they pull the next few droplets and like that.  All via the viscous force.  Once the stream is going off to one side (call it a radial displacement from where it would land in the absence of viscosity) the geometry changes and it might come back to the center or move off in some chaotic path.  But for certain pouring regimes, you can get a stable orbit of the touch down spot.  In those orbits, the radial centering force imposed by the implacable fall of new droplets of honey (connected in the stream of course) is counteracted by centrifugal force as each droplet is whirled around slightly by the geometry of the existing mound of honey.  That makes the (in)famous coil.

Coiling is energetically preferred over direct mounding, since you don't have to deform the stream as much from a coil formation to get it to be flat.  In effect, a coil is already quite flat compared to direct pileup, so it doesn't take as much distortion to merge a coil with an existing puddle, as it does with a directly piled up mound.  So coil-mounds are shorter than direct mounds.  So once a coiling motion is started it's unlikely to change unless the flow from the pour changes drastically or the location of touchdown changes suddenly.


",null,0,cdf09dd,1qoker,askscience,new,21
derioderio,"Smarter Every Day covered [this phenomenon](http://www.youtube.com/watch?v=zz5lGkDdk78).  Basically, the stream of honey has to get out of its own way when it comes to into contact with the bottom, so it naturally starts going around in a circle.",null,1,cdf452s,1qoker,askscience,new,8
llevity,"I know you're asking more about blood adaptations, which I can't answer, but they do have muscle differences in their legs that they use to hold on while sleeping. 

Essentially, their feet grasp when they relax, in opposite to our hands that open when we relax.  This is how thud sleep holding on with their feet. ",null,7,cdewu58,1qoier,askscience,new,32
null,null,null,5,cdf0elp,1qoier,askscience,new,28
thacked,"Medical student here. It's been a couple years since pathology, but I'll try and recall. This was literally the first topic in the class, about cellular adaptation to stress, so forgive me that the details are scant. 

Basically, intracellular stress (like stretching) will upregulate certain proteins that expedite cellular keratinization and proliferation. This leads to increased thickness of skin, because of higher numbers of dead cells, and increased thickness of each cell layer. 

This is not the source I'm remembering, but it talks a little about the question at hand. 
http://www.ncbi.nlm.nih.gov/m/pubmed/20456341/",null,66,cdewaqc,1qohr1,askscience,new,365
ChokingVictim,"I have a follow-up question. As a weight lifter, my palms are covered in large callouses. They get torn open every week, multiple times. I know they have gotten bigger over the past few years, but do they get harder? I no longer have feeling under them, so I can't really tell

Are two-year old callouses harder than one- or even 1.5-year-old ones? Or is there a natural cap that occurs early on? Do Olympic lifters/rowers have rock-hard callouses?",null,4,cdf33j9,1qohr1,askscience,new,35
chillbro15,Can thin skin(that is skin without a stratum lucidum) develop a callus? And would the development of a callus result in the formation of a stratum lucidum? (I was wondering this because the development of a callus causes a thickening of the skin.),null,2,cdez6i4,1qohr1,askscience,new,18
null,null,null,0,cdf9fer,1qohr1,askscience,new,3
maya99_208,"Podiatrist here.  It's the bodies way of dealing with 'sheer forces"" like those from pronating and wearing sandals.  In the summertime, people who wear flip-flops will get thicker calluses due to increased sheer forces on their heels because there is nothing holding their heels in place.",null,1,cdf8prx,1qohr1,askscience,new,3
DoctorChick,"There are several reasons, the main being that intravenous injections carry a far greater risk than intramuscular injections.  

These include high risk for air embolism if the medical professional is not properly trained. Often you have flu shots given by nurses, doctors, pharmacists, etc... Everyone of them may not be fully proficient in intravenous injections.

Intramuscular injections provide very low risk for patients, there may be muscle soreness or aches, but it is a better alternative to the risks of venous delivery. And they can be done with very minimal training. 

They are just as effective in absorption of chemicals as an intravenous injection, though they may vary in absorption time.",null,3,cdf1bxq,1qof15,askscience,new,7
endocytosis,"The purpose of a flu shot is to ""train"" your body's immune system that it needs to attack a specific target, in this case the Influenza virus.  The cells that are designed to and look for trouble, called [macrophages](http://en.wikipedia.org/wiki/Macrophage) and [dendritic](http://en.wikipedia.org/wiki/Dendritic_cell) cells, are in the skin and mucosal surfaces, where pathogens are most likely to attempt entry, not in the bloodstream, although they travel through the bloodstream.  The reason is it's much more likely a pathogen will be introduced in a cut in the skin, or something you eat, then suddenly appear in a vein, but your body has other defenses to fall back on as well if something gets into the bloodstream.  In addition, part of your body's antiviral defense is to send an alarm that it's been infected.  If it's localized, other cells nearby and in a nearby immune training area called a lymph node get the alarm, but it's contained to that general area, like the arm or thigh.  If it's injected into your bloodstream, there's a risk too many cells could sound the alarm, and you could go into shock.

There are several sites for an injection.  One is intramuscular, one is intranasal (the live, [attenuated](http://www.cdc.gov/flu/about/qa/nasalspray.htm) flu vaccine is used that way), another is called intradermal, where the top surface is inoculated, nothing else.  Smallpox and sometimes Flu is administered this way.  The route of immunization depends on how the vaccine is formulated, and how it was studied.  For example, putting an intranasal vaccine in a intramuscular shot probably will not work as well.  It's been formulated to train cells in the nasal mucosa.

TL;DR The vaccine formulation is the optimal formulation *for the vaccination site* from the research that was done on it.",null,3,cdf18xp,1qof15,askscience,new,7
thegreatunclean,"They won't separate without significant work; it wouldn't be like oil-on-water if that's what you are envisioning.  Silver and gold will alloy together and form new compounds depending on the proportions and small amounts of additional elements but a 50/50 mix will get you some form of [electrum](http://en.wikipedia.org/wiki/Electrum).

I'm almost certain there are standard names for various silver-gold alloys but for the life of me I can't find the table in my metallurgy textbook.",null,0,cderez9,1qo4gs,askscience,new,11
toxthrway,"Toxicology is an amazing science. Although I can't give a complete answer to your question because it is such a huge question, I will try the best I can to give a good answer.

Compounds are toxic because they interfere with physiologic processes. Understanding toxicity requires an understanding of the nature and chemistry of the compound, the local cellular environment, the distribution of chemical from ingestion to site of toxic action (in pharmacy parlance this is called pharmacokinetics or toxicokinetics and is made up of Absorption, Distribution, Metabolism, and Excretion or ADME for short), and the way the compound acts on the site of toxicity (pharmaco/toxicodynamics). A basic tenet of toxicology is that the dose makes the poison. In other words, enough of anything will kill you.

Chemicals like potassium cyanide and sodium cyanide kill swiftly by impairing oxygen delivery to the cell. These chemicals do this by binding strongly to hemoglobin and displacing oxygen because they have a higher affinity for hemoglobin than oxygen does. Carbon monoxide works this exact same way.

Lead is another interesting case. Lead is a metal that has a 2+ charge on its outer atomic shell. Calcium also has this 2+ charge on it's shell. When lead gets into the bloodstream, it goes everywhere that calcium does and the body acts as though it's supposed to do everything that calcium should do, but it doesn't. Since it is larger than calcium it ends up causing blockages and improper neural firing in calcium channels. It also ends up in bones which causes structural weaknesses. We lead test children because children are especially vulnerable to lead. Their high need for calcium means that their bodies will readily take up any lead given and incorporate it into bones and the brain, leading to physical and mental deficits. 
Lead can also come in other forms which have carbon chains attached to them up to a maximum of four. The more carbon chains lead has attached to it, the more easily lead penetrates into the brain, and the more damage it causes to the brain and in turn behavior. Naturally lead wouldn't just find four carbon chains and attach them, but petroleum scientists found out that adding four of these groups onto a single lead molecule created an amazing compound that would double fuel mileage. This compound was called tetraethyllead and was in leaded gasoline. I do not know about the health effects from people breathing this in, but people working in the tetraethyllead factories were known for going permanently insane in months.

Simple carbon molecules have different toxicities that depend on their structures. Cyclopropane, the simpest possible ringed molecule, was used as an anesthetic gas (although it had a tendency to explode). The straight chained molecule n-Hexane has six carbons in a straight line. When the body takes in these straight chained carbon groups, one of the natural metabolic alterations it makes is to add a ketone group to the second carbon in. The body does this to n-hexane and makes a chemical called a gamma-diketone (two ketones spaces four carbons away). This chemical just happens to attach to nerve cells and slowly kills them from swelling, causing slowed down movement and mental impairment. Benzene, a ringed compound which has six carbons as well, causes damage to DNA in bone marrow via impairing chromosomal exchange due to chemicals called epoxides formed in the body's attempt to metabolize it. Larger ringed compounds similar to benzene called polycyclic aromatic hydrocarbons (PAH's) cause cancer by damaging DNA through epoxide formation as well, but through physically damaging the DNA bases and the structural integrity of the DNA itself.

Then there are chemicals which attack the neurotransmitter systems. There are a lot of systems to talk about, but the best one to talk about is the cholinergic system. These neurons release acetylcholine from vesicles within to adjacent sites. Since the cholinergic system is very very old evolutionarily speaking, it is a common and effective target for plants and animals who want to ward off attackers. Spiders, frogs, snakes, and those shells that shoot poison arrows all have toxins that target the cholinergic system. Some cause you to release all your acetylcholine, causing seizures then paralysis. Others, such as botulinum toxin (BoTox) prevent the vesicles from docking causing local paralysis. Others block the receptors, causing your respiratory muscles to paralyze and you to die or appear dead. Scopolamine, a toxin found in plants, is responsible for the whole Haitian zombie epidemic thing. There are over 10 different targets that I could think of off the top of my head given the time just for that system alone.

As I said before the dose makes the toxin. Things that exist naturally in your body will kill you in excessive amounts. Water intoxication comes from the water diluting your blood, altering the balance of electrolytes such as sodium, potassium, and calcium, leading to seizures and potentially death. Pure sodium metal, if you were dumb enough to try and eat it, would burn the hell out of your skin and you might catch on fire. Eating enough table salt (sodium chloride) will kill you, though I don't know exactly how. On a longer time scale, too much sodium can lead to problems with your heart. Potassium is also deadly. The pure metal will burn you like sodium would, but potassium chloride which is used in small doses as a heart medication will  ",null,1,cdet68a,1qo4g9,askscience,new,14
JollySars,"Cyanide closely resembles Oxygen (O2) in terms of molecular orbitals, therefore will bind to the Iron centres in haemoglobin and more importantly in mitochondrial proteins involved in respiration. Cyanide being an anion forms complexes are both kinetically and thermodynamically very stable, as such Oxygen is unable to displace cyanide resulting in it blocking aerobic respiration pathways.

It doesn't matter what atoms a compound/ molecule is made up of, its how the species behaves that is important. The elements that make up your body are never found in elemental form, pretty much always as fairly inert organic molecules. There are plenty of examples of Carbon and Nitrogen species that would be poisonous to cells.",null,1,cderl8h,1qo4g9,askscience,new,4
Mt_Hood,"It has a lot to do with the specific structure of the ""poisonous"" molecule. For instance, aromatic hydrocarbons have a benzene ring as a base...benzene is toxic, but add a few hydrogen and carbon atoms, and all of a sudden you have a rose smell.

Some molecules, the difference is literally a kink in the ""chain""...if a carbon atom was 120^o rotated from where it currently is, it can be the difference between life and death.

It really depends on the specific substance and how it interacts with a biological system...not some general rule of thumb.",null,0,cderd0n,1qo4g9,askscience,new,3
iorgfeflkd,"There are several, including the Lunar Reconnaissance Orbiter.",null,0,cdeq3h1,1qo1zu,askscience,new,14
theansweris7,"LADEE! Just got there, really cool mission (taking a look at lunar exosphere and dust environment). What's fun about LADEE is the science orbit is at such a low altitude that gravitational perturbations (from the moon not being spherical and constant density) make it somewhat unstable.",null,0,cdfv83k,1qo1zu,askscience,new,2
numbakrunch,"Free fall is free fall no matter where you are. If you are in an airplane (like the ""Vomit Comet"" NASA uses to train astronauts) that is accelerating downward at the same rate as you, then you will be weightless there too. The point is that weightlessness is an artifact of being in a ballistic trajectory--no reactionary forces like rockets or a floor ""rising"" up to meet you.

An orbit is just another ballistic trajectory that happens to be an elliptical one around Earth, but the Apollo missions were also ballistic, just not in a way that orbits the Earth. So to answer your question, no real difference except the way the trajectory is shaped.",null,1,cdeqwkl,1qo0lx,askscience,new,20
wwarnout,"After leaving orbit, the Apollo spacecraft was no longer accelerating, it was ""falling"" toward the moon (it had enough velocity to continue to move away from the Earth, and eventually move into the moon's gravitational field).",null,2,cdepo4z,1qo0lx,askscience,new,9
lithiumdeuteride,"'Zero g' is a misleading phrase.  People interpret it to mean that there is no gravity in space, which is completely wrong.  It actually refers to sensed acceleration, which is what produces weightlessness when equal to zero.  Sensed acceleration is the difference between gravity and acceleration.

W = m*(g - a)

where W is weight, m is mass, g is gravity, and a is acceleration (all quantities except mass are vectors).

If you are in free-fall (equivalent to orbit), gravity is the only thing dictating your acceleration, therefore g = a, and (g - a) = 0, so you are weightless.

If you are standing on the ground, your acceleration is roughly zero, therefore your weight is W = m*(g - 0) = m*g.",null,0,cdex6sx,1qo0lx,askscience,new,6
pomjuice,"You need to think of this as a point of reference.  When you are orbiting the Earth, you are freefalling toward earth.  Now, when you are orbiting the moon, you are freefalling toward the moon.

Your question asks what about the in between. Each celestial body has a sphere of influence. When you are in between these spheres of influence, you would be orbiting the sun... and thus freefalling toward the sun (from an incredible distance at remarkable speed)",null,1,cdf24dr,1qo0lx,askscience,new,3
Mongoose1021,"Think of this question differently. When ""G"" is not zero, it's because something is pushing up on our feet, and has very little to do with gravity. Just feet.

On Earth, if Gravity had its way, we would fall towards the center, and the dirt we stand on would, too. However, there's dirt under the dirt (and some lava) it, so the dirt can't fall - and neither can we. In stead, the dirt pushes up on our feet. And, no matter how hard we kick the dirt, it's going to sit relatively still.

In normal situations, when there aren't 10^24 kg of dirt and lava under your feet, when you kick something, it moves away from you - and can't push on your feet. 

So, in terms of gravity and perceived G, there's very little difference between orbit and traveling to the moon. Any time you're not pressed directly against a solid, extremely massive object, 0G.",null,4,cdevujv,1qo0lx,askscience,new,5
Urgullibl,"Unlikely. In fact, nowadays, chemical castration is done by using a GnRH superagonist that dramatically decreases the levels of FSH and LH, which in turn pretty much turns off your testicles. So yeah, it's highly unlikely to have high testosterone if FSH and LH are low.

Edit: This may sound a little technical, so let me explain: First in line is your brain, which releases GnRH. GnRH tells your pituitary gland to produce FSH and LH. LH tells the Leydig's cells in your testicles to produce testosterone. Cut out one of these hormones, and you cut out everything that follows.",null,0,cdesz9k,1qnz6z,askscience,new,3
_Momotsuki,"LH and FSH signal your gonads to produce testosterone, they aren't ""precursors"". Having low LH and FSH but normal testosterone could mean that you're having an exogenous supply of testosterone (e.g. you've been injecting yourself) or there's something else in the body producing testosterone (e.g. a germ cell tumour).

As for manipulating FSH to change the level of hair in an individual without changing their testosterone - from my understanding it's impossible. It's the testosterone which is causing masculinisation (growing hair).",null,0,cdesxzm,1qnz6z,askscience,new,2
mc2222,"A good rule of thumb is that light travels as a wave but interacts with matter as a particle. This means that any *interaction with matter (atoms/molecules) must occur in discrete quanta* of **energy**. Things get very messy if you try to use the particle picture to explain how light *travels*. 

**To use the wave model:** (which, to me makes more sense in this context)

To use the wave model, let's go back to the derivation of the wave equation from Maxwell's equations. When you derive the most general form of the speed of an EM wave, the speed is v=1/sqrt(mu epsilon). In the special case where the light travels in vacuum the permittivity and permeability take on their vacuum values (mu0 and epsilon0) and the speed of the wave is c. In materials with the permittivity and permeability *not equal* to the vacuum values, the wave travels slower. Most often we use the relative permittivity (muR, close to 1 in optical frequencies) and relative permeability (epsilon_R) so we can write the speed of the wave as c/n, where n=1/sqrt(epsilonR muR).  **So in the most basic sense, it is because the material has some properties that are magnetic and electric, and so will effect the electromagnetic wave traveling through it.** (bold for emphasis only).

If you insist on using the **photon model** to explain the slowing of light, this is the best explanation I have found - its a bit of a mess:

&gt;A solid has a network of ions and electrons fixed in a ""lattice"". Think of this as a network of balls connected to each other by springs. Because of this, they have what is known as ""collective vibrational modes"", often called phonons. These are quanta of lattice vibrations, similar to photons being the quanta of EM radiation. It is these vibrational modes that can absorb a photon. So when a photon encounters a solid, and it can interact with an available phonon mode (i.e. something similar to a resonance condition), this photon can be absorbed by the solid and then converted to heat (it is the energy of these vibrations or phonons that we commonly refer to as heat). The solid is then opaque to this particular photon (i.e. at that frequency). Now, unlike the atomic orbitals, the phonon spectrum can be broad and continuous over a large frequency range. That is why all materials have a ""bandwidth"" of transmission or absorption. The width here depends on how wide the phonon spectrum is. [Fowels](http://www.amazon.com/Introduction-Modern-Optics-Dover-Physics/dp/0486659577)

A more brief explanation comes from [wikipedia](http://en.wikipedia.org/wiki/Photon#Photons_in_matter)

&gt;The slowing can instead be described as a blending of the photon with quantum excitations of the matter (quasi-particles such as phonons and excitons) to form a polariton; this polariton has a nonzero effective mass, which means that it cannot travel at c.
",null,2,cdeqbq4,1qnxzo,askscience,new,8
whathappenedtosmbc,"Light is an oscillating electric field. Because electrons are charged, they react to the incoming field and will start oscillating with the field. But these moving currents create a new oscillating field. The net effect of this cartoonish picture is a net field that moves slower. 

The assertion below that the speed of light is the same is not necessarily wrong, but more irrelevant. The speed of a bare photon is not changed by being in a material. But in a material a bare photon is not really observable, but the effective theory of light in the material where it in fact moves at a speed of less than c.",null,1,cdeqetb,1qnxzo,askscience,new,3
null,null,null,7,cdepfp1,1qnxzo,askscience,new,1
O_Zenobia,"You're asking about integrons!

Yes, plasmids can integrate into the host chromosome. The integrated plasmids can later transfer *chromosomal* genes by conjugation. 
Transferred genes can include those encoding antibiotic resistance, but I'm not aware of any effects on the persistence of this trait -- probably because they encode proteins that let them transfer back out of the chromosome and into transposable elements. 

http://www.ncbi.nlm.nih.gov/pubmed/9835490

http://www.ncbi.nlm.nih.gov/pubmed/11432416",null,0,cdermm9,1qnvaa,askscience,new,2
KarlOskar12,"If a plasmid codes for something useful like antibiotic resistance it is kept, it if does not it is quickly removed by the bacteria. It takes a lot of energy replicate DNA so the bacteria is very picky about what plasmids it keeps and what plasmids it will remove. It doesn't really change the definition of a resistant strain at all. If it is antibiotic resistant it is antibiotic resistance whether the resistance came form the first generation bacteria or it acquired resistance from a plasmid.
&gt; Does this process lead to more persistent antibiotic resistant strains

What do you mean by this?",null,2,cdeq49v,1qnvaa,askscience,new,1
bluemed17,"In short, breathing other kinds of smokes is also terrible for you.

Take, for example, benzopyrene one of the major carcinogenic and mutagenic agents in cigarette smoke. It isn't an added chemical to cigarettes but rather is a natural byproduct of burning organic matter. Benzopyrene is also made in burning wood, coal, and diesel. In fact, it can also be found in charred meat (at least according to wiki).

So, while some chemicals might be added to cigarettes during their processing, a lot of the ""bad things"" are simply a result of the burning process.",null,0,cdepvii,1qnu2z,askscience,new,10
bearsnchairs,"Many carcinogenic compound are created during smoking of a cigarette, or are transferred to the smoke. Right behind the ember is a region of high temperature and low oxygen.  Pyrosynthesis occurs in this region making various VOCs, volatile organic compounds which are low molecular weight and are responsible for most of the cancer risk, poly aromatic hydrocarbons like benzopyrenes, and other partially oxidized compounds. Behind this region is a zone of lower temperature where distillation occurs. Compounds are transferred intact to the smoke. Many nitrosamines  are transferred in this manner and they are very carcinogenic.

The main takeaway is that most of the cancer risk comes from compounds that are produced when burning organic material. Some risk is inherent in the form of nitrosamines which are specific to tobacco or are produced by microbial action or other aging processes. Different tobaccos contain different amounts due to the way they are processed.

Source: I was a tobacco chemist.",null,0,cdf0dl5,1qnu2z,askscience,new,3
Urgullibl,"What sets tobacco smoke apart from other kinds of smoke are nicotine (the compound you actually get addicted to and that's also pretty bad for your cardiovascular system in the long run) and Polonium-210, which is accumulated by the tobacco plant for some reason and results in [your lung being exposed to radioactivity](http://www.ncrponline.org/Learn_More/Did_You_Know_95.html) to the tune of 13 mSv per year for the average smoker.",null,0,cdeth95,1qnu2z,askscience,new,2
ryansouth21,"From what I can tell, most of the extra stuff are by-products of the tobacco curing process and the paper. Removing just a few of the carcinogens would probably increase the cost of the cigarette production, and not lessen by a significant number the chance of cancer. ",null,2,cdeoel8,1qnu2z,askscience,new,2
IAmMe1,"What an ""analytic solution"" usually means is that we can write it in terms of elementary functions: arithmetic, exponentiation, logarithms, trig functions, and hyperbolic trig functions. Usually this excludes the situation where we can write down, say, an integral of such functions but cannot evaluate the integral. For example, the integral of e^(-y^2) dy evaluated from -infinity to x has no analytic solution.

A numerical solution is one where we can make a plot of the solution - i.e. if I hand you particular numbers for the input variables, you can hand me the number for the resulting output. This happens when we, for example, make a computer solve a differential equation for us; even if there's no possible way I could write down an analytic solution, the computer still has ways to get numerical values.",null,2,cdeoju4,1qntp8,askscience,new,13
peni5peni5,"It means it can be expressed symbolically using a restricted class of functions. What that class is exactly is hard to say. Sometimes it's just elementary functions, maybe you could also use Bessel functions or something standard like that -- something well-understood. That's the best you can have, because you can see how the parameters of the equations enter the expression and how they affect the behavior of the solution. That can be hard to recognize in a numerical solution (e.g. the solution is the sum of terms f and g, and f is bigger than g for your simulations, you might not be able to study the smaller term g, and g might be important for other values of parameters).",null,2,cdepnmi,1qntp8,askscience,new,6
xiipaoc,"Sometimes you can write down an equation and solve it.  For example, the equation for a simple harmonic oscillator is x'' + w^2 x = 0, and the solution is x = A cos wt + B sin wt.  If you know where x starts and what the speed is at the beginning (x(0) and x'(0)), you can solve for A and B and get an equation.  I could go into explaining how you can get this solution, but it's a pretty simple formula.  Pick some value for w^2 and you know what x is as a function of time.  Now, the equation for a pendulum is Ø'' + (g/L)sin Ø = 0.  The formula to solve that isn't so simple!  As far as I know, nobody has been able to actually find a formula.  So to solve this, you have to use numerical methods.  You can program a computer to solve this equation (or pretty much any equation) starting at some value of Ø(0) and Ø'(0), and you can let the computer run the solver as long as you want and find Ø at any time you want; you just have to wait a while, and your solution will only be as close as your model can get it.  In contrast, in the simple harmonic oscillator, you can just plug in t into the equation and get an answer.  For the pendulum, you have to run the model enough times with the right parameters to get something close enough.

Basically, an analytical solution is an exact answer to a problem, and a numerical solution is an approximate answer.  It's not always possible to find the exact answer, so sometimes you have to use numerical methods.",null,0,cderxm9,1qntp8,askscience,new,2
-to-,"Analytical solutions are usually meant to fit two criteria:

* They can be written in closed form, as x=A, where A does not depend on x.

* In the expression above, A can be evaluated to arbitrary precision. What is meant by that typically involves how the cost of calculating the result depends on the required precision. Elementary functions, as well as most special functions, can be evaluated with techniques that ensure the number of operations increases at most proportionally to the number of digits of precision. Integrals or solutions to differential equations do not fit that criterion *in general*.

Note that an expression being analytical does not mean that the result can be evaluated easily. I've seen analytical expression with sums over 10^24 terms... On the other hand, finding a root of a function is very fast, but not considered analytical. YMMV.",null,0,cdeux0x,1qntp8,askscience,new,2
GOD_Over_Djinn,"An analogy might be to determine the value of the infinite summation 1+1/2+1/4+1/8+...=∑1/2^n where n goes from 0 to ∞. I'll call this number S. We can use some very primitive ""numerical methods"" to try to find S: just plug in higher and higher values of n and evaluate it and see if it looks like it's acting like it converges. So for n=2 we have S=1+1/2+1/4=1.75. For n=4 we have S=1+1/2+1/4+1/8+1/16. 1+1/2+1/4+1/8+1/16+1/32+1/64+1/128+1/256=1.9960. We might conclude by our numerical methods that S=about 2, or if we are very confident in the power of this numerical method, S=2.

But instead of doing that, we might realize that we can write the nth partial sum of S, which I'll call S(n) as

S(n)=1+1/2+...+1/2^n-1

But then

S(n)/2 = 1/2 + 1/4 + ... + 1/2^n

and so

S(n)-S(n)/2 = 1-1/2^n

which implies

S(n)=[1-1/2^(n)]/(1/2)

taking the limit of this as n goes to infinity gives 1/[1/2]=2, so we have determined *analytically* rather than *numerically* that S=2.

Now, this picture that I've painted of what numerical methods look like is almost clownishly simplistic. Numerical methods are sophisticated and accurate and often they are fast. But hopefully this gives you an idea of why it is nice to have an analytic solution.",null,1,cdfbzzy,1qntp8,askscience,new,3
my_coding_account,Is there any fundamental difference between analytic and nonanalytic functions?,null,0,cdeud52,1qntp8,askscience,new,1
claireauriga,"As others have said, an analytic solution is one where you can write what you're trying to find as an equation. For example, if you had dy/dx = x^2 + 3, and you could work out that y = x^3 /3 + 3x + c, where c is a constant. This is an analytic solution. If I know x, I can now work out what y is. 

In some cases, it's impossible or highly complicated to get to that equation. For example, I work with systems where I have a hundred different variables to find (x, y, z ...) and a hundred differential equations, and I need to solve them all simultaneously, and this is supposedly a simple system! We find answers for this - like 'what is x at time t = 300 seconds' - using numerical methods. These work by breaking it down into tiny little chunks and working them out - approximating curves with lots of little straight lines. If I had a diagram that would be easier to explain, but you can look up Euler's Method to learn more about a simple technique. 

Numerical solutions are not exact, but you can make them more accurate by doing smaller steps and more calculations. They are incredibly useful! ",null,0,cdeuyyo,1qntp8,askscience,new,1
hellenkellersdog,"I am surprised that you have not received an adequate answer thus far so I will do my best to recollect an old NOVA episode about mars. From what I can remember the reason that Mars had its atmosphere stripped away by the solar wind is because the magnetic field that surrounded the planet weakened because the core cooled/ stopped spinning as fast. *Think of the core as a gigantic spinning magnet* This allows more charged particles (solar wind) to enter the atmosphere and therefore strip it away. The Earth, on the other hand, still has a strong magnetic field from our core, which protects us and our atmosphere from the wrath of the Sun.",null,8,cdes24g,1qntjp,askscience,new,23
ggrieves,"The process of a planet losing atmosphere to space is called 'atmospheric ablation'  It is complicated in that there are many different mechanisms contributing to it with different factors. Earth does lose atmosphere to the solar wind, but it primarily loses the lightest species (hydrogen and helium) and a little bit heavier things like O if they're ions. Ionized species get picked up by the magnetosphere and interplanetary electric field and get accelerated away. We have a lot of water vapor in our atmosphere though, so earth replenishes the hydrogen and oxygen lost. Venus is depleted in hydrogen and O+, so its atmosphere is dominated by CO2 which is tough to photodissociate, therefore it can't heat up enough to reach escape velocity. Venus can therefore have a bit of an ionosphere due to gas ionization despite not having a geomagnetic field, and that helps protect it. Mars has neither the magnetic field nor active volcanoes spewing CO2 into the atmosphere, so other atmospheric components are removed faster than they are replenished. Earth is the only terrestrial planet to have a geomagnetic field that can extend upwards of ~10 earth radii into space. This is a significant shield. In order for a burst to strip earth's atmosphere, ion radiation would have to be intense enough to cause a significant increase in ionization of gas molecules. but at high density, they create a thicker ionosphere that can partially repel some of the magnetic field that would accelerate them to escape velocity. If the solar storm also had a strong enough magnetic field to partially neutralize earths magnetic field and the induced ionosphere, then it could possibly increase the ablation significantly. It would likely be extremely destructive to all artificial satellites, ground based electronics, and probably lethal to living organisms. [source](http://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=4&amp;cad=rja&amp;ved=0CEoQFjAD&amp;url=http%3A%2F%2Fwww.eos.ubc.ca%2F~mjelline%2F453website%2Feosc453%2FE_prints%2Fnewfer08%2Fmoore07.pdf&amp;ei=Lb2FUr3qFcfmkAeL84CoCA&amp;usg=AFQjCNFbXeIFZAgGY340N7VJsQN0cQR0Xw&amp;sig2=65Fw57r8gjsS2jrERyL-wg&amp;bvm=bv.56643336,d.eW0)

EDIT: Mars and Venus are also much lighter than Earth, so escape velocity is smaller and molecules don't need as much acceleration",null,1,cdesrbu,1qntjp,askscience,new,16
null,null,null,8,cdeoo7q,1qntjp,askscience,new,3
KarlOskar12,"If you take the eye color of the polar bear for example they all have brown or black eyes as an adaptation to their environment (darker eye colors protect their eyes). All polar bears live in the same environment. However, people do not. They come from Africa, Asia, Europe, etc. Those are all very different environments so you would expect the eye colors to all be different because the environments call for different colors.",null,3,cdeqhw0,1qnt7w,askscience,new,6
Surf_Science,Sexual selection ,null,3,cdercbe,1qnt7w,askscience,new,6
countingbyfives,"Fourth year pharmacy student here.

I'm still looking over the new recommendations, but it seems they're shifting towards more personalized treatment regimens, and less of a focus on hitting defined LDL goals.  A similar shift in was implemented in the ADA guidelines for treatment of diabetes.

Statins work primarily by inhibiting HMG CoA reductase, an enzyme that initiates the pathway for synthesizing endogenous cholesterol via hepatocytes in the liver. While this has a slight decrease in total cholesterol, the greatest effect occurs because the hepatocytes respond to this decrease by taking up more LDL cholesterol from the bloodstream for synthesis into bile salts.There appear to be other benefits as well - such as reducing atheromas - but the mechanism and evidence still needs to be further explored.

While I don't believe elevated blood cholesterol levels alone are responsible for cardiovascular disease and associated events, I do believe it's a significant contributing factor. Diet and exercise has been shown to have the greatest effectiveness  at preventing heart disease, and as such it often remains as the first line treatment. Patients rarely make the effort however, and I think in those cases drug therapy is required.
",null,2,cdeqz1m,1qnssw,askscience,new,6
dakami,"Basically, statins are perceived as incredibly safe, and have a significant effect on reducing heart attacks.  There was talk some time ago about putting them in fast food restaurants.  Don't laugh, significant moves in public health have always been about making treatments ambient -- see iodine in salt, or fluoride in water.",null,0,cdfdcrm,1qnssw,askscience,new,1
mzyos,"Final year med student here.

It is also of note that statins also have other useful properties. They are anti-thrombotic (a thrombus being a blood clot in a vessel that causes a heart attack). 

They also stabalize plaques of cholesterol in the vessels, and came sometimes cause them to devolve (these are the part of the vessel that can crack and form the blood clot.

  Another noted ability is to restore the function of the inner part of the blood vessel, essential for proper blood flow, which also helps avoid a clot forming.

  They decrease macrophages in the vessel walls consuming cholesterol, which leads to the plaques, and they have been noted to stop these cells proliferating, which would lead to increased cholesterol consumption.

So they don't just lower LDL, they do a lot more, and some of this is still being researched, so we don't have all the answers yet. Therefore, it is very sensible to stick anyone with risk factors on these.",null,0,cdojumw,1qnssw,askscience,new,1
Jetamors,"Because it's easy. Here are some of the other major options for estimating or measuring body fat:

* Calipers can be used to take measurements used to estimate body fat. For this, you need someone who's trained to do the measurements and a place to do them. The person being measured will have to go to that place, take most of their clothes off, and stand for a few minutes. You're going to end up with a bunch of numbers that have to be written down and/or input into a computer. If you measure twice, you're not going to get the same numbers both times.

* Body fat meters--run a mild current through the person being measured and use the impedance to determine their body composition. You need to buy a device for this, and accurate ones are expensive. It's going to be inaccurate if the person being measured is dehydrated, and it's going to change depending on how long it's been since they last ate or exerted themselves. You'll probably need someone to do the measurements.

* Dunk the person in water and measure their displacement, Archimedes-style. This is good because it's a direct measurement! But you're going to need a big tank of water. The person being measured is going to have to put a swimsuit on (they'll need a place to change) and has to be willing and able to be completely immersed in water. And again, you need a trained person there to take down the measurements.

There's a longer list of techniques [here](http://en.wikipedia.org/wiki/Body_fat_percentage#Measurement_techniques), but they all have similar issues: you need an expensive device, you need a person to take the measurement, the method isn't accurate or precise, etc. 

Compare that to BMI: all you need to know is weight and height! It's two numbers in a simple equation--no complicated math! We already take those automatically at every doctor's visit! Using inexpensive, accurate devices! You don't even need to take a measurement--you can just ask people what their weight and height are! We've been recording weight and height for tens or hundreds of thousands of people for a hundred years!

Is it the best method? Of course not--that would be underwater dunking, for my money. But if you want a quick and dirty way to [identify potential anorexics](http://bjp.rcpsych.org/content/185/4/312.full), or [estimate obesity rates in large populations](http://www.who.int/mediacentre/factsheets/fs311/en/) or [perform longitudinal studies over decades](http://www.ncbi.nlm.nih.gov/pubmed/15130155), it's hard to come up with a better method.",null,4,cdeqa65,1qnr75,askscience,new,30
patiscool1,"BMI doesn't work very well for individuals, but it works very well when stratifying populations into risk factor groups for hypertension/diabetes/colon cancer and tons of other diseases.

A very muscular person has a high bmi but they make up the very small minority of people with high BMIs. In general it's still a very useful tool for assessing populations.",null,5,cdeq9t4,1qnr75,askscience,new,28
KarlOskar12,Because very few people have tons of muscle and 6% body fat. BMI is still a good rule-of-thumb for the general population. If it obviously doesn't apply to someone then it shouldn't be used (obviously).,null,2,cdeqb0h,1qnr75,askscience,new,22
qyll,"Of course, there are methods to measure % body fat such as densitometry or imaging, but those are prohibitively expensive if you ever want to get a good sample size.

Anthropometric methods like waist circumference and BMI often do (believe it or not) just a good a job as predicting things like CVD mortality for a fraction of the cost, and it suffices for large scale nutritional studies.

In the end, money is the main reason.",null,1,cdeq1tb,1qnr75,askscience,new,6
darkness1685,"It is used so often because it is extremely easy to measure those two variables, and the ratio still gives you a pretty good indication of overall health. For example, those two measurements are usually recorded whenever someone gets a drivers license, allowing the potential for large amounts of data for any study on BMI (I'm not sure if this data is actually used in this way, just a thought).",null,2,cdepmg5,1qnr75,askscience,new,6
Simon_Riley,"Yes, it is a solid general statistical tool for a large sample.  However, I believe the OP is questioning its uses in the more specific scenarios.  It is definitely not ""obvious"" when BMI should be properly used!  For example in healthcare, BMI is something you can use for a large patient study but not necessarily when you are treating patients individually as it is being used currently.  For example, patients with chronic diseases (cancer, OPD, chronic HIV) develop a wasting syndrome called cachexia (loss weight and it is mostly lean body mass).  Now this includes a wide spectrum patients (oncology, pulmonary etc etc) whose primary concern is not weight loss per say.  However, loss of lean body mass is a huge contributing factor to death in those patients.  Since BMI is so commonly used as a way to access the healthiness of body weight, physicians will simply look at the BMI and think there's nothing wrong.  In fact, your BMI could be in the obesity range (over 30) but you could be suffering lots of muscle loss from cachexia.  In an effort to make things better, there is an ongoing effort where they look at the CT scan of patients to more accurately estimate body composition.  This led to me looking at lots and lots of CT scans at work.  Just wanted to chime in as it is my field of work! 

TL DR: BMI is solid for population studies but its limits in clinical setting is not being recognized, leading to its current over use.  ",null,0,cdfmvv2,1qnr75,askscience,new,1
wazoheat,"Okay, first, there's a misconception in your question that I need to clear up: Even if you dig a tunnel straight down, the ground above you is still pulling up. You've removed the matter directly above you, but the matter all around the hole is still pulling up on you. In fact, in the grand scheme of things the tiny amount of earth you would have removed above you would be insignificant, so the pull of gravity whether you dug a tunnel or magically transported would be the same.

Whew, okay that wordy caveat is out of the way. Your line of thinking is partially correct; gravitational force does decrease as you get into the body which is producing the gravitational force. However, as it turns out, [the decrease is completely linear](http://hyperphysics.phy-astr.gsu.edu/hbase/mechanics/earthole.html)! When you do [the complicated integration](http://hyperphysics.phy-astr.gsu.edu/hbase/mechanics/sphshell2.html#c1) out completely, the equations all sort out so that the gravitational force from the ""shell"" of earth outside of the radius you inhabit completely cancels out, just leaving the contribution from the earth below you. This means that if you go halfway to the earth's core, the force of gravity pulling you down will be exactly half of the force that was pulling you when you were at the surface. 30% of the way to the center means a 30% reduction of gravity, 99% of the way to the center is a 99% reduction in gravity, etc. Pretty cool, huh?

Edit: be sure to check out the other answers which are more correct: my answer assumes a planet of uniform density, which is not true for Earth.",null,0,cden827,1qnqmj,askscience,new,8
subroutines,"We have various brain regions (e.g. somatosensory cortex) that are responsible for processing sensory information (touch, [proprioception](https://en.wikipedia.org/wiki/Proprioception), pain, etc.) from the different parts of our body. The somatosensory cortex is laid out like a map (i.e. [homunculus](https://en.wikipedia.org/wiki/Cortical_homunculus)) of our body....



* [like this](http://i.imgur.com/gPlqUw4.png)
 

If I were to tickle your arm, neurons in your somatosensory ""arm"" region would become activated, and you would perceive the tickling sensation to be coming from your arm. Now, lets say I were to stick a tiny electrode into your somatosensory ""arm"" region and stimulate that part of your brain; you would also perceive a tickling sensation in your arm. So I have kinda tricked your brain into thinking something is touching your arm by artificially activating those neurons. 

When an amputee loses a limb, like their arm/hand, those neurons in the somatosensory cortex that process 'arm' and 'hand' information are no longer receiving their usual input. They are just sitting there with nothing to do all day. But neurons hate being lazy. They crave activation (because activation provides them with treats, like growth factors and neurotrophic factors). So they start to seek out new sources of activity. Right nearby to the arm-region is the neck-region and right nearby the hand-region is the face-region. The arm/hand regions are jealous of all the activity those other nearby regions are still receiving and want to get in on the action. So those arm/hand neurons start to send projections into the neck/face regions. So now, when something brushes across the amputee's face, some of the 'hand-neurons' are picking up that signal, and when something brushes across the amputee's neck, some of the 'arm-neurons' are picking up that signal. But since they are still technically 'hand-neurons' and 'arm-neurons', the amputee perceive these sensation as coming from their hand/arm! 


source: I'm a grad student at UCSD and was a TA for VS Ramachandran.",null,1,cdfbooa,1qno7e,askscience,new,5
CompMolNeuro,The primary sensory cortex is still there.,null,1,cdfbmpm,1qno7e,askscience,new,2
ProfEntropy,"Natural sugars (sucrose, glucose, or fructose) in non-diet sodas have a lot of functional groups that form a special type of intermolecular bond (called a [hydrogen bond](http://en.wikipedia.org/wiki/Hydrogen_bond)) with water molecules. It is these interactions that creates sticky solutions of water and simple sugars.

Diet sodas taste sweet because [sugar substitutes](http://en.wikipedia.org/wiki/Sugar_substitute) are added instead of the natural sugars mentioned earlier. These solutions are less sticky for two reasons: (a) most of substitutes taste much sweeter than sugar, so much less needs to be added to get to the same level of sweetness, and (b) they often form less hydrogen bonds with water as do sucrose, glucose, or fructose.",null,1,cdels02,1qnlk0,askscience,new,6
zachalicious,"You are correct that the stickiness comes from the sweeteners, but the difference is the sweeteners used. Most diet sodas are sweetened with aspartame, which is not sticky. Most regular sodas are sweetened with high fructose corn syrup (or, in some cases, regular sugar). Corn syrup especially is highly sticky, and regular sugar is sticky when mixed with liquid as well. I don't know the more scientific/technical answer, but that's the gist of it. ",null,1,cdelkyn,1qnlk0,askscience,new,2
WhoopyKush,"There's no significant difference in the way a larger oven converts electricity into microwaves, hence it is unlikely that there will be any significant difference in efficiency.  

With a smaller-wattage unit, the user must wait longer.  Their impatience may make them willing to accept food that hasn't been warmed as completely, so  they may use less power to heat their food, on average.",null,0,cdgkogy,1qnl5z,askscience,new,2
brosephlargewood,"The unit watts denotes energy per second. Therefore, if you run two microwaves for the same time the higher wattage model will use more energy. 

To determine which one is more efficient you would need to heat up a similar item in each, record the time it takes for the item to become sufficiently hot, and then multiple the time by the microwave oven wattage and see which number is higher (i.e. which used more energy).

There are a large number of other factors that could be at play though, such as power level, rotation, etc. ",null,0,cdfsipi,1qnl5z,askscience,new,1
wazoheat,"Yes. For instance, in Japan [seawalls are meant for two purposes](http://www.nytimes.com/2011/03/14/world/asia/14seawalls.html?pagewanted=all&amp;_r=0), protecting from both storm surges from typhoons and tsunamis from earthquakes. 

The [Galveston Seawall](https://en.wikipedia.org/wiki/Galveston_Seawall) in Texas is an example of a seawall built solely to protect from storm surges.",null,0,cdemitz,1qnjax,askscience,new,3
a_Dewd,"Seawalls can definitely reduce the impact of storm surges etc. however, the more you fortify the coast in one area the more devastating impacts it can have on other areas that do not have fortified coasts as the energy gets deflected. ",null,0,cderna4,1qnjax,askscience,new,2
homininet,"Absolutely other animals get them, If you happen to have a dog, look that their elbows. I know that my dog has some pretty big calluses there. Also, certain primates (old world monkeys: African and Asian monkeys) have some pretty big calluses, known as ischial callosities over what is known as the ischial tuberosity (basically the butt bone). And they are there for the same reason you say, they're abraded all day long, by sitting on their butts. ",null,1,cdelxo7,1qniul,askscience,new,4
iorgfeflkd,"Only to the extent that the underlying air pressure distribution depends on gravity. The speed of sound changes higher or lower in the atmosphere because the pressure changes because gravity pulls the air into a certain density distribution.

I can envision a scenario like a gas around a black hole where tidal forces are so strong that sound waves are more directly influenced by gravity, but I don't think such a situation exists. Maybe inside neutron stars.",null,1,cdepguj,1qnhy3,askscience,new,7
kloddant,"Gravity affects sound in a 2 ways that I can think of: 1. air pressure (no effect for an ideal gas) and 2. gravitational waves (super negligible)

1. Gravity increases air pressure, and air pressure increases air density, which decreases the speed at which sound waves propagate.  However, the increased air pressure increases the speed at which sound waves propagate, so these two effects cancel themselves out, and temperature becomes the main variable.  See the altitude variation section of the [Wikipedia article](http://en.wikipedia.org/wiki/Speed_of_sound) on the speed of sound.  

2. Gravity itself causes imperceptible [gravitational waves](http://en.wikipedia.org/wiki/Gravitational_wave) in space-time.  These wave patterns in space-time cause the matter that exists in it to move as well.  This movement of matter is a sound wave, and it would interfere constructively and destructively with other sound waves, but not to a degree that is noticeable by any device that we have now.  If [LIGO](http://www.ligo.caltech.edu/) can't pick them up, a guitar does not have a chance.  Gravitational waves happen whenever any bit of matter moves.  The amplitude of a gravitational wave is dependent upon the velocity of matter relative to other matter, not on the absolute strength of the gravitational field in one particular area.  Therefore, if your guitar were standing still relative to the Earth, both on Earth and in outer space, the strength of the gravitational waves between the Earth and the guitar would be zero in both cases.  If, however, your guitar were moving up and down relative to the Earth, then the gravitational waves between the two would be stronger on the Earth than farther from it.  However, in either case, these waves would be far to small in amplitude to notice, and any waves that were picked up would be drowned out by other noise, such as particles of helium and photons hitting the strings and such.",null,2,cdf83s2,1qnhy3,askscience,new,1
homininet,"The fallopian tubes and the ovaries are two developmentally different systems. The fallopian (or uterine) tubes are the remnant of a structure called the paramesonephric duct. The duct in an embryo basically just opens up into the future abdominal cavity. The gonads, on the other hand form from a ridge-like structure along the back part of the future abdomen. Eventually, things called germ cells migrate into that ridge and eventually will become the eggs. Unfortunately these two things are separate, so in order to get the egg to where they're going to meet the sperm, you need a collecting duct, which is the fallopian tube. So, through a process of folding ridges that is kind of complicated, the ends of the paramesonephric duct end up in close association to the developing ovary. Then the fimbriae (finger-like extensions on the end of the tube) form, and during ovulation, they kind of grab the ovum to ensure that the egg doesnt miss its target.

So in short, I dont know if the space does anything other than being a remnant of embryology. In fact, it can actually be bad, because sometimes the egg in fact does miss the fallopian tube, and will settle somewhere inside the abdomen. This is especially bad if the sperm has already made it to the egg, in which case it can fertilize inside the abdomen, leading to an ectopic pregnancy (http://en.wikipedia.org/wiki/Abdominal_pregnancy).",null,4,cdek61b,1qngcy,askscience,new,31
Alex_801,"When the egg leaves the ovary, it herniates through the wall into the celom (body cavity).  The uterine (fallopian) tube is sort of like the drain for the celom in this region.  So if it were connected to the ovary, it would have no way of draining the space around it.",null,3,cdevrli,1qngcy,askscience,new,1
noott,"Energy and momentum are two sides of the same coin, relativistically.  (Look up momentum four-vectors if you want to know what I mean, although it's not important for your question.)  

A force is **defined** as the time derivative of momentum (Newton's second law).  If an external force is applied, the momentum will change.  We find empirically that momentum is conserved universally.  

Kinetic energy is **not** a conserved quantity.  Total energy is conserved, but it need not be kinetic.  

One other important difference: momentum is a vector quantity (that is, it has a direction associated with it), while (kinetic) energy is a scalar.  

Now, what happens when two objects collide depends on the type of collision.  Is it elastic (KE conserved) or inelastic (KE not conserved)?  Also, it depends on the reference frame you're viewing things from.  [This wikipedia page has some good graphics to help with your intuition.](https://en.wikipedia.org/wiki/Elastic_collision#Equations)


",null,1,cdejyfg,1qng62,askscience,new,8
neha_is_sitting_down,"It depends on what kind of objects you are dealing with. In a somewhat idealized system you have 2 types of collision: elastic and inelastic. In both types momentum is conserved, but in inelastic collisions kinetic energy is not conserved. 

The reason momentum is conserved is because of newton's laws of motion. Kinetic energy however can be lost to all sorts of things like compression and heat.",null,0,cdek3wa,1qng62,askscience,new,1
spaceman43,"The difference will be the amount of force involved in the collision.~~In either case you describe, if the objects hit each other head on they will both stop as conservation of momentum tells us. But~~ in the case with higher kinetic energy, the speed is higher and hence the collision will happen more quickly. Newton's 2nd law can be stated as force= mass x acceleration, or alternatively force=(the rate of change of momentum). In the case with higher kinetic energy, the momentum of the each object will ~~go to zero~~ change very quickly and hence a larger force. This is why you could imagine wanting to be hit by a train moving extremely slow (almost not at all) before being hit by a bullet (less massive but moving extremely fast).",null,0,cdek8qg,1qng62,askscience,new,1
KarlOskar12,"On the [skin](http://en.wikipedia.org/wiki/Skin_flora) alone there are over a thousand. In the [gut](http://en.wikipedia.org/wiki/Human_microbiome#Gut_flora) there are another 500-1000. In total there are several thousand different species of bacteria living on/in a healthy human. This number increases as more types are discovered. Recently over 1,000 [new](http://www.theatlantic.com/health/archive/2012/12/1-458-bacteria-species-new-to-science-found-in-our-belly-buttons/266360/) bacteria were discovered in the belly button alone!",null,0,cdelj4k,1qng10,askscience,new,3
medikit,"Fortunately we still have great antibiotics for the resistant diseases that effect healthy individuals: ie MRSA, Pneumococcus, and Enteric causes of UTIs.

The super resistant pathogens you've been reading about (CREs) are not as biologically fit as their less resistant cousins and generally only exist inside of nursing homes and hospital and typically only the chronically ill acquire these infections. CREs and other commonly resistant healthcare associated pathogens like Pseudomonas, Acinetobacter, Stenotrophomonas, and Burkholderia are unlikely to be a major cause of illness for the general population. Improvements in antibiotic stewardship and infection control practices are needed to control the development and spread of these organisms.

These organisms concern me most:

1. Resistant TB: Here the key is isolation and effective treatment of infected individuals. There are new drugs down the pipeline but we need to make sure we are identifying resistant infections and treating them appropriately.

2. Resistant Gonorrhea: We have run out of effective outpatient options for some patients. This is a big deal.

3. Vancomycin Resistant Enterococcus: We do not have very many effective antibiotics for enterococcus. It is currently thought that VRE developed due to feeding animals Avoparcin. We need to rethink our use of antibiotics in farm animals. http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3426332/",null,0,cdf6ge2,1qnfvq,askscience,new,3
eekabomb,"[this article from the CDC](http://www.cdc.gov/mmwr/preview/mmwrhtml/mm4829a1.htm) is a good summary of how antibiotics changed the fate patients with infections. Fig 1 and 2 are relevant to your question.


I've heard the likelihood of death from a staph infection was about 70% in the pre-antibiotic era, but the only source that I can provide for that particular percentage is [this abstract](http://jac.oxfordjournals.org/content/61/2/246.full).


you are right to be concerned about the future of our society with the increasing rate of anti-microbial resistance, especially in the face of new strains of bacteria that are carbapenem resistant, etc.


most hospitals now have anti-biotic stewardship programs where pharmacy/ID specialists will limit use of antibiotics to cases where the susceptibility is certain (after micro does their lab tests). 


there are always some new antibiotics in the pipeline, but until the risk of death from infection becomes apparent to the people at the top of the food chain they are probably going to be taking a backseat to things like chemotherapy and biologics. when people start dying though, we'll definitely have a lot of pharma companies rushing to create new antibiotics.


imo death rates will not go back to pre-antibiotic times, as we have learned so much about the spread of infectious disease that things like doctors not washing their hands between patients will not be a contributing factor to large endemics; the fact that we have knowledge about how the bacteria are killing us has already placed the pre-antibiotic death rates in the far distant past.   ",null,0,cdenti0,1qnfvq,askscience,new,2
BillyBuckets,"As we age, the vocal cords and the structures immediately surrounding them thicken. Like any other vibratory instrument, more mass in the vibrating body means a lower frequency, which means a lower pitch. Disease and a lifetime of use furthers the thickening process.

Of course our pitch isn't only the mass of the vocal cord structures, but also their length (men have longer cords on average than women) and tension (you change your pitch by tightening/loosening your cords with a collection of minute muscles around your larynx). The thyroarytenoid muscle, which shortens the cords, atrophies with age leaving the cords in a longer state, further deepening the voice.

Lastly, the resonating structures of the body tend to get larger with age. People with COPD have larger chests (so called ""barrel chests""). The maxillary sinuses up in your skull, which branch off of the nasal passages, also get larger throughout life. This causes harmonics to shift lower as you get older, but I cannot attest to how much this contributes to perceived pitch.",null,0,cdgkkk1,1qnfta,askscience,new,1
MarineLife42,"If I remember correctly someone collected the Ig nobel prize for showing that fish can get seasick.  
Not sure about dizziness. Fish have three pairs of [otoliths](http://www.marinebiodiversity.ca/otolith/english/what.htm) in their head, usually behind and below the brain. They swim in a fluid-filled cavity with hair cells on the inside and, since they are heavier than water, always sink downwards so the fish can know where 'up' is. Also they vibrate in response to sound so they wrap up the hearing in one go. Our ears are a bit different but the main difference is that we don't have otoliths, instead the fluid transports the sound waves to our hair cells directly.  
But since there are many things that can cause dizziness and the ear is just one of them - ear nerves or blood pressure are others - I'd hazard a guess and say yes, they probably can.",null,0,cdensgr,1qndfw,askscience,new,2
CalzoniTheStag,"This happens every ~11 years and it happens over the course of months. It *can* increase the chances of solar flares, but for the most part you have nothing to worry about unless you have a heliocentric satellite floating around. ",null,4,cdeiqtt,1qnc26,askscience,new,17
peytong67,"From what I remember from school, The sun flips polarity on a regular basis (~11 year cycles). The Earth also does this. *This* is what we are scared of because it will have a huge effect on a lot of the advanced electronics that we use. We can't predict when the Earth will flip polarities, or what long term effects are caused by it. We just hope it doesn't mess up our gadgets.",null,0,cdfar7x,1qnc26,askscience,new,3
neha_is_sitting_down,"The cranes you describe use electromagnets.

Electric currents create magnetic fields around them. http://hyperphysics.phy-astr.gsu.edu/hbase/magnetic/imgmag/magcur.gif

By repeatedly looping wire around a metal rod and then running a large electric current through it, you can create a magnetic field similar to a permanent magnet (the kind you have on your fridge). The magnetic field from the wire loops adds up in one direction. It also magnetizes the metal rod. This happens because the atoms inside the metal all have what is called a magnetic moment (they all have a small magnetic field of their own). Normally the magnetic moments of the atoms in the metal are random, and so they all pretty much cancel out. But when the magnetic field from the wires goes through the metal, it causes all the atoms' magnetic moments to line up, which means the magnetic fields from all the atoms is now adding up, not canceling out. This amplifies the wire's magnetic field and results in a powerful over all field. [Here is an image showing the wires, the metal atoms' moments and the field](http://imgur.com/lA8nJjk).

Now that the current is flowing through the wire, the whole thing acts like one big powerful magnet. But when you turn off the electric current, the wire stops creating a magnetic field and most of the magnetic field goes away. The metal atoms which were lined up by the field quickly become randomized again when the field is turned off, and so the system stops acting like a magnet very fast.",null,0,cdejoc1,1qnb4i,askscience,new,2
Olog,"The usual kind of year isn't exactly the time it takes Earth go around the Sun once, even discounting the mess with leap years. It's the time from one vernal equinox to the next. Because the axis of Earth changes slowly, the orbital position where vernal equinox happens also moves. This is called a tropical year and it's the basis of our calendars. Earth's axis is primarily responsible for seasons and to have seasons not move around you need to base the calendar on vernal equinox (or somehow on Earth's axis in relation to the Sun).

Earth going around the Sun exactly once with respect to distant stars is called a sidereal year. It's about 20 minutes longer than a tropical year. Accumulate that difference for 200 years and you get 2.8 days. So after 200 years Earth would be about 2.8 degrees away from it's earlier position, which is equivalent to about 7 million kilometres or 500 times Earth's diameter.",null,0,cdeucva,1qn9mo,askscience,new,6
Calkhas,"No.

Aside from perturbative forces from other planets, the Earth's orbit is slightly eccentric (i.e., its orbital path is an ellipse that is not a perfect circle), and this ellipse precesses about the sun with a period of about 110,000 years. So the whole orbital path is gradually moving around the sun, if that makes any sense (google apsidal precession for a diagram).",null,0,cdf4qzx,1qn9mo,askscience,new,3
readams,"In an approximate sense yes, but it won't be exact.  For starters, calendar years aren't exactly 1 astronomical year long; to takes 400 years for the Gregorian calendar to adjust to the mismatch here.

The Earth's orbit is also constantly being perturbed in small ways by the presence of other bodies in the solar system such as Jupiter that moves it further off.

The Earth's tilt also slowly precesses so the earth would be tilting in a slightly different direction.

As you can see lots of small things make this not be true.",null,1,cdepjc1,1qn9mo,askscience,new,3
PhoenixMercurous,"There is a general answer to your question and a different answer to your specific question.

The general answer is that adding or removing one atom from a molecule changes the molecule's thermodynamic stability and less stable compounds are more reactive and thus tend to be better at messing things up by reacting with important chemical in the body.  An example of this would be Cl- the chlorine ion, and OCl-, the hypochlorite ion.  The chlorine ion is very thermodynamically stable and generally not toxic in reasonable quantities (its in table salt).  OCl- is the active ingredient in bleach and is very reactive, which means it tends to react with just about anything in the body and cause lots of damage.

CO versus CO2 is a bit different.  CO is toxic because its very good at binding to the iron ions in hemoglobin, much better at binding than oxygen.  It prevents the body from transporting oxygen and causes oxygen starvation.  Why it is so good at binding to ions is explained by [ligand field theory](http://en.wikipedia.org/wiki/Ligand_field_theory) and the details of the energy levels in O2 and CO.  Long story short, CO has a strong overlap between its unfilled pi* orbitals and most metal's d orbitals while O2 doesn't because of its different electronic structure, so CO will displace O2.  CO2 doesn't have the same effect because it has different electronic states than CO.

There is also the answer for biological molecules, which is completely different and not my field.",null,0,cdelh5l,1qn9ho,askscience,new,8
claireauriga,"Something that's poisonous or dangerous to people is usually that way because it reacts with something in our body to interfere with normal functioning. CO is dangerous because it binds permanently to our red blood cells, stopping them from picking up oxygen and taking it around our body to where it is needed. 

(In more detail: oxygen binds weakly to haemoglobin, and easily splits off again when gets low-oxygen areas like our cells. Carbon monoxide binds so strongly that it doesn't come off again, rendering that red blood cell useless. Fewer red blood cells = less oxygen to your cells = they start dying.)

Chemical reactivity is essentially governed by electrons: the electrons whizzing around different atoms interact with each other in different ways, giving up some of theirs or taking on more, sometimes sharing them, all sorts of things. The electrons whizzing round the C and O in CO are in very different conditions to those whizzing around CO2, which is why they react with haemoglobin in red blood cells whilst CO2 does not. 

Each element has specific properties which affect how it's electrons move or react - some, like helium, are very stable just as they are. In contrast, a hydrogen atom on its own isn't. It's got one electron, and it would rather have 0 or 2,  so it's more commonly found as H2, where two H atoms can share their electrons, or the H+ ion, where it gives away its electron to something else, or sharing electrons with other atoms like when it links up with carbon.  These kind of properties explain why things react the way they do :)",null,0,cdev915,1qn9ho,askscience,new,2
whathappenedtosmbc,"I'm confused. This is a way to document life as it is and see it in the future. Though I think normal video cameras are better for that. Why do you think that this would be able to see into the past. The telescope is moving slower than the speed of light, so there is no way it is going to see light that has left before you launch the telescope.",null,2,cdeir0c,1qn8j8,askscience,new,11
SpecterGT260,"No.  The theory of relativity states that light's speed is constant regardless of your own velocity. So it isn't as if you are going 99.9999...% the speed of light that you could reach out and grab a photon.  This is due to the way that time is perceived at speed, I believe.  but regardless, the light emitted is still coming toward you at the speed of light.  It isn't slowed.  It may be distorted depending on how this is all carried out (i.e. red/blue shifts) but it still travels to you at the speed of light.  While it is true that looking at a projection out in space will show you events that happened prior to the observation,t is technique, even as described, could never see further back than the point of initial launch. ",null,0,cdemnhw,1qn8j8,askscience,new,2
TreeWithInfiniteCats,"The point where my brain breaks is when I think about closer-to-earth-alternatives. If we set up a network of mirrors around earth that keep reflecting light, you still couldn't look back further in time than when the first mirror was set up. Earlier light simply never 'touched' that mirror. But when you send them away at less than the speed of light with my near-zil understanding of physics you can, even though earlier light still wouldn't hit the mirror, since it's going slower than the speed of light. Huh.",null,2,cdehdko,1qn8j8,askscience,new,3
Daegs,"It is theoretically possible, but we don't have any way to move anything massive at nearly that quickly. So lets look at what would happen (this is ignoring red / blue shifting and whatnot that would occur by the quickly moving speeds): 

Lets say the thing gets launched in 2020.

In the year 2040, the screen (a mirror is fine as well), will be 10 light years from earth. This means that light that left the earth in 2030 will be showing on the screen, and we'll see this in the year 2050, essentially looking 20 years into the past.


This is really the same as a video recording from a satelite in space, except that in that case, we can review it whenever we want to, and we don't have to wait for 20 years after the fact to watch it.

It is important to realize why this setup is no different from a telescope in orbit that is recording. ",null,8,cdeixzm,1qn8j8,askscience,new,4
99trumpets,"Put simply, the animal must have a good probability of reproducing at an old age. (or, alternate strategy, assist with survival of grand-offspring and great-grand-offspring - humans, elephants and a few other species do this.) Only then can natural selection actually select for longevity. 

This usually means that the animal must have a fairly low risk of being killed by predators in any given year. (Or starving to death, but predation seems to be the more important variable.) There are several reasons this might be the case: it may live somewhere with few natural predators (like the Kakapo and many island birds, before ground predators were introduced by humans); or it gets out of range of predators for part of the year (the bowhead whale, by far the longest-lived whale and the only one that regularly goes so far north that it's out of the range of the killer whale. Also hibernators - hibernating animals have lower mortality, and longer lifespans, than nonhibernators). Or it's particularly well-armored (like turtles). Or too huge to kill easily (like elephants and the other large whales). Or too smart (humans). Or it's just plain good at getting away. The latter reason is assumed to be the primary reason that birds and bats (which can fly) tend to live much longer than similar-sized nonflying animals.

You get the picture. Example: your average 20g mouse lives about 1-2 years before it dies of ""old age"", even when in captivity with excellent veterinary care, while the average 20g sparrow can live 6-8 years in a similarly protected captive environment with similar veterinary care. This difference exactly correlates to mortality rates in the wild: tag a bunch of mice and usually *all* of them will be killed by predators within a year; tag a bunch of sparrows and you'll typically see over half of them come back (and breed) the next year, and some sparrows will still be showing up, and breeding, in their 4th and even 5th year. ",null,1,cdejrki,1qn8dv,askscience,new,9
null,null,null,0,cdejr8e,1qn8dv,askscience,new,2
neha_is_sitting_down,"As long as you can still reproduce, there is no evolutionary reason for you to die. You are just as valuable as a younger creature. You don't hold back your species because if you are inferior you will simply die off as normal and if you are superior then it is good to have you continue reproducing.

However, there is no evolutionary pressure to develop longevity,  aside from the benefit of having parents and grandparents care for the young. This means that once you have reproduced, almost nothing that happens to you will be regulated by evolution. This is why we develop many problems as we get older. We never had an evolutionary need to get rid of them. If you die of blindness, arthritis, cancer, etc after your kids are grown up, evolution doesn't care. You weren't going to affect the gene pool anyway.",null,0,cdek02b,1qn8dv,askscience,new,1
darkness1685,"Life history trade-offs are an important concept in determining how species differentiate themselves ecologically. Many species fall along a well-known gradient called the R-K selection gradient. Typically, a species will either live fast and hard (short life-spans, fast growth rates, and large reproductive outputs), or slow and steady (long life-spans and low reproductive output), or somewhere in between. Flies live for only a week or two, but can produce thousands of offspring, while primates can live for decades but produce only a few offspring over that time. The difference of course is that the vast majority of those baby flies will die, while most of the baby primates will survive. Overall, the numbers of offspring who then go on to produce their own offspring is (very) approximately the same. There are of course, exceptions, as there are for most concepts in biology. Oak trees live for hundreds of years yet produce thousands and thousands of acorns, and the same general thing can be said for sea turtles. Overall, though there is good support for this life history tradeoff, despite some controversy in the literature. And in contrast to what some others have said, there definitely is selection shaping this gradient. It is a way of increasing ecological differences between species, which reduces competition between them, thereby allowing them to coexist.",null,0,cdekzys,1qn8dv,askscience,new,1
lostchicken,"Skype, along with just about any telephony system down to and including the analog telephone system, has to do ""echo canceling"" so you don't hear your own voice. This is actually a REALLY hard problem, from a signal processing point of view, but it's so well studied that it's largely a solved problem at this point. Basically, the echo canceler takes your speech, delays it and the subtracts that from the output.

The trick is deciding how loud the echo is and how long the delay is. You can use knowledge about the whole transmission path if you have it, otherwise you have to measure and adapt. Bell Labs has been doing this since the 60s using analog techniques but pretty quickly moved to digital processes.

http://en.wikipedia.org/wiki/Echo_cancellation",null,1,cdeqzfy,1qn7yk,askscience,new,24
expertunderachiever,"At the heart of echo cancellation is cross-correlation.  Basically you're looking for how similar your incoming sound is to the outgoing sound, once you find the offset of cross-similarity you can attenuate the echo.  You can do this on the spatial domain [e.g. looking at PCM samples] or on the spectral domain [looking at FFT bins over time].

I'm sure there are a bazillion patents on effective techniques too ...",null,1,cdeypkl,1qn7yk,askscience,new,4
iorgfeflkd,"Charge and spin aren't directly related. You can have charged particles with and without spin, and the same for uncharged. (electrons, pions, W bosons, and the Higgs boson are four respective example).

There isn't really a satisfying answer as to why electrostatic attraction exists, it's just part of the universe that we live in.",null,3,cdenvkd,1qn4mj,askscience,new,10
hikaruzero,"&gt;I've heard that charge is a function of the spin of a particle but why would particles spinning differently attract each other?

I can't tell you why opposite charges attract (or if such a thing is even really known), but the idea that charge is a function of a particle's spin is most *definitely* not true.  If that were the case, then quarks and electrons would have the same charge because they have the same spin, yet they do not -- and neutrinos have the same spin too, but do not carry charge at all.  Charge and spin are most certainly not related in this way.",null,0,cden7js,1qn4mj,askscience,new,6
master_admin,"This is the best answer I found:

The easiest way to understand this is perhaps not with Quantum mechanics and virtual photons. The ball &amp; momentum analogy works (sort of) for repulsion, but not at all for attraction, so the end result may just be more confusion.

Think fields instead. You have most likely seen a picture of how the sun ""bends"" space-time and creates curvature in the field. You can use the same image to visualize how charges ""bend"" the electromagnetic field. For instance let's say that a negative charge would bend the field downwards - like the sun would space-time - and a positive charge would have the opposite effect and bend the field upwards. In a sort of volcano-like fashion perhaps =) ...

Now. The field does not like to be bent. The default state of any field is uncurved, and any curvature in the field will always result in a force trying to even the field out. Many things in nature work like this, always striving for it's ""normal state"" or the most energy-efficient one.

So. When two charges are close enough to each other, their fields will interact. Two of opposite charge will neutralize the field in between the charges resulting in a net ""external pressure"" pushing them together. Two like charges will instead add more stress to the field in between them resulting in a net ""internal pressure"" pushing them apart. Perhaps this is a bit harder to visualize.

But whether you can visualize it or not doesn't matter. The important thing to understand is that there is a very fundamental principle of nature at work here.

Read more: http://www.physicsforums.com ",null,0,cdmaqcb,1qn4mj,askscience,new,1
neha_is_sitting_down,"When people are talking about wasting water, what they mean is wasting clean/drinkable water and or polluting water systems.

There is a lot of water on earth, but most of it is unusable, usually because it is either vapor, frozen, or salty. Overusing fresh water means there will be less water that we can use. The water that is wasted does not disappear, but it does become useless (until it is recycled either by treatment facilities or through the natural water cycle).

Wasting water can also pollute natural water systems like rivers, lakes, and seas/oceans. This damages the environment and hurts wildlife.",null,1,cdegbti,1qn3vn,askscience,new,9
KserDnB,"Well by definition yes, anytime something turned from h20 into something else this alters the overall level and assuming that molecules of h20 are not created and turned into other molecules at the exact same time then yes, the quantity alters.",null,1,cdesm1w,1qn3vn,askscience,new,1
Thunderkettle,A fairly significant rationale behind it would be ease of transit out of the GI tract.  With higher levels of water extraction the stool would be far more solid leading to rather uncomfortable effects that I'm sure we're all familiar with on occasion.,null,1,cdewlan,1qn3vm,askscience,new,2
dakami,"We're not maximally efficient for water retention.  Urine is about as inefficient as it gets.

There are costs to retaining maximum amounts of water, that you simply don't evolve to pay if your environment is assumed to have some degree of water supply.  Also remember pretty much everything we eat is *also* mostly water.",null,0,cdfdf8e,1qn3vm,askscience,new,1
optimist_electron,"If the ice were confined and you kept the temperature low enough for the pressure of the train not to cause a phase change AND the train exerted less pressure than the walls of whatever you were using to contain the water could resist, then yes.

I did some quick googling and found that you can keep ice frozen at something lower than - 55 degrees Celsius with up to 6000 atmospheres (88,000 psi).

However, the water being sealed (or in a pressurized environment excepting equal or more force than the train would cause some serious logistical problems for doing anything other than strictly lifting the train. ",null,0,cder3g0,1qn1vs,askscience,new,2
neha_is_sitting_down,"Any time you are in orbit you will be feeling weightless, at any height. 

This is because orbiting is actually free falling. The only difference is that you are not only falling down, you are also moving forward fast enough that you don't fall into the earth you miss it. http://my.execpc.com/~culp/space/rid_mtn.gif

In this [image](http://imgur.com/gildADs) you can see a cannon shooting a ball off of a mountain. In the first few shots, the ball is not moving forward fast enough and it falls into the earth. But in the later shots, it is able to miss the earth and achieves orbit. It is still free falling, just it is constantly missing the earth.

If you want to know more about gravity, just ask.",null,2,cdeg4tv,1qn0e7,askscience,new,6
TangentialThreat,"What you're really asking is ""if I am stationary relative to some point on the Earth's equator, how high do I have to be to reach stable orbit?""

The answer to that question is 35,786 kilometers, or pretty damn far. You would also be moving west at 3.07 kilometers per second.

Note that there is a difference between [geosynchronous](http://en.wikipedia.org/wiki/Geosynchronous_orbit) and [geostationary](http://en.wikipedia.org/wiki/Geostationary_orbit). A geostationary orbit hovers over a point; it appears stationary from the ground.

An object in geosynchronous orbit merely completes one orbit every 24 hours. Orbits that are elliptical or tilted can still have this property; a satellite will often trace a [figure eight](http://upload.wikimedia.org/wikipedia/commons/c/c2/Qzss-45-0.09.jpg) called an analemma relative to the ground.",null,0,cdeh1pn,1qn0e7,askscience,new,3
stuthulhu,"Geosynchronous orbit is not required. Simply that weight is the only force acting upon the body, as in freefall. For instance, if you fell to the surface of the moon, you would experience weightlessness (well, until you hit the ground) with no orbit at all. ",null,1,cdeg86d,1qn0e7,askscience,new,3
CalzoniTheStag,"Geosynchronous orbits are orbits that have the same rotational period as an earth sidereal day. It doesn't have much to do with gravitational pull in the context of what you're describing. 

Satellites in orbit use gravity to stay in orbit. They are in a constant state of free-fall towards the Earth. But their angular velocity keeps them from actually changing their distance from Earth (generally speaking, in reality this isn't entirely true as there are many other factors at play). ",null,0,cdegdp8,1qn0e7,askscience,new,2
chrisbaird,"*All* orbits experience ""weightlessness"" (a better term is ""free-fall""). In fact, all falling motions also experience ""weightlessness"" (if neglecting air resistance). What makes geosynchronous orbit special is that the period of the orbit matched the period of the rotation of the earth, so that the satellite hovers continually over the same general area on the earth. Other than that, it is really no different than other orbits. 

You are misunderstanding orbits. In *all* orbits, centrifugal force balances gravity as seen in the rotating reference frame. That is why an object orbits and does not crash into the earth. As you go away from the earth, gravity gets weaker, so you don't have to go as fast in order to create the right amount of centrifugal force to cancel gravity. Closer to earth than the geosynchronous distance, you have stronger gravity and have to go faster than earth's rotation in order to have enough centrifugal force to counter gravity. Farther from the earth than the geosynchronous distance, you have weaker gravity and have to go slower than earth's rotation in order to have enough centrifugal force to counter gravity. If the earth was rotating half as slow, but had the same mass, gravity, and everything else, then geosynchornous orbit would be at a totally different altitude.

You can make a rough calculation of geosynchornous orbit fairly easily: just set Newton's general equation for the gravitational force (the one with a 1/r^2 factor) equal the equation for centrifugal force and set the angular velocity in the centrifugal force equation equal to the angular velocity of the earth.",null,0,cdeidbw,1qn0e7,askscience,new,1
iorgfeflkd,"It's not the height that causes weightlessness but the speed. If you're falling, you'll feel weightless, and if you're moving fast enough you'll miss the Earth every time you fall. However, there is only one height at which orbits are geosynchronous. I think you mean geocentric.",null,1,cdej1zw,1qn0e7,askscience,new,1
drzowie,"Lossless compression makes use of known patterns in the data to reduce the number of symbols required to describe the data.  All lossless compression schemes either *do nothing* or *expand* most of their possible inputs. It's easy to see why: with *n* symbols, each of which can take any of *b* values, you can represent any of *b^n* possible patterns.  If you shrink the input to some number *m*, with *m*&lt;*n*, then you can only represent *b^m* possible patterns, leaving *b^n - b^m* patterns unrepresented in the smaller space.  If the scheme is lossless, then it has to represent those patterns somehow, and the only way to do it is to use additional symbols (i.e. to expand the input).

The trick in designing a compression scheme is to notice that some patterns are more likely to occur than others.  The likeliest ones get the shortest representation and the unlikely ones can get long representations, because they don't turn up very often.  

That line of thought leads to *Huffman coding*, which is a scheme that  dynamically identifies the most common symbol sequences in a stream, and represents them with short compressed representations, while uncommon ones get longer representations.  The term ""Huffman coding"" is colloquially used to describe anything that carries out that kind of optimization -- for example, many coding environments abbreviate common commands.

There's a whole body of mathematical theory (information and coding theory) that has developed around questions like yours, and it centers on the amount of ""entropy"" in a block of data.  This information-theoretic entropy is closely related to thermodynamic entropy -- it formalizes the concept of ignorance:  a system about which you have (or can glean) a lot of knowledge has low or no entropy, while a system about which you have little knowledge has high entropy. 

In the very unlikely event that you know *exactly* what you want to represent in advance, then you can perform lossless compression at staggeringly high ratios.  For example, if you know that you're about to receive an MP4 video stream of either the Star Wars initial trilogy or the first season of Star Trek, you can compress all 10^13 or so bits of the stream into a single bit (1 for Star Wars, 0 for Star Trek).  But if even one bit of either video stream is different than you expect, you will need to add more bits to represent that change.

In less absurd cases you can compress to the entropy limit of the stream.  English text coded as ASCII has about 1 bit of entropy per character, so you can compress about 8:1 in most cases.  ASCII anime have considerably less entropy and can be compressed losslessly much more than that.  Uniform-probability random bitstreams have 8 bits of entropy per byte and you can't actually compress them losslessly at all (on average).

**Edit:**  the information-theoretic entropy *is* the message being carried by the data -- it's the good part.  This is backwards for people who are used to thermodynamics, but it makes sense. The part of the coded symbol stream that is *predictable* carries no new information, since (after all) you could predict it.  The ""message"" is the new knowledge you glean by examining the symbol stream, that you couldn't predict in advance -- i.e. the entropy.  The point of lossless compression, then, is to pile as much entropy onto as few symbols as possible, so that it's very hard to predict the next symbol of a message stream while receiving the message.",null,2,cdeerhp,1qmwdx,askscience,new,22
xifeng,"This is entirely dependent on the data being compressed. Every file will have its own particular lower bound, its information entropy, and you can't compress it any further without losing some information.",null,3,cdee2kg,1qmwdx,askscience,new,7
bobroberts7441,"Not dispositive, but [here](http://www.dangermouse.net/esoteric/lenpeg.html) is a method that achieves a 6,291,456:1 compression ratio.

If you don't like that you might consider the [pifs](https://github.com/philipl/pifs). tl;dr pifs simply stores the start point and length of the file sequence within the irrational Pi, thus reducing any file to simple start pointer and size integers.",null,1,cder3eg,1qmwdx,askscience,new,4
dakami,"It depends.

There is such a thing as Kolmogorov complexity.  This is generally explained as the size of the smallest program that will emit a sequence of bits.  In general, we see compression programs reaching a limit as to how far they'll get, reversing from a sequence to programs that will generate it.

The thing is, the more domain aware you are as a compressor, the better programs you can generate.  ZIP will not compress an image nearly as well as PNG, for example.

At the extreme case, the size of a program that recognizes pi and simply generates an equation for it, will actually be very close to the Kolmogorov complexity of pi itself, and will be nigh-arbitrarily superior at compressing the infinite sequence than any other approach (pi itself is incompressible any other way).
",null,0,cdfdmln,1qmwdx,askscience,new,1
Platypuskeeper,"You'd normally use the word 'crystal' for a bulk solid, so something that's greater than a 10 nm radius or so (that's just my own feel of it though). For a just a dozen molecules or so it'd be a 'cluster'. 

It's just words though, there's no specific point where a cluster of coordinated molecules becomes a crystal.",null,2,cdeepud,1qmsxy,askscience,new,4
neha_is_sitting_down,"Any two or more water molecules that come together to form solid water (ice) will form a structure with the same geometry as a larger ice crystal.

When you can actually start calling that structure a crystal is probably more of an opinion.",null,0,cdeflu0,1qmsxy,askscience,new,2
Atomic_Armadillo,"Kind of an interesting question. The first question that should be answered first is: how many h2o molecules are needed to determine it's phase? 
I remember my chemistry teacher answering this question, but that was like 6years ago :/",null,0,cdg0yrx,1qmsxy,askscience,new,1
Javi2639,"Imagine a two dimensional person drawn on a piece of paper. Now draw a square around him. To him, there is no conceivable way to escape from the box. There is only up, down, left, and right, all of which are blocked. However, to a three dimensional observer of this (you), there is a simple solution: bring the person up out of the plane of the paper and place him back onto it outside of the box. What you have done is introduced a new dimension to him. This dimension would be completely invisible to him, and when he re-enters the second dimensional world he is accustomed to, he will be magically outside of the box. Now lets extrapolate this idea into three dimensions. Imagine you are in a cube. There is only up, down, left, right, forward, and back. There is no way to exit the cube that you can see. However, to a fourth dimensional observer, there is a very simple way to exit the cube: introduce a new dimension to you. This would be completely invisible to you, and when you re-enter the third dimension, you will be magically out of the cube. Also, a two dimensional being would cast a one dimensional shadow. We are three dimensional, and we cast a two dimensional shadow. Therefore, we can assume that a fourth dimensional being would cast a three dimensional shadow.",null,2,cdeemim,1qmsrj,askscience,new,36
drzowie,"Yes, there is.  [Here is an example of how to visualize construction of a 4-cube](http://www.youtube.com/watch?v=ykQmNrSKqGQ); [here's one showing how weird rotations in 4-space are.](http://www.youtube.com/watch?v=6tpsPYBrHy0).  With modern computers, it's no trick to generate visualizations like those (which use brightness and saturation as additional ""dimensions"" to augment the perspective).",null,0,cdef9nz,1qmsrj,askscience,new,6
JohnGaltish,"I may not be a Physicist, but here is how it was always explained to me:

Imagine you wanted to draw a 3D cube so a 2D person could understand it. There are 2 ways:
1. You unfold the cube into [this shape](http://www.math.union.edu/~dpvc/talks/2000-11-22.funchal/GIF/Cube-Unfolded.gif) or
2. You draw what it would look like [flattened](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Necker_cube.svg/220px-Necker_cube.svg.png)

Now you can do the same thing bringing a 4D object into 3D. So,
1. If you unfold a 4D cube in 3D it looks like [this](http://www.math.union.edu/~dpvc/math/4D/GIF/HCube-Unfolded.gif)
2. And if you ""flattened"" it, it would look like [this](http://im-possible.info/images/articles/hypercube/hypercube.gif)

Hope that helps. And btw, a cube in 4D is called a ""Hypercube"", so you can google that",null,2,cdeflol,1qmsrj,askscience,new,4
sparklesia,"You can visualize the shadow of a 4D object cast into 3D (for example, with a [tesseract](http://upload.wikimedia.org/wikipedia/commons/5/55/Tesseract.gif), a 4D analog of the cube).

Also, the best explanation of the 4th dimension I've heard is in [this video](http://www.youtube.com/watch?v=UnURElCzGc0) by Carl Sagan. He uses the relationship between 2D and 3D as an analog to our perception of the fourth dimension.",null,2,cdee91e,1qmsrj,askscience,new,3
I_Cant_Logoff,"It depends on whether you're visualising a spatial or temporal dimension. If temporal, then it's just time. If spatial, it would be a lot more difficult.

Remember, a dimension is simply just a quantity that things possess (in this case it is to identify the location of an object). There's really no way to visualise a fourth spatial dimension because our brains are hardwired to only imagine three spatial dimensions. Other than the three dimensions, there isn't really a fourth direction you can point to.

Since it is just a quantity, when dealing with four spatial dimensions, it is useful to think of it as three spatial dimensions and one unique quantity, like colour. Objects with the same ""colour"" occupy the same coordinate in that spatial dimension.",null,1,cdeemwu,1qmsrj,askscience,new,2
CarbonWeAre,"The technical answers on this thread are great, but for a more relaxed approach try reading [Flatland] (http://www.amazon.com/Flatland-Romance-Dimensions-Thrift-Editions/dp/048627263X/ref=sr_1_1?ie=UTF8&amp;qid=1384662735&amp;sr=8-1&amp;keywords=flatland) by Edwin A. Abbott.",null,0,cdg5mce,1qmsrj,askscience,new,1
therationalpi,"Depending on what you mean by harmonics the answer could either be ""none"" or ""location.""

The acoustic levitation I'm most familiar with uses a standing wave to levitate an object at the pressure nodes. If your question is how multiples of the input frequency make a difference for acoustic levitation, then the answer should be ""none"" because these standing waves are generated at a single frequency and thus lack harmonics.

If, however, you think of harmonics as being the modes modes of the standing wave, then the wave shapes of those harmonics are going to determine the locations of the pressure nodes, and thus the number and location of objects that can be levitated.

For example, let's assume you are driving a clear plastic tube with a hard end at resonance to create a standing wave. If you drive the tube at its first mode (the 0th harmonic), then there will be one pressure-node in the middle. If you drive the tube at its second mode (1st harmonic), there will now be two nodes, one at 1/4 of the pipe, and the other at 3/4 of the pipe. The third mode has three nodes, and so on.

Just a point of clarification, before someone jumps in to try to correct me. Depending on who you ask ""harmonics"" can either refer directly to the modes, or they can refer to the frequencies above the fundamental. I'm choosing to use the latter definition here. Second, we are talking about pressure nodes, as opposed to particle velocity nodes. For an acoustic resonator, the pressure antinodes correspond to particle velocity nodes and vice versa. Acoustic levitation is achieved at locations of stable pressure.",null,0,cdglgtu,1qmsn0,askscience,new,2
James-Wing,"One (little c) calorie is by definition, the energy it takes to heat one gram of water one degree Celsius. One (big C) Calorie, used in food, is actually a kilocalorie, the energy it would take to raise 1000g of water 1 degree. 
Therefore 1 Calorie is 4184 joules = 3.9 BTUs",null,0,cdee7jk,1qmrkj,askscience,new,6
do_od,"I compiled a few comparisions for you.

1 cal = 4184 J = 1.163×10^−6 kWh = 0.003964 BTU

Heat of combustion:

* Gasoline 10.8 kcal/g = 45 kJ/g
* Coal 6.45 kcal/g = 27 kJ/g

According to relativity (the E_0 = m_0 * c^2 part), 1 cal is equal to the rest energy of 4.7*10^-17 g (47 femtograms).

You can use a tool like [Wolfram Alpha](http://www.wolframalpha.com/input/?i=heat+of+combustion+of+gasoline) to find answers like these.",null,1,cdefcke,1qmrkj,askscience,new,5
neha_is_sitting_down,"A calorie is a unit of energy just like a joule. It is defined in respect to how much energy is needed to heat a specific amount of water by 1 degree celsius/kelvin.

We generally use it to talk about food. We use a calorimeter to measure the amount of energy in a food by burning it and measuring how much the burning reaction heats up a known amount of water.",null,0,cdefjs0,1qmrkj,askscience,new,2
PipettesByMouth,"The energy in 1 food Calorie (actually a kilocalorie) is equal to the energy of 1 gram of TNT exploding.  Every day, the average person goes through the same amount of energy released by blowing up 2 kg of TNT.  Obviously, the time over which the energy is released makes a difference.

I tried to find a video of that size of explosion, but all I succeeded in doing was watching rednecks shoot / blow up used appliances.",null,0,cdf94mr,1qmrkj,askscience,new,2
neha_is_sitting_down,"They don't output anything unless they have power. If you could build a device that simply sends out the EM waves that power the tag, then you could probably pick up the response with your radio, but without powering the tag, you will get nothing.",null,0,cdehscr,1qmrjq,askscience,new,5
spartandudehsld,"It depends on what you mean by ""perfect"". A mathematically perfect sphere and surface would have a singularity point contact.

Any real world interaction using the term ""sat"" implies a force. Generally gravity. In that case the stress strain properties of both objects would determine the size of the contact area. Thus no longer being a perfect sphere or flat.",null,5,cdedpgx,1qmq1y,askscience,new,26
Platypuskeeper,"Besides that perfect spheres don't actually exist since things are made of atoms, there are no totally rigid materials either. It will always flatten at the contact point. How much depends on how rigid and heavy the sphere is.",null,0,cdef48d,1qmq1y,askscience,new,8
russrobot,"The stress of contact forces is called Hertzian stress.   If a sphere touches a flat plate there is small local deformation that counteracts the applied force.  If the only force applied is the sphere's own weight then the equation for the radius of the small contact patch (assuming same material) is:

r=CubeRoot(2 Pi Mu R^4 (1 - v^2)/E)

Mu = density

R = radius of sphere

v = poisson ratio of material

E = young's module of material

For example.  4"" diameter perfect steel sphere resting on a perfect steel plate gives a contact patch radius of 0.0096 inches.

Hope this answers your question.


Source: Engineer",null,0,cdeo56h,1qmq1y,askscience,new,7
neha_is_sitting_down,"An ideal sphere on an ideal surface would have in infinitesimal point of contact where the surface is tangent to the sphere.

however, such a setup can not exist in the real world for the reasons described by the other comments.",null,0,cdefo3k,1qmq1y,askscience,new,2
BeaumontTaz,"If you're dealing with an ideal sphere in contact with an ideal plane, the contact is a point. For instance, the plane of the x-y axis and the unit sphere defined by x^2 + y^2 + (z-1)^2 =1 will only contact at (0,0,0).",null,1,cdennyu,1qmq1y,askscience,new,2
yourfiance,"Like was said before, if you'd somehow discovered a perfectly rigid material, the ""touching"" point of this sphere would ideally be a single atom upon another single atom. So two points that are not quite infinitely close to one another. Also, since we're talking about a perfectly flat surface here, there would be no friction, and as such the ball would be set into motion by even the smallest change in its surroundings... In which case two new atoms would ""touch"", and then again, and again.",null,1,cdet0ab,1qmq1y,askscience,new,0
bleedingLance,"So, you've probably seen the usual definition of the zeta function. Namely zeta(s) = 1+2^-s +3^-s +...

Now, this is a function of a real number, s. Moreover, it is only defined, thus far, for s&gt;1, since the above series diverges otherwise. Clearly, for any such s, zeta(s) would be completely real. But, what if I wanted to extend the domain of my function. That is, I now want to let s be any complex number. Well, it turns out that the series definition works so long as Re{s}&gt;1. So I've already encompassed a great deal of the complex plane (which, by the way, is just the set of complex numbers. Imagine real part on the x-axis and imaginary part on the y-axis).

Let's keep going. Maybe you've seen this nice property of the Zeta function: (1-2^(1-s) )zeta(s)=1-2^-s +3^-s -4^-s +...
Which we can rewrite as zeta(s)= (1-2^-s +3^-s -...)/(1-2^(1-s) )
What's nice is that, because of the oscillating sign in this new series, it actually converges for more values than before. We now have a formula that works for Re{s}&gt;0 (though note that there is still a problem at s=1. This is a property of the Zeta function, not a shortcoming of our extension). So at this point, we've conquered the right half of the plane. To get the left half, we need the [reflection formula](https://en.wikipedia.org/wiki/Riemann_zeta_function#The_functional_equation), which gives zeta(s) in terms of zeta(1-s). I can't really explain why this formula is true, since the derivation requires a good bit of complex analysis. But, if you accept this formula, then you see we can now define zeta(s) everywhere (except at s=1).

Now, you may ask why I would want to do this. As you said, the Zeta function is very important in number theory, as many things in number theory can be expressed in terms of the Zeta function. Well, complex analysis is very good at extracting information from certain complex functions. Thus, by making the Zeta function into of these functions, we have all of the powerful machinery of complex analysis at our disposal. For example, zeroes and poles are important in complex analysis. Often times knowing them means knowing everything you need to know, hence why the Riemann Hypothesis is so important. 

So, to answer your question, the original series definition that makes sense for number theory is completely real. It gains an imaginary component when we extend it to the complex plane hoping to gain powerful tools from complex analysis to help us.",null,2,cdem5qd,1qmp05,askscience,new,5
functor7,"Two main reasons. 

1) In Complex Analysis, integration is very ""easy"" in that all you really need to know is information about the poles (infinities) of a function. And in general things behave much nicer over the complex plane than they do over just the reals. For instance, an analytic function is uniquely determined by how it behaves on any set with a limit point. So extending the Zeta Function to the complex plane gives us really nice tools to easily compute things.

2) The Riemann-Zeta Function has intimate ties with a cousin of the Fourier Transform called the ""[Mellin Transform](http://en.wikipedia.org/wiki/Mellin_transform)"". It is true that the information about primes is contained in the Riemann-Zeta Function, but to access it we need to be able to do an ""Inverse Mellin Transform"" over the [logarithmic derivative of the Zeta Function](http://en.wikipedia.org/wiki/Von_Mangoldt_function#Dirichlet_series). And just like the Inverse Fourier Transform, this requires us to integrate in the complex plane rather than on the real line. Additionally, the logarithmic derivative has the zeta function as a denominator and so the zeros of the zeta function turn into poles and so the value of the inverse Mellin transform is determined by what happens with the zeros of the zeta functions, which (in turn) means that the distribution of primes is seen in how the zeros of the zeta function behave.",null,0,cdh534g,1qmp05,askscience,new,1
lasserith,Your clothes are made out of mostly polymers. At room temperature these polymers are quite stable and don't do a whole lot of stuff by themselves. As you wear your clothes you start to stretch out the polymers kind of like stretching out a rubber band. Because all the polymers are all connected to each other they can't just fully shrink back to their normal shape. When you heat your clothes you are adding enough thermal energy to the polymers to allow them to overcome any barriers and adapt their most stable shape. In effect they shrink. If you would like more detail or clarification just ask.,null,0,cdet0ez,1qmnxt,askscience,new,3
GreenTeaForDays,"Absolutely. We call it a ""diurnal cycle"". The obvious variables that follow the time of day are temperature, down-welling radiation (energy from the sun), latent heat (a.k.a evaporation and transpiration)-- essentially any variable that is directly dependent on the heat from the sun. 

It is important to understand that weather is driven by the uneven heating of the surface of the earth. Since the land surface is heterogeneous, and the sun's location in the sky (solar zenith angle) is constantly changing, the laws of thermodynamics are constantly driving air to move around and adjust to changing temperatures and pressure. 

Sometimes there are weather patterns that develop from a persistant temperature/pressure differential between two locations. For instance, sea breezes develop a diurnal cycle because the land heats up a lot during the day, and cools off at night. The oceans stays the same temperature all day. Hot air means low pressure, so during the day the air will move towards the land, and at night it will go out back towards the ocean. 

Another important factor is humidity. As air warms up it can hold more vapor water. In the tropics, for instance, the air is very humid and gets very warm in the afternoon. As warm air rises, the air cools and can no longer hold as much water. Clouds start to form, and then rain. This kind of rain is called [convective rain](http://www.weatherquestions.com/Cloud_formation_convective.gif), and in the tropics it can happen every single day at nearly the same time for months at a time ",null,0,cdfebqi,1qmng1,askscience,new,3
wazoheat,"Yes, though it is not a simple relationship. It will depend on your latitude, whether you are over land or the ocean, the terrain, the season, and the type of area you live in (in other words, desert, forest, farmland, city, etc.).",null,1,cdejxk5,1qmng1,askscience,new,3
cant_help_myself,"Of course. For example, most strong thunderstorms don't start forming until the afternoon, although this varies depending on where you live. [Here's a map](http://usatoday30.usatoday.com/weather/tg/wtorwhen/wtorwhen.htm) showing peak times of tornado activity across the US.",null,1,cdedf9m,1qmng1,askscience,new,2
king_of_the_universe,"One example: When the sun ""comes up"" in the morning, it starts to heat the atmosphere and the ground. This wandering frontier of temperature change necessarily brings a more dynamic air pressure situation (hence wind).",null,0,cdew7ya,1qmng1,askscience,new,1
homininet,"Howdy, functional morphologist here (who studies human and primate gait). The study of arm swing is actually pretty interesting, the first studies were done all the way back in the late 30's, and people are still studying it today (in fact a really recent literature review was just published this year!: http://www.sciencedirect.com/science/article/pii/S0966636213001185). 

In generally, the purpose of swinging your arms is to balance out motion thats happening in your legs. Your legs have to swing out of phase in order for you to walk. When this happens, it causes a net angular momentum that is transmitted to your pelvis, and then to your torso, and then, ultimately to your head. Obviously, we'd rather not have our bodies twisting back and forth with every step, so what happens is that when one leg swings forward (lets say our right leg), then the left arm swings forward, thus canceling out most of the angular momentum. The cool thing about swinging our arms though is that it seems to be passive (ie it doesnt take much muscle activation to make it happen), which means that it takes very little energy to do it. In fact, it actually saves us energy, and many studies have found that it actually reduces the metabolic cost of locomotion (how much energy you burn walking) by about 7-10%. The reason why is that if you didn't swing your arms, you have to use more muscle activity to prevent you're big heavy upper body from twisting around at every step.

This is also generally true of running as well.

Here are some more articles if you're interested:
http://jeb.biologists.org/content/213/23/3945.full.pdf
http://www.sciencedirect.com/science/article/pii/S096663620700135X
",null,5,cded6mz,1qmmk8,askscience,new,42
robged,"It is a really challenging task to keep making the transistors (switches), interconnects (wires), and oxides (insulators) which make up a computer chip smaller every 18 months. The IC industry spends billions of research dollars each year to do so. 

To make the components in a computer chip, engineers use a process known as photolithography. Photolithography can be thought of as a process similar to taking a black and white photograph, where light shines on a chemical, and a reaction takes place. The chemicals are then developed, but instead of the chemicals pigment changing, as in photography, here the exposed are washed away with a solvent.

The key to making the really tiny features on a computer chip, is that we can make a big glass plate (4"" to a side) with a opaque print on it. We can then shine light through it, focus that light with a lens onto our computer chip surface, and print a much smaller version of what was on the glass plate.

The size of the features we can create are then only limited by how much we can focus the light, but here is where it gets tricky. Currently the industry is using 192 nm light, which can only be focused down to a spot size of roughly the same diameter. Yet the IC industry is making 22 nm components! How the hell is this possible you ask? Immersion lithography and triple patterning is the current standard. The refractive index of water is higher than air, so in water light can be focused to a smaller spot than in air, so they flow water over the wafers during exposure. Then, for triple patterning, each feature requires three exposures, during which the wafer is moved a bit each time. All-in-all, it takes over 1000 steps and 2-3 months to make a computer chip using triple pattering and immersion lithography, and we can't make features smaller than ~10 nm with it (no matter how good it gets).

So what's next? We would like to cut to the chase, reduce steps and cost by employing a smaller wavelength of light, so the industry is trying to use 13 nm light. Doing so is not trivial, however, as glass becomes opaque to ultraviolet light, so mirrors must be used instead. Add to that the fact that we don't know how to make a good source of 13 nm light and the issues becomes increasingly tricky.

tl;dr? Engineers have to be incredibly creative / inventive to make CPU's smaller every 18 months.",null,1,cdej7pz,1qmmgq,askscience,new,7
afcagroo,"Materials science, electrical engineering, mechanical engineering, chemistry, optics, etc. all have a role to play to overcome the obstacles that arise from trying to cram more and more transistors into smaller and smaller areas.   
  
If it was just a matter of making the transistors smaller without changing anything else, it would proceed a lot faster. But every time you make the transistors smaller, it creates a bunch of new problems that were never solved before, or weren't solved to the degree that is now necessary.   
  
Just doing the photolithography and planarization to image and print things smaller is a whole huge set of problems of their own.   
  
There's both processes of continual, incremental improvement that happen, and radical changes.  When something gets to the point that just doing the same kind of thing better, with more precision won't meet the needs, then something more basic has to be changed.  And when that happens, there are usually a whole set of new problems to be solved.  
  
For one example, the shift from Al to Cu for interconnect metallization helped solve some problems, but it created a whole bunch of new ones. Cu is a contaminant if it makes its way into the silicon, and ruins the transistor properties.  So you have to use barrier metals to prevent its diffusion. So you have to figure out how to deposit those barrier metals so that they stick to the right things without having any cracks or pinholes, but with very low electrical resistance. And of course you need to be able to deposit and etch those barrier metals. Etc. Etc. ",null,0,cdhd7je,1qmmgq,askscience,new,2
greatgreenarklsiezur,"If by smaller you mean that the transistors are smaller and therefore chips are faster, there are many reasons that this trend continues.

This is called Moore's law, the law that the number of transistors per a chip of the same size doubles every two years. This trend has been going strong since 1971 and is set to continue for the foreseeable future.


As for why and how, you will have to read the page and research the processes they use to make these chips; they are myriad and complex. This is too large of a subject to explain here, and I fear it will require reading on your part. Here's a link to the Wiki page on [Moore's law](http://en.wikipedia.org/wiki/Moore's_law)",null,3,cdeer8k,1qmmgq,askscience,new,1
MissBelly,"You are correct. As you mentioned, the effects of PTH on the kidney involve reabsorption of calcium and wasting of phosphate, causing a loss in plasma phosphate concentration. You are correct that when calcium and phosphate levels rise together, there is increased creation of calcium phosphate, which (as I'm sure you know) only free calcium is elementally active. When the calcium to phosphate ratio increases (by saving calcium and wasted phos), more calcium is free in the circulation. There is a calculation called the calcium-phosphate double product, which is simply the product of serum calcium and phosphate levels. When above a certain value calcium phosphate is more likely to precipitate. Over the course of evolutionary history, the PTH receptors in the kidney which saved Ca and wasted Phos had an advantage, and that advantage was increasing the serum free Calcium the most efficiently without the precipitation of CaPhosphate.",null,0,cdem9rc,1qmlui,askscience,new,2
SpecterGT260,"The parathyroid also has a trigger related to phosphate levels.  You would die of cardiac arrest well before you precipitated calcium phosphate in your serum.  That said, the major reason the kidney does this is because parathyroid hormone's job is primarily to increase serum calcium which isn't very efficient if phosphate hangs around as phosphate is used along with calcium to make bone.  So your parathyroid hormone stimulates osteoclasts which turn calcium phosphate into calcium ion and phosphate ion.  While this is happening vitamin D is also working in the gut and increases absorption of both phosphate and calcium.  Phosphate levels impact osteoblast activity, so if the excess phosphate wasn't trashed you would just have a cycle of osteoclasts releasing bone and osteoblasts turning it right back into bone, thus sucking the needed calcium back out of the serum.  This isn't terribly helpful when you need to increase yoru serum calcium which is usually why you have PTH release in the first place.

I disagree with the other poster here in that ""only free calcium is active"".  I don't disagree directly, but more with her implication that you will not increase free calcium levels unless you dump phosphate.  The stoichiometry of the ions dictates what proportion of all components are associated or disassociated in solution. This proportion stays the same, so if you release tons of Ca and PO4, you get more of both until you saturate.  Before saturation you will have transient precipitations but they are minor and the system highly favors disassociation (which is part of the reason why you typically need osteoblasts to form your bone and bone doesn't just spontaneously form on top of itself).  

So in a nutshell, the bone resorption will increase calcium on its own.  But when everything else is working that calcium will get thrown right back into bone, so the body uses the kidneys to limit serum phosphate to keep this from happening. ",null,0,cden6f1,1qmlui,askscience,new,2
TheBB,"It could be any of those. I am a sample from the population of Norwegians, but I am also a sample from the population of male humans. And I'm also a sample from the population of male Norwegians, or just the population of all humans. Etc.

The [wikipedia page](http://en.wikipedia.org/wiki/Sampling_\(statistics\)#Population_definition) might be helpful.",null,0,cdecefw,1qmk0h,askscience,new,5
1337bruin,The population is the probability distribution that you're drawing a sample from and want to make inferences about. It's totally application dependent.,null,0,cdeecmm,1qmk0h,askscience,new,2
darkness1685,"A population is what you are interested in, and depends on the question being asked, and what type of inferences you are making. If your question is about something fundamental to drosophila flies, then yes, all drosophilas in the world would be your population. If you are asking a question about a specific species of drosophila, or drosophila in southern India, etc., you're population would be adjusted accordingly. In these cases, you rarely are able to collect data from the entire population, so you instead take a representative sample. However, maybe your question is indeed just about the flies that you have in your lab, then you could call this your population. However, you would not be able to use your conclusions to make inference about any other fly outside of your lab. So in short, your question can't really be answered without further information on the question that you are interested in. ",null,0,cdee1il,1qmk0h,askscience,new,1
ThatSexyLiberal,"When the universe originally formed, the abundance of hydrogen was much greater than that of today, resulting in massive stars that exhausted their fuel supplies over millions of years; collapsing violently in hyper novae explosions and the first black holes. These black holes eventually accumulated and merged, forming the first supper massive blacks of which galaxies would later be built around. ",null,1,cdedm61,1qmi0i,askscience,new,9
Calkhas,"It's possible, but unlikely (at least in my view). 

There is a limit to how quickly a black hole (or any object) can acquire mass. Disregarding problems with dissipating angular momentum (and this is no easy problem to solve) infalling matter will heat up quickly by friction as it falls down the gravitational well and will begin to glow xray hot. The radiation pressure will act to expel the matter behind it, in the least efficient case, by Thomson scattering.

From this, the Eddington limit, we can work backwards from estimates of the current mass of SMBHs (and their age) to say how big the progenitors must have been. We arrive at masses in excess of 400 solar masses. [see e.g., http://arxiv.org/pdf/1003.4404.pdf] This is bloody enormous, but in the metal-poor environment of the early universe, perhaps possible (though not seen in far redshift observations?). Although physical models of these systems are extremely poorly understood, the temperatures at the centre of these ""stars"" will be relativistic; i.e., sufficient to produce electron-positron pairs. Pair production reduces outward pressure: the star will not last long and we will have an intermediate size black hole soon. This method of collapse probably captures about half the progenitor mass in the black hole, making it very efficient compared to smaller supernovae. But the black hole still has to suck in a lot more mass to get to its present size. In the shallow potential wells of halo galaxies this is not an easy proposition. They are also feeing a lot of energy back into the local medium, slowing their rate of growth. (Quasars, for instance, are exceptionally bright. They also expel a lot of matter beyond their host galaxies.)

Could black hole mergers help? Perhaps, although estimating the collision frequency at early times is not easy. Today, galaxy collisions are quite rare in our local universe (but much more common than star collisions, for instance, which probably only occur ~ once in a spiral galaxy.) Even if you get a couple of black holes within a few light years of one another, there are big problems with shedding angular momentum again (maybe gravitational waves, which might be detectable!).

The real problem I have with this model is the ultimate fate of these SMBHs. They occur at the centre of every galaxy we observe, and only the centre. Does exactly one happen to form in every galaxy? Do loads form and they all somehow merge, thousands of light years away from their origins, without any being expelled to higher orbits by the galactic tugs of war? Why and how are they so stable in the centre? In spiral galaxies, why are SMBH masses so tightly correlated with the total galaxy mass?

These questions to me suggest that SMBHs form as part of early galaxy formation objects, as very massive (&gt;10^4 solar masses) objects initially, and, though they may shine very brightly during their collapse phase, they would never go through a stable ""star-like"" phase at all.",null,1,cdel0ht,1qmi0i,askscience,new,5
riversquid,"In short, yes. Stars form as a large collection of hydrogen. As it gets bigger, the gravity created forces the hydrogen atoms in the center to collide very very fast, creating helium. This is the process of nuclear fusion and where stars get their power. This process continues until iron is formed at the core, which will not give off any more energy since they aren't combining. Gravity is trying to hold the star together and fusion is trying to blow it apart. Eventually gravity wins this battle, and if a star is large enough, the star will collapse in on itself instead of going into supernova due to the massive gravity it creates. All the mass of the star is compressed into a single point, which is the birth of a blackhole.

Supermassive black holes are black holes that have been around long enough to suck in lots of mass. These beasts have ridiculous amounts of gravity, and that is why suns (the basis for solar systems) orbit around them forming galaxies.",null,1,cdefd5s,1qmi0i,askscience,new,3
All_I_Do_Is_Wheel,"It is my understanding, that once matter began to accumulate into large clouds, the centers drew in more and more gas, which never actually became a star, but gained enough weight to collapse in on itself. The vast amount of gas simply coagulated fast enough, that it went from gas to mass, without having ever been a ""star"", per say. Once the black holes were created, the remaining gas around them fell into place, revolving around the black hole, thus becoming a galaxy. Hope that helped. :)
",null,8,cdeajir,1qmi0i,askscience,new,2
iorgfeflkd,"It's problem is not chemical instability, but nuclear. It decays within about 20 minutes. Although it exists in some uranium and thorium ores, it is often studies in labs either as a decay product of actinium (this is how it was first discovered), or by making it by smashing oxygen nuclei into gold.",null,0,cde9lao,1qmh6z,askscience,new,14
99trumpets,"Plant cells do contain protein (every cell contains protein)- just not very much. Another way to say this is: the fiber:nitrogen ratio and also the carbon:nitrogen ratio are fairly high for herbivore diets. But there is some protein in there.

The herbivore solution is twofold: (1) break down the plant cells very thoroughly using ""fermentation vats"" in the stomach with bacteria that can digest through the cellulose of the  plant wall. (2) ""Bulk feeding"", e.g. eat a large volume of food per day (relative to body weight). Herbivores eat almost constantly, and are near-constantly voiding the indigestible fiber out the other end. Bulk feeding is especially prominent in the grazers (grass eaters) like horses and elephants.  

There are also associated anatomical specializations; those grazers that are best at bulk feeding tend to have the fermentation vat in the hindgut (e.g. the colon or cecum), while other grazers, and most browsers, tend to put the fermentation chamber in the foregut in some sort of multi-chambered stomach.

There is also an association between nutrient content of the diet, body size, and metabolic rate. Generally, the poorer the nutritional content of the diet, the larger the animal and the slower its metabolic rate. (though there are some exceptions). That reduces the g of protein that the animal must acquire per day, per kg of body mass. So, larger herbivores tend to be the grazers, i.e. they can survive on just grasses if necessary (elephants, horse, cow, elk). While smaller herbivores (small deer, antelopes like dikdik, various rodents, etc) tend to be browsers, i.e. they need more leaves and also sometimes need fruits and seeds and generally more nutrient-rich components.

source: [This](http://www.amazon.com/Comparative-Physiology-Vertebrate-Digestive-System/dp/0521617146) is the classic text",null,0,cdeiveo,1qmgev,askscience,new,8
asadasa,Cows have complicated digestive systems that use bacteria and other microbes to break down plant matter. These fauna actually can produce the amino acids that they would otherwise not be able to get in their diet. They then digest the byproduct of that whole fermentation process and get all the nutrients they need.,null,0,cdeixi0,1qmgev,askscience,new,1
dakami,"The formula for fats is roughly CnH(2n+1)CO2H.

The formula for carbohydrates is roughly Cm(H2O)n.  (m and n define a ratio.)

The formula for proteins is *really* roughly C400H620N100O120P1S1.

Essentially, given a very small amount of Phosphorus and Sulfur, everything else you need to make proteins is in fats, carbohydrates, and of course, air (which is mostly Nitrogen).  Conversion isn't free, but it's ultimately a thing that animals actually do. Even us.

Remember, bodybuilders are _optimizing_.",null,0,cdfdr60,1qmgev,askscience,new,1
the_good_time_mouse,"They get plenty of protein: they can digest cellulose, gaining access to the inside of plant cells. 

Grass is a nutrient poor food (not just protein) which is they eat a lot. Gorillas do this too.",null,3,cdef19r,1qmgev,askscience,new,3
Whisket,"I work as an engineer modelling water distribution systems for major utilities. The basic components of water utilities, or sometimes private water companies (these are less common) are the following:

* Reservoirs / Water source: This can be a water treatment plant, a connection from a neighboring utility, wells, or any other source of treated water.
* Pumps: Generally there are pumps located at EACH source of water. The pumps are not constantly running, but turn on to fill up water tanks when their levels start to drop off. Depending on the complexity of the system, pumps can help supply pressure during peak times. 
* Water Tanks: Water tanks are scattered at high points throughout a water distribution system area. Pressures within a water system is determined by the elevation difference between the water tank and the point of use (your house / business / whatever). 1 foot of elevation difference will equate to ~0.43 psi. The other things that affect pressure are pipes, pressure reducing station, and sometimes pump activity.
* Pressure reducing station (PRV): Exactly what it sounds like. Because it's possible that your house is served by a water tank that is 400 feet higher in elevation, you do not want to have 150 psi at your faucet. These PRV's are placed at points throughout the system to regulate pressures.
* Pipes: Self explanatory, but I include for this reason: the length, size, and material of pipes will affect pressure. Water travelling through pipes will experience friction, and this needs to be taken into account to accurately determine pressures.

Water systems are designed to meet ""peak flow"" conditions, and/or fire flow conditions. Historical data is used to determine the maximum flows experience from the city, and the system is designed around that. Furthermore, a ""fire flow"" condition models the affect open hydrants will have on pressures throughout the system.

Lastly, water tanks are sized according to these demands. There are state regulations (at least in California) that set requirements for how much water storage is needed depending on the demand conditions. The tanks will supply water, and when they start running low, the pumps at the water sources will kick on to supply water to the system and fill up the tanks.",null,13,cdec47n,1qmg5r,askscience,new,111
aBORNentertainer,"Large tanks placed above the level of use.  Water towers in rural communities and sometimes these tanks are on top of buildings in metro areas.  

What I want to know is, how do they get water to the top floors of skyscrapers?  Are they using pumps?
",null,3,cde9y4q,1qmg5r,askscience,new,17
Unsafely_Eject_USB,"I have programmed the computer systems used to distribute water (and also the sewage systems) in towns and cities.

The water towers previously mentioned are a great way of providing water pressure. The trend these days is to no longer use water towers but to use large storage tanks and use pumps to distribute the water, at least in the area that i work/live.

Water storage is essential because the water demands of a city change throughout the day. Much less water is used at night than during the day. The filters that are used to clean the water are best used at a constant water flow. So the water reservoirs may go down a bit during the day and go back up at night. The volumes of these reservoirs can be hundreds of cubic meters.

[Centrifugal pumps](http://en.wikipedia.org/wiki/Centrifugal_pump) are used to pump the water from the reservoir to the distribution system. These types of pumps are optimal because they can operate at a very consistent pressure throughout a wide range of flows and can even run with no flow without damaging the pump. 

Once the water leaves the reservoir is fed into the water mains of the city. As water flows through a pipe it losses pressure due to friction, changes in elevation, etc. Booster stations are placed along the main water lines to bring up the pressure again. A typical pressure is 45 to 50 psi.

Pump stations typically have multiple pumps. One pump is always running to maintain line pressure. As water demand increases more pumps turn.

Additionally, the water plant itself has multiple filters. The operators can decide how many filters to run based on projected water demands. Much more water is used in the summer to water plants and lawns.

That is pretty much a brief overview. If you want to know more about a specific process let me know!",null,1,cdec3y8,1qmg5r,askscience,new,9
EdibleBatteries,[Watertowers.](http://en.wikipedia.org/wiki/Water_tower),null,2,cde9wee,1qmg5r,askscience,new,4
ajschroeder,"Define great distances...

Anyway, in my home town we had municipal water from wells that was pumped into underground tanks. From there it was treated and then sent to the water tower so that the system could use gravity for pressurization. Tens of thousands of gallons of water sitting in the water tower will provide adequate pressure without having to use pumps constantly.

Surges, or ""water hammers"" were handled at the water meter wherever the service came into a building. It was basically an air pocket that would deal with the hammering effects from the valves opening and closing through use. I was also on the fire department and when we hooked up to a hydrant we had to be careful to not shut valves off too fast since there were no hammer arrestors on the mains themselves.

HTH",null,0,cdeboh4,1qmg5r,askscience,new,2
ShutUpTurkey,The city of Portland Oregon uses gravity to send water 17 miles from the Bull Run watershed via up to four water conduits 40 to 60 inches in diameter. The conduits fill very large reservoirs which are built on natural buttes to maintain pressure and also satisfy spikes in demand. The Eastern side of the city is supplied with water entirely without mechanical pumps. ,null,2,cdegi3w,1qmg5r,askscience,new,4
FL-Orange,"Aside of the water towers mentioned, some cities such as my own have multiple pumps.  Maybe 4 are running all the time and 2 or 3 are brought online during peak usage hours.  I'm pulling the number of pumps out of the air but the process is correct.  Tall buildings will have booster pumps unless they have a tank on the roof.  In SWFL I've yet to see a roof mounted tank.

Source:  I'm a plumbing designer.",null,0,cdebgb7,1qmg5r,askscience,new,1
jray2212,It Also usually takes several pumps to reach very high levels as the weight of the water causes pressure.   I can't remember the feet of rise to psi formula right now but it is used to figure how far up you can pump water then put in another pump and lift again.   There are also different types of pumps used in a water system.   There are transfer pumps which are for volume not pressure and pressure pumps for the opposite.   It is actually pretty cool how it all works together.    It is a shame we are not taught at least a basic understanding of civil engineering. ,null,0,cdedmn8,1qmg5r,askscience,new,1
sarahstrattera,"LTP is the way that connections between neurons, synapses, are strengthened. LTD is the process by which synapses are weakened. LTD is thought to result mainly from a decrease in postsynaptic receptor density or a decrease in presynaptic neurotransmitter release. When you use drugs, it is believed that you feel drug reward due to activation of the mesolimbic dopaminergic pathway (VTA, NAc, limbic system).

So drug use causing LTP in the VTA kind of makes sense, because by continually doing drugs you are pinging that pathway again and again and it becomes strengthened. The VTA releases dopamine, GABA, and glutamate which affects the nucleus accumbens. The NAc will naturally down-regulate receptors in response to being bombarded with neurotransmitter.

Hopefully that is helpful.",null,0,cdefrxh,1qmfwa,askscience,new,9
fartprince,"From ""Nucleus Accumbens Long-Term Depression and the Expression of Behavioral Sensitization"" Brebner et al 2005 Science:

""Within a given nucleus such as the NAc, it is suggested that the relative balance between LTP and LTD at different afferent inputs determines the expression of specific patterns of behavior (32). In sensitized animals, the abnormal induction of LTD at PFC inputs to the NAc may disrupt the influence of this pathway on neural activity involved in goal-directed behavior""

Since the reward pathway is not a linear A-&gt;B-&gt;C pathway, this is the most likely explanation. Behavior is a very complex phenomenon that arises from multiple brain regions crosstalking to each other. In fact, the paper also mentions that while drug effect studies have classically looked at DA pathways, it appears that the glutamatergic inputs from cortical regions seem to be the major player in determining excitability in these cell types. Other papers have also mentioned how specific manipulations of the glutamate pathway (either through glutamate release or through AMPAR manipulations) will directly abolish or enable drug-seeking behavior. So it's not as easy of a question to tease apart as a simple ""LTP vs LTD"" because it changes the question from a synapse-level analysis to a systems/circuit-level analysis. Of course, the next obvious question is ""well how exactly are these circuits interacting through LTD/LTP to influence behavior"" but I don't know if we know that answer yet, unfortunately!

(""Role of GluR1 expression in nucleus accumbens neurons in
cocaine sensitization and cocaine-seeking behavior"", Bachtell et al European Journal of Neuroscience 2008)

Edit: Upon re-reading this, I realize this probably wasn't the answer to the question you were asking. Sorry!

Second edit: This might have some better, more detailed information in terms of mechanism specifically at the AMPA receptor level:
http://perspectivesinmedicine.cshlp.org/content/early/2012/12/10/cshperspect.a012021.full.pdf",null,0,cdegr7c,1qmfwa,askscience,new,3
fu_man_shoe,"I know that LTP most likely stands for long term potentiation, but i'm trying to figure out what LTD means.  Is long term depression a term?  

I would look this up on google if it weren't for the fact that LTD is an acronym for so many different things and every cited paper doesn't actually spell out what LTD is.  ",null,0,cdehfhv,1qmfwa,askscience,new,2
subroutines,"[LTD is induced](http://www.sciencedirect.com/science/article/pii/089662739290248C) by repetitive low frequency (1 Hz) synaptic stimulation. [LTP is induced](http://www.pnas.org/content/99/11/7740.short) by a tetanic theta burst (5 Hz) or a low frequency input paired with postsynaptic depolarization. So there's that.

Also, before you buy into any of these redditor comments, you should be aware of two things: (1) the effects of drugs on reward circuitry has been intensely studied but is still not fully characterized, and (2) potentiation dynamics has been intensely studied and is only moderately characterized, but is not well understood on the network level (particularly for brain regions other than the hippocampus or cerebellum).


It seems like you are interested in how drug use could establish both LTP and LTD in the same reward pathway, and the best review on this topic IMO is by [George Koob](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2805560/pdf/npp2009110a.pdf). I will also point out that around 80% of NAc neurons are GABAergic, so it's their job to inhibit downstream neurons. Given that the reward pathway is a loop, one could glean how LTP onto inhibitory neurons would create LTD downstream. 


",null,0,cdfde4u,1qmfwa,askscience,new,2
TrustMeImaPhD,Is there any way for you to post the source of this information? It would be helpful to know more about what is confusing you.,null,1,cdeahpr,1qmfwa,askscience,new,2
coolerheads,"This is a nice review from the Surmeier lab of LTP vs. LTD in the striatum. 
http://www.ncbi.nlm.nih.gov/pubmed/19896832

I'm sorry though, about NA plasticity I got nuthin.",null,0,cdedeno,1qmfwa,askscience,new,1
electronday,"LTP and LTD are thought to be associated with very basic neural memory and learning.  The LTP can be thought of sensitization of the neurons to rewarding stimuli like the intake of a drug, while the LTD is related to the tolerance and reduced neural response to the same stimuli.  Take nicotine for example:  Regular smokers usually do not experience a buzz from smoking a cigarette, but studies have shown that certain parts of their reward circuits are sensitized to the rewarding effects of nicotine intake.  LTP and LTD is likely cooccuring in different parts of the brain.   

I'm not quite sure what you mean by it not making sense.",null,0,cdee1iq,1qmfwa,askscience,new,1
tehbored,"If you want to model all of the cellular processes at once, then no. Eukaryote cells are very complex machines. We have not yet created any models which simulate an entire cell. We haven't even fully modeled all the proteins in a cell yet. ",null,2,cde9x6g,1qmft7,askscience,new,16
Smoothened,"Yes and no. The answer to whether we can or cannot model a neuron depends on what specific aspect of a neuron physiology or behavior you want to model and to what level of complexity you want to take that model

Neurons are cells and as such they involve an incredibly complex network of systems and subsystems at the molecular level, from gene expression to membrane potential dynamics to receptor regulation, etc, etc. Current cellular models in general are still very far from comprehensively describing the workings of any simple cell, let alone a neuron. 

A more basic approach, which I think is the one you are getting at, is simply observing the behaviors of a given neuron in response to different stimuli and adapting them into a model. This can be done, but you have to bear in mind that many things determine the behavior of a neuron, such as what type of neuron it is and, more importantly, the neuron's connectivity to other neurons, which is both complex and dynamic. Thus, outside of the molecular aspects, a lot more can be learned from studying connectivity (see [Connectome](http://en.wikipedia.org/wiki/Connectome)) than just the behavior of a single neuron. 

Edit: I should clarify that observing the behavior of a single neuron in response to stimuli can be a powerful technique; I was writing specifically within the context of modelling. 



",null,0,cdea82m,1qmft7,askscience,new,12
burning_hamster,"I think the answer to your question depends on the time scale of the experiment. 

(i) Can we model the membrane potential of a neuron of arbitrary (but known) geometry and with known synaptic strengths? Certainly, at least to within a few percent. All you need to do is simulate a neuron as a bunch of compartments with resistances depending on the diameter, and then cable theory and a bunch of coupled differential equations describing the ion conductances will tell you how to update the membrane potential at each location depending on a particular input pattern. Biochemical processes really don't play a huge role at a time scale of miliseconds, the relevant time scale for input-output relationships in neurons. 

(ii) Can we predict how a neuron will update its individual synaptic weights according to a sequence of network activation patterns? Maybe. There seems to be a [huge variance in the spike time dependent plasticity](http://www.annualreviews.org/doi/abs/10.1146/annurev.neuro.31.060407.125639) rules, and unless you are simulating a pyramidal cell in CA1 (a well studied region in the hippocampus), then you might not have any/enough data to support a particular set of STDP rules, in particular if you believe that magnitude and direction of plasticity does not only depend on pairs of spikes but higher order spike train combinations (triplets, etc.) as well. 

(iii) Can we write down and model the full (biochemical) pathway responsible for remodelling the the cellular architecture (for example: growth and shrinking of synapses, axon/dendrite pruning, etc.) in response to different input patterns? Certainly not. We simply do not know them in great enough detail. Even if we did, I am unsure whether you could computationally manage a realistic model, in particular if you explicitely coupled it to the model of the membrane potential - how to do such multiscale models well is an active area of research for a reason.

 Tl;dr: On short time scales, an artificial neuron that  based on a physical model tries to replicate the input-output relationship of a particular neuron seems doable; modelling the long term behaviour including relevant biochemical processes seems challenging, in particular due to a lack well curated and systematically acquired data on these processes. ",null,0,cdek91r,1qmft7,askscience,new,3
DEATH-OF_RATS,"So far we certainly cannot model C elegans. As everyone else has said, how successful you are at modeling a neuron depends on your definitions of ""successful,"" of ""model,"" and of ""neuron."" We can simulate certain models of neurons, but they're all incomplete. Even if they're terribly complex models, their incompleteness is evidenced by our inability to recreate certain physiological phenomena with simulations. We can certainly recreate some neuronal behaviors, but not all, so a model is generally designed to recreate a specific set of behaviors that a given study is interested in.

There are various ways to measure inputs/outputs. With histology and electron microscopy, you can (very tediously) create a 3D model of the shape of neurons and their dendrites and axons, and even where the synapses are between neurons (a single synapse is on the scale of fives to tens of nanometers). With multi-photon microscopy, you can look at neurons *in vivo* or *in vitro* and see things like calcium flow through cell walls. This doesn't explicitly tell you when action potentials are happening, but there are reasonable experimentally established rules of thumb to infer that. [Also, it's really cool to watch videos of it](http://www.youtube.com/watch?v=nXo4GZxUkaE).

You can measure voltage levels and see action potentials of individual neurons with intracellular or extracellular electrodes or of multiple individual neurons with multi-electrode arrays (extracellular). Measuring local field potentials gives you general dendritic activity across a small area (it's kind of like EEG but with much higher spatial resolution and on a much smaller scale).

A single neuron will have about 10,000 synapses with other neurons, so we are simply unable to record everything that's happening. Generally people choose a method that suits their purposes, and do a lot of inference from there.",null,0,cdmjmq8,1qmft7,askscience,new,1
Ghost25,"The first sentence of the thread you linked says that they had some success modeling neocortical columns which are composed of many neurons. And yes programs like NEURON model neurons and neural networks.

http://en.wikipedia.org/wiki/Neuron_(software)
http://www.neuron.yale.edu/neuron/download
http://en.wikipedia.org/wiki/Blue_Brain_Project",null,3,cdeeooj,1qmft7,askscience,new,1
botanist2,"It's the [thrust reverser](http://en.wikipedia.org/wiki/Thrust_reversal).  The pilot activates devices that redirect the exhaust of the engines so that the engines help to slow down the aircraft faster and with less stress on the aircraft's brakes once it lands. 

[Here's](http://www.youtube.com/watch?v=k796qVt51dI) one in action with the apparent increase in volume that you're talking about.",null,35,cde9asg,1qmdju,askscience,new,360
spartandudehsld,"What you are describing is the [thrust reversal](http://en.wikipedia.org/wiki/Thrust_reverser) process.

I used to work on Pratt &amp; Whitney thrust reversers. The 'opens up' is probably the cowl over the reverser flow grates (can't remember the technical name) and below them are doors that swing down into the high bypass stream of a turbofan engine. This directs the air forward.",null,3,cde9b9j,1qmdju,askscience,new,27
maugoguy,"Airline pilot here, the noise is caused by the engine redirecting air flow creating the most amount of drag possible to slow the aircraft down, also it works in conjuction with the spoilers, so really alot of the time you are hearing noise generated by two different braking methods combined, the wheel brakes and tires cannot handle the job own their own. ",null,6,cdea175,1qmdju,askscience,new,21
Manhigh,"In addition to the thrust reversers that others have mentioned, naval pilots landing aircraft on a carrier assume that the arrestor wire is not going to stop them and that they will have to perform a ""touch and go"".  When they hit the deck they throttle up and prepare to lift off.  If the arrestor gear works, then they can power down.",null,3,cdeg9r0,1qmdju,askscience,new,13
calipilot227,"On most commercial airliners, the pilots will deploy the engines' thrust reversers on touchdown (which redirect the thrust to help slow the aircraft) and extend the speed brakes (small 'flaps' on top of the wings) to hold the wheels firmly on the pavement to improve braking power. The speed brakes themselves also produce aerodynamic drag which helps slow the aircraft.

Commercial airliners are perfectly capable of landing without the use of reverse thrust, but it helps to save wear and tear on the brakes.

Edit: [Here](http://www.hilmerby.com/md80/mdrevbuck2.jpg) is an example of a thrust reverser, seen on a Pratt and Whitney JT8D, the aircraft is a McDonnell Douglas MD-80.",null,2,cdel7be,1qmdju,askscience,new,6
expert02,"No one has mentioned sound reflection. In the air, sound goes in all directions. When you hit the ground, a portion of that sound is going to get bounced back at the plane.

Also, I'm curious if speed plays a part as well. Sound relies on air to travel. When you're going slower, the air is less turbulent. But I'm not sure what effect it would have.",null,0,cdemqay,1qmdju,askscience,new,3
colin8651,"Could it also aid the pilot with an aborted landing once already on the ground and the reverse was enabled?  The engine would be at max power, could they disable the reverse and be right back at full forward thrust again to takeoff in an emergency.

Or is it too late once those are deployed to reverse the operation and get back in the air.

I guess the reason I ask is because Navy pilots go to full thrust just as they hit the aircraft carrier.  I know they need to because of the cable could be missed and they don't have a run way to slow down on.",null,0,cdedcnp,1qmdju,askscience,new,2
Mithster18,"Engine reverse thrust. Well, the engines don't turn the other way, the thrust just gets directed forwards. Sounds counter intuitive, but hey it works :) The engine opening up is the redirecting of the thrust. Spoilers on the top of the wings ""spoil"" the airflow, thus decrease lift.",null,0,cderndz,1qmdju,askscience,new,2
gameshot911,"As good a time as any to ask a question I've always had - exactly how effective are the reverse thrusters?

Whenever I land in a plane I can certainly hear the engines spinning up, but I can never actually feel any effect in terms of deacceleration.  The brakes however are always obviously slowing down the plane.  So what percentage of a plane's stopping power is provided by the reverse thrusters?",null,0,cded17x,1qmdju,askscience,new,1
BikerRay,"They are also at a higher power setting on approach. Because they have flaps/air-brakes deployed, the drag is increased, so they need more power to maintain flying speed. This also means the engines are spooled up to a high power setting should they need to go around. (GA pilot, not jet, so I'm sure someone will correct me! But even a 150 has more power on approach if the flaps are used.)",null,1,cdefruu,1qmdju,askscience,new,2
boomitsjoey,"One of the big factors in causing sound a GTE (gas turbine engine) is the mixing of the hot air with the cold air. When the aircraft is in normal flight the hot and cold air are mixing quite nicely, and the turbulence formed in the stream is minimal. But when the thrust reverser's are engaged the mixture of hot and cold air is more harsh, thus the turbulance is increased and that is what causes the louder noises.",null,0,cdevqra,1qmdju,askscience,new,1
null,null,null,10,cdebttg,1qmdju,askscience,new,6
columbium,"Yes!  This changes the structure of the solid-liquid interface, necessarily affecting electron transfer across the interface.   Also, the isoelectric point of the surface almost certainly changed when moving from hydrophobic to hydrophilic.  This effective ""pH"" of the surface can greatly affect a reaction.",null,0,cdepos7,1qmdgd,askscience,new,4
S7R4nG3,"Yes, to my understanding. The hydrophobicity of a surface revolves around the disruption of the hydrogen bonds in liquid water. The surface can be used as a stabilizer for the solute as in [this](http://pre.aps.org/abstract/PRE/v84/i3/e031906) article outlining how a strongly hydrophobic surface greatly affects the conformational changes of polypeptides. 

Surface hydrophobicity does not affect reactions between non-aqueous solutes, thus can change and or stabilize your reaction in different ways depending on what you're doing. 

",null,1,cdeae32,1qmdgd,askscience,new,4
rockjb,"Well the caveat here is that hydrophobic molecules don't orient themselves on the ""surface"" if you're using surface to mean outside or environment facing.  Proteins fold for example to keep hydrophobic parts on the interior aspect of the protein.  In fact hydrophobicity is the largest driving force in determining tertiary protein structure.  Other hydrophobic molecules, from the moment of translation, are kept locked within a lipid bilayer membrane (first ER then Golgi then plasma membrane).  If you took say several polypeptides that were made entirely of hydrophobic amino acids and put them in water they would clump together and reduce the water facing surface area to a minimum: think oil in water.  ",null,1,cdeeiwp,1qmdgd,askscience,new,2
ixeont,"Sort of, but really the surface chemistry is effecting both the chemical reactions and the hydrophobicity. The chemical reactions on the surface will depend on what molecules/atoms are at the interface. So the surface chemistry will affect the chemical reactions that are occurring as well as the interactions with water.
Hydrophobicity is just a macroscopic physical manifestation of the same intermolecular forces that are shaping the chemical reactions occurring on the surface. ",null,0,cdejvfv,1qmdgd,askscience,new,1
lasserith,"I found an old paper that details how the hydrophobic effect can be exploited to drastically increase reaction rates. They performed Diels-Alder reactions in water with some random support structure. The water forced the diene and dieneophile together resulting in a far higher reaction rates than other solvents. A similar argument could probably be made for other transport limited reactions, and in my mind transport of materials would be the change to look at.

Paper : http://pubs.acs.org/doi/abs/10.1021/ar00006a001",null,0,cdet3yk,1qmdgd,askscience,new,1
SmellyRaghead,Yep. There are whole branches of surface chemistry looking at things such as that. One bloke I went to uni with studied surfactants in boundary layer conditions with regard to some reaction he had devised. Not my thing but it's out there.,null,0,cdev4ew,1qmdgd,askscience,new,1
TimeToHaveSomeFun,"Chlorine is a very effective oxidizing agent, meaning it is capable of oxidizing a lot of things while itself getting reduced in the process. Intuitively, this makes sense because the chlorine atom is very electronegative (meaning that it's happier existing as reduced Cl- anions as opposed to neutral Cl in Cl2). Chlorine also has a very positive standard reduction potential, meaning that it is capable of reducing anything with a lower potential, which just so happens to be most things. Because of this, chlorine can attack a huge variety of molecules, including most carbon-based compounds. Thus, it can effectively destroy living things.",null,294,cdebezc,1qm8be,askscience,new,1232
pound-sign,"To add to /u/TimeToHaveSomeFun's comment, in the context of water disinfection, the mechanism by which Chlorine kills pathogens in water varies: it is thought to either directly oxidize genetic material or destroy cell membranes.

Chemically, chlorine gas (bleach and calcium hypochlorite are also common species for disinfection) reacts in water to form an equilibrium with  hypochlorite and hydrochloric acid. The hypochlorite is important because it then forms an equilibrium with the highly effective oxidizing agent hypochlorous acid.
I hope that was ELI5...sorry.

It is also interesting to note that chlorination is NOT effective against hard-shelled cysts (e.g Giardia, which you may have hear of). This is one reason why affluent areas are turning away from chlorine water treatment plants (or, most often, using chlorine in conjunction with another disinfectant), though you still get the most ""bang for you buck"" with chlorine. 

(Note: the other disadvantage you may have heard about chlorine is in regards to disinfection by products, also it's sensitivity to organic matter... if you're interested in these or other water disinfectants, let me know. I can also point you to some sources if you're interested).",null,15,cdearfk,1qm8be,askscience,new,76
JaxHostage,"Chlorine chemically reacts with the moisture in your lungs, nose, throat, eyes, and on your skin, and creates hydrochloric acid.  The acid burns the various tissues in your body, your body reacts by attempting to flush the containment out, which provides more moisture for the chlorine to react with, creating more hydrochloric acid.  In burning the lining of the lungs your body is no longer able to extract oxygen from the air, coupled with your lungs rapidly filling with fluid, you suffocate.  All of this depends on the concentration and duration of exposure, IF you survive chemical burns are an certainty. ",null,7,cdemgx9,1qm8be,askscience,new,35
Iretrotech,"My Chem teacher explained this interestingly the other day. Basically, A chlorine atom has 7 valence shell electrons in it's outermost shell. It REALLY REALLY wants 8, so it decides that ripping electrons from atoms it comes into contact with is a cool idea. It was used in WW1 as a chemical weapon because, upon inhaling it, it rips apart you cell walls and your essentially drown in a ""cell smoothie"" of cytoplasm and blood. (Chem teacher explained this once, so feel free to correct me if I missed something/ am wrong.)",null,4,cdeouxy,1qm8be,askscience,new,19
holey_moley,"Chemist here coming late to the game.  This is how I explain chlorine's mechanics to my non-chemist clients.  Chlorine is very reactive (oxidative) particularly against organic molecules.  Ususally it comes along and lops off a carbon from an organic molecule which in essence changes that organic molecule to something else.  That organic molecule may have been a blue dye, but the action of removing just one of the carbon atoms from that blue dye could change it's colour.  That's why we use chlorine as bleach.  If that organic molecule is something that is essential for life, and the chlorine destroys it by ripping off one of the carbons, then it is no longer available to perform its life necessary function, and so, death.  Close enough is not good enough in organic chemistry, particulary biochemistry, so changing an organic molecule by removing a carbon atom doesn't necesarily cripple it's function, it often completely disables it.  ",null,3,cdeo11m,1qm8be,askscience,new,17
V1ruk,"Chlorine is really corrosive. It strips electrons out of anything it contacts. I've used it to sterilize gloveboxes for home projects. I'd read that in pandemic situations of tuberculosis, the World Health Organization switches to a 1/5th concentration of sodium hypochlorite to water to kill the spores.

I'd also read that adding a *small* amount of vinegar to the sodium hypochlorite solution will acidify it, which increases its killing potential. Instead of taking 1-10 minutes to sterilize, not sanitize, it will now take 1-10 seconds. Time is dependent on concentration.

So before you do this to clean the kitchen table. I read an article that adding vinegar to bleach unleashes chlorine gas. Twice during a lapse in concentration I've added too much vinegar which creates a lot more gas. Upon spraying this  I perceived a smell that reminded me of touching a battery to my tongue, sharply metallic. Then my throat started to feel raw, and my lungs felt like i was developing pneumonia. Just all coughing and mucus. 

Getting yourself to fresh air and breathing heavily works. It burns for a few days though. I've etched plastic with this stuff. It's truly mean. 

Also would like to ask if any chemists here could confirm what the hell I inhaled. CH3COOH + NaClO",null,3,cdefwdz,1qm8be,askscience,new,10
lyricandverse,"Also - from a physiological standpoint - it's important to note that when chlorine gets into the lungs and HASN'T reacted with body tissue, the free molecules will react with water turning it into hydrochloric acid gas. So...what doesn't alter your actual body-chemistry, corrodes it.",null,1,cdebywr,1qm8be,askscience,new,6
base736,"Follow-up question...  It seems like chlorine occurs in a lot of pesticides (many of [these](http://www.pops.int/documents/pops/), for example).  Is this coincidence, does chlorine act similarly when bonded to an organic molecule as it does when it's in its elemental form, or is something else at work here?",null,2,cdeeo13,1qm8be,askscience,new,5
12and32,"When chlorine steals electrons from whatever it can, the atoms that it steals electrons from undergo shifts in their bonds and their geometry.  This causes molecules bound to these atoms to undergo conformational shifts, or changes in their superstructure.  Most biomolecules only operate in their intended 3D form; disrupt this, and their structure can change, lose parts, or even collapse.

What complicates this is the reaction that chlorine undergoes when it contacts water. 

Cl2 + H2O -&gt; HClO + HCl

Hypochloric acid, the first product, is not only an acid (albeit weak), but its conjugate base, the hypchlorite ion, is a strong oxidizer. Hydrochloric acid is a very strong acid. Acids also induce conformational changes, though through a different mechanism. 

Highly-electronegative atoms like oxygen can act as proton acceptors via extra electron pairs. This also induces conformational changes; the oxygen molecule becomes less negative, which can affect the entire molecule through shifts in where bonds are located, since the oxygen attempts to shed that proton to return to a standard two unpaired pairs of electrons.",null,4,cdeh6kj,1qm8be,askscience,new,8
BioChemistryStudent,"Another point I would raise from a biochemical point of view, is that Cl- in solution is used by most organisms to help maintain osmotic gradients, and prevent themselves from taking too much water and dieing. It's a bit more complicated than that, but adding a 'large' volume of Cl- ions to any living organism's habitat is going to alter the equilibrium it's existing it, and will either desiccate it, or cause it to swell so much it pops. Cl- is also used in alot of secondary transport systems, and can gum up those works as well. 

In P.Chem the other day we were talking about how the concentration of ions in solutions changes the interactions between phospholipids, and I would hypothesis that the Cl- ions are also weakening the polar-polar head-group interactions between the phospholipids. 

So, to recap; the ions in solution will change the osmotic pressure of the cell, as well as weaken the cell membrane, allowing them to be lysed, thus killing your microbes.

There are exceptions to this however, the Halophile family of bacteria can live in extremely high concentrations of salt. Staph. Aureus for example, is normally a helpful bacteria that lives on your skin, it can survive in up to 12% NaCl solution. Chlorine doesn't kill everything, but it does a good job with those not used to it. 

Hope that help!",null,3,cdeh1kq,1qm8be,askscience,new,7
Throwaway1944719115,"Just FYI, diluted Chlorine bleach disinfects better than concentrated bleach. Also it works best when left on surfaces for 15 minutes, does    almost nothing if sprayed on and wiped off immediately.

To answer the question. Chlorine destroys the cell membrane and destroys the DNA inside, the component that allows for cellular reproduction.",null,2,cdelmt9,1qm8be,askscience,new,4
This_Explains_A_Lot,"So can i ask as it is related to my work. A few times a week we have small amounts of chlorine and hydrochlroic acid mix in our workshop. What is the gas that comes from the reaction and what kind of damage can it do? 

I would also like if someone can tell me why adding Cyanuric acid to a body of water makes the chlorine in that water last longer and seemingly become more effective?",null,0,cdet6cf,1qm8be,askscience,new,2
crusoe,"Chlorine's reactive nature is super effective at denaturing proteins. Denatured proteins lose their structure/function, thus killing the cell. Its a bit more complicated than ""It reacts with everything"", but apparently the damage chlorine causes can trigger various kinds of cell death in various ways.

Kinda like super-duper-oxadative stress.",null,1,cded8nz,1qm8be,askscience,new,3
jcpuf,"Physiologists:
I've wondered for some time whether the chlorine gas, upon reducing to the chloride anion, would make the mucous lining of the lungs more hypertonic, and in so doing draw more fluid across the alveolar walls into the lungs. Persistent cough is a symptom of chlorine gas exposure, but I'm not sure that that isn't just somehow a response to the reduced oxygen gas.",null,1,cdeii0b,1qm8be,askscience,new,2
jtssunny,"Eating brains does run a higher risk than eating muscle or other organs, but that risk is still very low. If you read about the epidemiology of Creutzfeldt–Jakob disease (CJD), the most common prion disease, the incidence was 1 in 1,000,000 from 1979 to 1994 and 5 in 1,000,000,000 in those under the age of 30 in the US. With that said, CJD is is thought to have a 20-50 year incubation period which could explain why it's so rare in young people. I think more supportive (or contradictory) evidence of this will start appearing in the 2000-2040 time frame (20-50 years after) the UK 80s-90s outbreak and those that ate that tainted beef.

Testing for BSE in cattle and discarding those that have the disease helps protect those that eat cattle organs (particularly the brain).  

TLDR: It's all about statistical probability and the risk of a prion disease infection is very low.",null,4,cdeawh7,1qm85s,askscience,new,33
Jf12,"Tougher for the prions to transmit from cows to humans, however, cultures like the ""Fore"" in Papua New Guinea experienced much higher rates of prion-related diseases, [Kuru](http://en.wikipedia.org/wiki/Kuru_%28disease%29), because they used to consume the body and brains of their deceased. Ironically, they only would participate in this ritual if the deceased looked healthy upon their death, and Kuru had a side effect of 'marbling' muscle making it more appetizing--only leading to the continuing spread of the disease. 

Source--wrote a research paper on Kuru and CJD for a physio class",null,5,cdeb5jo,1qm85s,askscience,new,17
the_ferpectionist,"In addition to what others have written, a major problem with the 90s  outbreak was how modern western food systems operate. 

For one, cows in many places were being given meat, bone and feather meal in their feed that may have been contaminated and may have spread the disease. On their natural diet of grass, cows may still spontaneously develop a prion disease, but it would be less likely. 

Secondly, meat products now often involve blending of meat from from different cows. That effectively multiplies the chances by however many individual cows contribute to a particular hamburger. When eating a brain, however, it is likely just a single cow involved.",null,14,cdei50m,1qm85s,askscience,new,15
null,null,null,1,cde8b92,1qm7or,askscience,new,6
infiniteg,"You might be interested in [this AMA going on right now](http://www.reddit.com/r/askscience/comments/1qm7or/what_caused_mars_to_be_as_arid_as_it_is_now/).  The MAVEN satellite is designed to figure out basically what you are asking, and the AMA is with one of the lead scientists working on the project.",null,2,cdecdms,1qm7or,askscience,new,5
I_am_Bob,"For mars to have had liquid water, it would have had to have a thicker atmosphere to retain enough heat from the sun. The rovers have provided ample evidence that liquid water did exist so we can deduce that at some point mars had a thicker atmosphere. It is also certain that mars at one point had volcanic activity, but now does not. So how are those two connected? Well earth has volcanic activity because of internal heat, caused mostly by radioactive deposits of uranium and thorium. We know that this heat is also responsible for the molten iron near the earths core that generates the magnetosphere that protects our atmosphere from solar winds, and us from harmful radiation. 

It would seem likely that mars did not have enough long half-life radioactive metals to keep the core molten as long as earth. The core froze, mars lost it's magnetosphere, the atmosphere was blown away by solar winds, the surface temperatures cooled and all of the water froze in the ice caps or blew away with the atmosphere. ",null,2,cdefpw9,1qm7or,askscience,new,3
Doug310,"Mars actually joins Earth in the 'Goldilocks zone' (just). The only reason it doesn't appear more 'Earth-like' is because it has a negligible magnetic field (this is needed to deter solar wind, which would strip the planet of its atmosphere). Therefore astrophysicists/astrobiologists believe that it may have closely resembled Earth billions of ago, while the liquid metals in its core were still continuously moving, supplying the planet with a magnetic field. Either the planet simply 'died' one day or it was 'killed' in a collision with another body. There's actually an interesting theory of how life may have existed on Mars before Earth (in microbial form). Then, billions of years ago, when the two planets shared similar orbital paths, they collided; breaking off debris to form our moon and killing life on Mars, while seeding it on Earth.
 http://en.wikipedia.org/wiki/Giant_impact_hypothesis",null,0,cdeohh8,1qm7or,askscience,new,1
Javi2639,"Although I am not completely sure about this, I remember hearing about a theory that Mars used to have an atmosphere and a magnetic field that protected it from the solar wind. This atmosphere was able to trap enough heat from the sun for liquid water to exist. For some reason, the iron core stopped rotating, so the electrical potential disappeared, and with it, the magnetic field. Without the magnetic field, the planet is too small to have enough gravity to keep its atmosphere around it, and the solar wind blew it away. All of the water evaporated off and left Mars the way it is today.",null,2,cdee5em,1qm7or,askscience,new,2
Grumpy_Scientist,"When you have a bulk material like rubber (insulator), silicon (semiconductor), and copper (conductor) the electrons energy levels of the individual atoms form energy bands when considering the bulk properties of the material. To move electrons through a material you must promote them from the valence band to the conduction band, in an insulator the band gap is large it takes a lot of energy to promote an electron, with a semiconductor the band gap is small taking only a little bit of energy to promote an electron, and in a conductor the valence band and conduction band are touching so only a very small amount of energy is required to promote an electron.",null,2,cde6rw1,1qm7by,askscience,new,16
chrisbaird,"""It is quite difficult to find information relating to electricity on the atomic level."" That is because electricity in typical materials does not happen at the atomic level. When trillions of atoms are bound together in a crystalline lattice to form a macroscopic solid chunk of material, the system no longer acts like a collection of individual atoms. Instead, the electron wavefunctions of the various atoms interact and their energy levels spread out into bands. The highest energy band of states is called the conduction band. The electron states in the conduction band are delocalized, meaning that individual conduction electrons spread out and become bound to the crystalline lattice as a whole and not to individual atoms. Electrons in the conduction band are act somewhat like free electrons and can move in response to an applied electric field to form an electric current because they are not anchored to any one individual atom.

The next lower energy band is the valence band. Electrons in these states are bound locally but are the most likely to get knocked into the conduction band (transition to a delocalized state) by a bit of light or heat. 

The larger the energy gap between the conduction band and the valence band (a localized electron state and a delocalized electron state), the more energy is required to lift an electron into the conduction band. Conductive materials have very small band gaps or zero band gaps so that electrons just naturally drift into this state or are easily lifted to this state by ambient heat and light.

Semiconductors have bandgaps that can be shrunk through doping or other effects, turning them from insulators into conductors.

Physically the size of the bandgap is determined by the strength of the bond on the highest energy electron after all the appropriate bonds have been formed in order to form a particular material. For instance, diamond and graphite are both made out of pure carbon, but graphite is electrically conductive while diamond is not. The difference between diamond and graphite is how the carbon atoms are bonded together. In diamond, all electrons are strongly bonded as part of covalent bonds in a tightly packed structure. In graphite, each carbon atom is bonded only to three nearest neighbors instead of four, so the fourth valence electron can become delocalized and conduct electricity.

As another example, if you have a semiconductor with a lattice structure where each atom is bound to four nearest neighbors, and you place a few atoms (doping) into the lattice that have five valence electrons, then the fifth valence electron will have no covalent bond to participate in, so it just becomes delocalized.",null,0,cde8w5s,1qm7by,askscience,new,6
spPad,"[wikipedia](http://en.wikipedia.org/wiki/Electrical_conduction) has a decent explanation of the phenomenon. It has to do with the amount of energy required to free an electron from its binding with its atom, and move freely within the lattice ([band gap](http://en.wikipedia.org/wiki/Band_gap)).",null,1,cde6wab,1qm7by,askscience,new,4
neuromorph,"energy barriers.  others have mentioned band gaps, but Ill go a step further.

Electrons in most materials travel through the HOMO (highest occupied molecular orbital) / LUMO (lowest unoccupied molecular orbital)  transfers.  IN some materials the LUMO has a low energy relative to the HOMO, so electrons are able to move between them easily.  This is classical electron hopping often observed in conventional conductors.

In other materials (semiconductors and insulators) the LUMO is a higher (energetically) than the HOMO.  and the electrons need energy to move into them.  You typically need to add this energy externally, in order to have electrons move in these materials. 

Everything gets a bit messy when we look at electrons moving as waves.  Quantum tunneling can occur, where a percentage of electrons can move through materials (regardless of the HOMO/LUMO energies).  In these cases, the physical distance between electrodes is very small, and the electrons can move through total insulators or even a vacuum.  ",null,0,cde9uy5,1qm7by,askscience,new,3
desertrat1973,"The number of electrons that an atom contains, and how
easily those electrons can be dislodged from their orbits, is
very important in our study of electricity. Any substance in
which electrons can move freely is called a conductor. Atoms
that are tightly bonded are very poor conductors of electricity,
while atoms that contain free electrons in their outer shells
(like copper) are excellent conductors of electricity.",null,0,cdeaglv,1qm7by,askscience,new,3
kamahin,"Much of is has to do with how the atoms bombs together and the number of electrons. Many of the metals we use for conductors are in the transition portion of the periodic table, where there is quite a few electron levels and things get wonky. Iron, Copper, and Gold are all there.  

Alternating double and single bonds thoughout a organic structure could allow for conduction too, since the bonds could alternate and pass the electrons through.

Bachelors in Chemistry. this is what I remember off hand without looking stuff up.",null,1,cdeapjw,1qm7by,askscience,new,3
referendum,"Someone correct me if I'm wrong, I'll need to teach some of this at a high school in December.

Conductivity in solids is a measure of how easily electrons move through the substance. This relates to the influence that the protons in the nucleus have on keeping the electrons there, and the interactions of the electrons between (shielding) and within orbitals.  So, elements with a lot of electrons with low ionization energies should be the best conductors.

[Side-by-side of conductivity and first ionization energy](http://imgur.com/aBWdNje)

Some reasons for this are:

The number of full orbitals between the valence electrons and the protons: 
•more full orbitals between valence shell and protons means more shielding from the nucleus.  •With the non-metals, as the p-orbitals become full, the number of protons increase, while the number of orbitals doesn't, so there is less shielding from the increased positive charge of the nucleus.  This pulls the electrons in, and acts to keep them from becoming delocalized.

I think this explains a lot of it, but I can't yet explain why gold has a lower conductivity than silver.
",null,0,cdfkayh,1qm7by,askscience,new,2
colechristensen,"Because your heart rate is variable, it's a rather difficult thing to use statistical methods, but we can build up practical ones.

How confident can you be that the beats you counted actually occurred between a period of 9 and 11 seconds? I'd say quite confident.  Between 8 and 12? Well that's too wide, I'm already pretty confident about the +/- 1 second... how about between 9.5 and 10.5 seconds? That seems to be cutting it too close, your heart only beats about once a second anyway and starting and stopping isn't perfect.

So I just measured on myself, 12 beats in 10 seconds, that's 72 beats per minute. (12/10*60)

If it were 9 seconds that's 80 bpm (12/9*60)

If it were 11 that's about 65 bpm (12/11*60)

65/72 = ~0.9
80/72 = ~1.11

So rounding off the numbers it's about +/- 10% so if you measure 72 like me, you can be pretty confident that the real value is 72 ± 7 bpm.

Notice that there is lots of rounding going on, and that's because there are a lot of estimations and this should be able to give you a scale of the uncertainty, a better value could be had by doing real science, but you can bet that the 'real' answer would be pretty close to the one I've given above.

",null,1,cde6vo4,1qm602,askscience,new,8
cant_help_myself,"No. Field of vision is determined by the placement of your eyes (if they were back further on the side of your head, you'd have a larger field of vision, but a smaller area of binocular vision). Pupil dilation increases the amount of light hitting the retina, but it doesn't increase field of vision, magnification, etc. It's akin to turning up the brightness of the light source on a microscope.",null,5,cde60wn,1qm4gi,askscience,new,21
Egmond,"Prey animals like sheep and goats have rectangular pupils that increase the field of view in the horizontal direction, because they need to detect predators. Their field of view in the vertical direction is decreased. 

Because of this phenomenon in animals, I would expect that dilated pupils increase the field of view of human eyes too. ",null,0,cdefsl0,1qm4gi,askscience,new,1
Sciencefreek,"Imagine the differences of all the Eukaryotic organisms you know.  Mitochondria have been living inside those cells since all the Eukaryotic organisms you know evolved.  In that time mitochondria have given up sexual reproduction in favor of fission.  This means that genes will be highly conserved. However that does not stop random mutations in mitochondrial DNA.  These mutations can build up and cause differences in the proteins that are made. 

Side note: This random mutation rate can be calculated and can act as  a kind of molecular clock.  This data can be used to show how long ago two different species diverged from one another.  

Side note II: Also since mitochondrial DNA is passed solely by the mother, the mutations can also be used to show maternal lineages and migratory pathways.  This was most famously done with humans, and led to some unusual discoveries.  ",null,0,cdel6q8,1qm4ce,askscience,new,2
Grumpy_Scientist,"Energy that can be used to do **work** is lost, the energy it self is converted to a less useful form as entropy increases. For instance when you burn gasoline the energy stored in its chemical bonds is converted into heat which is used to move your car forward. The carbon dioxide produced by the car contains less energy in its chemical bonds as that energy is now heat. 

Kinetic energy is converted to potential energy, if you roll a bolder up a hill and rest it at the top you have converted kinetic energy into potential energy. If you push the bolder off the edge and it roll back down the potential energy is converted back to kinetic energy.



",null,1,cde73ou,1qm3vj,askscience,new,6
DoomlordKazzak,"It dissipates. Let's say you put energy into a cup of water to heat it up. You've added energy to the system but now the cup is a higher temperature than the air around it so the energy begins to transfer into the air. This is called heat in thermodynamics. Heat is always the movement of energy of ""hot"" to ""cold"". This is the thermal equilibrium, the hot object will always heat up the cold object and it must lose energy to do so. It's not destroyed it just spreads out and moves on. 

Kinetic energy is transformed into heat, sound, elastic potential, gravitational potential... ect. energy. ",null,0,cde6zqq,1qm3vj,askscience,new,3
ChromaticAberrant,"According to Boltzmann Entropy in physics is related to the number of microstates your macrostate can attain (multiplicity). The more states your system can be in, the higher the entropy. A good example is removing a wall in a container that separates two ideal distinct gases at constant conditions: they will mix and the entropy is raised since each can now occupy a larger amount of volume (the whole container instead of a half) i.e. each particle can attain more miscroscopic position states that make up the macrostate.

This also highlights the ideas about dissipation and loss of useable energy brought forth by the other posters: you need to invest work to reseparate both gases implying that the rise in entropy meant that energy was spread out onto more states and relocating this energy into a small subset requires the use of work. One can understand this by noting that the probability for all particles of one species to be in one half is negligible and it's much more probable to find the system in a state where both gases are mixed across the container (more microstates!) - so, in a sense, entropy is something like a measure of the dispersion of your energy or the disorder of your system. One finds that in isolated systems entropy has to increase or at least stay constant during thermodynamic processes which means your system gets more disordered or the energy more dispersed.     

In principle what you should keep in mind is that energy does not disappear - it only changes form and dsitribution. Your example about the tea and kinetic energy all come from this direction: the heat stored in the tea dissipates into the environment. If you throw a ball against a wall its kinetic energy is either converted to heat or into deformations. In both cases your energy is stored into the various ways your environment allows for this and thus changes form and distribution which affects its accessibility. ",null,0,cdei3o3,1qm3vj,askscience,new,2
wazoheat,"There are a few competing reasons for why weather prediction is not perfect, and never will be:

1. **Computer models of the atmosphere are approximations.** We know the actual laws of motion for the atmosphere exactly; these are known as the [Navier-Stokes equations](https://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations). However, these equations have a property known as *non-linearity*; they can not be solved for exactly because the variables within them change in time and depend on each other. Therefore, we have to approximate.

2. **The atmosphere is *huge*, and our supercomputers are relatively small**. The highest-resolution computer model for global weather forecasting is the ECMWF's [Integrated Forecasting System](https://en.wikipedia.org/wiki/Integrated_Forecast_System), which runs with a so-called ""[T-number](https://www.meted.ucar.edu/nwp/model_structure/navmenu.php?tab=1&amp;page=3.3.0) of [1279](http://www.ecmwf.int/products/data/archive/data_faq.html#hres), meaning it can resolve features down to around 10 km (6 miles) on a side. This means that it has approximately 4,000 data points in each direction in the horizontal plane, in addition to having 137 vertical levels, for a total of 4000x4000x137~=2 billion points of data that need to be calculated *for each time step* (*note: this is actually a spectral model, which is much more complicated than this grid-point explanation, but it's approximately the same argument*). And due to a mathematical constraint known as the [CFL condition](https://en.wikipedia.org/wiki/Courant%E2%80%93Friedrichs%E2%80%93Lewy_condition), for higher resolution models you need a smaller time step in your calculations. I can't find any specific information for this model, but for a ~10 km resolution model, this time step needs to be around *30 seconds*. So not only do you have 2 billion+ data points, you must apply the model equations to all of these grid points every 30 seconds, or about 30,000 time steps for a 10-day forecast.

3. **Because our computer models are so coarse, we need to make *further* approximations**. As you probably know just from experiencing the world, a lot of weather phenomena are *much* smaller than 10 km. There can be significant differences in the atmosphere over the course of a few feet, nevermind miles. And even if there weren't, you don't get an accurate picture of a thunderstorm that is something like 30 km (18 mi) across when it's only represented in the model by 3x3=9 grid points. So, in order to resolve these small-scale features, models contain so-called *parameterizations*; basically simple toy models *within* the larger model to try to represent processes that are happening at very small scales. There are something like a dozen parameterizations needed for a good model, describing everything from turbulence near the ground to freezing and melting of ice and water within clouds. And while these do a pretty good job approximating the small-scale processes, they are inevitably inaccurate.

4. **Even if our weather forecasting models were perfect, we don't have enough observations of the atmosphere to know exactly what it is *right now*.** [Here is a map of weather observations made at Earth's surface on a typical day](http://www.wmo.int/pages/prog/www/OSY/images/GOS-surface.jpg). As you can see, there are significant gaps, even on the ground where people are all the time. And these observations are not continuous; they are taken only every hour on average, so there are time gaps in the data as well. Additionally, to predict the weather, just knowing the surface conditions isn't enough; you need to know the conditions for the whole depth of the atmosphere. [Here's a map of upper-atmosphere observations from weather balloons](http://www.wmo.int/pages/prog/www/OSY/images/gos-u-a.jpg). The gaps are even bigger, and they are only taken *every 12 hours*, leaving an even bigger time gap. Sure, there are other observations available from satellites and radar, but these don't actually measure the things we need to know like wind and temperature, they measure radiation being emitted and reflected from the earth, the atmosphere, and the objects found in the air, and these are converted through a complicated, imperfect set of computations to get an estimate for the variables we are interested in.

5. **All observations have errors**. No observing instrument is perfect; there will always be errors when measuring the things we need to know for weather prediction like temperature and wind speed. Typically these errors are small, on the order of 1-2 degrees or 1-2 miles per hour, but they have to be accounted for. The process of merging observations into the model in a way that accounts for both observation error and model error is known as [data assimilation](http://www.mmm.ucar.edu/people/huang/publications/inda4gras.pdf) (PDF), which is the field I work in.

6. **Finally, and perhaps most importantly, the atmosphere is chaotic**. All these little differences and errors I mentioned above, they might not seem all that significant. Maybe you don't care whether or not the model predicts rainfall down to the millimeter, or the temperature to within a degree. Why can't we even get basic questions like ""will it rain three days from now?"" correct? The answer is in chaos. The atmosphere behaves in such a way that small differences add up over time. It's often explained in terms of the [Butterfly Effect](https://en.wikipedia.org/wiki/Butterfly_effect); a butterfly flapping its wings in Brazil might be the difference that creates a hurricane in Gulf of Mexico a month or two later. And this isn't really an analogy, it is mathematically true: the extreme non-linearity (chaos) of the equations that govern the motion of air means that something that small can lead to huge differences weeks and months down the road. I once read that even if you had a perfect forecasting model, and perfect observations of the atmosphere from weather stations placed 1 meter apart for the entire depth of the atmosphere, you still could not predict whether or not it would rain a month from now. *That's* how chaotic the atmosphere is.

So with all these complications, I hope it seems that much more amazing to you that we can even predict the weather *at all*. And maybe cut your local weatherman a little slack, okay? :)",null,3,cded0ed,1qm3va,askscience,new,52
DrMantisofPhilly,"Despite our vast resources of technology, all the data that would have to be gathered for one area to correctly predict the weather would be immense. It goes from surface temperatures, to air pressures, air moisture, areas of low or high pressure, wind speeds, what the air is like near the tropopause...tons of stuff that can all be put into numbers and recorded. That and weather is always changing, so many factors can play into what the weather is going to be like for a day that it is also hard to account for that as well. Like the air that a certain front is going to encounter also plays a big role in the weather, and that is another thing that has to be accounted for, which takes more readings and data collection. 
Meteorologists are hoping to be able to better predict the weather with more powerful number crunching computers as well as more weather recording stations, but until then they are doing the best with what they have.  ",null,7,cde7ax0,1qm3va,askscience,new,25
the_birds_and_bees,"Broadly speaking, it is because the models we use to predict the weather are 'chaotic'.

A more descriptive word would be 'unstable'. They kind of work for a short period of time but over longer periods they quickly diverge from what actually happens. In the lingo this is called 'sensitivity to initial conditions'. Basically what it means is that unless your initial measurements are exactly precise (as in no error at all) then the model will quickly diverge from reality. You might say 'But can't we just measure more accurately?' which is usually a good answer, but with chaotic systems this doesn't help. Any error, however small, is amplified through time and will cause the model to diverge very rapidly.

A much simpler example that exhibits the same property is the [double pendulum](https://en.wikipedia.org/wiki/File:Double-compound-pendulum.gif). There's a relatively simple set of equations that describe it's motion but given an actual double pendulum we can't predict how it will behave long term because of this sensitivity to it's initial conditions. We can never measure it's starting position accurately enough to predict it's behavior in the long run.

John Cook has a nice numerical example [here.](http://www.johndcook.com/blog/2013/11/12/sensitive-dependence-on-initial-conditions/) TL;DR in this example, 6 decimal points of accuracy gives a model accurate to 7 seconds. Every extra decimal point of accuracy makes the model accurate for ~1 second extra. Put another way, you have to work ten times as hard for each extra second you gain.
",null,3,cdeaklh,1qm3va,askscience,new,8
DangerDave_,"Atmospheric Science major here.

Our models have gotten a lot better over the last decade due to an increase in computing power. The models can handle more data points and run them in a time frame that makes sense (12 - 24 hours). We are nearing the point though where an increase in computing power would not make a difference in accuracy. This is because we do not have accurate observations in many of the grid points. For example, as grids get smaller and smaller (on the order of a square kilometer) the current conditions in many places are just assumed from previous model runs. This causes error the further out the model propagates. 

There are a ton of gaps in the country for these observations, mountains, much of the US Midwest, and much of the Pacific ocean. getting these observations is costly too. A weather balloon costs a couple hundred dollars and a couple hundred are launched twice a day around the country. But there are millions of grid points and only a couple hundred balloons. 

TL/DR: We don't know what the exact conditions in the middle of Montana so we cant predict what will happen when the air propagates to the east. ",null,1,cde8w78,1qm3va,askscience,new,8
null,null,null,1,cde98k4,1qm3va,askscience,new,2
Kaarjuus,"Weather can never be predicted with 100% accuracy. To add to the points already mentioned (that our data points are scarce), there are two fundamental problems:

- a complex system like weather can only be predicted 100% accurately with a 100% accurate model. Meaning that we would essentially have to create a 1:1 virtual copy of our atmosphere and all that affects it, down to the individual molecules. Anything less and we run into the basic property of complex systems - their behaviour arises from complex interactions between their parts, and this behaviour is not predictable from the attributes of the parts themselves. There can be no shortcut here, the only way to get 100% true prediction is to run through all the small individual interactions.

- even if we magically had a perfect full-scale model with perfectly realistic physics, we *still* could not get 100% accuracy. Because complex systems are affected by even tiny differences in starting conditions. But measurements in the physical world are always imprecise, always with a certain inaccuracy. So even if we magically somehow managed to measure *everything* in our atmosphere and plug those numbers into our model, the model would soon go out of sync with the atmosphere, as the effects of the tiny differences between measurements and reality would start accumulating.",null,2,cdea8g3,1qm3va,askscience,new,2
rocketgolfer,"With practical problems of fluid dynamics (which includes meteorology), we simply do not have computers big enough to resolve the system fine enough to actually resolve all relevant scales of motion in the appropriate amount of solution time. There are a variety of models that can be used to provide ""closure"" to the systems, i.e., use additional equations to describe the scales of motion we can't afford to resolve, but all of these have their inherent flaws.",null,1,cded74i,1qm3va,askscience,new,1
295f423c5f2b37416d6a,"Weather prediction has (at least) two things working against it: data sparsity and algorithmic intractability.

The data sparsity issue is related to the limits of acquiring (let alone storing) measurements that describe weather. We don't have weather stations everywhere, so we have to make do with necessarily incomplete measurements.

The intractability issue speaks to the way predictions are made from the data. For a prediction to have value, it must describe an event that has yet to happen. A good prediction might require too much work and/or too much time. An okay prediction might require less work, but suffer from inaccuracy.",null,1,cdedzx8,1qm3va,askscience,new,1
Relax-Enjoy,"The rule of ""Random Chaos"" governs here, where every single molecule could make nearly infinite directions of movement based upon forces upon it. It is LIKELY to act in a certain manner, but that is mainly ruled by a bell-curve.

Now, imagine trillions upon trillions of molecules with the same ""odds"" of doing what is predicted.

This is Random Chaos.",null,2,cdeabj8,1qm3va,askscience,new,1
zalaesseo,"Probably because they have rigid legs and a heavy body. A chair standinf upright is likely to fall over, but a fallen chair is unlikely to stand rightside up. The insect is probably like that, but because they are lightweight, any small wind movements will topple them over.",null,0,cdefmv1,1qm2e4,askscience,new,1
MarineLife42,"When insects die, they muscles relax and their legs move into their default position, folded under the body. That creates a sort of pointy tip underneath the insect body so they topple over and come to rest either on their side, like flies, or even their back, like many beetles. Also, once a (dead) beetle is on its back - say by the wind or any other disturbance - this rounded shell makes for a very stable position. ",null,0,cdenwsb,1qm2e4,askscience,new,1
justin3003,"Yes, you absolutely can. The most common problem that arises from long term NSAID use is stomach irritation and/or bleeding. As was mentioned before, NSAIDs as a group have varying effects blocking COX1 and COX2 enzymes. COX1 is associated with creation of, among others, a group of molecules known as ""prostaglandins."" More specifically, COX1 creates prostaglandins not classically associated with inflammation (that's more COX2, which is why COX2 inhibitors like Celebrex can reduce inflammation without COX1 type effects like bleeding). There are many different types and derivatives of prostaglandins with various roles around the body, and one such role is protecting the stomach lining from acid erosion.

Also, as was mentioned in another post, long term NSAID use (regardless of the drug) carries a risk of increased heart attack and stroke risk. The mechanism isn't terribly important, but this effect is of concern if you have any family or personal history of heart disease and/or stroke. ",null,0,cde9xyx,1qm0fm,askscience,new,5
null,null,null,1,cde6pbc,1qm0fm,askscience,new,3
null,null,null,0,cdea50f,1qm0fm,askscience,new,1
vicklepickle,"The main issues are stomach ulcers/GI bleeds due to the effect on the stomach lining which is why you normally get given another tablet to take to counteract their negative effects. 

In susceptible people NSAIDs have a pretty bad effect on kidney function. My dad's specialist is really REALLY against the use of ibuprofen.

EDIT: by susceptible people i mean pre-existing risk factors for renal disease.",null,0,cdedn1z,1qm0fm,askscience,new,1
eekabomb,"Yes. /u/justin3003 has given a fairly sufficient overview of how COX1 inhibition can lead to the risk of GI irritation, inflammation, ulceration, and bleeding that can occur with long-term use of NSAIDs. I would also like to mention that this is a Black Box Warning, a warning mandated by the FDA due to seriousness of this risk, and that these problems can occur at any time without warning with the use of NSAIDs. 


the increased risk of cardiovascular and thrombotic events is also a Black Box Warning and nothing to take lightly. sudden death due to MI is the reason one COX2 inhibitor (Vioxx) was taken off the US market.


the differences between individual members of the NSAID class will lead some MDs to use one over the other in persons with certain health conditions (ie renal impairment, GI/stomach ulceration, cardiovascular disease risk). for example Naproxen is often preferred over Ibuprofen for long term use due to it's longer action and slower onset (this statement is not medical advice regarding the use of a daily NSAID). 


it is also worth mentioning that in some populations NSAIDs can have effects on platelet aggregation (bleeding) and K+ levels.


specific to the chronic use of NSAIDs for headaches I would also like to point you in the direction of drug-induced headache (i.e. if you take ibu all the time to prevent headaches, you're going to get headaches when you stop taking the drug and have them all the time)",null,0,cdeof97,1qm0fm,askscience,new,1
bearsnchairs,I worked at the CDC doing some blood analysis for a few years. We trashed the blood vials in biohazard bins which were tagged for incineration. They might have been autoclaved first and then incinerated. Disposal of bodily fluids is a very controlled and regulated process and must be thoroughly decontaminated to prevent the accidental spread of disease.,null,167,cde8dxp,1qlzxa,askscience,new,841
Beach33,"So basically there are 2 types of biological waste that a hospital needs to dispose of. There is red and yellow. Red trash contains needles, blood, urine etc. Yellow trash has placentas, chemo drugs, cultures from the lab, generally things that can't just be buried. Yellow is incinerated. Red is cooked and buried.",null,16,cde8jvn,1qlzxa,askscience,new,79
MidnightSlinks,"I'll add that typically a lab/phlebotomist typically won't draw significantly more blood than is needed to run the tests 2-3 times (as backup). Much of the blood is actually used in the tests themselves, especially if we're talking about running 25+ tests on someone who is being closely monitored in a hospital. The exception would be when multiple labs are assessing different components in which case they each need their own vial.",null,12,cde9s4u,1qlzxa,askscience,new,29
fascinatedtongue,"Depends on the place's current biohazard protocols.  The hospital/lab I work in keeps chemistry tubes in the fridge for a week; hematology for two days; coagulation for one day; and all other body fluids (not including urine) for three to seven days.  Cytotoxic samples are placed in a yellow tinted box for incineration and special handling.  Blood (and body fluids that are not urine) are taken off site and placed with multiple other hospitals blood to be incinerated.  Urine is poured down the sink and the containers are thrown into the regular trash unless it is bloody, in which the sample gets sent with the blood for incineration.",null,5,cdeav93,1qlzxa,askscience,new,13
jibbernaut,"I just want to add to the discussion that I work in a blood testing lab.  After testing is completed, we pour off the serum to make jugs of pooled serum that are sold to other facilities for use as a testing control.  The clot and tube are disposed of in red biohazard bins.  I know my facility makes a lot of money from the process, and it cuts down on the weight (and cost) of our waste disposal.",null,0,cdeht1o,1qlzxa,askscience,new,5
shakakka99,"Worked at a major laboratory for several years.

The blood is left refrigerated for at least another 1-2 weeks.  This is in case test results need to be repeated, or in case the original samples (the ones that are poured off into smaller tubes for individual testing) are lost.  Serum (spun down in a SST-serum separator tube) can last quite a while.  Whole blood, not as much.  It depends on the vial or tube it's stored in, as some have additives to preserve the sample.

When the blood is dumped, the tubes go into red biohazard bags.  These go into larger red boxes also marked with the biohazard symbol, and they are picked up several times a week by a disposal company.  I can only imagine what happens beyond that, but I'm sure disposal is expensive.",null,4,cdejub3,1qlzxa,askscience,new,7
Syphonfire,"I currently work as Biomedical Scientist specializing in Clinical Biochemistry in a large UK hospital. After analysis of a blood sample we store it in a fridge at a temperature between 4-7 degrees centigrade for up to seven days. 

This allows for retrieval if a test is missed off, which happens a lot more frequently than you would think with these large automated systems as well as the addition of further tests if requested.

Once the seven days is up it is discarded still in its tube into a clinical waste bag (these are yellow in the UK as per EU regulation) and sent off to be incinerated.",null,1,cdeamgf,1qlzxa,askscience,new,5
ulkesh-nolm,"I've worked in both human and animal labs.  The human lab would store everything possible for as long as possible in -20 or -70 freezers or even liquid nitrogen units for viral strains/tissue culture.  When it came to discarding waste samples would be autoclaved and then incinerated.

The animal lab would keep samples for a week and discard for incineration (no autoclave).  Some samples would be stored for longer if required (animal cruelty prosecution for example) mostly -20 with a very small number in -70",null,0,cde67ex,1qlzxa,askscience,new,2
null,null,null,0,cde9i9w,1qlzxa,askscience,new,1
null,null,null,1,cdeb6gf,1qlzxa,askscience,new,2
jimmy_beans,"I worked for a lab that tested plasma/serum/urine/other samples for pharmaceutical profiles.  The sample would be put through an extraction process (typically solid phase or liquid/liquid extraction) to separate and concentrate the substances we were interested in.  The rest of the sample (nonextracted portion) would end up diluted with chemicals like methanol or acetonitrile after the extraction process, meaning it had to be disposed of as biohazardous/chemical liquid waste.  All disposable items (tubes, cartridges, pipette tips, etc) that came in contact with the sample go into the solid biohazarous waste stream (red sealed bags).  Work areas areas are treated with bleach after finishing.  The solid and liquid wastes were shipped off in trucks at great expense, and I'm assuming the waste was incinerated due to the potentially biohazardous components.    ",null,1,cdeckqh,1qlzxa,askscience,new,2
gotospaceyoungman,"i currently work in a lab in a hospital, and we save all blood specimens from every patient, every day, for 1 week. part of the duties of one of the evening shift workers is to throw away the blood that is about to turn 8 days old each night, the disposal process is as described by /u/bearsnchairs. specimens are kept for a week should the situation arise that an MD or a pathologist finds some sort of anomaly or the patient's case changes such that new tests need a baseline test value to run against.",null,1,cdef2qf,1qlzxa,askscience,new,2
Forever_A_Student,"Molecular Clinical Med Tech here. We process blood tubes and screen for a whole bunch of tests, but mostly CML. They get tossed into Biohazard bins and incinerated. The blood we pour off post centrifugation gets combined with a bleach solution and then disposed of to prevent possible pathogens from spreading.",null,1,cdegemf,1qlzxa,askscience,new,2
Rockabellabaker,"The newborn screening program where I am collects specimens on blood cards (they used to be called Guthrie cards IIRC), and stores them in a secure temperature controlled facility for 18 years. Not the same as blood collected in vials, but interesting to know that they're not going to be destroyed for a long time. 

After the 18 years is up (or sooner, with parental request), the cards are recalled from storage and incinerated. This hasn't happened often as the program started running in 2006, and many parents aren't even aware that these samples are kept for that long.",null,1,cdegzvx,1qlzxa,askscience,new,2
firstfloornudist,"I work as a phlebotomist in a medical center and we simply throw out TONS of blood. It just goes into huge biohazard waste containers and shipped off. Honestly most of the blood you have taken will not ever be tested. In the lab I work at we draw so much extra blood on people it's insane and 95% of it is never used. We can draw ~30-40 ml of blood from you when only 1ml is needed for the testing ordered and then we come up with some crazy circle lie when patients ask ""why are you drawing so much?"" that makes it seem like its actually necessary.",null,0,cdets7s,1qlzxa,askscience,new,1
Draemor,"I did work experience in a pathology lab for about a week last year and organisation of blood samples was one of my jobs. They are first colour coded, then arranged in various trays. One their purpose is served they are sorted into larger trays which go into large fridges (think of meat storage) for one week exactly. The shelves are dated, so they are cleaned every morning. The used samples are then sent to a large bin ready for incineration.",null,0,cdeur3g,1qlzxa,askscience,new,1
chazzlebear,"I work at a pharm company where we use blood samples, plasma samples, live cell cultures etc etc. The general rule of thumb here is:

Place anything which contained any of the above into a strong disinfectant solution after removing all the liquid by suction into a big jar which also contains strong disinfectant. After 30 mins - 1 hour the containers are taken out, double bagged into auto-clave bags (just plastic bag things), sealed and sent to be autoclaved (high pressure + high heat machine). Anything that can't be autoclaved (usually smaller items that have been in contact with contaminants) are placed into incineration bags and well...incinerated.  Gloves used to handle anything like this are also incinerated. ",null,0,cdeatwd,1qlzxa,askscience,new,1
PipettesByMouth,"A quick search pulled up just a few papers that mainly concern its spectroscopic properties and electronic structure.  Half of those were computational.  The interest is mainly academic, which makes sense.  ""Sandwich compounds"" like Ni(Cp)NO and [ferrocene](http://en.wikipedia.org/wiki/Ferrocene) have been of considerable interest to inorganic chemists, and their discovery ultimately led to a Nobel Prize.

As far as the toxicity goes, it's probably a volatile source of nickel (which is toxic) and NO (also toxic).  I'm not sure why it would be more toxic than something like nickel tetracarbonyl.",null,0,cdf9t4q,1qlzta,askscience,new,1
adamsolomon,"What do you mean by ""the acceleration v. mass graph?"" What exactly are you plotting - i.e., acceleration and mass of what?

If you take a constant force and apply it to objects of different masses, measure how much each object accelerates, and then graph those against each other, you'd find a 1/x type graph, because of Newton's law, F=ma.",null,1,cde4mh9,1qlzon,askscience,new,8
infinitooples,"I will admit buoyancy is one of those things that gives me trouble.  It's actually a little complicated, but is a *really* important observation that has been around since ancient times, and forms the basis for one of Physics' best stories.  The easy answer is that the force is produced to lower the system's energy, but I am personally happier when I can kind of see the mechanism behind how a system lowers it's energy, so that's what I'll take a stab at here.

I think it helps to imagine a bubble of water first.  This bubble of water is filled with water and is exactly like the surrounding water.  Unsurprisingly, nothing happens.  Now change the bubble to nothing, a vacuum with some structure that keeps the surrounding water out.  Now the bubble has no mass, and therefore is not pulled down by gravity.  The water above and below it still do feel the effects of gravity, and being a liquid, can fall towards the earth by flowing around the bubble.  As liquid falls from the top of the bubble to the bottom, the bubble rises.  Note the bubble is also lighter than air, and so continues to rise until it reaches space, at which point there is no longer a buoyant force on it.  Now change the bubble to concrete, it clearly falls downward, but at a slower rate because it must push the water out of the way as it falls.",null,2,cde5ysu,1qlz7b,askscience,new,8
Zeinoun,"Its called the buoyant force or ""Archimedes' Upthrust"".
F=gdv

F: buoyant force or the force the liquid exerts on the other object immersed in it (always upward)

g: gravity

d: density of the liquid

v:  Volume of the displaced liquid (which is equal to the volume of the immersed part of the object)

now where does the density of the object fit in the equation? Well the denser the object is the more mass its V will contain. 
So if an object is not dense: its V will be high (and thus having a high buoyant force on it) and its mass will be low (thus having a low weight exerted on it). So since bouyant force and weight are opposite and both on the y-axis, we can conclude that the object will probably float (having  a low weight and high buoyant force on it).

Vise versa for dense objects. ",null,3,cde4ph6,1qlz7b,askscience,new,7
chrisbaird,"Fundamentally, buoyancy is caused by the force of gravity. Think of it this way: you have a playground seesaw (teeter totter) and a fat adult sits on one end and a tiny kid sits on the other end. Gravity pulls on both of them, but it pulls on the fat man more (don't forget that gravitational *acceleration* on earth's surface is constant for all bodies, but gravitational *force* depends on the mass). Because they are connected by the plank, gravity pulling the fat man down forces the kid on the other end to go up.

It is a similar situation with buoyancy. Place an object with a smaller mass (per given volume) then the mass of the surrounding material (per the same given volume). Gravity pulls on both, but it pulls harder on the more dense material. The dense material wins and falls, forcing the less dense object up and out of its way. ",null,0,cde93es,1qlz7b,askscience,new,2
Fenring,"In general, when current flows, the electrons that come out at one end aren't the same electrons that went in at the other, they're electrons that were already in the conductor and were pushed out. You can think of it like a tube completely packed full of marbles, but open on either end. At one end of the tube, you push another marble in, and it pushes the marbles in front of it, which push the marbles in front of them, down the line until it gets to the other end, where one marble falls out. None of the marbles were moving very quickly, yet the signal was transferred almost instantaneously.",null,3,cde5fzj,1qlyrr,askscience,new,20
zninjazero,"The term for it is ""electron cascade"" or ""electron avalanche"". When a high electric field develops between two points (the earth and the clouds for instance) the electric field can accelerate whatever floating electrons there are in the air to very high speeds. When the fast electron collides with an air molecule, it can dislodge another electron from it. Now you have 2 free electrons being accelerated down the electric field and they dislodge 2 more and so on. 

The result is the path of the electric field is now full of loose electrons and the ions they were dislodged from ; it is a gas made up of positive (ion) and negative (electron) particles, ie a plasma, which lightning is a type of. Electrons being as small as they are, they are accelerated very quickly, and the whole process is done and over with in a fraction of a second.

Drift velocity doesn't factor in,  because when the plasma is formed,  it is essentially a superconductor. ",null,0,cdeiqu7,1qlyrr,askscience,new,2
neuromorph,"Been a while since studying this.  Cloud motion and the accumulation of charge (electrets) will be the main source of the high voltage source, since they do not have a common ground with which to discharge.  

***(Side note:  I recall that lightning itself originates from the ground and moves up, and I cannot remember the explanation at this time.)***

However, the cause of the lightning bolt is that the charge build up on the cloud becomes high enough that exceeds the breakdown potential of air.  This is when a spark can form in air.  This is the same phenomena that occurs when you get a shock from walking in socks and discharging in door knobs and such (and this often happens in dry conditions).  

With clouds, the process is the same, but the voltage (potential difference) between the cloud and ground is much, much larger.  So instead of a small spark forming, the lightning bolt is formed, which is simply the discharge of electrons from the cloud to the nearest electrical ground (which in many cases is the true ground).

The speed of the discharge is fast, because of a cascading propagation of heat and ions.  Once a small discharge occurs, the breakdown potential of the surrounding air changes, as there are more ions present.  You get charge and heat that are not normally in air, changing the discharge conditions for the remaining electrons of the system.",null,0,cde9mwk,1qlyrr,askscience,new,1
Javi2639,"You will have no concept of the time of day because there is no sun. This will really mess with your natural circadian rhythm after a while. Also, you would probably suffer a vitamin D deficiency due to there being no light to catalyze the synthesis of it.",null,0,cde6ei7,1qlxwg,askscience,new,4
bonehead550123,"If you look at animal models, where the animal experienced long term light deprivation (around 6 weeks), there will be some neuronal degeneration. In these animals there is apoptosis of neurons involved with norepinephrine in the locus coeruleus, dopaminergic neurons in the ventral tegmental area, and 5-HT in the raphe nuclei.

",null,1,cdebh1q,1qlxwg,askscience,new,2
galinstan,"Can you clarify your question by describing the physical setup used in your alcohol burning experiment? Is it a flame applied to a sample in an open container?

Combustion requires fuel, an oxidizer, and heat. Also, the liquid alcohol does not burn directly; it must be vaporized first. It is harder to volatilize glycerol and ethylene glycol compared with the other alcohols you mentioned (e.g. vapour pressure and heat of vaporization.)

Glycerol can be made to burn in specialized burners that properly atomize the fluid into droplets, a process that greatly increases the surface area to volume ratio. This is challenging because of its high viscosity. The heating value of glycerol isn't really great compared with hydrocarbon fuels, and the possibility of forming toxic acrolein from incomplete combustion is certainly an issue to be considered!",null,0,cde6bea,1qlxjz,askscience,new,2
FatSquirrels,"They certainly will burn if you supply the correct heat and environment, but they will be more difficult to burn under ""normal"" circumstances.

The largest reason for this is that when you burn most liquid fuels you are actually igniting the vapors above the surface of the liquid, and not the liquid itself.  You need a lot of mixing with the air to get enough oxygen for the combustion to happen, and the bulk of the liquid simply does not have enough oxygen for the reaction to be self sustaining.

So when you are burning a fuel you ignite the vapor above the liquid, which heats up the remaining liquid and causes more to vaporize which then burns and the chain continues.  If the liquid is at too low of a temperature then not enough of it will be in the gas phase to create enough heat to sustain the reaction.

If you held a match above a container of glycerol you might see the flame get bigger, but once the match was burned the flame would die out.

If you want to compare actual numbers you can look at the flash points of all the compounds (which you can easily find in the sidebars on their respective wikipedia pages).  The flash point is the temperature the liquid needs to be at in order to sustain a fire with an outside ignition source (a spark or a match).  Just to give you an idea of the range of numbers, the flash points for the biggest alcohol you set on fire, [pentanol](http://en.wikipedia.org/wiki/1-Pentanol), is 49 C (assuming 1-pentanol) while [glycerol](http://en.wikipedia.org/wiki/Glycerol) is 160 C.

The other way you can more easily burn these hard-to-burn alcohols is by mixing them with more flammable compounds to get the whole thing up to temp using the more volatile component, and/or by aerosolizing the liquid so that you approximate the stuff being in the gas phase.",null,0,cde9rhb,1qlxjz,askscience,new,1
micromonas,"The fact that it is a combination of many factors (disease, diet, exposure to toxins, etc) is the reason why people say there is ""no firm answer.""

To summarize a few of the factors causing CCD: habitat destruction and herbicides reducing the population of wild flowers, so bees have to rely on artificial food sources like high-fructose corn syrup and agricultural crops, neither of which is a very healthy diet for bees. 

Speaking of agricultural crops, bees are constantly being exposed to pesticides, and there is new evidence that fungicides in combination with other pesticides are a lethal mix for bees. Additionally, a relatively new class of insecticides known as neonicotinoids are especially nasty to bees, and have been banned in Europe. 

But in the US there is a lot of resistance to the idea that new pesticides like neonicotinoids are causing CCD, mostly because modern agriculture depends on these chemicals. Additionally, neonicotinoids replaced other pesticides that are considered worse for the environment (like organophosphates) and nobody really wants to bring those back.

Furthermore, the modern system of agriculture that has bee hives traveling all around the country is stressful for hives. Some times of the year, like during winter or during the almond pollination season, all the bee hives in the US are in a few concentrated areas (like California's central valley). This facilitates the spread of disease and parasites, which find the weakened, stressed out bee hives to be easy targets.
 
In conclusion, CCD isn't caused by just one particular set of factors, it's many things, sometimes all at once. ",null,3,cde8vdw,1qlwki,askscience,new,5
ButtsexEurope,"We pretty much know why. A combination of pollution, pesticides, and colony collapse syndrome. The bees in China have all disappeared because of this. They need to use human pollinators now, which is great for job creation, not so great for the environment. 

There is no single answer, which is why people say it's a ""mystery"". ",null,2,cde9ahj,1qlwki,askscience,new,4
jray2212,"This is actually simple to answer.   Money is the problem.   The pesticides have already been outlawed in Europe Because they were hurting bee populations but the chemical companies pour money into government to keep them off their backs.   They are the ones that put out the propaganda that it isn't pesticides causing the problem.   There may be other issues too but the pesticide is the biggest.   They are having success in Europe with the bees now since outlawing these pesticides but I am afraid that until bees can buy more congress men than Dow chemical,  they are screwed. 
",null,3,cdef3n4,1qlwki,askscience,new,4
eekabomb,"these products contain some form of hyaluronic acid, which is active in skin repair. its degradation products increase cell migration to the area, which are involved in things like inflammation, keratinization, and forming the cellular matrix that makes up your skin.


if you want to get into the scientific/cellular level info you can [read the wiki](http://en.wikipedia.org/wiki/Hyaluronan) on hyaluranon or hyaluronic acid.",null,0,cdeouso,1qlw91,askscience,new,2
sporclesam,"Nope, not hyaluronic acid... but [Onions and Aloe] (http://en.wikipedia.org/wiki/Mederma) !! Majorly anti-inflammatory flavinoids. There is some [PEG] (http://en.wikipedia.org/wiki/Polyethylene_glycol) though. 
When it comes to scar reduction; your guess is as good as mine for these commercial formulations.

edit: eekabomb is right that many other products do contain Hyaluronan derivatives, but maybe not as primary ingredients.",null,1,cdh5q1c,1qlw91,askscience,new,1
Littlesheeps,"There are many theories of Abiogenesis, no theory holds consensus.

http://en.wikipedia.org/wiki/Abiogenesis",null,2,cde5cor,1qlv3l,askscience,new,3
vexing000,"amino acids come together to form proteins and other, bigger, ""macro"" molecules. proteins are a huge class of molecules, many of which are dynamic (actually move, change conformation). miller's experiment is pretty controversial but most people who accept evolution look at it as a pretty ground-breaking experiment. it doesn't prove evolution, but it's one of the pieces of evidence that we look at as the current accepted theory.",null,7,cde3gxb,1qlv3l,askscience,new,2
spPad,"Capillary action has nothing to do with it. Water is a liquid because of hydrogen bonds. Oxygen is so electronegative that it attracts the H atoms of neighboring H2O molecules. This force is sufficient to turn it into a liquid. 

Other, less electronegative atoms like sulphur do not have such a strong attraction between the molecules, which is why H2S is a gas at room temperature. ",null,0,cdencuj,1qluy0,askscience,new,1
baldeagleNL,"In the short term, I don't know if it does/has to, because space is virtually empty. The space just above the surface of the earth and other planets is quite full, so chances of a collision are high, but most matter in outer space is concentrated in rings around the sun. In between those, there is almost nothing, so the chance of a collision is very small.

The trajectory of the Voyager I is/was adjustable, but only for long-term changes, not for eminent collisions. The trajectory was carefully planned to avoid dangerous areas in the solar system.",null,4,cde3mrv,1qluvn,askscience,new,32
osterchrisi,"As far as I know chances of getting hit by a moving object in space smaller than the known ones (planets and their moons, large comets) are extremely small - even if you fly directly ""through"" the Kuiper Belt for example. I don't think that the Voyager 1/2 had any mechanisms or sensors to detect, let alone fuel to dodge moving objects like tiny asteroids and the such. Space is terribly empty.",null,1,cde3m1c,1qluvn,askscience,new,7
omg_wtf_n_stuff,Space is just mind bogglingly big. Objects in space are startlingly far apart. You should spend some time thinking about relative sizes of things in space -and their proximity to each other. It's a fun thought experiment and will leave you mind blown.,null,0,cdeed8o,1qluvn,askscience,new,3
CalzoniTheStag,"So space is big, really big. But understand that the typical ""space junk"" we hear about is mostly located around Earth and at the L-points. Voyager, obviously outside of the Earth's junk ring, doesn't have to worry too much about that. Plus, it never gets close enough to a planet to really interact with the space junk a given planet attracts. Large astroids usually have a defined trajectory that can be mapped using simulations and a lot of fun math, and I am sure Voyager can be steered clear of them in the event one is projected to come close. Though this probably wont happen since Voyager's thrusters are VERY low power and would take a long time to steer. And the chances of a large object hitting a fairly small object (Voyager) are VERY small. 

On a much grander scale, the chance of interacting with a celestial object given a random trajectory approaches zero very quickly. In interstellar space there is a vast amount of nothing, in intergalactic space there is an absurd amount of nothing. Theoretically, and mathematically, Voyager has a very high probability of never hitting anything large enough to skew its trajectory, even as Time approaches infinity. Additionally, as an object in space moves, the Universe expands around it, making the function approach zero a lot quicker. ",null,0,cdeg7rt,1qluvn,askscience,new,3
gorramcsn,"A device known as a ""[Whipple shield](https://en.wikipedia.org/wiki/Whipple_shield)"" is used on equipment such as the ISS, although I'm not sure about unmanned equipment such as space probes or satellites.

Some further reading about space debris can be found here:

https://en.wikipedia.org/wiki/Space_debris

http://orbitaldebris.jsc.nasa.gov/protect/shielding.html

http://orbitaldebris.jsc.nasa.gov/faqs.html",null,0,cdeqov0,1qluvn,askscience,new,3
SteamrollerAssault,"[Here](http://www.astrobio.net/index.php?option=com_retrospection&amp;id=4607&amp;task=detail) is an interesting article about whether or not NASA scientists were going to send *Pioneer 11* through what was then a hypothetical ring of Saturn. The probability of that spacecraft being destroyed was unknown, and its ultimate fate had a direct affect on the flight path of the *Voyager* spacecraft. ",null,1,cdeddtj,1qluvn,askscience,new,3
empath75,"Walter Lewin explains it pretty well here:

http://www.youtube.com/watch?v=9501V-D-SM4

The rod attracts lightning mostly because the charge density at the tips of pointed objects is higher, so anything with a sharp point (a tree for example) can attract lightning.  

One interesting thing about lightning is that each strike is actually multiple discharges, in both directions, which is why there's a strobe like effect.  ",null,0,cde3xc6,1qluif,askscience,new,2
Sannish,"&gt;By my current understanding, lightning is the exact same effect as when you get a static shock on your fingertip, it's just much MUCH larger.

Lightning is a different process from a static shock.  A static shock is the breakdown of air between the two electrodes (your finger and a door knob for example); lightning is a stepped-leader discharge process that has multiple steps.

Once we get the large voltage difference between a cloud and ground (about 1-10 MV) the air starts to break down in 100 meter jumps.  These jumps are somewhat random and resemble glass fracturing.  Eventually (but not always) one of the tips of discharge will reach an oppositely charged region (another cloud or ground) and then the whole thing discharges to produce lightning.

&gt;What specific atmospheric effects need to be in play in order to produce ground-strike lighting?
&gt;Aside from large metal rods, are there any other ways to 'attract' a bolt? Would it be possible to change ground-level magnetics or air-pressure, or other factors in order to try and simulate the effect of a Lightning Rod without the use of an actual rod?

There may be local ground features that can change the probability of lightning discharging.  For example a thundercloud over the ocean tends to develop a large voltage difference before producing lightning than a thunderstorm over land.  But the exact reason for this is not known.

The only system that is better at attracting lightning are rockets that leave an ionized trail (or a wire) behind them as they go into a thundercloud.  But for everyday protection a lightning rod will always be more cost effective than constantly launching rockets.

&gt; Are there any noteworthy exceptions as to when lightning might strike? Has it even been known to be produced in otherwise calm weather conditions, or is it only ever seen during heavy cloud-cover and storms?

Lightning can strike between a cloud and roughly anywhere ~10 km away.  If you can see/hear lightning you are in a location that can be hit.

Also lightning can form in volcanic ash plumes, but I don't think the lightning is a safety priority in that situation.",null,0,cdebb8a,1qluif,askscience,new,2
tkolstee,"There are a few basic ways to counter viruses and other malware.

The most common is through the use of signatures, as mentioned by others. A signature isn't an intentional label but a small section of code that can be used to find a match. If I was looking through random data for a copy of a particular book, I might pick a sentence that appears in the book but isn't likely to appear in others as a ""signature"" to search for that book. Antivirus vendors spend a lot of time researching, finding new viruses, and picking them apart to look for behavior and signatures. As mentioned, some viruses make modifications to themselves such as encrypting themselves, so that a single static signature won't work to detect them.

Another way to detect virus activity is by suspicious behavior. The boot sector of your disk is an area that exists outside of any files, or even the filesystem that's used to store the files. It's an area that's generally only looked at when you first boot your computer to get instructions on how to boot. If the antivirus program can observe some piece of running code trying to modify the boot sector, that's an example of suspicious behavior. There are many methods to rule out false positives, such as you legitimately modifying the boot sector on your drive during a software update or when making a new partition on your disk. There are also techniques to identify when code is modifying itself to defeat signature detection.

Antivirus software can do more than identify suspicious behavior as it's happening, but be proactive about checking code for suspicious behavior. Some antivirus vendors check files by executing them in a ""sandbox"", a protected environment where the code can run but not affect anything. Not only does this code have to be prevented from writing to disk, unprotected memory, etc., but execution limits are set so it can only use a given amount of the system's resources before being cut off. If these limits were not in place, a virus could put the computer in an endless loop and use up all of your CPU even inside the sandbox.",null,5,cde7g2g,1qltef,askscience,new,36
null,null,null,3,cde3qi9,1qltef,askscience,new,16
ebbv,"Viruses do exist which change their ""shape"" and make them harder to spot, this technique was created a long time ago and is called polymorphism. But in practice there's still a limited number of forms the code can take and still accomplish the same task. Eventually once the virus is identified, it's possible to know all of its possible forms (or some part of the code that doesn't change) and spot it. Sometimes this is not easy but there's always something you can do to detect it.",null,1,cde5oos,1qltef,askscience,new,4
Sarks,"The way I was taught it was that viruses have signatures. Say, there was a virus that copied whatever you typed, and the code for that virus was 00110011 (it'll be much longer than that, but you get the idea). The anti-virus software can scan your computer, and if it finds a string of code that looks like that, it will flag it as a virus. This can cause problems if you have a piece of software that has that code in it and isn't a virus, but you can get around it by telling the AV that that program is safe.

As for why they don't make original viruses, they do. And they cause problems. But AV software uses a sort of library of virus signatures, and whenever a new virus is found its added to the library. Thats why AV software updates so often, and why you should keep it up to date.

Plus, longer more complex code is a) harder to write, b) harder to make work and c) harder to update. If you want to take a virus for Vista, and make it work on XP or Windows 7 (just an example), it would be easier if the virus was smaller. That holds true for all code, not just viruses.",null,1,cde58qh,1qltef,askscience,new,2
1nf,"There are a couple different ways that anti-malware programs work.

The more traditional way is using signatures. These are some unique code that happens to appear in a program. For e.g. maybe the virus makes a message appears on screen and that message happens to have a particular error e.g. ""BadVirys"" could be its name. The anti-virus would try to find if that particular line appears in the code. Or, maybe it does something in a particular way, using a particular bit of code. Again, the anti-virus would try to look for that pattern.

But then, virus writers got around that by making their code change on its own i.e. a polymorphic virus.

For e.g. assume a virus does these things:

- copy itself to C:\Windows\System32
- write some entry in the registry at location A
- infect some process using some technique X

After morphing, it could:

- write some entry to registry at location B
- infect some process using technique Y
- copy itself to C:\Windows\SomeRandomFolder

It basically does the same thing but it looks different. The patterns that your AV was looking for are no longer there, which makes things more complex for the detection software. The virus could even use encryption to make its code unreadable until it decides to decrypt it. In that case, AV software would look for the part that does the decryption. That part could be changed too, of course through polymorphism.

Another way that anti-virus work is through behavior analysis. Viruses (or virii, whatever you prefer), tend to do a similar set of things. They will write files in locations that programs normally don't, or execute from places that isn't normal e.g. Temp folder. Other times, they will try to read other programs' memory locations. All these things are suspicious behavior and the anti-virus will look for those.

Polymorphic viruses tend to behave in different ways and anti-virus software makes use of the cloud. The installed AV clients communicate with their vendor's server to share bits of behaviors that they analyse. For e.g. they share ""normal behavior"" patterns of programs as well as ""malicious behaviors"". This data is shared among all the installed AV software for that vendor, on the different computers across the world. It's as if the AV software are studying together and sharing their findings. If some AV has detected a suspicious behavior, it can consult the ""cloud knowledge base"" to determine if some other AV somewhere has ever seen similar behavior and what action it took.

Finally, AV software can run a program they suspect to be a virus in a ""sandbox"" which is a protected space. The program believes it has full access to disk etc, when in fact, it is running in an ""enclosed"" space (sometimes called a jail), while the AV software monitors its behavior. If it does something bad, the AV knows the program is malicious and terminates it, otherwise it is allowed to execute outside the sandbox. However, some viruses are clever enough to know when they're running in a sandbox and will try to behave as normally as possible, and only do bad things when they are allowed free access for e.g. they might decide not to decrypt their malicious portions.",null,1,cdeb6r8,1qltef,askscience,new,2
OrbitalPete,"Pangea was not the original land mass - Pangea is only the most recent of at least 6 separate supercontinets which have formed.

http://en.wikipedia.org/wiki/Supercontinent

Basically, we've had continental crust on the planet for at least 4 billion years. Plate tectonics by its very nature recycles oceanic crust by producing at ridges, and subducting at trenches. Those subduction zones scrape off the silica rich sediments, and generate evolved silicic volcanic arcs. That is one of the ways we know to form continental crust. Subduction basically accretes lots of these arcs together to form bigger continental massifs.  Because subduction tends to occur at ocean-continent margins (oceanic crust is dense and can sink into the mantle, whereas continental crust cannot), it will generally tend - over time - to accrete the continent on one side of an ocean basin onto the continental mass on the other side of the ocean basin as that ocean basin gets subducted and closed.",null,0,cde3734,1qlsea,askscience,new,7
snowontherocks,"I believe that what you are looking for is someone to explain the original formation of crustal bedrock.  Unfortunately, little is known about it.  Continental crust is more silica-rich than oceanic, suggesting some sort of fractional process.  I do, however, know a little more about continents splitting apart and coming back together.  


Think of a large land mass like a blanket over a heat lamp.  The blanket becomes very concentrated in heat under the blanket, and eventually burns along these super-heated spots.  Same thing (we think) is what happens to supercontinents, and is called the Wilson Cycle.  The mantle becomes very hot under all that thick continental crust, and rifts apart to release that energy.  The new continents keep drifting apart.  As they are drifting apart, sediment from the land mass itself is being accumulated on the 'passive margins' of the continent.  When this sediment is heavy enough, it starts to subduct under the continent itself.  This draws the continents closer together, until they eventually collide again.  This cycle seems to take about 500 million years.  Places we can see these steps are rifting in East Africa, passive margin accumulation on the east coast of North America, subduction on the west coast of South America, and collision between Europe and Africa.  I highly suggest looking up maps of all these areas and watching projections of what the Earth has and will look like on youtube.  I find it all very fascinating, and suggest taking a global tectonics class if you ever get the chance.",null,0,cdg6rur,1qlsea,askscience,new,2
endocytosis,"If you drink coffee (most of us do), and ever added cream or milk but *not* stirred it, and watched it slowly dissolve into the coffee, you're observing a process known as diffusion.  Substances with a higher concentration tend to diffuse to a lower concentration until both substances are balanced, in this case, the coffee is a lighter shade and the milk is mixed.

The cells that line your intestines are focused on maintaining a proper concentration of water + salts in the fluid on the interior of your intestines, and also absorbing as much of the water and nutrients from the digested food as possible.  Normally once most of the nutrients and water are absorbed, it passes into the large intestines, and then…well, you get the idea.

As illiae mentioned, diarrhea can be caused from a number of reasons.  The simplest to explain is if you take a laxative such as [sorbitol](http://en.wikipedia.org/wiki/Sorbitol).  Sorbitol passes through your digestive tract largely unmetabolized, but when it is in the small and large intestine, it accumulates, and there is now a higher concentration of it compared to the interior of the cells (termed the osmotic gradient).  This causes the cells to put water *into* the interior of the intestines, instead of drawing water out, and thereby causing softening of stools, or in extreme cases diarrhea.

Diseases like *V. Cholera* can cause severe, life-threatening diarrhea by attacking the cells lining the intestines themselves, and hijacking the cells using a special [toxin](http://en.wikipedia.org/wiki/Cholera_toxin) into releasing Cl-, Na+, HCO3-, and K+ ions, and thereby massive amounts of water to compensate.  If a person is sick with *V. Cholera* their life is in danger due to the massive loss of water and electrolytes in the diarrhea-giving them water will actually make their condition worse, they need to replenish salts somehow, like with gatorade.",null,0,cdee7cb,1qlr31,askscience,new,5
illiae,"Diarrhea can be caused by a number of things, from bacteria to viruses to tons of other things, such as illness or medication. Physiologically, after passing through the stomach, water and nutrients are absorbed from your partially digested food via the intestines to help solidify it into stool. Generally, diarrhea occurs when the intestines are compromised and cannot properly absorb water, such as with inflammation or damage, or when the still-digesting food passes through too quickly so that there isn't time to absorb enough water. Water, blood, mucus, etc can also leak into the forming stool and add moisture, causing diarrhea.",null,0,cde5cmb,1qlr31,askscience,new,3
medikit,"To summarize there are four different broad mechanisms of diarrhea: (More depending on the level of detail you require but let's make it four for the sake of simplicity)

1. **Increased motility**: ie stimulant laxatives like caffeine, Senna, and Bisacodyl can cause this. So can opiate withdrawal.

2. **Increased secretion**: ie Cholera (kills through dehydration), Enterotoxigenic E. coli (ETEC). Some stimulant laxatives can increase secretion (This is one mechanism by which stimulant laxatives work). Senna and Bisacodyl increase secretion. Researching this topic I was surprised to learn that most GI viruses do not cause secretory or inflammatory diarrhea.

3. **Malabsorption**: Often Osmotic: Unabsorbale molecules in the intestines cannot be broken down and fluid is pulled toward the lumen by an osmotic gradient. (ie sorbitol (found in prune juice), lactulose, miralax, lactose intolerance). Also inability to absorb due to ""short gut"" (the length of the digestive track is too small to absorb enough fluid) or due to Giardia which physically blocks absorption in the GI tract. Enteropathogenic E. coli (EPEC), Rotavirus, Norovirus, Astrovirus and Celiac disease can also results in loss of absorptive surface area.

4. **Inflammation**: Infections with Invasive E. coli (EIEC/EHEC), C. diff, Shigella, Salmonella, Campylobacter, Yersinia, Amoeba, or diseases such as Crohn's disease or Ulcerative Colitis. Leakage of blood and pus can cause diarrhea as well, this is sometimes classified as ""exudative diarrhea"" but since they are part of an inflammatory process I will include exudative here.


Here is a great resource for mechanisms of infectious diarrhea (one of my interests): http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3035144/",null,0,cdf5ph3,1qlr31,askscience,new,1
Fenring,"&gt;Is the force between the bodies still described by Gm1m2/r^(2),

For the purposes of solar orbital mechanics, yes.

&gt;how do the guys at NASA account for decreasing radius as the two bodies approach?

By simulating the whole system. It's fairly straightforward to run a detailed simulation that tracks the position and velocity of each body at any moment in time, then updates them according to the current total force of gravity on that object. The simulations run by NASA are extremely accurate, which is how they're able to plan missions to other planets.",null,0,cde5tey,1qlr22,askscience,new,1
Fenring,"The difference isn't in the radiation itself, but in how it interacts with the walls and door. Visible light is absorbed or reflected when it encounters most solid objects, while the Wi-Fi signal passes through them, much like how visible light passes through glass.",null,3,cde5dvs,1qlqdf,askscience,new,5
chrisbaird,"Any given material is transparent to certain frequencies of electromagnetic waves and opaque to other frequencies of EM waves. Exactly at what frequencies transparency and opacity occur is material-dependent and arises from the interplay of many effects. Fundamentally, you can think of the degree of transparency of a certain material to a given frequency of EM waves as depending on the ability of electrons/molecules in the material to resonant in response to the EM waves, which depends on the strength of the atomic/molecular bonds that the electrons participate in.

Wood and plaster walls are mostly transparent to radio and microwave frequencies, such as used by WIFI. Thick metal walls are not.",null,0,cde9i52,1qlqdf,askscience,new,1
garycarroll,"No more than once.
(sorry, couldn't resist.)

Seriously, the answer will vary hugely depending on environment. You say a few steps down from the top of the food chain, implying the presence of at least a few predators. Predators look for weak prey, and so as the prey gets feeble, it's likely to get picked off. In a sense, this is dying of old age, but I think you mean ""left alone to die with no interference."" This would depend on lots of things, including the ratio of predators to prey, which is highly variable. 
Also, as an animal gets weaker it's more susceptible to sickness, or starvation from competition. The starvation is more likely in areas with very few predators which allow population to grow to the very limit the environment can feed. Thus, more predators increases the chance of being killed and eaten, while very few predators mean great competition for limited food, and increased likelihood of dying from starvation. A higher population may also increase incidence of disease transmission.

Dying from simply being worn out, without being starved, sick, or killed by predator or competitor is ""rare"", but that's a relative term.",null,0,cde49cn,1qlof7,askscience,new,33
justin3003,"As you can see from the picture, the condition is called ""sarcoma cranii,"" which is a type of osteosarcoma. By definition, sarcomas are cancers of what's called ""mesenchymal"" origin. Mesenchymal cells are stem cells that can differentiate into, among other things, osteoblasts. Osteoblasts are the cells responsible for bone growth. Thus, this condition causes uncontrolled bone growth. So what is seen on that skull is the result of abnormal bone growth rather than any erosion pattern.

Insofar as stopping when it hits something, I would doubt it. Cancers do not normally avoid structures in their way. Whatever tissues/structures the growth encounters will be either impinged by the growth or invaded directly.",null,0,cdeaajz,1qlo5n,askscience,new,6
JohnSmith1800,"The umbilical cord connect the foetus to the placenta, which in turn is attached to the uterine wall. At the placenta the foetal blood circuit runs in a series of villi (fingerlike projections) into a pool of maternal blood which has leaked into cavities in the placenta. The barrier between the two is extremely thin, sufficient to stop RBC's and other cells from crossing over, but allowing the transfer of nutrients from the mother into the foetal blood supply, and metabolic byproducts in the other direction.

By birth the organs of the foetus are all fully functional, and I believe that most of the metabolic wastes are being excreted by the newborn into the amniotic fluid, the exception being CO2. (I can't find a source for the functionality of the liver/kidneys of the foetus in my textbook, but I am confident it's correct.).

By the time the baby is born therefore, the placenta is only providing oxygen and nutrients and the newborn has a fully functioning liver and kidneys to deal with waste products/blood filtration. The lungs however are a slightly harder matter. They're filled with fluid, which holds them tightly shut and the first breath to force them open requires a lot of effort on the part of the newborn. Interestingly, from the time of the first breath until the day you die, your lungs are never again completely empty of air, there's always a residual volume which you can't force out no matter how hard you try.

Sources: L. Sherwood, ""Human Physiology: From Cells to Systems"" 8th Edn. / Second year physiology student.",null,1,cde5yy7,1qlln1,askscience,new,3
bellcrank,"In particularly cold and dry conditions, you suffer additional heat loss through exposed skin from evaporative cooling and convective transfer.  Wind chill is an attempt to account for that effect by providing an equivalent ""what it feels like"" temperature, in an ideal scenario where those effects are not present.  Heat-index in the summer works in a similar way - on very hot, humid days, you derive very little relief from the breeze because you aren't getting the benefit of heat loss through these processes, and it ""feels"" warmer.",null,2,cde5e5q,1qllgd,askscience,new,6
cmuadamson,"They all have exactly the same mass and exactly the same electric charge. There are other properties within them at the quantum level that are all the same, like the value of their spin and their lepton number, and some properties such as the direction of their ""spin"" that can take on one of a few values.  
Someone else can add up the combinations of the quantum numbers, but you end up a few possible electron states.  If two electrons have the same quantum numbers, and I hold them behind my back, mix them up, and then show them to you, there is no distinction which was which.

This does not include their position in the universe (which can be anything) and their energy level (which can be nearly anything).  

Are they the same in elements?  This is what gives rise to the periodic table. By the Pauli Exlusion Principle, two can't occupy the same position if all of their quantum numbers (like direction of spin) are the same. So the first electron in a hydrogen atom can be what it wants, but the 2 electrons in stable Helium must have opposite spin. Getting up into higher elements, the ""shells"" of electrons and how many electrons can be in each shell are governed by those possible combinations of the quantum numbers. If the topmost shell is not completely filled for an element, there can be differences of which electron states are filled in different atoms of the same element. Because the shells are filled in the noble gases like Helium and Neon, the electrons will have the same states if the atom is stable (and not in some transient ionization state)

",null,1,cde6yyy,1qllci,askscience,new,4
cloudofawesome,"Put simply, no. 

Newton's law of universal gravitation states that every point mass in the universe attracts every other point mass with a force that is directly proportional to the product of their masses and inversely proportional to the square of the distance between them. In other words, every single source of gravity, or mass, in the universe has a gravitational influence on every other source. The atoms in your body, and some star 10 billion light years away do indeed have a gravitational influence on each other. The reason you don't feel this is due to the vast distance in between.

So basically, there is no point in space where one could not be affected by any sort of gravitational force.",null,3,cde0wx3,1qlij6,askscience,new,9
TangentialThreat,"There is such a thing as a [Lagrangian point](http://en.wikipedia.org/wiki/Lagrangian_point). Any system with one body orbiting another body will have exactly five nearby points where the net gravitational force is zero, and two of them (the Trojan points, L4 and L5) are stable.

The solar system is not composed of two perfectly round planets, but still. The SOHO observatory uses the Earth/Sun L1 and people have talked about building permanent stations in L4 or L5.

",null,0,cde2oqg,1qlij6,askscience,new,3
super-zap,"&gt; pushed away by centrifugal force.

There is no such force acting on the planets.
There is only gravity, which pulls the planets (roughly) perpendicular to their direction of motion.
This in turn makes them change direction, but the force of gravity stays (roughly) perpendicular throughout the change in direction.

http://en.wikipedia.org/wiki/Circular_motion",null,1,cde2a03,1qlij6,askscience,new,2
Gargatua13013,"There is no shortage of mantle rocks exposed at the surface in Ophiolitic complexes.

The rocks are mainly ultramafic (meaning low silica and high Fe and Mg content), with the main types beeing peridotite, dunite, and hartzburgite. Because of their high Fe and Mg content and mineralogy (olivine, clino- and ortho- pyroxene, oxydes), the rocks are dark green. At the surface they quickly weather to medium brown (hence the term ""*dun*"" in ""Dunite"")

From these rocks, we see a lot of evidence of subhorizontal crystal stretching, which gives them a somewhat ribboned look, although some layering is also seen, at times, due to magmatic segregation (less dense mineral float to the top of a layer, dense ones sink to the bottom). Examples: http://origin-ars.els-cdn.com/content/image/1-s2.0-S0009254111002853-gr2.jpg

At the surface, they are commonly crossed by veinlets of asbestos. These are not really present *in-situ* in the mantle, but form through mechanical breakage and chemical reaction with hot seawater while the rocks are brought up to the surface.",null,0,cde4ums,1qlgxu,askscience,new,2
OrbitalPete,"In this exact location in space in what reference frame? Relative to the centre of the solar system? The sun? a given star or galaxy elsewhere in the universe?  The problem is that because the universe is constantly expanding (by which we mean the entire universe is expanding a bit like a stretching piece of elastic), there is no privileged reference frame. The earth is moving at huge velocities relative to the sun, the centre of the galaxy, or indeed any other celestial object you want to pick. 

So, for example, *just* considering the velocity of the earth around the sun (29.8 km/s) if you were standing on the ground as the sun set, and jumped 1 second into the future, you would reappear 29.8 km underground, leading to a very messy mystery for any passing future geologist. Doing that as the sun rose would put you 29.8 km in the air, at which point you suffocate, die, and your frozen body hits the ground below several minutes later.",null,1,cde29cv,1qlgib,askscience,new,4
MayContainNugat,"There really isn't anything such as ""this exact location in space."" Space doesn't have markers on it delineating one part of it from another. Consider a passenger in a car driving down a freeway at 70 MPH: Anyone in the car would say that person is remaining in his exact location in space defined by ""the front passenger seat"" and be perfectly correct about it. An observer on the side of the road might disagree, and also be correct.

There is no absolute frame of rest with which to answer this question; no special absolutely resting observer who could make such an observation.",null,1,cde2dl3,1qlgib,askscience,new,4
defloof,"Well I know that to properly kick down a door you want to kick right where the bolt would be going into the wall, since you're trying to break through the frame more than the door itself.  The whole bolt mechanism is pretty securely in the door, but only an inch or so in the frame normally. 

 If there were multiple locks you wouldn't be able to isolate your energy on that one point and there would be a lot less force going against the second bolt.  They're sharing the load anyway, which would make it harder to get through.

Basically yes, using two locks in multiple locations will make the door harder to get open with force.  Someone who can pick a lock will only be slowed down by a few seconds though probably.",null,1,cddyru5,1ql9jt,askscience,new,8
I_Luv_Country_Chicks,"Short answer, yes.  The more surface area in contact, the less pressure on any one point.  Also, in the mechanism of the lock, there's a maximum reverse tolerance.  With two or more locks, that force is divided.

The placement of top and bottom will share the stress put on anything that is fixed to the doorjam (hinges and the female side of the lock).  The wood (doorjam) would probably give before the lock mechanism.",null,1,cddynpc,1ql9jt,askscience,new,3
TangentialThreat,"If you look closely at bank vault doors you will notice that it locks in many points around the door.

Presumably the guys designing bank vaults know what they are doing. For a wooden door there's a limit though; the door and the frame that contains it are only so strong and it will never be battering ram proof.",null,0,cde06c6,1ql9jt,askscience,new,1
benjammin17,"Ethanol (the fun stuff) is oxidized once by [alcohol dehydrogenase](http://en.wikipedia.org/wiki/Alcohol_dehydrogenase) to [ethanal](http://en.wikipedia.org/wiki/Ethanal). This is the molecule which gives us the wonderful symptoms of a hangover. 
[Aldehyde dehydrogenase](http://en.wikipedia.org/wiki/Aldehyde_dehydrogenase) then oxidizes ethanal to acetic acid, which is relatively harmless in the body. So the usefulness of the oxidation is making a relatively toxic compound less harmful to the body.",null,0,cdeaq7f,1ql97w,askscience,new,2
OrbitalPete,"Firstly, Pangea was only the [most recent](http://en.wikipedia.org/wiki/Supercontinent) of at least 6 supercontinents that have existed in Earth's history. It formed about 300 million years ago, and started breaking up about 175 million years ago.

Secondly, as far as mass of the planet is concerned, the vast, vast majority is contained in the mantle and core, which are 2-6 times more dense. The crust makes up about 0.9% of the earth's volume, and just 0.5% of the earth's mass. Remember, the tectonic plates are up to *maybe* 100 km thick at their absolute maximum (35 km average for continental), whereas the earth is 12,742 kilometers across

Thirdly, the crust is lying buoyantly on the mantle, which deforms below it. What that means is that while continental crust is less dense and thicker than oceanic crust, the load on the underlying mantle is around about [the same](http://en.wikipedia.org/wiki/File:Airy_Isostasy.jpg).  

All this acts to make the effect on centre of mass negligible. ",null,0,cde1h04,1ql95u,askscience,new,5
iorgfeflkd,"There are about 100 unique elements, so there are about 100 kinds of single atom. But when you have two atoms, there are 10,000 possibilities. Three atoms, there are a million. When you start getting into many-atom compounds, the number of possibilities increases exponentially.",null,1,cddwsyl,1ql8co,askscience,new,5
RetraRoyale,"The electrons around an atom are arranged in different configurations when you look at different atoms. These differences in configurations account for most of the effects you see in molecules: different bond types, different spacial arrangements, different energy barriers... And then there're thermal effects too -- the behavior of an atom or molecule depends on its speed and what environment it's in.

So even a simple molecule with 2 arbitrary atoms can have a vast range of behaviors under a huge variety of different conditions. Plus, you can have any number of atoms in a molecule.

---

On the other hand, it's important to realize that most of the 'differences' between substances are exaggerated by our brains. There is a full spectrum of light, temperature, and configurations, but it's still just the same physics. Without brains to distinguish between configurations, it's all just the same natural laws acting everywhere. (But that's really a matter of philosophical discourse, not a scientific matter.)",null,0,cddyzhk,1ql8co,askscience,new,4
jlynec,"The best answer I've been able to get regarding this is: # of protons determine the element (1 = hydrogen, 2 = helium, ... h, # of neutrons determines radioactivity (isotopes), and # of electrons determines the properties of said element (when fluorine, a deadly gas loses an electron, it becomes fluoride, which helps keep teeth cavity free).

There are parts of this I have never found an answer to:

1. What determines whether an element is solid, liquid, or gas in ""normal"" conditions?

2. How does the # of electrons determine an element's properties (alkali metal, halogen, ...)

3. How does adding or subtracting a proton change the element into something completely different? You could take a slice out of an apple pie, but it's still apple pie!

4. 3, but with electrons.

5. Why does the ""island of stability"" exist? How does adding neutrons suddenly make them decay?

6. Why do the electron energy levels and sub-levels exist?

7. How can the same 3 things (e-, p+, n0) make soooooo many different elements?

8. How can 2 elements combine to make something different? (H + O makes water, Na + Cl makes salt, ...)

I'm sure there's more... I would absolutely love to get answers to any of these. This has been bugging me since high school!

EDIT: one of the points was HUGE, added one",null,0,cddzfxm,1ql8co,askscience,new,1
snipawolf,"Emergent Properties.

Simple interactions can interact with each other to form complex systems. 

Biology can be viewed as an emergent property of the laws of chemistry which, in turn, can be viewed as an emergent property of particle physics. Similarly, psychology could be understood as an emergent property of neurobiological dynamics, and free-market theories understand economy as an emergent feature of psychology.",null,0,cde07ys,1ql8co,askscience,new,1
tonberry2,"It is because matter is made out of fermions. Fermions are defined by their inability to share the exact same quantum state with other fermions (the Pauli exclusion principle). So when you place a bunch of fermions together in an atom, molecule, or extended solid, depending on how many fermions you have, you will have different quantum states occupied for each piece of matter and these pieces can then interact with other pieces of matter which may have the same or different available quantum states to add or subtract fermions from for an endless variety of combinations.

Contrast this situation with the bosons (for example photons, or light particles). Bosons can exchange quantum states with one another. The result is we don't see the same rich differentiated variety of things when light particles interact with each other (in fact it all appears to be blurred together as one thing).

EDIT: Wow, this got downvoted? Everything that was said above about atomic structure by others can be compressed down to the simple fact that protons, neutrons, and electrons are composed of fermions. Here are some sources saying the same thing if this idea seems unfamiliar: 

Statement:

http://nonlocal.com/hbar/bosonfermion.html

A more detailed article:

http://quantumwavepublishing.com/fermions-atoms-and-neutron-stars/",null,3,cde1zbx,1ql8co,askscience,new,2
hothead_bob,"Do they have to be 4x4 right out? You could cut 20 2x2 pieces, and put 4 of each together to make 5 4x4, otherwise I don't see how it could be possible. Neither 8.5 nor 11 are wide enough for 3 4x4 pieces, which is what you'd need. ",null,0,cddw9b0,1ql4h9,askscience,new,7
iorgfeflkd,"Temperature has effectively zero effect on radioactive decay rates, until it starts to get hot enough for nuclear reactions to occur.",null,0,cddwb97,1ql11j,askscience,new,2
sto-ifics42,"It depends on how precise you want to be about ""in the same location,"" but it'll ~~at least take centuries.~~ definitely be long after the Sun dies. 

For every 1 Earth year, each planet completes this many orbits:

* Mercury: 4.152

* Venus: 1.626

* Earth: 1

* Mars: 0.532

* Jupiter: 0.0843

* Saturn: 0.0339

* Uranus: 0.0112

* Neptune: 0.00607

To find out how many years until they're all in the same place again, ""simply"" find a whole number *X* such that when each of the above values is multiplied by *X*, they are approximately whole numbers. *X* would then be the number of years until alignment. (... I think. Could be wrong, feel free to correct me.)

EDIT: fixed estimate to match actual math",null,0,cddxkbq,1ql0rx,askscience,new,9
FullSizedForks,"This is extremely difficult to know with certainty.  The problem is that natural interactions and perturbations in the orbits of the planets cause their orbits, and thus their orbital periods, to change over time.  Remember that the solar system contains an untold number of bodies in various orbits around the sun and around each other.  These include the planets, their moons, an unknown number of comets and astroids, and other undiscovered bodies that continuously interact gravitationally with the rest of the solar system.  Simulating and predicting the paths of every single known *and unknown* body in the solar system is mind-bogglingly complex and prone to errors, and that's just considering short-term prediction.  Over the long-term, small differences and variations in the orbits of the planets and everything else in the solar system would start to mix things up beyond any methods of forecasting.


Again, these differences would be minute over short time scales (i.e., a few thousand Earth-years), but given tens-of-thousands of orbits (which could amount to millions of Earth-years for the outer planets), the changes would become increasingly difficult to predict with reasonable accuracy.  [The wiki article on the stability of the Solar System](http://en.wikipedia.org/wiki/Stability_of_the_Solar_System#Predictability) explains this in more detail, but the general idea is that the orbits and positions of the planets and everything else in the solar system become incredibly difficult to extrapolate over timescales exceeding 2 million Earth-years. 

[This response](http://www.reddit.com/r/askscience/comments/1ql0rx/how_long_would_it_take_for_all_the_planets_in_our/cddxkbq) from below is logical if we assume the orbital periods will remain constant over a short time.  However, if you do the math based on the info sto-ifics42 provided, you get a rough estimate that the current positions of Neptune, Uranus, Saturn, and Jupiter, with respect to Earth, will reoccur approximately every 5 million Earth-years.  Given that this is more than double the limit of 2 million years (in terms of reasonable hopes for accuracy), it's reasonable to conclude that there's no way to know for sure when or if the current (or any particular previous) positions of the planets will ever reoccur exactly as before.  There's just far too much variation and complexity to be even reasonably sure.

**TL;DR:  The solar system is huge and complex, orbits change, and thus accurate prediction becomes nearly impossible over the time scales required to create a reoccurrence of previous positions of the planets.**  ...and that actually kind of makes me sad.",null,0,cddz9di,1ql0rx,askscience,new,5
Allen_Maxwell,"I think the questions being asked has been addressed, but I would like to add the elements of universal expansion, perturbation, local group motion and position within the Milky Way and I will say that statistically speaking we will never be in the same position again.

Also, depending on your definition of planet, it might make the math above a bit more complicated. Dwarf planets and minor planets might not be considered in our definition, but they are wanderers by nature.",null,0,cdetlt6,1ql0rx,askscience,new,2
janvandersan,"I'm having trouble explaining in simple terms but here are two reasons why things become more fragile at colder temps: 1, as temperature decreases, dislocation mobility decreases because there is less thermal energy to help dislocations jump from one stable position to the next.  2, Young's modulus increases because bond lengths get shorter.

Young's modulus is basically the stiffness of a material.  Stiffer materials crack more easily than materials that can deform.  Less dislocation mobility means decreased malleability/ductility (increased brittleness).  If you're interested, have a look at the wikipedia [article](http://en.wikipedia.org/wiki/Dislocation) for dislocations.

Sorry if this is hard to understand.",null,1,cddvzaz,1qkx7r,askscience,new,5
null,null,null,0,cddxoz6,1qkx7r,askscience,new,2
solid95,"Cloth is a solid therefore carbon fiber is a solid. If you are referring to its stiffness, that comes from the resin. ",null,0,cdduk1b,1qkvwc,askscience,new,2
monkeydonut13,Is cloth not a solid anyway? ,null,0,cddvweh,1qkvwc,askscience,new,1
iorgfeflkd,http://xkcd.com/895/,null,1,cddtsgi,1qkvt5,askscience,new,14
humanino,"There are obviously flaws to such a simple analogy. For one thing, general relativity uses a *spacetime* curvature tensor, while the analogy merely uses a spatial curvature.

However, curvature is not an obvious concept to students who meet it for the first time, especially those who have not undergone the ""classical"" geometry lessons on curves and surfaces. From this perspective, there is some value in such a simple analogy. Remember that the masses in the analogy are supposed to live a 2-dimensional world. There is no ""downwards"" for them, **we** can only perceive ""downwards"" because **we are outside** the membrane. The 3rd dimension helps us visualize the curvature, the important point being that geodesics (shortest trajectories) are not ""straight"" in the usual sense. If we could embed 4 dimensional spacetime in 5 dimensions, we could visualize spacetime curvature.",null,0,cddweaw,1qkvt5,askscience,new,3
drzowie,"The membrane analogy is useful because the stretched membrane curves (in 2-D) in a similar way to how spacetime curves (in 4-D) in the vicinity of a star:  circumferences of concentric circles centered on the star are slightly smaller than you might otherwise expect based on the radial distance to the star (i.e. *pi* is not a constant!!!).  In order to be curved that way and be embedded inside a Euclidean space (like the space inside your room), the planar membrane has to pop out sideways.  But there's no Euclidean space into which spacetime is embedded.  

The ""curvature"" of curved spacetime just tells you that the points in spacetime are hooked up in a funny way that is different from how points are hooked up in a Euclidean space.  It doesn't imply that there's a 5th dimension into which the whole shebang is warped.",null,0,cddyw8n,1qkvt5,askscience,new,2
iorgfeflkd,"At what level? Here's a paper that covers the basics: http://www.uvm.edu/~pdodds/files/papers/others/1992/jaeger1992a.pdf

For more detail you'll want a textbook.",null,0,cddv7nh,1qkvqr,askscience,new,1
Platypuskeeper,"You might want to check out Coulson and Richardson, specifically [Volume 2- Particle Technology and Separation Processes.](http://www.amazon.co.uk/Chemical-Engineering-2/dp/0750644451) (It's a huge set of hugely popular chemical engineering textbooks)

It's the one I used when studying this stuff long ago. (not that I remember that much more than the fact that I did a lab on sedimentation - which is exactly as boring as it sounds. Sitting and watching stuff sink)
",null,1,cddvup5,1qkvqr,askscience,new,2
OrbitalPete,"As someone who's been working with granular physics for the last 7 years I can tell you that our understanding of these materials is still changing.  There's plenty of things that we have observed in them that we still don't fully understand, and we're a long way from having anything like the Navier-Stokes equations that we have for Newtonian fluids.

I would strongly recommend this as an introductory paper http://jfi.uchicago.edu/~jaeger/group/JaegerGroupPapers/granular/Granular_RMP.pdf
as well as the one iorg linked to.

From there I think the best way forward is to use google scholar to hunt down things specifically of your interest, and then /r/scholar to get hold of the individual papers. This one looks like a good place to start http://onlinelibrary.wiley.com/doi/10.1002/aic.14241/abstract and then following up references linked from it.

There's a vast literature out there, but it's not really the kind of thing anyone's running online courses on.
",null,0,cde1wln,1qkvqr,askscience,new,1
pateln2,"Your muscles are paralyzed in REM sleep, and not in delta wave/slow wave sleep. 
There is a condition called REM sleep disorder in which that paralysis fails and you end up moving/striking out during REM sleep, but, unlike sleepwalking, REM sleep disorder is characteristic of much more severe neurologic conditions (Parkinsons is one of them)",null,1,cddz77y,1qktkj,askscience,new,8
JohnSmith1800,"Metallic sodium and the sodium in NaCl aren't in the same state, which is why you don't see the same reaction.

Metallic sodium isn't an ion, it's just Na, however the alkali metals are all very reactive, and don't ""like"" staying in the un-ionised state, they tend to oxidise very quickly. They're so reactive that they're kept in oil normally to prevent them reacting with the air. When they're dropped in water, they oxidise and the corresponding reduction reaction (RedOx reactions occur in pairs) is the breaking apart of water molecules into hydrogen gas and hydroxide. Hydrogen gas is highly flammable, and the heat released in the redox reaction often triggers combustion, this is the small (for small amounts of sodium, very large for say caesium) flame that you see on the surface of the water. The reaction for the alkali metals in water is:
 
2Na(s) + 2H2O(l) → 2NaOH(aq) + H2(g)

As you can see in the above equation, NaOH is aqueous, which means it separates into its constituent parts in water, ie. Na+ and OH-. NaCl is also aqueous in solution, producing Na+ and Cl-. This illustrates the key reason why salt doesn't have the same reaction as metallic sodium: The sodium is already in it's oxidised state and can't be oxidised further, which means it doesn't react with the water. Hence, it doesn't produce hydrogen gas, energy and a boom.

",null,0,cddwrme,1qks7i,askscience,new,3
bellcrank,"You don't get any appreciable increase in solar radiation when switching from high to low altitude.  It's just not a large enough distance to make any difference.  In fact, the Earth is closer to the Sun during the northern hemispheric winter than in the summer by about 3 million miles.",null,1,cddsyz5,1qkr45,askscience,new,4
cloudofawesome,"If you're referring to radiation in the form of heat from the sun consider that the sun is approximately 150 million kilometers from earth. If one were to rise in altitude by 100 km, that is a difference of 0.000067% in terms of distance to the sun. That is not enough to affect temperature in any way. In fact, since the orbit of earth is not a perfect circle, earth's distance from the sun varies by roughly 5 million kilometers during the year which is a 3% difference. This is still not enough of a difference to greatly affect temperature to a level where one would notice. In fact, during the Northern Hemisphere's winter is when earth is closest to the Sun. I cannot confirm this for sure, so do not quote me on it, but I believe I remember reading that the difference that 5 million kilometers makes is roughly 2 degrees Celsius to the overall global temperature.

If you're referring to temperature in the atmosphere as one increases in altitude, then it in fact does get hotter...sort of. Well more specifically, it varies at different levels in the atmosphere. Refer to this chart on [Atmospheric Temperature vs Altitude](http://www.ux1.eiu.edu/~cfjps/1400/FIG01_019.JPG). You'll notice that in the thermosphere, temperature is increasing quite substantially. The very sparsely separated air molecules at this height can experience temperatures up to 2,000 degrees Celsius. However, one would not feel warm in the thermosphere, because it is so near vacuum that there is not enough contact with the few atoms of air to transfer much heat. A normal thermometer would read significantly below 0 degrees Celsius, because the energy lost by thermal radiation would exceed the energy acquired from the atmospheric gas by direct contact.",null,0,cde17dt,1qkr45,askscience,new,2
Heliopteryx,"I am not sure. Urine mostly contains urea, water, salts, ions, and any toxins that may have been ingested. These are all filtered out of the blood. There shouldn't be whole cells in pee. However, I imagine some cells from the linings of all the tubes the urine goes through would dislodge every once in a while. ",null,1,cddz08i,1qkr42,askscience,new,2
darsie42,"Liquid hydrogen has 71 kg/m3. The sun is roughly 3/4 hydrogen and 1/4 helium. The suns core has 16220 kg/m3.

If you are not picky and consider plasma or supercritical state a gas, then the answer would be yes.

Many compounds are not stable at supercritical conditions and there are no compounds in a plasma state, so for these cases the answer is no.
",null,0,cdea819,1qkqbk,askscience,new,3
High-Curious,"No, because compressing the vapor would eventually cause a phase transition to a liquid (or if the temperature is over the critical temperature, a supercritical fluid). ",null,0,cddxeca,1qkqbk,askscience,new,2
null,null,null,0,cddvqer,1qkq8n,askscience,new,1
uberhobo,I do it all the time in my lab.  Who told you that we can't make compounds?,null,1,cde3g8i,1qkpt1,askscience,new,7
joca63,"In short we do, and we can. In theory it is possible to create any known compound. We can easially make water, all we have to do is mix hydrogen and oxygen and light a match, not to mention numerous other reactions (mainly combustion) that have water as a byproduct. This is a significant paart of organic chemistry, we find a compound in nature (usually a plant) and try to recreate it. In practice it is usually much easier to get the chemical directly from the plant than to make it from scratch (this is why we refine sugar from beets and cane rather than make it). With inorganic materials the main barrier is that the conditions are too extreme. We can and do make diamonds. It is just hellishly expensive and doesnt give jewelry quality gems. 

Another point to bring up is that making things that we know about is actually academically boring. The most interesting science is done by making new derivatives of what we already have. This kind of process is especially important in the drug industry. The general procedure is to take something useful, do something small to it and hope it makes it more useful. This is how we got the opioids (heroin, morphine, codeine, etc). 

TLDR: We can and we do, but usually it is easier to extract a natural product rather than synthesize it",null,0,cdihyzl,1qkpt1,askscience,new,1
stuthulhu,Do you mean [carrying capacity](http://en.wikipedia.org/wiki/Carrying_capacity) or perhaps population equilibrium?,null,0,cddwr9g,1qkoxh,askscience,new,3
iorgfeflkd,"The signal transfers through the balls at roughly the speed of sound in the metal. Because they are so much smaller than the distance sound can travel through metal in a second, it appears instantaneous. If you had a bunch of balloons filled with jello in the same configuration, it wouldn't seem so instantaneous.",null,1,cddwv4w,1qklrf,askscience,new,3
infinitooples,"That's pretty good reasoning from balls to a rigid body.  The force is going to be transferred from one side to the other by a compression wave, and the speed of that wave is determined by sqrt((bulk modulus)/(density)).  So a wave would travel through an infinitely stiff object infinitely fast, and would have an instantaneous transfer of force.

I've never seen a perfectly rigid body, but they show up in the theory of mechanics a lot because they're easy to deal with.  Ever since Newton's law of Gravitation, people have been skeptical that some of the laws of physics seemed to act instantaneously, it's just that no one could really say *what* exactly was wrong.  The observation that the speed of light is finite, and the same in all reference frames finally allowed Einstein to put a finger on what was wrong with the rigid bodies in Classical Mechanics.",null,0,cde5d1g,1qklrf,askscience,new,2
jofwu,"The force would be transferred instantaneously *if the balls were perfectly **rigid***. In reality, there is no such thing as a truly rigid object. Matter simply doesn't work that way.

When the balls collide they deform. If the forces in the collision are really high (hitting them with a sledgehammer) they will deform inelastically- the deformations will be permanent. If the forces are small, the deformations are essentially elastic- like compressing a spring and allowing it to stretch back to its original shape.

It does take time for this compression to happen. The force propagates through each ball as a pressure wave. Perhaps that sounds familiar? That's because it's basically sound. Sound is nothing more than a pressure wave with a frequency inside the range that our ears can pick up. It's the same thing as [dropping a pebble in water](http://en.wikipedia.org/wiki/Capillary_wave) or the [movement of earth in an earthquake](http://web.ua.es/en/urs/disclosure/seismic-wave-propagation.html). Sound (pressure waves) moves through [different materials at different speeds](http://www.engineeringtoolbox.com/sound-speed-solids-d_713.html), depending on the nature of the material.

Of course on the scale of the desk toy it seems instantaneous. Unfortunately this is not the case, and you would notice on a larger scale.",null,0,cde6e3r,1qklrf,askscience,new,2
Platypuskeeper,"pH is a property of a solution, not a compound. 

The pKa of the phosphate groups is near zero, they are all negatively charged at neutral pH. ",null,2,cddxrly,1qkkn0,askscience,new,6
ShortArmedTRex,"If you have a sensitive enough pH meter, then yes, you can measure a pH change caused by dissolving DNA in water. First, a primer on pH...

pH is literally the negative log of the concentration of protons (or positively ionized hydrogen atoms, if you prefer) in solution. So:

pH = -log[H+]

In pure water (H2O), the molecule naturally breaks apart and reassembles itself into H+ and OH- ions:

H2O &lt;--&gt; H+ and OH-

Because of this dissociation, H+ ions in pure water have a concentration of 10^-7, or a pH of 7.
When you add an acid to water, say HA, then the hydrogen ions can also dissociate, usually in a different proportion than water:

HA &lt;--&gt; H+ and A-

This causes an increase in the concentration of hydrogen ions in water, which, because of the logarithmic relationship, results in a lower pH. Therefore adding ANY acid to water results in a lower pH, and DNA is no exception. If you take DNA and dissolve it in water, the hydrogen ions will dissociate from the DNA molecule, thereby increasing the concentration of hydrogen ions in water, resulting in a decrease in pH.

BUT the problem in measuring DNA is the sensitivity of your pH meter. Usually when you dissolve DNA in water, your concentration is very low! This means relatively few hydrogen ions dissociate from the DNA molecule relative to the number of hydrogen ions that dissociate from pure water molecules, so there is only a very small change in pH. If your pH meter is sensitive enough, or you have a high enough concentration of DNA dissolved in the water, you should be able to measure the difference!",null,0,cdfg22d,1qkkn0,askscience,new,1
galinstan,"Yes, the temperature of ice will get colder than 32 F if the ambient temperature is lower than that. The density of ice is inversely proportional to its temperature (i.e. as it gets colder, it becomes denser).",null,0,cde64f5,1qki72,askscience,new,3
adamsolomon,"Light has been present since at least a small fraction of a second after the Big Bang. However, for the first 380,000 years thereafter, light would bounce around from atom to atom, never travelling very far before hitting the next one, so it wasn't for 380,000 years that the Universe became transparent and light could travel through. That primordial light is what we now see as the cosmic microwave background.",null,1,cde2v19,1qkhcr,askscience,new,6
WifoutTeef,"The universe initially was very dark because there were no stars. However, the first stars lit up (once the universe cooled enough) around 100-400 million years after the big bang occurred.

Sources [1](http://www.scientificamerican.com/article.cfm?id=the-first-stars-in-the-un) [2](http://www.jwst.nasa.gov/firstlight.html)",null,3,cde1im7,1qkhcr,askscience,new,2
HenCarrier,"This [link](http://support.radioshack.com/support_electronics/doc66/66356.htm) contains general information regarding the different abbreviations you mentioned. Hopefully it's a start.

[FM (Frequency Modulation)](http://en.wikipedia.org/wiki/Frequency_modulation#Applications)

[SW (Shortwave)](http://en.wikipedia.org/wiki/Shortwave#Uses)

[MW (Medium Wave)](http://en.wikipedia.org/wiki/Medium_wave#Medium_wave_in_Europe) aka AM

[LW (Longwave)](http://en.wikipedia.org/wiki/Longwave#Broadcasting)",null,0,cddzrn3,1qkfqw,askscience,new,3
Oilfan94,"Icebergs aren't simply sea water that freezes, they are chunks that break off of glaciers and float around until they melt.  

Glaciers are formed over many thousands of years by snow falling and compacting...so it's mostly likely fresh water and wouldn't be salty tasting.  ",null,0,cddogs2,1qkens,askscience,new,6
Javi2639,"When drugs are first developed, they are synthesized by organic chemists in the laboratory. However, since this is very time consuming, expensive, and provides a low yield, microbiologists will create a gene to insert into bacteria for an enzyme so that they can synthesize it naturally.",null,0,cde6iy5,1qke0t,askscience,new,1
dapwnsauce,"There is no particular ""general"" reaction used since there are many types of steroid drugs that have different structures overall.  

An example is steroid drugs that resemble testosterone which is a hormonal steroid.  If you take a good look at [testosterone](http://www.anabolic-egg.com/images/molecules.gif) you will notice it is has the same skeletal structure as cholesterol.  As an organic chemist attempting to synthesize a drug similar to testosterone, I would start with a common starting material being cholesterol and proceed with some chemistry to alter the compound to my desired drug.

This is common for many drugs that are made in the industry.",null,0,cdeb16l,1qke0t,askscience,new,1
Whisket,"Yes it can! The phase of water, whether solid, liquid, or gas, is a function of both temperature and pressure. [Here](http://upload.wikimedia.org/wikipedia/commons/0/08/Phase_diagram_of_water.svg) is the phase diagram for water. It will show that above ~100kbar (100,000 times normal atmospheric pressure), water will be solid, as in ice, no matter what temperature. 

EDIT: Oops, I misread the question, so here is a more specific answer. The answer is yes, increasing pressure can ""melt"" ice into water, but only in very specific circumstances. For example, if you were to keep water at the constant temperature of 260k. At low pressures, this will be in the gas form. If you were to increase pressure, the gas will become solid at ~0.001 bar, then at ~1500 bar it will change to liquid, and then back to solid at ~4000 bar",null,7,cddo3e7,1qkdl0,askscience,new,48
TMaCtheCLaP,"Pressure, along with friction, is also part of the reason why ice skates work the way they do (or so we think). The blade has a very small surface area and a person is relatively heavy. Since pressure = force/area you can see the pressure exerted by the ice skate blades is very large. The pressure slightly compresses the ice underneath the skates, melting a bit of it to provide a slick surface for the blade to slide on.
Source: http://en.wikipedia.org/wiki/Premelting#Ice_skating",null,1,cddq10u,1qkdl0,askscience,new,10
TheFeshy,"That's how you make snowballs.  The force of you ""packing"" the snow melts a minute amount of water, which re-freezes and holds the snow together. 

So, some fun facts:  

* Ice that is cold enough that it has shrunk below the size of an equivalent amount of water can not be made into snowballs, because your pressure won't melt it.  I don't recall the temperature though, but [here is a fun read about it.](http://www.astrosociety.org/pubs/mercury/9801/snowball.html)

* Ice that forms in a vacuum [doesn't crystalize](http://en.wikipedia.org/wiki/Amorphous_ice), and therefore doesn't expand.  So cometary ice can't form snowballs.  My wife looked at me like I was crazy when I criticized a random scene in an episode of Enterprise where the crew built a snowman on a comet.

* There are a few other materials where the solid is larger.  I'm told one of these is apparently plutonium.  In an environment of the right temperature to have barely-frozen plutonium, you could have a plutonium-ball fight with plutonium slush.  Just don't make them very big...",null,6,cddrnun,1qkdl0,askscience,new,5
bobroberts7441,"How would this work in practice? If, as you apply pressure, the phase changes to liquid with a lower volume, which reduces the pressure which would cause refreezing. Would it be possible to actually achieve what the OP envisions? All I can think is some weird instantaneous phase oscillation.",null,0,cddsk19,1qkdl0,askscience,new,1
FriendlyCraig,"One of the reasons we don't have fur is the same reason others do! Fur is a lovely insulator, which means lengthy activity can overheat an animal. The lack of fur combined with our ability to sweat make for a pretty efficient way to cool down. If we had fur, we'd need to shave it off to stay cool, something early humanoids couldn't do. ",null,0,cddwyfj,1qkdjt,askscience,new,3
AndrewJamesDrake,"We evolved for a different setup than apes, although we do share common ancestors once you go far enough back.

Apes are gatherers, herbavores. They move slowly, so they don't have to worry about overheating. They are basically evolved to be fairly sedentary. When they do exert themselves, its designed to be for only very short periods of time. So fur, as a way to keep heat during cold months and to keep rain from robbing you of heat, was an advantage to them.

We, on the other hand, are built for long-term activity. Seriously, as species go we are the Energizer Bunny. When other creatures would have dropped from exhaustion, we keep going. Our lack of fur plays into that. We can go for hours before heat exhaustion sets in because we can sweat, and because our bodies don't hold heat very well. On a related note I need to explain why that is an advantage, we also have very efficient muscles. For the amount of space we take up, humans are ridiculously strong, and have extremely dense muscle tissue.

But since other animals can't keep going for so long, we suddenly have a novel way of hunting. Just keep hounding the thing until it passes out from getting too tired or too hot to keep going, and then club it to death with our highly efficient, powerful muscles. 

If we had fur, heat exhaustion would be too much of a threat for us to be able to chase our prey down, and since we evolved for very warm climates, the lack of fur wasn't a major problem during cold months.

Also: Even when cold is a threat, we are smart enough to appropriate something else's fur.",null,0,cddxhjn,1qkdjt,askscience,new,3
alanwpeterson,"The Out of Africa Theory says that homo sapiens developed in Africa. The general consensus on how we hunted prey was that we would run the prey down until they were simply too exhausted to flee and then we would go in for the kill. This is known as Persitence Hunting. That is the evolutionary advantage to sweat glands; we can run and cool our body that at the same time whereas an animal cannot pant and run at the same time. This practice of running the animal to death is still practiced by African tribes such as the Kalahari Bushmen and they run after prey for long distances sometimes stretching to 20 miles. Many anthropologists actually study the Kalahari Bushmen because they believe it is observing an archaic homo sapien behavior. It would make sense to lose the fur to prevent overheating so that we could go even further distances. Plus fur isn't a good combination with sweat glands because hair sucks in a decent amount of water so if we were covered in fur and sweating, it would be an added load of weight that we would have to endure.",null,0,cde03ef,1qkdjt,askscience,new,1
bjornostman,"Note that the explanation that *Homo sapiens* lost fur to avoid overheating is a hypothesis. We do not know this, because the hypothesis hasn't been tested. I agree that it is a decent hypothesis, but there could be other reasons. The Aquatic Ape theory hypothesizes that it was to be able to swim better*, and it could also have been due to sexual selection, or it could simply be a neutral trait, such that there is no adaptive reason that we lost our fur. It could also be a combination of selective agents.

* Personally think that is a bad hypothesis, not just because the whole theory has little support, but also because lots of mammals swim well with fur.",null,0,cdelgpq,1qkdjt,askscience,new,1
wildcard5,"Like someone already mentioned, there is no possible way to test this. So every *reason* as to why we don't have fur is just a hypothesis.

The most agreed upon hypothesis is that fur would attract parasites. So the brain of our ancestors picked being hairless as more attractive (sexual selection). The reason we have head hair is to protect us from excessive sunlight. The reason we have axillary (armpit) and pubic hair is because these areas release pheromones which help us pick mates and the hair helps trap these.

Other reasons include:

When we discovered fire, we no longer needed the fur for keeping warm.

Without the fur, it is easier to swim.

Without sweat glands we would have a very hard time in cooling down, and fur doesn't go well with sweat glands.",null,0,cdfe3iu,1qkdjt,askscience,new,1
endocytosis,"Solutes in a solution will lower the freezing point, since it's more difficult for the solvent's molecules to stack together into a solid if there's more solute present.  I don't have a chem textbook on me to give you the exact formula, but it [looks](http://van.physics.illinois.edu/qa/listing.php?id=18705) like about 6 teaspoons of salt in a quart of water will lower the freezing point almost 2 degrees Celsius (see the link for metric or more exact).  

You are absolutely correct that different solutes will affect the freezing points differently.  Table salt, NaCl, immediately dissociates when it dissolves into Na+ and Cl- ions, which affects the melting and freezing points.  Sugar will dissolve, but will remain as a sucrose molecule, [behaving](http://chemed.chem.purdue.edu/genchem/topicreview/bp/ch18/soluble.php) differently than the ionic Na+ and Cl- do when dissolved in water.",null,2,cddp2hn,1qkan3,askscience,new,6
gay_dino,"If you are looking at phase behavior, you want to look at **Gibbs Energy**. When water changes phases, it is trying to minimize Gibbs Energy. 

A fitting analogy is temperature and heat. Heat flows from high temperatures (i.e. an oven) to low temperatures (i.e. your frozen pizza). Heat will flow in this direction until it reaches an *equilibrium*, i.e. when the oven and pizza are the same temperature. The same analogy can be made about pressure and mechanical energy. 

And so it is with Gibbs Energy, even though it is more abstract and so harder to picture. When you have a bottle of water, the water particles will move between the liquid and vapor phase until an equilibrium is reached (i.e. the Gibbs Energy of both phases are equal). At 25C, the equilibrium is favored towards the liquid side, at 100C it is skewed towards the vapor side. 

You can already see Gibbs Energy changes when circumstances (such as temperature) changes. Another factor that influences Gibbs Energy is [Entropy](http://en.wikipedia.org/wiki/Entropy), or disorder.  (in fact, the full form is [G=H+TS](http://en.wikipedia.org/wiki/Gibbs_free_energy))

Basically, if there is impurities disorderly mixed in (as opposed to orderly separated) this shifts the Gibbs Energy equilibrium of the system. All of the sudden the Gibbs Energy of the solid increases because it takes more effort (or energy) for water molecules to find each other and arrange themselves in an orderly crystal-forming fashion. Thus, water molecules will be content to stay liquid for another few degrees. 

Read further [here](http://en.wikipedia.org/wiki/Freezing_point_depression). Hard to explain Thermodynamics of mixtures in a friendly way :)

TL;DR: Because the Gibbs Energy equilibrium shifts when mixtures (solvent + solution) interact. ",null,0,cddqov0,1qkan3,askscience,new,3
revilohamster,"The change in boiling and freezing points are known as colligative properties, meaning that the solute's chemical nature doesn't matter, what matters is just how much solute there is! These properties are caused by the chemical potential of the liquid solvent being  lowered by adding solute. This means that the vapour-liquid equilibrium occurs at a higher temperature, and the the solid-liquid equilibrium occurs at a lower temperature, explaining the shift in bp. and mp. 

Why do these effects occur? They are entropic in nature- we know this as they occur even in an ideal solution, so they can't be enthalpic. Consider a solution compared to the pure liquid. The solution has higher entropy- it has two components compared to just one, but the entropy of the solid and vapour phases remains unchanged (the chemical potentials of these phases is not affected by the presence of non-volatile solute). Therefore for a solution, the entropy of fusion is higher and the entropy of vaporization lower, relatively, compared to that of a pure liquid. So the melting occurs at a lower temperature, and the boiling at a higher temperature, for a solution compared to a pure liquid. 

Increase in boiling point DT can be given by:
DT = ((R*(Normal boiling point)^2 ) / (enthalpy of vaporization)) * (solute mole fraction)

Decrease in melting point DT can be given by:
DT = ((R*(Normal melting point)^2 ) / (enthalpy of fusion)) * (solute mole fraction)

Hope that helps... Let me know if anything is unclear (some parts probably are!)",null,0,cddpjv3,1qkan3,askscience,new,2
huyvanbin,"Roughly speaking, only in metals, electrons are ""scattered"" by the thermal motion of atoms in the metal, so the speed of the electrons is the same but they end up having to take a longer path, so the overall rate is lower.",null,1,cddq37i,1qk2hk,askscience,new,4
drzowie,"LED lights are more localized than traditional incandescent lights, yielding brighter images on the retina.  You can resolve motion faster with brighter lights, so the small, in-focus, bright points of light from the LED lights may seem to move slightly out-of-phase (and more perceptibly) than the rest of the scene, under normal jostling by the car.

Many LED lights flicker a few dozen times per second, so that they can be dimmed with pulse-width modulation.  That's because it's easier to change the amount of time the LED is ON or OFF in every (say) 30-millisecond chunk of time, than to actually change the amount of current running through the LED at once.  A side effect is that the flickering ""freezes"" small jostling at a random part of the jostle, so the whole scene may blur more than the LED lights in your mirror, but the LED will often appear to be at a slightly different place in the scene than the rest of the (non-flickering) stuff you can see.

LED numeric and text displays are even worse, flickering with low duty cycle so that they can be multiplexed.  Typically only one digit of a numeric LED display is lit at once, or one row of those scrolling tickertape LED signs; that saves cost by reducing the number of components necessary to drive the LEDs - but it means that each digit is lit at a slightly different time, so if you mirror vibrates even a tiny bit the LEDs won't seem to blur with the jostling so much as dance around one another.",null,1,cddncl0,1qjze8,askscience,new,6
kooksies,"Certainly, they are called ***alkaliphiles***.  
This book called ""The Prokaryotes"" has [a chapter on alkaliphilic bacteria; this small section is available for free online](http://link.springer.com/referenceworkentry/10.1007%2F978-3-642-30123-0_58#page-1).   ",null,0,cddo86u,1qjuqi,askscience,new,1
JDL523,"Soil scientist here! 

This is mainly due to a variety of factors that parent material has on the soil. First, geology determines what 'stock' of minerals the soil has. The geology will also determine how fast your soil will form, and how available any nutrients locked away in the crystalline structure will be released. i.e. soils developing on areas high in quartz or feldspar take a long time to form, and have really low nutrients and nutrient holding capacities compared to soils that are formed on mica-based parent material, due to their complex 'framework' type structures. Micas on the other hand have layered structures which are much more easily weathered, and once weathered, provide a large surface area for nutrients to interact with.

Next, geology tends to determine soil texture, which has a huge influence on water and nutrient retention, and therefore fertility since plants need to 1) have nutrients available in the forms they need and 2) need to be able to take up these nutrients before they was away (particularly NO3- and S).

Finally, geology can also determine the pH of the soil system, i.e. soils formed on limestone. By influencing the overall pH during formation AND once the soils are formed, the parent material regulates what reactions occur in the soil and what forms of N, P, K and S are plant available. 

",null,0,cde9kgb,1qju1j,askscience,new,3
Gargatua13013,"Minerals freshly brough up to the surface are usually outside of their field of stability as regards T and P. So they will undergo chemical transformation into other minerals which *are* stable at surface conditions, which will form the basis for the newly formed soil. These reactions are usually slow, but they can be accelerated by proper conditions such as warn and humid conditions. Since the type of source rock controls the mineral assemblage, it will also control the assemblage of reaction products. 

The main reaction products of these reactions belong the vast mineral family of clay minerals, with some hydroxydes. These have a variety of preperties, which have an effect on available ion content, capacity to store and release water, and so forth.

Weathering  in certain conditions can deplete the newly formed soil by a variety of processes such as further chemical weathering or mechanical winnowing away of clays.",null,0,cddq55g,1qju1j,askscience,new,2
nrj,"If you're asking whether HIV infects gametes, the answer is no. It infects mainly a specific immune cell (CD4+ helper T cells). Free viruses are present in bodily fluids and this is what transmits the virus.",null,0,cddnnvm,1qjt55,askscience,new,5
nanopoop,"Isentropic means constant entropy.  

If the entropy of a working fluid doesn't change over the course of a process then the process is said to be isentropic. 

Reversible processes that are operated with zero heat transfer (q=0)  are isentropic by definition.",null,2,cddobf2,1qjrfc,askscience,new,5
xxx_yyy,[Here](http://www.grc.nasa.gov/WWW/K-12/airplane/compexp.html) is a NASA page on the subject.,null,1,cddnn57,1qjrfc,askscience,new,2
iorgfeflkd,"Any dense material will do, but lead is one of the lest expensive dense metal. You could make radiation shielding out of gold or tungsten or uranium (although the latter kind of defeats the purpose). In some applications, concrete or water is used.",null,0,cddx29j,1qjnmf,askscience,new,2
EdwardDeathBlack,"Linear independance, Nullity, etc... are the first ""hints"" of important mathematical questions. The one I will highlight here is ""eigenmode"" and ""eigenvalue"", in an attempt to show you why they will unfold into something important. 

If a physical system of interest can be represented, even approximately, by linear algebra representations, then its ""natural modes"" will appear out of those concepts you are learning now. 

What do I mean by ""natural modes"", here are a few exemples, 

* the vibration frequencies of a violin strings (the ""sound"" of a violin)
* same for a drum
* the resonant frequencies of a bridge
* the energy levels of atoms
* the microwave energy distribution in a microwave oven
* You will also find hints of it in any systems that uses resonance, such as NMR and MRI, more at [this place](http://en.wikipedia.org/wiki/Resonance#Types_of_resonance) 

I suspect (but haven't seen the math) that Google search algorithms and Netflix recommendation systems are heavy on eigenvalue/eigenvector problems too...

So, it is really important. Understanding what is ""concealed"" on those matrices from linear algebra that tells us about the physics of the system they described is really what you are learning now. And how to ""reveal"" that ""concealed"" information....of course",null,0,cddzbav,1qjl3e,askscience,new,6
spPad,"Linear algebra is used in almost all compute-intensive tasks. It can efficiently be used to solve any linear or non-linear set of equations. A vast array of problems from aeronautical to computer engineering can be reduced to a set of equations, and they would be impossible to solve (within a reasonable runtime) without linear algebra. 
",null,1,cddxqd6,1qjl3e,askscience,new,5
LoyalSol,"You see the computer screen you are looking at right now?  Those pixels are one giant matrix.  

Linear Algebra is actually one of the most used branches of mathematics because anytime you are dealing with large data sets you need it.   

",null,0,cde8guv,1qjl3e,askscience,new,5
tauneutrino9,"All of quantum mechanics is linear algebra.  I thought the same thing when I learned linear algebra, then I took 6 semesters of quantum and now I wish I took more linear algebra.",null,0,cddyfdy,1qjl3e,askscience,new,3
1337bruin,"Linear algebra is useful because it is ""easy"", in the sense that most linear algebra problems can be solved efficiently by a computer. 

One could argue that the main technique in applied math is to transform a given hard problem into linear algebra so that it can be solved easily. Differential calculus, for example, aims to linearize arbitrary functions.",null,0,cdee7oq,1qjl3e,askscience,new,2
whatthefat,"Yes, you do have a significantly higher energy expenditure when you are awake and lying still than when you are asleep. The difference is around 20%, as can be seen in Figure 3 of [this paper](http://www.pnas.org/content/110/14/5695.full.pdf+html).

The reason for this is not completely understood, although there are changes in many physiological systems during sleep. Body temperature is lowered at night, and lower during sleep than during wake (even at the same time of day). [Heart and breathing rates are generally lower during non-REM sleep](http://jap.physiology.org/content/59/2/384.short) (which makes up about 80% of sleep in human adults) than during wake. Brain metabolism also seems to be [lower during non-REM sleep than during wake](http://www.sciencedirect.com/science/article/pii/0024320589900210).",null,1,cddo124,1qjl2w,askscience,new,4
iorgfeflkd,"There is a solid core. [Interior of Jupiter](http://www.lpi.usra.edu/education/explore/solar_system/images/interior.jpg). [Saturn](http://physics.uoregon.edu/~jimbrau/BrauImNew/Chap12/FG12_08.jpg). [Uranus](http://upload.wikimedia.org/wikipedia/commons/f/fe/Uranus-intern-en.png). [Neptune](http://solarsystem.nasa.gov/multimedia/gallery/Neptune_Int.jpg).

Knowledge about the interior composition comes from flying space probes North-South as they orbit or pass the planets, and observing how the gravitational field changes.",null,0,cddxa8s,1qjizv,askscience,new,2
SwedishBoatlover,"Gas planets exists because of the same reason rocky planets exists: gravity and the fact that all particles with mass are attracted to each other by gravity.

The gravitational force from particle A to B is expressed as: F = G((m1m2)/(r^2 )), where G is the gravitational constant, m1 and m2 are the two masses, and r is the distance between the masses.

Since the particles in a gas have mass, they also attract each other (as well as all other particles in the universe. Gravity have no range, but a squared fall-off). So if you stick many enough molecules together they will continue to stick together because of gravity. Most gas giants have a solid core though, at least the ones close enough to us to be observable (i.e. mainly the ones in our solar system). ",null,0,cddxrpa,1qjizv,askscience,new,2
cromonolith,"Maybe I'm ignorant, but what does ""delineation"" mean in this context?",null,1,cddz80w,1qjhpn,askscience,new,5
drzowie,"It depends on how sensitive your notion of ""level"" is.  A level is a gravitational equipotential surface.  Because of the Moon's influence, the equipotential surfaces around Earth are not fixed.  Typical variations in equipotential height are about 2 meters across 2 megameters, or about 10^-6 radians (0.2 arc seconds).  Something small like a toilet will sample a tiny piece of the equipotential surface it lies on -- basically a plane, whose angle changes slightly over the course of the day relative to the fixed structures around it.  A toilet bowl with 30cm diameter will thus show a shift of about +15nm on one side and -15nm on the other side over the course of 6 hours.  That's pretty small and difficult to measure by conventional means (you couldn't use an interferometer, for example, since the displacement is small enough that you'd have to use vacuum ultraviolet -- and the toilet water wouldn't be stable in a vacuum chamber).  But you should be able to measure the displacement in a bathtub or small jacuzzi using a near-UV interferometer.
",null,0,cddouxp,1qjff0,askscience,new,4
Dannei,"&gt;If I did my math right, if the earth were represented by a marble that was 1 inch in diameter, the sun would be about 989 feet away from the marble. (I used 7918 miles for diameter of the earth and 92.960M miles between earth and sun. Someone can verify)

That all sounds about right. At this scale, the Sun would be a sphere about 9 feet in diameter! Would you believe that I was able to find [an image of a sphere that size?](http://halmapr.com/news/india/files/2010/07/LMS-3M_blog.jpg).

&gt; How far away would Neptune be from the sun?

The Earth-Sun distance you've measured is known as the ""Astronomical Unit"", and is generally how distances within the solar system are measured. Obviously(!), Earth's orbit is 1AU in radius; Neptune orbits way out at 30AU, or thirty times the distance from the Sun that we are at. On your marble scale, that puts it about five and a half miles away from the Sun. This data is all well known, and you can find it easily with Google or Wikipedia.

&gt;How about a light-year?

A lightyear is 63,240 AU. On the marble scale, this is 11,800 miles - or a twentieth of the way to the *real* moon! The nearest star system ([Alpha Centauri](http://en.wikipedia.org/wiki/Alpha_centauri)) is just over 4 lightyears away. If we were to take our 5.5 mile wide Solar System out to Neptune, and shrink *that* into a marble 1 inch across, Alpha Centauri would then be 738 feet away - whilst the Earth and the Sun would be separated by about the thickness of your credit card.

The centre of our galaxy is 27,000 ly away - once again, this dwarfs out previous size scale; this puts the galactic centre 900 miles from the 1-inch solar system, or about the distance from Washington DC to central Iowa. It's pretty crowded down there - whilst there is only one star within 4 light years of us, there are *thousands* within the same distance in the centre of the galaxy.

We're also not at the edge of the Milky Way; if the centre was in Iowa, the galaxy would extend from the south of Mexico to the very northern parts of Canada. Meanwhile, we're on one side of a credit card in someone's pocket in Washington DC, and the sun is on the other side of that card...",null,1,cddppf9,1qj6nl,askscience,new,7
Clever-Username789,"My calculation got ~978 feet from the Earth to the Sun, assuming the diameter of the Earth is 1 inch, so your calculation is fine.

The distance (from the Sun) to Neptune is between 29.7-30.4 AU, where 1 AU is the distance between the Sun and the Earth. So that means the distance to Neptune is ~30 times 978 feet, which is roughly 5 and a half miles.

A light year would be ~11713.6 miles. Which is roughly half the circumference of the Earth at the equator. 

If you wanted to make a scale model of the solar system using these dimensions you would need a roughly circular area at least 11 miles across. ",null,0,cddoxoe,1qj6nl,askscience,new,3
ManOfClay,"Since you're working with your son, you might tell him this bit of trivia to blow his mind. We see the moon in the sky and think it's SO close! Well, if you were to grab every other planet in the solar system and line them up, they would fit between Earth and the Moon, with some room to spare. (Yes, even with Pluto, or two Plutos if you want, but not much more.)

Also, you might check out wolframalpha.com when you're looking for simple quick facts. For example: http://www.wolframalpha.com/input/?i=distance+from+earth+to+moon",null,0,cde8ysx,1qj6nl,askscience,new,3
civilizedanimal,"You'd have to be more clear. Can't really help you without knowing what tools you used/have, what steps you took, etc.

Your best bet is probably steam distillation. You'd need a heat source, a still, a retort, a condenser, an essencier, and a receptacle. Alternatively you could put the rinds directly in the water, use a heat source, still, condenser, and collection vessel. Then you'd have to use a separatory funnel to separate out the essential oils.

If you really wanted to use a centrifuge, you's have to find a way to liquify the rinds. Blender and water would probably work well. Then you'd likely have to heat the mixture to release a bit more oil. Then you could centrifuge and collect the oil. The problem with centrifugation is quantity. Most centrifuges are designed to spin a lot of small vessels. Those vessels would contain very little essential oil, and it would be difficult to extract anything from them after centrifugation. Even with one large centrifugation it still would be harder to separate the oils using this method rather than just using essenciers or separatory funnels",null,0,cddp4j2,1qj083,askscience,new,2
7LeagueBoots,"That is a really difficult topic and a good question.

First off, the paleolithic covers a time-span longer than our species and covers a time when there were multiple lineages of the Homo lineage living concurrently.  It also covers a range from when we were pretty much confined to Africa to covering all the continents except Africa.  Roughly 1.4 million years ago to 10,000 years ago (depending on what sources you reference).

During that 1.39 million year span covering every environment from arctic to tropical and forest to desert, and including Homo habilis and Homo erectus up to more-or-less modern day Homo sapiens it is a certainty that many social organizational structures have been attempted.

What we do know is that language has been a key part of our social toolbox for a long time and that specialized skills have been passed along from generation to generation for a long enough time and early enough for regionally diagnostic cultural markers to be apparent from early on.  In the older records this appears in the style, shape, and material preferences of stone tools.

We know that trade has been important for many tens of thousands of years, but that most communities did not range very widely to meet their needs.  This is especially evident in Neanderthal societies where most stone tools come from quarries close by.

We know that we have had organized underground mining projects for more than 30,000 years (the Nazlet Khater site in Egypt is a great example of this), which indicates some sort of social hierarchy.

We know that at least some societies took care of their elders, revered their dead, and ritually interred them (again, Neanderthals are one of the best known examples of this).  This indicates both a respect for age/experience/knowledge and a degree of social empathy.

From the colonization of eastern Indonesia and Australia more than 60,000 years ago we know that boat or raft building has been part of our body of knowledge.

We know we have had art and music for at least 50,000 years, which indicates an appreciation for abstraction and cultural support for non-survival based activities.

The issue is that once we move beyond the physical remains (which is all we have left of these previous cultures), all we have left is deduction and induction, which makes it extremely difficult to say, ""Yes, these people behaved in this manner.""  All we can accurately say is, ""The fact that these people engaged in this activity they must have behaved in such-and-so manner.""  To avoid controversy any assumptions must be overly conservative.  Witness the history of the debate on Neanderthal linguistic capacity to see the ridiculousness of some of the reductive stances taken by people on one side or another of these arguments.",null,0,cdgqyns,1qivxr,askscience,new,3
