author,comment,distinguished,downs,id,post,subreddit,type,ups
Osymandius,"I think you're coming at this from the wrong angle. There's no reason one signalling molecule does any particular function, bar that that's the function that its receptor effects.

When you say ""why"" testosterone - because that's the molecule that binds to its receptor. It wasn't that testosterone existed beforehand, and happened to produce a male phenotype upon binding to its cognant receptor. 

Any molecule which triggered that set of downstream pathways would do, but its vital that we have the molecular specificity such that only one molecule does. Imagine if blood glucose or a dietary fat bound to sex hormone receptors - it would be chaos!

Does that clear it up? I appreciate it's not an answer of molecular basis, but it's a question of terminology - their suitability has nothing to do with their downstream effects, only their specificity. ",null,2,cdgkow2,1qtuit,askscience,new,3
sapolism,"Its thought that nuclear steroid receptors sites all diverged from a common ancestor. The whimsy of mutation and the success thereof is what would have driven higher specificity in these receptors to bind specific molecules. Discussion of the cause of success here would get very complex, and I find it unlikely that we have enough DNA evidence to make a good argument for what actually happened.

I don't know what is likely to have happened on the ligand end of things either...
",null,0,cdgm2p1,1qtuit,askscience,new,1
homininet,"Good question. Most mammals are what are called dichromats. What this means is that they have two types of cone cells in their retina that are optimized to see blue/violet light, and red light. Certain primates are unique in that they possess a duplication of gene which has given them 3 cones instead of two (blue/violet, red, and green). These primates are catarrhines (old world monkeys, apes, and humans). Certain other primates have also evolved color vision but in different ways. Howler monkeys (new world monkey) have all evolved trichromacy, and in most other new world monkeys, only females are trichromats, and the males are all dichromats because the gene duplication responsible for this rides on the X gene. Some lemurs are like this as well.

Certainly this has also played a role in evolution. Most early mammals were nocturnal, and didnt really have a huge need for trichromatic vision. They did however, have great senses of smell. Primates have kind of done a double take, and foresaken their great sense of smell for a more keen sense of vision, and in the process, certain lineages became diurnal. When you walk around in the daytime, trichromatic vision is a huge plus, and catarrhines were able seize on this advantage due to this early gene duplication which cause them to be trichromatic. 

",null,0,cdglv7i,1qtuty,askscience,new,5
nicorivas,"Yep, colour is an advantage just by the possibilities of distinction. And also yes, there are animals that have the same receptors as us, although not all of them. The article on wikipedia on the [evolution of colour in primates](http://en.wikipedia.org/wiki/Evolution_of_color_vision_in_primates) is very informative.",null,0,cdgkv7e,1qtuty,askscience,new,1
SqueakyGate,"The current data is really really questionable. It really depends on who you ask, because personal biases often play a subtle but important role in the way people interpret these psychological tests. 

I would suggest reading up on the [delusions of gender by Cordelia Fine](http://en.wikipedia.org/wiki/Delusions_of_Gender): ""The author criticizes claimed evidence of the existence of innate biological differences between men and women's minds, as being faulty and exaggerated, and argues that cultural and societal beliefs contribute to commonly perceived sex differences....""The thesis of my book (no veils required) is that while social effects on sex differences are well-established, spurious results, poor methodologies and untested assumptions mean we don’t yet know whether, on average, males and females are born differently predisposed to systemizing versus empathizing.""

However, some criticism of her book have been made - although I don't know of any substantial rebuttals to her work. Given the level of debate from both sides, the lack of strong evidence one way or the other I have to say that overall psychological and mental differences in sex (e.g. men vs. women) is not strongly supported at this time. 

Gender is of course a social and cultural construction of the perceived way the different sexes should act, behave, dress, conduct themselves etc. In western societies, there is a two-sex two-gender system, that is male = masculine and female = feminine. But this is not the case for all cultures, some have multiple genders which overlap with the two sexes. Therefore it would be great to have a broader characterization and experimentation of people who come from cultures with more than two gender systems. It would be interesting to see how the two sexes perform when they are not held too the western standards of gender. 

What I am trying to get at is any test or examination conducted today has a really difficult time separating biological sex and imposed cultural gender stereotypes. 

",null,0,cdgwehk,1qtuwc,askscience,new,2
CompMolNeuro,All mammals at the very minimum.  I would posit that any animal capable of having a psychological profile will show gender specific behavior.,null,3,cdgkydf,1qtuwc,askscience,new,3
bellcrank,"Solar radiance, or solar insolation, is a typical measurement, though it may be difficult to search for specific data (like 'cloudy days').  It's a statistic very important to the solar power industry.",null,1,cdgn9gi,1qty2t,askscience,new,3
wazoheat,"If you aren't opposed to number crunching, you can check out the [National Solar Radiation Data Base](http://rredc.nrel.gov/solar/old_data/nsrdb/), which contains hourly data on the amount of solar radiation reaching the ground since 1961. [There are a lot of stations nation-wide](http://rredc.nrel.gov/solar/old_data/nsrdb/1991-2010/images/NSRDB_Stations_revised.png).",null,0,cdgu90b,1qty2t,askscience,new,1
Mazetron,"Well for starters you have to take into account air resistance and the decreasing force of gravity as you gain altitude.  You could calculate a fairly accurate angle to launch your rocket, but if you need more precision than that, your rocket may need to have smaller rockets on it to make adjustments.",null,0,cdgtkwx,1qtzfl,askscience,new,1
TheMacPhisto,"Rockets are guided based on planned trajectories that are planned prior to launch, or shortly after. There are formulas out there used, in tandem with guidance software, that map the most effective route out, even before the projectile is launched.

Using this pre-planned map of the ballistic trajectory (from launch to target) cross referenced with real time GPS data, RADAR data, Millimeter Wave Scanning, or any other type of guidance aid, the control surface software in the projectile will know what adjustments to make to return the projectile back to it's pre-determined path, and thus to it's target should any unforeseen factors alter it's course.

The ideal path is pretty ordnance specific. It also depends on the type of target you are looking to impact. The combinations of which are too numerous to mention in detail here. But an example would be the difference between an impact-fused warhead, compared to a proximity fused warhead, or the difference between striking a hard target like a bunker, or fortified above ground installation, compared to a soft target like a house or other standard building.

Most guidance systems these days operate the way I described above. this is known as ""passive"" guidance or ""semi-active."" As this type of guidance is incredibly difficult to interrupt or throw-off while at the same time being incredibly accurate.",null,0,cdhtbbz,1qtzfl,askscience,new,1
Pennsylvania_Fatts,"Yes. Separations science is devoted to creating pure chemicals from mixtures. Many different techniques can be used depending on the mixture.  Some common methods are chromatography, centrifugation, filtration, distillation, extraction, and recrystallization.


Besides separation, sometimes components can be detected from the mixture if they have properties that are unique. The physical phenomena that must be present for a type of analysis to detect a component are called the selection rules. Since you mention foods, you can also use your mouth and nose as to analyze, because your senses of taste and smell select for certain components of the mixture.",null,0,cdgwyr4,1qu1w8,askscience,new,3
Chemomechanics,You can use [chemical vapor deposition](http://en.wikipedia.org/wiki/Tungsten_hexafluoride)---flowing a tunsten-containing gas over the wafer and allowing it to react with silicon (at relatively low temperatures) to form metal and a gaseous byproduct.,null,1,cdgpeet,1qu2jp,askscience,new,7
LukeSkyWRx,"You can also electroplate with tungsten, typically takes place at even lower temps than CVD.",null,2,cdgvvq2,1qu2jp,askscience,new,2
Quantumfizzix,"It didn't start out that way, Pangea was just the last supercontinent that formed. The continents on the Earth roughly follow something called the supercontinent cycle, gradually changing from continents, to a supercontinent, back, and forth, over and over again. There were many supercontinents before Pangea, and there will be conceivably more in the future.",null,4,cdgm2kz,1qu6b0,askscience,new,54
homininet,"So, the world didn't quite start out as one big land mass. The supercontinent Pangea actually existed during the Permian period (around 275 million years ago). But of course the world is much older than this, and the continents were moving around before this, but the farther into the past you try to reconstruct continental positions, the harder it gets. [Here](http://www.scotese.com/earth.htm) is great website that has beautiful maps of the positions of the continents back to about 650 million years ago. And if you dig around a little, you can find proposed continental positions even earlier than this. ",null,5,cdgm5kl,1qu6b0,askscience,new,32
Osymandius,"So by and large the majority of ""substances"" which break down other molecules are enzymes. Enzymes are complex proteins that are very difficult to ""design"". 

Lifted from wiki, the active site of ethanol dehydrogenase:

The active site consists of a zinc atom, His-67, Cys-174, Cys-46, Ser-48, His-51, Ile-269, Val-292, Ala-317, and Phe-319. 

The zinc coordinates the substrate (alcohol). The zinc is coordinated by Cys-46, Cys-174, and His-67. 

Phe-319, Ala-317, His-51, Ile-269 and Val-292 stabilize NAD+ by forming hydrogen bonds. 

His-51 and Ile-269 form hydrogen bonds with the alcohols on nicotinamide ribose. 

Phe-319, Ala-317 and Val-292 form hydrogen bonds with the amide on NAD+.

So - we have a coordinated metal ion centre which acts as a redox centre. We have 9 amino acids which are by no mean consecutive; they're scattered across the protein structure and only brought together in 3D space by the complex tertiary folding procedure. This is very difficult to predict for cytosolic proteins. We also need to create a site which accepts both ethanol and our electron donor (NADH) in close enough proximity for one to attack the other. Once (hypothetically!) we've managed to do all of this, we need to add control mechanisms to integrate this new enzyme into metabolic pathways. It needs to be turned on and off as required and it needs to be transcribed and translated as required. This means we need to integrate its stimulus into a receptor's function and subsequent signalling pathways.

I've skipped over a great many steps, but this should give you a rough idea of how complex our systems are and how difficult tinkering with them is. If you're interested - have a look at the bioengineering attempts at modifying Rubisco, a key enzyme in CO2 fixation in plants. It's a very slow enzyme and the idea is if we speed it up/decrease the back reaction we could dramatically increase plant yield and solve the food shortage we will shortly be facing.

If you'd like to know more, reply and I'll see what I can do.",null,3,cdgki64,1qu7az,askscience,new,8
Jobediah,"Respiration in water is more energetically costly that breathing air for two main reasons. First, water has way less oxygen in it than a similar volume of air. This means that an organism has to move more fluids around. Second, water is heavy and viscous (or sticky). Therefore, it requires more energy to move a given volume of water than air. Water must be sucked or pumped through gills which typically have tiny slits and the resistance is very high. So aquatic organisms must allocate a larger percentage of their overall energy budget to respiration than terrestrial organisms.",null,0,cdgth91,1qu7b1,askscience,new,4
Mazetron,"It's not that they would ""go faster or slower"", the speed is controlled by the driver.  The car would be able to accelerate faster, though.  That means if you were to slam on your brakes, the car would come to a stop more quickly.  Also, you would be able to slam on the gas harder before your tires start slipping.",null,0,cdgth0x,1qu8j5,askscience,new,4
bellcrank,"A hypothetically gigantic raindrop would experience stress during descent that would split it into smaller drops.  There's a tendency for drops to be large enough to fall, but not so large that they are forced to split, narrowing their size to a specific range.",null,2,cdgn57s,1qubr8,askscience,new,23
null,null,null,3,cdgly7l,1qubr8,askscience,new,4
dbaker102194,"The polar forces of water are what hold water together and give it surface tension and all that observable stuff. But eventually the mass becomes too great, and there are too many air molecules buffeting it, for the hydrophillic force of water to hold itself together. Water droplets appear to be around the optimum size for water to travel through the air at the air density near ground. ",null,2,cdgndqt,1qubr8,askscience,new,3
Saelyre,"Because when they reach a certain size they can't stay suspended in the air and start falling.

Source: http://ga.water.usgs.gov/edu/raindropsizes.html",null,4,cdglgm4,1qubr8,askscience,new,4
7LeagueBoots,"Having two nostrils may be more a product of the evolutionary parsimony of the bi-laterally symmetrical body-plan than of any advantage of two over one.  With a very few exceptions we (and other animals) exhibit a bi-lateral body-plan because it is easier to code for in an evolutionary sense.

The nostrils are close enough together that it is unlikely that smell directionality is a factor for most species, especially considering the way scents diffuse and how slowly they spread, unlike sounds which maintain their directional nature due to the speed at which they travel.",null,1,cdgr71l,1qubtr,askscience,new,8
ragingclit,"Having two nostrils allows animals to assign directionality to perceived smells. Rather than just detecting the presence of a smell, animals can identify which nostril the smell is stronger in, and by how much, and use this to determine the direction that smells are coming from. The same basic thing applies to ears and sounds.",null,7,cdgmyo7,1qubtr,askscience,new,8
ShazbotSimulator2012,"Each nostril functions differently. At any time one nostril tends to be narrower than the other due to increased blood flow, making it function better for detecting certain odorants that are aborbed more slowly, that would pass through the high-flow nostril too quickly. Interestingly, which nostril has more airflow tends to change throughout the day.

http://news.stanford.edu/pr/99/991103smell.html

[and a video from Vsauce that summarizes the above study](http://www.youtube.com/watch?feature=player_detailpage&amp;v=eiAx2kqmUpQ)",null,0,cdh0lvu,1qubtr,askscience,new,2
loctopode,"Two nostrils have a larger surface area than one. The function of nasal hair has been suggested to prevent particles entering and so a larger surface area would be more efficient.
Some people suggest that the two nostrils are better at detecting different odours, or that it helps with detecting odour direction.",null,0,cdiedwy,1qubtr,askscience,new,2
endocytosis,"Yes.  You can see them with a light microscope but need to use high magnification.  A video is [here](http://www.youtube.com/watch?v=vvnEsOaKxuw).
As far as fertility goes, there's a general formula for sperm motility (%motile/%nonmotile)/field, but I don't know it off the top of my head. Other factors like Calcium and other ion content of semen, and DNA integrity of the sperm are [important](http://humrep.oxfordjournals.org/content/28/1/274.short) indicators, not just motility.

EDIT: Would not recommend trying to ejaculate into a slide and viewing under a microscope.  See a doctor if you're worried or curious about your health.",null,0,cdglku2,1quet2,askscience,new,17
Eldritter,"You don't need to ejaculate onto a slide.  You need about 1 microliter of semen on a slide, probably along with a very small drop of water or saline to observe the motile cells via microscopy.  

For context this means that you could just touch your finger to the slide w/ semen on it and it would have thousands of sperm.  You can observe the cells for sure at 400-600x mag quite well.  Probably at 100x you could see spots moving.",null,0,cdgpmoa,1quet2,askscience,new,5
prooveit1701,"Depends on the power of the microscope but probably yes. You can predict fertility based on the mobility of the sperm cells however, other factors effect the quality of sperm that are not necessarily visible - such as genetic defects etc",null,1,cdglex8,1quet2,askscience,new,5
swollennode,"Yes, actually. This is how they do male fertility test. They ask the men to masturbate into a cup, then a lab tech take a drop of the semen, put it on a slide, then cover it with a cover slip, put it under a microscope and examine for motility. They also look for sperm morphology. Like is the head normal? Is the tail normal? is it swimming normally?",null,0,cdh2qto,1quet2,askscience,new,1
Javi2639,"Hybrid orbitals don't actually exist. At the time of their postulation, it was the best theory to explain what was going on. The current accepted theory is called molecular orbital theory, which is much more complicated, but explains what we observe a little closer than hybridization theory. However, since hybrid orbitals are easy to understand and explain, we use them for most purposes. It's like with Newton's and Einstein's theories of gravity. Technically, Newton's equations are incorrect, and Einstein's are closer to what we observe, but since Newton's are so close, we use them for most practical purposes and only use Einstein's when we have to.",null,4,cdgmb7j,1qufpd,askscience,new,11
bohr_exciton,"The orbitals you are describing are called hydrogen-like orbitals. These orbitals are only the exact solution to the simplest system to which they can be applied- a centrosymmetric system of two oppositely charged particles, such as the hydrogen atom. Even in similar system, such as heavier atoms, these orbitals are no longer exact because of interactions between multiple electrons. In molecules  there are even more complications from the presence of additional nuclei in addition to the extra atoms.

However, even in these more complicated systems, we still use the language of hydrogen-like (or atomic) orbitals. The reason for this is twofold, 1) this practice can greatly help our intuition and 2) these orbitals are a convenient starting point for calculations. In regards to the first point, the case of the ""hybrid"" orbitals is a good illustration of the convenience of using the hydrogen-like orbitals. It is fairly straightforward to think of an sp2 center, say in a carbon double bond, as consisting of a mixture of one s and 2 p orbitals. While this description is definitely not exact, it nonetheless give quite a bit of intuition regarding the resulting physical and chemical properties of that center, from its geometry to its reactivity. 

As for the second point, it turns out that in many systems, the atomic orbitals turn out to be very good building blocks (more technically a good basis set) for describing the electronic structure of more complicated systems. The idea here is similar to hybridization, but more general and systematic. For example the electronic density at some point in space could be described as 0.5C_2pz + 0.2O_2s +..., in this case showing that it's a superposition of electronic density of a carbon 2pz orbital and an oxygen 2s orbital and so on weighted by certain numerical coefficients. This picture is especially convenient when atoms interact rather weakly with one another, however it is always valid. In principle one can create a complete basis set out of specific combinations of these atomic orbitals, and could thus exactly describe any physical system.",null,0,cdgnebw,1qufpd,askscience,new,9
defrandymoss,"Hybrid orbitals are useful for explaining bond geometries, but to get geometries of these we must know the shapes of the original atomic orbitals.
A more current and accurate view of bonding interactions is molecular orbital theory, where a linear combination of atomic orbitals gives the properties (including geometry) of the bonding molecular orbitals.
Both of these theories are built on first knowing the properties of the atomic orbitals, which you get from solving Schrodinger's equation in spherical coordinates. The atomic orbitals are the more fundamental entities, so we must ""go through"" them before moving on to the more complicated bonding interactions between atoms",null,0,cdgmuos,1qufpd,askscience,new,5
snusmumrikan,"Venemous =/= poisonous. 

Venom is injected, whereas poison can be ingested/absorbed etc. So by eating the animal, the honey badger swallows the venom sacs or whatever the venom is in and it is processed, diluted, broken down or excreted like all other food. 

The venom of a snake would be dangerous to it if it injected it into the honey badger's blood stream (this actually happens, but the honey badgers are also really tough and usually pass out, sleep and then wake up to eat the snake afterwards!).",null,2,cdgnfus,1qufwl,askscience,new,19
bellcrank,"While there is some minor justification for believing that urbanization can cause storm-splitting, the more likely scenario is that you are simply paying attention to Chicago, so you notice every time it doesn't get hit.  If you paid close attention to a particular (arbitrary) location on a dart-board, you'd notice it never seems to get hit.  It's just an observation bias.",null,9,cdgn3jk,1quhk6,askscience,new,44
ACuteMonkeysUncle,"http://www.isws.illinois.edu/atmos/statecli/tornado/NewMaps/IL-Tornadoes-13.png

Here's a map showing every tornado in Illinois from 1950 to 2010. As you can see, Cook County and the city seem a bit ""underrepresented"" tornado wise. There are others as well, though, such as DeKalb county don't have too many tornadoes either. 

So, part of it is confirmation bias on your part. But still, it seems that just to the south in Will County there are a whole bunch of tornadoes, including that pink one, which must be the Plainfield tornado of 1990. That, I cannot explain. It might just be a fluke, it might not. I will defer to the expertise of others, here. ",null,1,cdgq8xv,1quhk6,askscience,new,13
KarlOskar12,"Depending on the textbook or other source you look at the number varies typically between 28 and 32 ATP. The low 28 takes into account the amount of energy required to move the NADH from the CAC into the mitochondria. Some also attribute varying amounts of ATP production to the proton gradient during the ETC. Nothing else is happening to make it produce less ATP, some people just crunch the numbers differently.",null,0,cdgqipx,1qumwp,askscience,new,2
Osymandius,"Fairly crucial point:

3 ATP synthesised per rotation of ATP synthase. Number of protons required to rotate ATP synthase through 360o depends on the number of c subnunits (varies between 8 and 12 from organism to organism). This means in 1 organism if you've moved the same number of protons you might not get the same number of ATP as another organism with **exactly** the same number of protons in the intermembrane space.",null,0,cdh2kol,1qumwp,askscience,new,2
codyish,Mitochondria can vary in efficiency by losing (leaking) electrons that travel through the electron transport chain or by having protons leak through the membrane. I know that there are several factors that can cause this but the one I've seen studied the most is variation in membrane composition. ,null,0,cdh7bip,1qumwp,askscience,new,2
Platypuskeeper,"Well, if you start with the [RNA world hypothesis](http://en.wikipedia.org/wiki/RNA_world_hypothesis), you only had RNA in the beginning. Note that DNA is synthesized from RNA, more specifically Ribonucleotide Reductase turns ribonucleotides into deoxyribonucleotides, which is not an very easy chemical reaction.

So why not just continue to use RNA for everything? Well one of the key differences here is that DNA is more stable, especially if the chains are long. So it makes sense that it'd be worth the energetically-expensive of producing DNA in return for this increased stability, and increased ability to form more complex life forms. 

So you might flip the question around here: Using DNA for storing the genome makes sense, then. But is it really necessary for transcription and all that? Since they use shorter strands (and the empirical fact this doesn't happen), it seems there wasn't any benefit in using DNA for more than what it's used for. 

Those are just the broads strokes though, I'm sure a biochemist can fill in more details.
",null,3,cdgqpyq,1qunlm,askscience,new,6
KarlOskar12,"You have a lot of great questions here so I'll try to answer a couple them as best I can. My organic chemistry is pretty terrible so I'll try to avoid it as much as possible. DNA is an extremely stable molecule which makes it perfect for storing genetic information. mRNA is a very good messenger because it is degradable. This is optimal because not all proteins need to be made all the time. The longer the mRNA is around, the more protein will be made. If it were mDNA being made then cancer would be much more prevalent as a result.",null,2,cdgqtcp,1qunlm,askscience,new,3
redappless,"In a recent study they had suggested that it was more probable that carnivores had originally consisted of a omnivorous diet. Shifting from a carnivore to a herbivore over time seemed impossible due to the dramatic structural changes a species had to take over time. http://www.sciencedaily.com/releases/2012/04/120416154417.htm

Survival of the fittest due to environmental changes and competition would definitely have a role in the diet of an animal. As carnivores originally had an omnivorous diet, the type of food they had more access to (less predators, more abundance) probably influenced overtime the adaption of their digestive system to that type of diet. As they had an omnivorous digestive system, adapting and developing specialised anatomical and physiological characteristics to either a carnivore or herbivore overtime wouldn't be so hard.",null,0,cdgrok2,1qusb3,askscience,new,8
Owl_,"Higgs bosons don't maintain the Higgs field, nor do photons maintain the electromagnetic field, etc. These particles are excitations of their respective fields. The fields exist whether there's a particle there or not. 

Not really. We have a pretty solid idea of what the Higgs boson is like. We could, of course, be wrong about something or have yet to discover something new about it, but that's unlikely. 

The Higgs field is an all-permeating field. That description was redundant, though; a field, by definition, exists in all space and time. There's no escaping it. ",null,2,cdgzde9,1quzsd,askscience,new,10
quarked,"Be careful when you say the Higgs boson ""maintains"" the Higgs field. This isn't really a good description of what the Higgs is or does.

Like others have mentioned, the Higgs boson is just an *excitation* of the Higgs field. Similarly, there are other gauge bosons which mediate their respective forces. However, and this is the important part, particles do not need to interact with Higgs boson to interact with the Higgs *field itself*.

In the standard model, all the fundamental forces are *mediated* by the gauge bosons which ""carry"" their respective force, but that is different from the Higgs interaction, which gives most particles mass because the Higgs field has a non-zero expectation value. ",null,1,cdh0x0p,1quzsd,askscience,new,7
LazinCajun,"To add to owl's comment, The Higgs boson is unstable.  It decays *very* quickly compared to human detectable timescales.

Broadly speaking, the LHC deduces the existence of the Higgs (and many other unstable particles) by examining the decay products very carefully over a large number of collisions.",null,1,cdgzisl,1quzsd,askscience,new,5
MCMXCII,"The Higgs boson is an excitation of the Higgs field just like the photon is an excitation of the EM field. But unlike the massless photon, the Higgs boson is *very* heavy, which means its a lot harder to make than a photon. That's why we need very energetic collisions in the LHC to find Higgs bosons.",null,0,cdh3wom,1quzsd,askscience,new,2
do_od,"There are many factors you have to consider when taking digital cameras into space. First, you have the radiation environment that can be damaging to electronics. There is also a phenomenon called outgassing that can occur in the vaccuum of space. What happens is that gasses, solvents and moisture evaporate from materials, in particular from electronics, adhesives and such, which can form [deposits on the optics of your camera](http://en.wikipedia.org/wiki/Outgassing). Most batteries, and in particular the commonplace LiIon type, have bad performance in low temperatures and that can also be a problem in space. 

Cameras intended for space duty are built with special [radiation-hardened](http://en.wikipedia.org/wiki/Radiation_hardening) circuits and even metal radiation shielding of critical parts. Outgassing can be mitigated by pre-treating components by heating them under low pressure, ""boiling off"" any solvents and moisture trapped in the material. To get reliable operation from the batteries it might be necessary to have special equipment to keep the batteries at acceptable temperatures, or use other power sources.

That being said, a standard camera might work to snap a few selfies on the moon, but it is also possible that it wouldn't even survive for that long. Even if you can take a few pictures, the quality may be reduced due to condensed moisture on the optics and data corruption from radiation damage. Reliable operation is very improbable because the same technology that make electronics so cheap also make them less radiation resistant. 

A film camera on the other hand will usually work. NASA used medium format [Hasselblad cameras](http://en.wikipedia.org/wiki/Hasselblad#Hasselblad_cameras_in_space) for the Apollo missions and they performed very well. Eliminating the electronic parts also eliminates most of the problems with taking pictures in space. 

e: Added some wikipedia references.",null,0,cdh14ai,1quzsj,askscience,new,9
aussiekinga,"I can't see there being any problem. Digital cameras work by taking not of where light hits on an electronic plate. As light still travels in space, being a wave, it is still going to hit on the plate in the same manner. The camera would then experience the same thing.

A film camera, I imagine would be similar. It has chemicals that react when light fits the film. Again, as the nature of light isn't different the way it interacts with the film wouldnt be different. 

I guess exposure of the film or the camera itself to vacuum could cause an issue, but a clear air tight case (like for underwater photos) should be enough to fix that.

The only way it would not work is if the nature of light is different. While it can't be effected by physical objects (refractions etc.) or gravity I'm not sure that there would be any major different in space. I guess you would guess less solar flares on the lens?",null,1,cdh0wgr,1quzsj,askscience,new,1
thewetness,"From a mechanical engineer's perspective, this statement isn't totally true.   It assumes that the instrument is right at capacity, meaning that it is right about to break due to the tension. Most things have what we call a factor of safety incorporated into their design. The factor of safety is basically the ratio of how strong it actually is (the maximum load it could survive) divided by how strong it needs to be (the maximum load it will actually experience in its use). So when it is at capacity, it has a factor of safety of 1. So if it experiences any load higher than intended, it will fail. However, when you have a factor of safety greater than 1, say 2, then increasing the load (tension) by 25% will not cause it to break.

Also, it says every part of the instrument needs to be stronger, it ignores the possibility that one part of the instrument is ""weaker"" than the rest. To get an idea of this, consider a tensile testing ""dogbone"" specimen: http://www.faimaterialstesting.com/site/images/physical/comp_tens/dogbone_sample_caliper_2.jpg

Basically, the ends are much thicker than the middle so we know where the specimen should break when we pull on the ends. The instrument could have a section that is the weakest link, and even that section would likely have a factor of safety.

From a general engineering standpoint, if something is going to experience 25% more load, then if we want to keep the same factor of safety, then we should increase strength by 25%, but usually, we have the factor of safety because we simply can't be sure what the maximum load is going to be. So we don't always have to worry about making things 25% stronger for most applications. High performance products like those in the aerospace industry have low factors of safety, near 1, because higher factors of safety mean higher weight. For musical instruments, worrying about weight isn't our primary concern, so I would expect to see this logic in instrument design.",null,0,cdh029z,1qv0py,askscience,new,4
datums,"The problem here is that Double basses do not have adjustable truss rods.   
  
With a normal guitar or electric bass, the neck is rather flexible. The tension of the strings want to make the neck to bend forward, becoming concave, while the neck wants to bend backward, becoming convex. When properly adjusting such an instrument, one must balance these forces, using a *truss rod*. The truss rod runs through the neck and allows the curvature to be finely adjusted. Properly adjusted, the neck is slightly concave, rather than straight. This gives the strings room to vibrate without hitting the fretboard.   
  
If the truss rod is adjusted improperly, two things can happen. If it is *too* concave, the strings will be high off of the fretboard, making the instrument difficult to play, though it may sound great. If the neck is too flat, or even convex, the strings will strike the fretboard when plucked, resulting in a horrible sounding and unplayable instrument.   
  
As I said above, the problem with a double bass is that they do not have adjustable truss rods. The instrument has been made to have a certain curvature given a certain tension, and this cannot be altered. Though damage may not occur, the playability of the instrument will be compromised if string tension is significantly altered.    
  
Having read your post again, I see that I am hopelessly off topic. ",null,0,cdh97er,1qv0py,askscience,new,5
wazoheat,"Clouds are made of very small particles of water and/or ice, typically around [20 micrometers (0.00079 inches) across](http://apollo.lsc.vsc.edu/classes/met130/notes/chapter7/ccn_drop_prec.html). At those particle sizes, light doesn't interact the same as it does with a single, uniform object. Light traveling through clouds undergoes a process known as [Mie scattering](http://hyperphysics.phy-astr.gsu.edu/hbase/atmos/blusky.html#c3). Light is scattered in all directions by this process, though it is primarily *forward-scattered*, meaning the light generally continues in the same direction it came from, though at an altered angle. Each photon of light reaching your eyes through clouds has likely been ""bounced around"" several times before it reached your eyes, which is why you can see light coming through the clouds but you can't see what is behind the clouds.

The thicker (in depth) a cloud is, the more the light is scattered before it can make it through. This is why ""thin"" clouds seem quite wispy and translucent, while thick clouds appear dark from below.",null,1,cdh0adf,1qv0zy,askscience,new,4
quantum_lotus,"To add to what others have said here, plastids (mitochondria and chloroplasts) once had complete genomes, but over time most of the genes that started out in the plastids have been moved into the nuclear genome.  For a sense of scale, mitochondria have about 1,000 proteins (gene products) working inside of them, but human mitochondria only encode 13 genes in their genomes.",null,0,cdh5yfb,1qv58e,askscience,new,6
DutchmanIII,"No, that wouldn't work. As you say, it is theorized that mitochondria and chloroblasts were individual organisms and that they formed a symbiosis with other cells. The fact that they still contain fragments of their own DNA supports this theory.

Through evolution, these organisms have come to specialize in a certain task to their host cell (e.g. production of ATP) and have given up other functions essential to survival.",null,0,cdh2pj2,1qv58e,askscience,new,4
sporclesam,"The [endosymbiotic theory] (http://evolution.berkeley.edu/evolibrary/article/history_24) states this. As DutchmanIII mentioned, these organelles have given up their extraneous functions and evolved into single-track organelles, yet hang on to bacterial ancestry such as its genome &amp; ribosomal machinery. As an endosymbiont evolved into an organelle, most of its genome [transferred to the host cell] (http://www.ncbi.nlm.nih.gov/pubmed/18430636).",null,0,cdh56on,1qv58e,askscience,new,3
datums,"The consensus seems to be that prosody, the melody of the word, is more important to a dog than the complex series of sounds that actually make up the word. It follows from that that the language is not particularly important. What matters is which words are chosen, and how they are pronounced. People tend to talk to their dogs in a particular tone, with exaggerated emphasis, and that it what they are able to recognize. ",null,0,cdh9fqs,1qv865,askscience,new,2
iorgfeflkd,"They also rotate, and the rotation causes them to bulge out at the equator. That's why large bodies are oblate spheroids.

Objects smaller than about 200 km in radius don't have enough mass for gravity to overcome the inherent rigidity of the materials, so they stay aspherical.",null,0,cdgy59u,1qv88y,askscience,new,17
FewRevelations,"Gravity from the planet's star only pulls on one face of the planet at a time, which, combined with an elliptical (rather than round) orbit, makes the planet more oblong (in more extreme gravitational conditions this can cause tidal heating, like on Io).",null,5,cdgyh7c,1qv88y,askscience,new,4
datums,"Slightly besides the point, but not all IMAX cameras use film. There have been digital IMAX cameras for about 5 years, and newer digital IMAX cameras can shoot in 4k resolution.    
  
As for how digital effects are added to analog film - the film is scanned, frame by frame, into the digital domain. Once in the digital domain, all the computer effects are added. Then a digital print of the film is produced. This is analogous to scanning a picture of an old family picture, photoshopping an alligator in, and then printing it on your inkjet printer.  ",null,1,cdh8e9b,1qv9oz,askscience,new,5
D_I_S_D,"You seem to be under the impression that a cinema has separate projectors for the movie and for the CGI, this is not the case.

Normally CGI effects are added to film in the post production phase of the movie, after the physical shot has been taken. The script may call for ""Panning shot with giant robot"" so the director will film the ""Panning shot"" and the ""giant robot"" will be added later during the editing phase essentially ""frame by frame"" using a digital image manipulation program.

Once the final cut of the film is decided upon the cut is then printed onto film to be distributed to the cinemas. The production houses can produce films compatible with the cinema's projector, so for an IMAX movie they just make an IMAX compatible print.

Due to their cost/size IMAX cameras are not used too often, even ""Dark Knight Rises"" is only approximately 1/3 genuine IMAX shot footage, the rest being regular footage that is later re-cut and printed to fit the IMAX format for the IMAX cut of the film.",null,3,cdh1s1d,1qv9oz,askscience,new,6
iorgfeflkd,"That terminology is generally used to refer to nuclear reactions like in bombs and power plants. Basically, a large nucleus, like uranium, breaks apart into several smaller nuclei and neutrons, all of which are moving very fast. This can either happen spontaneously or can be triggered, like when Uranium is hit with a neutron. [Here](https://en.wikipedia.org/wiki/File:Nuclear_fission.svg) is a schematic.",null,0,cdgvu0k,1qva0j,askscience,new,3
TheMac394,"To go into a tad more detail than others here, let's step back and look at how atoms are put together.

The nucleus of an atom - what we're concerned with - is made of protons and neutrons. Now protons are positively charged; as you might now, positive things repel other positive things. If you get a bunch of protons and neutrons together in a nucleus, they'll all repel each other - this is the electromagnetic force; if this was the end of the story, well, we wouldn't have any atoms. Luckily, there's something called the *strong nuclear force*. The strong force is, as the name suggests, very strong, but acts only over a very short distance. If you get protons close enough together, though, the strong force will become enough to hold them all together.

Having a nucleus, then, is a matter of balancing out the strong force and the electromagnetic force: if the two are equal, you'll be stable. To make some nuclei more stable, you can also add neutrons. Neutrons also feel the strong force, but don't feel the electromagnetic force, so they generally act to increase the force holding things together.

Now, if an atom doesn't have the right ratio of protons to neutrons, it won't be stable. In this case, it will be radioactive, and will decay, emitting some radiation in the process to get rid of energy and become more stable.

What if you're right on the brink of stability, though? You've got the right ratio to hold the nucleus together, but any small change would throw that ratio off. In this case, you have the possibility for *fission* - splitting the nucleus.

Fission works like this: a borderline-stable nucleus will get hit with a neutron and absorb that neutron. The neutron's energy causes the nucleus to bulge out, like the second picture [here](http://upload.wikimedia.org/wikipedia/commons/thumb/e/e0/Stdef2.png/150px-Stdef2.png). Now, remember that the strong force - the one holding things together - acts over a very short distance. When the nucleus bulges out, the strong force can't reach all the way out to the edges of the bulge to pull them in; since our nucleus was very close to unstable to begin with, this turns out to be the final push it needed. The new nucleus can't hold itself together, and will basically break in half, as depicted in the picture above.

It's worth noting that this can - theoretically - happen to pretty much any nucleus; it's just a matter of getting enough energy in your neutron to cause fission. Practically, however, there only a few nuclides for which this is possible; specifically, we usually deal with Uranium and Plutonium.

Now, in the case of Uranium and Plutonium, since they're heavy and not super-stable, they release energy when they break apart; the theory behind this is complicated, just know that ""stable"" is more-or-less equivalent to ""low-energy"", and if a nucleus becomes more stable, that energy has to go somewhere. This is where the gamma radiation that we associate with nuclear reactions comes from - it's essentially pure energy being carried away from the nucleus as it splits.

Fission also produces a few free neutrons, which don't become a part of either of the halves of the splitting atom. Remember that fission was initially caused by a neutron hitting our atom - if we've got a bunch of fissionable atoms, and each fission produces a neutron, then those neutrons can go on to cause other fissions, causing a chain reaction. This is how nuclear reactors and nuclear bombs work: you cause a lot of fissions over a very short time, which releases a *lot* of energy. That's why most people associate ""splitting an atom"" with explosions; in reality, splitting only a single atom will produce some energy and radiation, but won't immediately result in a massive mushroom cloud.",null,1,cdhbzh0,1qva0j,askscience,new,4
wesTRONcorp,"I like all of the comments so far.  Simply though and stepping back to ""what is an explosion?""... a rapid increase in volume and energy.  This brings us to why nuclear fission is so energetic.  When an atom splits, what was previously held ever-so-tightly together--the atoms nucleus--by the strong force is now suddenly released.  The nucleus must now expand immensely and take up more volume than a tightly bound nucleus.  Furthermore, all those protons/neutrons are able move independently with more possible motion when broken apart.  Also, the energy (strong force) holding the nucleus together must be released....etc...",null,0,cdh1utd,1qva0j,askscience,new,2
tea-earlgray-hot,"When discussing porous materials surface areas, we normally speak of gas-accessible areas. This is the ""real"" surface area, that stuff can actually get to and interact with.

More formally, we measure the amount of gas adsorbing to surface of the sample, then back-calculate the area using the size of the gas molecule and the mass/volume of the material. This is normally measured as an isotherm, and modeled with Langmuir or BET theory.

Edit: It's probably relevant to mention that if your surface is electrochemically active, your can use this to determine surface area instead. It's more practical for very small samples (&lt;25mg), but is somewhat poorly defined.",null,0,cdh2g06,1qvacl,askscience,new,3
iorgfeflkd,"I don't know if it's the case for this specific device, but capacitors often have many parallel layers of material inside them. If you add up the surface area of each layer, it is much greater than the cross sectional area of the device. For example, if there are 10,000 sheets of graphene each of which are a square centimeter (just making these numbers up, don't know the exact details), it will have a surface area of a square meter.",null,1,cdgy707,1qvacl,askscience,new,2
dampew,"In terms of ""how is this possible"", remember that there are mathematical objects like fractals with infinite surface area and finite volume -- if the surface is complex enough, the surface area can be huge.

In this case, the surfaces are bunches of graphene sheets.  The basic idea, I think, is that they take graphite or thick graphene and oxidize it -- the lattice is filled with oxygen everywhere.  When they remove the oxygen the whole thing flakes apart and forms pores at the nanoscale.",null,1,cdh1fvh,1qvacl,askscience,new,1
therationalpi,"The answer to this may seem straightforward, but it's important to remember the very long history that records had. Over the years there were several ways that we converted the grooves in the record to sound. I'm gathering a lot of information from the [relevant Wikipedia article](http://en.wikipedia.org/wiki/Turntable#Pickup_systems), but I'll supplement it with my own knowledge as appropriate.

Before getting started, it's worth going over what the ""bumbs"" in a vinyl record are. The groove of a record is a time-record of the acoustic pressure for a musical recording. In general, these recordings are built from combining multiple recordings off microphones and mixing them into a song. The job of the recording engineer was thus to record these sound by picking the right mics and placing them in the right places, as well as balancing the levels of the many tracks and applying any effects necessary. An additional job was to be mindful of the limitations of the record itself, since vinyl records didn't have as large of dynamic range as modern media (Though admittedly we don't take advantage of that dynamic range anymore either).

The earliest record players were completely mechanical. A metal (or diamond tipped) stylus  would sit in a groove and follow them up and down as the record turned. In earliest devices, the stylus was directly connected to a stiff diaphragm. The change of surface area from the stylus tip to the diaphragm made this effectively a mechanical-acoustic transformer, changing the mechanical motion of the stylus directly into acoustic waves. The diaphragm would be augmented with an acoustic horn, which would help the waves from the diaphragm couple better to the air. This was, essentially, a pure acoustic transformer meant to match the mechanical impedance of the diaphragm to the acoustic impedance of the air.

Later phonographs used electric transduction. At this point, the tone-arm comes into play. Essentially, the tone-arm is a mechanical lever attached to the stylus. When the stylus moves, the tone arm transfers the stress to a cartridge containing the pickup. The second generation of record players relied on piezoelectric materials for pickups: materials that respond to mechanical stress by generating an electric potential (voltage). This voltage could then be amplified and sent to a regular loudspeaker. The first ones used free floating piezoelectric crystals (like a sand), but had very poor quality because of non-linearity of the transduction. Later loadspeakers used piezoelectrics suspended in a ceramic material, which performed much better.

The next generation of pickups are very similar to the standard moving-coil loudspeaker, but in reverse. The tone are moves a magnet in a magnetic field, which generates an electric current that can be amplified. Alternatively, the tonearm moves the coil around a magnet, to the safe effect. The only difference here is wether the magnet or coil weighs more, which essentially depends on how many wraps of wire the coil is made out of. If you are curious about this type of pickup, just look up loudspeakers to get a general idea.

Between magnetic and piezoelectric charges, you've probably got most of the record players that people used. Some later models used strain gauges, where an electric resistance is altered by the strain put on the device. This resistance could be placed in a network of resistors with a constant current (like a Wheatstone Bridge), and the differential current could be used to measure sound.

More interesting are the optical methods that developed late in the history of records. These are called ""no-touch"" sensors, and they are still the preferred method for anyone who doesn't want to risk damaging their rare records, like collectors or archives. A classic no-touch sensor uses a laser reflecting off the grooves to measure their depth. These are used in place of a standard stylus on a tone-arm, and the record player looks quite similar to a standard record player. Alternatively, with enough resolution, you can simply scan the record player like you would a document. The grooves are then read directly from the image and converted to sound. This is the preferred method for archives, like the Library of Congress, because the sound of the recording can still be enjoyed without risking the physical record itself getting damaged. It does, however, require very high quality scans to be effective.

Please feel free to ask any other follow-up questions, since most of this info was just the basic stuff from Wikipedia!",null,2,cdh512s,1qvamu,askscience,new,5
ModernTarantula,We have a small muscle attached to the eardrum when it pulls tight it makes the eardrum vibrate less. That gives us its latin name the tensor tympani. It makes the tympanic membrane tense. It has a reflex to tighten with loud sound. It protects your ears from a sharp noise.  However if the loud noise continues it will stay tense long after.  Paralysis of that muscle makes people sensitive to loud noises (hyperacousis),null,14,cdh0iby,1qvdn1,askscience,new,44
maleslp,"Hello! Speech-Language Pathologist here. I believe what you're referring to is called temporary threshold shift (TTS). This is a phenomenon where the blood vessels which supply blood to the little hairs in the ear which, in turn, send a signal to the brain to interpret the energy into sound (they're called cilia) become constricted (vasoconstriction), thus inhibiting the signal and causing the brain to interpret the new ""pattern"" as a ringing. This can last anywhere from a day to a week, the former more common. Caution! Not giving the ears enough time to recover CAN result in PST, or permanent threshold shift (a.k.a. tinnitus). If the cilia don't have enough time to recover, they WILL die and do NOT grow back like normal ""hair"". As far as I know, we're not sure why the lack of energy coming from the cilia is interpreted as a ringing. 

That was a pretty quick explanation; I'm sure an audiologist would be able to provide more technical insight. 

Here's a source link: http://www.sfu.ca/sonic-studio/handbook/Threshold_Shift.html",null,3,cdh3yqz,1qvdn1,askscience,new,27
Anacanthros,"In terms of hearing loss, which may not be precisely what you're asking about:

Inner hair cells in the organ of Corti (the sound-detecting part of the cochlea) are being shorn away.  When sound reaches the cochlea, it moves segments of the basilar and tectorial membranes, which are connected by the kinocilia of inner hair cells.  This movement stretches the hair cells' cilia, and the stretching is what causes electrical signals to be generated and sent to the brain.  When the movement is too severe (a consequence of loud sounds), the cilia can break.  They don't grow back.",null,3,cdh3vj9,1qvdn1,askscience,new,7
null,null,null,7,cdgz10c,1qvdn1,askscience,new,7
wyliee,"Yes.  Yes is the answer to your question technically, however let me start with some definitions to be sure I am answering the question you are really asking.  A gene is the common term for the part of the DNA that gets transcribed through the process of transcription.  What we most commonly think of are protein coding genes.  These are transcribed then translated to make a protein.  However, a gene doesn't have to be protein coding.  There are genomic regions that get transcribed, but don't get translated.  For example, there are many genes that encode entities that function as RNA molecules.  These are transcribed from DNA into RNA, but are not translated. They are still functional, they play a regulatory role as an RNA molecule.  Since they are never translated into a protein, they don't need stop (or a start ) codons.   Most of the ribosome itself, which translates the RNA into protein (where an RNA needs the start and stop codon to be properly processed) is made of these enzymatic RNAs.  So the technical answer to your question is yes, you can have a functioning gene without a stop codon.

If you are asking only about protein coding genes, these do not need to have a stop codon to be transcribed into RNA, but do to be their properly functioning protein.  It is possible for them to be processed by the ribosome. What typically happens is that the ribosome will just keep going along till it hits the next stop codon or runs out of RNA.  This usually results in a nonsensical protein and can cause problems for the cell.
",null,1,cdgxkly,1qvf2d,askscience,new,9
ModernTarantula,". the stop for transcription are the [termination bases](http://www.chemguide.co.uk/organicprops/aminoacids/dna3.html). mRNA are further modified before translation, removing introns. They wrap and fold and are subject to degradation. A very large mRNA would likely be unstable and not usable for the ribosome.  Now lets say teh revers the termination sequence is before any stop codon. the protien would be truncated-- short.",null,1,cdh0ezl,1qvf2d,askscience,new,3
Memeophile,"Transcription termination is usually not far past the stop codon, so stop-codon read through is often not a big deal. Obviously there are exceptions. Amazingly, however, global read-through of specific sets of stop codons are pretty well-tolerated in E. coli, as evidenced by [the famous amber, ochre, and opal suppressor mutants.](http://en.wikipedia.org/wiki/Stop_codon#Amber.2C_ochre.2C_and_opal_nomenclature)

",null,0,cdh649o,1qvf2d,askscience,new,1
null,null,null,1,cdgzgt6,1qvfzb,askscience,new,7
dampew,"Well, there are a lot of composite particles that can form bosons.  For instance, Helium 3 also forms a condensate (superfluid).  Superconducting electrons form cooper pairs.  There are also lattice vibrations (phonons), electron oscillations (plasmons), electron-hole pairs, and so on that don't necessarily form condensates but do obey bose statistics.",null,1,cdh168l,1qvfzb,askscience,new,6
Fenring,"I'll add to the other responses by pointing out that, even within the elementary particles, the Higgs boson is not a gauge boson.",null,1,cdh1msx,1qvfzb,askscience,new,6
Aeolitus,"As a general answer: It isnt. 

What you need to consider, is that a boson is composed of fermions (if it is not an elementary particle.) All fermions adhere to the Pauli Exclusion Principle - but if they form a boson together, then they do not need to do so anymore, since the pauli principle is valid only for fermions, and they form a boson now. Thus, there is no Problem with BECs. Dont think of the Pauli Principle as something that prevents fermions from ever coming close to each other. It is a mathematical Rule that describes the behaviour of fermions. Nothing more. As soon as we are not looking at fermions anymore, Pauli's Principle is of no importance anymore, and this is the case in Helium-4.

Another Example is btw. Lithium-7, which we can condensate just fine and is a Boson: 3 Proton + 4 Neutron + 3 Electron, all fermions on their own, form a Boson. 

Source: Physics Grad Student, did my Bachelorthesis on Condensates. 

EDIT: 
As /u/dampew pointed out, we can make condensates of fermions aswell. However, this works by pairing the fermions to something between bosonic molecules and cooper pairs by tuning via the feshbach resonance, and is thus in a way just a condensate of bosonic molecules. If you wish to take a further look into this topic, I recommend looking into the publications of Prof. Ketterle from MIT. Especially the Doctor Thesis of Martin Zwierlein is highly recommended.",null,0,cdh2kqn,1qvfzb,askscience,new,4
readams,"Here is an article that discusses the common origins and subsequent independent evolution of multiple kinds of eyes:

http://www.nyas.org/publications/detail.aspx?cid=93b487b2-153a-4630-9fb2-5679a061fff7

There were some very simple common origins of eyes, but they mostly evolved independently.  Notably this tells us that eyes are actually not that hard to evolve.",null,0,cdgyafj,1qvio1,askscience,new,7
angry_squidward,"Some were convergent evolution. One example is the eye of a squid. They presume this because humans and our ancestors have a hole in the back of our eye that causes a blind spot while squids do not, so that leads to the speculation that they were formed differently and independently. Thats about all I remember from class.",null,0,cdgy2bp,1qvio1,askscience,new,2
lazy_smurf,"This is complicated because your question isn't entirely clear. If by ""health"", you mean weight gain/loss, it is very hard to overeat truly healthy food. have you ever tried eating 3000 calories of meat and vegetation? your stomach and satiety hormones simply won't let you. healthier food also tends to impact insulin levels and sensitivity, controlling blood sugar and thus, hunger.

If ""health"" for you goes beyond just weight control, there are many vitamins and minerals that we must eat to maintain proper health, and junk food tends to be extremely low in these, as well as plant-based compounds that are very good for you (think fruit phytochemicals and leafy greens). 

while it is true that the sheer number of calories matters for weight loss, it is clearly easier and healthier to eat higher-quality foods for these calories than lower-quality ones.",null,1,cdh625v,1qvipm,askscience,new,10
pairyhenis,"It's definitely not a dumb question. 

Some things to consider/bear in mind:

- Eating ""healthily"" is a poorly defined concept, and the many items can be both healthy in one regard and unhealthy in another (for example, the association of certain types of fats, omega-3's, with both improved metabolic health and a small increased risk of prostate cancer)

- ""Healthy"" eating is invariably dependent on how much of a particular nutrient you consume; compounds in red wine improve cardiovascular (heart and circulation) health, but drinking 2 bottles a night won't make you healthy. 

- Certain classes of compounds worsen metabolic measures.  For example, a type of fat compound very similar to the omega-3 fats (which are highly beneficial), the omega-6 fats, are linked to increased inflammation, which drives many disease states including obesity, diabetes and cardiovascular disease.  Trans-fat consumption is even more strongly linked to these disease risks.

It's also worth saying that calorie content is not accurate for many foods, due to the method of estimation used.  This often fails to accurately account for inefficiencies in digestion, which can, for example, be altered by the bacteria in the intestines (the microbiome).  The [wiki article](http://en.wikipedia.org/wiki/Atwater_system#Theoretical_and_practical_considerations_relating_to_the_calculation_of_energy_values) discusses some issues.
",null,1,cdh6e7b,1qvipm,askscience,new,7
w0den,"being healthy is not maximized by minimizing weight, thats the first point it guess, also every kind of ""unhealthy"" is only a matter of dosage. For example protein is considered healthy. But too much protein is unhealthy. Sugar is considered unhealthy but without it(or it being split out of more complex carbs) you have no short term energy. The only thing thats is kind of out of that principle is water since it can't be essentially unhealthy when consumed in mass but its unhealthy to consume too little. Also too little diversity in food can lead to not having enough substances to process certain types of food. I can't really describe this any better since english is not my main language.",null,0,cdh6wsm,1qvipm,askscience,new,2
davbob,"Food that is considered bad for you, cakes sweets and soda and the like, are said to contain empty calories. This means that they provide calories with very little other nutrients. Eating nothing but these will lead to malnutrition. Your bidy knows when it is malnourished and makes you hungry so you want to eat more. When you eat more you add more calories. Even if you do keep your calorie count eaten below your calories expended, if you arent getting the correct nutrients from your food the body goes into starvation mode and burns muscle before fat.  It also converts any left overs into fat to lay down a store in cade of emergency.",null,6,cdh1lhb,1qvipm,askscience,new,7
KarlOskar12,"Actually, eating [healthier foods](http://www.mayoclinic.com/health/junk-food-diet/MY01589) isn't obviously better. That study looks at a 10-week period, and all signs pointed to increased health even though he was eating a junk-food diet. The long term consequences aren't known on such a diet because people who eat like that eat far too much of it so they have way too many calories, and consume far too much cholesterol. But it supports the theoretical principal that it doesn't matter where you get your calories from, a calorie is a calorie.",null,2,cdh8dam,1qvipm,askscience,new,1
Spaturno,"Your premise is wrong. It's not just a matter of calories, a lot of other things count: how much bad stuff you ingest, and can't be expelled, and make damages to your body. Bad quality fats and high quality ones are not digested in the same way, despite having the same caloric values, one will hurt you and the others won't.",null,13,cdh4big,1qvipm,askscience,new,3
null,null,null,1,cdh060v,1qvj38,askscience,new,6
tmatzz,"This is essentially how a condenser works.  If you simply put cold water into a condenser with hot gas/liquid flowing through it the cold water would heat up to the same temperature as the hot liquid.  Not much condensing would happen.
",null,1,cdgynq9,1qvj38,askscience,new,2
PostalPenguin,"&gt;what sort of detrimental affects would I have if I was distilling an alcoholic spirit WITHOUT having the condenser being cooled with the cool water in/out system?

A significant portion of your alcohol would remain in the vapor phase and simply dissipate rather than being condensed into liquid. Once the condenser was at equilibrium with the vapor temperature, no more alcohol would condense and as a result the alcohol would remain as a vapor until it was released into the air. 
",null,0,cdh9m7i,1qvj38,askscience,new,1
Karmic-Chameleon,"Also of note, your water flow should go in at the bottom of a condenser and come out of the top.

My understanding (though I may be wrong) is that as the cold water cools the hot vapour inside the condenser it heats up. Heated water is less dense than the cold water so tends to rise. Put the cold water in at the top, trying to force it downwards, and it has to compete against the in-rushing cold water.",null,2,cdh5kig,1qvj38,askscience,new,1
StringOfLights,"Marsupial pouches actually vary quite a lot in their morphology (the pouch is also called a [marsupium](http://en.wikipedia.org/wiki/Pouch_%28marsupial%29)). [*Dromiciops*](http://animaldiversity.ummz.umich.edu/accounts/Dromiciops_gliroides/) has a mere [indentation](http://www.mammalogy.org/uploads/imagecache/library_image/library/1629.jpg) instead of a full-on pouch. Some marsupials have the forward-facing pouches we're used to seeing, although they may be fairly small. Some have backwards-facing pouches, like wombats, koalas, and the [Tasmanian tiger](http://www.mnh.si.edu/onehundredyears/featured_objects/Thylacine/thylacine_painting_large.jpg).  The American opossum *Didelphis* has a [slit that runs up and down the body](http://wwwdelivery.superstock.com/WI/223/1990/PreviewComp/SuperStock_1990-18873.jpg) (hopefully that photo isn't too gross!). These different morphologies provide different advantages, so depending on what the species is doing ecologically a forward- or backward-facing pouch might be selected for.

However, it's hard to track the evolution of the marsupium in the fossil record because it's something that is only present in soft tissue. Marsupials do have a bone called an [epipubis](http://en.wikipedia.org/wiki/Epipubic_bone) that in theory supports the pouch. The problem is that the epipubis also [varies in morphology independent of the marsupium](http://books.google.com/books?id=HmQTTChULOMC&amp;lpg=PP1&amp;ots=9y1N5kv2XX&amp;dq=marsupium%20evolution%20marsupial&amp;lr&amp;pg=PA56#v=onepage&amp;q=marsupium%20evolution%20marsupial&amp;f=false), so the marsupium doesn't structurally depend on the epipubis being present. This means that we can't really infer what the pouch looked like based on the epipubis. It's also present in a bunch of early mammal groups, including the early eutherian [*Eomaia*](http://en.wikipedia.org/wiki/Eomaia), so it seems to be something that's ancestral and we in fact can't really assume it even indicates the presence of a pouch. It may just be an ancestral character that placental mammals lost.

There is somewhat of a [phylogenetic pattern to the morphology of the marsupium](https://www.researchgate.net/publication/250927274_The_marsupial_pouch_implications_for_reproductive_success_and_mammalian_evolution).  Two of the living lineages that lie more basally within marsupials, Didelphimorphia and Paucituberculata, both have taxa that have a vertical slit in the pouch. If we consider what the marsupium is for - protecting young that are clinging to their mothers' teats - it's fairly easy to see how an indentation and perhaps then a slit would be selected for, and depending on what each lineage did ecologically, such as climbing, burrowing, or running, the indentation or vertical slit would close either anteriorly or posteriorly and the different pouch morphologies themselves would be selected for. 
",null,0,cdh0xfs,1qvksp,askscience,new,15
patchgrabber,"It seems to not be that necessary. [This](http://jdr.sagepub.com/content/early/2013/06/05/0022034513492336.full) study says that unless you're at risk for periodontal disease, it's not really a statistically significant increase in prevention.",null,2,cdh3eu1,1qvo2f,askscience,new,9
justin3003,"Whether the biannual timing of the cleaning itself is necessary, I cannot say for certain. But, there is no doubt that regular access to proper dental care is essential. Decaying teeth left untreated can and do become very dangerous abscesses ([1](http://www.webmd.com/oral-health/tc/abscessed-tooth-topic-overview)). Furthermore, published studies have shown that improper oral care can also increase your risk for inflammatory mediated diseases such as heart disease and stroke ([1](http://www.ncbi.nlm.nih.gov/pubmed/20882577), [2](http://www.sciencedirect.com/science/article/pii/S0002870307005418)). And finally, just like with the eyes, a good number of serious diseases manifest as oral symptoms observable to the trained eye ([1](http://emedicine.medscape.com/article/1081029-overview#showall)). It simply isn't enough to go to the dentist when things start hurting/bleeding/etc.; one should view dental visits as an essential part of the preventative medicine routine. ",null,2,cdh8nuk,1qvo2f,askscience,new,4
JimmyGroove,"When you actually hear sound, the nerve endings in your coclea in your ear is stimulated by waves trasmitted to it through the bones of the middle ear, the eardrum, and the air.  These nerve endings then transmit nerve impulses to your brain, where the primary auditory cortex analyzes them and produces the sensation of sound.

In the case of auditory hallucinations the auditory cortex is activated without imput from the ear due to any number of possible factors (physical damage to the cells, stimulation from another part of the brain, etc.)",null,2,cdh12vc,1qvoma,askscience,new,29
neuroPSYK,"A pharacological explanation is an N-methyl d-aspartate (NMDA) receptor hypofunction in corticostriatal and corticoaccumbens projections which leads to a sensory overload in the cortex:

NMDA receptor hypofunction in the thalamus (from the ventral tegmental area etc.) leads to a tonic inhibition of the thalamus. This, in turn, leads to less excitatory drive on gamma aminobutryric acid (GABA) neurons and therefore the thalamus cannot do its job of filtering information (sensory input). The result is that sensory information is allowed to run rampant to the cortex with little/no filter and positive symptoms (auditory hallucinations etc.) often result. 

Basically, the 'filter' that is used for auditory information isn't working, so any neural activity that is generated (from inside or out) is allowed to reach the cortex and the patient will 'hear' them. It is difficult to distinguish what is coming from *outside* and what is generated internally, so the auditory hallucinations are quite convincing. 

*Someone with more knowledge in this area can hopefully expand/criticize this post [not really my area]*",null,1,cdh402d,1qvoma,askscience,new,10
Stanage,"So when you refer to soap as being ""slippery"" this may seem like it is a different phenomenon than say, when you feel ""slippery"" on ice.  Both stem from the same physical quality, however: a loss of friction.

Soaps and many other bases or hydrophobic materials are known as [surfactants](http://en.wikipedia.org/wiki/Surfactant).  This is due to them having bot a hydrophilic (water loving) and hydrophobic (water fearing) component.  When soap comes into contact with water, it forms a lather that forces the interface between the soapy water and whatever solid it is touching (your skin) to become very hydrophobic, since the hydrophilic portions of the soap molecules are dissolving the water.  Your skin is covered in hydrophobic oils, known as sebum, which are secreted by the [sebaceous glands](http://en.wikipedia.org/wiki/Sebaceous_gland).  Sebum is made up of primarily fatty acids and triglycerides, which are what soap molecules are comprised of.  This makes your skin to be largely water-resistant, and therefore quite hydrophobic.  The interface between your hydrophobic skin and the hydrophobic portions of the soap molecules causes the ""load"" of the soap to move easier across your skin, thus reducing the coefficient of friction, and acting as a [lubricant](http://en.wikipedia.org/wiki/Lubricant).  This is where the ""slippery"" feeling of soap comes from.

As far as the ""slippery"" ice feeling, that has to do with the ultra smooth surface of the ice interfacing with the relatively smooth surface of your shoes, much like the hydrophobic interface between your skin and the soap particles.  The coefficient of friction between the two materials is low, and slipping occurs.",null,0,cdhrz50,1qvp39,askscience,new,3
wwarnout,"Astronomers assume that the laws of physics are the same everywhere, primarily because there is no evidence to the contrary.  Therefore, they could predict a trajectory pretty well.  However, given the vast distances, there would have to be provisions for making minor course corrections as the spacecraft neared its destination.",null,0,cdhic6k,1qvp9m,askscience,new,1
homininet,"Howdy. In the video clip, there doesn't seem to be any shot of the pangolin actually running, just walking. There are many differences between walking and running, and from a human point of view the main difference is that running involves a short period of time when both the limbs are off the ground. So by that definition the pangolin isn't running, and I'd be surprised if they ever do. You're right that for many species speed is important in evading predators, but pangolins are covered in those keratinous scales (basically like fingernails), and when a predator gets close to them, they roll up into a little armored ball (like this: http://www.earthrangers.com/wildwire/omg_animals/pangolin-armour/). So moving fast isnt really a high priority for them.

What is a high priority though are those massive claws that they use to dig up ants and termites. And in evolving those giant claws, they've really sacrificed the ability to use their hands and forelimbs for locomotion. The claws just really arent meant for bearing weight and walking and running. So instead, most of the time they walk around on two legs (bipedally) just like us. But if you watch the video closely, they will put their hands on the ground for short periods of time, presumably for balance. So really the way they walk is just kind of a byproduct of the fact that they have those massive shovels on either hand. 

Also fun fact, pangolins are the only other mammals (I believe) that walk bipedally in the same way that humans do, by taking alternating steps (L,R,L etc). Most other bipedal mammals hop. ",null,0,cdhkjh2,1qvpyw,askscience,new,3
samcobra,"Generally, the hormonal imbalances affect the self-regulation of hunger and satiety. People consume more calories and don't feel full. Other hormones such as lack if thyroid hormone can also lower the basal metabolic rate, or the rate at which your body burns calories at rest.",null,0,cdh4x4w,1qvsjq,askscience,new,3
ohnobananapeeeeeels,"let me just preface this by saying metabolism is a very complex thing -- we are STILL making new discoveries about how the body processes and stores the calories that we take in.

like /u/samcobra said, there's a component regarding hunger and satiety (i think he/she was referencing hormones like ghrelin and leptin), but that's not the whole picture. you were asking specifically about how, even if a person takes in less calories than their basal metabolic rate, they can still gain weight.

your thyroid is the main player in metabolism. if it's underactive, then you can gain weight no matter how little you eat. this is because the body kind of bypasses that step where it ""uses"" most of your calories and just places them directly into storage, i.e. fat. you probably will have enough energy to get by and keep your organs running okay, but you won't feel like a million bucks.

that's the basics of it, more or less. but i do want to spend a bit of time talking about the OPPOSITE, wherein you can take in a bunch of calories yet not gain weight. and i'm not talking about exercise, either. back in the day, there was this pesticide called 2,4-DNP. it had the particularly interesting effect of weight loss. it did this by messing with the system of ATP generation (also known as chemiosmosis or oxidative phosphorylation).

to boil down making ATP for you, your body pumps protons to one side of a membrane so there's a buildup on that side. since the forces of the universe want to achieve equilibrium across the membrane, the protons are shuttled through an enzyme called ATP synthase. the movement of the protons generates enough energy for a phosphate to be pinned on to ADP (adenosine diphosphate), making it become ATP (adenosine triphosphate). ATP is the main energy currency for our bodies.

now, if you take in 2,4-DNP then the protons don't go through ATP synthase. the chemical causes protons to pass back through the membrane without creating ATP. in essence, your body goes through the process of taking your food, breaking it down, and readying itself to extract energy from your food, but without actually making any energy whatsoever. it's really quite fascinating.

but anyway, don't take 2,4-DNP because it's a pesticide and i'm quite certain people have died from it.

source: [2,4-DNP](http://en.wikipedia.org/wiki/2,4-Dinitrophenol)

[thyroid](http://en.wikipedia.org/wiki/Thyroid#Hypothyroidism)",null,0,cdh6q2n,1qvsjq,askscience,new,1
drzowie,"Since drag scales as something between directly and quadratically proportional to speed, and work is force times distance, the faster you go the more work you will have to do.  So, *ceteris paribus*, you do better with uniform speed than with variable speed and the constant speed on the flat is the best way to go from the standpoint of energy required to move the car.

But you don't actually care about energy used to move the car, you care about how much fuel you have to burn to move the car from place to place.  Depending on the engine in the car it could be more efficient to drive the hills.  Some conventional normally-aspirated gasoline engines are much less efficient at low throttle settings than at high throttle settings - in vehicles with those engines you do significantly better by ""punching it"" up the hills and shifting to neutral going down the hills.

",null,1,cdh7dos,1qvsro,askscience,new,4
do_od,"In the ideal case (no friction or air resistance) a car travelling at a constant speed on a flat road will not use any energy at all. The same is true if there is a mountain or bulge in the way, as long as the car starts out with sufficient momentum to reach over the peak. If the car goes down a valley it will speed up and then slow down in a symmetrical fashion so that when it comes out of the valley it will be going as fast as it was when it started out. The velocity of the ideal car can be described at any point using this equation:

v = sqrt ( 2 \* g \* h + v_0^2)

*v* is velocity, *v_0* is velocity at the beginning, *g* is acceleration of gravity and *h* is the vertical height relative to where the car started from. The direction of *h* is down, so going up will make h negative. 

You see that the greatest height the car can climb to is h = -v_0^2 / 2g, and after that you will have to add energy somehow. 

As for real cars, the power required to maintain speed is higher the faster you go. You will have to supply more information to make your question precisely answerable, because it is possible to use less energy to go up a hill really slow than driving fast on a flat road. Are the cars required to arrive at the same time? How do you measure distances, horisontally or parallell to the ground? ",null,0,cdh29hh,1qvsro,askscience,new,1
Shadow14l,"The air and atmosphere are rotating with the Earth as well. Let's say you could suspend yourself in the air 1m above the ground. Assuming there's no wind but still air, you aren't going to start magically moving around because the earth is rotating below you. Unless something is actively holding you in place against the rotation or air currents, you'll stay exactly positioned above that spot of ground.",null,0,cdh2ivs,1qvt9i,askscience,new,11
stuthulhu,"The plane and the atmosphere are rotating with the earth. This is also why, given that the earth is rotating at ~1000 mph at the equator, if you jump in the air for 2 seconds you don't land half a mile away (or explode messily against any grounded obstacles hitting you at 1000mph). 

The airplane must exert thrust to change its movement along with the earth, which is essentially what it is doing while flying.",null,0,cdh7d06,1qvt9i,askscience,new,5
Ruiner,"You can answer that. When you measure the speed of an airplane, do you measure it with respect to what? Or in other words, what it is that propels an airplane?",null,1,cdh2fce,1qvt9i,askscience,new,6
JacFloyd,"The Wiki suggests 0,1µSv dose for one banana. Making it 5µSv dose for 50 bananas. Average backgound dose for one day is 10µSv so you would only increase your dose by 50% for the day. You need 100mSv annual dose for increased cancer risk and 400mSv dose in short period time to receive symptoms of radiation poisoning, equaling to to about 2700 bananas every day for a year or 4 million bananas in short period of time, respectively. You get higher doses than 50 bananas by just taking a x-ray scan or taking a flight from LA to NYC. [Link w/ sources](http://xkcd.com/radiation/)

EDIT: These are comparisons of equivalent dose and ignore potassium homeostasis (In reality, excess potassium is quickly eliminated from the body). Also, due to activity of radioactive potassium, the actual dose is estimated as dose over longer period of time. (thanks /u/radioactivefallgrout).",null,20,cdh2ty0,1qvuk1,askscience,new,125
Osymandius,"Bananas are potassium rich, but if you aim to get some sort of radiation damage from them, you're much more likely to kill yourself by disrupting your K/Na balance which permits conduction of electrical impulses through nerve fibres and, more importantly, cardiac tissue. This used to be used as a [lethal injection](http://en.wikipedia.org/wiki/Lethal_injection#Potassium_chloride) method.

You have to appreciate the levels of radiation we're talking about here - you pick up more by going on a long haul flight or have an X-ray, let alone a CT scan.

Edit: Yes - the mother of all diarrheal experiences.",null,13,cdh2hya,1qvuk1,askscience,new,74
BCMM,"Bananas are only radioactive^1 because they contain potassium, and all naturally-occurring potassium contains radioactive isotopes. Incidentally, potassium is necessary for survival, so short of consuming expensive, artificially purified K39, there's no way to avoid a radioactive dose equivalent to eating a healthy number of bananas.

If you somehow managed to eat massive quantities of potassium-rich food, you'd [die of something else](http://en.wikipedia.org/wiki/Hyperkalemia) long before experiencing any radiation-related symptoms.

^1 Significantly more radioactive than other foodstuffs, that is. Almost everything is at least a bit radioactive.",null,0,cdh4nb2,1qvuk1,askscience,new,16
GimletOnTheRocks,"**No.**  Potassium levels in the body stay relatively constant via homeostasis.  Bananas are only radioactive in that they contain a lot of potassium, a portion of which is a radioactive isotope of potassium, K40.  Unless you ingest pure K40 rather than a mixture, you can't really increase the radioactivity in your body.  Any excess is excreted before the K40 can do damage with its long half-life of 1.25 billion years.

Other radioactive isotopes are much more damaging, particularly those with shorter half-lives and those which are not naturally occurring.  Cesium 137 and Strontium 90 are two examples.

Eat all the bananas you want without worry!",null,0,cdh7qta,1qvuk1,askscience,new,7
Hristix,"The banana equivalent dose (for some reason often confused with a Becquerel) was basically created as a bit of a joke to make fun of reporters and public officials that absolutely freaked out (usually to generate interest in whatever their goals were) over tiny amounts of radiation, and then completely ignore absolutely face meltingly huge amounts of radiation, like those occasional criticality accidents or lost sources that end up exposing people.  Rarely hear much about those in the news unless it has something to do with a nuclear power plant.",null,0,cdh9kjb,1qvuk1,askscience,new,2
perusername,"NSAIDs (Non steroidal anti-inflammatory Drugs) can be absorbed through the skin. They are best used on joints on the body that are close to the surface of the skin.


They can be absorbed into the blood stream, however, this isn't common. The toxicities can lead to stomach ulcers and high blood pressure.

Edit: Missed a question - they often also have dosage recommendations or sizes of amounts to use in the instructions.",null,2,cdh3q97,1qvutl,askscience,new,18
Pest,"In osteoarthritis (where inflammation is relatively limited) I don't think there is much conclusive evidence showing efficacy of topical NSAIDs vs oral. The structure of the joint is such that penetrance of the drug into the joint (in sufficient concentrations to have a biological effect) is limited, as the drug would have to pass through the skin, muscle, joint capsule and synovium into the joint to have any effect on the cartilage and bone that make up the joint. [Citation](http://www.bmj.com/content/329/7461/324)

That being said, the case may be different in rheumatoid arthritis where  the tissues that surround the joint are much more involved in the disease. ",null,0,cdh4nf0,1qvutl,askscience,new,7
Ariadnepyanfar,"Correct me if I'm wrong, but little molecules that are fat soluble can be absorbed through the skin.  They are absorbed through the surface skin cells and released, and then taken up by cells further down and so on.  Some can penetrate down to the bone in this way.  By ""little molecules"" I mean molecules that have 420 protons or smaller (420 atomic weight or smaller).
",null,1,cdh3s4d,1qvutl,askscience,new,2
twentyone_21,"It depends what products you are talking about.

NSAIDS can somewhat be topically absorbed into the system, but produces less adverse reactions than its systemic counterparts. It must be absorbed somewhat to work, but it does not need to completely enter systemic circulation to work. Abstract: http://www.ncbi.nlm.nih.gov/pubmed/15871609

Counterirritants, on the other hand, give nearby neurons different irritation to counteract the actual pain from the joint. They don't have to be absorbed to work. ",null,0,cdhahg2,1qvutl,askscience,new,1
Bakkie,"Topical application of drugs is a well established method of administration. If you take an asthma inhaler you are performing a topical application to the ""skin"" /tissue inside your respiratory system.
Patch application of medicine such as nicotine, Medrol Dosepacks (pain relief and anti inflammatory) are a well establish part of an orthopedic /rheumatology arsenal.

In the US the data base which the doctors use is PubMed.com. It is maintained by the National Institute of Health (NIH) and aggregates peer reviewed journal articles. It is NOT internet medicine. It is free .  Since it collects all international journals I assume it can be accessed outside the US.

As of 3:00p.m. CST on November 18,2013, on PubMed, the terms topical capsaicin yields 1002 results. Here are a couple examples.

If you want to confirm the  process by which topical capsaicin works, you need only go through the list. (In the search term bar, capsaicin alone gave me roughly 25 ways to search)

1. Eur J Pain. 2013 Sep 24. doi: 10.1002/j.1532-2149.2013.00400.x. [Epub ahead of print]
Topical analgesics for neuropathic pain: Preclinical exploration, clinical validation, future development.
Sawynok J.

2.Mechanisms of topical analgesics in relieving pain in an animal model of muscular inflammation. Duan WR, Lu J, Xie YK.
Pain Med. 2013 Sep;14(9):1381-7

3.Eur J Pain. 2013 Nov;17(10):1491-501. doi: 10.1002/j.1532-2149.2013.00329.x. Epub 2013 May 6.
A novel approach to identify responder subgroups and predictors of response to low- and high-dose capsaicin patches in postherpetic neuralgia.
Martini CH, Yassen A, Krebs-Brown A, Passier P, Stoker M, Olofsen E, Dahan A.



The headnote are free. Abbreviations are explained in the headnotes. Some articles are available in full at no charge. All assume you have  expertise and understand technical terms. If you don't understand a term, do a quick Google search.

You will find the answer to your specific question by poking around in there.

Source: insurance defense attorney who takes multiple medical depositions",null,1,cdhetzx,1qvutl,askscience,new,1
murphyw_xyzzy,"There are a few questions here, unpacking them:

1) Can drugs effectively penetrate skin and have an effect? Certainly, this is a 'yes', look at DMSO on a door knobs.

2) Can the effect be local? Yep. Though if it goes through the skin, it will probably circulate in your whole system, there can be a larger effect near where it is put on the skin.

3) Without being toxic (or ""contaminating"")? Need to pin down the substance in question. Some stuff isn't toxic and is delivered this way.

4) Can it help with joint pain? Well, maybe. Is the pain really ""in the joint""? Would loosening the muscles or warming the muscles and tendons near the joint help? Would increasing the circulation to the area also possibly help? I'd lean towards ""yes"" for some of these. I am skeptical, like the submitter, about if it would really help the bone contact surfaces, but maybe these arrows are pointing 'near' the pain and the product is to help with the surrounding tissues.

Edit: Soo many typos!",null,5,cdh4aur,1qvutl,askscience,new,3
null,null,null,3,cdh6qnd,1qvutl,askscience,new,1
papagayno,"""Acne occurs most commonly during adolescence, affecting an estimated 70-90% of teenagers.[4] In adolescence, acne is usually caused by an increase in testosterone, which occurs during puberty, regardless of sex.[5] For most people, acne diminishes over time and tends to disappear — or at the very least decreases — by age 25.""

http://en.wikipedia.org/wiki/Acne_vulgaris",null,16,cdh7a0w,1qvuw8,askscience,new,37
Ruiner,"Not exactly. ""Mass"" itself doesn't increase once you increase velocity. The faster an object moves, the harder it is to accelerate it, though. This is only due to the properties of special relativity, an in order to interpret that in a classical context, objects are often said to have a ""relativistic mass"", which increases with velocity. 

Nevertheless, if you are inside an object moving fast with respect to another observer, this is your reference frame and from your point of view, your velocity is 0. Think about this: you are moving very fast with respect to the same, and the sun is moving very fast with respect to the center of the galaxy and so on...",null,1,cdh1hcd,1qvv6f,askscience,new,17
Fenring,"&gt;Centrifugal force

You can't really separate this one. The problem is that the question really has to be considered in terms of General Relativity, in which context the centrifugal force is treated in exactly the same way as gravity (in a sense, it's actually a form of gravity). The centrifugal force is going to be the dominant effect here, causing your overall weight to be reduced.",null,2,cdh1ltx,1qvv6f,askscience,new,3
KerSan,"I'm kind of surprised that none of the answers so far have pointed out that your question is ill-posed. Special relativity concerns itself only with *inertial* frames of reference, which means that you cannot have acceleration or gravity (though these are actually the same thing under Einstein's principle of equivalence). Weighing people involves moving beyond special relativity, at least slightly.

Nevertheless, as /u/Ruiner points out, the answer to your question is 'no'. That is because your weighing scale is also in the same frame of reference as you, so the principle of relativity tells us that your airplane may as well be stationary and the observer is the one speeding past you. Any relativistic effects observed by the moving observer cannot cause them to disagree about the reading of your scale. The effect of relativity on the physics of your body are the same on the physics of the scale itself, so everything cancels out.",null,1,cdh98yl,1qvv6f,askscience,new,1
SynbiosVyse,"What you're experiencing is just the increased blood flow to the area, obviously not a permanent increase in muscle mass. A few hours after your workout and they should be back to normal size, although with some tearing. Then they'll actually increase in mass within the week.

The pump has very little to do with, and is not necessary, to actually get a good workout in and improve mass. However Arnold and a lot of other body builders agree, if it helps you motivated.. and feels pleasurable... it doesn't really hurt you.",null,1,cdh4nx5,1qvv8c,askscience,new,8
5p0ng3b0b,"Here is a wikipedia article and 2 discovery articles that will help you understand more about this topic. Although there are common rat-like ancestors, there are also better intermediate examples that were amphibious and maybe give a better resemblance.  
On the size thing, there are only speculations of course, but the most common theories involve the abundance of food and the skeleton adapting to water(less restrictions).  
I can't find a reference to the blow-holes but if you look at the images of the skeletons and digital reconstructions of intermediate species you will notice that their nostrils start from the edge of their long noses and gradually goes further back which would give them some advantage being fully submerged.

  
http://en.wikipedia.org/wiki/Evolution_of_cetaceans  
http://news.discovery.com/animals/whales-dolphins/blue-whale-larger-than-ever-120131.htm   
http://news.discovery.com/animals/dinosaurs/how-dinosaurs-got-so-big-120131.htm",null,25,cdh2q3u,1qvvpn,askscience,new,92
mehmattski,"Here are some non-Wikipedia webpages:

[The Panda's Thumb](http://www.pandasthumb.org/archives/2008/03/whale-evolution.html) explains how embryonic development of the blowhole in the modern dolphin is very similar to what we observe in fossil whales.

&gt; In most mammals, the nose opening is located near the tip of the snout. In modern dolphins, on the other hand, it is located on the top of the head, above the eyes. It is called the blowhole.

&gt; In development, the nose opening shifts from the tip of the snout (arrow in left embryo) to its position on top of the head.
&gt; 
&gt; Ancestral whales also have their nose opening near the tip of the snout, and the shift to the forehead is documented evolutionarily by fossils.

The page also notes a transition from nostril to blowhole evident in the series of fossils known as *Pakicetus*, *Ambulocetus*, and *Kutchicetus*. Each organism has a blowhole position between that of modern ungulates and that of modern whales. http://pandasthumb.org/nasal_drift.gif

A great deal more information about the evolution of whales can be found at [TalkOrigins](http://www.talkorigins.org/features/whales/).  
 ",null,1,cdh4fye,1qvvpn,askscience,new,19
atomicrobomonkey,long story short is their blow hole is a nostril that move to the top of the head.  Size is because they live in water.  A whales weight would crush it's own organs on land but they are almost neutrally buoyant when in the water so there was nothing stopping them from evolving to be bigger.,null,9,cdh30h3,1qvvpn,askscience,new,20
homininet,"You also might want to check out this website (http://www.pasttime.org/?p=476) and the associated podcast. One of the recent episodes was about whale evolution. Whales are actually more closely related to cows and deer and hippos (artiodactyls) than they are to rats, and what they probably evolved from was something totally unlike rats (and also something unlike any modern artiodactyls). 

As far as the body size increase, one thing to keep in mind is there are a lot of biomechanical constraints to body size in terrestrial animals. Its really hard to achieve a massive body size, and only a few things have done it (sauropod dinosaurs for example). But once you get into the ocean, the musculoskeletal system gets a little bit of a load off because of the buoyancy of the water. So its at least a little easier for animals to achieve huge body masses.
",null,5,cdh3ln5,1qvvpn,askscience,new,18
iamdelf,"This is sort of tangential to the original question, but I've always wondered about it.  Since whales, particularly baleen whales feed directly off of plankton and therefore skip the entire food chain in the ocean, was their evolution associated with a mass die-off or mass extinction event due to food stress in other ocean dwelling species of the time?",null,2,cdh6dat,1qvvpn,askscience,new,7
bbqrubbershoe,"Since the fossil record is not complete geneticists have the best evidence.  Richard Dawkins' book The Ancestor's Tale has a chapter called The Hippo's Tale that is very informative (and available for free on books.google).

""Hippo's Tale"": Greek root words: Hippos = horse, potamus = river. Zoologically however hippos are artiodactyl, meaning ""even-toed.""

Wikipedia: ""This group includes pigs, peccaries, hippopotamuses, camels, llamas, chevrotains (mouse deer), deer, giraffes, pronghorn, antelopes, sheep, goats, and cattle. The group excludes whales (Cetacea) even though DNA sequence data indicate that they share a common ancestor, making the group paraphyletic.""

""Paraphyletic"" means that artiodactyls shared a common ancestor, or concestor, with whales (Cetacea). The composite name is Cetariodactyla.  But even this is kind of wrong because ""according to molecular evidence, whales are deeply embedded within the even-toed ungulates. Hippos are closer cousins to whales than hippos are to anything else including other even-toed..."" (Dawkins p197)

""Gathering all this together, we can sketch a forward chronology as follows. Molecular evidence puts the split between camels (plus llamas) and the rest of the artidactyls at 65 million years, more or less exactly when the last dinosaurs died. Don't imagine, by the way, that the shared ancestor looked anything like a camel. In those days, all mammals looked more or less like shrews. But 65 million years ago, the 'shrews' that were going to give rise to camels split from the 'shrews' that were going to give rise to all the rest of the artidactyls. The split between pigs and the rest (mostly ruminants and hippos took place about  55 million years ago. Then the whale lineage split off from the hippo lineage not long afterwards, say about 54 million years ago, which gives time from primitive whales such as the semi-aquatic Pakicetus to have evolved by 50 million years ago. Toothed whales and baleen whales parted company much later, around 34 million years ago, around the time when the earliest baleen whale fossils are found."" (Dawkins p200)",null,1,cdh64is,1qvvpn,askscience,new,5
loveyourlies,"I saw a BBC Documentary that noted marine life can grow to colossal sizes underneath Antarctica (showing a colossal octopus as an example).  

Maybe whales have spent a much longer time (speaking in hundreds of thousands of years) closer to a colder climate?  This would differentiate them from other, more tropical, fish that managed to survive due to their incredible numbers and camouflage techniques.",null,0,cdh8xah,1qvvpn,askscience,new,2
drinkermoth,"tl;dr: The nostrils moved slowly to the end of the snout then back the head to mean less time and effort had to be spent on breathing and the increased size helps thermo regulation efficiency.

In the beginning the quadrepedal mammals took to the water.  Many species did this, as the water - lakes, rivers, the sea and such bodies - contained a plethora of resources for the mammals to use.  These included food, space (possibly the opportunity to escape predators), cooling, etc. 

This was good and many mammals were happy just paddling in the water, after all there were already specialised water animals (fish etc.) that did a great good job of exploiting the water's resources, and so the niches were already pretty saturated.  Evidently however, at least one species found it advantageous to its fitness (survival and reproductive capability for itself and future generations) to venture deeper and deeper into the water.  

This move occurred over many generations and over many more species ventured deeper into the water.  Now there is no way of discerning this with certainty, but if I can be allowed to offer a likely explanation it may have gone like this:

as you get deeper into the water you must occasionally come up for air and so you might rear up and lift your head to breath, over time and generation those individuals with nostrils higher on the nose did better than those with them lower on the head; they could spend less time breathing and more time looking around.  

Once the nostril got to the tip of the nose pressure then drove it back, allowing future generations to lie flat in the water, it is likely that eyes migrated downwards and to the sides (if they had not already) to allow better vision of the water below to watch for predators.  And that is how the whale got it's nose.

As for size, the buoyancy provided by the water around allows animals under the sea to grow in a much less restricted way.  The pressure of the water meant that less investment needed to be made in skeletal strength, muscle mass, blood pressure etc. 

also there is a pressure to grow in size as there is a direct correlation between size and an organisms efficiency at maintaining body temperature (especially important for species that regulate their own temperature like mammals), basically bigger is better for staying warm.  Also water is a more efficient thermal conductor than air and so there would be pressure to put on fat for that reason as well.  By no means do you have to be big to be a water dwelling mammal, but as the whales (and Pinnipeds) specialised into their roles it became important for most of them to keep warm, either due to their deep diving or geographical location.",null,0,cdh9s9g,1qvvpn,askscience,new,2
HughJorgens,"Nobody has pointed out that stress (from  environmental change or new predators) causes more mutations, which helps animals adapt quicker.  The other thing to consider is that animals who are well adapted to their environment (in a low stress environment) don't tend to change.  It's called punctuated equilibrium.  When they change, it happens all at once, in response to new stress.
TLDR  animals don't change unless they need to.",null,2,cdh500v,1qvvpn,askscience,new,2
null,null,null,18,cdh3opg,1qvvpn,askscience,new,7
barc0de,"If you are an astronaut, in a space suit, firing a gun, then yes - you will hear something as sound will be transmitted through your glove into the suit. It wont sound anything like a normal gun firing, as most of that noise is generated by the shockwave of the bullet going through the air.

The astronaut floating a few feet away will hear nothing, unless the bullet hits them of course",null,2,cdh29ez,1qvvtd,askscience,new,16
Ruiner,"No, it won't. Sound needs a physical medium to propagate, so you won't hear anything in the vacuum.",null,4,cdh1fv9,1qvvtd,askscience,new,10
UpsidedownGround,"The land did not rise from the water, per se. The land masses had already begun to form by the time there were oceans on earth's surface. 


Very close to earth's formation, it was a giant ball of undifferentiated molten material. As it began to cool, that material started to separate based on its density, with the heaviest stuff sinking to the planet's center and the lighter stuff bobbing to the top. The lightest stuff is what makes up the continental landmasses. 


Those landmasses aren't static, however. Earth's crust is fragmented into a bunch of tectonic plates that sit atop the outer mantle. This mantle is somewhat plasticky (think a wad of silly putty), and so it flows in a manner of speaking, dragging the tectonic plates around with it. Sometimes the plates collide, scraping off bits of continental crust onto each other.  When all of the continental crust just happens to be accreted together at one time, we call the landmass a ""supercontinent."" We have good evidence that there have been seven such supercontinents in earth's history. Pangaea was the most recent, occurring about 300 million years ago.  It split apart as the tectonic plates underneath it started flowing away from each other, ripping the continent asunder and creating the Atlantic Ocean.

Source: Second year geology student.",null,0,cdh45il,1qvw5f,askscience,new,46
null,null,null,18,cdh4tfy,1qvw5f,askscience,new,1
thatool,"&gt;I was told that crystals grow by magnetic direction.

Do you have any sources for that? I'm not aware of any natural crystals having magnetic poles. Well except for magnetite. Are you referring only to magnetite? Even in that case I find it hard to imagine that it is affected by local magnetic fields. Natural magnetite tends to be formed of multiple domains that oppose each other and form a net zero total magnetism.

",null,2,cdh3af9,1qvwsv,askscience,new,7
binkabi,"The size of Andre the Giant is inlarge due to a syndrome called acromegaly. Acromegalycauses overproduction of growth horomone, resulting in the large size of the patient. Several studies including [Leon-Carrion et al, 2010](http://jcem.endojournals.org/content/95/9/4367.full) supports the hypothesis that the overprodustion of GH (and IGF-I) leads to impairment of the brain, especially the memory (LTM and STM).

EDIT: Please take note of the correction /u/Coldhardt made to this post
",null,306,cdh4dcy,1qvy7q,askscience,new,1272
TillyGalore,"Craniofacial development research has shown that the way the skull grows is in part due to the brain. The skull is not resting on or compressing the brain, in fact the brain can move around inside the skull with some freedom, yet it is attached to the skull through many dural attachments. With that being said, the increased skull density will not push on the brain, because in conditions like gigantism, the fontanelles do not close prematurely and therefor will grow along with the brain to a size supportive of the brain tissue. ",null,11,cdh5mcq,1qvy7q,askscience,new,62
null,null,null,18,cdh4s43,1qvy7q,askscience,new,40
sporclesam,"Usually caused by [pituitary adenomas] (http://www.cancer.org/cancer/pituitarytumors/detailedguide/pituitary-tumors-what-is-pituitary-tumor), acromegaly such as Andre's could cause vision loss and severe headaches (symptoms not due to increased bone mass, but similar to stroke) apart from other body dysfunctions [detailed here] (http://www.nlm.nih.gov/medlineplus/ency/article/000321.htm). ",null,4,cdh4es8,1qvy7q,askscience,new,15
DJ_the_Greek,"In the case of gigantism, chronic heart failure begins to take place do to the extreme demand for Oxygen ad blood to the disproportionately large muscles, organs, and tissues.  Typically, the cardiac muscle itself compensates by growing larger and stronger (hypertrophy), which eventually leads to the heart muscle functioning in a progressively less elastic fashion.  Basically, the heart gets to a state where it is not able to stretch and contract adequately to perform its function.  
I realize this is a question about the brain, but I thought the description of heart failure might draw a distinction between the effects of gigantism on the heart vs the brain, i.e. There is not an increase of demand on the brain like there is for the heart.  I'm seeing some threads here talking about how the brain is affected, but I personally haven't done the research.",null,1,cdhh0y0,1qvy7q,askscience,new,7
null,null,null,3,cdhaou5,1qvy7q,askscience,new,8
Manilow,"Endothelial dysfunction (pathalogic changes to the lining of your blood vessels) is common in people with acromegaly.  This leads to a much higher risk of cardiac events like heart attack and stroke, when combined with the changes seen in the cardiac muscle.

Endothelial degeneration is associated with cognitive impairment, both long term and acute impairments from ischemic events.

So yes, it does effect the brain eventually.  Anything that effects cerebral perfusion will effect it in the long run. ",null,0,cdhdaqw,1qvy7q,askscience,new,1
null,null,null,11,cdh63cq,1qvy7q,askscience,new,6
null,null,null,68,cdh4e5i,1qvy7q,askscience,new,10
SwedishBoatlover,"It is hypothesized that gravity is mediated through elementary particles called ""Gravitons"". This is a hypothesis, not a theory, which means that scientists are far from sure. It has not been proven whether gravitons actually exist or not, but it would make sense since the other three forces are mediated by elementary particles (electromagnetism through photons, strong interaction by gluons and weak interaction by W and Z bosons). ",null,0,cdhfz90,1qw4kk,askscience,new,5
iorgfeflkd,It's part of the universe we live in. Any answer you get will just reduce to that.,null,4,cdhhh7j,1qw4kk,askscience,new,6
tliff,"Maybe nitpicking but those two don't really hold. 2 is even and is prime. 5 ends with a 5 and is prime.

What you're listing are [divisibility rules](https://en.wikipedia.org/wiki/Divisibility_rule) which are simple ways to tell if a certain number is divisible by another. You can use those rules to find the divisors for a number and if you only find them to be 1 and the number itself the number is prime but simply fulfilling one of these rules is not enough.",null,0,cdhg84f,1qw5gv,askscience,new,8
iorgfeflkd,The number of prime numbers below x is approximately x/ln(x). This is not the most precise known approximation.,null,1,cdhhmue,1qw5gv,askscience,new,4
GOD_Over_Djinn,"Here's one that isn't just a divisibility rule. If p is prime, then for any natural number n, n^(p)-n is divisible by p. This is called [Fermat's little theorem](http://en.wikipedia.org/wiki/Fermat%27s_little_theorem). This gives us a nice way to test if numbers *aren't* prime (note that the implication doesn't go both ways, a fact that gives rise to what are called [pseudoprimes](http://en.wikipedia.org/wiki/Fermat_pseudoprime). ",null,0,cdhiklt,1qw5gv,askscience,new,6
rlee89,"Any number (other than 3) whose digits sum to a multiple of 3, must have 3 as a factor, and thus not be prime.

&gt;If it's even it cannot possibly be prime.

(other than 2)

Of course, 'even' is defined as 'divisible by 2' so that one's rather trivial.

&gt;If it ends in a 5 it cannot possibly be prime.

(other than 5)

That one is really just an artifact of the base 10 of decimal representation.  You can for similar reasons say that any number other than 2 ending in 2, 4, 6, 8, or 0 cannot be prime (though that does overlap with 'cannot be even').",null,0,cdhg9bp,1qw5gv,askscience,new,2
IncongruentModulo1,"The rules you described are actually division rules. If a number is even, that means that it is divisible by 2. If a number ends in 5 or 0, then it is divisible by 5. Clearly, the only primes divisible by 2 or 5 are 2 and 5, respectively.

More interesting is the fact that every real natural number has a unique prime factorization. 

This is not true for something like the complex natural numbers, for example. 20 + 0i = (4 + 0i) * (5 + 0i) = (2 + 0i) * (2 + 0i) * (2 + i) * (2 - i)

The fact that every real natural number has a unique prime factorization is quite useful in number theory since there are a good number of arithmetic functions that are additive and multiplicative. From Wikipedia:

`Then an arithmetic function a is`

    additive if a(mn) = a(m) + a(n) for all coprime natural numbers m and n;

    multiplicative if a(mn) = a(m)a(n) for all coprime natural numbers m and n.

Maybe you can see that if we have the prime factorization of a number, say n = {p1}^{a1} * {p2}^{a2} * ... * {pk}^{ak}, then to compute f(n), you just need to do f({p1}^{a1}) * f({p2}^{a2}) * ... * f({pk}^{ak}) or f({p1}^{a1}) + f({p2}^{a2}) + ... + f({pk}^{ak}). This will come up a lot more than you might expect. Here are some examples of [multiplicative](http://en.wikipedia.org/wiki/Multiplicative_function) and [additive](http://en.wikipedia.org/wiki/Additive_function).
",null,0,cdhgfd5,1qw5gv,askscience,new,1
iorgfeflkd,"The amplitude of an electromagnetic field is the strengths of its transverse electric and magnetic fields, measured in Volts per meter and Teslas respectively. If you want to look at it in terms of photons, the intensity is related to the number of photons as well as the square of the amplitude, so the field strength is related to the square root of the number of photons.",null,1,cdhhgny,1qw6dq,askscience,new,8
Hiddencamper,"The differences are in the way they are designed, and the enrichment of the fuel.

Lets talk about bombs first. Nuclear bombs utilize very high enrichment fuel (&gt;90%). Nuclear bombs are designed such that when they are assembled into a critical mass, the goal is for them to go prompt critical, then release as much energy as possible before the bomb blows itself apart. Nuclear bombs have no control systems, and all feedback mechanisms are designed to try and make the bomb as effective/efficient at releasing energy as possible.

Nuclear reactors are very different. They generally utilize low enerichment fuel (&lt;5%), (exception: naval reactors). Nuclear power plants are designed and operated such that they cannot maintain full core criticality on prompt neutrons alone, they rely on delayed neutrons. Prompt neutrons are generated in about 10^-4 to 10^-7 seconds, while delayed neutrons take several seconds to be generated. Because power reactors rely on delayed neutrons, their power changes take several seconds to occur, and this allows time for active control systems or passive feedback mechanisms to control reactor power. (Passive feedback mechanisms include the Doppler effect, voids/boiling, temperature effects, liquid density effects, etc) In a worst case scenario, it buys enough time for reactor protection system to automatically scram the reactor on high flux signals. Nuclear reactors utilize sufficient negative reactivity coefficients that they cannot undergo rapid out of control power loops (some exceptions, like when Chernoby's RBMK reactor was operated inappropriately. Part of this was the particular design of this plant, part of it was the way it was inappropriately operated). 

Both ""reactors"" (bomb and power) generate heat. The difference is how fast you get to that peak power output. Bombs get there in moments, while nuclear reactors take quite a while. A nuclear bomb will go from completely subcritical to an explosion (several orders of magnitude) in moments. The worst a nuclear power reactor can do, is about a 2-3 times power increase, before negative reactivity coefficients pull this down, and/or a reactor scram terminates the power increase.

Just an interesting anecdotal data point, the worst case reactivity increase that I've seen in a commercial BWR happened when a reactor cooling valve opened at the maximum possible rate due to a control system error during plant startup testing, which caused the valve open signal to bypass the valve motion limiter. The sudden increase in cooling reduced the steam voids in the reactor, which led to a prompt neutron reactivity spike of just over 300% neutron flux, which was terminated by a reactor scram in about 1.1 seconds. No fuel damage occurred because 2 seconds of a flux spike does not allow enough time for heat to transfer to the fuel cladding and cause damage. The plant was a ~3000 MWth BWR.",null,0,cdhixmx,1qw712,askscience,new,2
wwarnout,"Uranium is usually used for power generation, and is mined from Uranium ore, which is mostly U-238 with a small amount of U-235.  The latter is the fissionable component (i.e., the radioactive part).

In low concentrations, U-235 will decay in a fission reaction (it breaks apart), with heat as the by-product.  Power plants use Uranium that has been enriched to about 5% U-235, which produces a lot of heat, but not quickly enough to cause an explosion.

Weapons-grade Uranium is enriched to about 90+% U-235.  It also produces heat, but in this case, the heat is produced so quickly that an explosion occurs.",null,1,cdhg2mj,1qw712,askscience,new,2
afcagroo,"There are several different technologies for constructing security ""holograms"" (they usually aren't literally holograms).  None of them can be replicated with a standard offset press or something like a laser printer; they require specialized and generally expensive equipment. For one thing, they generally require feature sizes that are smaller and require more accurate registration; some as low as 100nm. For some types, the ""hologram"" is constructed in layers, with the reflective material angled in a specific direction.  
  
Certainly it is possible for a counterfeiter to buy/build the equipment to make security holograms, but the capital outlay would be significant.  ",null,0,cdjqryq,1qw788,askscience,new,1
klenow,"The proteins, fats, and carbohydrates are (for the most part) still there. They are not volatile enough to boil out. Remember that your food, just like, is mostly water. ",null,1,cdhjo08,1qw9xs,askscience,new,2
therationalpi,"The system, as you defined it, isn't closed. For a resonating chamber to amplify sound, there has to be acoustic energy getting pumped into the resonator. The trick is simply that the acoustic energy isn't getting dissipated, so the total energy in the system is increasing linearly with time until it reaches equilibrium with the losses.

The best way to think about this is that the resonator is a device that takes acoustic energy at a given frequency and holds on to it with low dissipation. The analogy I always hear is water getting poured into a bucket. If I am pouring water into a bucket with a small hole, the water level in the bucket will increase. As the water level gets higher, the pressure will increase, and water coming out of the hole will increase. Eventually the volume of water pouring out of the hole is equal to the water getting poured in. In this case, the bucket is the resonator, the hole is the dissipative loss in the resonator, and the water is energy. Note that, at any given time, the water inside the bucket is greater than the volume coming out, but if you stopped pouring water in, eventually the bucket would totally drain.

Likewise, with the resonator, the acoustic energy is getting dissipated at a rate proportional to the energy stored in the resonator. Eventually the losses will match the input energy, and your resonator will reach equilibrium. The amplitude of the wave at this equilibrium is higher that the input wave, but we can still account for all the energy if we look back in time to when the source started going.

Hopefully that answered your question. Cheers!",null,4,cdhhdrr,1qwdkp,askscience,new,11
SingleMonad,"I'm going to assume you really are asking this question:  ""If I tie a guitar string between two nails on a post, and pluck it, I don't hear much sound.  How does the body of a guitar make the sound louder?""

First:  Why is the sound so soft to begin with?  Because, the string doesn't move much air.  The string is so small that it just doesn't make good 'contact' with the medium of the air, and the wave energy can't move off of the string.  There is an analogy to this in electronics called [impedance matching](http://en.wikipedia.org/wiki/Impedance_matching).

The body (or resonator) is an *impedance matching* device.  The string can couple more efficiently to body via the guitar bridge.  The walls of body have more area of contact with the air, and can move more air.

What /u/therationalpi said about energy is correct.  In equilibrium, the same amount of energy moves from string to resonator as moves from resonator to air.  It's the coupling efficiency that makes it work.",null,1,cdihccf,1qwdkp,askscience,new,3
uberhobo,"Yes, it's called a match.  It's even simpler than electrolysis, actually.  

But, if you want a method that's about as complicated as electrolysis, you're looking for fuel cells.",null,3,cdh7ldc,1qwdlb,askscience,new,12
homininet,"Very complicated-ly....

So I might do a little bit of a cop-out and give you the short version and a link to a youtube video if you're more interested. It is actually very complex, and I could dedicate a chapters worth of explanation to it, and many have.

So here is a great video, detailing in painstaking detail how it happens (http://www.youtube.com/watch?v=5DIUk9IXUaI). 

But essential the heart is original a tube. With a venous end, and an arterial end. This tube ends up undergoing some folding, which basically separates it into an ventricular half (in front), and an atrial half (behind). However it is still a tube, and there is a single opening between the ventricular and atrial halves. Then it gets tricky. A series of septa sweep down towards the center of the developing heart, and partition both the atrium and ventricle into left and right halves. However, this whole time blood is flowing through the heart, so a series of holes are gained, lost and divided to ensure that blood can still get where its going. Eventually the septum dived the heart into something that we would recognize in shape as a human heart. But the final problem is that in a fetus, oxygenated blood comes from the mother, not the lungs. So there actually exists a little shunt in the heart thats used to allow oxygenated blood into the heart in such a way that it can get to the aorta and bypass the lungs. When the baby is born, the shunt is closed, and blood flows the normal way.

So thats what happens in a nutshell. It is very complicated, so let me know if you want clarification on any of the stages. ",null,0,cdhkum4,1qwdzu,askscience,new,3
Henipah,"The first step is to form a tube that runs from up to down through your chest. This is basically a large blood vessel. It then [folds](http://php.med.unsw.edu.au/embryology/images/4/4b/Heart_Looping_Sequence.jpg) to form a more familiar shape. On the interior the heart cells remodel using the *turbulence of flowing blood* to tell them what to do. They form the septa that divide up the chambers. 

[More info](http://php.med.unsw.edu.au/embryology/index.php?title=Cardiac_Embryology)",null,0,cdhkqzn,1qwdzu,askscience,new,2
wazoheat,"**tl;dr: Using some very rough estimates, it seems that cigarettes do likely have a small but discernible effect on air pollution**

This calculation was a bit harder than I thought it would be at first, but I think we can make some solid conclusions from the following estimates.

There are [about 5 trillion cigarettes](http://www.longwood.edu/cleanva/cigbutthowmany.htm) produced each year. Let's assume that each one that is produced is smoked. To look at the impact on pollution, let's look at one specific type of harmful pollution: [PM_2.5](http://www.nasa.gov/topics/earth/features/health-sapping.html), or small particles less than 2.5 micrometers across. This type of pollution is one of the most important for impacts on human health, and is mostly attributable to combustion sources (fires, power plants, cars), so probably most relevant for this calculation. 

Each cigarette weighs [0.8 grams](http://viewknowdo.blogspot.com/2012/01/how-much-does-cigarette-weigh.html), so assuming that each cigarette is entirely burned and turned into particulate matter, this will add 4 billion kilograms of this type of pollution to the atmosphere each year. The whole cigarette is not burned and released in to the air, as we can see from ashtrays and cigarette butts, so the number is probably closer to 1 billion kg.

Now this might seem like a big number, but we need to compare it to other emissions to be sure: The worldwide emissions of PM_2.5 are [41 million tons per year](http://www.worldenergyoutlook.org/media/weowebsite/energymodel/IIASA_Emissions_Impacts_WEO2011.pdf), or about 37 billion kg.

So if we assume that every manufactured cigarette is smoked, and 25% of the smoked cigarette is released into the atmosphere, that means that 2-3% of particulate pollution worldwide can be attributed to cigarette smoke. Now, this is a very rough figure, but even if it's off by a factor of ten, it would still be significant enough to be measured. Additionally these calculations don't take into account that cigarette smoke is concentrated around where people are, whereas other sources (such as power plants and wildfires) might not be.

This is all just educated estimates, but I think this is the best answer I can give. It also agrees with some related studies [such as this one](http://tobaccocontrol.bmj.com/content/13/3/219.full).",null,1,cdhiftx,1qwe0g,askscience,new,3
MCMXCII,"When using Ampere's law, you draw Amperian loops around currents to find the magnetic field generated by these currents. But the current must flow into and out of the loop.

No imagine you have a wire carrying current into a capacitor. Now draw your Amperian loop so that the current pierces through it on one end, but the other end is in between the two capacitor plates. You have an Amperian loop where current goes in but it never comes out. So using the old version of Ampere's law, you'll find that there is no magnetic field. But that's nonsense because the wire is carrying current, so clearly it must have a magnetic field. 

Once the displacement current term is added to Ampere's law, you find that the changing electric field between the capacitor plates generates a magnetic field, and all of physics is restored to how it should be.",null,1,cdhie33,1qwe28,askscience,new,4
fishify,"Are you familiar Faraday's Law, that there is electric field generated from a time-changing magnetic flux?  The displacement is the analogous term in the other direction, providing the generation of a magnetic field from a time-changing electric flux.",null,0,cdhm1d2,1qwe28,askscience,new,1
quantummonkey25,"In the case of lions, social groups are typically one or two males and a pride of females. Infanticide usually occurs when a new male takes control of the group, thus any offspring born after an event are likely to be that of the new male. Other species, such as chimpanzees, which have beta males present in the groups, have much less established paternities. In fact, prior to genetic testing, all family lineages in chimpanzees, such as those described by Dr. Jane Goodall, were documented following only mother-child connections.",null,0,cdhmcyi,1qwfl3,askscience,new,3
iorgfeflkd,"x^(2)+y^(2)=R^(2) where R is the radius.

If you want it as a function, y=(R^(2)-x^(2))^(1/2) for half of a circle, and minus that for the other half. The function is only real for |x|&lt;=R.",null,1,cdh7ewa,1qwhkl,askscience,new,11
DarylHannahMontana,"&gt; infinitely small points

Mathematically, a point doesn't take up any space - that's the way it's defined to begin with - so there's no cause for concern. Thus, the usual formula x^2 + y^2 = R^(2) is still the right formula. 

In reality, if you try and draw a circle, you're only going to get an approximation. Even assuming that we could get it perfectly circular, the pen stroke is going to have some width (this sounds like the concern stated in your question). In this case, assuming the pen width is constant (a big assumption), the equation for the figure drawn would be something more like 

|x^2 + y^2 - R^(2)| &lt; ε

where ε a constant.

If the stroke width varies, it would be something more like 

|x^2 + y^2 - R^(2)| &lt; ε(x,y)

i.e. ε can vary with x and y.

And if it's not perfectly circular, it could be something like 

|x^2 + y^2 - R^(2)(x,y)| &lt; ε(x,y)",null,1,cdh8pm1,1qwhkl,askscience,new,6
__Pers,"One formula for a perfect circle (a la Dido's Problem, named after the Queen of Carthage in Virgil's *Aeneid*) is that the circle is the extremal shape maximizing enclosed area A for a closed curve of perimeter L. More specifically, the isoperimetric inequality 4 pi A &lt;= L^2 holds, with equality in the case of a circle in Euclidian space. Generalizations to curve space (e.g., the surface of a sphere) are possible. ",null,0,cdh8obx,1qwhkl,askscience,new,4
-Ignotus-,"x^2 +y^2 =r^2 with r as the radius.
How do you get there?
Let's start with the definition of a circle: a circle is a collection of points that all have the same distance to a certain point (the center). (Might be using the wrong terms here, as I learned math in Dutch, but you should get the point)
Let's start with point P(a, b). The distance from P to (0, 0) is then sqrt(a^2 +b^2 ).
Basically because of Pythagoras we have a^2 +b^2 =distance^2 . So for a distance of 5 it would be a^2 +b^2 =5^2. If you were to enter this into wolframalpha you'd get a nice circle, because every point (a, b) that fulfills the equation has a distance of 5 to (0, 0). 

Note: my terminology might be completely off, but I think you get the point.
",null,0,cdhh55a,1qwhkl,askscience,new,1
Cheesecake_Tiramisu," 

Lipinski's Rule of 5 is the general method to determine whether a drug will pass through the BBB, here are the points:

- Not more than 5 hydrogen bond donors (nitrogen or oxygen atoms with one or more hydrogen atoms)
- Not more than 10 hydrogen bond acceptors (nitrogen or oxygen atoms)
- A molecular mass less than 500 daltons
- An octanol-water partition coefficient[5] log P not greater than 5",null,0,cdhb7zi,1qwhwq,askscience,new,3
Phoenix1989,"This is known as the [solar cycle](http://en.wikipedia.org/wiki/Solar_cycle). 

As for the objects in the pictures I can only guess that they are sun spots or related to the temperature differential of the sun (Brighter spots being hotter and darker spots being cooler than the surrounding area).",null,0,cdhhyg4,1qwkvy,askscience,new,1
TangentialThreat,"It is very challenging (by which I mean impossible) to focus a laser such that the beam does not disperse. Beams tend to be very slightly cone-shaped instead of perfect cylinders.

It is possible to, say, [hit a specific region of the moon](http://en.wikipedia.org/wiki/Lunar_Laser_Ranging_experiment) with a laserbeam for rangefinding experiments. The spot size is many kilometers across by the time it gets there and is no longer useful for burning things because the energy is so spread out.

If you aimed a laser at interstellar space, the light would keep going forever but would be spread over a larger and larger area. The universe has some dust and gas to absorb the beam and reradiate the energy as heat, but interstellar matter is also rather sparse and not a significant source of loss.

It should be noted that Star Wars physics does not match up very well with real-world physics. For example, ""laser"" beams often move visibly across the screen and look like somebody shot a glowstick out of an anemic crossbow. Real lasers move at the speed of light.",null,0,cdhf2gl,1qwlju,askscience,new,16
EagleFalconn,"You may find [this old, old thread](http://www.reddit.com/r/askscience/comments/1anyst/in_the_famous_doubleslit_experiment_how_do_we/c8z9q5g?context=3) useful. Happy to answer follow up questions.",null,1,cdhe0w3,1qwlju,askscience,new,8
sloan_wall,it will continue 'flying through space' dissipating very very slowly.,null,1,cdhd5js,1qwlju,askscience,new,4
thephoton,"We often say that the output of a laser is a [collimated beam](http://en.wikipedia.org/wiki/Collimated_light) --- that is a beam whose rays all travel parallel indefinitely.

But in fact the output is more accurately described as a [Gaussian beam](http://en.wikipedia.org/wiki/Gaussian_beam). A Gaussian beam has a *waist* which is a region where it approximates a collimated beam. However there is a trade-off: the narrower the waist, the shorter its length. 

Beyond the waist, the beam diverges, and at very far distances the power hitting a given area will fall with an inverse-square relationship to the distance from the source, just like light emanating from a point source.

The beam waist/length trade-off can also be seen as a consequence of [diffraction](http://en.wikipedia.org/wiki/Diffraction) -- a light beam passed through a narrower aperture (or otherwise confined to a narrow cross-section, like at the beam waist) will diffract at a larger angle.",null,0,cdhs0b6,1qwlju,askscience,new,2
wazoheat,"It's not just metals, it's all materials. [Melting and boiling require energy](http://hyperphysics.phy-astr.gsu.edu/hbase/thermo/phase.html), since it involves breaking inter-molecular bonds. Conversely, condensing and freezing release the same amount of energy due to the re-formation of these bonds.",null,0,cdh8tps,1qwlnc,askscience,new,7
MayContainNugat,"Between updates, the computer would keep rough track of position, velocity, and orientation using a system of accelerometers and gyros. It would be periodically updated by the navigator taking sextant sightings of stars and measuring their angles against landmarks on the Earth or Moon (The positions and orientations of Earth and Moon were hardwired into ROM memory already). The spacecraft was also being tracked by stations on Earth, and these readings would be uploaded to the computer by telemetry. There were additional other systems providing position data. For instance, during lunar landing, it would receive altitude data from a radar altimeter.

Lots of people were asking this exact same question in 1969. NASA produced a cool [popular but technical video](http://www.youtube.com/watch?v=YIBhPsyYCiM) describing exactly how the computer was built and operated. [This one](http://www.youtube.com/watch?v=fsyDQex0c-M) demonstrates how it was actually used in flight. Pretty fascinating stuff.",null,4,cdhcq9k,1qwmjb,askscience,new,28
vjnexus,"The source code for the apollo 11 mission was released as open source a few years back for the 40th anniversary of the moon landing. http://googlecode.blogspot.com/2009/07/apollo-11-missions-40th-anniversary-one.html

There is an emulator available to run it as well. If you dig through the code you can find some pretty interesting comments in it.",null,0,cdhg8ja,1qwmjb,askscience,new,9
midsprat123,"While there were on board instruments, most data was calculated on Earth then sent to the ship. Important thing to remember is that the on board computers just received data and acted on it because the craft would have been to cumbersome for it to all take place on board. A really, really good video on the guidance system can be found [here](http://www.youtube.com/watch?v=vU5G9VsoER8)",null,0,cdhe3w4,1qwmjb,askscience,new,3
Manhigh,"Theres a relatively long term launch window that is governed by the alignment of the Earth and the target planet.  The Earth and another planet will have the same relative alignment around the sun after one synodic period.  For Earth and Mars this is approximately every 730 days.

Think of efficient ways to get to Mars.  If you wanted to travel the shortest distance, you would aim to hit Mars at its close-approach to Earth, flying radially away from the Sun.  This is incredibly costly from a propellant standpoint.

Generally, the most efficient way to get between two circular coplanar orbits is the Hohmann transfer.  You would leave Earth tangential to it's path, and arrive at Mars tangential to its path, travelling 180 degrees around the Sun.  The problem is, Earth and Mars aren't quite coplanar, so it's actually impossible to do this with a 180 degree transfer unless you utilize a midcourse correction, and even then its pretty inefficient.

Instead, we generally take somewhat shorter or longer trips.  If you look at page 33 of http://www.ntrs.nasa.gov/archive/nasa/casi.ntrs.nasa.gov/20100037210_2010040782.pdf you'll see an example of a ""porkchop plot"" which shows how much energy the launch vehicle has to put into launching the spacecraft to leave and arrive on the specified dates.  There are two local minima, one for shorter trips, and one for longer trips.  This is generally the opening of the launch window.  Launch early and you pay a penalty, and launch late and you'll pay a penalty.  That's the basics of the launch window as you mentioned it.

Other factors that go into this are:


- budgets:  Operating a spacecraft requires an expensive team of smart people.  Launching when you can afford to pay them sometimes effects timing.


- lighting conditions:  After columbia the shuttle was required to launch in daylight.  For robotic missions like MAVEN this usually isn't the case.

",null,0,cdhf3hj,1qwphe,askscience,new,17
monkeyfett8,"You plan a transfer orbit between planets such that you pick your launch time and your time of arrival at the other body.  By picking the start and end times you can calculate how much fuel you need to get from one orbit to the other.  You do this for many start and end positions and you can [map out the energy, and thus propellant, needed](http://en.wikipedia.org/wiki/Porkchop_plot).  With this you can say, if I launch November 18th, 2013 and arrive September 22nd, 2014 you will need X amount of delta V which in turn defines that you need Z kg of fuel.   It all comes down to the alignment of Earth at launch and Mars when you plan to get there.

Typically in this map of start and end times, you pick the point which requires the minimum amount of fuel.  You can also pick based on wanting a shorter flight, etc.  Once you design your spacecraft you can oversize your fuel tanks just in case you need some more at any point.  This margin lets you do extra maneuvers as needed.  (Alternately if your launch vehicle is not maxed out for lifting mass, you might be able to get some extra delta V there, without using your spacecraft itself)

Now, since you picked your orbit to be the minimum fuel needed, and you now have a bit more fuel than the minimum, you can freedom around this optimal point.   So this extra fuel might mean you have enough to launch on November 19th, 20th ... through December 7th.  However you might not have enough after the 7th.  You probably wouldn't try to launch early, so your window will start with the optimal time and end with the last date you would have sufficient fuel.  If you miss the end of your window you have to wait until the planets are in a good position again to allow for the fuel you sized to get you there.

**TL:DR:** MAVEN's most fuel efficient launch point to transfer to Mars is probably Today, but each day after requires more fuel. There's enough fuel to postpone the launch until the 7th of December.",null,3,cdhfh6i,1qwphe,askscience,new,6
gfpumpkins,"Red. If you didn't have at least some oxygen in your blood at all times, you'd be in serious trouble. ",null,0,cdhbc3s,1qwpio,askscience,new,2
baloo_the_bear,"Arterial blood is bright red while venous blood is a much deeper, nearly purple, red. The reason for this is the oxygen content of the blood. If you nick a vein, however, the blood will contact the air and grab oxygen out of it, giving it a bright red appearance.",null,0,cdhcbrz,1qwpio,askscience,new,1
Daniel_Oss,"Blood is red all through out the body, however venous blood (blood with less oxygen on its way back to the heart) is a much darker red than oxygen rich blood. The reason for this is due to hemoglobin in your blood tends to give off a red spectrum but other animals contain other molecules to help them carry around oxygen such as hemocyanin which gives off a purpleish spectrum. In other words, there is purple blood around but it is NEVER present in humans.",null,0,cdhqxly,1qwpio,askscience,new,1
Apollo_Felix,"Think of a switch where if the input is high (5V) then the two pins are short circuited. However if the input is low (0V), then the two pins are disconnected. The following [circuit](http://imgur.com/5OLiNdk) is then a NAND gate. If only one switch is low, the output voltage is always 5V (high). If both are low, the output is also high (5V). However if both are high, then the output is low.
The switches can be made using a FET transistor where the inputs are connected to the [gate](http://imgur.com/rpdF5R2).
An [XOR](https://en.wikipedia.org/wiki/NAND_logic), or some other gate, can be made using this same circuit. An XOR gate would look like [this](http://imgur.com/8uWDZ5O).",null,3,cdhdl4m,1qwqb7,askscience,new,34
afcagroo,"I think that most of the [Wikipedia articles](http://en.wikipedia.org/wiki/AND_gate#Implementations) are pretty clear.  Although the AND gate implementation only shows an NMOS circuit, not [CMOS](http://www.c-jump.com/CIS77/images/figure_3_7_AND_gate.png) such as is typically used. (CMOS circuits tend to be more power efficient, although they use more transistors than simple NMOS only circuits.)  
  
Keep in mind that an NMOS device turns on with the gate presented with a HIGH signal, and a PMOS turns on when the gate is presented with a LOW signal. ",null,8,cdhcjb8,1qwqb7,askscience,new,29
dirtpirate,"Others have given more detailed answers, I'll just pitch in with a flash game that challenges you to build circuits including logic gates and things like RAM by drawing n-type and p-type silicon as well as metal paths (simplified versions through). [Engineer of the people](http://www.zachtronics.com/play-kohctpyktop/). Quite fun, but it has a steep initial curve and gets incredibly complex towards the end.  ",null,3,cdhfr7o,1qwqb7,askscience,new,13
bisbyx,"**TRANSISTOR PART -- MAKING SWITCHES**

First, transistors. Imagine you have an upside down shaped T tube. In the vertical part, you have a magnet that blocks the horizontal part. Water will not flow from the left to the right. Now imagine if you put a magnet above the vertical part, the blocker will get magnetized up, and liquid can flow through.

This is a gross simplification, but this is basically how a MOS transistor works. You have a large positive pool of positively charged silicon. You have 2 ""gates"" of smaller negative charge silicon sitting in the positive charge. Electricity can't flow from gate to gate, because the positive charge is blocking the flow. So between the gates, we apply a capacitive oxide layer with metal on top of it. When we apply positive charge to the metal, like charges repel each other - the positive charge is repelled away from the surface of the large positive silicon pool. This then allows the charge to flow from the 2 gates. ""MOS"" means Metal, Oxide, Semiconductor. This is a great picture that explains it. http://en.wikipedia.org/wiki/File:Mosfet_saturation.svg (the diagonal line section is the ""neutral"" section that can contain current. once enough charge is appplied to the metal gate, the diagonal line will connect the 2 sides.

**GATES PART -- USING SWITCHES TO MAKE LOGIC**

Now the gates themselves are a bit trickier (lol, not really) and I dont have my ECE110 book with me. But basically, ANY logic can be built from a NAND gate. Based on DeMorgan's law (!A + !B = !(A*B)) and the fact that A NAND A = !A.

This http://en.wikipedia.org/wiki/File:NMOS_NAND.png is how you make a NAND gate. Remembering back to how transistors work, output F is connected to the high voltage V_cc (if there is no current, then the resistor has no voltage, so F = V_cc - see Ohms Law : V = iR). However, if A and B are BOTH positive voltage (1), then current can flow across them. If either one is not positive (0), then the flow is broken from V_cc to ground and the entire thing dies.  If current can flow, then we have a connection from V_cc to ground, which means current is flowing across the resistor, forming a voltage. Which means that F is no longer equal to V_cc. Depending on the voltage drop across the transistor, the voltage wont actually be 0V, but it will be a logical 0 (not enough to trigger another transistor).

Now we have a NAND. quite simple to make.

**BUILDING LOGIC -- THE REST OF THE GATES**

There are simpler ways of building the other gates, but lets just pretend we ONLY have NAND gates. We can still make all the other basic logic gates.

* NOT A = A NAND A   (this is drawn as !A)
* A AND B = !(A NAND B)   (we're using !, because we made it in the previous step)
* A NOR B = !(A OR B) = !A AND !B (demorgans law)
* A OR B = !!(A OR B) = !(!A AND !B) (run a nor through a not gate)
* A XOR B = (A AND !B) OR (B AND !A)

source: i have a degree in electrical engineering.",null,2,cdhr26o,1qwqb7,askscience,new,11
null,null,null,9,cdhik2v,1qwqb7,askscience,new,15
Kwibbian_Kel,"Not a direct answer, but here's something that might show you it's not as funky as it sounds:

Do you have any lights in your home that are controlled by two separate wall switches?

If so,  they probably work like this:  

 * both switches up or both switches down,  the light is off.  

 * when one switch is up and one is down,  the light is on.

You know what this is?  It's an XOR circuit!
",null,2,cdhgifz,1qwqb7,askscience,new,5
manofoar,"Folks have covered the theory behind the development pretty well. In a modern context, gates are built up through assembling transistors to work in concert to create the functionality.  In modern chips, these transistors are created by layering semiconductive materials with conductive ones, or by ""doping"" silicon with a thin layer of conductive material in a pattern.  The process of doing this is known as lithography.  

In the lithographic process, transistors are assembled upon a silicon substrate (with appropriate infusions of conductive materials) layers in such a way as to create the conductive and semi-condictive regions that can then be used as a transistor. The sale of these operations is on the order of nanometers. When you hear about chips that have "".1"" nanometer transistors, that means that the size of a transistor is 1nm in width, as measured from conductive region that ""collects"" the electrons, to the other region that ""emits"" the electrons, and with a semiconductive region between that also has in contact with it a ""base"".

In more laymans terms, you can consider the ""emitter"" to be just that - electrons flow OUT of the emitter. the ""base"" is the control, and the collector is where the electrons go ""to"".  the base controls this flow of electrons by means of a voltage applied to it.

These terms actually originate from back in the vacuum tube era of electronics, albeit with modifications. In the old vacuum tube days, you had your cathode, anode, and a grid. These functioned identically with that transistor - your anode emitted electrons (hence why the emitter is called that"", the cathode ""collected"" them, and the grid was what controlled the current flow between the two.
",null,1,cdhjliy,1qwqb7,askscience,new,4
mbizzle88,"The basic idea:

* AND: If you have two switches in a series (like [this](http://www.renesas.eu/media/edge_ol/engineer/04/img_02.gif)) then the current can only pass through if both switches are closed.

* OR: If you have a circuit with two switches in parallel (like [this](http://www.renesas.eu/media/edge_ol/engineer/04/img_03.gif)) then current can pass through it if at least one switch is closed.

* NOT: Electrical current will always take the path of least resistance. So if you have a switch that, when closed completes a ""short circuit"", current will travel through the short circuit rather than to the output (like [this](http://www.bottomlayer.com/bottom/banks/images/batt_NOT_gate.jpg)).

With these three kinds of circuits you can construct any other logical gate (such as XOR, NAND, and NOR).",null,0,cdho4sw,1qwqb7,askscience,new,3
VennDiaphragm,"Can you imagine a NOT gate (an inverter)?  If you know how a transistor works (sort of like a relay), this gate is very simple to imagine.  Put voltage on the base, and it opens the circuit.  If that circuit opens to ground and you have a pullup resistor, it's a NOT gate.  

How about a NOR gate?  That's like an inverter but it has 2 inputs, and if either of them are ON, the output is OFF.  Imagine connecting both of the inputs to the NOR gate to the base/gate of a transistor with appropriate resistors.  Basically, if either line has voltage, it will turn on the transistor.  

Once you have a NOR gate, all other gates can be made from it.  If you don't understand this, I'll quickly explain.  First, you can tie the inputs and you get an inverter.  Second, accept this formula: !(A|B) = (!A &amp; !B).  The left side of that equation is a NOR gate, so if you look at the right side it's telling us that if we invert the two inputs to a NOR gate, we have an AND gate.  And so on.

Of course, modern gates are designed with special properties for power reduction, speed, impedance, etc. ",null,1,cdhgktq,1qwqb7,askscience,new,3
deadlywoodlouse,"I'm a Computing Science student who has studied electronics, and I am really interested in their fundamentals.

Modern computing relies on electronics, mainly because of the advantages of (lack of) size and (abundance of) speed. However, you don't need to use electronics, or electricity at all for that matter. In fact, the first computers were purely mechanical, using gears and levers in all sort of cool ways. The US Navy used analogue mechanical computers for calculating where to target enemy ships, for example ([This](https://www.youtube.com/watch?v=s1i-dnAH9Y4) video explains how they worked, if you're at all interested). You could, if you wanted to, build an entire computer out of sticks if you want, but I don't recommend it. It would be very big, very slow and also very loud.

If you want to understand how modern electronic logic gates work, see some of the other responses here. But if you want to understand how they can work in general, then carry on reading this.

The thing main thing about logic gates (and computing in general) is encoding. That's all there is to it. For example, we could say that a lit light corresponds with logic 1, and an off one with logic 0. We could just as easily do the opposite, with a lit bulb corresponding with logic 0. The point is that it doesn't matter what you use, so long as you are consistent with it.

I've made an [album of diagrams](http://imgur.com/a/HCHME#0 ""Sorry for the quality"") for you, to help explain using (wait for it) sticks. I've assumed you're familiar with symbols for logic gates; if you aren't, [here](https://en.wikipedia.org/wiki/Logic_gate#Symbols)'s Wikipedia's entry for logic gates. My apologies for the crappy MS Paint quality, I wanted to get it ready for you as soon as possible.

As TheBlueShrike said [here](http://www.reddit.com/r/askscience/comments/1qwqb7/how_are_logic_gates_andorxor_etc_built/cdhik2v), redstone in Minecraft can help a lot with visualising logic. As an example of just how far Minecraft can go, someone [built an ALU](http://www.youtube.com/watch?v=LGkkyKZVzug&amp;feature=player_embedded ""Arithmetic and Logic Unit"") out of redstone. (The circuit was based on a [computer you can build yourself](http://nand2tetris.org/ ""Nand2Tetris""). It's semi-related to what you've asked: you build a computer from NAND gates up).",null,1,cdhk1di,1qwqb7,askscience,new,2
rocketsocks,"Transistors can be used natively as NAND gates. The rules of logic show that you can build any other logical operations using just NAND gates, so all you have to do is wire up transistors in different ways to achieve any possible logic circuit.

Consider NOT, for example, that's just ""NAND(x, 1)"" or ""NAND(x, x)"". AND is then just ""NOT(NAND(x, y))"". OR is just ""NAND(NOT(X), NOT(y))"". NOR is just an inverted OR. XOR is a bit more complicated but I think you get the idea.",null,0,cdhn4mr,1qwqb7,askscience,new,1
null,null,null,6,cdhk32u,1qwqb7,askscience,new,6
iorgfeflkd,"Consider 1/2+1/4+1/8+1/16... each term is smaller than the last and each additional term brings the series closer to 2. This is a convergent series, each additional term brings it closer to converging on a finite value.

Consider 1/2+1/3+1/4+1/5+1/6+1/7... this series does not converge. It reaches infinity with an infinite number of terms, or an arbitrarily large number with a very large number of terms.",null,5,cdh9rsp,1qwqfu,askscience,new,16
Weed_O_Whirler,"Your first impression may be that if you add up an infinite number of anything, that their sum must be infinite. However, this isn't necessarily the case. 

The most common example is the series 1/n^(2), where n is integers. Writing out the first couple of terms you get 1/1 + 1/2 + 1/4 + 1/16 +... You can try it, keep typing as many of those as you want into your calculator, and you'll see that while the sum is (of course) always increasing, no matter how many you punch in, it will never get larger than 1.7 (in fact, it goes to ""pi squared over six""). 

If this is still confusing, imagine an even simpler series: 0.9 + 0.09 + 0.009 + 0.0009... you can see that each term of this series will get you a little closer to 1, but you'll never be larger than 1. So from these examples you can start to see how series convergence works. You are adding up an infinite number of things, but if the series gets ""smaller"" quickly enough, the sum can be contained. ",null,7,cdhcdk7,1qwqfu,askscience,new,19
TheBB,"A series converges to a value *L* if you can get a finite part of the series to have a sum as close to *L* as you like by picking enough terms. (This is called a partial sum. By ""enough terms"" I mean that the partial sum should obey the closeness criterion for any number of terms larger than a lower bound.)

A series converges if a series converges to *L* for some *L*.

Maybe easier is to just think in terms of partial sums. A series converges if the sequence determined by its partial sums converges in the sense of sequences.",null,1,cdhak5m,1qwqfu,askscience,new,4
__Pers,"Convergence means that the sequence of partial sums of the series (the nth partial sum just means adding up the first n terms of the series) approaches a limiting value h. This means that for each positive ""error"" value that you might define, call it epsilon, you can always find some finite number of terms N such that when you sum at least N terms in the series, the result is within epsilon of the sum of the infinite series. 

Incidentally, there are special and useful kinds of divergent series called asymptotic series that don't converge in the sense above, but yet often yield a very good approximation to a function in the neighborhood of a reference point, provided you only take the first few terms. ",null,1,cdhanph,1qwqfu,askscience,new,4
rlee89,"It means that there exists some number, such that you can arbitrarily close to it, merely by adding enough terms in the sequence.  Put another way, no matter how close you want to get to the limit, there is some sufficiently large number of terms you can add to get that close.

One caveat to that is that the series needs to stay near that limit even if you add more terms.  In other words, after you add that sufficient number of terms, it won't get further away than that arbitrarily picked closeness (and as a caveat to that caveat, the needed number of terms so that it stays close could be more than the first time the series gets that close to the limit).

Contrasting with divergent series (which diverge to infinity in a monotonic fashion), a divergent series is one in which you can reach any arbitrarily large number by simply adding enough terms.  There can be no finite number as the limit, because it will exceed any arbitrary number by any arbitrary amount you care to pick.",null,0,cdhg0rt,1qwqfu,askscience,new,3
Ag3ntD,"If you are down with integrals (area under the curve) converging or diverging, then you will be fine with series!  As you said, if a definite integral (of a positive function) converges, then there is finite area under the graph of that function.  Likewise if a series converges then we mean that we can ascribe a finite value to that sum.",null,0,cdhs702,1qwqfu,askscience,new,1
Osymandius,"Yes and no. 

Yes: Horizontal gene transfer has previously occurred between mitochondria and chloroplasts to the eukaryotic nucleus. This falls in line with their having been prokaryotes once upon a time. 

We see eukaryotic signalling stubs in bacteriophages (viruses). [Snazzy paper](http://www.pnas.org/content/early/2012/10/17/1216635109.abstract)

Aphids have ""managed"" to acquire their own biosynthesis pathways for carotenoids rather than from a dietary source. [Another snazzy paper](http://www.sciencemag.org/content/328/5978/624)

No (or at least no in the way I suspect you're asking): The problem with horizontal gene transfer is it's from one cell to one cell. This isn't a problem if your whole organism is one cell. But if you happen to have 5x10^10 or so cells, then the significance of lateral gene transfer in a single cell is somewhat abrogated. However, it does happen on an evolutionary time scale, but not in the way that bacteria have lateral gene transfer and immediately gain antibiotic resistance in 1 generation.

",null,1,cdhe0s6,1qwqr1,askscience,new,6
SqrrlBait,"There's a new paper out by Coelho on HGT in fungal species with data showing that Aspergillus has genes from Candida species.  So, yes, it occurs.

Coelho, M. A., Goncalves, C., Sampaio, J. P. and Goncalves, P. (2013) 'Extensive intra-kingdom horizontal gene transfer converging on a fungal fructose transporter gene', PLoS Genet, 9(6), e1003587.",null,0,cdhml32,1qwqr1,askscience,new,2
I_am_Bob,Most of the earths water probably came from comets and asteroids after the surface had cooled a bit from planetary creation. Oxygen is produced by stars during fusion. So when a star supernovas it ejects the oxygen as well as unused hydrogen. After they cool off a few million degrees they may end up colliding and bonding to form water ice crystals that eventually amass in to comets. ,null,0,cdhborc,1qwrrm,askscience,new,3
_NW_,"[Resonance](https://www.google.com/#q=resonance) is when you excite something at its natural frequency.  Think of a tuning fork, and then apply that idea to just about anything.  Pushing somebody on a swing, bouncing on a diving board, a flag waiving, a bell ringing, a guitar string.  Basically, anything that can sustain a periodic motion has a preferred or natural frequency.  Constructively encouraging that natureal frequency is resonance.",null,0,cdhcm4o,1qws23,askscience,new,1
quarked,"Resonance is a phenomenon that can occur for any system with a natural frequency. If you *drive* a system at the same frequency as its natural frequency, you will pump energy into the system.

Try taking some object attached to a string to make a simple pendulum. Pendulums have a natural frequency they will oscillate at if you let them swing (f~(g/l)^(1/2)). You can see the natural frequency for yourself by letting the pendulum swing freely. Now try *driving* the pendulum by holding the string and moving it back-and-forth. 

If you move the string very slowly, the pendulum won't swing (it will just move with the string). Similarly, if you move the string very quickly to and fro, the bob won't move (it doesn't have time to ""respond"" to the string's rapid movements). The pendulum doesn't ""see"" oscillations very far from its natural frequency. But, if you move the string back-and-forth around the pendulum's natural frequency, it will resonate and the pendulum will swing higher and higher (until the oscillations become too big and it no longer behaves regular). At *resonance*, you are driving energy into the system. You might say that at resonance, the system most easily ""accepts"" the energy driving it.

",null,0,cdhdh2l,1qws23,askscience,new,1
SwedishBoatlover,"Another way to look at resonance is that you are in resonance when you can keep the amplitude of an oscillation constant with the least possible effort.

Think of pushing somebody on a swing. You want to keep the amplitude (i.e. the swing goes to the same height every time). If you push right after the swing has reached it's peak, you don't have to push very hard to keep the swing going. You are in resonance with the swing. Now instead start pushing when the swing is at it lowest point. You will have to push really hard, first absorbing the energy currently in the swing, then you need to push it back up. You are out of resonance with the swing. ",null,0,cdhgjz3,1qws23,askscience,new,1
TheBB,"You can't define medians of infinite sets^(1). As you say, there's no reason to prefer zero to five or negative five.

^(1)Well, I guess it could be generalized to finite measure spaces.",null,4,cdhbdw5,1qwsmr,askscience,new,23
DarylHannahMontana,"Well, yes and no.

I mean, if you are just asking ""what is the median of the set of all integers"", then there isn't one, because, as you argue, there are the ""same amount"" of other numbers below and above any given number, and so there's no good reason to pick one number over another.

On the other hand, if your set is ""all real numbers between 0 and 4"" there are an infinite amount of numbers in that set, so given any number x between 0 and 4, there are an infinite amount of numbers below x and an infinite amount of numbers above x.

However, in this case, you can make a good argument for why 2 should be the median of this set, it's pretty intuitive that it should be, and this can be defined in a mathematically rigorous way also.

Likewise, with a more complicated probability distribution, say the weight of all people ever born, we have to consider every real number between 0 and, say, 1500 lbs as possible (since a person's weight changes over their life, let's consider their maximum weight). Thus we have an infinite amount of weights to consider (we're pretending we have a scale that's accurate to an infinite precision). Nonetheless, we can still talk about the median of this set; it's just the number such that exactly half the people to ever live weighed less than this, and the other half weighed more than this.",null,0,cdhfxf0,1qwsmr,askscience,new,8
rlee89,"&gt;Basically if there are an infinite amount of numbers, then isn't any number technically the median of that infinite set, since there are is an infinite amount of numbers above and below that number?

I believe that the issue with that lies more in that you are attempting to run statistics on an unbounded uniform probability set than an issue with the definition of median.  Such a set breaks *several* statistical measure, primarily as a consequence of the probability measure of any bounded subset of elements being [measure zero](http://en.wikipedia.org/wiki/Null_set).

If we wish to generalize the notion of a median to an infinite set, the most intuitive way to do so would be to call the median the point at which the integral of the probability density of the regions below and above that point are equal.  This works well for any set in which the elements can be assigned nonzero probabilities or probability densities.  There are some minor technical complications with integration in the case of discrete sets, because you end up integrating over Dirac deltas, but this issue is not that hard to handle.",null,0,cdhfpeb,1qwsmr,askscience,new,4
protocol_7,"The notion of a ""median"" is a property of [probability distributions](https://en.wikipedia.org/wiki/Probability_distribution), not of sets of real numbers. (This gets conflated with finite sets because there's a natural choice of probability distribution, namely the uniform distribution.) There's no uniform probability distribution on the set of integers, so the integers don't have a natural choice of probability distribution.

As a result, it doesn't even make sense to talk about ""the median of the set of integers"". To elaborate, what if you were to ask for ""the sum of a set of five integers""? Well, the question doesn't quite make sense, because some key information — which five integers are being summed — is missing. Likewise, given any probability measure on the real numbers, you can talk about the median, but it doesn't make sense to talk about the median if there's no probability measure.",null,0,cdhu9ik,1qwsmr,askscience,new,2
king_of_the_universe,"That's like saying ""If the universe is infinite, isn't every point the center?""

But that's not true. ""Infinite"" means that there is no end. Hence, in the universe case, the exact size of the stretch *is undefined*. You can't say ""? divided by 2 = undefined, hence I'll just say that *every* value is the correct result"".",null,0,cdhue65,1qwsmr,askscience,new,2
Spiralofourdiv,"I believe you are knocking on the door of [measure theory](http://en.wikipedia.org/wiki/Measure_(mathematics).

If you are defining the median to be the element on an ordered set such that there an an equal number of elements on each side of it, then no, a median cannot be defined on an infinite set.

However, that's not necessarily what ""median"" means in what we call a ""metric space"". All a metric space is is a space that has a ""notion of distance"". What that means can be ignored for the most part aside from exactly what you think it means: you can take two points in the space (elements in the set) and there exists the idea of a ""unit"" and that we can count how many units their are between two points. That is, a definition of distance exists, and thus we can use numbers as ""measurement"" of this distance.

The real line is a metric space. Take the set [0,2]. It's an infinite, ordered set, right? But we know that the median is 1. How can that be? The reason is because we've kinda redefined ""median"" within a metric space to mean something different: now a median is the value such that there is equal **distance** between it and the endpoints of the set. A median now describes equal distance and NOT equal number of terms. 0.5 isn't the median even though there are infinitely many elements between it and either of the endpoints of the (ordered) set, the metric is NOT the same, it's 0.5 on one side and 1.5 on the other. If the set isn't infinite, we care not whether it exists in a metric space, and as long as the set is ordered we can use the element definition of median.

Now consider (-inf, +inf). Well, that's a metric space too, but all points are equally distant from the endpoints, so no median! We thus have one more stipulation:

The metric space must be finite. That is, since we have a metric, the endpoints of the set must have a finite measure between them, if they don't, then the measure between a point and at least one endpoint will be infinite and thus meaningless.


So finally: Yes, you can have a median on an infinite set if it's a finite, ordered metric space, and perhaps other structures, but you are right that the notion of infinity does mess up defining a median for certain sets.",null,0,cdkxla1,1qwsmr,askscience,new,2
null,null,null,5,cdhgztd,1qwsmr,askscience,new,2
iorgfeflkd,"It's better to think of momentum as something that moving objects have, rather than an explicit product of mass and velocity. p=mv just applies to slow moving objects, as an approximation.

The full relationship between energy, mass, and momentum is E^(2)=(pc)^(2) + (mc^(2))^2",null,42,cdhaiv4,1qwtsc,askscience,new,314
floydos,"E = mc^2 is an approximation of:

**E^2 = m^2 c^4 + p^2 c^2** 

Since photons have no mass (m = 0), the equation becomes:

E^2 = P^2 c^2

*This is quite easily rearranged to* 

p = E / c

*and since E = hf*

**p = hf / c**

*and since Wavelength = c / f*

p = h / Wavelength
",null,10,cdhhrvp,1qwtsc,askscience,new,60
rupert1920,"You can also check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/g3qlc/is_light_massless_why_is_it_affected_by_gravity/) in /r/sciencefaqs, for common questions.",null,3,cdhd8ph,1qwtsc,askscience,new,13
cdstephens,"p = mv is not true in a modern context. The momentum of light is written as p = hf/c. p = mv is just an approximation. 

Depending on who you ask, you can also say photons have a relativistic mass, where the relativistic mass is defined as E/c^2, and for an object with a nonzero rest mass, m0 * gamma (the same gamma used in special relativity). This in part gives rise to the suggestion that energy and mass are directly connected; that an increase in energy in something should increase its apparent mass. I think there's some minor controversy though about whether it should be taught.",null,2,cdhd8ii,1qwtsc,askscience,new,9
brwbck,"I figure I'll add to this by explaining why light can impart momentum even under the classical EM wave theory.

It has to do with the fact that light has both electrical and magnetic components. When a light wave interacts with a charged particle, the electric field exerts a force on the particle, and the particle moves. Now that the particle is moving, the magnetic component of the wave will affect it. The force of a magnetic field on a moving particle is perpendicular to both the motion of the particle and the direction of the magnetic field. This produces a force in the same direction as the wave is traveling. Thus the light pushes on the object.

The question of where the momentum is ""stored"" before the wave interacts with something is hard to explain, and is one of the weaknesses of the pure wave theory.",null,3,cdhlqfj,1qwtsc,askscience,new,7
Gimlis_Axes,"Photons have no mass? I thought that a black hole is 'black' because it's gravitational attraction is stronger than the speed of light and does not allow any light out. So I figured light must be subject to gravity and therefore have mass. Or am I, as usual, making an ass of myself. ",null,3,cdhktqb,1qwtsc,askscience,new,6
mc2222,"A good rule of thumb is that *light travels as a wave but interacts with matter as a particle*. This means that any interaction with matter - absorption and emission - must occur in discrete chunks of energy  which we call photons.  For all other cases, it's often simpler to think of light in terms of EM waves.

Light is an electromagnetic wave - it is a changing electric and magnetic field that move through space together.  Waves are fundamentally a way that *energy* moves through the universe.  Waves are often (always?) associated with some type of field (like the electric field).  A wave has the ability to move objects that come in its path, for example, a water wave will move a boat up and down, or a wave in the electric field will cause an electron to move.  Since waves can exert a force on an object and move that object, this means they must also have momentum.",null,3,cdhdtow,1qwtsc,askscience,new,5
circleget,"you are correct in both assumptions of energy and momentum in relation to its mass. However, through experiments regarding the photoelectric effect one has to consider the particle property of light whose energy is no longer associated with the intensity of the light source, and instead is proportional to it's frequency (E=hf). when you substitute this into E=pc you find that the momentum of light is now P= (hf)/c, no longer requiring mass to define momentum.",null,0,cdhhued,1qwtsc,askscience,new,3
carlinco,"Another way to look at it is through pure wave mechanics. When something creates light, it basically stirs up vibrations, not unlike a stone thrown into a pond. Those vibrations spread out, not through momentum, but through exciting whatever is around them. The next excited place around will again excite everything around it, in any direction - so no real momentum, only an apparent momentum at the circular front of the ""excited"" area. And when those waves hit something, that thing might get moved around - like a stone lying in the water. Which is pretty close to what happens when light is absorbed for instance by a solar cell, and moves electrons.

This is also why laser rays gets weaker the farther they go - more and more light goes off in different directions, due to lack of momentum.

However, the word momentum can still be used in a meaningful way with waves - to describe the intensity of the ""wave making"" in certain contexts, or the amount of momentum which they can transfer to matter.

The water analogy does not go too far, however, as there is no real equivalent in space, like an ether.",null,1,cdhka14,1qwtsc,askscience,new,3
mrbeanbag,"The bottom line is that if you hit something with a photon, it will move. We can either choose to say that photons have momentum, or we can throw conservation of momentum out (not such a great idea). So we say photons have momentum, and then derive equations that accurately describe their momentum.

While it may feel more intuitive, there is no good reason that massive objects should carry momentum either, except that we would like *something* to have momentum so that we can actually do physics.",null,2,cdhkdhk,1qwtsc,askscience,new,6
CBunneh,"Photons have a momentum given by De Broglie's equations which are derived from E = mc² ____ m=E/c² _________ As E=hf, therefore we can state that: m=hf/² ______ p=mv ______ p=(hf/c²).c ______ p=hf/c _______________ As c here is the velocity of light in a vacuum, then this is also v and as _______ v= fλ ________ Therefore: p=hf/fλ _____ and thus we have found that as the frequencies cancel out, we can say that: p=h/λ     From this derivation, we can say that we can that p can occur without a need for mass.",null,3,cdi9im8,1qwtsc,askscience,new,4
Hekatoncheir,"There are some studies regarding the relationship between lower metabolic rates and longevity, like this one http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2881137/

Basically, the idea is that the slower the rate of everything happening in an organism, the longer it can live for relative to the same organism in a normal condition.",null,6,cdhdjye,1qwu5y,askscience,new,48
Mackerie,"Memory research is such a hot topic in Psychology/Neuroscience right now. There's so much research out there because it's so easily testable.  

There's 3 stages of memory, encoding, storage and retrieval and changes in any of these stages will affect how a memory is stored and subsequently how it is recalled. For normal people (ie: people without lesions in their brains), the encoding phase is, I think, usually the most important in memory consolidation. 

There is a theory that elaborative rehearsal will aid memory. For example, when you see a stimulus, you can shallowly process it (ie: just look at it) or you can make more connections with other systems in your brain (ie: I saw this yesterday too or this other time, it was blue etc...). Elaborative rehearsal will always help encode memories better. This is why teachers and profs suggest finding a way to personalize the information you study or new ways to synthesize information. 

Another factor is what type of memory you're trying to recall. Episodic memories (memory of personal events - what we usually consider 'memory') will always be easier to recall than semantic (facts) and 'feel' more poignant because there's a lot of details associated. One theory that might explain why is that semantic memories are more generalized and abstract version of episodic memories to allow for application across different contexts and thus less details.  

Emotional impact is also a large factor as well. Events that are more emotionally salient will be remembered better than non-emotional events. There are various studies of this and if you want, I can suggest a few.  

Also you can't forget about learned associations (this is more into the realm of implicit memory (ie: skill memory and priming etc...). For example, if I always associate a certain classroom with a time when I failed a test, then I will always activate certain negative stereotypes when entering the room (a context) and end up activating other associated memories and thought about failing tests or times when I've failed.  If you have strong associations between a context and a certain stimulus, you will have stronger activation scripts for activation of memories of that stimulus.  

Hopefully this will satiate your interest in memory research slightly until someone else can come along with a more detailed answer! c:

TLDR: the brain is mf'ing confusing. ",null,0,cdhe2fe,1qwuy1,askscience,new,3
tishtok,"In addition to /u/Mackerie's good summary, when you're remembering a memory you've remembered in the past, you're often recalling a memory of your memory, not the original memory. So if you've recalled something recently, even if the original episode happened a longer time ago than a more recent episode, it may be easier to recall. ",null,0,cdhf3eo,1qwuy1,askscience,new,3
patchgrabber,"There are a variety of reasons, but they all revolve around an increase in the number of nutrients needed for algal growth. All lakes sit somewhere on the [trophic state index](http://en.wikipedia.org/wiki/Trophic_state_index), which is a measure of the productivity of a body of water. Virtually all lakes (left to their own devices) become more eutrophic over time, even though many may start as oligotrophic. This is a natural process of life; as organisms die their biomass sinks to the bottom becoming sediment and the nutrients from their dead bodies/cells/etc. are recycled back into the water for other organisms to use. 

Over enough time (think centuries), the sediment builds up and begins to fill a lake from the ground up, but more nutrients keep getting added, so there is more available for organisms like algae to use. The most important of these nutrients are nitrogen and phosphorous. Now people, by agricultural runoff or dumping of industrial waste or other processes add even more nitrogen and phosphorous to the water, speeding up eutrophication of that body of water. Lake Eerie had a big problem with this a while back, and Eerie is further along on the trophic index than the other Great Lakes. So as time goes on, unless we actively counter the eutrophication of these lakes, they will eventually all become like Eerie, and this will happen sooner because we are actively adding to it.",null,0,cdhbukh,1qwv8i,askscience,new,30
shiningPate,"One of the reasons algal blooms are increasing in recent years is the increased application of animal waste directly onto farm fields, and the phosphorous that subsequently runs off from them. Phosphorus was at one time widely used in household detergents and is included in farm fertilizer. During those times, there was a great deal of phosphorus flowing into the streams and lakes. With the creation of the EPA and passage of the clean water act, much of that phosphorus was eliminated from urban effluent and farm runoff but... it could be regulated because industrially produced fertilizer has a defined content. Its application could be regulated. The increasing practice of using animal waste direct on the fields is in effect an end run around environment regulations on chemical fertilizer. Animal waste ""is just shit"". There isn't a factory producing it that certifies it contains X pounds (or Kg) of phosphorus per ton of fertilizer applied. There have been attempts to begin regulating the use of animal waste as fertilizer, but the farm lobbying groups having previously had their ability to put unlimited phosphorus on their fields taken away have stymied the EPA's attempts to regulate it. It is currently a bone of contention between the states whose runoff enters the Chesapeak Bay. There are lots of dairy farms in NY and PA whose cow pastures drain into the bay. Similarly in Maryland and Virginia, farmers routinely apply chicken manure on their fields. Even though the bay is choking under the nutrient loads in the run off water, the farm groups have so far managed to fight off most regulation of the  application of manure and farm field run off into the bay. ",null,0,cdhdsnv,1qwv8i,askscience,new,8
un-scared,"One of the grad students a couple offices down from mine just gave a talk on the lake Eerie algae blooms recently. From what I remember it's simply a case of nutrient inputs (particularly phosphorus) increasing again. I say again because his isn't the first time algae blooms have been a huge problem for lake Eerie, what makes these notable is that phosphorus levels were reduced in the 2000s and the lake cleared up significantly. For some reason (I can't actually remember why, seem to recall most of the blame was on agricultural runoff) the phosphorus inputs have increased again in the last couple years causing these large blooms.

If the nutrient inputs are decreased again the lake should clear up significantly but that's much easier said than done.

",null,0,cdhpghl,1qwv8i,askscience,new,1
null,null,null,1,cdhemhf,1qwv8i,askscience,new,1
baloo_the_bear,"Both have their advantages. Laying eggs saves the mother from needing to carry the fetuses for an extended period of time during gestation, and is 'cheaper' in a metabolic sense. Giving birth to live young is more expensive metabolically (meaning the mother will need more food) but the offspring are less vulnerable (and more mobile) than their shelled counterparts. 

One of the major things that has affected the evolution of live birth is head size. One of the reasons human babies are so helpless when born while a deer can plop out and start walking around immediately is that the head size required to fit a human brain is way too big for a human female pelvis to birth. In contrast, however, a deer does not require such a complex brain and therefore it can develop to a higher degree *in utero*. This is also why babies' skulls are not completely developed at birth, because the skull literally needs to be able to squeeze through the birth canal.",null,23,cdhc8q4,1qww57,askscience,new,168
r_n_b,"All those referring to head size and bipedal movement as reasons for early human births are mentioning an outdated theory.

In reality, the fetus is born when the maximum level of metabolism is reached in the mother i.e. the point at which the mother is unable to produce any more energy or metabolize fuel, regardless of the number of calories consumed.

This value is so accurate in predicting births that you can track the metabolism of a mother via her breathing or urine (can't remember) and estimate how long she has been pregnant. 

Found it:
""My metabolic rate was about double what you'd expect – for a non-pregnant woman. But I was five months pregnant: I was providing energy for the developing foetus too. When my metabolic rate rose to 2.1 times the normal rate, I would go into labour – my body would no longer be able to provide the energy being demanded by the foetus.""

[Source](http://www.theguardian.com/science/2013/jun/30/childbirth-metabolic-rate-obstetric-dilemma)

",null,6,cdhjmbb,1qww57,askscience,new,44
floppylobster,"Live births are quick events and can be done on the move. Eggs are susceptible to attacks from small mammals. And once you have a lot of small mammals eating eggs, while live-birthing their own offspring, the balance soon shifts to live births being more common.

Also, 'from an evolutionary stand point', what works, works. What is, is. Just because something is better does not mean evolution selects it. Evolution is survival of the fittest. As in ""survival of what fits"", not what is strongest or best. Just what works best in the current environment.",null,5,cdhiotd,1qww57,askscience,new,23
Fix_Lag,"Eggs can only contain so much nutrition which the fetus can use to develop before an organism has to hatch.  There is a limit to egg gestation periods based on shell strength because the more nutrition material you try to cram into the egg, the stronger the shell has to be to support the weight.

Live births, obviously, do not have this problem, and as such most organisms with complex brains have live births.  Live birthing does, however, have the problem of size--an organism that grows too large while gestating will die during birth (possibly killing the mother) and if it is too small it will be underdeveloped and not survive.  In humans the size issue is often that a baby's head is too large to fit through the hole in the pelvis.",null,0,cdhk0vi,1qww57,askscience,new,3
ragingclit,"Squamate reptiles (i.e., lizards and snakes) have evolved viviparity more times than any other other vertebrate radiation. The predominant hypothesis for the evolution of viviparity in squamates is that viviparity evolves in repsonse to colder temperatures. This is supported by a number of studies, including [this recent large-scale analysis across Squamata](http://onlinelibrary.wiley.com/doi/10.1111/ele.12168/abstract?deniedAccessCustomisedMessage=&amp;userIsAuthenticated=false). I'm wary of some of the estimated speciation and extinction rates and ancestral state reconstructions, but the correlation between temperature and parity mode is solid.",null,1,cdho3jw,1qww57,askscience,new,4
joe12321,"To expand and stress a little bit of what floppylobster said...

From an evolutionary standpoint, If xxx-trait is better, why does this species do/have yyy-trait? is not a great question.  

(To play a little fast and loose with anthropomorphizing the concepts...) At no point does evolution get to look through all the possible solutions to a problem and choose the best.  Rather, when a change in a species occurs, if it enhances reproduction, it may stick. 

So let's say (oversimplifyingly) a species starts giving live birth, and it works well.  It's gonna go on and keep working!  And all further evolution will be around that behavior.  Even if laying eggs works 20% better, there's no intelligent mechanism that will wait around for or design that behavior instead.",null,1,cdhs3yd,1qww57,askscience,new,3
dawgfan64,"In a sort of related question; why did either laying eggs or live birth arise when reproducing asexually was much more efficient? To put it another way, how/when did organisms make the jump from reproducing asexually to reproducing with sexual organs?",null,0,cdhpp2f,1qww57,askscience,new,1
M4rkusD,"It's got something to do with r-K selection. r-selection happens when you're a species that gives birth to a large number of offspring (sometimes in the millions) but don't invest a lot of energy in parental care. K-selection happens when you only get a limited number of offspring but invest a lot of energy in parental care. Viviparous animals (like most mammals) are generally K-selectors. So with only a limited amount of offspring they can carry their young around inside of them for an extended period. Egg-laying animals (Reptiles, Amphibians &amp; lower) are generally r-selectors. Fish can lay millions of eggs (so they don't have the room to gestate them inside of their body, purely due to the numbers) and don't bother with parental care.",null,0,cdhvmdw,1qww57,askscience,new,1
Diamond_Jared,"Quality and quantity. If you're going to have a lot of kids, eggs are probably a good idea since you don't waste as much time and energy. If you're just going to have one kid, you need it to be ready, to be crass, right when it gets out, and a lot of live birth animals are more or less.",null,1,cdi1dig,1qww57,askscience,new,1
fishify,"While there is no verified unification of those three forces, here is the way it works mathematically (depending on your background, this may or may not be sufficiently non-technical).  The electromagnetic, weak, and strong forces are associated with a number, a 2x2 matrix, and a 3x3 matrix, respectively.  Grand unified theories that unify these three forces embed these matrices in a single larger matrix.",null,1,cdhm357,1qwz5t,askscience,new,7
MCMXCII,"The idea of unifying the strong, weak, and EM forces is known as a Grand Unified Theory (GUT). So far, there isn't one. It's still a work in progress. The weak and EM forces have been unified, in that they behave the same way at high energies. But so far no one has been able to add the strong force into the mix. And gravity is a whole other beast in and of itself.",null,4,cdhig1s,1qwz5t,askscience,new,5
GeoGeoGeoGeo,"It depends entirely on the type of glacial ice that covers the region as to how much the landscape is altered. Temperate or warm (wet) based glaciers are very erosive though the debate between what contributes more to topography (the melt water or the ice itself) is still debated. As the degree of sliding is largest within these types of bodies of ice, they tend to alter the landscape quite dramatically, forming glacial valleys (the classic U shape depicted in text books). They also form horns, arretes, cirques closer too their formation point within the accumulation zone. Large bodies of water can form in the glacier (englacial), below (subglacial) on top (supraglacial) in the front (proglacial) or on the sides (lateral), such as lakes, braided river systems and during deposition can create massive deltas, kames, morraines, eskers, drumlins, crag-and-tails, etc. 

Summary of temperate (warm or wet) based:

- ice is at the pressure melting point throughout the glacier
- thus the mean annual temperature of the ice is about 0o
- geothermal heat and melt are concentrated at the glacier bed
- abundant meltwater and high glacier velocities from sliding 


With cold based glaciers, also known as polar, the body of ice is below the pressure melting temperature from surface to base and so there is little movement (only internal creep or deformation of the ice - no sliding on along the base). These types in fact *[preserve](http://www.livescience.com/38696-greenland-ghost-glaciers-prevent-erosion.html)* the landscape beneath them! Revealing ancient forests and [hardy plant life](http://www.nature.com/news/wild-flower-blooms-again-after-30-000-years-on-ice-1.10069) 10's of thousands of years old.

Summary of cold-based (polar) glaciers:

- the ice is below the pressure melting point throughout the glacier
- that is the glacier is frozen to its bed (permafrost) and thus can move only by internal creep ",null,0,cdio9zr,1qwzqt,askscience,new,1
JDL523,"The landscape changes dramatically through glaciation. As glaciers move over the landscape, they gouge and scrape away large amounts of materials. As the glaciers retreat, all the material they picked up gets left behind, and forms a large variety of landforms (hummocky terrain, undulating terrain, drumlins, and eskers to name a few). Further, you'll get glacial lakes forming, like Lake Agassiz, which, when they let go, can carve out large valleys, as millons of cubic meters of water are released from them. ",null,1,cdi8sg2,1qwzqt,askscience,new,1
stuthulhu,"You are correct. Your shove would essentially propagate through the material at the speed of sound (in that material).

This question often comes up in terms of a ""perfectly rigid"" object being suggested as a means of communication faster than light, the problem being nothing is perfectly rigid.

Here are some related posts that may provide you more information:
http://www.reddit.com/r/sciencefaqs/comments/fj1qd/if_i_had_an_infinitely_stiff_rod_could_i_push_and/",null,20,cdhg3rz,1qx01x,askscience,new,172
opsomath,"Well, your 2x4 would weigh about 65,000 pounds (30,000 kg)So, your shove would do very little indeed.

The vibrations from your shove would travel at the speed of sound. Assuming you pushed with enough force to move it 1 meter in a short time (say a second) that's an acceleration of around 30,000 kg*m*s^(-2), 30,000N. The buckling strength of a 2x4 is about 2400N. So, the 2x4 would be shredded by the force involved.

tl;dr You'd break your board.",null,1,cdhyhfe,1qx01x,askscience,new,9
null,null,null,61,cdhnqcu,1qx01x,askscience,new,12
iorgfeflkd,"It was actually pretty low tech, they just used a really long rope with a weight at the bottom and saw how much rope was pulled before it stopped.",null,1,cdhiolz,1qx1qy,askscience,new,6
atomfullerene,"I can't think of a specific example, but it's pretty likely, for two reasons.  First, experimental design can be flawed, leading to people being really certain of wrong things happening.  Second, there are a _lot_ of papers out there.  Enough that even 10^-4 is likely to pop up once in a while",null,1,cdhj68q,1qx255,askscience,new,4
Schmetlappio,"SWCNTs of appropriate diameter and folding angle have been demonstrated to have such a high potential for hydrogen storage that there are groups researching their application in fuel cells for hydrogen powered vehicles. We haven't been able to match this storage experimentally, but it should eventually work. As for ""shooting protons down them"" I'm not sure what you mean. ",null,1,cdhtusa,1qx46i,askscience,new,2
__Pers,"It is possible to do so. [Here](http://prst-ab.aps.org/abstract/PRSTAB/v6/i3/e033502) is one recent article. 

Edit: Here's a [press release](http://www.osaka-u.ac.jp/en/news/ResearchRelease/2013/04/20130409_2) for another, recent article that includes a graphic of how protons are accelerated. ",null,0,cdhwopm,1qx46i,askscience,new,1
Doener_wa,"Carbon nanotubes (CNT) may have different diameters. According to [this paper](http://cmliris.harvard.edu/assets/JPCB_106_2429.pdf) the diameter depends on your reactants. It may be quiet large (up to 12 nm), so yeah I think you could to this since protons are realy small, way below 1 A (10^-10 m). ",null,5,cdhgu1m,1qx46i,askscience,new,2
do_od,"Spacecrafts like this usually have instruments and components that are sensitive to contamination, and really expensive too. The overall scale of investment make it logical to do everything practically possible to limit the risk of failure in every stage of spacecraft manufacture. The clothes help limit airborne particles (dandruff, vapour from respiration, coughs and so on) and are only part of a grand scheme of keeping spacecraft assembly as clean as possible. See [clean room](http://en.wikipedia.org/wiki/Cleanroom). Also it looks awesome. ",null,2,cdhgw3p,1qx6ad,askscience,new,9
rupert1920,"You should note that receptor-ligand interactions are never limited to _one_ ligand per receptor, so it's not like there are only 5 molecules taste receptors respond to. Likewise, it's not like there are several thousand different receptors, each for the unique chemical your nose is sensitive to.

What part of the post you linked to doesn't answer your question? [This comment](http://www.reddit.com/r/askscience/comments/11385w/what_is_the_mechanism_by_which_an_atom_or/c6iy6yt) and the resultant discussion gives a good overview of the study of olfaction - not to mention that the [Wikipedia article](http://en.wikipedia.org/wiki/Olfaction#Study_of_olfaction) lists the competing theories on how the receptors interact with the ligand.

So knowing all this, your question:

&gt; ... is there a more comprehensive source of images of all such chemicals humans can sense?

is akin to asking if there is a comprehensive list of images of all chemicals humans can see, or touch. If such a list exists, it'll be a _very big_, if not limitless, list, and there isn't much purpose in making such a list. The closest you can get is any [MSDS](http://en.wikipedia.org/wiki/Msds) database - just look up any chemical and you'll find, under physical characteristics, whether it has an odor.",null,1,cdhgpwh,1qx7t3,askscience,new,5
lilitryan,"The olfactory system is very complicated actually. You don't smell different organic molecules because each organic molecule can have more than one functional group, and olfactory receptors are based on functional groups of the of the odorant. When you smell something and the molecules get in your nasal cavity, the receptors on the olfactory cells will detect all the different functional groups on those molecules and each type of receptor will send the information to the corresponding glomeruli, and glomeruli in turn will send the information they receive to the thalamus where all the different functional group signals will be put together to create a distinct smell perception. So each olfactory receptor recognizes different functional groups on an odorant molecule. The main point is different odorant molecules will trigger different combinations of responses from the olfactory cells. That is why we can smell so many different variations of odors.",null,1,cdj5g42,1qx7t3,askscience,new,2
Kegnaught,"Lactase persistence is the ""official"" title given to our ability to continually produce lactase (the enzyme responsible for the breakdown of lactose), and this persistence appears to be unique to humans. Right off the bat, this suggests that our long ago ancestors did not have this trait, as no other mammalian species share it. While mammals are capable of producing lactase while they are young, this ability does not last past infancy.

It's believed that it arose within the last 10,000 years due to some aspect of animal husbandry, and our ingestion of dairy products, though the exact mechanisms for its selection are unknown. Furthermore, lactose intolerance is actually correlated with race, and we know that lactose intolerance is actually more common than lactase persistence among the global population, so it appears to be more common in certain ethnic groups than others, providing further evidence that it arose independently among certain ethnic groups at some point in our (evolutionarily) recent past.",null,1,cdhjcyx,1qx8oq,askscience,new,22
DNAthrowaway1234,"Genetic mutations occur at a relatively predictable rate. By comparing the individual DNA sequences of lactose tolerant and lactose intolerant individuals, the minimum number of mutations required to go from one to the other can be estimated, and converted into a measure of time. 

Sources:
http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1182075/

http://en.wikipedia.org/wiki/Coalescent_theory",null,2,cdhoxuu,1qx8oq,askscience,new,11
beetlesaurus,"See (Bersaglieri et al. Am J Hum Genet 74:1111-1120, 2004) for one of the first papers to come up with the 5k-10k ya figure.

It's just math. Population-level genetic events like this are analyzed with statistics that detect how ""coalesced"" (or similar) a population is, how frequent the allele is across a few generation, how well it's associated with other alleles that mark selection, and use that information to estimate the time it took for the genetic pattern to get that way. 

I think a statistician should take it from here and explain the coefficient of selection, etc., because they'll do a better job than me... ",null,0,cdhr04k,1qx8oq,askscience,new,4
Clack082,"Hey I'm just a lowly microbio student but I might be able to help out some until someone more educated comes along. Just don't take anything I say as verified truth.

So a big part of figuring out when specific traits came into existence is looking at the genetic code of different populations. We can look at different populations and see ok this trait only developed after this group became separate from this other group. The ability to digest lactase actually had multiple origins we have determined because there are multiple genes which allow this which occur in different places in the genome. 

So for example, and this is just me talking, say we found the gene allowing for lactase digestion on chromosome four in  northern Europeans but a different gene allowing for it on chromosome ten in Mongolian people. And say Southern Europeans didn't have either gene. This tells us the trait developed in Northern Europeans after they split off from the Southern Europeans. And that Mongolians probably evolved their gene separately and we could look at related populations to determine when that came about. We can tell when these splits occurred by looking at other genes and by comparing what we see genetically with historical and archeological data.

I recommend Dawkin's book The Selfish Gene, where he does a much better job of explaining how gene's are actually the driving force behind evolution and how events like this happen.


Wikipedia has a decent article to serve as a starting point, if you're interested in learning more about the spread of lactase tolerance.",null,2,cdhg2tv,1qx8oq,askscience,new,7
vegetablehater,"As evolution proceeds certain mutations are selected for or against, depending if they are helpful or hurtful to the survival of an organism in the current environment.  The individuals who have the ""good"" mutations will do a better job of accumulating resources throughout their lifetime, or simply not die, thus have more offspring than the individuals without the beneficial mutation, and thus be a larger percentage of the next generation.  

The thing is that, evolution doesn't just pick out a winner and make sure everyone in a population has that adaptation.  Evolution doesn't engineer a solution, it has to work with the variation that already exists in nature and optimize it.  That means that not everyone has exactly the same DNA sequence and you can make comparisons between individuals to see what is different.  

If you look for the specific mutation that expands the expression of lactase to the entire life vs. just as a baby.  You can see that it is centered in Europe, and populations outside Europe have a much smaller percentage of the population with that specific mutation that allows them to do that.  

There are some complicated statistical tests that you can do based on how much of an advantage it provides to be able to drink milk, and the generation time of humans to try and get an idea about when a mutation like that might have arisen, given how prevalent in a population it is now.  In this case they also combine it with some archeological data about how long ago they know humans invented dairy production and kind of put the two together to get that 10,000 year old estimate.  

There are also other ways to calculate how old a mutation is based on specific signatures that evolution leaves in DNA sequence as time passes.  Not every mutation is an important one.  Unimportant changes often get accumulated at a standard rate and you can kind of count up those around a change you care about up to get an idea about how long selection has been acting on a specific gene or region of the chromosome.  This can also tell you if a mutation has been selected for (positive selection), against (negative selection), removed from a population (purifying selection) or a bunch of other stuff depending on how crazy you want to get with your modeling and statistics.  

It is kinda cool that DNA is both the blueprint for building every thing that is alive on earth, as well as a history of what has happened to arrive at that specific version of an organism all at the same time.  

The primary article about the milk drinking in Europe is here and open access (free to read) in case you want to read it for yourself:

http://www.ploscompbiol.org/article/info%3Adoi%2F10.1371%2Fjournal.pcbi.1000491


TLDR:  They combined archeological data, population genetic data and some statistical models to get the 7,500 to 10,000 year estimate.  ",null,1,cdhodwl,1qx8oq,askscience,new,3
FizixPhun,"The heart of your question lies in solid state physics.  This is a subset of quantum mechanics aimed at understanding why solids behave the way they do.  On a quantum level, current is still just the amount of charge that moves through some space in a given time.  The only difference is that the charge is now packaged up into discrete units (electrons, protons and other charged fundamental particles).

To understand how current flows in a material you first have to understand electrons behave in a material.  The key feature of solid state physics is that many materials are crystals.  This means that the atoms are spaced periodically.  As you mention, band structures are the way that we summarize the effect of this periodic potential.  Basically, a band structure just relates an electrons momentum (p=mv=hbar k) to its energy.  The momentum can be positive or negative, the sign only denotes direction.  In free space this is very boring, Energy=(m v^2 )/2 = p^2 /2m=(hbar k)^2 /2m.  When you throw in a periodic potential, this becomes modified and results in bands.   Actually calculating band structures is quite difficult.  The key idea is that there are ranges of energy where the electron can live and ranges of energy where the electron cannot live.  

The electrons in a crystal live in the band structure.  Each atom of the crystal brings a certain number of electrons with it.  They fill the states in the bands starting from the lowest energy.  Each of these states has a specific momentum associated with it.  When a band is filled, the next electron has to be placed in a state in the next highest band.   Applying a voltage to a material is the same as applying an electric field to the material (E=V/l where l is the length of the material).  In the semiclassical picture, electrons with charge -e, feel a force F=-eE in the applied electric field.  This force accelerates the electrons from lower voltage to higher voltage (they are negatively charged so lower voltage is actually higher energy for them as Energy=V*q where q is the charge, including the sign).  These moving electrons constitute your current.  A caveat to this is that electrons really live in quantum states and no two electrons can live in the same state(Pauli exclusion principle as electrons are Fermions).  The electric field really moves electrons from states with one momentum to states with a momentum that is in the direction of the electric field.  If the band is full, all the states are full and the electric field cannot change the electron’s state so no current flows.  This is an insulator.  When a band is partway filled, there are states that the electric field can move the electrons to.  This allows a current to flow.

Transistors are a little more complicated.  The main thing you have to understand is p doping and n doping semiconductors.  Imagine you have a crystal of silicon.  If you take out a silicon atom and put a phosphorus atom in its place, you suddenly have an extra electron.  A single phosphorus atom won’t change your band structure as you still have 10^23 silicon atoms so it’s like you just added an extra electron to your system.  Semiconductors have a filled band with another band with only slightly more energy (.5ish eV).  This extra electron from the phosphorus can’t live in our filled band, called the valence band, because there are no more states.  It must live in the next band, the conduction band.  If you apply an electric field, this electron in the conduction band can flow because pretty much all the states in its band are empty.  This is called n doping because we added an extra negative charge, the extra electron.  If instead of a phosphorus atom we add an aluminum atom, we have one less electron.  If the aluminum steals an electron from a neighbor, this neighbor now is missing an electron.  Instead, of thinking of the aluminum as stealing an electron, you can think of the aluminum as giving the neighboring atom an empty state.  This empty state is called a hole in solid state physics.  A hole is basically a missing electron and it behaves like a particle with charge +e.  If you apply an electric field to it, it can move around by trading places with an electron.  Again, you get a current.  We call this p doping a material as it is now missing an electron or you can think of it as having positively charged particles, holes.  Transistors are semiconductors with a p doped region surrounded on both sides by an n doped region or vice versa.  Honestly, I study physics and not material science or electrical engineering so I’m not super familiar with the details of how a transistor works.  I hope this helps.  Sorry it’s so long winded.

edit:I explain how to find band structures in two limits down in the comments.

edit 2:Paragraphs",null,61,cdho5j8,1qx8zd,askscience,new,368
akanthos,"Conduction band electrons aren't treated discretely, and aren't part of any particular orbital in general. Rather they are said to be 'delocalized,' with probability distributions that are spread out over the entire lattice instead of atomic or molecular orbitals. This is usually modeled as a 'sea' of electrons around cations. One model if this you can look up is the Drude model. 

A more quantum phenomenon is the promotion of electrons from the valence band into the conduction band. Electrons can tunnel across this potential barrier.

",null,20,cdhg6x3,1qx8zd,askscience,new,100
mcmad,"The picture of conduction you're describing is actually that used in the Hubbard model - used to study graphene nanoribbons for example. In these materials electrons are localised (or located) on individual atoms and this is very much the way to think about it.

But metals are very different. Some of the electrons in metals are actually spread over the whole material. This is why metals conduct electricity so well, as these electrons are free to move through the whole of the metal.

These two types of ""orbitals"" come up all over quantum mechanics. If you want to read more about this, the localised states are called bound states,  where as the delocalised states in metals behave like free electrons.
",null,2,cdhjdt9,1qx8zd,askscience,new,16
wbeaty,"&gt; would current moving through a regular metal be like each electron appearing in the closest neighboring orbital, as a chain reaction, when a voltage is applied to the metal?

It might help you to visualize the Classical Physics version first.  Then add in all the QM weird stuff on top.   Start with Newton and Maxwell.   There's lots of justification for this since electric currents certainly aren't inherently QM.   The QM becomes significant when the carriers are very low mass and in periodic potential wells: electrons in crystals.   But this doesn't apply to electric current in oceans and electrophorisis and sparks and in human tissue, where the moving carriers are enormous ions and in some cases are even directly visible.  To get a visual gut-level feel for the physics behind electronic components, assume that conductors are hoses full of salt water rather than metals.
___

OK, first, all conductors are always full of positive and negative charged particles, with one or both being mobile.  Whenever a conductive material is disconnected or sitting on a shelf unpowered, its charge carriers will be in constant high-speed ""thermal"" motion, same as with any liquid or gas.  This motion is required, since without it, all the opposite particles would fall together and become electrically immobile.  For example in sea water, if the positive sodium and negative chloride atoms all paired up and stuck together, then conductivity would vanish.  Any applied voltage wouldn't cause any carrier flow.   Thermal motion keeps the carriers free to respond to an e-field.

So, a conductor is like a container of gas, with the gas composed of two populations of particles:  positive charges and negative charges.  Overall the conductor starts out neutral, yet it's still extremely electrical, since the positives and negatives can flow along separately in different directions.  That's a Classical model of electrolytic conductors.

In Classical metals the positive particles are much more massive than the negative, and the positives are connected to the ""container,"" and can only move as it moves.  So, a Classical Physics model of a metal would be a positively-charged sponge wetted with a fluid of mobile negative particles.

What happens when we apply a difference in potential to the ends of a long conductor?   Well, all conductors are electromagnetic shields.  If we try to create an internal e-field along the length of a long conductor, this field will not instantly appear inside the material.  Instead, mobile charges at the surface of the conductor will try to flow so as to produce zero field inside the conductor.   For a perfect, zero-ohms wire, the applied e-field would only cause the mobile charges on the surface to start flowing.   So, close a switch, and all the mobile charges within the surface of the wire all suddenly begin flowing at about the same time.  They flow quite slowly.   But they all start up at once, like turning on a conveyor belt.  Next, quite rapidly the outer layer of flowing charges interacts with inner layers of movable carriers, and the deeper layers begin flowing as well.  The surface current ""sags inside"" the wire, and within a fraction of a second the entire charge-cloud inside the conductor is in slow motion.  In metals the positive stays still and the negative cloud moves along.   During currents in electrolytes (and in unrefined semiconductors, and in intrinsic semis) there'd be two populations of carriers moving slow in opposite directions; interpenetrating clouds of positives and negatives both flowing through each other.

Note that this slow avearage motion is added to the constant high-speed ""thermal vibration"" of all the charge-carriers.  The random ""dance"" of carriers is always there, and the momentary velocity of individual carriers is, on average, immensely fast.  (In salt water the charges are wiggling at the speed of sound; in metals the electrons fly around at nearly the speed of light.)   An electric current happens when the entire ""dance floor"" then moves along very slowly.   I like to visualize this as a screen full of television white-noise inside the conductor, where electric currents exist whenever the fast-sparkling screen starts moving along at about a tenth of a MPH.  This velocity is proportional to amperes: double the drift velocity and you double the value of current.

How about Classic Physics bandgaps?

:)

That's easy.   In our above model of metal as ""postive sponge containing a negative fluid,""  let's imagine that the negative particles can either be stuck to the sponge surface, or they can be flowing freely around inside the pockets of the sponge.   Let those stuck particles move along the sponge surface without breaking free.   That gives us some low-energy particles on the sponge.  ""Low energy"" because they've been attracted in and trapped.  It takes electrical energy to pull them away against the electrical attraction between positive sponge versus negative particle.   Also, we then have a population of ""high energy"" particles which haven't fallen down to the sponge surface, and they remain wandering around in the material at a ""higher"" level.   If a free high-energy particle should fall down and crash into the positive sponge, this gives out energy: it produces a significant EMP, a ripple of EM waves.   And, large impulses of EM waves are able to occasionally free one of the negative particles sliding around against the positive sponge surface.  If a particle breaks free, it momentairly casts a shadow and absorbs the EM waves which knocked it loose.

That's raw silicon with heavy p++ n-- doping.   Should I do intrinsic semis?   PN junctions?

:)


",null,1,cdhsltp,1qx8zd,askscience,new,4
novaya_zemlya,"At its core, the existence of a band gap in a semiconductor is a quantum phenomenon. Let's say you have a semiconductor with a large band gap (1 electron Volt) at 4 Kelvin. You apply a voltage difference of 0.5 electron Volts and still, the electrons don't go anywhere because there isn't enough voltage or thermal energy to excite the electrons across the band gap. But what IS a band gap? It's not a vacuum, it's just a bunch of quantum states that our would-be conduction electrons cannot occupy due to selection rules. Other things (bonded electrons, etc.) are occupying those states, so our Valence band electrons can't go there.

There are some phenomena where current is carried in discreet quanta (e.g. single electrons) that can be measured. However, this does not really apply to normal conductors (such as metals) under normal conditions (such as ambient temperature and pressure.) As others have said, current in a normal wire can be described by a ""sea"" of delocalized electrons which are free to move with an external electric field without being constrained by the localized electric potential of the individual atoms in the wire.

BUT, when you start looking at nanoscaled, low dimensional systems (such as carbon nanotubes, graphene lattice, or quantum dots, especially at low temperatures) conduction becomes quantized very quickly. In these special cases, instead of a sea of free electrons participating in conduction, you can have just a few, sometimes just ONE electron that is able to move from its current energy state to another available energy state. 

You start observing things like [Coulomb Blockade](http://en.wikipedia.org/wiki/Coulomb_blockade) and measuring [discreet quanta of conductance](http://en.wikipedia.org/wiki/Conductance_quantum).",null,1,cdhqie4,1qx8zd,askscience,new,4
drwho9437,"You can end up with band structure via tight binding or nearly free models of electrons in a solid. I feel like you are asking are the electrons really bound tightly or are they free?

I think you could generalize your question even further, what does anything look like at the quantum level. We pick wave and particle models because those classical systems share properties of the quantum world but neither is really correct. 

Transport in a solid can take place in all sorts of ways. There are electrons which are highly localized in some materials. These often create bands with low mobilities. In simple metals (say Na) the nearly free picture works well. 

But what does it look like? Well you have your normal uncertainty principle trade-offs but you really are asking I feel is what is the coherence length of the wavefunction. Over how many lattice points is the wavefunction spread out. Vanilla quantum statics you learn on the undergraduate level does not treat dissipation or inelastic scattering. If you presume atoms are localized and you have some scattering and you have a voltage applied the big spread out wavefunction occasionally (actually all the time) should collapse and localize at an atomic site. If it scatters without collapse it would be just entangled with the state of the atom and that process is hard to get (see quantum computing) so we can pretty safely say in the transistors you are studying that it is very particle-y at each scattering and wave-y between them. 

This is all very hand-wavy but they are reasonably useful toy models I think.

Some real models of transport in metals:

* http://en.wikipedia.org/wiki/Drude_model
* http://en.wikipedia.org/wiki/Free_electron_model
* http://en.wikipedia.org/wiki/Fermi_liquid_theory
* http://en.wikipedia.org/wiki/BCS_theory

Overall transport is often modeled with the Boltzmann equation:

* http://en.wikipedia.org/wiki/Boltzmann_equation

Most of this stuff is graduate level solid-state physics. ",null,1,cdht6ni,1qx8zd,askscience,new,4
majornoon,"You can't really think of it like a chain reaction, when you have a bunch of atoms next to each other in a solid they give rise to energy channels that the electrons can move in.  It's a joint thing, atoms further away have less impact, but the energies the electron can travel at are determined by the structure as a whole.  A cool fact is that if there weren't imperfections (i.e. missing atoms, etc) in the solid, there would be no resistance!  Drude model treated electrons moving as if they were colliding with atoms, but this fails for many phenomena.",null,0,cdhlq8j,1qx8zd,askscience,new,2
apocryphite,"Like /u/akanthos said, electrons are delocalized. At least the conduction ones electrons - most electrons in a metal are still bound to the atoms, but one or two electrons (typically, depends on element) are donated to the ""electron sea"". Keep in mind the particle/wave duality. Conduction electrons are more like smeared out waves.

A current through a wire is a very slow drift of all the conduction electrons (the [drift velocity](http://en.wikipedia.org/wiki/Drift_velocity) is something like centimeters per hour!).

About applying a voltage: The voltage relates to the potential energy of the electrons. Imagine I roll a ball up a hill. I'm working against gravity, so when on the top of the hill the ball has some potential energy. The ball can roll down the hill, you can even make it roll through stuff and do work.

Now, I take an electron and I push it towards a negative charge. I'm fighting the Coulomb force, so pushing the charges closer is a bit analogous to rolling the ball higher up the hill.

Say I take two boxes and put electrons in them. In one box, they are tightly squeezed together. The other box is less crowded. Now I connect a tube between the two boxes - electrons will drift from the box with higher potential energy towards the one with lower energy. This is like putting a battery in an electrical circuit, ie. applying a voltage :)

If someone asks really nicely I can talk about scattering, resistance and ballistic conduction, but it will have to be later.",null,0,cdhossz,1qx8zd,askscience,new,2
CoolStoryJohn,"A less detailed/nuanced answer for those curious about the basics of these questions:

P1: Current is merely the aggregate motion of electrons.  So, if one were to ""watch"" current, one would be viewing the displacement of a large batch of electrons from point to point in an electrical circuit.  In a standard metal, remember that electrons can be viewed as ""pooled"" together (almost like a special type of bonding mechanism).  That pool would be the group of electrons that compose the current.  Naturally, though, a circular or rectangular bar (i.e. a complete circuit) doesn't just inherently have a current.  A voltage needs to be applied to the circuit in order to ""excite"" (if you will) the group of electrons into motion...thus producing the current.  

P2: Voltage is really just the electric analog to gravity's potential energy.  You have an object raised some distance ""h"" off of the ground (which we'll define as our reference point--h = 0m) and that object has a potential energy of mgh ((mass)(gravitational acceleration)(h)).  There is no physical indication of the object having that potential energy, but it merely contains it as a result of being displaced from the reference point (the ground in this case) while in the presence of a gravitational field.  Voltage can be viewed almost exactly the same, except the ""object"" is a point in an active electrical circuit and the physical ground is the electrical ground (defined as 0 Volts).  ",null,1,cdif07s,1qx8zd,askscience,new,3
imsowitty,"Of course there are many ways to think of this sort of thing, but one that may be useful is simple electrostatic attraction/repulsion.  If you can convince yourself that like charges repel, then flowing current ends up as positive (or negative) charges trying to get away from each other.  An electric potential is set up by pumping extra charge into one section on a conductor, and letting electrostatics push the remaining charges away.  

If you're getting into transistors, you may be dealing with depletion regions and work functions, but the same idea applies.  A lower work function material simply has less affinity for its electrons, so it's more likely to give them up to another material with more affinity for electrons (call it a lower energy state), until enough charges build up on the high work function material that the built up charge prevents further current flow.  ",null,0,cdhoh83,1qx8zd,askscience,new,1
venikk,"A voltage is defined to be the potential of a coulumb of charge in an electric field. This is synonymous to gravitational potential of an object. More mass means more gravitational potential, and more charge means more voltage. Likewise the closer two charges are the higher the voltage, and the closer two massive objects are the more they attract. They also are both attracted by the inverse square law relationship. The attraction increases exponentially as they approach eachother.

Voltage is a man-made abstract quantity, that is anywhere and everywhere there is a difference in charge. 

An Ampere is defined as one coulumb of charge passing through one Ohm of resistance per second. So current is literally electrons passing through a conductor. It should be noted that these electrons behave like wind or gases. Not all electrons are going in the direction of the current, but on a per charge basis most are. It should also be noted that the electrons aren't necessarily traveling at the speed of light or at the speed of the current. In a circuit electrons are pushing on eachother, and it is the speed of which the first and last electrons push on eachother that gives us the lightning fast speed of electricity. The physical drift velocity of a electron is generally less than a cm per second.",null,0,cdhpelk,1qx8zd,askscience,new,1
MaterialsScientist,"Great question. It's difficult to imagine the true multi-particle wavefunction for a gazillion-particle crystal, so we have to imagine pictures that we know are wrong but are still useful. Band theory is one of these pictures.

According to band theory, an electron is simultaneously present in every unit cell of the crystal, like a wave. As the wave moves, it's like the electron moving.

(In reality though, the electron is a point particle.)",null,0,cdhpvqm,1qx8zd,askscience,new,1
TangentialThreat,"This is the sort of thing people who breed plants and animals do all the time.

Similar arrangements where A breeds with B and B breeds with C but A doesn't breed with C are pretty common in nature; they are called [ring species](http://en.wikipedia.org/wiki/Ring_species). Speciation is often less than complete and nonlinear.

",null,2,cdhjbim,1qx9rg,askscience,new,12
DrPeavey,"Snowflakes form in the atmosphere by a process known as the [Bergeron Process](http://weather.cod.edu/sirvatka/bergeron.html). 

[*(Instructional video here)*](http://www.youtube.com/watch?v=WByQ3yKxj1s)

Snow and ice crystals form macroscopic, hexagonal shapes due to the angles at which hydrogen bonds form between water molecules during the deposition process.",null,2,cdhk2m4,1qxfng,askscience,new,4
Gargatua13013,"Basic crystallography: there are a number of symmetry classes for crystals based on the shape of the crystal lattice: cubic, rhobohedric, orthorhombic, monoclinic and hexagonal.

The form of ice crystal stable at normal temperature and pressure is hexagonal.",null,0,cdhplc2,1qxfng,askscience,new,1
iorgfeflkd,"It would look the same over large scales, there would just be other parts to observe.",null,38,cdhiwos,1qxn8n,askscience,new,155
lsdkdlsdk,"The universe is literally infinite in every direction.  Light travels at a finite speed and the universe has only existed for a finite amount of time though, so it's only possible to see the universe to a certain ever-expanding distance from your present location.  The parts of the universe that are close enough to see are part of the observable universe, the parts that are too far to see are not.  

The parts of the universe outside of the observable universe shouldn't be different from the parts of the universe inside of the observable universe.  It should have galaxies, stars, planets, etc also.  There's no edge anywhere out there.  If you were to travel to what is presently the edge of our observable universe, you would find yourself in the center of a different observable universe that looks pretty similar to our own.  

I'm guessing that you were asking this question from the perspective of time, though, and how the further out you look, the older the thing you're looking at gets.  

One important thing to keep in mind is that the universe wasn't always transparent to light.  Prior to the cosmic microwave background emission, it was opaque.  If you look out as far as is physically possible, you'd eventually see nothing beyond a certain point due to this.  Let's assume you've got some way around this.  Maybe a neutrino telescope or something?  I dunno.  

Sometimes hypothetical scenarios are useful, and provide insight to certain situations.  This isn't one of those scenarios.  A telescope like you described, that lets you see parts of the universe that haven't yet had enough time to radiate light our way, isn't possible to construct.  I'm sure you knew that, but the reason this isn't a good hypothetical scenario is because even if you did have such a magical telescope, there's no meaningful answer to what it would see.  As you observed parts of the universe farther and farther you'd eventually get to the point where you were viewing things happening mere moments after the big bang, then eventually to the exact moment of.  Looking any further would be completely meaningless.  There isn't a physical answer to what you'd see, or a theoretical answer, or any kind of satisfying answer at all.  ",null,19,cdhn3fa,1qxn8n,askscience,new,43
dont_read_this_plz,"Well the observable universe is what we can see of the universe from a fixed point. However changing that point also changes  the perspective of the ""observable universe"" we we can see. Its like climbing up a tree looking around for a bit then climbing up a tree a mile away, you're seeing different part of the observable forest, and there's more of the forest you haven't seen. Did this help?",null,9,cdhkbh8,1qxn8n,askscience,new,31
TwirlySocrates,"Most scientists think that the universe is the same everywhere. So if I were somehow living 13.7 billion light years away form here, it would be the same as here, only different galaxys.

If you're wondering what the universe was like 13.7 billion years ago, that's a totally different question.

Also, there's no such thing as a ""past the border where the laws of physics don't exist yet"". The universe is infinite (we think) and the laws of physics are the same everywhere (we think).",null,0,cdhtbh9,1qxn8n,askscience,new,13
pmreddick,"I won't claim that I actually know what I'm talking about here, but from what I can understand the universe doesn't have ""an edge."" It's kind of like if we lived on the inside of the Earth's surface rather than on top of it; so gravity pulled us outward rather than toward the center. Look as far as you want in any direction and 1. you're always in the center and 2. you will never find an edge.

On top of that, there's a weird theory that the [entire universe could be smaller than the observable universe](https://www.khanacademy.org/science/cosmology-and-astronomy/universe-scale-topic/big-bang-expansion-topic/v/a-universe-smaller-than-the-observable).",null,4,cdhn1jd,1qxn8n,askscience,new,11
ComplainyGuy,"Meta, when we dont know;

Can we just say ""sorry op, your question can only be answered artistically""

ESPECIALLY when it comes to things like ""why did humans evolve...an enjoyment to music"" or ""what does light look like if you are going faster than it and shine a light on it and then enter the 6th dimension""",null,3,cdhwpw4,1qxn8n,askscience,new,5
lekjart,"The best way I have found to understand an expanding 3D universe is the following:

Imagine you are working in a 3D authoring tool such as Maya or Google Sketchup. Now imagine that you define a regular cubic grid that goes on to infinity in all directions, where each cell has a unit length, say 1 m.

Now finally, assume that you can input a scaling factor, let's call it S, that can scale the length of the unit length.

So if S equals 1, then the unit length of your grid is 1.

If S equals 0, then the unit length of you grid is 0, which basically means it is undefined. You can't really picture a cubic cell with zero length sides.

The transition from S=0 to S &gt; 0 is basically your Big Bang: you go from a completely undefined (and non-existing) grid to a grid that extends infinitely in all directions with a unit length of S.

There is no center for the Big Bang. The whole grid comes into existence everywhere in space simultaneously in all directions.

Now finally, imagine that you increase the value of S smoothly with time. Your grid is basically expanding uniformly in all directions. If you position yourself at any point in the grid and measure how fast other points in the grid are moving, you will observe that all points seem to be moving away from you. Furthermore, the farther away a point is, the faster it goes away from you. For some grid distance (that depends on the how fast S is changing) the points beyond will be travelling at a speed faster than light as measure from your point. This means that information from those points in the future will not be able to reach you. This effectively defines an event horizon and that is the effective size of the measureable universe from your point of view. The Universe still goes on to infinity (except for some weird topology), but for all practical purposes it has a finite observable size for you.

PS: This is not a completely mathematically accurate description, but if you can wrap your mind around the above, you are more than halfway in understanding this.

Here are more gory details if you are interested:

http://en.wikipedia.org/wiki/Metric_expansion_of_space


",null,2,cdhykx6,1qxn8n,askscience,new,5
Rastiln,"I've seen a lot of speculation in this thread, and as a layman it's only made me more confused.  Let me ask some specific followup questions, and I hope that somebody qualified can explain this to me.

1) I've heard different people in here say that the universe is infinite and that the universe has an edge.  Do we know with a reasonable (let's say p&lt;.01) amount of certainty that it really is infinite?  Is most of our knowledge here based on factual data, or theoretical extrapolation, or pure speculation?

2) If the universe IS infinite, how is is ""constantly expanding""?  Does that simply mean that all objects are continuously moving further from one central origin (potentially the Big Bang), or am I off-base there?  If I am correct and all objects are moving away from some point of origin, then are they all moving to areas that are, or at least used to be, completely empty?",null,0,cdhwyjd,1qxn8n,askscience,new,2
logic_card,"The universe is expanding, if you could travel fast enough to reach the ""edge of the observable universe"" you would have to travel faster than the speed of light.

You could speculate that this will result in you traveling backwards in time, in which case the ""edge of the observable universe"" wouldn't be a place but a time, around about the time of the big bang.",null,1,cdi29hu,1qxn8n,askscience,new,2
Anhanguera,I believe the edge of the observable universe is as far from us in light years as the universe is old in years. We can observe that far because that's the farthest light could have travelled since the universe started. So I think there is no way to see past this 'barrier' but physics wise the 'laws of nature' are pretty universal by definition aren't they?,null,20,cdhjiy9,1qxn8n,askscience,new,6
null,null,null,13,cdhqevb,1qxn8n,askscience,new,1
null,null,null,26,cdhmeqh,1qxn8n,askscience,new,5
redditor5690,"Bootstrapping is the term I think you're looking for.

The first electronic computers were programming with wires. The presence or absence of a wire at a decoded junction represented each data bit.

There were no ""languages"" then, just simple machine coded instructions.

After that, it's one small step after the next.",null,1,cdhrya2,1qxqf8,askscience,new,9
disconnectedLUT,"You could program the memory with switches (a set of data switches, a set of address switches, and a write switch). Then you could have a reset switch to set the processor to start executing code that you would have manually entered.

Deciding what data to write to memory is a simple matter of looking up byte codes for the assembly instructions you want to execute. This wouldn't be too difficult because early computers didn't have very many or very complicated instructions.",null,1,cdhotwc,1qxqf8,askscience,new,3
Chilangosta,"I thought that [this explanation](http://www.reddit.com/r/AskReddit/comments/1qogor/what_is_one_thing_you_cannot_understand_despite/cdezhe1?context=1) by /u/topupdown was very good and very easy to understand. He basically explains the evolution of programming, starting from the point you're discussing, with the very first computers. ",null,0,cdi7os2,1qxqf8,askscience,new,2
robogen,"Initially it started as hardcoded commands on circuitry. Literally, binary code. If electricity is coming through, then true. If no electricity, than false. Even modern day circuits work this way; the CPU in your computer can only communicate with binary digits. There was then developed a way to 'program' the binary digits, and save the newly created software. The code was incredibly hard to read, as the code was simply ones and zeros. And when I say 'read', this was still in the days where you had to write your code with punch cards.

Eventually there came assembly code. A compiler was created, which can read in assembly code and convert it to binary. Assembly code is much more human readable, and makes it easier to write a program. Still though, it was a complex language, and difficult to learn. So, a new compiler was written for a language called Pascal (or it was Fortran by this point, I get it mixed up). Literally, Pascal was written because the creator felt lazy and didn't want to work with such a complex coding language. 

Most languages are written on top of another one, and everytime this happens, the code becomes easier to read and work with, but much more resource intensive unfortunately. 

For examples sake, look at Java. Java is a fairly easy to read language, and decent to work with. But the code of the Java compiler is written in C++, which is built on top of C, which requires a compiler of its own to send instructions to the CPU in Assembly, which then get interpreted into Binary code which the CPU uses to actually run the program.

Sometimes I get into explanations and tangently get away from my original point, does this answer your question?",null,0,cdhswwy,1qxqf8,askscience,new,1
bc87,"http://en.wikipedia.org/wiki/ENIAC#Programming

The first computer was huge. It took up a whole room and used a huge quantity of electricity(compared to today's computers). They literally programmed using cables, switches, and plugs.

There's a documentary about the first-generation programmers. They were literally first-generation programmers, as there were no others before them (The scientist that designed the machine didn't count themselves as programmers).

http://www.imdb.com/title/tt1587359/

It's available on netflix.",null,0,cdlrno4,1qxqf8,askscience,new,1
O_Zenobia,"The other issue is that viral loads (and so infectiousness) are not level throughout the ""latent"" period -- a misnomer anyway, as the virus is setting up shop in various tissues throughout that time.

People are most infectious in the acute phase, during the burst of early replication a month or so after infection. The risk of transmission in the acute phase is more like 1/50. They probably have short periods of being highly infectious after the acute phase, too. During an outbreak of herpes or a syphilis infection, for example, viral loads go up and open sores facilitate blood-blood or blood-mucous membrane contact. 


Pilcher, Christopher D., et al. ""Brief but efficient: acute HIV infection and the sexual transmission of HIV."" Journal of Infectious Diseases 189.10 (2004): 1785-1792.

Cohen, Myron S., and Christopher D. Pilcher. ""Amplified HIV transmission and new approaches to HIV prevention."" Journal of Infectious Diseases 191.9 (2005): 1391-1393.",null,0,cdht4nv,1qxqmw,askscience,new,4
disconnectedLUT,"That rate is per sexual encounter. So a woman who has vaginal sex with an infected male partner twice a week for a year has a 1-(1-.0008)^(2*52)~=8% chance of infection. Over 5 years this becomes 34% or over ten years 56%.

The point is that even though the 'per encounter' rate is very low, the overall chance of infection can be very high for people who either don't know that their partner is infected or have frequent sex with many partners.",null,0,cdhp80o,1qxqmw,askscience,new,3
pheel23,"So many ways to answer you questions. I'll try and be brief.

The numbers are correct. Female to male transmission is quite low. However people are having sex. Billions and millions of people are having sex. Given data on condom usage there is very high rates of unprotected sex. Low risk over repeated risk taking results in a significant amount of HIV. This is for the U.S. and mostly true for the west coast. ",null,1,cdhoy4y,1qxqmw,askscience,new,2
medikit,"NSFW

People aren't unlucky, they just have a lot of unprotected sex. Especially those who are most at risk.

The anus and vagina have epithelium that are not keratinized so the virus can pass through them more easily. They can also develop micro-tears through which the HIV virus can travel. The penis including the glans (head) is keratinized and it is not as easy for the virus to cause infection in. The foreskin (present if uncircumcised) is not-keratinized and there is an increase risk of infection in uncircumcised individuals. Concurrent genital herpes and other ulcerative lesions can increase risk for all partners. Using a condom profoundly reduces your risk of HIV acquisition. Adherent HIV treatment (HAART) of the infected partner or prophylactic use anti-HIV therapy (PrEP) has also been shown to reduce risk.

Even so the rate is not 100%, virus needs to survive and make it into the blood. The virus that survives needs to find a receptive cell and successfully infect that cell. There are a number of places where this can go wrong and the virus will fail.

Risk of infection is as follows:

* Receptive anal intercourse: 1 transmission/200 sex acts
* Insertive anal intercourse: 6 transmissions/10,000 sex acts
* Receptive oral sex: 1 transmission/10,000 sex acts
* Receptive vaginal intercourse: 1 transmission/1000 sex acts
* Insertive vaginal intercourse: 1 transmission/10,000 sex acts

1. http://www.uptodate.com/contents/nonoccupational-exposure-to-hiv-in-adults?source=search_result&amp;search=hiv+infection+risk&amp;selectedTitle=1~150
2. http://www.ncbi.nlm.nih.gov/pubmed?term=10430236
3. http://www.ncbi.nlm.nih.gov/pubmed?term=18684670
4. http://www.ncbi.nlm.nih.gov/pubmed?term=20397962",null,1,cdhoz85,1qxqmw,askscience,new,2
BoxAMu,"Power delivered by the source is P = VI.  The question is what the power is going to be used for.  For a resistive element in a circuit, V=I/R, so P_res = I^2 R.  This shows that the power lost to resistance goes like the square of the current, so to cut down on resistive loses one wants to minimize the current.  

If voltage is made very high and current is made very low, then the total power P=VI can stay the same, but less of it will be wasted on resistance (which simply heats the wires).  This means that for high voltage transmission, more of the power is spent on doing useful work.",null,0,cdho6z2,1qxtyn,askscience,new,5
selfification,"/u/BoxAMu has a relevant answer but it in my opinion, it doesn't quite address why there is different amounts of power loss when you vary voltage.

I'll try:  Power = voltage * amperage.  Correct.  Consider for a moment a power generator that produces a certain constant amount of power (from burning coal or splitting atoms or whatever).  You can't change this amount.  You goal is to get as much power to your customers (desired power loss) as possible and to reduce as power loss in your circuit (undesired power loss) as possible.  After all - all the power must be consumed/lost at some point.

Let's consider a simple series circuits of 2 resistors.  Pretend that one of these is the load resistor (the customer's toaster) and the other is the resistance of the power line.  From the above constraints, you can't actually change the voltage at all.  The voltage you can apply is entirely determined by the max power you can dissipate.  The only way you can change the amount of power on the load is by varying the resistance of the power line.  If you can reduce the resistance of the power line to 0, then all power will be delivered to the load resistance.  In a purely resistive circuit, you can't vary voltage while keeping the total power output constant.

So clearly all this voltage up/down conversion has to do with circuits that aren't purely resistive.  Power transformers are coupled inductive circuits.  It uses Faraday's law to exploit the fact that a changing magnetic field inside a closed loop with generate a non-zero ""voltage"" (more technically, a non-vanishing line integral of the electric field along the closed loop).  You can read about this at http://en.wikipedia.org/wiki/Transformer.  The trick here is as follows - there are 2 circuits: the circuit from the power source through one side of the transformer windings and the circuit with the load through the other side of the windings.  If these two windings were isolated from each other, no power would flow from one to the other.  The power supply (i'm assuming an AC power source), would see some self-inductance from the windings that would act like an antenna, but that's about it.

If we coupled the two circuits, things change.  The impedance seen by the power supply now changes (impedance is the fancy word for resistance with capacitance and inductance worked into it).  The power supply now sees the load resistor, but doesn't directly see it.  Because of the way the transformer is structured, it sees the load resistor as a multiple of what the load's impedance actually is.  This ratio of what the power source sees and what the impedance actually is is determined by the square of the ratio of windings on either side of the transformer.  You can see the trick we've done here.  Instead of simply lowering the resistance of the wire, we instead found a way to cheat and increase the impedance of the load, helping us proportionally deliver more power to it (not necessarily true when dealing with impedances but it happens to be so in this case).

The voltage balance on either side of the windings is an outcome of this ""cheat"" that we performed.  Since the actual load expects a certain voltage to satisfy its power requirement, the apparent load that the power source sees through the coupled circuit now requires a much much higher voltages (based on the square of the ratio of windings) to deliver the same power.  Therefore the power source increases its voltage to account for this, decreasing the total current in the circuit to remain at the same power level.  This circuit *does not* obey Ohm's law - at least not the naive formulation of it.  The apparent load seen by the circuit is *not* a resistor but has inductive elements to it.  More advanced treatment of this has to deal with the impedance matching of the circuits so that power actually makes it to the other side of a transformer.  In a circuit is not correctly impedance matched, power never actually makes it through to the coupled circuit and is returned to the source which is very very bad.",null,0,cdi0f5w,1qxtyn,askscience,new,2
iorgfeflkd,"We can measure how far away things are, and we can measure how fast things are going. Hubble in the 1920s made observations of the speed and distance of supernovae, and found that the farther away they are the faster they appear to be receding from us. There are two conclusions that can be drawn: the Earth is in the centre of the Universe and everything is moving away from it, or everything is moving away from everything else. The latter implies the expansion of the universe.",null,2,cdhls0q,1qxw1j,askscience,new,5
The_Duck1,"As far as we know, there *isn't* an edge to the universe. It may well be infinite. [However, since it has only been a finite amount of time since the Big Bang, light has only be able to travel a certain distance and so we can only see out a certain distance; the region we can see constitutes the ""observable universe.""]

When we say that the universe is expanding what we mean is that on very large scales, all the galaxies are spreading apart. We know this because when we observe distant galaxies we see that they are all receding from us. We can tell that they are receding because the light we receive from them is [redshifted](http://en.wikipedia.org/wiki/Redshift) by their velocity.",null,0,cdhqzss,1qxw1j,askscience,new,2
null,null,null,2,cdhlxvg,1qxw1j,askscience,new,1
Helen___Keller,"Hi! What you're getting confused about is what it means for something to be a ""larger"" infinity.

In naive, common-sense terms, yes, we are mapping a smaller infinity to a larger one, but in strict mathematical terms, the two infinities are exactly the same. Let me explain:

In mathematics, we measure the size of a set using what is called the ""Cardinality"" of the set.

We say set A and set B have the same cardinality if and only if there is some bijective (one-to-one and onto) mapping between sets A and B. For example:

There is a bijective mapping between the natural numbers {0,1,...} and the integers {...,-1,0,1,...}. It is constructed easily: simply have the even natural numbers map to non-negative integers, and the odd natural numbers map to negative integers. Thus, mathematicians will say the Natural numbers and the Integers have the same cardinality, or the same size.

To answer your question:

&gt; Is every point actually represented?

Yup! This can be verified by proving your transformation is one-to-one and onto. If it is, then each element of your 2x2 square is mapped to by one and ONLY one element of your unit square.

Finally, let me wrap up by saying that not all infinities are the same size, even though we have seen that some (1x1 vs 2x2 squares, integers vs naturals) are the same size. It has been shown (via Cantor's famed diagonalization argument) that there is an infinite set that is *strictly* larger than the natural numbers (for example, the real numbers). There does not exist a bijection between the natural numbers and the real numbers.",null,2,cdhp90o,1qxwuy,askscience,new,12
control1757,"'Measure' isn't the same thing as cardinality. You can map [0, 1] to [0,2] using f(x) = 2x and the cardinality of sets is different but it's still a well defined bijection. 

But clearly it makes sense to say that one of these sets is larger than the other. Naively it's a pretty straight forward process and works exactly how you would think, [0,2] has a 'measure' of  2 with respect to the fact that we are in 1d. Or that [0,2] X [0,2] has measure 4 with respect to the fact that we are in the 2nd dimension. 

But a wrench was thrown into this with the Banarch Tarski Paradox http://en.wikipedia.org/wiki/Banach%E2%80%93Tarski_paradox which basically just says that using this naive construction  of measures, ZF set theory and the axiom of choice they were able to construct 2 spheres from 1 of the same size without doing anything like stretching the boundary. Because of this measure theory/integration is now one of the first math courses you'll take if you go onto graduate study or in your senior year. ",null,0,cdn8pf1,1qxwuy,askscience,new,1
AutoModerator,"Thank you for your submission! Unfortunately, your submission has been automatically removed because it contains the phrase ""**ELI5**"", so it is possible you are looking for /r/explainlikeimfive. If you would like scientific answers, you can repost your question to /r/AskScience though! Experts will always simplify and explain, so that even difficult concepts are easy to understand. Thanks for understanding. :)

*[I am a bot](/r/AutoModerator/comments/q11pu/what_is_automoderator/), and this action was performed automatically. Please [contact the moderators of this subreddit](/message/compose?to=%2Fr%2Faskscience) if you have any questions or concerns.*",moderator,4,cdhmc3k,1qxyx3,askscience,new,3
chidgeon,"A lot of the times, drug abuse/addiction causes your body to create a new standard of homeostasis which revolves around taking the drug. So your new ""normal"" state is dependent on the substance and tolerance begins to develop to certain effects. Once you stop taking the drug abruptly, that's when withdrawal occurs and why symptoms are usually so severe. Basically, your body doesn't know what to do with itself because it's missing something it once considered normal to function. ",null,0,cdhq3t3,1qxzmb,askscience,new,4
derpalist,"With opiates, what happens when you become addicted is this:

Opiates(Morpine, heroin, oxycodone etc) mimic natural endorphins. Endorphins are the chemicals your body uses to combat pain(Both mental and physical), to cause sensations of pleasure and happiness. Opiate drugs are basically the same as these natural endorphins, except they do the job a lot better(Have a more potent effect). When you introduce opiate drugs into your system over a long period of time, your body stops the production of its own endorphins. It does this because when it is receiving endorphins from outside, there is no reason to waste energy producing the stuff. It is this deregulation of the natural production of endorphins that is the physical addiction: If you suddenly stop consuming the opiates that your body is now dependent on, you will find yourself in a situation in which you are not receiving opiates from outside, nor producing your own; this is withdrawal. Over a period of time(About 3-7 days in the case of heroin addiction) your body will begin its own production of endorphins again, but during those 3-7 days, your life will be absolute torture, as the chemicals that combat pain and cause pleasure, are simply absent from your body. The only thing you feel during this withdrawal period is pain.

Source: I'm an opiate addict",null,0,cdhyaoi,1qxzmb,askscience,new,1
KarlOskar12,"Addition is frequently given two criteria: physiological addiction and tolerance (There is mental addiction, but that's a completely different field of study). Tolerance is built up when receptors are exposed to a stimulus enough to desensitize them (which can happen by down regulating presentation of the receptor, conformational changes in the receptors, by killing off the cells, etc.). Physiological addiction sort of goes hand-in-hand with tolerance. It occurs when the body adapts to the presence of exogenous molecules stimulating receptors in the body by ceasing production of that molecule. This is why withdrawals cause paradoxical effects. Opiats for example cause constipation, withdrawals cause diarrhea.

Source: various textbooks and articles",null,0,cdi14rk,1qxzmb,askscience,new,1
molliebatmit,"There are enough nucleated cells in blood (that is, not red blood cells) that DNA can be extracted from whole blood.

Prior to the availability of DNA amplification (PCR), it would be possible to take whole blood and determine its blood type (A, B, AB, O), which could exclude people and therefore possibly suggest that the blood belonged to a specific person in a group.",null,1,cdhp6zk,1qy0jh,askscience,new,5
MCMXCII,"Photons and gluons both have zero mass. However systems of photons and/or gluons can have mass. Mass is equivalent to (with a conversion factor of c^2 ) to the energy something has in its rest frame. So baryons, which are bound together by the strong force, which can be represented as the exchange of virtual gluons, has that strong binding energy in its own rest frame. So that energy contributes to its mass, even though on-shell (real) gluons don't actually have any mass.

&gt;Does this mean gluons are 'moving' within the nucleus at close to the speed of light and gain mass?

Real gluons are alway traveling at the speed of light, and they can never have any mass, by definition. Virtual gluons can have whatever mass you want, but it's okay because virtual particles aren't actually real.",null,0,cdhox24,1qy17k,askscience,new,8
arumbar,"The substance that you overdosed on makes all the difference - there is no general 'overdose reaction' that occurs.  Remember what Paracelsus said: ""All things are poison, and nothing is without poison; only the dose permits something not to be poisonous.""  Overdosing on simply water (water intoxication) can lead to death through changes in electrolyte compositions leading to brain damage.  

Some other examples: overdosing on opiates or benzodiazepines/barbituates can lead to death through respiratory depression (you simply stop breathing).  Overdosing on iron can lead to severe metabolic derangements and shock through its effects on cellular respiration and other processes.  Similarly, aspirin poisoning can lead to metabolic acidosis and arrhythmias.  Acetaminophen poisoning can irreparably damage the liver, a vital organ.  The list goes on and on, but almost each substance has its own mechanism of injury and causes death in a different way.",null,1,cdhp5c3,1qy4xr,askscience,new,7
owaisofspades,"Basically you've got two main muscles in your throat for producing sound. Your posterior cricoarytenoid which you use to produce sound, and your vocalis, which you use to modulate your pitch. People with better voices are just able to control their vocalis better, allowing for greater and better controlled pitch ranges",null,0,cdicmp1,1qy5mz,askscience,new,1
cuweathernerd,"~~[This is a map of all officially recorded tornadoes in MI.](http://i.imgur.com/HpRoc5F.png)~~ - I'd say the more obvious trend is fewer tornadoes to the north.   There are tornadoes in the western part of the state.

That being said, the lake does modify the air above it.  Supercells and strong storms are *surface based* - they get their energy from parcels of air that start out near the ground.  So how the air close to the surface is modified will impact the storm.

In the case of severe weather months, the lake is cooler than the equivalent land. This means the low level temperatures are also cooler.  That has some benefit to tornadoes because it'll lower the cloud base a bit.

However, weaker ""low level lapse rates"" will also take away some of the storm's energy, called *convective available potential energy"" - which is very dependent on surface temperature.  Those weaker surface lapse rates will also weaken near surface updrafts (less change in temperature/density with height). 

That added stability from the cooler surface air is going to be the biggest impact.

(in this most recent storm, the system was long since linear when it got to you: discrete storms are much more tornadic.  Had the overall conditions remained favorable for individual storms, your tornado threat would have been more significant)",null,0,cdht76x,1qy5nn,askscience,new,2
ididnoteatyourcat,"You are removing the fuel. A candle flame consists of a pocket of vaporized fuel (wax) whose surface burns, fed by oxygen. When you blow out a candle you are blowing away that pocket of vaporized fuel before it has a chance to be replenished. You have blown out the candle. The candle remains hot enough for a while to continue issuing fuel vapor, but you have blown away the burning surface that previously provided enough heat to ignite the vapor. ",null,0,cdhys9x,1qy5pe,askscience,new,5
Andannius,"You're more or less right that the volume of oxygen available for burning is increased when you blow on a candle. What you've missed is that the temperature of the oxygen you're blowing on the candle is actually much, much lower than the temperature of the air (which is slightly depleted of oxygen at this point) surrounding the candle while it's burning. You're essentially then reducing the temperature at the wick to the point where it can't sustain a flame and goes out. ",null,3,cdhqj9w,1qy5pe,askscience,new,2
Staus,"The Standard Hydrogen Electrode is relatively easy to set up and, unlike the ones you mention, pretty close to the middle of the electrochemical series.  The chemicals in question are not too expensive nor too reactive and can be acquired in high purity, and can be stored for a long time with no large loss of quality.  For these reasons it was a good thing to use historically and the scale hasn't been changed.  No one actually uses them that often, though, these days.  

In reality most people measure these things versus a calomel electrode (mercury-mercury chloride) since that's even easier to set up (no gas!) or, even better, a standard silver electrode (silver-silver chloride) which is thankfully much less toxic.  Both of those (and many others) can be purchased cheaply in a self-contained package.  Just gotta keep them wet and topped up with KCl solution.  

http://en.wikipedia.org/wiki/Silver_chloride_electrode",null,1,cdhtt7z,1qy5rp,askscience,new,3
bjos144,"It's basically history.  People did a lot of early work with Normal Hydrogen Electrodes (NHE) because they could understand them, they're stable, and well defined.  These are a Pt wire in a solution of 1M HCl with Hydrogen gas bubbling around the electrode.  They have good properties for a stable zero potentail.  (If you need more on choosing a reference I refer you to Bard and Faulkner.)  

The problem is they're a huge bitch to work with, and eventually people found other electrodes to use.  But just like in a physics 2 class where you can set a potential anywhere as long as you dont change it, electrochemisty was stuck with the problem of choosing a zero.  They eventually extrapolated on the NHE in a theoretical fashion and defined the SHE (Standard Hydrogen Electrode) which has a defined potential of zero at all temperatures.  This is because it's defined in a theoretically infinitesimal morality acid solution (If memory serves).  After that it's just a matter of calculating/measuring the potential difference of your new reference (often Ag/AgCl) with respect to the community accepted zero.

TL;DR:  History, like most conventions.  ",null,0,cdhtmq7,1qy5rp,askscience,new,1
Astromike23,"While the technique of using a single telescope over 6 months works well for [parallax](http://en.wikipedia.org/wiki/Parallax) of nearby stars, it won't work for interferometry.

In order for interferometry to work, you must be capturing the same wavefront at the same time with at least two distant telescopes. This is because the separate telescopes must act as though they are part of the same telescope - you can think of it essentially as two reflective patches on an enormous, otherwise dark mirror. Interferometry allows the wavefronts from different telescopes to be in superposition and interfere, just as they would in a hypothetical enormous telescope. 

In order to properly allow these wavefronts to interfere on separated telescopes, that also means knowing the distance between your two telescopes to within one-quarter of a wavelength. This addresses your second question - for visible light, that means getting down to a precision around 100 nanometers. Knowing that kind of separation between orbiting telescopes, while difficult, is still technically possible...but it's still a bit outside our grasp both in terms of technology and funding.

There certainly have been proposals to create a series of orbiting telescopes as a space inferometer - [TPF-I](http://en.wikipedia.org/wiki/Terrestrial_Planet_Finder) stands out among them. As it is, however that project was cancelled as the funding for both the technology required as well as the telescopes themselves is currently lacking.

**TL;DR**: Aperture synthesis requires observing the exact same wavefront at different locations, so that's a ""no"" to your first question. For the second, it's technically feasible, but still slightly outside our technological capabilities and no one has been willing to fund it.",null,0,cdhsvxl,1qy62g,askscience,new,2
Rhioms,"Mostly No, there is a lot of planning that goes into making the parks the way they are, and this involves a lot of landscaping. ",null,0,cdhtng4,1qy6rg,askscience,new,3
O_Zenobia,"The methodology you're looking for is called ""reverse genetics"", but you don't need to do anything that crazy. 

Check out this paper:

Watanabe, Shinji, et al. ""Production of novel Ebola virus-like particles from cDNAs: an alternative to Ebola virus generation by reverse genetics."" Journal of virology 78.2 (2004): 999-1005.

Essentially, they took apart the Ebola genome and cloned genes for each capsid protein into plasmids, which they transfected into cells in culture. The virus-like particles assembled from their components. 


Or you could make a replication-incompetent virus and observe cell entry that way.",null,0,cdhtc6z,1qy845,askscience,new,3
sporclesam,"Additionally , there is the interesting/scary issue of [aggravated human virulence as well] (http://healthland.time.com/2011/12/21/bioterror-should-scientists-describe-how-to-make-a-man-made-killer-flu/). But viral infection studies can be done by suitably modifying certain aspects of the genome (the [HIV1 Protease] (http://www.rcsb.org/pdb/101/motm.do?momID=6) comes to mind as an achilles heel) in trying to ""dumb down"" the virus. Attenuation is necessary, but it also depends on what is being studied. 
However, replacing the genome would be counterproductive as you would lose the machinery which dictates cell entry &amp; replication. ",null,0,cdhtgo3,1qy845,askscience,new,2
OrbitalPete,"Like this
http://www.space.com/12633-perseid-meteor-shower-space-photo.html",null,1,cdhulmm,1qy87w,askscience,new,3
LemurianLemurLad,"This question is very awkwardly worded.  I'm going to assume you mean most mechanoreceptors (cells that sense pressure), in which case it's definitely the fingertips.  Following that, it's probably the lips or the surface of the eye, but I am less certain on those two.

If you'd like a better answer, you'll probably need to be more specific about what you're looking for.  Some areas are very good at sensing heat, while others will respond better to other stimuli.",null,1,cdhrtwq,1qyb5l,askscience,new,2
homininet,"One rough way to look at this is by looking at the relative contribution of the sensory cortex of the brain to certain body parts. There is a famous figure called a [homunculus](http://sciencedefined.files.wordpress.com/2012/01/sensory-maps.jpg) which shows this. You can see from the figure that places like the face and lips and fingers and hands are incredibly sensitive relative to other areas.

Also google cortical homunculus if you want to see some other werid pictures of this representation.",null,0,cdhxf6m,1qyb5l,askscience,new,1
Stanage,"Yes, you *can* control the oxidation state of the product, although like most things, they will come in an equilibrium.

Although it is a completely different and unrelated process, think of protonation of a product.  Think of PO43-, which can occur as H3PO4, NaH2PO4, Na2HPO4, and so on.  You manipulate the product of these reactions by altering the pH of the reaction, with lower pH values favoring the more protonated state of PO43- since there is a higher abundance of available protons.

The same thing occurs with oxidation state.  In order to oxidate chemical A, you *must* reduce chemical B (B is the *oxidant* here, and A is the *reductant*).  In addition, you must be in favorable conditions so that once you oxidize A using B to make C, that chemical C is in the presence of enough of another oxidant (one strong enough) D to oxidize C to chemical E, and so on.  

Typically, when you react two species in a redox reaction, you will only be in favorable conditions for the production of one product in a given oxidation state, **assuming you do not have an excess amount of the oxidant**.  If you do have an excess amount, and say your oxidant is oxygen gas (O2), then you can keep oxidizing a compound until O2 is no longer strong enough to oxidize the resulting species to a higher oxidation state.",null,0,cdhs7s7,1qyb7u,askscience,new,2
Urgullibl,"Eventually, you will be capable of understanding the language as-is. Languages differ in grammar and syntax, which makes this sort of simultaneous in-head translation impractical for your brain.

Source: I speak 5 languages. English isn't my first, and I wrote this comment without thinking about its content in my native language.",null,2,cdhsj8m,1qybcz,askscience,new,15
null,null,null,1,cdhxul6,1qybcz,askscience,new,2
sargonkid,"I speak three languages very fluently (native level) - The first one was from birth - the second I started when I was about 10 - (moved to another country).  It took me a long time to not have to translate at all - decades of constant use of the second langauge - in fact - I did not speak the first langauge after moving.  About 15 years ago (age 40), I started a third langauge (immersed in it)- and it too, has been taking a long time.

It seems the older we get, the harder it is to make another langauge native to us.  I am in no way an expert - just know what I have gone through - and I am guessing the only way to become truley multilingual is to start from birth and learn all at once and at the same time - equally.

Funny thing though, I still dream mostly in my first langauge.

EDIT - Understanding a language has many levels.  To get by day to day is a lot lower level of understanding than communicating the abstract - such as politics or religion.  The true test of fully understanding (for me, not necessarily for others) was when I could carry on a deep, emotionally charged, controversial conversation.",null,0,cdj2bhz,1qybcz,askscience,new,1
null,null,null,0,cdhykph,1qybkw,askscience,new,2
jadiusatreu,"Insects that are more closely related can indeed hybridize. However, most hybrid suffer from sterility or at the least have very poor fitness: Meaning they wont survive very long or might not be able to reproduce.

The Concept of a Species is really whats gets you into trouble because, what we label as two species may still be able to reproduce with one another.  We classify species by letting nature do the work: If they don't recognize each other as mates in nature, we call them separate species.  But there are cases where two species are still closely related (Genetically similar enough) to reproduce. This hybrid rarely (if ever) goes on to become a separate species (as far as we know, there is some evidence of that possibility).

But as mentioned before, you can't get two very different insects to mate, genetics are two different. Plus they may not love one another.",null,0,cdi25r2,1qybkw,askscience,new,2
FujiKitakyusho,"Hydrogen combusts according to the equation


2H2 + O2 --&gt; 2H2O (-286 kJ/mol)

This reaction is exothermic, but it is not spontaneous.  It requires a minimum amount of energy to initiate, known as the activation energy of the reaction.  When you light hydrogen on fire, the spark or other source of ignition you use provides the initial activation energy. After that, the energy released by the reaction itself sustains the reaction by providing the activation energy for more hydrogen to react, and this continues until the energy released by the combusting hydrogen is insufficient to provide the activation energy for more reactions, or until one or more of the reactants is consumed. When you cool a reaction, you remove energy by conduction, convection or radiation, and reduce the amount of available energy to activate more reactions.  Thus, it is not the absolute temperature that determines whether a reaction will proceed, but rather the balance between energy produced and energy lost. As soon as more energy is being lost through cooling than is being produced by combustion, the amount of available energy decreases until it falls below the activation energy, at which point the reaction stops.
",null,0,cdhxe90,1qybou,askscience,new,5
The_Serious_Account,Hydrogen ignites at 500 degrees Celsius.,null,2,cdhyd1m,1qybou,askscience,new,1
lasserith,"Most clothes are made of polymers. These polymers are actually semi-crystalline liquids at room temperature. If these polymers are heated too hot they lose their crystallinity and will melt together. The lower the temperature these polymers form a 'melt' the cheaper it is to process as the factory that produces them needs to use less heat. Thus most synthetic fibers clothes are cheap because they use polymers with a low melt temperature. On the other hand because cotton is a natural product it doesn't need to be produced/melted but merely spun. Thus it doesn't matter the temperature at which it it softens. This means that in general you need a far lower temperature to soften out synthetic clothes then cotton clothes. 

Synopsis: Temperature required to iron is based off the temperature required to soften the polymers that make up the clothes. 

Edit: Nice example: Think of two extension cords tangled up. When they get all tangled it requires a certain amount of energy to untangle them (Shaking or moving the cords through knots etc). If you apply too much energy say by securing one cord to your truck and the other to a post and driving off you don't solve the knot problem but rather are more likely to break the cord.",null,0,cdied90,1qyc3x,askscience,new,1
OrbitalPete,"While we may be able to work out how the composition of Mars' atmosphere changed over time, we certainly do not currently have the means to replenish it, and there's definitely no way we're giving it a magnetic field.

Kim Stanley Robinson's 'Mars' trilogy gives a great account of how we might go about terraforming Mars, with the best suggestion for generating an atmospehre being to use comets. Basically, keep towing comets into Mars approaches so they burn up in the existing atmosphere, adding lots more water etc. But this is way beyond our capabilities at the moment, and would require a phenomenal amount of cometary material.

Starting up the geodynamo that would maintain a magnetic field is even further off the scale of possibility. The amount of energy you would have to inject into the mantle and core of Mars is just staggering, and we have no method for doing so even if we were able to generate that amount.",null,0,cdhuobt,1qyd2k,askscience,new,2
EdwardDeathBlack,"From what I have been taught, pure evolutionary snafu, just like [the laryngeal nerve](http://en.wikipedia.org/wiki/Recurrent_laryngeal_nerve#Evidence_of_evolution). 

The octopus, which evolved its eye structure separately from our evolution path, has it built the [""right way""](http://en.wikipedia.org/wiki/Cephalopod_eye) . So it is perfectly possible to do it the ""right way"", but we are in a metastable trap caused by evolution. ",null,0,cdhtiaj,1qydk6,askscience,new,3
opticreason,"Yes it's just evolution. Evolution thinks up a good idea and even if it's not perfect, it runs with it. 

Cephalapods, which  are thought to have evolved vision in parallel to vertebrates, actually have this [""better"" design](http://en.wikipedia.org/wiki/Cephalopod_eye)

",null,0,cdhrq4d,1qydk6,askscience,new,1
jswhitten,"For much of Earth's history the temperature at the poles was much warmer than it is today. We just happen to be in an ice age right now.

If the CO2 level in the atmosphere became high enough, the greenhouse effect would again result in less temperature difference between the poles and lower latitudes.",null,1,cdi7llr,1qydow,askscience,new,5
Gargatua13013,"A - ""Comfortably"" is a very relative term - The Inuit live in the high arctic and consider their territory an eminently comfortable place to inhabit.

B - If by comfortably, you mean comparable to conditions in the seasonal temperate zone, there were elaborate forests, with redwoods and mixed hardwood-conifer assemblages growing on Ellesmere Island during the Eocene, about 30 My ago (see:http://www.thecanadianencyclopedia.com/articles/fossil-plants). As you can see on this map, Ellesmere was in an Arctic position at the time: http://eas.unl.edu/~tfrank/History%20on%20the%20Rocks/Nebraska%20Geology/Cenozoic/cenozoic%20web/2/Timescale.html#6 .

C - Worth remembering: the effect of oceanic currents such as the Gulf Stream on actual climate is tremendous. As an example, consider the respective positions of Kuujjuaq and, say, Thurso in northern Scotland. Kuujjuaq is at the edge of the treeline with a definitely arctic climate, while the vicinity of Thurso is surrounded by fields denoting agricultural activity which would be unimaginable in Kuujjuak.

As far as your vision of aurorae above cityscapes, we see some quite regularly in southern Canada, around Montréal for instance provided you can get far away from the city lights and its light pollution. You might want to have a look in northern cities such as Oslo, St-Petersburg, Murmansk, Arkhangelsk and Edinburgh.

Also, keep in mind that aurorae do not form at the pole *per se*, but in a ring centered on the poles. ",null,0,cdi3o0d,1qydow,askscience,new,2
disconnectedLUT,"I bet even the poles on Mercury are hotter than a comfortable temperature...

I think it is always possible for the poles to be hotter, but I'm not sure if something large enough to be a planet could have a comfortable temperature at the poles and the equator. Perhaps a planet that has less/different land mass than earth could have better ocean currents to smooth out the temperature more effectively...",null,0,cdhsjds,1qydow,askscience,new,1
ssjsonic1,"The effect that /u/veganakos mentioned is not a strong effect.  The dominating mechanism is as follows:

As you turn up the temperature, the composition in the atmosphere changes.  Cool stars have many transition lines because the atmosphere has molecules and they have many ways of absorbing photons.  Heat up the atmosphere, and the molecules break into single atoms.  Single atoms also have lines due to the transition between electron states.  However, turn up the heat more and the electrons begin to be stripped away from the atom.  More heat means less electrons bound to atoms and less possible transitions.  Hydrogen and Helium transitions require a good chunk of energy, so hotter stars will produce deeper absorption features here.  However, for the hottest stars, the fraction of ionized H and He increases and the lines become weaker once again.

Here are some ionization energies ordered from the outermost electron to the innermost electron (remember H and He only have 1 and 2 electrons):
http://dept.astro.lsa.umich.edu/~cowley/ionen.htm

Edit: Notice that most elements have a small requirement to remove the first electron, but a much larger requirement for the second electron (H and He are an exception to this where the first electron requires a medium amount of energy).",null,0,cdia12v,1qygpc,askscience,new,2
veganakos,"Due to the strong gravitational forces at the surface of a star, heavier elements tend to settle down at deeper layers. However, if the stellar envelope is sufficiently cold, [convection](http://en.wikipedia.org/wiki/Convection_zone) becomes the dominant mechanism with which energy is transported from the interior to the surface. This allows heavier elements from the deep layers to move at the surface.    ",null,0,cdhuwwi,1qygpc,askscience,new,1
grikgrok,"Someone can probably write up a more comprehensive explanation but, first of all the bases bond Adenine to Thymine and Guanine to Cytosine, this results in a ""coding"" strand and a ""template"" strand in gene transcription. 
     Genes have information stored in codons, groups of three nitrogenous bases. Each possible sequence codes for an amino acid, the start of a gene, or the end of a gene. Here's a table showing the codes. http://www.uic.edu/classes/phys/phys461/phys450/ANJUM02/codon_table.jpg . The genes are ""transcribed"" to RNA molecules which leave the nucleus of the cell and ""translated"" into proteins.
     This translation takes place in the ribosomes of the cell, tRNA with complementary ""anti-codons"" bind to the RNA and add their amino acids to the growing protein.
     As not all genes are expressed all the time, you don't have hair growing out of your eyeballs, gene expression is regulated. Eukaryotic gene regulation is rather complicated, hopefully someone can give a good explanation of it here.
*the U in the table refers to Uracil, the RNA equivalent of Thymine, there is no thymine in RNA",null,0,cdhspea,1qyi5k,askscience,new,2
baloo_the_bear,"The DNA base pairs are grouped by threes. Let's say there is a length of DNA that reads ATGTGTCACATGACA. Now, RNA polymerase can come along and create the appropriate complementary mRNA for this strand: UACACAGUGUACUCU. In this strand, Thymine is replaced by Uracil. 

Now, the base pairs are grouped by threes: UAC-ACA-GUG-UAC-UCU. Each of these groups, or **codons** is matched by a specific, complementary tRNA. The tRNA that matches UAC will be ATG, and will correspond to a specific amino acid. As the codons get read, tRNA will bring in amino acids that are specific for each codon and eventually a protein strand will be made. ",null,0,cdhyagj,1qyi5k,askscience,new,2
itsokaybyme,"A,T,C and G's are on a strand of DNA, certain sequences of bases act as a trigger/flag for RNA polymerase and say, ""hey I'm a sequence of genes, I make a protein, copy me."" This flag is called a promoter region. This region is highly regulated and different subunits that attach to RNA polymerase ""see"" different promoter regions (think of a screwdriver with interchangable heads). 

The sequence that the RNA polymerase copies is usually pretty long and becomes mRNA (messanger RNA) which goes to be translated into a protein by a ribosome. However mRNA can be cut up and rearranged by various enzymes and RNA to alter its message, hence why we can get a lot of information from our DNA even through it is only 4 bases.

This whole process is also tightly regulated. Different hormones and chemicals affect how and when genes are transcribed. It's a very complicated process! Hopefully this helps to give you a brief into to the subject.

Source: Molecular Biology major. ",null,0,cdj5meb,1qyi5k,askscience,new,2
OrbitalPete,"You start with a river passing over a relatively flat geography.

Now you add some tectonic uplift, or a sea level retreat. You've now added a height difference between source and ocean, so there is more potential energy available. The river now has to cut down to reach the ocean. As it cuts down you can experience more uplift, leading to an gradual but persistant vertical carving of the canyon system.  That's about as complicated as it gets. IF the gradient of the river is low enough you can generate nice wide meanders, or if it's carving down quickly at a steep gradient you tend to get straighter channels. All of this can be effected by the local geology, so if for example the river passes first over a hard rock, then a soft one it will erode the soft rock preferentially, leading to waterfalls.",null,0,cdhuq3r,1qyi8i,askscience,new,4
OrbitalPete,"Yes. In fact, per unit area, the UK gets more (reported) tornadoes, whirlwinds and waterspouts than ~~anywhere else~~ any other country in the world*. The difference is that we don't have the big open plains environment that allows truly massive storms to develop, so what we get are usually no more than an F0.

In the US you have a conflagration of factors which produce very strong tornadoes - you have a very wide flat basin plain across the centre of the country, over which cold polar air can interact with warm tropical air.  In Europe and Asia these air masses have a very much harder time coming together because we have the Alps and the Himalaya respectively. 

*See wazoheat response below",null,1,cdhujc6,1qyihv,askscience,new,7
Olog,"In short, because Earth moves in its orbit. This is really best explained with pictures. NASA has a tool to view orbits of various small bodies, [here](http://ssd.jpl.nasa.gov/sbdb.cgi?sstr=ison&amp;orb=1). It's a bit clunky to use so I took some screenshots, [here](http://imgur.com/a/ixdii).

The first image is Jan 1st 2012, the comet is beyond Jupiter's orbit and Earth is roughly between the comet and the Sun. The second picture is Apr 1st, Earth has moved to the left side of its orbit while the comet has moved fairly little. It should be fairly easy to see why the comet appears to move sideways. It stays mostly in the same direction as seen from the Sun but Earth is moving sideways.

Up until now, Earth has been moving left, but now it starts moving right, so naturally the apparent motion of the comet will also change direction, and you can see this in the picture you linked.

The reason why the apparent motion doesn't change direction again in October is that by now the comet starts getting close to Earth's orbit, and inside it by November.

Edit: I just realised that your website actually has a video which might better visualise this than my pictures. The orientation is just different. And also note that the time slows down the further it goes. The date is Jan 1st just as the picture fades in and Earth is between the comet and the Sun. April is when Earth has done quarter of an orbit, only a couple of seconds into the video, and starts moving up again.",null,1,cdhv4j1,1qyip9,askscience,new,4
OrbitalPete,Image here http://www.nasa.gov/images/content/738138main_ISON_track.jpg,null,1,cdhuqr9,1qyip9,askscience,new,2
Diracdeltafunct,"Well they dont really ""orbit"". The word orbit is kind of an artifact of the original model of the atom.  Now its often still erroneously kept around because each eigenstate of an electron carries some angular momentum (similar to an orbiting or spinning body) and we thus call the quanta of angular momentum ""orbitals."" The shape of these orbitals are entirely dependent on this angular momentum.

Electrons are negatively charged and are thus attracted to the nucleus. The electron does not just get pulled into the nucleus for  reasons: 1.  It has a large amount of translational energy to keep it moving  2. There is an eventual strong force repulsion that creates a barrier. ",null,1,cdhym9u,1qyjyy,askscience,new,6
MayContainNugat,"Electrons are attracted to the nuclei of atoms because of the electric force, not gravitation. But like gravitation, the attraction (in this case, between positive and negative charge, as opposed to masses) prevents the electron from flying off and keeps it close to the nucleus.

The shapes of orbitals, labeled S, P, D, F, etc., are due to the electron not really being a hard sphere like a planet, but more like a wave. When electrons are free, not in atoms, their waves take on the familiar wavy shapes of sines and cosines that you might find on vibrating strings. But when held inside an atom, they take on the shapes of vibrating spherical membranes (like soap bubbles or the surface of the Sun). Spheres can vibrate all as one (the whole bubble going in, out, in, out, etc) and that would be the S wave. Or... two poles of the sphere can move inwards while the rest of the sphere moves outwards... and then the other way around. That would be the P vibration... and so on.",null,1,cdi54ru,1qyjyy,askscience,new,5
chrisbaird,"The force binding electrons to atomic nuclei is the electromagnetic force, pure and simple. Gravity is far too weak at this scale to play any role. The electromagnetic force is fundamental, so there is no deeper force causing it. The simple picture of an electrically negatively charged electron being attracted to an electrically positively charged nucleus is fundamentally true and is not some over-simplified picture taught to beginners. The electromagnetic force is carried by photons, so photons are exchanged between objects that have electric charge as they interact with each other.

The electrostatic force is a central force, similar to gravity, but when you add in magnetic effects, it gets more complicated. Also, electrons are not solid balls, but are quantized wave functions that spread out to fill the atom. The ""orbitals"" of an atom are not averaged locations of the electrons. The orbitals *are* the electrons. At a single instant of time, the entire orbital exists in its spread out form.

The shape of electrons when in the form of atomic orbitals is caused by an interaction of electromagnetic attraction and angular momentum, and by the fundamental quantum nature of an electron as a smooth, three-dimensional vibrating wavefunction.",null,0,cdiyd15,1qyjyy,askscience,new,2
rupert1920,"It is because the two are not the same.

Your example of a coin on a turntable has constant angular velocity, regardless of distance from the center. In other words, no matter how far away it is, it makes one revolution in the same period of time. This is _forcibly_ imposed on the coin by the friction of the turntable.

If you look at the equation for [centripetal force](http://en.wikipedia.org/wiki/Centripetal_force#Formula), you'll find that for constant angular velocity, centripetal force increases as a function of distance from the center. At some large distance, friction will not be able to apply enough centripetal force - and your coin will slip and be flung off.

In the case of gravity, you can find this is not the case. The inward force [does not increase as a function of distance](http://en.wikipedia.org/wiki/Law_of_gravitation) - in fact, it _decreases_ as a square of distance - so it can never maintain the same angular velocity as you change the distance.",null,0,cdht8uy,1qykuu,askscience,new,7
Astromike23,"Unlike solid-body rotation, gravity decreases in force as the distance squared. Two coins on a turntable make a full ""orbit"" in exactly the same amount of time. Two planets around the Sun do not.

In order to keep an object moving in a circle, there must be an acceleration acting on that object. (By acceleration, I don't mean that it's speeding up, but rather that its velocity is constantly changing direction.) There will be a balance between the tangential velocity always trying to propel the object away from the circle, and the acceleration keeping it on the circle. This careful balance can be written in equation form as:

a = v^(2)/r        (1)

...where a is the acceleration, v is the velocity, and r is the radius of the circle. In the case of gravitational acceleration, Newton taught us that's it's equal to:

a = GM/r^2           (2)

...where G is the gravitational constant, M is the mass of the central body, and r is again the radius. We can set these two things equal and solve for v:

v^(2)/r = GM/r^2

v = sqrt(GM/r)    (3)

So, the velocity of an orbit decreases as  as one over the square root of the size of the circle. In other words, if a planet is 4 times as far from the Sun as Earth, its velocity will be half that of Earth's to stay in a circular orbit. This is simply because gravity weakens as one gets further from the source, and so the velocity must also decrease to stay in a circle - otherwise it would go flying off.

Now in the case of a turntable, the velocity increases with increasing distance. The coin at the edge of the turntable traces out a larger circle than the coin near the center even though they both must make a full circle in the same amount of time, so the edge-coin must be moving more quickly. In fact, the equation for how quickly a coin must move will be:

v=ωr

where ω is how fast the turntable is spinning. We can substitute that v into equation (1) to find the acceleration:

a = (ωr)^(2)/r = ω^(2)r^(2)/r = ω^(2)r   (4)

So, in the turntable case, the acceleration to stay in a circle actually *increases* with increasing distance, very different from gravity. This makes sense - imagine you were sitting on a giant rotating turntable the size of city block. It would be easy to sit still if your were close to the center...but at the edge you'd have to hold on for dear life.

**TL;DR:** Because gravity decreases with increasing distance. ",null,1,cdhtklg,1qykuu,askscience,new,2
chrisbaird,"Fundamentally, it depends on the strength of the interaction between the objects that are orbiting. For instance, the limbs of our galaxy orbit like a collection of independent bodies (like planets in our solar system). But in the nucleus of our galaxy, the interaction between stars is strong enough that they orbit more like a solid sphere (like your turn table). The interaction or atoms in the turntable at different radii are strong enough that the object is rigid so that they must all rotate with the same angular frequency. The different atoms lock each other into this type of motion. In a rough sense, the atoms at smaller radii pull the atoms at large radii faster then they would normally go if they were left to orbit independently. In the same way, in the nucleus of our galaxy, the attraction between stars is strong enough that the inner stars pull the outer stars faster than they would normally go if they orbited independently.",null,0,cdixxy4,1qykuu,askscience,new,1
Goeatabagofdicks,"Believe it or not, gravity is actually considered a weak force.  With response to the question you asked, like the others have said, you are referring to cetripetal force.  With gravity, you are creating a warp in space time depending on the mass of the object.  So, when astronaughts are ""weightless"" in space, truly they are not.  The are constantly falling towards the earth.  Idk if you are from America, but here, in various malls and such, we have a concave coin donation object where you place a quarter in and it spins around and around going faster and faster until its ultimately deposited into the hole in the center.  This is a great representation of space time.  The more concave it is, the greater the mass of the object.  Size really doesn't matter, it's mass.  If I had a cotton ball the size of the earth, it would warp space time significantly less than a lead ball the size of the earth.  Further, the ""falling"" of the object is independant of the rotation of the earth.  Now lets speak of centripetal force.  Lets say I attach a pole of infinite strength to Pluto.  It takes something like 248 years for Pluto to have one rotation around the sun.  If its orbit in the solar system was directly related and fixed to the rotation of the earth, it would be hauling butt around the sun.  Much like if I put you on rollerscates in a parking lot.  I'm in the center holding a pole that you are whipping you around.  Depending on the length of the pole, the faster you go.  Gravity is a warp in time space, what you suggest is centripetal force.",null,3,cdhuhyx,1qykuu,askscience,new,1
wazoheat,"""Radar"" maps like that on Intellicast, at least back in early 2011, were not actually detecting the precipitation type. The radars were ""dumb"" in that they could only estimate the intensity of the ""echo"" reflecting back from what the radar beam hit in the air, whether it be rain or snow or dust or birds. Maps like the one you show above were made by a process of stitching radar images from all the individual sites together, then using an algorithm based on actual surface observations to estimate what the precipitation type is throughout the US. So the ""circle"" of snow probably didn't exist at all, but was probably due to a bad observation or two.

Though is probably different now because, with the national [upgrade to dual-polarization radar](http://www.nws.noaa.gov/com/weatherreadynation/news/130425_dualpol.html), radars can now directly estimate the precipitation type.",null,0,cdhv0be,1qylwu,askscience,new,2
coloroftheskye,"There are particles which we call Force carriers. These particles are used to explain how forces act on a distance. For instance in the sense of magnetic attraction and repulsion we explain it by an exchange of ""virtual"" photons. As you may know photons are massless and thus the distance they can travel is infinite. other forces such as the strong and weak nuclear force are mediated by other particles which do have mass, That is why these forces get much more weaker over a distance than the electromagnetic force. On very small distance scales these forces are stronger than the electromagnetic force and thus protons can exist (protons are positively charged and thus repel each other but on the small scale in the nucleus the nuclear force is stronger). For the very same reason fusion is so hard to achieve. We first need to overcome the electromagnetic repulsion from the electrons and nuclei before the nuclear force can take over.

wikipedia is quite helpful with this: http://en.wikipedia.org/wiki/Force_carrier",null,1,cdhtyt4,1qymea,askscience,new,3
teramut,"No, different regions expand at different rates depending on how much matter there is. Underdense regions expand faster, and overdense regions slower. There are also stable structures, galaxies, where the expansion has first turned into contraction, and contraction has been balanced by internal pressure and rotation. 

The expansion rate is constant only if you take averages over large enough regions. There are some attempts at evaluating this homogeneity scale, above which the universe looks homogeneous, and it's roughly 300 million light-years. So in regions larger than this, the expansion rates should be equal on average.",null,1,cdhzv2b,1qymma,askscience,new,2
null,null,null,4,cdhxpdz,1qymma,askscience,new,1
afcagroo,"It is really hard to address this without knowing what is meant by memory ""performance"". If it means the speed or bandwidth of the memory, then the answer is fairly simple. Most of the Moore's Law improvements in memory are used to put more bits onto a die, rather than to increase how fast bits can be accessed.  The former is simply more valuable in general than the latter, since (as someone else mentioned) the bandwidth is somewhat constrained by other factors such as the PCB traces and number of I/O's available in a given space. 
   
There is a tradeoff to be made.  You can keep the bit cell capacitance fairly high so that the charge transfer settles quickly and you can read the cell faster, or you can reduce the bit cell capacitance (and make a smaller cell) but take a penalty for longer settling times. 
  
It is possible to instead make the memory faster but not much more dense, and to some extent that has been done in the past on SRAMs. But at some point, you get two where making the chip faster is nonsensical, since you can't get the information in/out any faster in a reasonably practical system.  
  
If you look for charts of DRAM *density* as a function of time, I think you will find it improving at a similar rate to processor performance. At one time it was actually improving a bit faster, but I don't know if that's true today.   
  
Some of the manufacturing technologies are different, too. Processors tend to use straight CMOS logic. DRAMs need to maximize the capacitance of the bit cell, so they have gone to some trench technologies and vertical stacking that require unusual processes.  ",null,0,cdi1els,1qypga,askscience,new,4
spPad,"RAMs are interconnect dominated - so the larger you make each bank, the slower it will get. Each bit-cell in RAMs have been getting much faster, but the problem is that the interconnects get worse at smaller technology nodes. This is bad for interconnect dominated systems.

Secondly, your off chip memory (DRAM) is usually not limited by the speed of the RAM, but the speed of the bus. The wires on the PCB are *huge* compared to the wires within the chip.

Lastly, you do see the impact of smaller device sizes on the capacity of the RAM, not necessarily on its speed. Back in 2007 I was running a laptop with 64MB DDR RAM and was perfectly satisfied. Today, my phone has 2GB. ",null,0,cdhz0sw,1qypga,askscience,new,3
Luminarie,EDIT: They guy below is actually right :D.,null,1,cdhvl6u,1qypga,askscience,new,2
mzellers,"For the processor, having more transistors allows for increased parallelism.  That allows you to do more work in each cycle. Some processors will even proceed down both paths of a conditional branch and throw away the results of the wrong path!  Today, multiple core CPUs are quite common.  For a memory chip, other than allowing for a wider data path, there is no similar gain.",null,0,cditmwv,1qypga,askscience,new,1
_wigga_,"Radioactive decay is a nuclear process not an atomic process. It also is a statistical process and one nucleus does not have an influence on another. Each nucleus decides independently when it decays. So the answer is no it does not matter which state your substance is in

If your substance is a solid, liquid, gas, frozen to almost 0 K, the radioactive decay rate would be the same.

Actually there was a paper once that showed the possibility of delayed radioactivity at lower temperatures but this is nonsense and was disproved by experiments.",null,3,cdhwqoo,1qypjq,askscience,new,4
FizixPhun,"I don't know the exact answer but here is my guess given my knowledge of Bose Einstein condensates and radioactive decay.

First, let’s talk about radioactive decay.  Basically, this is when an atom has a nucleus that is too large to be stable and a chunk flies off to make it more stable.   The result is an atom with a different atomic number, hence a different element from our original atom, and some other decay products.

Now, let’s discuss Bose Einstein condensates.  This is a phenomenon that occurs when many bosons occupy the lowest energy state, the ground state.  The wave function for our system must include information about all the particles.  Bosons are identical particles that are required to have a wave function that is even under the interchange of any two labels for our particles.  The key nugget is that changing the state of even one of the bosons changes the wave function for all particles and is therefore energetically unfavorable.

Now let’s combine these aspects.  To get a Bose Einstein condensate we must first have atoms that are bosons.  Now we also need these atoms to be radioactive which pretty much limits us to very heavy elements.  Now cool them to a sufficiently low temperature that they are all in the ground state and behave as a condensate.  If an atom radioactively decays, it is no longer the same type of identical particle and it can no longer be part of the condensate.  The Bose Einstein condensate only occurs when the bosons are indistinguishable.  Clearly, atoms of different elements are distinguishable.  This would cause the wave function to change as it goes from n identical bosons to n-1 identical bosons and one other atom.  Basically, we just have one less atom in our condensate.  I’m almost 100% confident that being in a condensate would not cause all atoms to radioactively decay if one does.  I’m less certain but suspect that being in a condensate would not prevent an atom from radioactively decaying.  In reality, this would be pretty difficult to measure as the critical temperature below which a condensate occurs scales as 1/mass.  As previously stated, radioactivity occurs mostly in very massive elements, which means that the condensates would be very hard to achieve because they would require exceptionally low temperatures.

Maybe some atomic and ultra-cold experts can comment on this? ",null,1,cdih7an,1qypjq,askscience,new,2
adamhstevens,"&gt; How do they know these were rivers and oceans of specifically water and not some other liquid, or for that matter solid glaciers? Also how do they know the rivers entered oceans instead of simply evaporating?

We don't. There are theories that it could have been caused by 'fluidised' carbon dioxide (CO2 can't exist as a liquid under current martian pressures and temperatures, which of course may have been higher in the past, but it can exists as a weird mixture of gas and dust that forms a liquid-like fluid that can flow), but really, the only candidate for a liquid at the conditions that we expect could have occurred is water. There just aren't that many substances that exist as a liquid that at standard planetary conditions and are also abundant enough to form flows.

There are also people that think the features could have been caused by glaciers, since a lot of the morphology is similar. However, in depth studies show important features that can't really be caused by glaciers, e.g. http://www.wired.com/wiredscience/2011/06/teardrop-shaped-island/. There are a lot of different channels on Mars, though, so it could be there are some caused by both mechanisms.

In terms of the oceans, some of the channels (generally the shallower ones) do just stop and appear to have evaporated and/or infiltrated into the regolith. However, there are also some features that look like deltas, which would suggest a river joining an ocean e.g. http://photojournal.jpl.nasa.gov/catalog/PIA04869.",null,2,cdhxg1y,1qypmp,askscience,new,3
adamsolomon,"&gt;Since the universe itself moves faster than light, would it be hypothetically possible to see the light from an extremly old object in its Infancy, say a few billion light years away, while being next to that object in present day? If not, under what conditions, if any, would this be possible?

No. The Universe doesn't actually move faster than light - or move at all. It's expanding. This means that individual points - galaxies, say - move away from each other. Nearby ones move away from each other below the speed of light, faraway ones recede from each other faster than light. But light travels in straight lines, so once it's past you, it's past you. In order to see old light from something sitting next to you, the Universe itself would have to be curved like a sphere, so that light goes all the way around. The trouble with this is that a Universe curved like this will eventually recollapse in a Big Crunch. It turns out that light emitted at the Big Bang can only go around the whole Universe at most once before the Big Crunch happens.

(By the way, that's all hypothetical - there's no evidence to suggest that the Universe has that curvature or will ever have a Big Crunch.)

&gt;I saw a picture of the universe an extremely short time after the big bang and it made me wonder if that picture included the ingredients for our galaxy, solar system and even our planet. It has to, right? So I was wondering more specifically about individual objects as well.

The ingredients, definitely. Now, we wouldn't be able to see the light from the place that became *our* galaxy, since we emitted that light billions of years ago and we haven't moved (as I discussed before). But we can see the seeds of distant galaxies which today would look much like our own.

As for individual objects, you can't see those because they hadn't formed yet. The seeds you see in the images of the cosmic microwave background (the oldest light in the Universe) represent slightly overdense and underdense regions of space. The overdense parts collapsed under gravity, forming stars and galaxies in the process.",null,0,cdhvior,1qyq5k,askscience,new,2
null,"a) The universe isn't currently expanding faster than light.  It did so during it's inflationary period but it's expansion slowed drastically after that.  It's true that the expansion rate is accelerating but it still isn't anywhere close to c.

b) You could come up with some geometric configuration that would allow a light ray to travel in a closed path of a few billion light years so you could observe it from it's origin.  It's possible such a configuration exists in the universe but since we can only observe from Earth's neighborhood we wouldn't be aware of it.

c) The late big bang environment included hydrogen, a tiny bit of helium, and an even tinier bit of lithium.  That's it.  Everything else was manufactured by stellar processes.  Elements heavier than iron were manufactured in supernovae.",null,1,cdhv2y3,1qyq5k,askscience,new,2
adamsolomon,"I'm not sure quite what you're referring to - maybe you could find an example from a popular exposition you've read.

Quantum field theory does have wiggle room. In fact, quantum field theory isn't a theory at all, but a framework, and you can define all sorts of quantum field theories. Some of them (like the ones making up the standard model of particle physics) correspond well with reality, and others are just toy models on paper. And these have some wiggle room.

What a good quantum field theory does have is some guiding principle, namely a symmetry. For example, pretty much any good quantum field theory should look the same no matter what speed you're travelling at, or how your lab is oriented. (Physicists call this Lorentz symmetry.) More complicated mathematical symmetries exist and are crucial to defining other field theories. For example, electromagnetism has a symmetry called U(1) gauge symmetry - don't mind the name, it just means that if you change the variables in a certain way, the changes cancel such that the equations of the theory stay the same.

These symmetries are crucial to modern physics, because they do restrict the kinds of terms you can write in your equations. Namely, once you choose the symmetry your theory has, your equations can only have terms which don't change under that symmetry.

So when you define a field theory, you tend to choose the symmetry, and then choose which allowed terms are there, what the constants are, etc. It's not as if the theory has a perfectly rigid structure - though you definitely don't have complete freedom.

VERY IMPORTANT, by the way - if your theory doesn't obey this symmetry or that, it's not as if it's logically invalid. It may have theoretical issues to overcome, it may very well disagree with experiment, but it's not a priori illogical.

Newtonian mechanics has symmetry too, even though I think it was rarely thought about in that way. It has, for example, spacetime translation symmetry - if I move around in space or wait a long time, F is still ma. F = ma + 1 N would maintain this symmetry, but something like F=ma + 1 N * (the year in AD) wouldn't.",null,1,cdhvfih,1qys8x,askscience,new,7
KToff,"1. The mass of the earth is so incredibly large that it is impossible to accumulate any significant amount of garbage. The US produces roughly 250 million tons of garbage per year (2.5 *10^11 kg of solid waste). The mass of the earth is give or take 6*10^24 kg. This means that (at the current output) in a million years the US would not even produce a tenth of a million of the earth's mass.

2. Even if you were to launch significant chunks of the earth into space (and neglecting the energy requirements of doing that), removing a lot of mass would not change the days in a year (they almost exclusively depend on the mass of the sun and the distance between earth and sun), the length of the day (depends on the rotation period of earth which is not directly affected by removal of mass). Depending on where the mass goes it could have an effect on the tides because the moon earth system would change quite a bit, if one of the partners suddenly changed weight.

TL;DR; We could never throw enough stuff into orbit to have any significant impact (in the sense of changes in the behaviour of the celestial body earth)",null,0,cdhvxzj,1qysak,askscience,new,3
hooligan333,"Light attracts zooplankton, as it usually indicates the presence of bioluminescent bacteria which they feed upon. [Source, which also explains much below](http://www.alphagalileo.org/ViewItem.aspx?ItemId=117844&amp;CultureCode=en).   
These zooplankton in turn are a significant food source for many smaller fish. So these fish will readily approach such beacons (which conveniently also make their intended prey easier to spot) with the promise of an easy meal. As coming across a predator utilizing bioluminescent lures is significantly less frequent than coming across said bacterial colonies, this 'gamble' frequently pays off for the fish. So for these fish to develop an aversion to light sources would deprive them of a major food source and this would ultimately be far more detrimental to the survival of a species than losing the occasional individual to such deceptive predators. So natural selection, working as it does, continues to favor the fish that go towards the light...

Edit: Formatting",null,0,cdhx0zd,1qyt1z,askscience,new,16
Izawwlgood,"Think about it this way; those predators are mimicking something the prey fish want. The question shouldn't be 'why would prey fish not evolve to avoid that?' but 'what do the prey fish want that glows?'. 

You know how crocodiles stalk watering holes waiting for gazelle to come drink water? Gazelle can't evolve to not drink water.",null,1,cdhyo68,1qyt1z,askscience,new,5
lmo2th,It does complete it's shell. Cyanides have a counter ion (usually Na or K) or a hydrogen atom also bonded to the carbon. It gets 3 extra electrons from the C-N triple bond (one from each bond) and one extra from the counter ion or hydrogen atom. That gives a total of 4 electrons. Add this to carbons 4 valence electrons and you get 8 - a complete octet.,null,1,cdhztdg,1qyupv,askscience,new,5
dragonnyxx,"Cyanide isn't a chemical compound. The word ""cyanide"" refers to chemicals which contain the cyano (CN) group, such as potassium cyanide or hydrogen cyanide. These neutral molecules *do* contain complete electron shells, whereas the CN ion by itself does not.

""Cyanide"" is used as a general term for such chemicals because the free CN ion is poisonous, and therefore it doesn't particularly matter whether it was potassium cyanide or sodium cyanide or hydrogen cyanide you were exposed to, you're going to be poisoned the exact same way from any of them.",null,8,cdhzx18,1qyupv,askscience,new,4
FizixPhun,"While not very mathematical, here is the intuitive way I understand it.  Think about what is happening microscopically when you transition from one phase to another.  Lets take ice melting into water as an example.  When you heat ice up to 0C, it begins to melt.  In solid ice, the molecules are well ordered and are hydrogen bonded with their neighbors.  When you melt it, you are breaking these nicely ordered bonds.  This takes energy in the form of heat.

Equivalently, you can also think about the fact that entropy is different in two different phases of a material.  Entropy can roughly be thought of as how disordered a system is.  The molecules in ice are nicely bonded into a lattice, much more ordered than the moving molecules in water.  Even though you are not changing the temperature when you melt the ice, you increase the entropy.  This increase in entropy costs heat as a change is entropy is defined as the amount of heat added divided by the temperature at which the heat is added.",null,0,cdifxam,1qyxr6,askscience,new,1
__Pers,"It's only strictly true that they can't if the surfaces of the waveguide are perfect conductors. 

Assuming that they are (or that the conductivity is high enough that they can be treated as such), then the surface of the conductor is an equipotential, meaning that the electric field must vanish inside the waveguide. The magnetic field of a TEM wave is proportional to the electric field, so the magnetic field also vanishes. This is why you need at least two topologically distinct conducting surfaces (e.g., a co-axial cable) to propagate a TEM wave. 

Edit: typo. ",null,0,cdhzrp3,1qyy8r,askscience,new,3
Pluckerpluck,"Before anything forms we have a cloud of dust. This will eventually become our solar system.

This cloud will almost certainly have some amount of spin around its center of mass. It could be minute, but as the dust contracts conservation of angular momentum will increase this spin. At the same time these particles will start smashing together an start forming planets.

At this point you can think of it in a few ways. One way is to realize that the system is basically a big ball of mass rotating around a central axis. Perpendicular to this central axis inertia (centrifugal force) is stopping the sphere collapsing. However, parallel to this axis this force/inertia doesn't exist. As such the sphere can collapse along this axis and thus forms a disk (this is my preferred way of explaining this). It's similar to how spinning pizza dough in the air becomes a disk.

Another way to look at this is to imagine two planets orbiting in planes at a slight angle off from each other. At this point it should be relatively easy to realize that these planets will attract each other and thus get closer together (and thus move into more similar planes of orbit).

The final way to look at this is in a more mathematical way and state that the system ""likes"" to be in it's lowest energy state.  If we conserve the total angular momentum of the planets, then a flat disc has the least total kinetic energy. As such when the system slowly loses energy (from collisions and gravitational effects) it will move towards a state of lower kinetic energy and thus move towards a disk shape.",null,1,cdhz44g,1qz13p,askscience,new,8
bellcrank,"This appears to be a frontal cyclone, or midlatitude cyclone, not a tropical cyclone.  Europeans often give names to these systems, though this practice is not typical in the United States.  I think The Weather Channel has started giving names to frontal cyclones in the US, but it is not an official practice.",null,0,cdi8rv7,1qz3jz,askscience,new,2
wazoheat,"Is it a real cyclone? Yes. ""Cyclone"" is a term in meteorology for *any* rotating cyclonic (counter-clockwise in the northern hemisphere, clockwise in the southern) storm of any size or origin, from dust devils and tornadoes to hurricanes and frontal storm systems.

Is it a *tropical cyclone*, akin to typhoons and hurricanes? No. It is a ""cold core"" cyclone with associated warm and cold fronts, just like every other average storm system in the mid-latitudes (between 30 and 60 degrees away from the equator). I'm not sure where the name came from, I can't seem to find any information on it.",null,0,cdif77e,1qz3jz,askscience,new,2
OrbitalPete,"Mars' volcanic activity is very different to that on Earth. Earth volcanism is driven by plate tectonics, with the vast, vast majority of volcanoes being located at either constructive or destructive plate margins. In contrast, volcanism on Mars is believed to be far more analogous to hot spot volcanism (that which forms the hawaiian islands for example).  Mars has no active plate tectonics due to the fact it's cooled down. That same explanation is highly likely to explain why Martian volcanism has died down.

Martian volcanoes are basaltic in nature - the same as plume volcanoes here on earth. That suggests they are forming from a partial melt within the mantle, then rising buoyantly through the crust. because Mars never had the active tectonics on a scale similar to Earth the crust has never been able to get highly silicious, hence crustal contamination of melt as it rises has not seemingly varied the chemistry too much from the original basalt.

It's possible Mars is still volcanically active, we're just in a window where we have not observed any eruptions. It may be that the eruption rate has died down significantly such that thousands or even millions of years may pass between events.

One interesting thing to note - because of the low atmospheric pressure on Mars, it means that while the chemistry of the basalts is fairly similar, instead of flowing out leisurely as they do on earth it becomes possible for gas bubbles to inflate to many times what you would expect in a heavier atmosphere. That drives the ascending jet more powerfully, which in turn means you can get far more violent eruptions from what on earth would behave as a fairly low-energy effusive eruption.",null,14,cdhzrvz,1qz3mf,askscience,new,86
TheGreatFabsy,"Until an expert gets here, I'll offer some of my knowledge.

Simply put it's because Mars isn't active.
The core is ""shut down"", cold. That's why there isn't a magnetic field on Mars, also the reason Mars lost it's atmosphere. The Sun is bombarding the planet with high charged particles, effectively ""sand blasting"" the atmosphere. Good thing the Earth is active! It's magnetic field is repelling these particles, kinda like James Bond deflects that bullet with his magnetic watch! (MythBusters busted the Bond myth, tho.)",null,8,cdhzecc,1qz3mf,askscience,new,17
baloo_the_bear,"Thermal burns don't just affect the most outer layers of skin. The heat energy that caused the burn will continue to dissipate into deeper layers of skin. Pouring cold water over a burn helps to give that heat energy somewhere to go besides your tissue and helps prevent the burn from going from a first degree to a second degree (or second to third, etc). ",null,1,cdi6elf,1qz402,askscience,new,3
brawnkowsky,"well, burns interfere with the epidermis' ability to prevent water loss via destruction of cells (these cells prevent most evaporation of water from plasma and interstitial fluid), and the large amount of energy transferred to the skin causes water to evaporate.  running water over a burn restores that lost water, which probably is the cause of the 'good feeling'.

http://www.anaesthesiamcq.com/FluidBook/fl3_2.php  ",null,2,cdhzow6,1qz402,askscience,new,2
Das_Mime,"[This diagram should hopefully help clear it up](http://www.moonconnection.com/images/moon_phases_diagram.jpg)

When the moon is up in the daytime, it's between you and the Sun, so you're mostly going to see the part of the Moon that is in shade at the time. If you want to see a full moon, then you (Earth) need to be between the Moon and Sun-- which happens at nighttime.",null,0,cdhyawx,1qz4pb,askscience,new,10
Queen-of-Hobo-Jungle,"It is called Lunar Cycles. The moon rotates around the earth every 30 days, approximately. When it is in front of the sun, we don't see it at night, but it occasionally casts shadows over the tropics as eclipses. Directly across from the sun at night, and it appears full for most of the night. Any time inbetween, the earth is casting a partial shadow, and as the moon moves from waning to new, it appears closer to dawn.",null,6,cdhzdkw,1qz4pb,askscience,new,4
do_od,"Correct, you could build a x-ray camera with a pinhole lens just as you would build any other pinhole camera. You could also use a [zone plate](http://en.wikipedia.org/wiki/Zone_plate) which has the advantage of greater aperture for a given resolution, that is it makes sharper images than a pinhole of the same area. If you know the wavelength and focal lenght of your setup you can calculate the geometry for the zone plate using the formula found on wikipedia or just google zone plate generator, there are lots of online tools. Zone plates can also be approximated with an [array of pinholes like this](http://2.bp.blogspot.com/_9xmT5wmPOhY/Rx2TyQ19hxI/AAAAAAAABH8/3slGBsx1nF8/s400/pinsieve.jpg). 

Edit: Ofcourse, fabricating the plate is a different matter. For visible light it is as simple as photographing a template with a film camera and use the negative. I would suggest etching the pattern in a thin metallic film on a substrate that is transparent to x-rays, sort of like how [integrated circuits are made](http://en.wikipedia.org/wiki/Photolithography). ",null,2,cdhzqtr,1qz517,askscience,new,9
uzzors2k,"Here is a [forum thread](http://4hv.org/e107_plugins/forum/forum_viewtopic.php?106277.120#post_109615) by some amateurs using a single pin-hole camera to determine the x-ray emission points in a vacuum tube. You can see the actual [x-ray ""camera""](http://4hv.org/e107_plugins/forum/forum_viewtopic.php?106277.150#post_109830) here. The x-ray source is a high voltage rectifier tube, reverse biased and run at anode voltages far above it's specification. About creating a lens, I am unsure.",null,1,cdi0ead,1qz517,askscience,new,3
ron_leflore,"A single pin hole would work.  The problem is that the throughput is so small, that you would need super strong sources and/or very long exposure times.  So his type of lens is not used for x-rays.

Several x-ray telescopes have been built using grazing incidence techniques.  For more information on x-ray lenses, start here http://en.wikipedia.org/wiki/X-ray_optics",null,0,cdi8zfu,1qz517,askscience,new,1
ZeroCool1,"**The main factor is the Gibbs free energy of reaction occurring in the battery.**  Temperature plays a role as well.  


The Gibbs free energy of a reaction is a measure of the ""usable work"" available from the reaction.  In general for a reaction to occur, a negative number is required, indicating an excess energy.  If a reaction has a positive Gibbs, the reaction does not occur spontaneously, due to a lack of energy.  Sometimes energy can be given to the reaction through an increase in temperature, sometimes no amount of temperature increase will make it run.

In fact there is a relationship between the Gibbs free energy of reaction, the equilibrium constant of a reaction K (or Q if it is not in equilibrium), and the cell potential.  See slide 15 for the ""triangle"" of equations.

http://courses.washington.edu/bhrchem/c152/Lec14.pdf

The whole lecture explains the concept.

Here's a great picture:

http://chemwiki.ucdavis.edu/@api/deki/files/126/triangle_diagram.png?size=bestfit&amp;width=463&amp;height=298&amp;revision=1",null,0,cdi0msg,1qz6fm,askscience,new,3
BadDadWhy,"This [Equation](http://upload.wikimedia.org/math/d/3/b/d3bf57847ca22c9cee3a1865d3548eae.png) shows that most of it comes from the materials difference, then that is adjusted by the differences in concentration of the parts. They don't go into internal and external resistances but in real life they are part of the circuit and effect the cell voltage. 

You may notice that temp doesn't come into the equation but batteries work worse in cold weather. That is because the kinetics of transport are slower, you get less current not less voltage.

Me heap big electrochemical engineer.",null,1,cdhyp0u,1qz6fm,askscience,new,2
loctopode,"It differs between animals, but in general:
Sleep is a regular reduction in activity. There is a reduction in body temperature and oxygen consumption. The metabolic rate in sheep is reduced by up to 13% while sleeping, humans reduced by up to about 12%. The body can still respond to external stimuli and arousal (""waking up"") can happen quite quickly.

'Torpor' is similar to sleeping, but can last for a few days. The metabolic reduction is greater, and can be down to about 30% basal metabolic rate. Body temperature can lower, but remains above ambient. It's used especially by some small mammals who undergo torpor daily to avoid times when food is scarce. Arousal can take up to an hour.

Hibernation lasts for much longer (several months). Possible triggers can be reduction in temperature, food availability, light level etc (and will vary between species). Heart rate decreases. Metabolic rate is lower than in torpor, potentially going down to about 5%. Body temperature drops even further, coming very close to ambient temperature. ""Respiratory acidosis"" occurs, where the intra/extra-cellular pH decreases. This inhibits the activity of some enzymes and helps reduce metabolic rate. Normal bodily reactions to a reduce in core temperature are stopped. Response to external stimuli reduced in comparison to sleeping. Arousal can take a few hours and happens periodically (number of theories why they wake up, some suggest it's to restore energy, others say it's to initiate an immunse response, with some evidence indicating animals are susceptible to parasites while hibernating). 

The cause of a coma varies, but when in a coma the body doesn't reduce temperature or metabolic rate like hibernation. The individual may wake from the coma within a few days, weeks etc or not at all, it can vary with the cause.

Death is a real reduction in temperature and metabolic rate. The heart stops beating (not just slows like in hibernating). The body then can eventually ambient temperature. However, unlike in hibernation, it's very diffcult to recover from this without (i.e. medical help).

So hibernation can be seen as a greater/deeper sleep, but there are important differences especially regarding metabolic/temperature rates.",null,0,cdidwqq,1qzb91,askscience,new,5
null,null,null,0,cdibpk8,1qzb91,askscience,new,2
iorgfeflkd,"The unit we use to describe time is arbitrary, and the age of the universe is consistent under proper conversion between them.

Humans use Earth years when discussing long periods of time to communicate with other humans, because that's what humans are familiar with.",null,1,cdi0xva,1qzc7u,askscience,new,4
heyamipeeing,"What you should be asking is if every where in the universe experiences this moment in time simultaneously. Even though the universal constant (c) prevents us from witnessing events in ""real-time"", it is interesting to wonder whether every place in the universe has experienced the same amount of elapsed time since 0 (the Big Bang) or if some unknown effects cause time to speed up or slow down. If someone could shed some light on this that would be awesome.",null,0,cdi18pt,1qzc7u,askscience,new,2
Whisket,"To start, there is a distinction between temperature and heat. In a given amount of water, the temperature of water is the *average* energy level amongst all of the molecules of water. However, heat is the *quantity* of energy that the amount of water has in total. As an example, the ocean has a lot more heat than a boiling cup of tea, but the cup of tea has a higher temperature. The quantity of heat in the ocean is greater than the tea, but the average temperature in the cup of tea is greater than the ocean.

At room temperature, the average energy levels in the water is not great enough to cause it to evaporate. However, individual water molecules do not all have the same quantity of heat, and some can gain enough energy to evaporate. This amount of energy is then lost from the source of water (evaporative cooling), but it will soon be replaced from the ambient air (at room temperature). Thus, the average temperature will remain constant, but individual water molecules are capable of evaporating if they gain more energy.

This is also why things will dry faster with higher temperature, even below boiling. The average temperature of the water molecules is higher, and a molecule takes less deviation from the average to gain enough energy to evaporate.",null,9,cdi3e43,1qzdx1,askscience,new,37
Cassiel23,"Water has energy.  At any temperature, molecules of water move.  When the movement of water molecules is sufficiently fast to break their attraction to other water molecules, the molecules are released into the atmosphere as vapor. 

This occurs at a temperature of 100 degrees F for water molecules, but at lower temperatures there is still some energy transfer between molecules sufficient to break molecular attraction and release the water as vapor (i.e.; ""drying"" the wet object), although this process is much slower and less efficient than at higher temperatures.",null,0,cdi3c3a,1qzdx1,askscience,new,8
BigPapaTyrannax,"Consider a closed container half full (or half empty) with water. There is a equilibrium between the two phases of water, the liquid phase that we can see and the vapor phase that we cannot. The amount of water molecules in either phase is dictated by thermodynamics. Without getting into the complex math behind it, at higher temperatures, the liquid water has a higher chemical potential and will be more likely to enter the gas phase. The bigger the area for the vapor phase to expand into, the more molecules will enter the vapor phase (once again, tons of math that you probably don't want to see). Basically, you can get more of the liquid to turn into gas by heating it up or by giving it more room to expand into. 

How does this apply to your question? Well, things ""dry"" at room temperature because there is an equilibrium between the liquid phase of the wet object, and the vapor phase of the room around you. The liquid phase and the vapor phase must come to an equilibrium. Since the room is so large, and the wet object has relatively small amounts of water, the equilibrium will be such that the small amount of liquid is converted entirely to vapor. 

Source: Undergrad bioengineering student, currently taking advnaced physical chemistry courses, please feel free to add to or correct my understanding.

",null,1,cdi3h4h,1qzdx1,askscience,new,8
rupert1920,Check out [some of these past threads](http://www.reddit.com/r/askscience/search?q=water+boil+dries&amp;restrict_sr=on).,null,1,cdi3vz4,1qzdx1,askscience,new,6
NastyEbilPiwate,"Temperature is a measure of average energy. In room temperature water, some molecules will have a lot of energy and some will only have a little. The total energy averages out and the water will be 'room temperature'. Some of those molecules with a lot of energy will end up having enough that they are able to escape from the liquid and float off, having evaporated.

The remaining molecules will absorb energy from the room (the loss of the high-energy molecules reduces the average energy and thus temperature of the water so that energy will flow from the room into the water), and so some of them will be sped up enough that *they* can then escape.

Eventually, all the molecules will have gained enough energy to break free of the remaining liquid, and there'll be nothing left.

Edit: The distribution of energy will look like this: http://i.imgur.com/STMeQDi.png",null,2,cdi3fs7,1qzdx1,askscience,new,4
florinandrei,"The boiling point is not a magical temperature where water just starts to evaporate. Evaporation happens at all temperatures, it's just much stronger when water is boiling.

Things get dry at room temperature because some evaporation happens even at that low temperature.",null,1,cdi5409,1qzdx1,askscience,new,3
sexdrugsandsam,"If you think of temperature, think of it as the average speed the molecules are moving. So if the water is an average of 30 degrees Celsius, that means that there are some molecules that have very little movement, some that move closer to the average and some moving exceptionally faster. The molecules which are moving exceptionally faster (i.e. the temperature of the single molecule would be 100+ degrees Celcius) would themselves 'evaporate' and leave the liquid and taking on the properties of a gas. ",null,1,cdi5nym,1qzdx1,askscience,new,3
rocketsocks,"Evaporation.

OK, I'll try to explain as simply as possible... go:

Temperature is, essentially, average kinetic energy of molecules. In a liquid the molecules will have a range of kinetic energies around that average. Some molecules will have enough energy to move into gas form if they are near the surface. Because this is a molecule with above average KE escaping it lowers the temperature of the remaining liquid.

However, assuming the liquid is in thermal equilibrium it will return to the same temperature as its surroundings, and the evaporation process will continue. This is a ratchet effect, which can cause all of the liquid to evaporate, but how long that takes depends on the amount of liquid, temperature, vapore pressure, etc.",null,1,cdibwmc,1qzdx1,askscience,new,3
MayContainNugat,"When sugar dissolves in water, it isn't melting.

When water dissolves in air, it isn't boiling.",null,9,cdi59z4,1qzdx1,askscience,new,6
The_Serious_Account,"I assume you're talking about a [particle in a box?](http://www.lightandmatter.com/html_books/lm/ch35/figs/particle-in-a-box.png) I don't think there's any good answer as to 'why'. It's a standing wave in a box and that's how the equations of a standing wave fall out. You fix the end points by an infinite potential, so the wave function has to disappear at those points. I don't know what else to tell you. 

edit: Maybe your problem is that you're thinking about it as an actual particle in a box. It's not a particle, it's a wave. If you probe it with a measurement there's a certain probability you'll have an interaction with it. That probability is given by the square of the amplitude which happen to be zero in the middle for the first excited state.",null,3,cdi23wb,1qzfqb,askscience,new,6
null,null,null,1,cdi17ok,1qzfqb,askscience,new,2
Ingolfisntmyrealname,"As other have mentioned, I'm assuming you're talking about the ""infinite square well potential"" or what's also called the ""particle in a box"".

This question is technically the same as asking: ""What's the probability that the particle will be found at any point x0 inside the box?"". The answer to this questions is ""zero probability"". Why? Because technically there're an infinite number of points between x0=0 and x0=a. This applies to every point in the box, not just the middle

What you want to ask, then, is: ""What's the probability that the particle will be found at some interval, x0+dx?"" (where dx is some small interval). When you ask the question like this, you will get a real, nonzero probability. This includes finding the particle ""in a small interval centered around the middle of the box"". This probability is, as you can see from the wave-function (squared), rather low, but it is not zero.",null,0,cdi539t,1qzfqb,askscience,new,1
selfification,"Because you're using an intuitive framework where like dots or shiny spherical balls needs to move from one place to another.  You're using a mental framework where particles are described as items have:
a) an outside and an inside.
b) a center (and possibly spherical symmetry around it).
c) a fixed/rigid shape.
d) equations of motion that consist of continuous translations/rotations.

These are all wrong.  The ""particle"" in a quantum world is so different from the particle you are used to that it no longer does it any justice to call it a particle.  It's called that for historical reasons and because some of its mathematical properties carry over from Newtonian mechanics which dealt with particles.  It would be like trying to describe a iPhone to someone from the 30s and starting off with ""well, it's a phone which...""  No!  That mental model is simply not useful or helpful.

If you saw everything as waves, your sanity has a much better chance of coming through a QM class intact.  When you strum a guitar, it's strange that the 2nd normal mode of vibration of string has a node in the center, but it isn't earth shatteringly confusing because nobody calls that vibration a particle (well...  I guess you could call them phonons if you really want to stretch your credibility).  Electrons are similarly just waves.  They follow the rules of wave mechanics.  They reflect off of potential wells.  They can form superpositions.  They form standing waves or travelling waves depending on the situation involved.  They can spread over large volumes.  They do all the things that your regular intuition about waves tells you they do.  There is an interesting kink known as the measurement problem that you'd still need to surmount, but you'd need to do that whether you think of it as a particle or as a wave, so it's just more sensible to think of it as a wave from the get go.",null,0,cdi94ao,1qzfqb,askscience,new,1
crashd5,"John Hunt, from http://www.ccmr.cornell.edu/education/ask/?quid=1308

Astronomical calculations, measurements of seismic waves and observations (about meteorites and magnetic fields) have led to our understanding of the interior of the Earth.

Once Isaac Newton determined that the density of the Earth was about twice the density of most rocks, he concluded that the interior, which we cannot sample directly, must consist of much denser material.

When a powerful earthquake shakes the Earth at a certain point, some of the energy radiates as seismic waves from that point and these waves can be detected around the planet. If the source and the detector (seismograph) are on opposite sides of the globe then the waves that arrive first at the detector are the ones which travelled through the center of the Earth! By studying the nature of these waves geophysicists have determined that the outer core of the Earth is liquid (because certain types of waves cannot propagate through a liquid) and that the inner core of the Earth is solid (because it allows a different type of seismic wave to propagate through it).

Meteorites are considered to be remnants of material that either did not consolidate into something large enough to be a planet, or are fragments of planets that exploded far away from the Earth. Many meteorites are composed primarily of iron which is a strong piece of evidence about the composition of the Earth's core. The density of Iron is close to 8 grams per cc, whereas most surface rocks are closer to 2.5 grams per cc. This higher density also agrees with Newton's calculations.

In addition, the Earth has a magnetic field which is always present and whose origin must be related to magnetic material inside the planet. Although the solid iron in the inner core is too hot to be magnetic, it is believed that the movement of the molten iron in the outer core is responsible for the magnetic field.

",null,0,cdi3x2n,1qzgs5,askscience,new,12
Platypuskeeper,"Only if the air had the same density as the feather, which would take compressing the air to a liquid state, if not more.
",null,1,cdi2jw6,1qzhot,askscience,new,4
Claclink,"the sonic boom created by a bullet is a strong enough shock wave to kill bacteria.

http://business.highbeam.com/137753/article-1G1-94870619/boom-youre-dead",null,463,cdi9p7l,1qzhu7,askscience,new,1833
ArmyOrtho,"Army orthopaedic surgeon here.  This is a very good question, and one I get asked all the time.

Short answer: no.  Bacteria on the end of a bullet is still infectious in a wound.  You'd think that because it's going fast enough and it gets hot enough that bullets are sterile.  This has been proven time and time again to be false.  

COL Louis A. LaGarde in 1903 performed a study where he took .30 calibre rifle bullets (rifle = high velocity, supersonic), dipped them in anthrax and shot cows.  The cows lived and contracted Anthrax.

I've taken countless numbers of bullets and fragments out of bodies and as a rule, each of them are treated as if they are infected.  The treatment is different for low velocity versus high velocity, but the principle remains = they are all treated as dirty wounds.

Long story short - bullets aren't sterile.

http://archive.org/stream/gunshotinjuriesh00lagauoft/gunshotinjuriesh00lagauoft_djvu.txt",null,59,cdi5772,1qzhu7,askscience,new,436
pthors,"You can transform yeast via microprojectile bombardment.
http://www.ncbi.nlm.nih.gov/pubmed/2836954
Basically, it's a shotgun blast with DNA coated tungsten or gold nanoparticles impacting a paste of yeast cells spread out on an agar containg petri plate.  There's a massive kill zone at the center of the plate where the particles hit, with survivors who receive DNA being away from the center of the blast.  So, at least in that circumstance it's pretty easy to kill a lot of microogranisms with a ""bullet"".  Might not be exactly relevant to the question, but interesting all the same.",null,33,cdi5u7o,1qzhu7,askscience,new,179
CocaineSmellsSoGood,"Former ballistic technician at one of the worlds leading labs here. There are too many variables to be calculated without running a serious simulation. 

1. Millions of organisms and their individual traits to consider. 

2.The projectile speed and design. A superheated area of gases exist around most projectiles in flight, as well as a very strong shockwave coming from the leading edge of the projectile. Possibly keeping anything from coming in contact with it. 

3. Centrifugal force. All bullets spin violently for stabilization in flight. It would take a lot for new airborne organisms to overcome the centrifugal force AFTER penetrating and surviving the shockwave. I would ponder the slower the bullets velocity the better the chances are of it happening. A musket ball out of a muzzleloader traveling 450-850fps? Probably. Anything else traveling faster? Doubtful. An exception may be shotgun loads. Lots of spaces to hide, tons more surface area, and relatively slow speeds in the ballistics world. Edit: words.",null,8,cdi4wur,1qzhu7,askscience,new,61
borthuria,"Many information would be needed : First  what is the pressure around the bullet trajectory : since the bullet is travelling at high speed, there is a high pressure in from of it and low pressure behing it. Something like [this](http://youtruth.weebly.com/uploads/1/3/1/8/1318459/504309298_orig.jpg?164) :

Second, we would need to know the pressure needed to ""break"" the microorganism : depending on the microorganism, they all have their own characteristic and they don't react to pressure differently, since they are airborne, I think we COULD assume they all ""break"" at the same pressure.  (I would need to confirm this from a biologist, since airborne microorganisms could be fungus, bacteria or Viral)

Third, it would depend on the bullet you fire and it's velocity.

It is more a question to ask a biologist then a phycisit, the phycisist in me ask the bioligist this one :

""what would happen if a microorganism would be put in a pressure gradient""",null,14,cdiawrp,1qzhu7,askscience,new,49
edge000,"If we are talking about the temperature of the bullet being the mechanism for microbial inactivation then we are talking about a process similar to pasteurization. 

The answer is definitely yes. The heat generated from a bullet being fired will kill microorganisms. This isn't a terribly interesting question from a microbiology standpoint, it is easy to kill some microorganisms.

The real question you want to ask is how many you are killing. That fact of the matter is that even killing 90% of microorganisms in the trajectory of a bullet isn't terribly effective, due to the rate at which microorganisms can multiply. 

When you talk about inactivating microorganisms, it is measured in terms of logarithmic reduction - 90% =1 log vs. 99.9% = 3 log vs. 99.9999% = 5 log; etc. 

The other concept to keep in mind when we are talking about heat is that any temperature over ~ 50C can kill microorganisms, the question becomes *how long are they exposed to it.* 

Going back to [pasteurization](https://en.wikipedia.org/wiki/Pasteurization) - this was designed to be a 5 log reduction of viable microorganisms in milk. There are different mechanisms, but they basically balance time of exposure and temperature. The lower the temperature, the longer the product needs to be exposed to it, and vice versa. The most common type, [UHT](https://en.wikipedia.org/wiki/Ultra-high_temperature_processing) utilizes 135C for about 1-2 seconds. 

I looked around some to try and find the temperature of fired bullets and landed on what amounts to an [advertisement for thermal camera.](http://www.advancedimagingpro.com/print/Advanced-Imaging-Magazine/Infrared-Camera-Measures-Bullet-Heating/1$180). The image they had showed temperatures ranging from 170C to &gt;500C at different points on bullet. To determine what extent organisms are destroyed you need to know the organism, the temperature and how long it is exposed to it. 

If I had to guess for the length of time a bullet is in the air, 170C may not achieve 5 log reductions whereas the 500C probably would, at least. This doesn't take into account the bullet cooling along its trajectory. ",null,1,cdi4w47,1qzhu7,askscience,new,29
realised,"I cannot locate any published sources regarding microorganisms in the bullet's trajectory but regarding microorganisms present on the bullet itself can be found [here](http://www.ncbi.nlm.nih.gov/pubmed/621766).

It only looks at one specific type of microorganism as well as only low-velocity bullets (unsure how they differ as not a gun person myself).  So, other microorganisms may be impacted differently.",null,7,cdi4lcu,1qzhu7,askscience,new,18
null,null,null,29,cdi8mks,1qzhu7,askscience,new,36
Yannnn,"Your question is akin to 'Do human bullets kill people'. The answer is yes, but not always.. it depends. My educated guess would be that larger organisms will break apart from the shock. But smaller bacteria will probably be rather untouched: the bullet will go too fast to properly transmit any heat and their small bodies will have relatively little mechanic stress.

It may surprise you, but guns are actually used in genetic engineering. [Have a look here for a 'Gene gun'.](http://www.youtube.com/watch?feature=player_detailpage&amp;v=8kS5TOLRKdc#t=44) If my memory serves me correctly, a large percentage of organisms die during the bombardment, approximately 50~70%. And of the surviving cells only a small percentage have the 'new' gene, approximately 0.1%. But this usually is more than enough.

edit: If you down vote me it would be nice to know why. I'd like not to make the same mistake twice. Thanks!",null,7,cdiiycv,1qzhu7,askscience,new,10
locosenor,"Odds are that bullets under the speed of sound, the micro-organism will simply be pushed away by the force of the bullet just like a small bird being pushed out the way by the force of a truck. The reasoning behind that would be that when something cuts through air like a car or bullet there is still a tiny layer of air around the tip http://i.dailymail.co.uk/i/pix/2009/03/26/article-0-041778A6000005DC-509_960x564.jpg (best exemplified by that).
That small layer of air will usually push substance away from the speeding organism or being (due to the low speed of the object) before it actually hits the speeding object often saving it's life. 
But if the bullet would break the speed of sound (guns that have a fifty-caliber bullet like a Barret M107) the micro-organism will be killed. The shock alone would kill it and that little bubble won't be enough to stop it as the speeding bullet would be too fast.

Edit: Forgot to mention that the heat caused by air resistance and the gunpowder igniting the bullet would instantly sterilize the bullet (killing everything in it's way)",null,1,cdi6e4u,1qzhu7,askscience,new,5
shaffer620,"OK, so the bullet can kill the microbe if it can create a sonic boom. The bullet can also kill the microbe if it heats up warm enough and allows for a long enough exposure to transfer the heat. However, I believe the main question that activeNeuron wants answered is if the bullet can kill the microbe just by impact. like if the bullet was shoot and while traveling through the air it hit a bunch a microbes. Would that be enough to kill it? for that question I Have no idea.",null,5,cdiftr2,1qzhu7,askscience,new,6
knobtwiddler,"this can only be answered properly by experimentation. because it depends on the type of microorganism (virus, bacteiria, amoeba?), and where in the bullet trajectory the organism finds itself.

... but, it's going to end up being some proportion killed, damaged, and unharmed based on proximity to the muzzle, characteristics of the gun and bullet that might heat it up; many factors that will determine whether an individual microbe would survive or not.  

At the end of the trajectory, the bullet velocity will be a fraction of the muzzle velocity, and the bullet may have cooled down, so some microbes might survive any surface heating from being stuck in the microscopic texture of the bullet, or the pressure wave from the imact.  

As a thought experiment, imagine a wall traveling 3000 feet per second (approximate muzzle velocity of a rifle round) smashes into an amoeba.  Amoeba accelerates from 0-3000 fps instantly, squeezing the water out the side and rupturing its membrane, while heating it. That could kill a single cell, depending on the velocity and temperature of the round, and what type of microorganism.  

With a virus, it's hard to say. They involve less complex machinery that might be more impact resistant than larger water-filled cells.  

I wrote this all without reading prior comments. I'm sure some physicists or biochemists can give a better explanation than me but anyway... very interesting question. 

tl;dr  yes",null,0,cdir3ng,1qzhu7,askscience,new,2
Ph30n1x5000,"Theoretically it can only move or change them.  I'm sure you already know matter can't be created or destroyed, therefore it would push these so-called microorganisms out of its way and/or make some sort of molecular bond thus changing the atmosphere around it.",null,0,cdiosyi,1qzhu7,askscience,new,2
emodestroyer,"I once loaded a carpenter ant inside a 7.62x54r shell, the pressure and powder flash alone probably disintegrated it, we found no trace of bug guts or remains of the bug. If something is inside your bore (like a bug) and a bullet comes out travelling between 1000-2800fps (depends on caliber) it would be microscopically crushed and reduced to its smallest particles, unless it obstructs the bore with enough mass to interrupt the bullets path through the rifling. Basically inside the rifle is a combustion chamber until the bullet exits toward its target.",null,0,cdis4p9,1qzhu7,askscience,new,1
Saint_Oliver,It seems to me that most of the posts on this thread deal with microorganisms that are already on the bullet when it is fired or are on the target. What about MO's that are in the path of the bullet? i.e. what is the effect of a large impulse on a microorganism? Would fluid dynamics simply move the MOs out of the path of the bullet as it flies by?,null,0,cdisi7f,1qzhu7,askscience,new,1
3rdopinion,"Emergency doctor here, and while I don't have a particularly advanced understanding of ballistics, this is an important concept in traumatic .  Large caliber bullets (I'm actually not sure precisely at what point a bullet is considered large caliber), are significantly less likely to cause an infection than smaller ones, though any projectile increases the risk. The majority of infections caused by bullets tend to be from organisms usually present on the skin, suggesting that the bullet has dragged in small bits of skin and superficial tissue. Experiments have demonstrated that, when applied to a bullet prior to firing into live tissue, certain bacteria have a propensity to cause infections. Bacteria that form spores are more likely to remain viable after firing. ",null,0,cdj98ua,1qzhu7,askscience,new,1
TheGloriousHole,"Think about the difference in impact between punching a fly in midair and punching a standing person.

Bacteria has an extremely small inertial mass. It's an extremely oversimplified guess but I would assume based on that, that no it wouldn't kill them.

To accurately work that out, there are a lot more calculations you'd have to do but you know... Guessing is easier.",null,0,cdi3rj7,1qzhu7,askscience,new,1
patchgrabber,"From my experience with bacteria and phages, you are describing a lysogenic infection. The lambda phage is an example of a phage that inserts its DNA into *E. Coli* but does not kill the cell because of a repressor gene that keeps the phage from hijacking the bacterium's machinery.

The interesting part of some viruses is that they have an enzyme called a [site-specific recombinase](http://en.wikipedia.org/wiki/Site-specific_recombination), which targets a specific DNA sequence 20-300 bp in length. This recombination isn't random, so no important genes are affected. If it were a cell in a human body, and there were a virus that inserted itself into an important gene, the cell would likely just lyse, since it cannot remain viable. This insertion could also trigger oncogenes and cause cancer, so there are a variety of possibilities depending on which gene is disrupted. HIV inserts its DNA randomly, but acts much like a lysogenic bacteriophage otherwise.",null,1,cdi2rq8,1qzhuk,askscience,new,6
snusmumrikan,"Many viruses only transiently express their DNA instead of integrating into the host genome, and these are often used as vectors to shuttle DNA into cells by molecular biologists. It might seem like an inferior system for the virus as the DNA doesn't last as long as it would if properly integrated, but on the other hand it runs no risk of inserting into a vital gene region as you so rightly point out, so it has benefits and disadvantages (not that I'm saying viruses have thought about this).

As a virus which kills its host is a stupid and short-lived virus and as such they generally do not 'want' to kill off the cells they enter. Many rely on specific conserved sites, such as the AAVS1 site used by commonly used adenoviruses (PNAS. 1990 vol. 87 no. 6 2211-2215), which do not kill or inhibit the growth of the host cell. This helps maintain the propagation of the virus DNA and the insertion itself is not disease-causing.

Other viruses do cause disease by insertion but cause rapid cancerous growth, a method of quickly propagating the amount of viral DNA. These oncoviruses insert oncogenes which tend to push the cell into a proliferative stage of the cell cycle. For example HPV viruses, which you will know from medical advice can cause cancers, insert proteins E6 and E7 which inhibit tumour supressing host genes (Cell. 1990 Dec 21;63(6):1129-36.)

Other viruses will insert randomly and may or may not cause damage to the cell. To bring it back to your question, viruses often interfere with genes important to the host (and this includes inserting or disrupting the flanking regions which may be responsible for repression/promotion or other regulation of the gene), the thing you want to look for is whether this is beneficial for the propagation of new virus particles or not.",null,0,cdi5qxp,1qzhuk,askscience,new,3
sfoglia301,"Well, the best human example that I can think of is Hepatitis B Virus.  In this virus it is possible to get liver cancer without any liver damage/cirrhosis.  This is because HBV is a DNA virus that integrates into the genome.  If it integrates into an important tumor suppressor, boom, loss of function and you get liver cancer.  ",null,0,cdi90my,1qzhuk,askscience,new,2
gettingoldernotwiser,"Lots of examples of genome insertions which are non-lethal (to the host cell).  Virus-induced cancer is an example of a mutation which actually gives the host cell an advantage in replication.  Good for the host cell and the inserted viral genome, though bad for the organism in general.

If a viral genome were to have evolved which inserted itself into a gene which is REQUIRED for host cell survival, then the host would die and the viral DNA would not be propagated.  Not a good survival strategy for the virus, so it's not seen in nature.",null,0,cdiezu6,1qzhuk,askscience,new,2
bwc6,"&gt; Does the E coli just die?

Yes, that cell would probably die from the lack of mRNA.

&gt; What if it was a human instead of an E coli?

The individual cell that was infected would die. This would probably prevent the virus from spreading throughout the person. Viruses can cause problems for people if they interrupt important genes. Others in the thread have mentioned cancer as a good example.

&gt; how do virus genes pick where they're inserted into the host genome?

There is a ton of variation here based on the type of virus. For some viruses the insertion site is random. Others have very specific sites depending on site-specific recombinase, mentioned by patchgrabber. ",null,0,cdj4zsn,1qzhuk,askscience,new,1
SimpleBen,"On the fingertip, you can sense the orthogonal displacement of your skin with a detection threshold of roughly 25 microns at 0.5 Hz (a 2 second smooth displacement). At 60 Hz  for a half second that threshold is under 10 microns, and it drops to 1-2 microns at 250-300 Hz.   
  
For movement parallel to the surface of the skin, stimuli a few orders of magnitude smaller (around 10 nm) can be detected simply because they drag the skin from side to side.    
http://jn.physiology.org/content/81/4/1548.short  
  
Of course, based on this 1999 study published in the Journal of Neurophysiology, scientists created another study to demonstrate the same level of sensitivity and published their results in Nature. And the reviewers must have been very ignorant not to notice this.",null,0,cdi6z76,1qzjrl,askscience,new,13
Mushucanbar,A very good article was published recently in Nature on the topic of the limit of our [tactile perception](http://www.nature.com/srep/2013/130912/srep02617/full/srep02617.html). Interestingly it was found that our sense of touch is sufficiently sensitive to detect nanoscale surface features as small as 10nm. ,null,0,cdim5u0,1qzjrl,askscience,new,2
yeast_problem,"I was thinking that the rain drop phenomenon was a perception of heat energy, i.e the small drops rapidly cooled a spot of skin and you are detecting the heat loss not the physical size of the object.

So I went to google smallest temperature difference skin can perceive and accidentally found [this](http://psycserver.psyc.queensu.ca/lederman/013.pdf) research showing how skin temperature affects the perception of roughness.",null,0,cdinthx,1qzjrl,askscience,new,2
null,null,null,2,cdi5cew,1qzjrl,askscience,new,2
mousicle,"The chance of this happening are pretty slim.  Most of the man made material in space is not relatively far from the Earth.  Geostationary orbit, the farthest you would practically place a satelite, is only a little more then 10% the orbit of the moon.  At those distances the gravity of the Earth dominates the gravitational attraction of the debris by orders of magnitude.  So the chances of all those bits mutally attracting and forming one body before they are pulled back to the Earth is virtually zero. ",null,0,cdi7bdh,1qzp06,askscience,new,3
Surf_Science,"1) Race is rather arbitrary, irrelevant, and somewhat silly. There are no hard lines between human ""races"" everything very much exists along a continuum. With some populations (which again will have muddled borders) with higher levels of homogeny (people are more genetically similar) you may see elevated levels of some genetic disease. This is particularly common with founder populations where a small group of people are the genetic ancestors of a large group (example 8 million people in Quebec are largely the progeny of 3,000 people). 

2) With respect to immunological diseases the answer may cut more ways. Higher levels of diversity may allow the MHC molecules of the individuals to respond to a higher diversity of antigens. Conversely infectious disease have a tendency to have reduced virulence in the populations they are derived from. Because of this it is possible that some individuals of a mix race background could loose the benefit of this attenuated virulence. With respect to this attenuation I believe that TB is a good example with Africans for example having reduced outcomes when infected with Asian derived TB. ",null,6,cdi6qrb,1qzqit,askscience,new,14
heresacorrection,"I would say **yes**. The more diverse an organism's background the more likely they are to be heterozygous for a mutation in any given gene or regulatory region. Knowing this and the fact that most genetically inherited diseases, both Mendelian and non-Mendelian (that we know of), are recessive rather than dominant would lead us to the conclusion that more diversity (assuming that is what differentiates between races) decreases one's chance of having those diseases. 

This would be further supported by the many studies showing that out-crossing helps avoid genetic bottlenecks and can help negate a loss of variation in a population (e.g. due to a founder effect).

EDIT: For this question to further define the diversity of having ""different"" races, I would push for a definition where we might say that different haplotypes could be thought of as derived from different races. I agree with *homininet* (below) who suggested we use ""ethnicity"" instead.",null,0,cdine0v,1qzqit,askscience,new,3
owaisofspades,"Are you asking whether mixed-descent people are more likely to have genetic disease, or whether they are more immune to certain disease?  The answer is different for each.

Surf_Science mentioned founder effects in Quebec, and founder effects are important to your question. For example, it's widely known that Ashekenazi Jewish populations have a higher prevalence of many genetic diseases because their population has alot of carriers for lots of different mutant alleles. So theoretically, if you are of half-ashekenazi jewish descent and half caucasian, you would most likely have less mutant alleles through inheritance than someone who is of full ashekenazi jewish descent and therefore you could say that half jewish half caucasian populations would theoretically have less prevalence of genetic disease than 100% ashekenazi jewish populations (but still more than 100% caucasian populations)

As far as resistance is concerned, the opposite would be the case. Two examples of alleles that increase resistance to malaria are sickle cell trait (common in africa) and G6PD (common in africa, middle east, and south asia). In this case if you are mixed african-south asian, you're more likely to carry one allele each of two different genes which both give you malaria resistance than someone who is just south-asian, but this increased resistance to malaria would come with the complications that go along with being a carrier for sickle cell. and just FYI having two alleles of G6PD and/or sickle cell is bad (although sickle cell is much worse)

hope this helps!",null,0,cdi8nrl,1qzqit,askscience,new,2
ModernTarantula,"the question can be separated. First race has to eliminated, rather refer to populations. Next genetically related diseases are not the same as immunity. Lastly hardiness in plants is not the same as immunity. So genetic disease are present much more in specific populations--thalassemia, Tay Sachs, Factor V Leiden, cystic fibrosis--plenty more. Immunity is more complex, there are specific inherited immune diseases (boy in the bubble). But subtleties in immunity is not a hard science.",null,0,cdio3cx,1qzqit,askscience,new,1
hikaruzero,"I don't think the discovery of the Huge-LQG has casted very much *doubt* on the cosmological principle, but it *has* called it into question for further analysis.

The identification of any structure large enough to violate the principle is definitely troubling, but the existance of such a large structure is not *impossible* assuming the cosmological principle, it is just very unlikely.  We could be looking at an anomaly that is just not very probable -- after all there's an entire observable universe for improbable things to occur in -- but we should also keep in mind that the Huge-LQG dataset is quite small overall (73 quasars).

The Arxiv.org paper you linked to (from which your second quote is from) also does indeed show that (paraphrased) the algorithm used to identify the Huge-LQG regularly produces false positives for identifying large structures, and that the Huge-LQG may actually be two or more smaller structures nearby, or may not even really be a ""structure"" at all.

It's definitely still too early to throw the cosmological principle out, considering the wealth of data that supports it.  Just because one counter-example is found doesn't mean the statistics are necessarily wrong.  What we'd really need are other independent observations that also show a violation of the cosmological principle.

Hope that helps.",null,0,cdi5yib,1qzqjp,askscience,new,2
snusmumrikan,"Tanning hasn't changed your phenotype because it has nothing do do with the alleles in your genes. It is an environmental change, just as with the giraffe an abundance of nutrients is more likely to be responsible for the growth spurt than anything else, nothing in the genes or allelic ratios has changed and therefore the 'phenotype' has not changed. Phenotype is the visible part of gene expression, it doesn't encompass 'absolutely everything you see'. A tattoo has not changed your phenotype. 

What you're hinting at is called Lemarckian evolution which was proposed by Jean-Baptiste Lemarck in the 18th century. He wondered if, say, chopping off a leg of an animal would lead to three-legged offspring. Obviously geneticists now see that as obviously wrong due to the nature of hereditary gene transfer to offspring. 

More recently this idea has been recalled with respect to epigenetics: the hereditary transfer of genetic activity which is not associated with the actual DNA sequence itself (you can inherit differently packaged/masked/expressed DNA which may be expressed in a different manner to someone else with the identical DNA sequence).

As a potentially interesting aside you might also want to look at DNA polymerase V. This DNA pol is part of the DNA SOS response (for double-stranded damage) and as a last resort may ignore damaged or slipped sections of DNA, essentially excising them and re-ligating the pieces at either end - which if carried through into the gametes would result in a heritable lack of removed DNA sections, a molecular version of exactly what Lemarck predicted over 200 years ago.

(Edited for spelling mistakes)",null,0,cdi4xmq,1qzqrc,askscience,new,6
KarlOskar12,"Let's take the giraffe example...If a group of giraffes lives in an area with a food source found very high off the ground only the giraffes with very long necks (which will only be giraffes with very long neck genotypes) will survive. They will have babies who also possess the very long neck genotype and over time the average neck length of giraffes in that area will increase. The variation in genotype doesn't come from one individual giraffe, but the genetic variation found amongst various giraffes.",null,1,cdi4605,1qzqrc,askscience,new,6
bwc6,Random variation combined with natural selection. [source](http://www.literature.org/authors/darwin-charles/the-origin-of-species/),null,0,cdj5qtt,1qzqrc,askscience,new,3
atomfullerene,"Basically, the stretching or tanning or what have you doesn't directly result in new adaptations.  Instead, think about the giraffe egg which mutates, eventually producing a giraffe with a 2cm longer neck.  That giraffe goes throughout life getting a bit of extra food, has a few extra babies (some of which have 2cm longer necks).  The longer necks result in more babies on average.  Rinse and repeat until all the giraffes have 2cm longer necks....and then wait for another mutation to occur.",null,0,cdi94w9,1qzqrc,askscience,new,2
darkness1685,"Natural selection can be viewed as a sorting mechanism. The individuals who have a genotype that results in the most well-adapted phenotype will have greater reproductive success (i.e., produce more offspring). Therefore, the favorable genotype will become more common in the population, and the less favorable genotype will be sorted out. Phenotype cannot influence the genotype of a particular organism, but it can influence the average genotype of the population as a whole through time. Avoid Lamarckianism! ",null,0,cdiabj2,1qzqrc,askscience,new,1
owaisofspades,"oh did you mean heartbreak as in getting dumped and/or emotional shock? My bad, I thought you meant a heart attack. In this case it might be Prinzmetal's angina, which is a stress-induced chest pain (similar to heart attack pain). This happens because stress can cause your coronary artery to spasm, causing ischemia (loss of blood flow) which induces pain.

As far as the stomach is concerned, the shock of having our heart broken could potentially overstimulate your vagus nerve causing a drop in blood pressure (which  can give you that sick in your stomach like your going to puke feeling) Not 100% sure is this is what you meant by stomach pain though",null,2,cdi9qws,1qzriw,askscience,new,4
KarlOskar12,"The discomfort is a result of stressing the Parasympathetic nervous system (PSNS). During high emotional stress the PSNS can get over-activated causing pain in the chest cavity and stomach. The PSNS is responsible for the digestive processes so it innervates the esophagus, stomach, intestines, etc.",null,0,cdixgnx,1qzriw,askscience,new,2
owaisofspades,"Your heart is innervated by the phrenic nerve, which has the spinal roots C3-C5. C5-T1 are the roots for cutaneous innervation of your upper limb. Because of the overlap in nerve roots, the pain in your heart radiates to other parts of your body innervated by the same nerve roots, in this case your pectoral region and arm",null,2,cdi9b5z,1qzriw,askscience,new,3
sfoglia301,"This would be ""referred pain"".  This is because the nerves that innervate the heart are ""visceral nerves"" and are not very precise as are sensory nerves in the skin . As a result, if your heart is ""hurting"" in a heart attack, you feel visceral pain throughout the dermatome (level of the nerve) where the heart's nerves arise in the spinal column.  In humans, that is in the chest and left arm.  ",null,3,cdi9251,1qzriw,askscience,new,2
iorgfeflkd,NGC 1277,null,1,cdi5527,1qzsxi,askscience,new,4
botanist2,"Yes.  There are seasonal differences in oxygen production in the Northern and Southern Hemispheres because plants are dormant in the winter, but the extent of the fluctuations depend on scale.  At a local scale there might be a lot more seasonal variation than there is on a global scale, simply because there's still a lot of ocean and tropical areas where photosynthesis is still occurring, and that reduces the impact of dormancy in other regions. 

Edit: One of the most cited papers involving global variation in O2 and CO2 concentrations in the paper done by [Keeling and Schertz](http://132.239.121.69/publications/ralph/3_Seasonal.pdf) from 1992 that I thought you might find interesting",null,0,cdi5znl,1qzto7,askscience,new,6
darkness1685,"Absolutely. If you look closely at the famous CO2 graph from Mauna Loa, you can see atmospheric concentrations fluctuate on an annual cycle. This is due to global vegetation being more productive in warm seasons than cold. The same is true for Oxygen, but in the opposite manner of course.
http://www.skepticalscience.com/images/CO2_vs_oxygen.gif",null,1,cdia3nc,1qzto7,askscience,new,6
Blah2435,"While the amount of oxygen does drop a bit, around 70-80% of oxygen comes from plankton, algae, and other things that thrive in the ocean. So although it does change the amount of oxygen some, it definitely is not the biggest producer of oxygen so it doesn't effect the amount as much as you would think.",null,0,cdin7cd,1qzto7,askscience,new,2
threegigs,"Big reason number 1: Titanium implants aren't rejected by the body.

2: A high strength to weight ratio, with ~~low~~ [edit] **high** modulus (not too flexible).

3: Not corroded or degraded by bodily fluids.

4: Requires no maintenance (fit and forget).

5: Osseointegration (will readily join with bone tissue).",null,3,cdi890y,1qzwpq,askscience,new,10
SneakerofSneaks,"It's a strong metal the resists corrosion very well.  It is slow to react with water and air.  It's generally non-reactive when it's temperature is in the range of the human body.  (Although it's considered highly reactive in the thousands of degrees range, the human body of course does not operate at those temperatures).  It is easily molded, plated and coated with as well.",null,3,cdi73qu,1qzwpq,askscience,new,2
nimobo,"Bulb flickering occurs due to either faulty connections or voltage fluctuations. Storms cause faults in the circuit resulting in power surges and fluctuations. This is why you see flickering. The bulb does not go out unless the circuit is broken, commonly happens when trees fall on the power lines, or the bulb itself burns out.",null,0,cdif7su,1qzx32,askscience,new,9
ScintillatingWit,"The effectiveness of silver compounds as an antiseptic is based on the ability of the biologically active silver ion (Ag+
) to irreversibly damage key enzyme systems in the cell membranes of pathogens. http://www.ncbi.nlm.nih.gov/pubmed/16766878",null,0,cdi71rf,1r00tq,askscience,new,8
SneakerofSneaks,"The highly reactive nature of the silver ion.

Because of the outer shell configuration, it reacts with most organic compounds.  It also has the ability to form numerous inorganic and organic compounds.  Because of this, it's toxic to bacteria, yeasts, and at larger quantities is toxic to us.",null,2,cdi7d2m,1r00tq,askscience,new,6
medikit,"Silver also appears to make antibiotics more effective:

http://www.nature.com/news/silver-makes-antibiotics-thousands-of-times-more-effective-1.13232
http://stm.sciencemag.org/content/5/190/190ra81",null,1,cdjr25i,1r00tq,askscience,new,3
chocolate_powder,The tape on the inside is magnetic. By altering the tape's magnetization it will produce an electromagnetic pulse when moving quickly past a playback head. The cassette will either play the music or rearrange the particles to reflect the music on the tape to record based on the orientation of the playback head.,null,1,cdieuny,1r028e,askscience,new,3
afcagroo,"/u/chocolate_powder answered this, but I found his wording a bit difficult to decipher. So here it is again, worded differently: 
 
The tape in an audio cassette is coated with a material which is susceptible to magnetization.  To record on such a tape, it is moved over a write head which contains an inductor (coil). The inductor creates a changing magnetic field as the electrical signal is passed through it, and that changing signal is recorded on the tape as it moves over the head. The coating becomes magnetized, with the direction and strength of the magnetism being an analog representation of the audio signal that was run through the write head's inductor.  (The audio signal could have come from a microphone, or a guitar pickup, or whatever.)
  
The playback process is the opposite. As the tape moves over a read inductor (coil), the varying magnetic orientations/strengths induce an electrical current.  That current is amplified and sent on its way, to be played back as an audio stream through a speaker. 
   
Note that a cassette recorder can have two separate heads, one for writing and one for reading, or they can be combined together.     ",null,0,cdj7lpo,1r028e,askscience,new,1
justin3003,"Two major elements allow for macrophage resistance. 

* Cell wall: The properties of mycobacterial cell walls make them very hearty organisms generally. Mycobacteria have mycolic acids in their cell walls, which make them resistant to many different types of degradation/destruction, including macrophages (by being resistant to lysosomal acidification and enzyme degradation) and many antibiotics. 
* Anti-ROS enzymes: Many mycobacteria, including M. tuberculosis, produce catalase (which turns hydrogen peroxide into water and oxygen) and superoxide dismutase (converts the superoxide radical (O2-) into hydrogen peroxide and oxygen). Without reactive oxygen species (ROS) activity, macrophages have few weapons left to use.",null,0,cdibm0j,1r02eg,askscience,new,8
jyaron,"In short, Mtb is capable of hijacking the normal metabolism of its host macrophage/dendritic cell towards an anti-apoptotic phenotype which will prevent the cell from performing a pro-inflammatory cell death to inform neutrophils to recruit to the area and kill of the macrophages. Additionally, it hijacks the important iron pools of the host cell which it needs to proliferate by secreting mycobactins through the arrested phagolysosomal compartments.

Essentially, the Mtb cells will disrupt the host phagocyte from performing the normal destructive processes by changing the basic physiology of the cell.

Reference: [Urdahl K, Shafiani S, Ernst J. Initiation and regulation of T-cell responses in tuberculosis. Mucosal immunology. 2011;4(3):288–93.](http://www.nature.com/mi/journal/v4/n3/full/mi201110a.html)",null,0,cdprroo,1r02eg,askscience,new,2
gabesubdo,"All network devices, including wireless routers, have a unique ID called a MAC address. The MAC address of an access point is one of the things that gets broadcasted out in the open, so you can see the MAC addresses of all nearby access points even if you're not connected to any of them.

Several companies have, through different means, mapped out the geographical coordinates of as many access points as they can. For example, Google used to have a device in their Street View cars that would also record the addresses of all the access points they drove past (they might still do this, I'm not sure).

So when your iPhone has WiFi turned on, it can make a list of the access points nearby, send those MAC addresses to Apple (or another company providing this service), and get an approximate location back, if the company has seen one of those access points before. Your phone might even cache some of this data so it can do the lookup without sending data to Apple.

It's not the most accurate way of getting a location, but it's another method the phone can use if GPS isn't available. Otherwise, you'd have to go by the closest cell tower, which is very inaccurate.",null,0,cdibbne,1r05xd,askscience,new,3
lamboleap,"Cell towers are probably spread farther apart than the wireless access points at your school. Because there is greater distance between cell towers, your phone probably estimates the distance to your destination based on the closest tower (strongest signal). Now apply that idea to your school's routers. Because the internet at your school is affiliated with a specific location (your school), it can give you a closer estimate to the location on your map than the triangulated cell signal.",null,4,cdib51e,1r05xd,askscience,new,2
xavier_505,"SMS message are effectively sent as a individual 'transaction' which requires paging, channel setup and message transmission in both directions. This is done in a similar way to how phone calls are connected, and it takes time to perform. 3G and 4G Internet connections are either always 'connected' (eg: UMTS) or are in low-latency standby modes. Additionally, SMS messages are not necessarily treated with great urgency by baseband processors or applications since the protocol was never intended to provide low-latency.

2G packet-switched data (eg: GPRS) also takes a bit of time to setup if they are in a disconnected state, though only one end (the user) has to do this channel setup. The network's GGSN is connected to the rest of the internet via low-latency means.",null,1,cdimzfa,1r0725,askscience,new,4
amnesiajune,"In short (well... as short as I can make it): SMS has to travel directly from you to the recipient through protocols that were defined and standardized in the 1980s (GSM and CDMA). These protocols can't be updated because they have to support any 2G cell phone. Any online message goes through a server at Facebook/Apple/BlackBerry/etc.. The way their messages are sent aren't standardized, so they can upgrade their servers and the apps that you and the recipient use to send/receive messages.

Even though your online message has to go through a third party, technology has advanced so far beyond what was conceivable in the 1980s that it's faster to go through a third party server than to go directly between phones using technology that was developed in the 80s and 90s",null,1,cdibc73,1r0725,askscience,new,2
homininet,"Sure, particularly in primates and great apes. There are several instances in which human viruses and diseases have been passed to chimpanzees or gorillas from humans, particularly researchers and ecotourists. These include things like respiratory viruses, polio, a disease called yaws, and potentially ebola. Many of the field sites where primatologists study gorillas and chimpanzees require masks whenever people get close to the animals to try to reduce the risk of spreading human-bourne diseases (check out the picture on this webpage: http://www.conservenature.org/learn_about_wildlife/chimpanzees/chimp_human_disease.htm)

Heres a link to a news article about a recently published study of this very issue: http://www.livescience.com/9565-human-viruses-kill-great-apes.html

",null,1,cdidd04,1r077s,askscience,new,7
abstrusey,"Veterinarian checking in: we call this ""reverse zoonosis"" or, less commonly, ""anthroponosis"". Just for clarity's sake, the term zoonosis should be sufficient, as it means transmission *between* humans and non-human creatures, without indicating directionality; however, the phrase has stuck.  Reverse zoonosis is incredibly common with a large number and variety of diseases. The purest examples come from infectious agents that do not cause disease in humans but do cause disease in animals; for example, there is a fungus called Batrachochytrium dendrobatidis that humans (who carry the agent without any disease symptoms) can transmit to frogs. The fungus is often deadly to the frog if no treatment is provided.

A more common example is found in the 2009 outbreak of the H1N1 influenza virus, poorly nicknamed ""swine flu"". It has had at least 13 confirmed case of reverse zoonosis to cats, 1 confirmed case to a dog, and a number of suspect cases to ferrets (between 2009-2012), according to the Veterinary Pathology journal. The actual number of cases was likely many fold higher. There were also many healthy pigs who caught H1N1 from a human.

Primates tend to be at a high risk of catching an infectious agent from humans, because of our similarities. Metapneumovirus is a common cause of human respiratory tract infections, and it has been implicated as a fatal respiratory disease in chimpanzees in captivity and the wild. Other examples can be devastatingly fatal, such as the Ebola virus.

There are many more examples, including: tuberculosis in elephants, the human gut microbe Serratia marcescens which causes white pox disease in elkhorn coral, Methicillin-resistant Staphylococcus aureus can be shared with household pets, Helicobacter pylori historically and bafflingly wiped out many research colonies of stripe-faced Dunnarts, herpes simplex virus affects marmosets and tamarins, human mumps causes canine parotiditis, human diptheria causes mastitis in cows, hepatitis can be passed to non-human primates... the list goes on, and on, and on...

As a fun fact to end it all: approximately 60% of all human pathogens are zoonotic, and greater than 75% of all new and emerging infectious diseases are zoonotic.",null,0,cdijjom,1r077s,askscience,new,5
FizixPhun,"http://en.wikipedia.org/wiki/File:CNTnames.png

Basically, it gives you the direction in which you cut the graphene to get the end of the nanotube.  Think about m/n as a slope.  In Cartesian coordinates I can represent the slope in a similar way, (a,b).  This would mean that for every a you move along the x axis, move b along the y axis.  For instance, (1,2) would mean that if I moved x by 3, I would need to move my y by 6.  The only difference with the carbon nanotube notation is that the lattice vectors a1 and a2, the equivalent of the x and y axis, are not perpendicular.  This is illustrated nicely in the wikipedia picture.",null,0,cdif596,1r07ep,askscience,new,2
armour_de,"The indices n and m define a direction along the graphene sheet that the carbon nanotube could be considered to be rolled along.

This is done similar to how a cartesian vector can be written as (3,4) or v=3x+4y where x and y are the unit vectors defining the direction of the x and y axis.

The hexagonal structure of graphene has a natural set of directions related to the points on one of the hexagons in the perfect lattice.  [This](http://en.wikipedia.org/wiki/File:CNTnames.png) image from wikipedia shows a_1 and a_2 in the upper right.

The (n,m) values for a carbon nanotobue then makes a vector v=na_1 + ma_2 .

This choice of representation is used as it allows the indices n and m to be integers. Cartesian vectors (n^' ,m^' ) could be used to define identical angles but then n^' and m^' would rarely be integers.

The [carbon nanotube article](http://en.wikipedia.org/wiki/Carbon_nanotube) wikipedia has some more details on this near the start of the single walled section.",null,0,cdifqjh,1r07ep,askscience,new,1
Platypuskeeper,"Salt water is denser. As for how that affects buoyancy, that should be easy to figure out through [Archimedes' Principle](http://en.wikipedia.org/wiki/Archimedes%27_principle).
",null,2,cdib9j1,1r084w,askscience,new,6
Gargatua13013,"An subset of this problem is central to the study of the genesis of ore deposits associated to black smokers, namely what happens to the buoyancy of waters of grossly different salinities and temperatures when they interact. There is a seminal paper on this by Takeo Sato, (1972), ""*Behaviours of ore-forming solutions in seawater*"", Mining geology, **22**, 31-42.

Pressure is also an important variable. Turns out in the right circumstances, the brine plume will be denser than seawater and collapse to the seafloor where it may form a brine pool, which was observed in nature years after Sato's paper.",null,0,cdifh1g,1r084w,askscience,new,1
Weed_O_Whirler,"No. Instantaneous acceleration would require infinite force, and infinite force would cause all sorts of bad things to happen. 

So, what is actually happening? Well, you (and really, everyone) is a little squishy, so when you first get hit by the bus you slowly start to move, but moreso you start to deform- getting squished. Thus, you will ""slowly"" (slow being relative here, as it will still happen in a fraction of a second) move up to the speed of the bus. 

In fact, this speed is what matters. That is why if you were surrounded by a squishy ball, getting hit by a bus would not be as problematic, since that time it took you to speed up to the speed of the bus would be longer (and thus, you acceleration smaller)",null,0,cdiablh,1r086u,askscience,new,7
matts2,"You are a compressible being. So the force travels through you at different rates. Think of this: you are bruised on the side you are hit, not all through your body. What happens is that your are hit and there is force transference. Some of that force it too much for the local bonds and they break (broken blood vessels, broken bones, etc.) so moves though you and accelerates your body.",null,0,cdibs6x,1r086u,askscience,new,3
nonchalantkiwi,"No, not instantaneously. Someone getting hit by a bus is a great example of how Impulse works. Impulse is the force exerted on a particle (in this case, the bus exerting a force on a person) for a certain amount of time. Now with this collision, the time will be very small, but not instantaneous because if it were instantaneous then the impulse would be infinite, which is impossible.  ",null,0,cdiadfm,1r086u,askscience,new,2
diazona,"It's not really so accurate to say microwaves have less energy than visible light. You can have any amount of energy in microwave radiation, just like you can have any amount in visible light. It just comes in smaller increments (i.e. photons) when you use microwaves.

In a nutshell, visible light doesn't work because it gets reflected, whereas microwaves tend to make it into the interior of the food. (Of course that's only the simplest version of the story.) See [here](http://en.wikipedia.org/wiki/Dielectric_heating) for more information.",null,1,cdigaqm,1r091o,askscience,new,9
baloo_the_bear,"Microwaves interact with water molecules and cause them to vibrate. On a molecular scale, vibration **is** heat energy. The heat from the vibration of water molecules heats the food being microwaved. The effect of microwaves only works on water, which may lead to rapid drying of the food, which is why when you over-microwave something it becomes brittle.",null,3,cdib7e7,1r091o,askscience,new,8
goatherder100,"Basically its like microwaves are pingpong balls (lower energy) and light would be basket balls (high energy). If I hit you with one ping pong ball, nothing. If I hit you with a basketball, you notice it. If I pelt you with 10million ping pong balls you will really notice it. Kinda the same thing. It is not the amount of energy EACH photon has to deposit, it is how MANY photons there are to deposit energy. A microwave creates a high density flux of a large number of lower energy photons that will each deposit all their energy into the substance causing the average kinetic energy of the substance to increase. That is what heats it up. ",null,0,cdii8a0,1r091o,askscience,new,5
myarlak,when molecules absorb microwaves it causes them to spin rapidly releasing heat due to the friction of the molecules rubbing against each other.  visible light cause electronic transitions which are not translated into motion thus do not cause friction.,null,2,cdij6qb,1r091o,askscience,new,3
king_of_the_universe,"&gt; Why can we use microwaves to heat our food, when microwaves have less energy than visible light?

But the light of your Zippo isn't as hot as the summer sun, while both are visible light. Yes, a microwave photon has less energy than a visible light photon. No, microwave light doesn't hence have less energy than visible light. Duration of exposure and amount of photons are important, too.",null,0,cdioxr2,1r091o,askscience,new,1
iorgfeflkd,"Well, what is a magnetic field? A simple definition is that it's a region of space where moving charged objects experience torque. Light isn't charged, so it doesn't experience torque as it moves through a magnetic field.",null,2,cdibahb,1r09mw,askscience,new,9
SingleMonad,"There's not really a reason, at least not in terms of something more basic.

The electromagnetic field obeys the *principle of superposition*, which means that if you have the equivalent of two fields (either both electric or  both magnetic) in the same place, the resultant field is the (vectorial) sum of the two.  

For example, if you have two magnets, when you bring them close to each other you can imagine the two fields adding to each other in the space around them.  Depending on how you orient the magnets, you could make a stronger or weaker field in the common region.

Light is wave.  It propagates because as the electric field changes, it generates a changing magnetic field, which generates a changing electric field, and so on and so on.  This is the content of [Maxwell's equations](http://en.wikipedia.org/wiki/Maxwell's_equations) #3 and #4.  If you aren't put off by an appeal to mathematics, really squint at those equations and you'll conclude that if the fields obey superposition (E_total = E_1 + E_2), then the formulas do too.  That's not an *explanation*, of course.  Maxwell's equations were deduced to agree with experiment.  There exist different physical media that *don't* obey the superposition principle, rubber, for example, if the amplitude is very large.

It would be a very weird universe if EM fields did *not* obey superposition.  I could be watching a movie, and if someone shined a flashlight across my view of the screen, I would see distortion.

At the risk of adding confusion, there is a deep connection between superposition of EM fields, the bosonic nature of the photon, and causality.  I wish I could explain it, but it's a result from quantum field theory, and I don't know a simpler explanation.

**TL;DL:**  That's just how it works.  The fancy name for it is [superposition](http://en.wikipedia.org/wiki/Superposition_principle).

Edit: Spleling",null,0,cdifs54,1r09mw,askscience,new,6
A_Mathematician,"Short answer is that light is not a charged particle and wouldn't be affected by any sort of H-field.  However check out the [Zeeman effect](http://en.wikipedia.org/wiki/Zeeman_effect) and the [Stark effect](http://en.wikipedia.org/wiki/Stark_effect) as an interesting result occurs.  In short, materials that are emitting light, if placed in a static magnetic field, will have their spectral lines (light waves) separate. This is how the magnetic field of stars and other astral bodies are measured.  If I find any of my experiments on it I will reply to my comment with photos in an edit.",null,0,cdimugr,1r09mw,askscience,new,2
chrisbaird,"Fundamentally, magnetic fields and light are just specific forms of electromagnetic fields. All electromagnetic fields are composed of photons. So, fundamentally, your question is really, ""why does one photon not bounce off another photon"". The reason is because they are bosons and they carry no electric charge.

All quantum particles can be classified as fermions or bosons. Fermions (such as electrons), obey the Pauli Exclusion Principle which states that no two fermions can occupy the exact same quantum state at the same location at the same time. So when two fermions approach each other, they must eventually ""bounce"" off each other to avoid ending up in the same quantum state. But bosons, such as photons, do not obey the Pauli Exclusion Principle and *can* be in the same quantum state at the same time. This leads to interesting effects such as lasers, superconductivity, and Bose-Einstein condensates. It also means that two photons cannot directly effect each other. Instead, they just pass through each other. What determines whether a particle is a boson or a fermion is how it spins. Integer-spin particles are bosons.

Also worth noting is that photons themselves carry no electric charge, so they are not effected by each other. Even if a particle is a boson, it can still interact with others of its kind if it carries charge. Such is the case with gluons, which carries color charge.",null,0,cdixln1,1r09mw,askscience,new,1
mc2222,"light is a *changing* Electromagnetic field.  If light travels through a region of space, we can use the analogy of a pond to describe the EM field.  Light is the ripple that travels on the pond after the surface is disturbed.  We can consider the depth of the pond to be how strong the *static* EM field is in this region.  since light is the ripple, the size of the ripple is completely unrelated to how deep the pond is.  So, if light passes through a region of free space with a magnetic field, it is unaffected.

Having said all that, magnetism in a *material* can effect how light travels through that medium.  Basically, the magnetic field effects the matter, and the organization of matter relates to how light propagates through it.",null,0,cdj3fhk,1r09mw,askscience,new,1
Needless-To-Say,"Are you refering to the idiom of ""A sucker born every minute""?

If so we need to define the percentage of newborns that would be considered suckers.

Borrowing snusmumrikan's data of current birth rates and required births per year and making the (hugh) assumption that birth rates have been relatively constant and are linear with population. My best estimate would be when the world population was roughly 50,000,000 and according to my research this equates to about 1000BC",null,0,cdibryg,1r0agb,askscience,new,3
snusmumrikan,"Where are you getting the one every 8 seconds data? 

The data from the UN I've just looked up seems to suggest 2.5 births per second.

I tried to do some quick rough maths to answer your question but it means finding a year with half a million births, which will be so long ago there won't be any reliable records. ",null,0,cdibbop,1r0agb,askscience,new,2
glarn48,"A pretty interesting question. Intuitively I see no reason why genetics would predetermine someone to like one form of music over another. It seems much more likely to be due to culture and exposure. For example, if your 
parents played Frank Sinatra all the time, it's likely you too will like Sinatra because it's familiar to you. (See http://link.springer.com/article/10.3758/BF03201171 for a short-term demonstration of familiarity effects in music preference)

Is there any evidence for heritability of music preference? This question doesn't seem to be well-studied. A twin study is one of the most commonly ways of looking at heritability. An older study suggests music preference is not heritable (http://journals.cambridge.org/action/displayAbstract;jsessionid=BF7D791BC14FD12F7D16CC48CCFD7C7E.journals?fromPage=online&amp;aid=1362580). I found a press release from Nokia about the results of a pretty broad twin study that suggests a high heritability of musical preferences (http://press.nokia.com/2009/11/12/nature-or-nurture-study-reveals-musical-genes/). I'm very dubious of this study for several reasons, not least of which is that it doesn't seem to be published anywhere peer-reviewed, but also because the listed heritability is so high (for reference, a ballpark estimate of IQ heritability might be 50%).

Heritability isn't necessarily intuitive though. For example, the heritability of IQ is dependent on SES (http://pss.sagepub.com/content/14/6/623.short). That just makes it even more important to have some information about their methods and population of that Nokia study before you can interpret their conclusions. I wish I had a more rigorously scientific study to share, but hopefully someone else will come along with an insight. ",null,0,cdicgay,1r0as7,askscience,new,1
latent_variableZ,"Palmer, Schloss, and Sammartino (2013) stated as introduction to their study on human preferences in harmony: “although empirical research on aesthetics has had some success in explaining the average preferences of groups of observers, relatively little is known about individual differences in preference.” This about as true of a statement as one could make in regards to aesthetics. Neurobiologically, we can observe there is activity in “such and such area” of the brain when “such and such stimuli” are perceived. However, the variation in the subjective experience and idiosyncratic responses related to aesthetics do not lend them self to a “this is fact” statement of why we have different tastes. The best I can do with my background (I am a doctoral candidate in clinical psychology with an emphasis in forensic and neuropsychological testing) is tell you socialization and a person’s cumulative learning history appear to have the largest effect size. The interaction between nature (biology) and nurture (the environment) is complex and most likely leads to the variations we observe in aesthetic taste, learning literally changes the brain.  However, learning appears to be a very powerful factor in determining aesthetics.  Palmer and Griscom (2013) provide a good review of the topic and give examples of studies that demonstrate aesthetic differences according to age, gender, and cultural; the take away while some preferences may be innate (“infants have a bias toward looking at dark-yellow and light-red and a bias against looking at light-blue and dark-green, nearly the opposite of [western] adults”) learning appears to be a powerful factor in determining preferences.  Learning is also implicated in the differences observed between gender preferences (girs tend to like pink/boy tend to like blue), theories tend to overwhelmingly postulate this happens due to socialization.  Historically, records show it was not until the mid-19th century the blue pink dichotomy emerged and became so prevalent.

References: 

Palmer, Stephen E. &amp; Griscom, William S. (2013) Accounting for taste: Individual differences in preference for harmony, Psychonomic Bulletin &amp; Review, 20(3), 453-461 DOI: 10.3758/s13423-012-0355-2  

Palmer, Schloss, &amp; Sammartino (2013) Visual Aesthetics and Human Preference, Annual Review of Psychology, 64, 77-107 DOI: 10.1146/annurev-psych-120710-100504",null,0,cdij457,1r0as7,askscience,new,1
albasri,"I don't have an answer, but there is an emerging field / group of researchers interested in [neuroaesthetics](http://en.wikipedia.org/wiki/Neuroesthetics). I'd start there.",null,0,cdizuds,1r0as7,askscience,new,1
juliuszs,"That ""clear screen"" is a metal shield with holes smaller than the wavelenght of you microwave's radiation, so waves get stopped. The metal walls are not going to let you use your walkietalkies, but regular walls are not metal. You do get attenuation due to scattering and absorbtion, but the rf radiation is not stopped by typical walls.",null,1,cdic6gg,1r0bov,askscience,new,4
MCMXCII,"&gt;And the closer something does travel to the speed of light, the slower time goes relative to that object. Is this correct?

If you're moving at the speed of light *relative to me* **I** see **your** time passing slower.

&gt;So theoretically, if something travels faster than the speed of light, this object would travel back in time.

Something moving faster than the speed of light would violate causality.

&gt;I was hoping someone could explain to me why light and time are directly related.

The speed of light is sort of a ""conversion factor"" between space and time. If you take a time coordinate and multiply it by the speed of light, you make it into a distance in meters, even though it represents a time in seconds (or whatever other units of distance and time you choose).",null,3,cdiccso,1r0cxy,askscience,new,8
The_Duck1,"&gt; I was hoping someone could explain to me why light and time are directly related.

Light isn't special. There is a special speed, called c. Nothing can travel faster than this speed. All massless particles travel at this speed. Light consists of massless particles called photons, and thus travels at c. Light happened to be the first phenomenon discovered that propagates at this speed, so we call c the ""speed of light"" for historical reasons.",null,3,cdil5lv,1r0cxy,askscience,new,4
sloggz,"&gt; Why is it different for light? Using a similar example, if a star explodes, it takes time before that light reaches you. I would assume that if you outran the light, it would only mean that you wouldn't see that happen for a bit, and not that you were outrunning the event itself.

It's really just about your frame of reference. In almost all practical purposes, light IS the event. As far as we understand physics, you can't outrun the light at all. No matter how fast you run away from light, light will still (from your point of view) be moving at the speed of light. 

If a supernova goes off 100 light-years away, when does it make sense to say that the event ""happened"" here. When the first signs of that event become observable? If so, than that's the light. That's the whole ""Relative"" part of relativity. Whether or not you say ""That 100 light-year away supernova is happening now!"" and ""I'm seeing the light from a 100 year old supernova!"" is all... relative.

",null,4,cdiggxs,1r0cxy,askscience,new,4
null,null,null,5,cdiky3p,1r0cxy,askscience,new,3
nimobo,"Objects cannot travel faster than the speed of light. Because if an object travels at the speed of light, its mass becomes infinite. Moving any object requires energy. Infinite mass will need infinite energy to keep moving. Since the universe has finite energy, travelling at speed of light becomes impossible. So you should not compare sound with respect to light.

As an object approaches the speed of light, time is just measured differently for the object and a stationary person observing it. 

Lets say, you are taking a trip, (from point A to point B and back to A), approaching the speed of light, and I am observing you in a stationary position. For me it would seem that you have taken 10 hrs to complete the journey. But for you it will seem you have completed the journey in 6 hrs. So it would appear to me that time has passed slowly for you in completing the journey. This is called time dilation.

",null,5,cdiiz71,1r0cxy,askscience,new,4
wishfulthinkin,"Sleep and fatigue, along with many other functions, are controlled by the hypothalamus, and the sleep cycle is regulated by neurotransmitters such as adenosine, dopamine, GABA, histamine, and hypocretin, but not by inhibition.


For example, caffeine is a stimulant because it is an Adenosine A1 receptor antagonist, meaning it binds to receptors that adenosine would otherwise bind to, preventing the effects of the adenosine.


Dopamine interacts with norepinephrine receptors, inhibiting its effects - which means a decrease in the production and release of melatonin. Interestingly, the researchers found that these dopamine receptors only appear in the pineal gland towards the end of the night, as the dark period closes. Therefore, the researchers conclude, the formation of these heteromers is an effective mechanism to stop melatonin production when the day begins and to 'wake up' the brain.


Each of the neurotransmitters I mentioned serves a difference purpose in regulating the sleep cycle.  Here's a good paper describing the effects of different neurotransmitters on sleep: http://med.stanford.edu/psychiatry/narcolepsy/articles/natureneuro5.pdf",null,4,cdigdbq,1r0dd7,askscience,new,10
null,null,null,2,cdigblo,1r0dd7,askscience,new,7
quantummonkey25,"Viral genetic replication, especially in the case of retroviruses, have few, if any, error-checking  mechanisms, which results in a high rate of mutation. Combined with the high numbers of viral particles that can spawn from a single cell, this results in a much more rapid rate of evolution.

As for the second one, I have not personally heard of such a case. I do know viruses are often implicatied in horizontal gene transfer, a property that is routinely exploited in genetics labs to deliver particular genetic information to a target organism.",null,0,cdicdvj,1r0dir,askscience,new,13
Farnswirth,"I asked something along the lines of the second part of this question a while ago.  Here's what /u/schu06 (a virologist) responded with:

&gt;Life is thought to have begun as RNA in the primordial soup - chemicals coming together and forming RNA (just a hypothesis as have yet to properly prove it). 

&gt;It has been shown that there are RNA molecules that can catalyse reactions such as self-splicing and self-cleaving (take [Hammerhead](http://en.wikipedia.org/wiki/Hammerhead_ribozyme) as an example). 

&gt;If an RNA can be be enzymatic it's not beyond the realms of possibility that it can develop function as an RNA-dependent RNA-polymerase and self-replicate. This self-replicating entity would just produce more and more of itself allowing evolution to kick in. Mutation would occur giving other ""species"" of the same RNA. Interestingly, it has also been shown that the same sequence of RNA can fold in two different ways to carry out two different functions (so will be able to do more than just replicate)...

&gt;It would get to a point where becoming compartmentalised from the outside world is advantageous to an individual RNA strand, so other molecules don't steal your enzymatic actions. This compartmentalisation could also be controlled if the RNA molecule has other enzymatic activities (as mentioned in the previous paragraph). Compartmentalisation gave the first ""cell"". Since this cell can replicate itself independently of anything else, it isn't a virus by any modern definition. So it then becomes a matter of semantics, would you call this self-replicating cell a bacteria? It isn't a virus because of self-replication

Basically, the answer to the second part, from what I've gathered is a matter of definitions and semantics.  As soon as something ""evolves"" from an RNA-based structure, it is no longer considered a virus, because by definition a virus cannot replicate without first infecting an existing cell.  

*tl;dr - At least for the second part of your question: we don't really know because no one has ever observed it.  But it is not outside the realm of possibility.*",null,1,cdigwl1,1r0dir,askscience,new,5
JeremyJBarr,"Ill chime in here with a slightly different angle. Viruses and ""living"" cells shouldn't be considered as completely separate entities. There is lots of new and exciting research suggesting much more of a symbiotic interaction between the two then we ever realized or thought possible. 

Two recent [papers](http://www.plosbiology.org/article/info%3Adoi%2F10.1371%2Fjournal.pbio.1001626) show that a bacteriophage (virus that only infects bacteria) tail spike and a human transcription factor that regulates the insulation of our nervous system (myelination), share the same protein coding sequence. This raises the evolutionary question of where did this transcription factor, that shares such homology with a viral protein, originate? 

It is possible that the gene was similar horizontally transferred. But another interpretation may hint at an ancient interaction between a phage particle and a single cell eukaryote, which may fit in your definition of something 'living evolving from a virus'.
",null,0,cdihu6s,1r0dir,askscience,new,5
zmil,"/u/quantummonkey25 answered the first question quite well. As for the second, it depends on what you mean by ""living."" Plenty of biologists consider viruses themselves to be alive, in which case the answer to your question becomes trivially yes. If you want to limit your definition of ""alive"" to cellular life, well, it's still a little tricky. It has been hypothesized that the eukaryotic nucleus evolved from a virus, this is known as the [viral eukaryogenesis hypothesis.](http://en.wikipedia.org/wiki/Viral_eukaryogenesis) I don't consider it a particularly *likely* hypothesis, however. There are many other cases of organisms co-opting a virus or part of a virus for its own purposes, but generally this is just a case of a few viral genes getting incorporated into an organisms genome; are the genes ""alive""? They're part of something living, certainly, but I don't think that's what you meant by evolving from a virus. This is known as horizontal gene transfer, as alluded to by /u/quantummonkey25. ",null,0,cdih1z7,1r0dir,askscience,new,2
owaisofspades,"because viruses have RNA, when they replicate their genetic material the RNA polymerase can replicate the entire strand, whereas DNA replication cuts a tiny bit off the end of the strand every time, therefore they can replicate their RNA a virtually unlimited number of times. Because of this unrestrained replication, the probability of mistakes arising during replication increases, and these mistakes can occasionally lead to beneficial mutations. As mentioned earlier, other organisms can only replicate a limited number of times in most cells, so the probability of having a mutation let  alone a beneficial one is relatively low.

The second question is beyond the scope of my knowledge",null,3,cdicdri,1r0dir,askscience,new,1
joca63,"Unless I'm mistaken it should decrease net calories, but increase the calories you can digest. When something is cooked there will always be partial oxidation somewhere which decreases the energy contained in the food (think, if you over cook you get charcoal, if you really over cook you get carbon dioxide). However cooking makes most nutrients more easially digested. This is particularly true with fruits and veggies. Chewing raw veggies breaks surprisingly few cell walls to allow access the nurtients. Cooking breaks them on a large scale.

In short, cooking is almost a pre-digestion, it does use some calories in the food, but it allows access to more than it uses.",null,0,cdigq6w,1r0fjt,askscience,new,4
lasserith,"The calories themselves are derived from how your body naturally processes the chemicals included in them. Look at this guide for an example: http://www.nutristrategy.com/nutrition/calories.htm. As you can see each class of chemical in your food is associated with a certain energy density. Fats as a rule produce 9 calories when broken down by your body. As your body breaks down all chemicals of a similar class in the same way we are able to accept this 9 calories per gram of fat measurement and apply it to all sorts of foods. What this means is that as long as you don't drastically alter the chemical structure during cooking such as by burning it you can assume that calories in = calories out. Only a few chemicals used in cooking actually substantially change their dietary impact during the process, such as baking powder/baking soda but these are already negligible. 

TL,DR: Cookies are still bad for you but so so delicious.

Edit: As to the question of cooking adding or removing calories this is I suppose possible but I can't off the top of my head think of any thing commonly cooked with that would undergo a dramatic change. What little alcohol is cooked off when you cook with wine would help to decrease calories. There is I guess the possibility of degrading fiber if you use a high enough temperature to produce molecules which your body could possibly break down thus increasing the calorie cost but that would be far outside the range of temperatures at which you cook at.",null,0,cdie19g,1r0fjt,askscience,new,2
pucklermuskau,"not necessarily limited per se, but definitely a major constraint. Nairboi, Kenya comes to mind, as a city that has definitely expanded, but its road infrastructure is causing major issues, and its hard to fix because the roads that need the most work are the ones that are used day in and day out. China is helping them build a new highway system to take the load off, but yeah: historical decisions are often headaches...",null,0,cdida95,1r0gty,askscience,new,1
JimmyGroove,"While existing infrastructure can certainly limit a city's growth (and your case is a good example of that) there is always the possibilty of removing and replacing it.  That becomes more and more expensive as time goes on, but whether or not it ever becomes prohibilitively so depends on a myriad of economic and government factors that would be really to complex to answer simply.  For instance, a city that is wealthy enough might be able to afford huge infrastructure changes, but if the government of that city has no way to secure rights from current property owners growth might never happen, while a poorer city with a government that has more power might actually have a chance to replace limiting infrastructure.",null,0,cdigpd7,1r0gty,askscience,new,1
NassT,"In addition to the oxygen that makes up water (the O in H2O) there can also be oxygen (O2) dissolved in water, just like you can dissolve sugar or salt in water.  That's what people are talking about in this case.",null,0,cdic7sc,1r0iwf,askscience,new,5
hatsune_aru,"Gas can dissolve in water.

Fizzy drinks like Coke, Sprite, Beer and fizzy Champaign all have Carbon Dioxide dissolved in it. Oxygenated water, as the manufacturer claims, has more oxygen dissolved in it than just normal water.

Fun fact: most water-soluble compounds, like sugar and salt, are more soluble at higher temperatures. However, most gases, such as Carbon Dioxide and Oxygen, are more soluble at lower temperatures. That's why warm coke seems to have less fizz! ",null,0,cditnyo,1r0iwf,askscience,new,3
SqueakyGate,"We can't know for sure but here are some interesting dates:

1. ~60,000-50,000 years ago it is postulated that humans reached a point known as [behavioural modernity](http://en.wikipedia.org/wiki/Behavioral_modernity). ""One [hypothesis] holds that behavioral modernity occurred as a sudden event some 50 kya (50,000 years ago) in prehistory, possibly as a result of a major genetic mutation or as a result of a biological reorganization of the brain that led to the emergence of modern human natural languages"" These humans were like us in every way, so it is very very likely that you could mate with and produce viable offspring with these humans. Moreover these humans could be brought up in a modern society and would be able to blend in perfectly from both a physical and behavioural perspective. 

2. Anatomically modern humans arose about 200,000 years ago. It is very likely that you could mate and produce viable offspring with these humans as well. Although the evidence is less clear that these individuals behaved like us. For example we are unsure if they could have developed the same level of language complexity as ourselves. These anatomically modern humans could be brought up in a modern society and would be able to blend in perfectly from a physical perspective, however they may not be able to blend in from a behavioural perspective. It is worth noting that ""he second [hypothesis] holds that there was never any single technological or cognitive revolution. Proponents of this view argue that modern human behavior is the result of the gradual accumulation of knowledge, skills and culture occurring over hundreds of thousands of years of human evolution"". Therefore the dates for ~60 to 50 kya may not be so hard and fast.

3. Other homo species. We know that about 1-5% of human DNA is [Neanderthal](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens) in origin. We know that early humans and neanderthals could interbreed and produce viable offspring. Since it is very likely that modern humans could reproduce with ancient humans living 200,000 years ago than it stands to reason that modern humans and neanderthals could interbreed and produce viable offspring. A few important caveats are worth mentioning:

* 1-5% DNA can be explained by a small number of interbreeding events. This means that interbreeding events were probably uncommon.

* Human and Neanderthal populations did not overlap completely, therefore not all humans encountered neanderthals or vice versa. This means any interbreeding that occurred was not widespread.

* Neanderthals evolved about ~600,000 years ago in Europe, conversely humans evolved about ~200,000 years ago in Africa. Humans migrated out of Africa and into Asia about 100,000 years ago. Humans and neanderthals only encountered each other in Europe from about 50,000-25,000 years ago. Thus neanderthals and humans had been evolving independently for thousands of years. Behaviourally Neanderthals and Humans were very different, even more so around the time when they first began to encounter one another. Humans were better communicators, better innovators, engaged in long distance trade, and were expanding into new environments as well as exploiting them in novel ways. In contrast Neanderthals were somewhat limited: they had a relatively stagnant tool culture and and their cultural expressions were not as complex as humans. Humans at the time were beginning to express themselves symbolically, whereas neanderthals seemed only to be able to make jewellery, possibly some cave art and possibly engaged in symbolic burials.

* We know nothing about the context in which the exchange of DNA took place...we don't know if it was consensual or not. This has important implications for how the two populations viewed each other. Were they hostile? friendly? did they compete over resources (archeological evidence suggests yes, they did) or did they cooperate with each other? These questions help us better understand the complexity of their social worlds and how we might delineate these two populations (separate species, subspecies or the same species?)

* We know that flow of DNA was in one direction. ""While modern humans share some nuclear DNA with the extinct Neanderthals, the two species do not share any mitochondrial DNA, which in primates is always maternally transmitted. This observation has prompted the hypothesis that whereas female humans interbreeding with male Neanderthals were able to generate fertile offspring, the progeny of female Neanderthals who mated with male humans were either rare, absent or sterile"". This raises questions about hybrid vigour, and questions the assumption that we were the same species. At the very least it appears as if we were speciating, given the level of behavioural separation which had already taken place.

What does this mean for other hominin species like *H. erectus*? We can't know with 100% certainty if modern humans or even ancient humans could mate with these other extinct species. However, given the low levels of breeding with Neanderthals, the extensive evidence which suggests we were very different behaviourally and the lack of support for strong hybrid vigour I would come to the conclusion that it is unlikely that humans, ancient or modern, could produce viable offspring with any other hominin species other than neanderthals.

[More on our coexistence with Neanderthals](http://en.wikipedia.org/wiki/Neanderthal#Coexistence_with_H._sapiens_sapiens)

[More information on the Neanderthal Admixture Hypothesis](http://en.wikipedia.org/wiki/Neanderthal_admixture_theory#Neanderthals)",null,3,cdiio7h,1r0k55,askscience,new,19
TwizzlyHashtag,"There is also the issue of the theorized hormonal arms race between the fetus and the mother/host, wherein the fetus produces hormones that attempt to take over the host's resources, and the mother produces hormones to counter that.

However, a human woman from, say, 50,000 years ago would theoretically be too far behind in the evolution of said hormone production, leaving her to be overwhelmed and killed by a modern fetus, because the modern fetus will have been evolved to produce said hormones more aggressively.  Maybe. 

Some example discussions of this phenomenon:
http://books.google.com/books?id=FJixmoT5olEC&amp;pg=PA177&amp;lpg=PA177&amp;dq=mother+fetus+hormone+arms+race&amp;source=bl&amp;ots=OdcOT29uu1&amp;sig=xI2MNYMVlQpyp7M6gfucTVSBQGQ&amp;hl=en&amp;sa=X&amp;ei=GpCOUo79H-igiQKlyYGQDg&amp;ved=0CDQQ6AEwAQ

http://www.ulm.edu/~palmer/Mother.htm",null,0,cdk0nup,1r0k55,askscience,new,1
glarn48,"Great question! Not in my wheelhouse but I was intrigued. Any time you ask a ""why"" question about evolution, there's always some degree of speculation involved, but that doesn't mean we can't think of logical explanations regarding evolutionary pressure, and it turns out many researchers seem to have asked this very question. I'm going to summarize the findings of one paper (http://www.tandfonline.com/doi/abs/10.1080/1357650X.2013.824461#.UoxJv8Rwq6V), though they cite many other competing hypotheses.

Their theory is built around issues of mobility, given that mobility is crucial for survival and reproduction in all species, predator and prey alike. If part of the body is injured moderately or even fatigued, the brain can monitor the body via feedback loops and compensate for injuries by making dynamic adjustments to movement. However if the brain and body are both compromised by an injury, this compensatory ability is diminished. The authors posit that because a non-lethal injury that damages the head and body is likely to be unilateral, having contralateral control by the brain can preserve these compensatory effects. 

The paper does not discuss the issues surrounding sensory processing that you raise in your question though, but the explanation holds to a certain effect here too. A truly severe injury to the right side of the head might knock out the right eye completely AND the right occipital lobe leaving the left eye ineffective; but such a serious injury would not fall under evolutionary pressures because animals who sustained such a serious injury wouldn't live regardless of whether sensory control was ipsilateral or contralateral.
",null,1,cdimdq9,1r0qp0,askscience,new,3
lastsynapse,"The short answer is that we don't really know, but the evolutionary process occurred very early in the history of vertebrates.

[These folks](http://www.ncbi.nlm.nih.gov/pubmed/18780298) suggest it is because decussation is less prone to errors.  There's some more references on the [quora](http://www.quora.com/Why-are-so-many-brain-pathways-crossed-and-when-did-decussation-develop-in-phylogeny) question, too.  Others suggest it has to do with injury and injury prevention, that with crossed representations you can defend yourself on the side of attack, but that's probably hooey.

The weird thing is that you have ipsilateral cerebellar representations which are contralateral on the cerebral cortex.  Eyes are mixed, projecting both ipislaterally and contralaterally.  Everything crosses in such weird ways.  If there is a reason evolution selected for crossed representation, it has a whole lot of explaining to do, that won't be reduced down to one simple sentence.",null,0,cdmktyb,1r0qp0,askscience,new,2
armrha,"Have some aspirations of super villainy, eh?

Well, unfortunately, 'mind control' implies some kind of agency to it that they don't really have. They don't 'know' what they are doing, and the mechanisms they do it aren't super-complicated machines they install in the host's brain or anything like that. They're just little tricks that ended up working in their favor. It's evolution in action -- they never 'figure out' the brain of their host, they brute force it with the evolutionary algorithm.

Take the commonly dragged out example of Dicrocoelium dendriticum. It's a trematode which 'mind controls' infected ants to go hang out on top of blades of grass in order to get eaten by cattle and such. At some point, they were just like many other non-behavior affecting parasites, but eventually some kind of small change to their genome resulted in some very slight chemical imbalance inside the ant hosts, that resulted in slightly more of the fluke's ancestors reproducing than they otherwise would.

This could have been any number of things. A chemical that makes the ant work harder, giving it more time to get eaten. Maybe it secreted something that slowed the ant down, just making it an easier target. It's impossible to say exactly.

After the first change, generations go by which more success, and slowly tiny changes that result in more reproductive success start to take hold. One generation moves a little closer to the nerve center, things get more efficient. Whenever a change is not beneficial to the host, it ends up failing to reproduce (or at least not outpacing reproduction). So the flukes that had their ants hang out on grass in the blazing sun didn't make grandchildren. Selection slowly weeds out the most beneficial effects.

Viewed as a whole as they exist today, they almost seem to be aware of what they are doing, like little pilots steering an ant ship. But that's just the culmination of thousands of tiny little brain hacks they've discovered along the way, automatic parts of their life cycle that interact with the host in an interesting way.

Could such a thing happen to humans? Absolutely, but again, it'd be on an evolutionary timescale. I mean, parasites exploit what is available to them. Parasites piggyback on your biology, altering it somewhat, and anytime things are altered there's always the potential for the brain being effected. There's conjecture that Toxoplasmosis infection makes people more prone to risk taking, for example. 

There's a constant arms race between complex, cooperating groups of cells like ourselves and the smaller complex cooperating groups of cells that want to get in there and defect, taking all the free resources they can get their hands on. Ultimately, that's what the entire parasite game is about. So the answer is yes, but probably not in the way you were thinking. Especially as far as not being able to cause any specific, meaningful actions as far as we think of them as persons.",null,2,cdipevt,1r0qvp,askscience,new,7
wretched_beasties,"This isn't just with insects. Parasites are masters at manipulating host behavior, even in humans. For example, the guinea worm (Dracunculus medinensis) drives infected humans to soak their feet in water, facilitating the release of their eggs into a water source and continuing their life cycle.
",null,0,cdjelnm,1r0qvp,askscience,new,3
iorgfeflkd,"In that specific scenario, the source of the gravitational field would cease to be due to the rest mass of the star, but rather to the energy density of all the gamma radiation from where that star used to be. As it spread out, the gravitational field would change accordingly. This is because in general relativity, the source of gravity is not just mass but a more complex quantity called the stress-energy tensor, which is primarily dependent on energy density and pressure.",null,1,cdiiv2u,1r0tsg,askscience,new,4
baloo_the_bear,"Not really. Calories are a measure of the energy stored in the food, and eating calorie-dense food does increase the sensation of satiety, but the food you eat gets broken down into sugars, fats, and amino acids and is absorbed. The food causes an initial spike in blood sugar levels, but it is quickly converted into long term stores if it isn't used pretty quickly. The body likes to maintain a relatively narrow range of parameters; staying alive requires very specific set points for things like serum glucose concentrations, pH, temperature, etc. 

The feeling of hunger has a few proposed causes, from emptying of the stomach to dips in blood sugar. We do know that there are hormones which can affect the feeling of hunger and satiety, and they are ghrelin and leptin, respectively.

Hunger is odd in the sense that a person can survive for days without eating, yet we experience hunger several times a day, and whether we need food to survive or not. 

Hormone states are constantly in flux, and insulin, glucagon, ghrelin, and leptin are some of the hormones that play roles in the feeling of hunger.",null,0,cdiio5d,1r0wnc,askscience,new,9
Rocker232,"It really depends on what animal you are talking about, many adaptations have evolved. Some animals such as penguins use huddling as one technique, some frogs completely shut down their bodies and freeze. Other animals have a substance in their blood that prevents freezing. The amount of adaptations to combat against cold weather/temperatures could go on and on.",null,1,cdilly0,1r0x4h,askscience,new,4
sporclesam,"When its extreme, most [ectotherms](http://en.wikipedia.org/wiki/Ectotherm) go into stasis/torpor. Endotherms generally consume more nutrition &amp; conserve body heat through physical means such as those Rocker232 mentioned. Animals such as some bears will hibernate during extremely cold weather conditions. Many smaller animals such as hares can burrow in and conserve body heat in such fashion. Deer generally migrate in extreme conditions. 
You are underestimating the impact of fur, it is quite useful (along with subcutaneous fat) in such conditions. 

All these are physical means; there are numerous [bio-chemical ways](http://www.nature.com/scitable/knowledge/library/extreme-cold-hardiness-in-ectotherms-24286275) in which animals (and plants) fend themselves in the cold. ",null,0,cdiq2j6,1r0x4h,askscience,new,2
BoxAMu,"First: the state of a system encompasses the whole trajectory of its particles consistent with a given energy.  One section of that trajectory moving to another part is not a change of thermodynamic state.

Second: entropy is a statistical concept.  The fundamental laws of particle interaction are time-reversible, and the second law of thermodynamics does not apply until one considers an ensemble of many systems.  Or, one could consider the interacting particles to have some internal structure.

What would happen if two macroscopic charged bodies were released and allowed to attract one another?  Eventually they would collide and bounce back, only to re-collide and bounce back again and again.  Either the system would continue to oscillate like this, or more realistically, some energy would be dissipated in each collision until the bodies stick together.  The conversion of lost energy to heat represents an increase in entropy.",null,1,cdiiymq,1r0xju,askscience,new,6
cephsdiablo,"""When two oppositely charged particles attract towards one another via the electromagnetic force, does the decreasing volume of the system""

If you isolate to opposite charges those charges will attract towards each other with one given equation (I'm sure you know which equation this is). This equation shows that they are interacting only with each other not the entire system.

""result in fewer microstates per given macrostate of that system (and therefore an evolution towards decreasing entropy)?""

You can't assume that two microstates coming together creating one macrostate creates a decrease in entropy. Here is an easy example. Hydrogen and Oxygen in two smaller ""microstates"" create a much larger explosive macrostate when combined. 

If we could combine an electron and proton together to the point where they could create a reaction, I'm not sure what would happen and I'm not even sure if that is physically possible. Has anyone done it before??

A decrease in volume does not necessarily dictate entropy decrease.",null,0,cdip0ww,1r0xju,askscience,new,1
un7ucky,"hi production horticultrlist here. 
no the exocarp (peel) of a fruit will not grow back, but it will (plant depending) attept to form a scab to protect undeveloped fruiting bodies. most mature fruit will likely rot. 

any specific plant you had in mind when you asked?",null,2,cdil5ru,1r0xqx,askscience,new,8
Truck43,No. The skin is part of the flower that becomes the structure on the fruit. It cannot grow back. ,null,3,cdijcjb,1r0xqx,askscience,new,3
rupert1920,"There is no reason it won't be possible. You'll get a hydrogen ion - or a [hydron](http://en.wikipedia.org/wiki/Hydron_(chemistry\)), which is highly reactive.",null,0,cdiia6d,1r0y6n,askscience,new,9
myarlak,"you would make a proton, happens all the time
",null,0,cdij1oa,1r0y6n,askscience,new,8
Mazetron,"First of all, wormholes are only theoretical at this point.  We have no evidence of real wormholes existing and we are no sure whether or not they could exist theoretically.

However, they are fun to think about.  [Here is a rendering of what a real wormhole might look like](http://en.m.wikipedia.org/wiki/File:Wurmloch.jpg)

Go check out the Wikipedia page, it's got some good stuff.",null,2,cdilx53,1r10y1,askscience,new,12
BoxAMu,"This is a general question about light, regardless of the source producing it (sun, candle, laser, etc.)  Light spreads out as it propagates, becoming weaker in intensity because the same energy is spread over larger and larger areas.  Light doesn't actually disappear unless it is absorbed by matter, in which case the energy it carries is converted into a different form (motion of particles for example).

It's definitely possible to trap light in a box or other type of resonator, just like sound can be contained in a resonant chamber (like an acoustic guitar body).  In a resonator, waves propagate around the structure continuously.  They don't decay unless they leak out of the resonator or are absorbed in the walls.  For light, this has been achieved with a variety of different structures.  Some of the best are made from micro-sized (~10^-6 - 10^-5 m) spheres in which the light is confined to circulate near the walls.  However, these can still only confine the light for a short period of time (micro or nanoseconds).  ",null,0,cdiiimb,1r1184,askscience,new,2
iorgfeflkd,We are near the centre of our observable universe.,null,0,cdii8uj,1r11s1,askscience,new,4
Lirkmor,"Everything is moving away from everything else (broadly, not taking into account galaxy collisions and such), so even if we were observing from another planet light years away, we would still see the same effect. This is because objects aren't moving away from each other *through* space, but rather space *itself* is expanding.",null,1,cdiin49,1r11s1,askscience,new,4
e_t_,"Space itself is expanding, so everything is getting further from everything else. It's not like inflating a balloon. One method of enlarging an image is to divide the original into a grid, then recreate each square but bigger. Every point on the big copy will be further away from its neighbors than on the small picture, but that doesn't give you any information about where the center is.",null,1,cdiit62,1r11s1,askscience,new,1
MarineLife42,"Depends where in the body, and whether the heat can dissipate. There are many people now who have a brain implant that does just what you said - deliver a very low voltage of a specific point in the brain, helping with such things as Parkinson's, depression and other neurological issues. So when used correctly, it is even beneficial.  
Our entire nervous system relies on low voltages running as signals, controlling voluntary and involuntary movement of muscles. Here, a low voltage would interfere. ",null,1,cdj7oqo,1r1272,askscience,new,3
Jeeebs,"There is a condition called Bromism (Not something from How I Met Your Mother), where someone ingests too much bromide, and it can react with things in the body causing pretty serious problems. Typically this is from bromide salts, but it's also handy to note that BVO is not a salt.

HOWEVER... Bromine atoms are great leaving groups, so in much of the harsh conditions of the body, it is very possible the bromides are leaving the fatty acid chain and causing the same havoc.",null,0,cdinsbq,1r12y1,askscience,new,2
mutatron,"Right now it's Spring in the southern hemisphere, and it's just business as usual in the vast tropical regions. Here in Texas the leaves are only just changing, but we also have evergreens. I suspect you have evergreens in Massachusetts too. Not only that, but around 50% of the oxygen on Earth comes from plankton, and another 20% comes from algae.

There's a seasonal variation in local oxygen levels described [here](http://answers.google.com/answers/threadview/id/769958.html):

&gt; The levels of O2 are altered by the fall/winter in the northern
hemisphere, but not to a detectable level. Plants (both deciduous
(leafy) trees and many bushes and grass) do not perform photosynthesis
during the fall and winter months. This results in a cyclical
variation in Carbon Dioxide (CO2) levels. This season variation is
~5-6 ppmv: parts per million by volume. The total amount of CO2 is
approximately 380 ppmv. So the CO2 level cycles by ~1.5% annually.

&gt; O2 should change for the same reason, but the fraction of O2 to CO2 in
the atmosphere is 549:1 (by volume). Or O2 is 209,460 ppmv to CO2 ~380
ppmv. So the percentage variation of O2 is &gt; 0.002%.",null,1,cdiioni,1r13vn,askscience,new,9
DanielSank,"&gt; What are the parts of the D-Wave quantum computer?

~Sigh~

What D-Wave is selling has not been proven to be a quantum computer.

&gt; what general parts are there in a quantum computer?

I have [this saved from when /u/whittlemedownz explained it a while back](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp) but I'll give me own spin.

A quantum computer has a bunch of individual information storage elements analogous to the bits in a classical computer CPU or memory. The physical incarnation of the bits in a quantum computer must be build so as to exhibit quantum mechanical behavior. As such they are called ""quantum bits"" or ""qubits"". There are many possible physical elements that can be used: atoms, electron spins, photon polarization, superconducting circuits, and more. The D-Wave machine uses superconducting circuits. Each one is a loop of superconducting wire. The current in the wire can flow clockwise or counterclockwise. Because it's quantum the qubit can also be in a state that is a superposition of both directions of current. The bits interact with one another via the magnetic dipole of each loop of current: each bit feels its neighbors' magnetic fields.

The computer also has wires that carry signals into the bits to control them in various ways. In the case of D-Wave this is by applying external magnetic flux to the loops. In fact this control circuitry is extremely difficult to get right and in my opinion is the most impressive thing about D-Wave's machine.

&gt; but what exactly am I looking at - some sort of scintillating CPU?

The thing in the center is the chip with the superconducting qubits. The rest of it is control wiring and a cryogenic mount. Look in the dead center. See the rainbow colors in the black square? That's the chip. The rainbow color happens because of diffraction off of the tiny lithographically defined features. Those sets of itty bitty parallel lines at the border of the black chip are [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding). The wire bonds connect wires on the chip to wires on the green circuit board mount. The circuit board has its own wires (the thin lines in the green board) which fan out and connect to those bundles of what look like braided copper wires (the brownish things with the white labels on them). The gold colored metal just a mounting apparatus. It's probably gold plated oxygen free copper, a commonly used material in cryogenic applications.

Source - I work in a quantum computing lab",null,0,cdina0c,1r140w,askscience,new,9
spirit_of_loneliness,"It's hard to explain 'quantumness' in quantum computers without refering to quantum physics, but in extremely simplified version it's about 'more than 2 states', as 'regular' computers work only on two states ('voltage' or 'no voltage', so 0 or 1) and everything is built on this. Now, what makes quantum computer 'quantum' is the fact, that it can work on those 'usual' 2 states (0 and 1) and everything BETWEEN them (let's say a current state can have 40% of '0' and 60% of '1' ), this is based on fundamental physical principle called [superposition](http://en.wikipedia.org/wiki/Quantum_superposition)  

Basically, as you probably know, ""quantum computer"" works on qubits (named this way to differentiate from regular bits, reason above), so it boils down to the question how those qubits are implemented in given machine.  
Scientists already managed to invent more than one implementation, just take a look [here](http://en.wikipedia.org/wiki/Qubit#Physical_representation)   Of course, leaving all the 'standard' electronics, quantum computer is usually built around hardware, that can interface with physical materials, for example. laser and photon detectors (when qubits are implemented with photons), in general 'something' that can read the current state of our physical implementation (yet another simplification - read how much % of a '0' or '1' is there currently)",null,7,cdineb8,1r140w,askscience,new,7
Das_Mime,"Sound is a compression wave, it travels by massive (which in this context just means ""having mass"") particles like molecules bumping into each other and transmitting that bump forward. Imagine being in a dense crowd, and then a group of people charge into one edge of the crowd. Their momentum will be tranferred to the next person, and the next, and so on, propagating a wave outward through the crowd.

Radio waves are electromagnetic radiation, which occurs when you have an accelerating charge. The electric and magnetic fields (which can exist in empty space) keep oscillating and propagating themselves through space. They don't require a medium (other than spacetime and electromagnetic fields) because they aren't transmitted via massive particles.",null,2,cdij03b,1r15bm,askscience,new,36
BoxAMu,"This had 19th century physicists stumped too!  It used to be believed that a medium for EM wave propagation called the luminiferous ether pervaded all space.  However, this should change the properties of EM interactions (namely wave speed) for observers moving relative to the ether, just as motion with respect to still air changes the effective properties of sound.

Long story short, this change of wave properties is not seen.  Light always travels at c = 3 x 10^8 m/s in free space regardless of the relative motion of observer and source.  Moreover, based on the theory of relativity it is meaningless to even try to define some absolute space in which the ether could be located.  No medium for the propagation of EM waves exists.

So what IS an EM wave?  The question is hard to pin down.  The laws of physics correctly explain all observed EM phenomena, so it seems that this question is not directly relevant to experimental science.  But indeed, the idea of a wave without a medium is hard to visualize.",null,1,cdijd5s,1r15bm,askscience,new,15
Qazerowl,"I kind of get the feeling you don't know what radio waves are (if you do, ignore this).

Radio waves aren't actually sound, they're actually a type of light. Radio antennas can detect radio waves, and are programmed to make noise based on what signals they get. So, sending radio waves through space is more like Morse code with lasers than yelling into a megaphone.",null,3,cdikn7r,1r15bm,askscience,new,9
50bmg,"As i understand it, EM radiation is a wave created through *self propogating* electric and magnetic fields. That means there is no medium necessary for the wave to move through it - just the capability for empty space to have a quantifiable magnetic/electric field (which can be zero at any point, so it's not exactly a medium like water or air). The quantized form of this wave is a particle (the photon). Sometimes it is easier to picture radiation as photons traveling through space. ",null,0,cditcon,1r15bm,askscience,new,3
Roar_of_Time,"Sound can be recorded into a microphone, where it is converted into electrical energy and stored in a computer hard drive. The electrical energy can then be converted into a radio wave using an antenna. Radio waves are really small particles/waves called photons, and are a form of electromagnetic radiation. Visible light is exactly the same thing, just at a smaller wavelength, as are X-Rays, Gamma Rays, Ultraviolet Rays, etc. Photons require no atomic medium to travel, so they can travel through space. They travel at the ""Speed of Light"" through space. Photons can be picked up by other antennas and converted into electrical energy again, and then be converted into mechanical energy by a vibrating magnet, or a speaker. This mechanical energy is transferred through the air by the atoms in the air colliding with each other in a wave pattern. I suppose you could see them as a sort of atom sized photon, some people call them phonons. The colliding atom chain reaction enters your ear canal, where it is converted back into electrical energy and processed by your brain, which you perceive as sound. By changing the electrical energy that vibrates the magnet, the vibration can be faster or slower, making different sounds by changing the force applied to the atoms. 

Here's a nifty chart for the electromagnetic radiation spectrum if you're interested: 

http://mynasadata.larc.nasa.gov/images/EM_Spectrum3-new.jpg

And here's a rather cool video I found that shows that sound waves are simply moving atomic matter.

http://www.youtube.com/watch?v=wvJAgrUBF4w",null,0,cdinc4m,1r15bm,askscience,new,1
CoolStoryJohn,"A very simplistic answer is that sound is essentially a sequence of vibrations that travel through a region (molecules in the environment being the vehicles for vibration, i.e. molecules in the environment are responsible for the transportation of the sound) whereas a radio wave can be viewed as its own transport vehicle (""self-sufficient"").  Space has no atmosphere (and thus no molecules), so sound cannot travel.  However, as mentioned, radio waves aren't dependent on their environment in that way, so they are able to travel in space.  ",null,1,cdiy9vv,1r15bm,askscience,new,2
GlorifiedTapeOp,"Sound is a compression wave,  which means that energy is transferred between particles.  If there's no medium in space for the sound compression wave to transfer it's energy through, then the sound becomes void. 

Radio waves are essentially light (electromagnetic radiation) which don't require a medium like particles with mass for sound. 

Edit: Das_Mime answered this question better",null,0,cdiyezb,1r15bm,askscience,new,1
RationalAnimal,"
This is a good question.   And the posters above are not wrong, as far as their answers go, but there is a sense in which the original question is not answered. 

As others have pointed out, contemporary physics has it that light exhibits a wave/particle duality.  

The sense in which light is a wave is, in modern physics, only a metaphorical sense. In the literal sense, a wave is a higher-order property of some physical substrate.   Waves in water are the more voluminous sections of water in a pattern of more (wave) and less (trough) voluminous sections of water.   Waves of sound are patterns of sections of air with greater and then lesser air pressure.   Light ""waves"", however, are not now believed to be real waves in any other physical substrate.  

For a very long time, it has been known that light exhibits behaviors that waves exhibit.  Light diffracts through thin slits, for example, into fan-shaped patterns of different colors of light.  Light exhibits interference behaviors, like water waves do.  This evidence led scientists to believe, for a very long time, that light, too, was a wave.  So, they reasoned, if light were a wave there must be some physical substrate of which it is a wave.   Aether was posited as the substance patterned variations in which were light waves. 

It turns out, however, that there is no other evidence for aether other than the fact that light seems wave-like.   There is no detectable drag on moving objects in the vacuum of space.  There are no relative-motion effects exhibited by light relative to the direction of earth's motion around the sun, as one would expect if earth were traveling with respect to aether.  And so on.  

So, the notion that there is aether was dropped, but we were still left with the awkward exhibition of wave-like behavior of light.   That's pretty much where we are now, as far as a comprehensible physical model of light is concerned.  The claims that light is a ""self-propogating"" wave, or that light is the kind of wave that is not a wave in anything else, are not really meant to *explain* the puzzling sounding claim that light is a wave of nothing.  That is, these claims are not meant as ways of thinking of waves in a literal sense as a *physical model* of what light really is.  

They are just ways of saying that light exhibits wave-like behaviors sometimes, and that a representation of light as a wave permits one to predict the behaviors of real light accurately.   Even the claim sometimes made that we can think of light waves as ""probability waves"" isn't meant to offer a description of a physical model of light.  It is a shorthand way of saying that there are higher probabilities of detecting photons at certain places in beams of light and that the pattern of these probabilities has a formal resemblance to physical waves. 

So, the question of *how* light can be a wave without being a wave of something isn't really answered by modern physics.  It's better to think of modern physics as telling us that that macro-level, real waves are not a physical model of real light, but that light nevertheless exhibits wave-like properties.   What light *really* is is something, modern physics tells us, that has no analogue in our direct, macro-level experience.  But we know it travels through a vacuum.  So, there you go.  ",null,0,cdjj6kf,1r15bm,askscience,new,1
myarlak,"yes, if it is a water reactive substance like calcium carbide water will react exothermically and ignite the gases produced",null,0,cdij0ig,1r15t3,askscience,new,12
FatSquirrels,"Besides a chemical reaction with water itself, anything that has an [autoignition temperature](http://en.wikipedia.org/wiki/Autoignition_temperature) below 100 C could theoretically be ignited by heating it with boiling water.  However, if the material did not react with water and you poured the water on top of it you might simply smother any flames that would be normally produced.",null,0,cdix6p7,1r15t3,askscience,new,3
null,null,null,6,cdij52z,1r15t3,askscience,new,2
rupert1920,"Molecular weight doesn't quite matter - the filter doesn't operate base on [size exclusion](http://en.wikipedia.org/wiki/Size-exclusion_chromatography).

Your Brita filter removes most ions by [ion-exchange](http://en.wikipedia.org/wiki/Ion_exchange_resin) - these are stationary phases with pre-existing slots for ions to bind to. The ""exchange"" occurs when your impurity replaces the existing ions in binding to the stationary phase.

The [activated charcoal](http://en.wikipedia.org/wiki/Activated_charcoal) (the black powder that sometimes leaks out) is good for removing a large range of other impurities, and they work by non-specific [adsorption](http://en.wikipedia.org/wiki/Adsorption). In that sense, they _are_ able to remove dissolved organic compounds. However, if you're talking about something like dissolved sugars, you're often looking at concentrations way beyond trace amounts.",null,2,cdij2gr,1r16yk,askscience,new,11
MayContainNugat,The C-14 scale is corrected for variations in atmospheric C-14 by calibrating with tree rings of definite known ages. ,null,0,cdipwgt,1r17l3,askscience,new,5
humanino,"We first should acknowledge that we can only speculate about answers to those question. The final stages of black-hole evaporation are not well approximated by quantum field theory in curved spacetime, one would have to use a full non-perturbative theory of quantum gravity. 

Although very slow at first, the process eventually appears explosive. Because of this, (in my understanding) most people believe nothing is left behind. That is still explicit in Hawking's latest papers for instance. The total lifetime, estimated from the power emitted by Hawking's radiation (a formula which may not apply at late stages, but this stage should be short), varies like the cube of the mass of the black hole. For a black hole of one solar mass (10^30 kg), the lifetime would be 10^67 years. One should pause to contemplate such a number. The lifetime of the universe is a mere 13.7 billion years in comparison. Most black holes are (much) more massive than the Sun.

That is the reason people hoped microscopic black holes would have formed in the early universe, whose explosion we could possibly detect today. ",null,0,cdikbx3,1r17mc,askscience,new,4
Quantumfizzix,"All that /u/humanino  said is true, so keep that in mind.

There is no such thing as ""not enough mass"" for an event horizon to form. All that's required for a black hole is density. Obviously, it needs SOME mass, but that amount of mass can be as small as necessary so long that the volume is proportionally small. Once something becomes a black hole, it stays that way until completely evaporated. There is no remnant.

",null,2,cdiknps,1r17mc,askscience,new,4
5k3k73k,"There are at least two hypotheses:

Hawking radiation increases the smaller the black hole becomes. So much radiation is lost in the last few moments that it looks like the black hole explodes.

There is a threshold at which a black hole becomes too small to interact with other particles, halting Hawking radiation, and it becomes a stable WIMP.",null,0,cdj40qf,1r17mc,askscience,new,2
DanielSank,"A relative of mine does this stuff for a living so I can tell you what I've learned by asking the same question. Note that this is a combination of *what I've heard* and what makes sense.

Issues that you have to worry about when sending electronics to space:

* Radiation. Without the atmosphere as a shield there is more radiation in space. When a circuit gets hit by this radiation a variety of things can go wrong. A digital memory element may change state. This could lead to errors in CPU or FPGA controller chips. This is particularly a problem if the physical features are really small, so you have to get ""old"" parts that don't use current lithography scales. Analog parts may have problems too but I'm not sure. Parts that are qualified to not have errors when exposed to space levels of radiation are called ""radiation hard"" or ""rad hard"".
* Vibration. During launch the payload is subjected to vibrations. Why is this a problem? I think it's because the connections to the chip can break. Microchips are usually fabricated on a substrate of silicon, and the contact pads for wires going in and out are pretty small in cases where many wires are needed. You can't just solder to that. A common way to connect to the pads is with [wire bonds](http://en.wikipedia.org/wiki/Wire_bonding), essentially very thin threads of metal that are sort of mashed onto the contact pads.
* Packaging. Consumer electronics are packaged in plastic. If you look inside your keyboard you'll see a variety of black square or rectangular things with metal pins sticking out. The black is plastic and the silicon die is inside. Plastic absorbs water from the air. When you send that into space it gets cold and I think the problem is that the water freezes, expands, and cracks the package. This would probably rip off some of the wire bonds. Now, having said that a guy in my lab put a consumer FPGA into a 4 Kelvin cryostat and it worked so I could be completely wrong about this.

As for the cost, there are two factors. The first is that making something with the right packaging and robustness against radiation means you aren't using your standard fabrication line. That means the price is higher just because of supply/demand. That aside, an enormous cost is *testing*. When NASA builds a satellite they require not only that the parts in principle deal with the things listed above, but also that they are actually tested by

* Vibrating them
* Operating them under irradiation
* Thermal cycling
* ...and more.

That means you need extra test equipment, people, and time. when you buy a mouse for your computer that's just a part off of a fab line that makes a million of 'em a day. Maybe a small number of parts from each batch are tested for quality control. When you build a satellite, you test every single part.

EDIT: I think a lot of contemporary parts use degenerately doped silicon, so I guess carrier freeze-out isn't a problem. Does anyone know if this was a problem ~30 years ago?",null,0,cdilxiv,1r1bka,askscience,new,11
GProteins,"Short answer: No.

Your tears are more than just water. This is unfortunate for your no-blink plan, but fortunate for your eyes! They contain things like lysozymes, which help prevent eye infections by killing bacteria. It also contains various lubricating oils and some salts, which may just be left over from being drawn from blood plasma.

But either way, if you lived in an environment humid enough not to blink, it's humid enough to have a LOT of bacteria around that you don't want in your various mucous membranes.",null,0,cdn9i8i,1r1fvd,askscience,new,1
mrmayo26,"If I remember correctly, the trauma to the head causes a great deal of force and potential injury to the brain. Therefore in order to protect itself (although I'm unclear how this protects it or helps it exactly) it stops doing non-essential functions and reduces its activity. Although I'm sure someone else knows more about this subject than myself

",null,0,cdjaena,1r1hma,askscience,new,2
ericgdc,"There is a torsional force that takes place when you receive a concussive blow. This torsion 'stretches' various midline structures within the brainstem, areas that control breathing, alertness, etc. (think of wringing a wet towel, the center structures being 'twisted' the most). This is thought to play a role in the sudden loss of consciousness that takes place with certain blows to the head.



",null,0,cdjankh,1r1hma,askscience,new,2
LoyalSol,"Yup,  even in mixtures like corn starch and water you can still sink if you stay still long enough and of course you can't breath it. ",null,0,cditifq,1r1i97,askscience,new,3
steeeeve,"Non-newtonian fluids can actually be more dangerous than newtonian ones, because as a person attempts to swim or tread water, the fluid increases in viscosity around their limbs. 

[Quicksand](http://en.wikipedia.org/wiki/Quicksand#Properties) is an example of a non-newtonian fluid, and though its dangers have been exaggerated in pop culture, it's certainly still possible to drown in it.",null,0,cdj0w3k,1r1i97,askscience,new,3
endocytosis,"[Chlorophyll](http://en.wikipedia.org/wiki/Chlorophyll), what plants use to make energy, absorbs most light in the UV or near-UV range of light, although they can absorb some light in the yellow-red range (they reflect green, so plants have greenish leaves).  An incandescent lamp won't provide enough light in this range, and usually most other light sources won't either.  Artificial sun-lamps basically just provide shorter wavelength, ""blue-purple"" color, or possibly UV, although that is a bit more dangerous to your eyes, so the plants can use that for photosynthesis.  Sunlight has all of visible light, in addition to other wavelengths.",null,0,cditxys,1r1icj,askscience,new,2
chaseoc,"A thin sheet of aluminum foil would offer very little protection against cosmic rays unless it were extremely magnetized. Cosmic rays are extremely high energy particles emitted from black holes and stars... they travel very very close to the speed of light. Some carry as much energy in one tiny little atom as a thrown tennis ball. Because they are so small, there is a very small chance they will actually impact any atoms in that sheet of aluminum unless it were very very thick... and even if they do impact, they usually create a shower of particles that will continue on if the material is not thick enough from the point of collision to absorb them.

The same is true for normal radiation. You need thick lead containers to fully absorb radioactivity.... and normal radiation is not nearly as powerful as a cosmic ray. On earth it would be easy to protect, but in space wrapping a spacecraft with lead plating is not really feasible. Magnetic fields are an option, but to be effective with such high energy particles it takes a lot of energy... and to generate a lot of energy usually involves a very weighty reactor. 

http://en.wikipedia.org/wiki/Cosmic_ray",null,0,cdizta3,1r1j7j,askscience,new,5
trebuday,"This is still a topic of hot debate in climatology.  There are several ideas regarding the cause of the Mid-Pleistocene Transition (MPT), and a quick google search brought up these two papers:

[Clark, et.al., 2006](http://geosci.uchicago.edu/~archer/reprints/clark.2006.MPT.pdf)

[Martinez-Garcia, et. al., 2006](http://adsabs.harvard.edu/abs/2006AGUFMPP21B1688M)

Both of these papers attempt to address inconsistencies in various theories regarding the MPT, and both agree that the cause of the MPT was probably a very complex set of interactions between many or all of the various climate change processes (silicate weathering, deglaciation, changes in insolation, changes in ocean circulation, etc.). 
",null,0,cdj2kl9,1r1j7m,askscience,new,1
DevinTheGrand,"The menstrual cycle is a primate only means of female reproduction.  The reason why female primates have a period is because they shed the lining of the uterus when pregnancy does not occur.  Most other mammals use an estrus cycle, which results in the re-absorption of the uterine lining by the uterus.  (As an aside, rats can actually re-absorb miscarried young through their uterus as well, which is pretty awesome)

Animals that use estrus also are usually only sexually active during their period of ovulation.  This is called ""being in heat"" which I'm sure you've heard before, even if you didn't know the context.  Occasionally bloody vaginal discharge will occur, but it is not necessary, and it does not contain uterine lining.",null,2,cditwrk,1r1jah,askscience,new,11
MarineLife42,"Apart from /u/DevinTheGrand's excellent explanation, please note that what we call ""period"" in women is only a very recent phenomenon. Up to about 100 years ago, a woman of fertile age would have maybe four or five menstrual discharges throughout her life; the rest of the time she would have been either pregnant or lactating. My own great-grandmother had fourteen children of which ten survived into adulthood - totally normal then.  
Consider that at that time, girls/women began puberty much later than now, and you can see that there was hardly time for her to get anything approaching a 'period' going.  
In fact, menstrual bleeding is a kind of emergency backup program that the body runs when no egg fertilization occurs, which explains why it causes so much discomfort for many women. As far as the body is concerned, an egg going unfertilized is not supposed to happen.  
With mammals not under human supervision, including marine mammals, it is like the olden days with humans - as soon as a female is in estrus, sex will occur and she will end up pregnant. In the wild his is the norm, not being fertilized is the exception. ",null,8,cdj7il5,1r1jah,askscience,new,0
varodrig,"Not really, not in any significant way.   
Heat is the lowest/simplest form of energy (that we know of). This means that all forms of energy eventually degrade into heat. Let's take an example of two electric heaters. 'A' is just a heater. 'B' is a heater that also has a fan, a few LEDs and a speaker that reads out the time every hour.  
For both A and B, the electricity used by the machine's coils will all be converted to heat by the resistance. But the interesting thing about B is that even the air movement (fan), light (LEDs) and sound waves (speaker) will all degrade into heat as well.  
The only way A will be more efficient than B is via the MINISCULE energy lost by the sound waves and vibrations leaving your sealed room through the wall. ",null,0,cdj06vt,1r1jp1,askscience,new,2
arble,"Infinitesimal means smaller than any real value and yet nonzero. Saying that the difference between two values is infinitesimal is therefore meaningless because there is no real quantity that this difference could correspond to. For entropy change to be zero in a reversible process, the starting and ending temperatures must be the same. If they're the same (not infinitesimally close but actually the same), you can't extract any work from the engine no matter how long you run it.",null,2,cdirhbw,1r1kuj,askscience,new,5
Guanglais_disciple,"You mean the temperature difference between reservoir and heat exchanger is approaching zero, I'm sure (subtle but important distinction). Usually, a power cycle operating at Carnot efficiency does zero work, although I'm struggling to justify this. Maybe someone else can expand on my answer. 
",null,1,cdiuh75,1r1kuj,askscience,new,3
rocketgolfer,"The point of the Carnot cycle is not that there is no entropy increase, but that there is no wasted entropy increase. In the Carnot cycle, all energy involved in the temperature change from hot to cold is converted to work. In fact, peak efficiency is possible only with an infinitely large difference in temperature between the hot and cold resevoirs. If you go through a truly isentropic process, no net work will be done.",null,1,cdixb1v,1r1kuj,askscience,new,3
xxx_yyy,"&gt; ... the temperature differential between the hot and cold reservoir is infinitesimal ...

This not an accurate description of a Carnot cycle.  There is a finite temperature difference between the hot and cold reservoirs.  The working fluid cycles between the two temperatures in such a way that there is never any heat flow between finite temperature differences.

In an ideal Carnot cycle, the total entropy does not increase - it remains constant.  That's why Carnot cycles are reversible (can be operated in rather direction around the heat cycle).  One way gives you a heat engine, and the other gives you a refrigerator.

One needs to have a finite T(hot) - T(cold).  The efficiency of a Carnot heat engine increases as this temperature difference increases: 

Efficiency = [T(hot) - T(cold)] / T(hot)",null,0,cdjdm7b,1r1kuj,askscience,new,2
lazlokovax,"When you say or hear a word, certain neural pathways related to that word's meaning fire in your brain. There is a self-inhibitory aspect to this activity: i.e. when they fire repeatedly, the firing becomes less intense with each repetition. You could think of it as the part of your brain that understands that word being overloaded, and so the volume is temporarily turned down.

It's called [semantic satiation](http://en.wikipedia.org/wiki/Semantic_satiation).",null,6,cdipge9,1r1lgt,askscience,new,28
syvelior,"It's called semantic satiation, and neuroimaging results suggest that it has to do with how people process semantics rather than sensory satiation / adaptation (Kounios, Kotz, &amp; Holcomb, 2000).

**References**

Kounios, J., Kotz, S. A., &amp; Holcomb, P. J. (2000). On the locus of the semantic satiation effect: Evidence from event-related brain potentials. *Memory &amp; Cognition*, 28(8), 1366-1377.",null,4,cdirb65,1r1lgt,askscience,new,13
albasri,"This question has been asked several times before. Here are a few of the previous posts:

http://www.reddit.com/r/askscience/comments/mwrzz/why_is_it_that_when_you_repeat_a_word_over_and/
http://www.reddit.com/r/askscience/comments/1i0x5z/can_the_effect_of_semantic_satiation_occur_in/
http://www.reddit.com/r/askscience/comments/scz2a/why_do_words_written_or_spoken_in_the_language/
http://www.reddit.com/r/askscience/comments/m9pji/why_do_words_become_meaningless_when_you_say_them/

Unfortunately, because of the high volume, a lot of really interesting questions and great answers can get buried. The searchbar can be a great way to explore /r/askscience, although it can be a bit difficult to get just the right question. I found the above links by searching for ""repeat word"" and ""say word again""

",null,1,cdizmb4,1r1lgt,askscience,new,7
ModernTarantula,First hydrogen peroxide is [not a good](http://www.webmd.com/a-to-z-guides/wound-care-10/slideshow-wound-care-dos-and-donts) for open wounds. Then is it good as a surface antiseptic? Probably [ok](http://apb.tbzmed.ac.ir/Portals/0/Archive/Vol2No1/8.Ghotaslou.pdf). Then does catalase help the bugs. Looks like it is [so.](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC301784/) not all aerobes make catalase,null,0,cdji36m,1r1lh6,askscience,new,3
Javi2639,"The idea is that these bacteria would break down the hydrogen peroxide with catalase into water and oxygen gas, which bubbles to the surface of the wound. These gas bubbles would also bring most of the bacteria to the surface, which then be washes away. The immune system can then take care of the smaller amount of bacteria left. There's actually a debate about whether or not this is effective, but that's the theory. ",null,0,cdj89vu,1r1lh6,askscience,new,1
Madau,"Mutations involving albinism are related to pigmentation.  Any organism that produces a pigment can have a mutation in some part of the pathway of deployment, production, or structure that causes some form of ""albinism"" although I'm not saying all organisms can become white.

Fruit flies are a very well understood organism in genetics.  There are many variations of fruit flies that have different colors due to albino-like mutations.",null,0,cdivk5t,1r1m8u,askscience,new,2
NAG3LT,"In part because it reflects light in the same way as white paper does - it scatters it. This scattering means that the reflected light is distributed over a larger area and thus is less intense. You usually don't feel blinded by a white matte paper in the sunlight either. 


Another important thing to mention is the fact that not every surface reflects all the light, but only some smaller amount of light gets reflect. Your mirror may reflect over 90% of light falling on it, but if it gets dirty the refelection will be dim. On average, Moon reflects just 1/10 of the light falling on it. However, Moon's surface has retro reflective properties, so its brightness at full Moon is more than twice the brightness at half Moon. Still, it never becomes a perfect reflector and the scattering of reflected light still lowers the intensity a lot. ",null,0,cdioqh4,1r1mf7,askscience,new,5
SqueakyGate,"Yes, humans have always been omnivores. 

The hominin line and the pan line diverged about 6 million years ago from a common ancestor. The hominin line includes humans and all of our extinct relatives, like the neanderthals. The pan line includes the living chimpanzee and bonobo as well as all of their extinct relatives. This common ancestor was not a chimpanzee/bonobo and it was not a human. It was its own ape species. Based on some commonalities in both human and chimpanzee/bonobo diets (as well as inferences from other ape diets) we can surmize that this last common ancestor was also an omnivore. Chimpanzees and bonobos hunt for raw meat and it makes up a small portion of their diet. The rest of their diet consists of raw plant material, like fruits, leaves, roots etc.

There are a few major dietary milestones in the hominin lineage. 

1. The very early hominins, like [ardi](http://en.wikipedia.org/wiki/Ardi) are not associated with stone tools and they are not associated with fire. From these two observations we suppose that Ardi's diet was a lot like a modern chimps. Mostly plants and some scavenged meat (birds eggs, recently dead animals etc.).

2. Around 2.6 million years ago we see the first evidence of [stone tool use](http://en.wikipedia.org/wiki/Stone_tool) this may not have changed the way [australopithecines](http://en.wikipedia.org/wiki/Australopithecine) hunted for meat, they were probably still mostly scavengers. Australopithecine species lived from about 2-4 million years ago. 

3. With [H. erectus](http://en.wikipedia.org/wiki/Homo_erectus) we begin to see the first evidence of the control of fire. First we see changes in the morphology and dentition of H. erectus which some authors (listed below) link to a change in the diet. Cooked food allows an individual to have a higher caloric gain than raw food because the cooking process essentially pre-digests the food for us. These authors among others postulate that the cooked food diet is what finally enabled brains to grow larger. Brains are an energetically expensive organ and so is digestion. A constricting caloric intake meant that the brain could not grow too big even if their were environmental or social pressures for it to do so. By increasing their caloric intake by converting to a cooked diet this released the constraints on brain size and so it began to grow. So a cooked diet explains *how* brain size grew, but not *why*. Physical evidence for fire in the forth of hearths etc. dates back about 750,000 - 300,000 years. Thus ancestral Homo species were cooking food for a lot longer than humans have ever been around. However, we still don't know how pervasive this cooked food diet was, did all species living at this time know how to control fire? did all populations within the species? In any case some of them did - so their diet changed from raw to cooked, but not in composition. It was still mostly plants (fruits, nuts, tubers...) and some meat. They also had refined hunting techniques as tool cultures also became more complex. However, they were probably hunting small game (e.g. rabbits) rather than large game. I'll get into why this is probably the case if you like.

4. Humans arrive on the scene about 200,000 years ago. At this point Neanderthals are living in Europe and H. erectus has managed to spread herself all the way to the far corners of Asia (China, Java, India...). These early humans had the same tool cultures as these other species and they very likely also new how to control fire. Humans migrated out of Africa about 100,000 years ago and then things really began to get interesting. Tool cultures improved, humans kept expanding into new environments...we were innovators. We exploited and adapted. We became hunters. Our tools began to become so sophisticated that we could hunt from a distance (e.g. throwing a spear). In contrast it appears as if Neanderthal tool technology was stagnant at the time, and their spears could not be thrown - they were close ""combat"" spears. This is important because a lot of the neanderthal bones we have are broken, they have fractures etc. Humans on the other hand have less of these and this is attributed to the idea that we could hunt at a distance lowering the risk of injury. That being said big game still probably didn't make up a large portion of our diet. Modern hunter-gather societies have diets that consist mostly of plants, and some meat which is typically communally shared. This meat is more often than not small game. The success of small game hunting is on the order of every 1-3 days, whereas large game hunting is more on the order of once a month. You cannot reliably feed a population meat with large game using these early hunting techniques. So those depictions of early humans or neanderthals taking down a mammoth or a lion are very very unlikely. Try a fuzzy little rabbit, or collecting eggs from a nest.

5. 10-12,000 years ago humans begin domesticating plants and animals. Almost everything you find in the grocery today is a domesticated plant or animal, many of which date back to these first domestication events. Peppers, cauliflower, broccoli, corn, potato, carrot, wheat, barley, oats, rice, quinoa, lemons, grapefruit, oranges, apples, pears, olives, plums, millet, spelt, sunflowers, beans... all domesticated plants. Some of whose wild counterparts you would not even recognize - [teosinte](http://nrm101-summer2010.community.uaf.edu/files/2010/07/corn-and-teosinte_h1.jpg) or [wild potato](http://www.ars-grin.gov/nr6/bapotwso.jpg). So I often find those who argue that certain plants are not ""good"" for humans to eat because they have only recently been introduced to the diet (~10,000 years ago) often don't understand that the vast majority of our diet consists of plants and animals that never existed in their current form over 10,000 years ago. So humans around this time still had diets that consisted mostly of plants and some meat. However as populations began growing their own food and housing their own animals the types of things we ate changed. Moreover some populations began to acquire unique adaptations. For example, [lactase pesistence](http://en.wikipedia.org/wiki/Lactase_persistence) is a gene mutation which occurred about 10,000 years ago in certain populations. It allows an individual to continue to digest milk *naturally* into adulthood by keeping the production of the enzyme lactase turned on. Lactase digests lactose, the sugar which many people are intolerant too. Lactase persistence is more common in cultures/populations where dairying is also common. 

6. But what does this mean for our modern diets? Well as omnivores we can eat pretty much whatever we want (baring plant products which have a lot of cellulose, a sugar we cannot digest) without too much trouble. As long as we get all the nutrients we need we are fine. Today, many people live in places were access to food is easy. Moreover you can access a wide variety of foods from around the world that would otherwise be inaccessible. **We have the unique opportunity and privilege of living in a society and time where we can tailor our individual diets to reflect our preferences, our intolerances, our allergies and our moral or ethical obligations.** For example, being a vegan or vegetarian is not impossible but rather very much attainable in today's society because we have access to many different food resources which can make up the difference in terms of nutrients and caloric intake.",null,1,cdiu8do,1r1n2n,askscience,new,4
desig_nate,"There's actually very little contribution of current oil prices to current gas prices, it's about a four-to-eight week lag, depending on who you ask. Much of the market pricing is dominated by futures prices (i.e. price speculation) on various commodity exchanges due to hedging or pure speculation.

[This](http://money.howstuffworks.com/oil-speculation-raise-gas-price.htm) article is a good primer on speculation. [This one](http://useconomy.about.com/od/supply/p/oil_gas_prices.htm) talks a little more about the supply-and-demand aspect of it.

Unfortunately it's hard to point to one or two direct determinants of gas prices. If you look nationwide, much of it is determined by access to oil and gas pipelines (check out [this map](http://www.eia.gov/pub/oil_gas/natural_gas/analysis_publications/ngpipeline/images/ngpipelines_map.jpg)). In local markets, pricing strategies are more dominant, since everyone has roughly the same access to pipelines.

Hope that helps somewhat.

EDIT: Wording.",null,0,cdj1zck,1r1n6h,askscience,new,2
Jobediah,"Problems with a whale farm range from being extremely impractical, to being biologically unrealistic, to not knowing what these animals need to survive to being uneconomical.

Impractical- You'd need to cordon off a huge area that included all the resources whales need.

Unrealistic- Many whales (most? all?) range widely, some thousands of miles from their wintering and birthing grounds to their feeding grounds.

Mysterious- We don't know all the things whales do or need and this would make it likely the experiment failed.

Uneconomical- yeah, this would fail because whales generally take a long time to mature and reproduce. Farming generally requires fast reproduction and growth to be sustainable. And see other ways this would fail above. 

These are all general problems. There might be individual species for which some of these issues could be avoided but I high doubt there are any for which all of these problems could be solved.",null,0,cdirg85,1r1o8p,askscience,new,9
atomfullerene,"Arthur C. Clarke one wrote a story about herding whales [The Deep Range](http://books.google.com/books?id=-fYpAAAAQBAJ&amp;lpg=PT152&amp;ots=xV-sRgT5v6&amp;dq=arthur%20c%20clarke%20whale%20herding&amp;pg=PT152#v=onepage&amp;q=arthur%20c%20clarke%20whale%20herding&amp;f=false)

It's not likely it will ever be done, but if you were going to domesticate whales you'd pretty much have to herd them rather than try and pen them up somewhere.  Guide pods of females around the antarctic ocean between zones of rich plankton, protect them from predators, harvest most of the offspring every year.  It would still be quite impractical, though.  The expense of just keeping ships out there to keep an eye on them....

",null,0,cdj31w8,1r1o8p,askscience,new,2
null,null,moderator,1,cdioent,1r1o8p,askscience,new,2
marley88,"There are plans for this. So yes, it is theoretically possible.
[Here.](http://www.theguardian.com/world/2002/jan/14/highereducation.research)",null,2,cdiqrkh,1r1o8p,askscience,new,2
ignotos,"The predicted trajectory already takes the sun into account - it's already feeling the gravitational effect of the sun - getting closer to it doesn't really change this.

Also, the mass of the comet doesn't make much difference - it is still inconsequentially small/low-mass compared to the object it is orbiting, and often the mass of the orbiting object isn't even considered in cases like these.",null,0,cdjrz4b,1r1q27,askscience,new,1
fladam,"I believe that 'funny feeling' you are referring to is actually the effect of inertia. 
Things which are moving in a certain trajectory tend to 'want' to continue moving in that same trajectory, or things which remain at rest tend to 'want' to remain at rest.. 

Let me explain, imagine being on a roller coaster which is travelling along a straight horizontal path, and then suddenly the path shoots up at some angle, say 45 degrees for arguments sake, then inertia is that force which pushes you down into your seat. In the same way, if you were to feel a sudden drop on the roller coaster, you feel yourself coming out of your seat and you get that butterfly feeling. So if you were at rest in zero-gravity and you were to propel yourself downward then your body will feel a force in the direction opposite to the direction you propel yourself due to inertia which is independant to gravity in this case. This is because the force will be provided by your own propulsion. ",null,0,cdiv1w5,1r1qqi,askscience,new,2
endocytosis,"Probably not.  It depends on how shortly after death (seconds, minutes, hours, days), and also on the recoverability of eggs and sperm.  Generally sperm need the semen that is produced in the ejaculatory ducts and seminiferous vesicles to remain viable and healthy, but it is [possible](http://www.advancedfertility.com/testicsperm.htm) to collect sperm with a syringe by literally (gently) draining off the top layer of sperm that collects in the epididymis with a needle.  This would need to be done very quickly if a person died.   
Eggs can be collected from the follicles [directly](http://www.genea.com.au/Library/Fertility-Treatments/Assisted-conception/IVF), but usually the woman is given the hormone FSH first to maximize the chances that the follicles will produce an egg that can be collected.  If this has not occurred, the chances would be negligible, because with at most minutes to spare, *and* the hope that the woman happened to be ovulating at the time, the surgeon would have to find the follicle that was about to release an egg, and then successfully harvest it, which is not a simple procedure.

TL;DR Since cells in the body start to die within minutes of no oxygen, and the timing would have to be exactly correct to collect eggs with no prior preparation, eggs and sperm would not be viable after death.",null,0,cditqux,1r1r5j,askscience,new,3
ddubspecial,"With today's understanding of the standard model, we have no real answer if what gives particles charge or spin. Before Higgs, we didn't have a definite answer for mass either.  
There are theories that attempt to explain these fundamental properties. In string theory, tiny 1 dimensional strings of energy vibrate around in 10 dimensions if space. It is thought that the shape of these dimensions and the way the strings vibrate through them gives a particle it's properties.  Just a theory though. It's all math at this point. ",null,3,cdistj9,1r1uhx,askscience,new,15
fishify,"Charges arise in physics due to the ways particles and fields are modified by certain transformations known as symmetry transformations.  Things that transform differently will have different charges.

In our understanding of physics today, one of the fundamental elements of a physical theory is the set of transformations that leave the equations of motion unchanged.  Such transformations are called the symmetries of the theory, and each particle and field will transform in some specific way under such a transformation.  (By the way, particles and antiparticles differ with respect to a variety of charges, not just electric charge.)  Antiparticles and particles will transform in opposite ways under a given symmetry transformation associated with their charge.

Note that your idea that there is some constituent that carries one or another charge to distinguish particles and antiparticles doesn't really change anything; why should *those* two constituents have opposite charges?  ",null,0,cdixkwe,1r1uhx,askscience,new,6
zeug,"Antiparticles are predicted by the Dirac equation, which was an attempt to get a quantum mechanical equation for the electron which satisfied special relativity and accounted for the two spin states for the electron.

Dirac was able to do this by writing down an equation similar to the wave equation, but that does not have any solution for real or complex numbers. The trick was to reinterpret it as a 4x4 matrix equation operating on 4 component vectors.

The first two components naturally appeared to be the two spin states of the electron, and the last two were also spin up and down, but had a difference in sign that caused quite a bit of a puzzle. One consequence of this change is sign is that these last two components would correspond to particles of opposite charge when electromagnetic interactions are added.

So basically, if you want to satisfy what is observed about quantum mechanics and special relativity, and write down an equation to describe a particle with two spin states, such as the electron, the simplest way to go is the Dirac equation, which predicts antimatter components.


So you don't need charge or an electromagnetic field to have antimatter, it is simply there. The neutrinos have no electric charge, and there are corresponding antineutrinos also without charge. 

But it is true that if you add a term to the equation that gives the particles an interaction with the electromagnetic field, the antiparticles have to have an equal and opposite charge to the particles.

",null,0,cditj9k,1r1uhx,askscience,new,5
iorgfeflkd,"No, charge is just an inherent property of those particles.",null,4,cdirxwh,1r1uhx,askscience,new,5
sporclesam,"Even without forward facing vision, animals such as Horses &amp; other herbivores (majorly [Monocular Vision] (http://en.wikipedia.org/wiki/Monocular_vision)) have great field of vision but poor depth of field (Look up [Stereopsis](http://en.wikipedia.org/wiki/Stereopsis) for more on DoF). 
Other links: [1] (http://en.wikipedia.org/wiki/Binocular_vision), [2 (sorry for the garish blue!)] (http://www.artinarch.com/vp05.html)

SO navigation is no issue. Its interesting how there is a distinct link when it comes to predator-prey relations (with predators more frontal while prey being more lateral in their vision range).
",null,0,cdiy4hq,1r1wof,askscience,new,1
MarineLife42,"Not a dumb question at all.  
Rule of thumb: If you can see the eyes looking at the front of the animal, it can see you. So yes, they can see forwards but have poor depth perception. Also, there is usually a small area in front of their heads where they cannot see. Further forward, the angles of view overlap.  
Animals like this don't necessarily use their eyes for navigation, though they certainly use them to look for obstacles close to them. Primarily their eyes are there to warn them of predator's movements, while navigation is often done based on smell and sound.  
So a horse might wander to a farther place that smells good, while its eyes tell it about objects such as trees or the position of other horses in the immediate vicinity. ",null,0,cdj74p7,1r1wof,askscience,new,1
skorps,"With our eyes, an animal can not see. But vision is not a requirement for navigation. There are many other examples of ways animals navigate. Bats have echolocation in the dark, many bugs use smells, and many animals can detect vibrations. Some  environments do not require eyes (too dark, undergroubd) other animals just can invest the energy in evolving eyes when other less expensive methods work ",null,3,cdiramq,1r1wof,askscience,new,2
xenneract,"Odor depends on the release of small volatile molecules known as odorants. Inside orange peels is a small organic molecule known as [limonene](https://en.wikipedia.org/wiki/Limonene), which is responsible for the smell. Limonene has a vapor pressure of about 1 mmHg at room temperature, which means that it will exist almost entirely in the vapor phase. So as soon as you peel your orange, all of the limonene in the peel rapidly vaporizes and diffuses through the room.",null,0,cdivrnt,1r1xaf,askscience,new,8
miczajkj,"I can choose an coordinate system, where the Origin and the trajectory of the object lie in the same plane and make this plane the xy-plane. 
The origin is now (0,0). Now if the particle is moving with constant velocity v (constant velocity also means always the same direction) I can choose t=0 in a way, that d(0) = (d_0, 0) and v(0) = (0, v). Than the position of the object at an arbitrary t is d(t) = (d_0, vt). 

This is only an coordinate displacement, so the formula is completely non-relativistic. The relativistic effects only arise in the objects reference frame.

Okay, now let's have a look on what we think about the particles position. At the time t we see the particle at the position b(t). As the light from the particle has taken the time t' = |b(t)|/c this is equals:

b(t) = d(t-t') = d(t-|b(t)|/c)

So now we have an implicit equation to calculate b(t) from a given d(t)!
Let's substitute it into our equation. Notice, that the x-coordinate doesn't change (it's always d_0) so we only need to investigate the y-coordinate: 

y = v*(t-sqrt(d_0² + y²)/c)

(always read y as y(t).)

We can easily solve this equation vor y: 

First we introduce ß = v/c and write the equation as

y - ßct = -ß sqrt(d_0² + y²)  
y² - 2 ßct y + ß²c²t² = ß² (d_0² + y²)
y²*(1-ß²) - 2ßct y + ß² (c²t² - d_0²) = 0  
y² - 2 ß/(1-ß²) ct y + ß²/(1-ß²) (c²t² - d_0²) = 0  

y = ß/(1-ß²) ct +- sqrt(ß²/(1-ß²) c²t² - ß²/(1-ß²) (c²t² - d_0²))  
y = ß/(1-ß²) (ct +- sqrt(c²t² - (1-ß²)(c²t²-d_0²))  
y = ß/(1-ß²) (ct +- sqrt( ß²c²t² + d_0²(1-ß²))

We can now think about what y(0) should be: because the particle is moving towards positive y-values, y(0) should be negative - it is a position the particle has been before t=0. So we need to take the negative root. (The other root belongs to the so called advanced particle [in comparison to the retarded particle] that is no physical solution to this situation). If we choose to substitute ß=v/c again, we get: 

y(t) = 1/(1-v²/c²) (vt - v/c sqrt( v²t² + d_0²(1-v²/c²))

Now the current position of the particle is (d_0, vt) = (u, w) so the observed position as a function of current position and the velocity is

(x(u, w, v), y(u, w, v)) = (u, 1/(1-v²/c²) (w - v/c sqrt(w² + u²(1-v²/c²)))

What can be expressed more nicely by using ß=v/c and  γ = 1/sqrt(1-v²/c²) as

(x, y) = (u, γ²(w-ß sqrt(w²+u²/γ²)))

To find the solution in other coordinate systems you can now make simple coordinate transformations. (Note that it get's more complicated, as v -&gt; v(cos ϕ, sin ϕ) because you need to know the direction too.)

It's a funny thing, that all those relativistic factors show up, even if we didn't choose to use any features of special relativity. 

[Can you think of the reason?](#s ""It's because the whole formalism of relativity is inspired from the asumption of retarded potentials. It needs to reproduce the feature of delay between the emission of a signal and the measurement of this signal at another position. Therefore these factors are inherent if you choose to consider finite speed of information."")
",null,0,cdiznjj,1r1xvr,askscience,new,3
shamdalar,"As stated, your problem seems to be overdetermined. Let's leave light out of it, and assume we get a message traveling at speed c in a straight line from the object traveling at velocity v in a straight line, but we don't know the direction it came from. We know the current position and distance d from the object. If the angle at A is theta, the angle at our position is arcsin(sin(theta)*(v/c)) by the law of sines, and we can use law of sines again to find the distance from A to B.",null,1,cdixxsq,1r1xvr,askscience,new,2
hikaruzero,"&gt;What is the formula for finding this observed location given the current location and velocity of the object?

It's the simple formula for displacement with a constant velocity.

d = d*_0_* + vt

You should remember this formula from high school physics class.  It was pretty much the easiest one.  :)

&gt;Since it's for a little relativity game, it would be nice if the formula/algorithm were expressed in matrix/vector manipulation (eg cross product, dot product, matrix multiplication etc) and simple operators such (eg multiplication, addition subtraction, square root, etc.)

Just copy the formula above three times (one for each spatial axis) and put a subscript labelling the axis on every variable except t.  Then put them vertically in square brackets.  The only operation is simple vector addition -- you can add the components independently of eachother.  You can also combine them into a single magnitude of the vector with the Pythagorean theorem, which you should have learned in geometry class in jr. high school:

d = √(d*_x_*^(2) + d*_y_*^(2) + d*_z_*^(2))

Not sure what that has to do with relativity though.  Indeed it really doesn't have anything to do with relativity -- the relativistic formula is identical, it's just that the variables take on different values in different reference frames, and those variables are related to eachother in different frames by way of a [Lorentz transformation](http://en.wikipedia.org/wiki/Lorentz_transformation).

&gt;Trigonometric functions such as sine, arcsin cosine are tolerable but not so good. Solutions that require calculus probably can't be used for performance reasons alas.

You don't need any of this -- the problem's not *that* hard.",null,1,cdivtu1,1r1xvr,askscience,new,1
Rangi42,"First of all, inventing a way to describe how stuff works in terms of atoms, gravity, spacetime, or other such models requires the same kind of creativity as inventing a way to depict your senses using [geometric shapes](https://en.wikipedia.org/wiki/Nude_Descending_a_Staircase,_No._2) or [colored points](https://en.wikipedia.org/wiki/Pointillism), if not more. To quote Voltaire, ""There is far more imagination in the head of [Archimedes](https://en.wikipedia.org/wiki/Archimedes%27_principle) than in that of [Homer](https://en.wikipedia.org/wiki/Iliad)."" The difference is, artists use their intuitive inspiration to fuel their works, whereas scientists have to define [their theories](https://en.wikipedia.org/wiki/Atomic_theory) in concrete enough terms that they can be understood and tested by others.

You bring up the example of atomic theory. Democritus somehow had the inspiration of matter being made up of atoms 2400 years ago, but at the time his idea remained just an interesting concept. [John Dalton's theory](https://en.wikipedia.org/wiki/Dalton%27s_atomic_theory#Atomic_theory) in 1800 was more than just a neat idea: he took the time to deduce what consequences would follow from the initial idea, such as different types of atoms having particular weights, and used his theory to [explain his observations](https://en.wikipedia.org/wiki/Law_of_multiple_proportions).

The thing is, 100% certainty is impossible. Alternative theories can always account for the observed evidence if you twist them enough. The geocentric model of the solar system was bolstered by adding more and more [epicycles](https://en.wikipedia.org/wiki/Epicycle) to fit the planets' observed orbits, and even though the heliocentric model is much simpler, the geocentric one isn't actually ruled out.

A rule of thumb to decide between two models is [Occam's razor](https://en.wikipedia.org/wiki/Occam%27s_razor), which basically says that you should prefer the simpler theory as long as it explains the same data. Before Einstein came up with his theory of relativity, Newton's law of universal gravitation seemed like a good enough model for how mass moves through space. The [precession of Mercury's orbit](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Perihelion_precession_of_Mercury) could be dismissed as observational error, or a result of some unknown celestial body influencing it, or other explanations. When relativity's predictions not only matched Mercury's observed orbit more precisely, but also [explained other phenomena](https://en.wikipedia.org/wiki/Tests_of_general_relativity#Deflection_of_light_by_the_Sun), Occam's razor would suggest adopting relativity over Newtonian gravitation.

As for how scientists gather data to test their theories and invent new ones: it's hard. Measuring [tiny](https://en.wikipedia.org/wiki/Quark), or [huge](https://en.wikipedia.org/wiki/Supercluster), or [distant](https://en.wikipedia.org/wiki/Pulsar), or other extreme things requires technological breakthroughs, which rely on the same sort of insight as artistic or scientific breakthroughs. There are [theories](https://en.wikipedia.org/wiki/Insight#Theories) about how people come up with new ideas, and attempts to [program computers](https://en.wikipedia.org/wiki/List_of_machine_learning_algorithms) with some of our abilities, but we're nowhere near understanding or reproducing it yet. One technique is to predict the side-effects something will have and look for those instead. Black holes, for instance, can't be observed directly, but [the way they bend starlight](https://en.wikipedia.org/wiki/Gravitational_lensing) can be predicted, and telescopes check if the actual sky matches the predictions.",null,27,cdir7gf,1r1xxj,askscience,new,161
__Pers,"Atomic theory is one of the most well established and experimentally validated theories in science. Are we 100% sure of anything in science? No, but in this case, we're very confident we have the essential theory correct. 

Like the rest of science, it came about through application of the scientific method: proposing hypotheses, testing said hypotheses, and refining one's models as needed.

As astonishing as it may seem, while atoms are indeed small, [we've even been able to trap and study individual atoms in isolation](http://www.opticsinfobase.org/ol/abstract.cfm?uri=ol-35-13-2164). ",null,7,cdir6fh,1r1xxj,askscience,new,41
yeast_problem,"I think the easiest way is to learn the history of the discoveries. The greeks believed that elements could be divided into smaller and smaller pieces until an ""atom"" remained, but they didnt even know about electrons or the periodic table.

Try looking at the [Thomson Model](http://en.wikipedia.org/wiki/Plum_pudding_model) to get a feel for some of the intermediate stages of discovery.

You can calculate the mass of charged particles by firing them through a magnetic field and working out the force the magnetic field applied from the radius of the curve they make.",null,0,cdirad3,1r1xxj,askscience,new,5
RainmakerUK,"It is the best model we have -right now- to describe our observations of matter. 

In the world of Science, we are never sure. There is no ""sure"" in Science. We only have best guesses, which are replaced by better guesses over time, often as a result of observations which can't be explained by our current model. 

Always remember, ""the map is not the territory"". Our equations and models are simply our best descriptors of reality. They are a functional tool. They are not the roadmarkers of reality itself. ",null,1,cdiyswc,1r1xxj,askscience,new,6
HoopyHobo,"CERN is 99.999999999% percent sure that they have found the Higgs Boson, and that absolutely pales in comparison to how certain we are about protons, electrons and neutrons. Scientists will probably always tell you (correctly) that they can never be 100% certain about anything, but the infinitesimal percentage that isn't accounted for shouldn't really matter to anyone, especially those of us who aren't physicists.",null,1,cdj6vr7,1r1xxj,askscience,new,5
TurboCommander,"From Albert Einstein and Leopold Infeld, in their co-written book “The Evolution of Physics.” 

“In our endeavor to understand reality we are somewhat like a man trying to understand the mechanism of a closed watch. He sees the face and the moving hands, even hears the ticking, but he has no way of opening the case. If he is ingenious he may form some picture of a mechanism which could be responsible for all the things he observes, but he may never be quite sure his picture is the only one which could explain his observations.”

“He will never be able to compare his picture with the real mechanism and he cannot even imagine the possibility or meaning of such a comparison. But he certainly believes that, as his knowledge increases, his picture of reality will become simpler and simpler and will explain a wider and wider range of sensuous impressions.”",null,2,cdiyefe,1r1xxj,askscience,new,4
TheGoodMachine,"There is no such thing as “100% sure”. All we know is that until now all our observations agreed on that, and not a single observation disagreed, and hence it is ***useful*** (in predicting the world), which is exactly all that matters. And we did a shitload of observation (usually as a result of experiments).

So while this is a valid layman question, it’s an invalid type of question if you really think about it. A very common one though, which we all asked at least once in our lives.

Hence there will be no answer that will make you happy. You have to change your question. :)",null,0,cdj00dm,1r1xxj,askscience,new,2
Hypothesis_Null,"Yes we are 100% certain.

We are not 100% certain what makes up protons, neutrons, and electrons, and even less certain of what makes up what we think makes them up... but we are not wrong that things we call protons neutrons and electrons exist and act approximately as we've modeled them.

There is too much evidence and consistency in the model we have.  The model cannot be proven 'wrong' at this point.  It can be proven to be a constrained version of a more general model.  We could discover them doing something we didn't notice before if we put them under extreme conditions we havn't observed them under before.  But that will only result in a deeper understanding of them, not a different one.  Any new rules we learn will have the old rules as emergent approximations when put under the more common conditions we're used to. 

The common analogy is Newtonian Physics.  Newton wasn't wrong.  It's just when you things get extremely massive and/or fast, or are massless, the equations no longer accurately describes their behavior.  But if you use the more general equations Einstein and others have developed since, the equations reduce back to the Newtonian approximations when used on everything from large molecules to stars going slower than 1% lightspeed.

**TL;DR** We are certain, because they have been observed behaving as we believe them to behave too consistently and too much.  Anything at this point will not disprove what we know, but simply shrink the error bars on what we do not.",null,0,cdj5r5l,1r1xxj,askscience,new,2
dracho,"On almost any level, yes, we are sure.  

On the other hand, nothing is ever 100%.  Nothing.  Sorry to get philosophical, but there's always an exception to the rule.  Different universe?  Inside a black hole?  Dimension X?  Who knows?  Nobody, with 100% certainty.",null,3,cdiwu3h,1r1xxj,askscience,new,4
CoolStoryJohn,"The atomic model can be viewed as just that: a model.  We've had scientific models in the past that have been outdated/scrapped, and  inherently, any legitimate scientific model will be open to the same treatment given sufficient evidence.  That being said, the atomic model has been incredibly successful in both explaining behavior and predicting it.  At its base, the atomic model is based on induction (i.e. experimental evidence and generalization of specific results).  We've used the atomic model to explain the results of various experiments, and used those results/explanations to predict future cases (which have largely been accurate).  So then, up until this point: so far so good.  ",null,2,cdixr0j,1r1xxj,askscience,new,3
coniform,"&gt; Are we 100 % sure that the matter is made of atoms , the atoms made of protons, electrons and neutrons ect... ?

Yes. There is overwhelming, irrefutable evidence supporting the existence of atoms, and the particular structure of atoms.

&gt; how did we even deduce such things?

Here is a very clever and historically groundbreaking approach: [brownian motion](http://en.wikipedia.org/wiki/Brownian_motion#Einstein.27s_theory).

You can see brownian motion yourself...get a cup of water and watch very closely that the particles will move very erratically.

&gt; And how could we calculate the number of electrons of each element? And most of all , how could we calculate each of these particles' mass if they are so small ?

This has to do with how we figured out how the elements were [ordered in the periodic table](http://www.rsc.org/education/teachers/resources/periodictable/pre16/order/atomicnumber.htm).

All of these problems were studied just over a century ago. Isn't it amazing? This was barely after the discovery of the electron, and before the development of quantum mechanics. All of these very basic ideas foreshadowed an entire century of amazing scientific ideas.",null,1,cdjbl8k,1r1xxj,askscience,new,2
logical,"The atomic theory of matter is a proven theory.  We are certain of it.  We did not deduce it, but we induced it.  

This theory, like all proven scientific theories, start with observations of reality and arrive at generalizations by integrating them with other observations.  Contradictions of observations in controlled experiments rule out possible explanations and confirmations lead to further validation.  

It is important to note that additional knowledge can further refine a theory, but that in the context of what was known the original theory is not incorrect, but limited.  

If the atomic theory of matter was false, atom bombs and nuclear reactors wouldn't work, the periodic table of elements would not be such an accurate predictor of what elements exists and what their properties were, molecular science wouldn't work (which is what is behind most of the chemicals in your life, from cleaning agents to medicines) and so much of what you depend on from science would simply have to obey different laws and possess different properties.  

You can find a history of the timeline of the discovery of the atomic theory of matter [here](http://atomictimeline.net/index.php).

There are some good courses available that summarize the history of this theory if you look for them.",null,0,cdjcfij,1r1xxj,askscience,new,1
HighPriestofShiloh,"No.  Only 99.999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999999999999
99999999999999999999999999999999999999999999999999999%",null,2,cdjdtst,1r1xxj,askscience,new,3
dopsi,"Actually we can't be sure that these elements even exist, this is a model (which means an approximation) of the universe, build from our knowledge. For now this is the most accurate model we have, but it has its limits, like every model.

If you have a look back, the Greek astronomers believed the Earth was in the middle of the universe and the Sun and planets turned around it in circle. Their model gave them accurate enough results for their purpose and they didn't need another model. We know it isn't the case, but this was discovered when scientists got new and more accurate methods of measuring the world, that's why the current model could be seen as senseless is several centuries.

That's why we can't say we're 100% sure, because it's a model.",null,9,cditw52,1r1xxj,askscience,new,8
ChocoThornton,"I recently graduated with a degree in the History and Philosophy of Science, and this is a popular [realism/anti-realism debate](http://plato.stanford.edu/entries/realism/). Are scientific theories what actually happens, or are they just tools used to understand the Universe? It's a vast philosophical concept with no answer I'm afraid!",null,6,cdiw8bc,1r1xxj,askscience,new,6
YoYoDingDongYo,"Here's one: http://link.springer.com/article/10.1007/BF01055264#page-1

Here's another: http://www.ncbi.nlm.nih.gov/pubmed/1175391",null,0,cdir0dx,1r1xxt,askscience,new,8
My_Nipples_AreOnFire,"Your question actually touches on a great deal of pharmacological principles. Classical pharmacology assumes that a drug's therapeutic/toxic effects are directly related to concentration achieved at the site of action (ie. the tissue/cells that the drugs are effecting). This concentration is directly correlated to the concentration achieved in the blood, since this is essentially how much drug will be distributed to the site of action. This assumption leads to the conclusion that taking more of a drug will ellicit a greater effect:

* Greater drug concentration administered -&gt; greater blood concentration achieved -&gt; greater distribution to site of action -&gt; stronger elicited effect

This principle has essentially dictated pharmacology for a long time and has had good results.

However, I think this is what others are getting at, this does not mean that greater dose necessarily means more **therapeutic** effect. This simply means greater dose produces greater effect. After a point, this effect leads to a **toxic** effect rather than a therapeutic one. As you increase concentration you will reach a point where 100% of respondents are experiencing the drug effect, but you enter a concentration where toxic effects are produced. So to answer your question, classical pharmacology (actually pharmacokinetics, to be specific) would say, in terms of effect, no. Greater concentration of drug will elicit greater effect up to a point, but this does not necessarily equal the desired therapeutic effect. Just the drug's chemical effect.

This is the distinction that must be made. Because some drugs (such as those listed by /u/YoYoDingDongYo) will have some idea therapeutic dose, but prescribing a dose greater than that could lead to a greater *drug* effect that inhibits it's *therapeutic* effect.",null,2,cdj4v6k,1r1xxt,askscience,new,4
mrdeath5493,"So basically I think you would want to look into dose-response curves.  [This one](http://www.softchalk.com/lessonchallenge09/lesson/Pharmacology/dose_response.png), for morphine, shows that at some point increasing the dose  no longer increases its analgesic effects (pain relief).  But if you keep going, you start getting way way too much respiratory depression.  So if you were giving a dose that was too high for a particular patient, then you could decrease the dose and it would be just as effective at pain relief but with very little of the undesirable effect.  In a way, a decrease in this manner is making better use of the drug.  even though I don't think this is what you were looking for, it applies to most drugs and is a very good example to start with.

From here it is important to note that it is all relative.  Drugs have many effects, but are mostly used for a single effect.  I'll try to focus on the single therapeutic uses instead of the side effects.  In trying to answer your question, I found myself at a theoretical conundrum.  The smallest dose of a drug would be 1 individual particle.  Is there a drug out there that has its greatest effect at 1 particle and then decreases in effect as you increase the dose.  I wasn't sure that was possible, [however /u/YoYoDingDongYo provided an excellent example](http://www.reddit.com/r/askscience/comments/1r1xxt/chemistrypharmacology_are_there_any_drugs_that/cdir0dx) where a drug was more effective at *1 part per billion* than at higher doses which actually kinda blew my mind.

Why is this?...The following is educated guessing:  Well, the drug in that study exerts an effect a 1ppb, and at some point past that it must breech a threshold where it activates another biological process that works against the desired effect.  Either a detection mechanism is triggered or as it becomes more concentrated the cell might start to actively eliminate it.  I would like to see studies to see if there was an induction of elimination and if it was permanent.  Also, it might be helpful to look at 1,2,3,4, etc. ppb to see if there might actually be a threshold and perhaps an increasing effect on its way to 1,000 ppb.",null,2,cdiwwtj,1r1xxt,askscience,new,3
ModernTarantula,"Here are a couple of concepts: In immunology prophylaxis is the action of vaccine wherein the first exposure has a small immune response but subsequent exposure has a magnified immune response. Anaphylaxis wherein prior exposure leads to a magnified detrimental immune effect. The last of these is not from immunology--tachyphylaxis, repeated exposure depletes the response until absent. This is seen amongst tweekers who use meth until they deplete all neurotransmitters in their brains.",null,0,cdjhidk,1r1xxt,askscience,new,1
eclarep,"One example of a drug that has the effect you mention is Trazdone, an antidepressant that is much more commonly prescribed for sleep problems.
At a lower dose, much too low for antidepressant effects, trazdone has sedative effect but it takes a much higher dose to have antidepressant properties.
So, as a prescription sleep aid, the effect is dependent on the dose in a somewhat inverse manner. Above a certain threshold and up to a limit, it causes sedation, outside that range it can help with depression.
This is not medical advice, just a case that is an example of a drug of the kind you asked about.
[source](http://www.medscape.org/viewarticle/508820)",null,0,cdkulfh,1r1xxt,askscience,new,1
ringboard,"If something causes a increase in one effect, then it is by default causing a decrease in another effect. For example the Non-steroidal anti-inflammatory drug, COX-2 inhibitor, will decrease the activity of Cox-2, but by doing this it is increasing anti-inflammation effects. So higher doses means less Cox-2 activity but more anti-inflammatory activity.",null,4,cdiu26c,1r1xxt,askscience,new,2
patchgrabber,"Molecular Clock Hypothesis tries to estimate how far apart organisms are evolutionarily by means of using specific proteins. Some proteins, such as cytochrome c (present in almost all organisms) seem to have a fairly consistent time between neutral mutations, meaning that if most mutations are neutral (have no effect on fitness), and if they occur at more or less regular intervals, you can estimate how many new mutations you should see in a generation. 

Thus, by measuring the number of mutations in that protein from the time when two now distinct species had the same or very similar versions of these proteins, one can theoretically estimate the time these species diverged. There are several limitations of this process, like fossil prevalence, generation time and metabolic rate, among others. So while it may not be a perfect process, it's not without its uses.",null,73,cdiruch,1r1z4w,askscience,new,382
oliverisyourdaddy,"I'm an evolutionary anthropologist!

They compared the genomes of humans and chimps, estimating the total number of divergences (changes).  Then they calculated the average number of mutations (changes) in one generation (by comparing the genes of parents and children).  

Then they performed the following calculation: 
[(Number of total divergences)/2]/(mutations per generation)
to determine how many generations have passed since the divergence of humans and chimps.  (They divide the total number by two because the divergences represent changes accumulated in both the chimp genome AND the human genome, whereas you want the number of generations for just one species, since they're happening simultaneously.)

Now that they have the number of generations, they convert that to a time by multiplying that number by the average generation time - that is, the age at which a parent has a child (the average child, not first or last). 

So basically, find out how different the genomes are, find out how many mutations happen per generation, and calculate how many generations have passed.  Then multiply by the number of years a generation is.

Finally, they corroborate it with fossil evidence.  We can date fossils using isotope dating, so if we have fossils for all the ""intermediate"" species dating back to a common ancestor for two species, we can get a good timeframe for their divergence.  The problem with fossil evidence is that it's actually very limited for non-human apes.  We have a good fossil record for the human lineage, but not for the chimp, gorilla, or orangutan lineage.  The next closest primate that has a really good fossil record is actually macaques (a type of monkey), so calculations are often checked against the macaque record.  For a long time, our ape calculations actually didn't jive so well with the macaque record.

Something interesting happened in 2012 (I could be misremembering the year).  Scholars named Scally and Durbin proposed that the calculations had all been incorrect because they had used generation time for *current apes*.  Larger animals tend to have larger generation times (bigger animals have kids later, take longer to mature), and extant modern apes are generally larger than their ancestors.  Therefore the ""generation time"" variable was decreased a little, and these guys' new calculations fit better with the macaque evidence. 

Edit: wording",null,14,cdiub9d,1r1z4w,askscience,new,82
skadefryd,"I'm gonna stick out like a sore thumb in this thread, because I have a very different picture of how these estimates are obtained. I look forward to being proven wrong, though, because there are some people in this thread who know a lot more about the subject than I do.

My understanding has always been that divergence time estimates are generally obtained based on fossil calibration. These estimates are then compared to the number of (purportedly neutral) substitutions to obtain a neutral substitution rate and hence mutation rate, *not the other way around*. Measuring mutation rates in vivo is really hard, and we've only just recently been able to do it with any degree of precision, and a variety of factors can cause it not to agree precisely with mutation rates estimated phylogenetically (though they typically agree to within fifty per cent or so).

Any of the above might be completely wrong. Maybe /u/patchgrabber or /u/jjberg2 can set me right.",null,3,cdj46ol,1r1z4w,askscience,new,12
null,null,null,0,cdiyvny,1r1z4w,askscience,new,3
nedved777,"How do we figure out the ""number of substitutions per base pair per generation for a given piece of DNA?""  Is this something we can find using, for example, only two or three generations of chimpanzee DNA, or is it something requiring us to count the number of substitutions over thousands of years in a fossil sample whose age was determined by another method (e.g. radioactive dating)?

Since we are talking about specific proteins, some of which (cytochrome c) are present in almost all organisms, is there any reason we can't monitor rate of mutations per generation in some species of bacteria or something to find the rate?",null,1,cdiu46g,1r1z4w,askscience,new,2
null,null,null,1,cdiytjz,1r1z4w,askscience,new,2
null,null,null,13,cdirtxz,1r1z4w,askscience,new,4
euneirophrenia,You can make the surface of an object as complex as you'd like to make the surface area huge and the volume tiny. At the extreme you can have  fractal curve like the [Menger sponge](http://en.wikipedia.org/wiki/Menger_sponge) which has infinite surface area and zero volume (at infinite iterations),null,0,cdjripe,1r215u,askscience,new,2
LXZY,It would depend on what you set as zero.  ,null,0,cdizk4y,1r21m7,askscience,new,10
MayContainNugat,"The value of the potential energy isn't observable. Only changes of energy matter. So the value itself is arbitrary, up to a constant. ",null,1,cdj0cth,1r21m7,askscience,new,6
rlee89,"Gravitational potential energies are usually expressed as negative values.  Under such a representation, 0 is the energy when you are infinitely far from the object and at the top of its gravity well, and the potential energy becomes more negative the further down you are in the gravity well.

There are a few reasons why one might do this.  Picking zero energy at infinite distance natural leads to all potentials being negative and it is the least arbitrary point for zero to be chosen, since the potential energy goes to minus infinity at the object and any given distance would be an arbitrary choice.",null,0,cdj2sav,1r21m7,askscience,new,5
MCMXCII,"As others have said, potential energy scales are arbitrary. However it's common to represent bound states with negative potential energies.",null,0,cdj13cv,1r21m7,askscience,new,2
LoyalSol,"As other's have mentioned,  it depends on what you define your reference energy as.   A positive value means you are above that reference energy and a negative value means you are below that.

",null,0,cdj68ch,1r21m7,askscience,new,2
PricaCells,"Social grooming is not exclusive to just apes. In fact, its quite common in animal behaviour - and is also present in Humans. However, this may not be in the same context as chimpanzees. Keep in mind that different primates have different patterns of grooming as well.

Amongst people, we see this explained in studies by Nelson, H. (2006), 'Human mutual grooming: an ethological perspective on its form and function' and Nelson, Holly and Geher, Glenn. (2007-09-15) 'Mutual Grooming in Human Dyadic Relationships: An Ethological Perspective'. 

To summarize, there is grooming, but mostly in romantic couples. These are studies that have been done on western societies, however, and from an ethnographic perspective I believe that there are many more examples of grooming in different relationships as well. Social grooming as a form of touch communication is exceptionally important in many mammalian species, and I'd answer your question with a yes. The same is happening with humans, but in different was, and through different means., ",null,2,cdixqbb,1r23q6,askscience,new,7
snickeringshadow,"Tentatively yes, but it's complicated. I'm sure if you look hard enough you could find examples of humans grooming each other as part of social bonding. It's a little cliché but think about your 'typical' teenage girl slumber party where they paint each other's nails. That certainly qualifies. The problem is it's rather difficult to come up with universal explanations of how we do this because human behavior is patterned by culture as well as biology. Humans like to create formalized, idiosyncratic rituals that govern social interactions. All humans are social, but *how* we are social varies between populations and changes over time. This makes it really difficult to get at the biological basis for behavior - as they are phenotypically inseparable from the environmental or 'cultural' components. ",null,1,cdiy215,1r23q6,askscience,new,7
bearsnchairs,Without knowing more specifics it could be due to the mixing in your container. Even when doing a simple acid base titration with an indicator you can get transient color change whee you have a local high concentration of titrant. If you increase the stirring rate it becomes harder to form these areas of high concentration so you don't get patches of color change.,null,0,cdiuncm,1r265d,askscience,new,4
SimpleBen,"Graying is dependent on the balance of two opposing processes. The first is the loss of pigment, which is dependent on inflammatory factors, natural catalase levels, and H2O2 levels in the epithelium. The second is the restoration of pigment, which is UV exposure dependent.  
  
The scalp above the ears doesn't get a whole lot of UV exposure. ",null,0,cdiyfc4,1r275u,askscience,new,6
Truck43,"It may cause a conventional explosion and release of nuclear material, but modern nuclear weapons (older designs are less safe) include safeties to prevent nuclear detonation in event of fire or accident. To have a nuclear detonation requires precise timing of the detonation of the explosive shell, unlikely in a lightning strike. ",null,3,cdiwbg3,1r2adb,askscience,new,11
Sannish,"The fuel tanks of the nuclear warhead would explode, creating a radioactive plume (e.g. a dirty bomb).  I would guess that the currents from the lightning strike would not trigger the nuclear fission process as I assume the warhead design has some basic safety/grounding features in the circuitry.",null,3,cdiwer1,1r2adb,askscience,new,5
redditor5690,"I've seen 60's and 70's nuclear warheads up close many times.

The casings will not allow electrical flow from outside.  Think of what happens when a car or plane is struck my lightning, then imagine how much better it would be if the engineers actually chose that capability as a design requirement.

I would be more concerned with the heat that might be generated in the case if sufficient current flowed through it on the way to electrical earth ground. That heat might be enough to trigger an explosion of the conventional explosives which act as the detonator for the weapon, but those explosive charges, arranged around the U-235/P-239 core, must explode with extremely precise timing to produce a nuclear explosion. But, setting one off would be enough to cause a ""dirty bomb"" explosion which would contaminate a large area.

But, there's always Murphy to consider.",null,0,cdjhktw,1r2adb,askscience,new,2
pucklermuskau,"its a trade off of investment. Some species invest little in their offspring, in terms of the amount of energy contained in individual eggs, and in terms of the amount of parental investment afterwards. That means individual offspring have little chance of survival. Other species reproduce less, and later, but invest more in the individual, so that particular individual has greater chance of survival... reproduction per se isnt the goal: successful reproduction is the goal: your offspring need to themselves reproduce!",null,1,cdiu7cw,1r2b9n,askscience,new,10
Unidan,"I think this question isn't so much asking about the trade-offs between being a long-lived organism versus a short-lived one, but rather *why* long-lived organisms would evolve in the first place, correct?

If that's the question, the current idea is that within very *stable* environments, being able to exploit scarce resources can be advantageous.  With this, longer-lived organisms with more experience or learning may be selected for!  

If you're a very short-lived organism in a stable environment, but not necessarily very competitive, you most likely *won't* be favored if you're going up against an organism that has experience in exploiting a particular resource.  So, eventually, evolution has produced organisms that may require large amounts of parental investment, long periods of growth to maturity and more, but these will give an advantage over those that are only equipped with that with which they are born.

Does that make more sense?",null,5,cdj14hm,1r2b9n,askscience,new,9
polistes,"In addition to the answers already given, I'd like to also illustrate this issue with the difference between short-lived plants and long-lived plants (I like using plants because they cannot run away from their problems). Some plants live for many years, while others only live one season. There are advantages for both strategies. Short lived plants use resources quickly and don't have to invest too much in adaptations for surviving hard conditions, like a dry period or winter. They simply make lots of seeds, which can survive these difficult periods. On the other hand, if there is a year in which the growing season is very tough as well (prolonged drought, insect pests like locusts), the population of single-season plants gets a large blow, because many plants won't make it to seed production and you end up with less seeds being deposited.

Now if you are a long-lived plant, you invest a lot in these survival skills and grow slower. If you endure a particulary hard year, you may end up with not having enough energy to make flowers and seeds. However, this is not that bad, because you'll have a new chance next year! For example, there are a lot of plants in remote areas that struggle to attract pollinators for their flowers, so they end up not having seeds in many consecutive years. However, because they are long-lived, they still have many opportunities to pass their genes. If you were a short-lived plant, your population would die of pretty soon.  

So, there are both advantages and disadvantages of these strategies, and neither is particulary better.",null,0,cdjkefq,1r2b9n,askscience,new,1
chrisbaird,"I think you meant to ask ""Is a gravitational pull fundamental"". Of course gravitational pulls exist, they are just not fundamental. Fundamentally, gravitational effects are caused by the warping of spacetime by mass and energy, and not by a Newton-style force field stretching out from massive bodies. But just because a concept is not fundamental, does not make it wrong, imaginary, or useless. Most of the things we talk about in science (e.g. friction, centrifugal force, Van der Walls force, buoyancy, etc.) are not fundamental. The only fundamental things in the universe are the ones in this chart: 

http://en.wikipedia.org/wiki/File:Standard_Model_of_Elementary_Particles.svg 

But describing the motion of a child on a swing strictly in terms of quarks, electrons, and photons would be unnecessary and difficult.

 ",null,1,cdiylvn,1r2dfo,askscience,new,8
iorgfeflkd,"If you drop something, it falls. That is a manifestation of the existence of gravitational pull.",null,2,cdiwi9z,1r2dfo,askscience,new,4
fishify,"Via Einstein, we learned that we can understand gravity as a manifestation of space and time being curved.

Alternatively, we can not take into account this curvature, and then we must define a force to account for gravity.  Said force is attractive and pulls on objects.",null,0,cdiy566,1r2dfo,askscience,new,3
rupert1920,"They have a very good idea on what it's _supposed_ to do. This data is available from early research _in vitro_.

In terms of side effects, this is gleamed in toxicological studies and [clinical trials](http://en.wikipedia.org/wiki/Clinical_trials) - specifically Phase I, where the drug is administered to healthy volunteers. To move onto later phases of the trials, safety and efficacy must be demonstrated to the relevant regulating agencies in the country.

Especially now in the era of [rational drug design](http://en.wikipedia.org/wiki/Rational_drug_design), a drug is synthesized with the explicit purpose of interfering with some known biochemical pathway. So it's really not like drug companies are just randomly administering chemicals just to see what they do.",null,0,cdivgvd,1r2dfy,askscience,new,10
snusmumrikan,"Starting a new comment because I'm directly answering your question but that is not to take away from some valid points made by Rupert and botanist. 

I have experience in early R&amp;D at the UKs largest pharma (devilishly difficult clue there). 

For a new drug to have even the slimmest chance of getting to first-time in human (FTIH) it has to have gone through extensive and rigorous tests. Initially almost all drug design begins with the development of assays (repeatable standardised test experiments) which are used to screen a company's vast library (millions and millions) of compounds. These assays are becoming more extensive and phenotypicaly relevant over time as the biggest problem in drug development is attrition, and for every step further down the chain you take a compound before finding out it is not fit for use as a medicine it costs exponentially more - you want to be almost completely sure you're compound is viable before progressing it. A shift away from standard model cell lines and towards disease-relevant tissue samples derived from patients is currently underway as this gives significantly more confidence that the 'hits' are actually good, whilst doing away with the confidence, experience and reliability of those extensively studied model cell lines. If you're interested I can send you sections of my Masters research project which was in this area and has the appropriate references.

Once you have your hits from your initial screen these are then run against standard assays for things like cardiac liability (will it mess up ion channels in your heart?) or general toxicity to know whether it shouldn't be allowed in your body at all, you'd be surprised just how often certain molecules come up as great hits (elevating the protein which is deficient in a disease, for example) but turn out to be the old red herrings like HDAc inhibitors.

After this the compound will be carried forwards into animal trials, and this is where there is too much variation to try and cover it all but it will usually start in something small with a shorter lifespan like mice, potentially recombinant mice which have had a disease phenotype replicated within them, along with healthy controls and so on. After this other animals may be used, for example for respiratory and cardiac models, dogs are used as they represent the most human-like model.

Once the animal testing has been done extensively and for a long period to identify any and all side effects, if there are none which would indicate toxicity or severe side effects in humans the company will spend several months compiling the data, checking it, analysing it extensively before submitting it for Phase 0 clinical trials. These are before phase 1 trials and involve extremely low doses in relatively very few people in order to assess at an early stage the pharmacokinetic (where it goes inside you) and pharmacodynamic (what it does) implications of the compound. 

As you can see it takes a lot to get there and companies want to be incredibly sure of their compound before going near a human because A: it costs an armada full of money and B: no one wants another thalidomide. 

This is obviously a huge subject and i have left out many of the initial stages and assumed the compound came from a small molecule library (as opposed to chemical design, structural rational design or biologics such as antibodies) but the general principles remain true. 

Hope this helps, happy to answer anything else as long as I feel comfortable with my knowledge. 
",null,0,cdj1lcx,1r2dfy,askscience,new,7
czyivn,"All the rules of pharma are written in blood.  Past experiences with bad drugs have shaped our current practice to prevent patient deaths.  Here are the major things that prevent human deaths during drug trials:

1. Test it before it gets to animals.  Lots of general mechanisms where drugs kill people are already known.  Inhibition of hERG channels causes cardiac deaths, so they have an assay where all drugs are tested for hERG inhibition.  Liver damage is another mechanism, so all drugs are tested for toxicity to hepatocytes. 

2. Test it extensively in animals.  Before it ever sees a human, the drug has usually been tested in at least rats and at least one larger animal (dogs, monkeys, pigs).  We aren't smart enough to predict animal toxicity in advance (and anyone who says otherwise is a liar), so the only way to be safe in humans is to test it in other animals first.

2. Dose escalation.  They start out in humans at doses that are usually much less than where they expect serious side effects.  They gradually increase the dosage until side effects start to appear, like elevated liver enzymes.  If any of these symptoms are troubling, they stop the dose escalation immediately, and typically remove that patient from the trial.",null,0,cdizkvg,1r2dfy,askscience,new,3
chrisbaird,"There are two basic ways to stabilize a projectile: 1) passive control, such as fixed fins or spinning the object, and 2) active control, such as tilting the exhaust cones in the appropriate direction, having small position-correcting thrusters, or moving wing flaps. A lot of rockets have both because using passive controls means that your active controls have an easier job to do. But you can get by with no passive controls such as fins if your active controls are advanced enough. 

It depends a lot on the application. Many rockets, such as missiles, spend a large portion of their trajectory in ballistic motion (i.e. rocket engines turned off). In such a case, you can't use gimballing the rocket nozzles to stabilize flight, so fins become more important.",null,1,cdix4y7,1r2e54,askscience,new,20
DUFFYSTE98,"It's all about the exhausts. They can change the direction of the exhausts which changes the path of the rocket.

Hope this answers your question, if you want to read more into it http://inventors.about.com/library/inventors/blrocketcontrol.htm",null,1,cdiw9st,1r2e54,askscience,new,4
Needless-To-Say,"Typically the signals are separated within the common medium by frequency shifting. Each individual endpoint would have a dedicated frequency range within the common medium. Copper cables have a much lower threshold of channels than Cable which in turn have a much lower threshold than Fiber.

When the signal is digital the data is typically encrypted to prevent unathorized access. Anyone trying to Sniff the data would first need to decrypt the data. With older, analog technology, it can be a much simpler matter to ""sniff other peoples signals""

When you get into wireless tech, things get far more interesting with multiple simultaneous security measures in play to prevent eavesdropping but nothing is completely secure.",null,3,cdj234i,1r2esb,askscience,new,5
iorgfeflkd,"The extreme example is [Anatoly Bugorski](http://en.wikipedia.org/wiki/Anatoli_Bugorski) who had a high energy proton beam go through his head. He was injured, but survived.",null,96,cdiwr26,1r2gwb,askscience,new,489
DAlder_HardlyKnowHer,"Alpha particles are just that, they are equivalent to a He2+ atom. Which consists of 2 protons, 2 neutrons, and no electrons. Very few things are smaller than alpha particles including isotopes of hydrogen ( H ). Alpha particles are already too large to pass through your body. However, if the source which emits the alpha particle is already inside of you ( ingested or inhaled ) it will cause extreme amounts of damage, pending the amount.",null,21,cdizx62,1r2gwb,askscience,new,113
50bmg,"1 atom won't do much unless it contains extraordinary amounts of energy (see: http://en.wikipedia.org/wiki/Oh-My-God_particle). 


A stream of protons (essentially hydrogen nuclei) would do exactly this if carefully controlled:
http://en.wikipedia.org/wiki/Proton_therapy


A more energetic beam would do this:
http://en.wikipedia.org/wiki/Anatoli_Bugorski


Essentially what happens is the proton will likely collide with another atom (although many will make it all the way through without hitting anything!). Depending on the energy of the proton, and the type of atom it collides with - several types of atomic reactions could occur, most of them altering the molecule it collided with quite dramatically, or releasing heat and various types of radiation. In the worst case - it would break or alter a DNA strand and potentially cause cancer, however in most cases the cell with the damaged DNA would just be unable to replicate and therefore die. ",null,7,cdj40hn,1r2gwb,askscience,new,42
fillterfood,"Astronauts in space report seeing flashes in their vision when they close their eyes. It is thought that these are caused by cosmic rays (nuclei of atoms) shooting through them and hitting their retinas. While I imagine some level of damage is happening, it doesn't seem to be too dangerous.

The most obvious danger is an increased risk of cancer. If the atoms collide with DNA, they can break it apart. While a single killed cell here or there isn't much of a problem in our bodies. It's when the cell gets reprogrammed to start growing endlessly that it becomes a real threat to our health.

So I think the answer to your question is a bit of a yes and no. Physically, there seems to be no risk, biologically however, combined with a bit of bad luck, it can kill.",null,11,cdizsy1,1r2gwb,askscience,new,40
EdPeggJr,"The [Oh My God particle](http://en.wikipedia.org/wiki/Oh-My-God_particle) was a single particle, likely a proton, which hit with a force ""equal to that of 50 Joules, or a 5-ounce (142 g) baseball traveling at about 100 kilometers per hour (60 mph)."" Detectors have confirmed 15 similar events.",null,4,cdj41fd,1r2gwb,askscience,new,15
bertrussell,"This happens to everyone, all the time. There are cosmic radiation sources and also terrestrial radiation sources. Sure, radiation has its issues, but the body can actually handle a fair amount of radiation throughout a person's lifetime.

http://video.mit.edu/watch/cloud-chamber-4058/",null,2,cdj7wu3,1r2gwb,askscience,new,12
dddm,"A single atom or particle (or photon) passing through a human body would never be able to cause much damage, although a beam containing many individual particles certainly could.  There is a range of energies that are most damaging and it's not accurate to say that higher energy radiation sources always produce greater radiation damage.

If the energy is too low, below about 5-100 eV depending on which [definition](http://en.wikipedia.org/wiki/Ionizing_radiation#Ionization_and_the_definition_problem) is used, the radiation is not energetic enough to ionize the target material.  This type of non-ionizing radiation is generally not much of a concern (sitting near a normal light bulb probably won't cause much long term damage), but there still are some [damaging effects](http://en.wikipedia.org/wiki/Non-ionizing_radiation#Health_risks) mostly due to heating in the target material.

Above the ionizing threshold, radiation damage generally increases with energy up to a certain point.  This is because radiation damage is generally proportional to the total energy deposited.  If the energy of the incident particle isn't too high, it will stop within the target material, indicating that all of its kinetic energy was deposited into the target.  In this regime, higher energy particles will always lead to a larger radiation dose.

However, the situation isn't as clear when the incident particle has enough initial energy to pass through the target.  Even if a particle passes through the target, much of its initial energy may be deposited in the target material along its path.  The rate of energy deposition (called the [stopping power](http://pdg.lbl.gov/2006/reviews/passagerpp.pdf)) as a function of depth within the target is described by the [Bragg curve](http://en.wikipedia.org/wiki/Bragg_peak).  As an approximation, the shape of the Bragg curve depends mostly on the species and initial energy of the incident particle, and on the density of the target.

The rate of energy deposition by a particle generally decreases for shallow depths as the particle initial energy increases.  Thunderf00t has an excellent video describing this effect (the pertinent discussion is towards the end of the video, but the whole video is relevant):

https://www.youtube.com/watch?v=oj6v8MtuVdU

For very high energy, the thickness of a human body may be a very small portion of the maximum radiation depth, and the stopping power in this depth range would approach zero as the particle energy increases.  So there exists a threshold energy above which the particle actually does less radiation damage compared to (comparatively) lower energies.

Estimating this threshold energy for simple geometries, such as a human hand in a large vacuum, can be done by integrating the stopping power up to the target thickness and comparing the result for different energies.  For more complicated geometries, for example those involving non-vaccum materials near the target that may produce secondary particles when exposed to the initial radiation, the system generally needs to be simulated using particle tracking toolkits such as Geant4, FLUKA, or MCNP.

In any case, a single particle cannot impart an infinite amount of energy into a target material.  The maximum deposited energy would be on the order of 100's of MeV for protons, yielding a single proton maximum effective dose of maybe 100 nSv.  This is an negligible dose compared to [other common exposures](http://en.wikipedia.org/wiki/Orders_of_magnitude_(radiation\)), such as the daily background dose of about 10,000 nSv.

A string of particles, which would be a model for a particle beam, can certainly lead to intense radiation damage, since a typical particle beam may contain on the order of 10^10 individual particles.",null,2,cdjaj1n,1r2gwb,askscience,new,7
madscientistuk,"Sixty Symbols (a video series from the University of Nottingham about Physics, Astronomy and Maths) answered a similar question which I think is relevant.

As part of the video series they have a number of videos where they asked physicists, astronomers and maths lecturers questions submitted by the sixty symbols viewers. The first question of the 2nd video about viewers questions was:

""If I put my hand in front of the beam at the Large Hadron Collider, what would happen to my hand.""

The responses are great in my opinion :) [YouTube video - Putting your hand in the Large Hadron Collider...](http://www.youtube.com/watch?v=_NMqPT6oKJ8) question answered 0.00-3.54.

This was so popular they then had a second video where they asked scientists working at the Large Hadron Collider the same question. Again I think the answers are great.

[YouTube video - Hand back into the Large Hadron Collider - Sixty Symbols](http://www.youtube.com/watch?v=lVefgfmFg9o)

Edit - hopefully fixed the links",null,1,cdjemqa,1r2gwb,askscience,new,5
yinz_n-at,"Depends. Lost of Probability involved. The higher the energy the more damaging but also the more likely it'll just go straight through you (gamma ray) . Low energy particles are less damaging but more likely to collide with a nucleus in your body (atoms are vastly empty).

A neutron from radioactive decay is pretty damaging because it has a high probability of colliding with a human nucleus. 

And one nuclei isn't all that damaging because the probability of one particle causing damage is very small. Now a flux or beam of particles will do some serious damage. 

Side Note: Ingesting a radioactive material is pretty dangerous because every decay product will be absorbed by your body. Alpha particles are pretty easy to shield against but if theyre not shielded then they'll definitely knock some nuclei around (worst case is knocking a DNA nuclei out causing mutations.) 

Hope that helps!

Source: Nuclear Engineer",null,1,cdjawm3,1r2gwb,askscience,new,5
KillerInYourCloset,"Sorry, this might sound *very* unscientific- but wasn't there an example of someone who put their hand in front a particle collider? I believe they had really no adverse affects. Sorry, I can't look into it much I'm on a sev A call, but not really listening unless I hear my name :)",null,0,cdjb6yn,1r2gwb,askscience,new,3
OnlyOneWithThisName,"I believe this question is a very important obstacle to consider in regard to interplanetary space travel within the immediate solar system, and most likely long distance interstellar travel as well. This article about the [health threat of cosmic rays](http://en.wikipedia.org/wiki/Health_threat_from_cosmic_rays ""en.wikipedia.org"") will only partially answer your question, but I believe it is a worthwhile read.",null,0,cdj9o6m,1r2gwb,askscience,new,2
antpuncher,"You have trillions of neutrinos passing through you every second, and you're doing just grand.

The important piece is how much energy the particle can deposit in your body.  This depends on how much energy the particle has, and what the ""interaction cross section"" is.  Like it sounds, the cross section is kind of like how big it is, but for quantum mechanical things it's not really a property of the _size_ of the object, and has to do with things like the electrical force.   

Then it also depends on where it goes.  You could probably take a packet from the LHC through your ear lobe and only lose some skin, but a few wrongly placed alpha particles and you have brain cancer.",null,6,cdj9rbr,1r2gwb,askscience,new,8
websterandy42,"What you are talking about is a very rare scientific concept known as radiation. Haha,  one of the most common types of radiation (and the first to decompose after a nuclear reaction) is literally a helium nuclei. While your cells are tough enough not to be hurt by small amounts. What it really affects is DNA, DNA does not heal so damaged DNA can will work but not properly, the cells will begin reproducing at an increased rate. This is how tumors form.",null,0,cdjmbsd,1r2gwb,askscience,new,2
recycled_ideas,"It depends if it hits anything,  and if so what. At the subatomic scale there's a lot empty space and any given particle could fly right through you doing no damage quite easily. Of course it could also hit something and damage it.  Enough particles and enough hits could kill you. A few hits could cause cancer and kill you over time. Or your body could repair the damage and nothing happens. 

You're actually being hit by particles all day. ",null,0,cdjotgd,1r2gwb,askscience,new,1
denchpotench,"Charged particles would cause a lot more damage than uncharged ones. For example beta radiation is just fast moving electrons or positrons. These have charge and can ionize atoms, if these are ionised at certain points in strand of DNA it can cause the parent cell to die or become cancerous. If an uncharged atom flew through you it might take a few atoms out on the way but would affect far fewer than a positron stripping electrons from many atoms.",null,0,cdk0oc3,1r2gwb,askscience,new,1
PricaCells,"These are known as sun-sensitizing drugs. This is because they cause certain side effects only when in direct sunlight, such as photoallergy, or phototoxicity. The latter is more common, and it occurs when the skin is exposed to sunlight after certain medications are injected, taken orally, or applied to the skin. The drug absorbs the UV light, then releases it into the skin, causing cell death. So, that's what I can tell you. ",null,1,cdixufz,1r2id5,askscience,new,19
null,null,null,0,cdjhvqy,1r2id5,askscience,new,1
Surf_Science,"A tentative contig should be just what it sounds, a draft contig. 

For those who don't know what a contig is. 

DNA is sequenced in very small bits, usually 75 to ~800 base pairs. This bits are lined up to make a longer sequence. A contig is one of those alignments and contigs are usually millions and millions of base pairs. A contig of the mouse genome for example could be more than 50 million base pairs. ",null,0,cdizwc9,1r2isv,askscience,new,5
PraecorLoth970,"I attended a lecture that addressed exactly this issue. It was by Dr. Gerald H. Pollack who has a lot of studies published on water. I've found a video lecture which looks similar to the one I watched, so I recommend you watch it in its entirety. It was fascinating. So many things that I took for granted and he, with compelling evidence, showed how wrong many of my assumptions were.

The video lecture is [here](http://www.youtube.com/watch?v=XVBEwn6iWOo). The part about clouds is at about [48:30](http://youtu.be/XVBEwn6iWOo?t=47m22s) but in order to understand the explanation, you need to watch the entire thing.

If I may try a TL;DW: It's because on many interfaces, water molecules aggregate on a liquid crystalline structure which he proposed as being similar to that of ice, which is negatively charged (this is called the exclusion zone). This would give water droplets's surface a negative charge, and the atmosphere is positively charged (ground is negative, atmosphere is positive). This results in the negatively charged water droplets to attract each other (like likes like. Weird, huh?), because between two negatively charged surfaces of droplets there would be a bigger concentration of positive atmospheric charges, and the droplets would be attracted to this, and would move towards each other, and thus coalesce into a cloud. There is a [TEDxTalk](http://www.youtube.com/watch?v=i-T7tCMUDXU) which is shorter. Youtube has lots of other videos with him.

Edit: Made my TL;DW a bit more clearer and more accurate.

Edit 2: Some images from his articles and videos.

* [Abstract 1](http://i.imgur.com/XLXVqHT.png)
* [Abstract 2](http://i.imgur.com/c2PrPxg.png)
* [Abstract 3](http://i.imgur.com/XkpMnkv.png)
* [Abstract 4](http://i.imgur.com/nncKR1w.png)
* [Exclusion zone](http://i.imgur.com/tUUF8nq.png)
* [Charge in the exclusion zone](http://i.imgur.com/aOlwvlT.png)
* [Tube with EZ; Produces flow without pressure difference. Energy source is ambient radiation, infrared](http://i.imgur.com/euZ0PvO.png)
* [Positive region created due to negative spheres.](http://i.imgur.com/ckAQYVq.png)
* [Negative region created due to positive spheres](http://i.imgur.com/vzz3DZH.png)
* [Like attracts like](http://faculty.washington.edu/ghp/images/stories/likelike.png)
* [Formation of the exclusion zone (Video)](http://vimeo.com/7294988)
* [Water droplets on water surface (Video)](http://vimeo.com/7279153)
* [Pollack laboratory webpage](http://faculty.washington.edu/ghp/)
",null,8,cdj5gbr,1r2j3i,askscience,new,17
SpicyBuffaloFeather,"Based on my understanding of meteorology: air with differing humidity levels and temperature do not readily mix, especially in fairly calm weather. Instead, a parcel (bubble) of warm air will rise if surrounded by cooler air until it cools adiabaticaly due to expansion. When this happens, the relative humidity of said parcel approaches 100% (relative humidity is a measure of how much water vapor a given unit of air is holding vs. how much water vapor it can hold before it is totally saturated. When the parcel cools sufficiently, it reaches the saturation point and water vapor begins to condense on condensation nuclei (fancy science terminology for bits of dust floating around in the air). If enough water vapor condenses on said nuclei, it will become heavy enough to fall. When it falls a process known as ""collision and coalescence"" occurs which basically means the water droplet hits other water droplets and combines to make a bigger droplet, thus increasing the speed at which it falls. However, a raindrop can only get so big before friction with the air causes it to break apart and the process starts anew. 

Fun fact: rain drops do not actually look like like [this](http://www.psdgraphics.com/file/water-droplet-icon.jpg) but infact look like more like [this](http://www.sailingissues.com/raindrop-shape-evolution.png)

Edit: I realize I kind of digressed from your question there. To answer your question, differential heating of the earth's surface is why water vapor does not evenly distribute itself throughout the atmosphere. Warm air can hold more water vapor than cold air and in general, warm air at the equator moves toward the poles while cold air at the poles moves to the equator in a futile attempt to achieve equilibrium. This overly simplistic model is based on the [hadley cell](http://serc.carleton.edu/images/eslabs/hurricanes/3d_hadley_md.v3.jpg). ",null,1,cdjeyox,1r2j3i,askscience,new,6
__Pers,[This article](http://www.scientificamerican.com/article.cfm?id=why-do-clouds-always-appe) in *Scientific American* addresses your question in some detail. ,null,2,cdiz70k,1r2j3i,askscience,new,9
rocketsocks,"Water vapor is transparent, you literally cannot see it.

When you have a pot of water on the stove boiling and you see the ""water vapor"" rising above it's not the vapor you see it's tiny water droplets recondensing as it cools off in the air. Those tiny droplets are still hot (nearly 100 deg. C) and they are easily blown around by air currents, which is why they continue to rise.

Clouds are visible because they are composed of water droplets, not just vapor. ",null,1,cdjbhmp,1r2j3i,askscience,new,2
meaningless_name,"Does coarsely ground coffee make a stronger cup? I don't think it does. The higher surface area of finely ground coffee maximizes surface area exposed to water, as you pointed out. This definitely means fine grinds will produce stronger coffee in a given amount of time.",null,3,cdiz3ra,1r2jdi,askscience,new,24
2dwgs,"Coarsely ground coffee is usually exposed to water for more time (french press, for example) which leads to a bold cup of coffee. But finer ground coffee has more surface area exposed, which leads to a stronger flavor when comparing coarse vs. fine *given everything else is the same* (brew time, water amount, origin, roast, etc.)",null,0,cdizqhy,1r2jdi,askscience,new,8
endocytosis,"Not a barista, but increasing the grinding time on something has an effect on surface area.  Here's an example:

4 coffee beans, 16 x 16 x 16, total volume = 16384 units (4096 units each, assume for simplicity the coffee is cubic in shape)
SA = 6*L^2 = 6*256 = 1536 x 4 = 6144 units total surface area

The coffee goes through a special grinder and the grinds are perfectly halved in size to be Coarsely ground coffee:
8 grounds of coffee, 16 x 16 x 8 = 16384 total volume (2048 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*16) + 2(16*8) + 2(16*8) = 8*(512 + 
256 + 256) = 8192 units total surface area

This makes sense.  You don't put coffee beans into a coffee maker, you grind them up.

The coffee again goes through the special grinder and the grinds are perfectly halved in size to be Finely ground coffee:
16 grounds of coffee, 16 x 8 x 8 = 16384 total volume (1024 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(16*8) + 2(8*8) + 2(8*8) = 16*(256 + 
128 + 128) = 8192 units total surface area

No Change.  So we keep on grinding...

The coffee once again goes through the special grinder and the grinds are perfectly halved in size to be Super Finely ground coffee:
32 grounds of coffee, 8 x 8 x 8 = 16384 total volume (512 units each, no size is lost)
SA = 2ab + 2bc + 2ac = 2(8*8) + 2(8*8) + 2(8*8) = 32*(128 + 
128 + 128) = 12288 units total surface area

Now the surface area is greater, but it is worth noting that the relationship between surface area and grind time isn't exactly linear.  However, the more finely ground something is, *usually* the slower it is that the coffee passes through.  More coarse grinds are bigger and don't fit together as well, so there's more room for water to pass around the bits and pieces.  Finer grinds will pack together more tightly and slow the passage of water, probably increasing flavor and caffeine extraction.

TL;DR: Grinding coffee more increases surface area, but you need to fine-tune your grinding to get the maximal effectiveness.  Also the finer the grind, the longer the water will sit.",null,2,cdj21cy,1r2jdi,askscience,new,3
null,null,null,13,cdize84,1r2jdi,askscience,new,9
fishify,"In the first several minutes after the Big Bang, the first nuclei formed.  At this time, this matter consisted of about 75% hydrogen and 25% helium; there was a little bit of lithium and beryllium formed, too.  (Hydrogen nuclei are the easiest to form -- quarks bind to form nucleons, free neutrons are unstable, and protons are single particles, whereas nuclear fusion processes are needed to generate even the slightly heavier nuclei.) To this day, we see that the matter in the universe is around 3/4 hydrogen and nearly 1/4 helium.

Atoms, rather than nuclei, did not become stable till the universe was about 370,000 years old; prior to this, the universe was so hot that electrons and nuclei did not reliably bind.

The nuclei heavier than the ones made right after the Big Bang are primarily formed via the nuclear fusion processes that go on in stars.  The first stars formed when the universe was around 200 million years old.  Over time, stars convert some of their matter to heavier nuclei: first helium, and then heavier nuclei (especially carbon, nitrogen, oxygen, neon, magnesium, silicon, sulfur, and iron, but other nuclei up to iron as well), and then nuclei beyond iron arising during supernova explosions.

So the short answer answer is that a lot of hydrogen was formed in the early universe, and all the subsequent nuclear processes have only converted a small fraction of that.",null,0,cdiy1qz,1r2kqn,askscience,new,8
f4hy,"Hydrogen is the simplest atom. It is just an electron orbiting a proton. In the early universe, the aftermath of the big bang, there were processes which could create protons, but protons repel each other unless you can get them really close together. So in the aftermath of the big bang, we were left with a bunch of protons, and electrons were able to find them, but it was not until stars formed and nuclear fusion at their cores to fuse the protons together into heavier atoms.",null,0,cdixjmh,1r2kqn,askscience,new,4
do_od,"A full water bottle would not be crushed because water is very incompressible. [This experiment](http://earth.geology.yale.edu/~ajs/1969/ajs_267A_11.pdf/70.pdf) shows that in order to press a liter of water into a 9 deciliter bottle you have to subject it to a pressure equivalent to that on the bottom of a 33 km ocean, three times deeper than the Mariana trench which is the deepest known on Earth. Salt water and oil is similarily incompressible. ",null,0,cdizgkk,1r2lsg,askscience,new,19
blackhawk0093,"Deep-sea ecologist here. When we send down our collection chambers we ALWAYS fill them with surface water. As mentioned earlier on this thread, water isn't very compressible so going down 1,000 meters or so isn't going to cause a huge effect. On the other hand, there's been a few times when people forgot and sent them down with air, and they literally imploded on the side of the sub. This is (partly) also why all of the electronics on the sub are encased in oil- it won't short everything out like water or implode like air, and you can actually increase the pressure as the sub descends so it is always under a slightly higher pressure than the surrounding seawater (so small leaks will leak out, not in - better to lose some oil than gain some seawater). ",null,0,cdjbeer,1r2lsg,askscience,new,8
topher-dot-com,"The bottle would shrink but it wouldn't be crushed, because liquids aren't very compressible. [Here] (http://docs.engineeringtoolbox.com/documents/309/water-density-temperature-pressure_2.png) is a graph of the density of water as a function of temperature and pressure. even at 200 times ambient pressure the density only increases by about 1%. 

It doesn't really matter what is in the bottle as long at it is completely filled with liquid then it won't be crushed because liquids aren't very compressible.

Bathyscaphes, use gasoline to trim buoyancy because it is a liquid (so it won't compress at those depths) but it is less dense than water. So it is used to help it return to the surface since the ballast tanks that have air in them on the surface now have water, which can't be pushed out because the pressure is too great.",null,2,cdizvs2,1r2lsg,askscience,new,4
Thew_Nell,"This is probably a bit off topic, but you might be interested in the movie ""The Abyss.""  One of the characters is able to withstand incredible deep oceanic pressure because of this idea.  He 'inhales' a liquid that replaces the gasses in his body to reduce compression.  However,  this isn't entirely accurate, because it is a movie, but I thought I would bring it up regardless.
",null,1,cdj52ec,1r2lsg,askscience,new,3
S7R4nG3,"Ok, from what I'm remembering when going through dive training, if you take a full perfectly water-tight waterbottle that allows no water to escape regardless of pressure and that imparts no bouyancy, then yes it would crush as you moved it further down the water column. 

When you fill the water bottle at the surface, you are sealing in the water at the ambient sea-level pressure, thus as you moved it down the water column, the pressure around it increases crushing the bottle to compress the contents to ambient pressure. 

I specify contents here because water at sea-level generally has more dissolved gases that would allow the bottle to compress somewhat.

When considering similar effects of different liquids, its quite interesting to look at older [bathyscaphes](http://en.wikipedia.org/wiki/Bathyscaphe) that used gasoline to trim their bouyancy due to its higher density than water. You can also read up on [Henry's Law](http://en.wikipedia.org/wiki/Henry's_law) that defines the solubility of gases in liquids at pressure.",null,11,cdiz9wf,1r2lsg,askscience,new,3
puma721,"The idea of a fan isn't that it makes your room cooler (although ceiling fans can move cold air from the ground upward, or hot air off the ceiling)
The benefit a fan gives you is that you feel cooler because the air is more efficient at pulling moisture off your skin.",null,0,cdj2jp7,1r2n13,askscience,new,14
LogicForDummies,"Depends on three factors mainly. 1) The insulating value of the ""walls"" 2) the temperature difference between the ""inside"" and the ""outside"" of the sealed room and 3) The amount of heat produced by the fan motor.

If the outside temp is lower AND the insulating value allows enough heat to pass through the walls at a rate higher than the heat added from the motor AND the rate of thermal transfer is higher with the air moving over the ""walls"" (think heatsink) versus static air after factoring the added motor heat, then it will cool the room. Otherwise it will warm the room. But if you were in the room (which also adds heat) you would still ""feel"" cooler with the fan on even though you could be heating the room, up to a point.",null,0,cdiygfh,1r2n13,askscience,new,9
S7R4nG3,"This question is highly dependent on what you have inside the room ""measuring"" the temperature?

If you are taking a person inside and using their sense of temperature as a ""measurement"" then yes, they would feel cooler as a result of normal sweat evaporation on the skin.

If you are taking a thermometer in the sealed room, then it will always read the ambient temperature of the room as it is uninhibited by the effects of [wind chill](http://en.wikipedia.org/wiki/Wind_chill#Explanation).

So, with a dry thermometer at the stable room temperature, the effect of adding a ceiling fan to the room would not change its reading. 

However, because you state that the room would be warmer than the ambient temperature around the room, the effect would be just as you specify, it would increase the rate by which it cooled to the ambient temperature.",null,0,cdiyfx5,1r2n13,askscience,new,5
InternalEnergy,"Chemical engineer here--lots of training in thermodynamic analysis.

Let's make some assumptions. First, the room is completely sealed off to flux of matter. Not a single atom passes through the doors, walls, etc.

Second: the room is well-insulated, so an adiabatic assumption is valid. No heat is transferred through the boundaries of the room.

Third: the boundaries/walls are completely rigid. They will not move at all even if the pressure inside the room increases.

Ok. Now let's do some thermodynamics. First law of thermo states that matter and energy is conserved: neither created nor destroyed. With no heat flux (adiabatic), matter-material flux, or pressure-volume work (rigid boundary), the only input to the room is the electrical energy required to power the fan. Moving stuff takes energy--air is no exception, and certainly the blades of the fan have mass too. So we have energy coming in, in the form of electricity.

Ever feel the heat coming off of your computer or smart phone during use? Electrical energy converts easily to heat.

But we insulated the room, remember? And we sealed it too, so no hot air leaves, to take that energy away. So the end result is the room gets warmer. 

In fact, the only reason why a fan might make you perceive the room as cooler is because you sweat. The moving air evaporates that sweat, which requires heat energy, which your body supplies. 

Yay, thermodynamics!",null,0,cdj4a4m,1r2n13,askscience,new,3
blacksheep998,"It has more calories in the sense that hot things have more energy than cold ones. But it wouldn't have more usable calories. Not directly at least since the body cannot directly convert heat into a usable calorie source for metabolic action.

Indirectly though it might give us a little more energy. When you eat something it enters your stomach and rapidly changes to body temperature. In the case of something cold this means it absorbs heat from your body. This heat has to be replaced and the body does so by burning calories.

If the food you ate was warm then the heat energy would be absorbed by your body which would not need to burn as much to keep warm, for a few minutes anyway.

I'm on my phone so I can't do any calculations right now to see what exactly the difference is, but its not a lot. Not what you'd consider a 'significant amount.'",null,8,cdiym89,1r2n1j,askscience,new,56
Gibonius,"Most foods have specific heats around 0.6 to 0.8 kcal/kg*C.  A steak has around 2000 kcals/kg in chemical energy.  If we look at a +40C change in temperature (reasonable), we're looking at 26 kcal/kg (for steak).

That's 1.3% more energy, which I'd say counts as ""not significant."" 

That's neglecting any consideration of how the body uses thermal energy.  I'd be reasonably comfortable assuming it's far less than 100% efficient.  ",null,5,cdiz4o9,1r2n1j,askscience,new,23
Oznog99,"Only in one specific case of hypothermia.

The body produces about 100W of heat, and in normal operation regulates internal temperature to ~98.6F by regulating blood flow to the skin and perspiration.

If you drink 1L (1KG) of icewater at 0C (hopefully this takes awhile), this will steal 37,000 ""small calories"" (the proper chemistry term) , which would be 37 ""food calories""/""large Calories"", to warm it up to body temp.  This is confusing, but the calories for food you know are actually ""kilocalories"".

But the body doesn't work that way.  As you drink icewater, the body senses a small temperature drop and reduces blood flow to the skin.  If you were sweating before, you'll stop sweating now. The skin is not as warm and stops losing as much heat to the environment.  

Soon the same generation of heat in the body, with a *reduction* in heat loss through the skin, restores temp to normal and everything goes back to how it was before.

In general the body does NOT increase metabolism to restore heat, so the cold water (or food) does not count against your dietary calories.  

There is an exception, and that's ""shivering"", an automatic emergency defense.  Shivering can consume 252 kcal/hr!  IF you were shivering, and drank hot unsweetened tea with no dietary calories, the added heat will reduce the shivering condition and thus decrease the amount of calories you burn during that day.  

It is an unusual case, though.  And a terrible concept for a diet plan.  As noted, simply turning down the AC to 58F will not cause your metabolism to increase at all.  You must be *shivering*, which is very uncomfortable, and wreaks havoc on body processes because a hypothermia is an emergency condition the body is responding to.  You can't think clearly, digestion may slow down or speed up, your muscles may cramp.  Greater calorie consumption can be had simply with moderate exercise, and going for a walk is far more enjoyable- and effective- than trying to take a bath in icewater.

",null,2,cdjd0b7,1r2n1j,askscience,new,7
CanadianSnow,"There are two things to consider, when both dishes have the same amount of potential calories
1) the amount of body heat required/expended to heat the cold food as it enters your body so it can be digested. Our bodies do not digest cold food, even ice cream is first heated to body temperature before it can be digested fully.
2) The thermogenic effect of food relates largely to spiciness of food, however a very hot dish will cause the body to expend calories via sweating to cool the body down from the heat/spiciness of the food.",null,0,cdj4g7x,1r2n1j,askscience,new,2
rightwaydown,"Actually food cooling can undergo chemical reactions changing it's exact extractable energy. For instance potatoes left to go cold have a higher percentage of resistant starch. 

I'm can't think of many examples though. In general I think they would be very similar. Of course there are many examples of food gaining calories from being cooked, but that's not your question.",null,0,cdjnarv,1r2n1j,askscience,new,2
ModernTarantula,The Calories of food is that which is generated by burning it completely in a closed stove. Our metabolism is the breakdown  and rebuilding of our body. The calories needed for that is only chmical not thermal. If we wanted to save energy used to maintain body temperature we should use the stove to heat the air not the soup.,null,0,cdjgo5l,1r2n1j,askscience,new,1
atomfullerene,"The fundamental continental arrangements which lead to an ice age (namely the formation of the Isthumus of Panama, and, for Antarctic ice, the Drake Passage)  have not changed in the past few thousand years, so barring anthropogenic climate change, we'd be heading into another glacial period as soon as we swing back around the proper point in the [Milankovitch cycle](http://en.wikipedia.org/wiki/Milankovitch_cycles).",null,0,cdjpewm,1r2ppd,askscience,new,3
ModernTarantula,"Just saw on TV. The Earth wobbles on its axis. At a certain wobble winters would be shorter The orbit also changes from more elliptical to less. Less elliptical the winters are warmer. Both those conditions are current.  However we are closer to the next glacial period, due to be in 1500 years.",null,2,cdjh7qp,1r2ppd,askscience,new,2
iorgfeflkd,"There really isn't much antimatter in the known universe. Why, is a mystery. Positrons are created fairly frequently and they often annihilate with electrons. If you look for positron-electron annihilation in the sky you see it [coming from the galactic center](http://www.cesr.fr/~jurgen/spi/spi511map-pr-hres.jpeg).",null,2,cdj006a,1r2pq1,askscience,new,16
miczajkj,"It mostly does. When for example an positron is produced in the ß^+ Radiation (what happens rather often), it ionizes the medium it is moving in and slows down until it finally annihilates with an electron.

The biggest part of antimatter in our universe consists of anti-neutrinos: just like neutrinos they just don't interact with other matter/anti-matter very often and are therefore very likely to have a long lifespan. They are for example produced in the ß^- radiation. 

As we know so far, there is no particular difference between matter and anti-matter that can explain, why there is much more matter, so much more, that anti-matter can't really enjoy it's existence until it annihilates with regular matter. This is one of the unsolved mysteries of particle physics. ",null,0,cdj09t7,1r2pq1,askscience,new,3
selfification,"So you know how we write decimal numbers?  What exactly are we doing?  We're trying to represent numbers as digits that we ascribe a meaning to based on their place in the sequence.  If our number is in base b and our digits are a0,a1,a2...  Then the number we are trying to represent is a0 + a1*b + a2*b^2 + ...  Power series generalize this concept.  In a power series, your coefficients can take on any value and you can use an arbitrary base.",null,1,cdjg3jo,1r2u62,askscience,new,4
DarylHannahMontana,"Conceptually, I think of power series (and Fourier series) like I think of a chemical compound having a ""recipe"" in terms of its component elements. For instance, the chemical aluminum silicate is 

   Al_2 O_7 Si_2

i.e. to make an aluminum silicate molecule, you need 2 aluminum, 7 oxygen and 2 silicon. 

In a similar way, we can decompose a function in terms of simpler functions (polynomials or sine and cosine), though there are some obvious differences, mainly that you can have any amount of each component (unlike compounds/elements, where only integer multiples happen), and there are an infinite (but [countable](http://en.wikipedia.org/wiki/Countable_set)) number of ""elements"" to consider. For instance, the function e^x and its Taylor series

   e^x = 1 + 1 x + 1/2 x^2 + 1/6 x^3 + 1/24 x^4 + ...

give you a ""recipe"" for e^(x): you need a 1, an x, half of an x^(2), one-sixth of an x^(3), etc.

Alternatively, if you already know some linear algebra, a more directly parallel idea is that of a vector space, and the idea of a basis. Recall that a basis for an n-dimensional vector space is a collection of n vectors {v_1, v_2, ... v_n} that ""span"" the entire vector space. That is, every other vector can be written

   w = c_1 v_1 + c_2 v_2 + ... + c_n v_n

with *some* choice of numbers c_1, ..., c_n.

This analogy is especially apt for Fourier series, where there is a notion of orthogonality; i.e. you can rigorously define what it means for two functions to be ""perpendicular"" and if you know some linear algebra, you know that this can make bases much easier to work with.

There are also ""series expansions"" of functions that are much more general than power series or Fourier series; depending on the setting and application, there are reasons that make some of these more ""exotic"" series better suited to the task at hand. If you're interested, you might look up [wavelets](http://en.wikipedia.org/wiki/Wavelet).",null,2,cdjop6k,1r2u62,askscience,new,4
aczelthrow,"Power series are used in two major ways.

Often in analysis (calculus and such) power series are simply types of functions that behave like polynomials of infinite degree. Sometimes they only converge (or make sense) for particular values of the variable. Sometimes we write other functions in terms of power series, such as exp(x) = sum(n = 0 to infinity) x^(n)/(n!). There are methods to figure out how to write a given function as a power series. In that sense, a power series is way to represent a function that is often more convenient. For instance, if you had paper and pencil only, how would you compute exp(2)? The best way is to use the power series. (Many calculators and mathematics software internally use power series, or something related, to compute exp, sin, cos, etc.)

In algebra, by contrast, we sometimes use ""formal"" power series. A formal power series looks just like a usual power series but it's regarded as its own object divorced from any notion of values of x for which it converges or not. The coefficients of the formal power series may be chosen from a given ring or field (generalizations of real numbers). We can add, multiply, even divide formal series using the usual rules, and doing so we define a ring or field of power series that are of interest to algebraists. This kind of power series is less encountered in applications.",null,1,cdjhknw,1r2u62,askscience,new,2
Spiralofourdiv,"Which part is confusing, the ""power"", or the ""series"" part?

It's important you understand what a series representation of a function is fairly well before you can understand what makes a power series special. A power series is just a clever way of rewriting a function as a, perhaps infinite, polynomial using its derivatives and a center. How it works takes a bit of calculus background. If you have a calculus background and are still mystified, then your hang up probably will be answered sometime throughout a real analysis course. 

However, I have a feeling exploring series' in general as representations of functions might answer part of your question. A great example is a Fourier Series, which is a way to describe **any** periodic function as a sum of sines and cosines. First, consider and understand [this](http://en.wikipedia.org/wiki/File:Sine_curve_drawing_animation.gif) illustration of sine. Now look at what a [Fourier Series](http://i.imgur.com/w1IuPKJ.gif) looks like in the same fashion. If you were to add an infinite number of circles, you'd have a square wave, even though it's simply a sum of the very-non-square trig functions. Each additional circle is just another sine or cosine term in the series, and this makes sense in the picture! A little circle is added on that behaves the same way. The specific radii and angular frequency will depend on the series, but study this gif and it will make sense how the series approaches a square wave.

Most series work in a really similar way: they start out as a broad approximation and then, as more terms are added, get closer and closer to the function. Sometimes you can describe the function in a finite number of terms, and sometimes you need an infinite amount. A power series is no different: each term in the sequence of partial sums is a more accurate approximation of the function than the proceeding element. With an infinite amount of elements, we get infinite accuracy and we're done.

If you're asking WHY an infinite series is easier to deal with than the original, nice looking function, all I can say is there are too many reasons to count, or at least applications to count. Most of the time it has to do with certain operations being significantly easier to do on the series representation.",null,0,cdkx38e,1r2u62,askscience,new,1
wwarnout,"For stars whose axis of rotation is not pointing toward us, we can measure the light (spectra) of the left and right sides of the stars.  Since one side is moving away from us, and the other is moving toward us, there is a shift in the spectra (similar to the doppler shift that causes a car coming toward us to sound different than when it goes away from us).

By observing this difference, astronomers can calculate the rotational speed.",null,0,cdj6zlq,1r2xu4,askscience,new,4
adamsolomon,"The Milky Way is quite a standard spiral galaxy. Nothing especially out of the ordinary. The Milky Way, by the way, is filled to the brim with stars with planets, and it's quite likely that some of those have life. Already we're beginning to find planets almost like Earth around other stars, and those are generally hard to find - not to mention we can only look in our solar neighborhood. The same goes for any other decently-sized galaxy. So it's not as if life on Earth is necessarily a rare feature in itself, either.",null,0,cdj5orq,1r2xuh,askscience,new,7
datums,"This is a politically loaded question, but in general, the answer is no. Most of the significant gains have come as a result of regulation by governments. Two examples of this are CFCs (chlorofluorocarbons), and PCBs (polychlorinated biphenyls). Though they were both highly damaging, and demonized in the press, they were both widely utilized in markets that had not banned them.      
  
A potential reason for this is that their use is not always easily tied to particular consumer products. For example, PCBs were used mainly in the operation of heavy machinery. It would be hard to say that a product was 'PCB free', given the number of mining, refining, and manufacturing steps involved in making most consumer products. One might be able to say that a pencil is PCB free, but it would be difficult to say that none of the machinery involved in any of the steps required to make a pencil were PCB free. 
  
",null,0,cdja8ra,1r2yus,askscience,new,1
ScootMaBoot,"A penny would sink.

[The density of water at the bottom of the Mariana Trench is only ~5% greater than at the top.](http://en.wikipedia.org/wiki/Mariana_Trench)

Liquid water (and liquids in general) are [not very compressible](http://en.wikipedia.org/wiki/Compressed_fluid), especially when compared to gases.",null,2,cdjc7my,1r2z86,askscience,new,7
varodrig,"Since liquids as virtually incompressible, water has virtually the same density even at the bottom of the ocean. The density of copper is 8960 kg/m3. The density of water is 1000 kg/m3. Therefore a penny would sink all the way to the bottom without much noticeable slowing.",null,0,cdjk3iu,1r2z86,askscience,new,3
jofwu,"Sounds to me like you're mixing up pressure and density. While a gas's density is directly related to pressure, this is not typically the case for liquids (including water).

Buoyancy is the force responsible for making things float, and it is a function of the relative densities of the fluid and the object. Water pressure is not involved.",null,0,cdjz5t6,1r2z86,askscience,new,1
OrbitalPete,"Erosion is overwhelmingly responsible.

Once the plate has moved sufficiently for an island to lose its source magma, there is no replacement material being supplied. AGes along the chain look like this: http://www.uhh.hawaii.edu/~kenhon/GEOL205/Chain/chnmap.jpg

It's basically impossible to say exactly how big the other islands and seamounts were at their peak, but there's no reason they couldn't be similar sizes to Hawaii. There may also have been smaller islands grouped clser together at times. Once you're looking at the old seamounts it's likely that the material preserved now is simply the highly agglutinated material surrounding the vent itself, with large amounts of the surrounding pillow lavas and hyaloclastite (which form the bulk of submarine volcanics) having been strongly eroded.",null,0,cdjiimo,1r329d,askscience,new,4
RageousT,"Well, the hotspot is, somewhat unsurprisingly, hot, and thus the crust above it is hot, and thus buoyant. This contributes to Hawai'i being as high as it currently is. As the hotspot moves away from the old islands, they cool, and thus sink. Not entirely sure how much this compares to the contribution from erosion, however.",null,3,cdj8ng3,1r329d,askscience,new,2
quality_is_god,"Engineer here.
Essentially the answer is never. 

Unless the force exceeds the yield stress of the steel in the spring causing the steel to fail, or if the spring is heated up and cooled down (re-crystallizing) in the deformed shape, or it completely rusts, it will spring back when the force is released.

Metals can fail under repeated compression/release cycles if close to the yield stress, but a constant load like a weight will not cause steel to permanently compress (with the  above noted exceptions).

Many of the engineered structures you take for granted rely on the permanence of the elasticity of steel below the yield stress.",null,0,cdjdjkq,1r35c8,askscience,new,13
nosignificanceatall,"It is common to describe the creep rate of a material by an empirical power law of the applied stress, i.e.  
dε/dt = k\*σ^n where k is a constant determined by the creep mechanism, temperature, etc.  If we substitute ε=σ/E, then we have a simple differential equation for the time-dependence of the stress.  It can be solved to yield  
σ(t) = ( σ(0)^(n-1) - E\*k\*t\*(n-1) )^1/(n-1)

This model predicts that the stress never reaches zero, so there will always be some expansion when the load is removed, but that the expansion becomes arbitrarily small as time increases.  If you run experiments/look up tabulated values to determine E, k, and n, then you can use the above equation to predict how much your material will expand when you remove the applied load after some time t.

Adjustments can be made to the differential equation to better suit the particular material that you are working with.  For example, when the dominant mechanism of deformation is dislocation creep (as is probably the case for your example of a spring), it is common to replace σ^n with (σ-σ\*)^n - that is, the material ceases to creep once the stress drops below some threshold σ\*.  In this situation, the material will still expand by σ\*/E even if you leave it under stress for an indefinite amount of time.",null,2,cdje1vg,1r35c8,askscience,new,5
Trill-Nye,"Materials scientist here. Technically, all materials will, over a long enough time scale, relax at the atomic level such that any stresses are negated.

Stress, like almost everything else, really happens at the atomic level. When you compress a string, the atoms in that metal must rearrange in some way to accommodate this deformation. This can means stretching of atomic bonds or simply the grinding together of microscopic grains within the metal, for example. These structural rearrangements will leave the atoms in a high energy state.

Thermodynamically, atomic systems tend to relax to their lowest energy accessible state. Thus, when the atoms are rearranged during deformation, they will then move around until they adopt an arrangement that minimizes the material's internal stress, one similar to that exhibited prior to deformation. The problem is that, at room temperature, this process is extremely slow. The motion of atoms, called diffusion, is, for most materials, only appreciable at high temperatures.

So yes, the relaxation time of an applied stress can be calculated if a lot of material specific values related to atomic vibrations, diffusion, grain boundary slide, structure defects, etc. are known. For most engineering materials, at room temperature, this will yield very long times (sometimes geologic timescales). Plastics, on the other hand, often relax very quickly, such that it can be observed in laboratory experiments. A good place to start if you want to learn more would be to read about the highly industrial-relevant process of [annealing](http://en.wikipedia.org/wiki/Annealing_(metallurgy)). ",null,0,cdjey0z,1r35c8,askscience,new,4
FlyingSagittarius,"Springs are usually made of metal, which has already been discussed.  Ceramics act similarly, within the elastic limit at least.  Polymers, on the other hand, can have both a strain and a strain rate, so if you apply a constant strain to most plastics they will eventually flow to relieve the stress.

The basic equation for this behavior is e = (S / E) + (T / V)*t.  e is the normal strain, S is the normal stress in the material, E is the tensile modulus, T is the shear stress in the material (which always exists, even if only normal force or stress is applied), and V is the viscosity.  If you set a value for e and you know everything else, you can solve for the time.  E and V are material properties, T can be calculated from S, and S can be calculated from e.  The resulting expression is a differential equation that shows the stress in the material asymptotically approaches zero, and the strain rate asymptotically reaches zero.  The time constant depends on the material properties.

This is for a constant strain, though.  What about for a constant stress?  This would be like suspending a weight with a polymer rope.  As the polymer rope supports the weight, the polymer flows to relieve the stress.  But the weight responds by dropping down, and ""reapplying"" the stress.  So the stress stays constant, and the rope would respond with a constant strain, plus a constant strain rate.  So it'll stretch initially as the weight is applied, and will stretch even more as time goes on.  (Eventually it'll stretch too much and snap, though.)",null,0,cdjij1v,1r35c8,askscience,new,3
mc2222,"when shooting with a round aperture in a camera, every point will be imaged as a tiny round point.  When imaging something like a star (a very good point source), the image will be a point, but the out of focus image will be nothing more than a big round disk.  It gets a bit more interesting when we change the shape of the aperture.  Let's put a disk in the center of the aperture to block some light - [like the design of some telescopes](http://nimax-img.de/Produktbilder/normal/10215_2/Meade-Schmidt-Cassegrain-telescope-SC-203-2034-8-UHTC-LX90-GoTo.jpg).  The aperture for such systems is a doughnut shape.  When you try an image a star, it will be a pinpoint at the focal plane, but will be doughnut shaped either inside or outside of focus, like [this](http://legault.perso.sfr.fr/airy_collim_2.gif).

Bookeh is simply exploiting the shape of the aperture to make out of focus point sources take on that shape.  Want heart shaped bokeh? Simply cut out a heart shaped pinhole and put it in front of the camera.  The camera will still focus on what you want it to, but everything that's out of focus will look like a heart.",null,0,cdjdc4h,1r35z3,askscience,new,1
kooksies,"I believe fibrocartilage is stronger than hyaline cartilage. 
This is because it contains type I collagen which form thicker fibres and are packed more densely together than collagen found in hyaline cartilage. Meaning it has good tensile strength, flexibility and general rigidity.

Fibrocartilage can be found in tendons and ligaments, but they are mainly found in the tissue that separates your vertebrae.   

It shouldn't break as easily as hyaline cartilage, but i have no idea how it would affect you during working out. You should probably seek a  doctor's opinion !",null,0,cdjarwg,1r369o,askscience,new,2
KarlOskar12,"Cartilage is poorly vascularized tissue and as a result of low blood supply (amongst other things) it has poor regeneration capabilities. When damaged it is not replaced by another type of cartilage. However, cartilage that is repeatedly injured becomes highly vascularized and consequently calcified and it starts to turn into bone. This process causes arthritis.",null,1,cdj9cjp,1r369o,askscience,new,1
therationalpi,"Great questions, I'll try my best to answer this.

Your questions are about a [vibrating string](http://en.wikipedia.org/wiki/Vibrating_string). The equation for the frequency of a vibrating string is given by

f=(1/2L) \* √(T/µ)

Where f is frequency (pitch), L is length, T is tension, and µ is linear density (mass per unit length). The answers to your first two equations should be apparent from that equation, but I'll explain each by itself.

1. You increased the length while holding T and µ constant, thus f decreased.

2. You increased L while also increasing T and lowering µ (µ decreased because the mass of the rubber band is conserved, just spread over a longer band now). µ ought to change as µ=µ_0\*L_0/L, where µ_0 and L_0 are the initial linear density and length respectively. Thus, the new equation becomes: f=(1/2) \* √(T/µ_0\*L_0\*L). Tension will also increase as you increase the length, but if it's increasing more than just linearly, you'll find the frequency increasing.

As for three, I'm not certain, but I would assume that temporal resolution is to blame.",null,1,cdj7r5w,1r36co,askscience,new,5
IAmMe1,"Let's talk quickly about what determines pitch. The pitch of the sound wave is determined by its frequency, which is set by the number of times per second that the object producing it (the rubber band) wiggles up and down. Now, the wiggling of the rubber band itself is set by two things: the wavelength of the wiggles and the wave speed in the rubber band.

The wavelength is basically how long a wave has to be to ""fit perfectly"" into the amount of rubber band available to wiggle. This means that the wavelength is basically the length of the wiggling part of the rubber band (over 2 for the fundamental frequency, but that's not important).

The speed of waves in the rubber band is set by the band's density (a property of the material it's made out of) and the tension in the band. Higher tension makes the wave travel faster.

Also, you should know that pitch goes up if wavelength goes down and pitch also goes up if the wave speed goes up.

1) You have kept the tension in the band the same while increasing the length. This means you haven't changed the wave speed, but increased the wavelength, so the pitch goes down.

2) You have increased both the tension (EDIT: and decreased the density) and the length. Increasing the tension (EDIT: and decreasing the density) should increase the pitch, and increasing the length should decrease the pitch, so it's a question of which effect is bigger. It turns out that for your rubber band, the effect of tension was bigger. This may not be true for all rubber bands at all points in their stretch.

3) Not my field, so I'm going to have to leave this one for someone else.",null,0,cdj7spc,1r36co,askscience,new,3
KarlOskar12,"3) Our eyes can see about 30 FPS. When the rubber band *blurs* as you say, it just means that it is cycling at a faster rate. Side note, if you get a strobe light where you can alter the rate of light flashing and you set it to flash at *the same rate as the rubber band is vibrating* the rubber band will appear to be stationary (as long as you shut all the other lights off).",null,0,cdjabwu,1r36co,askscience,new,2
Professor_Snuggles,"A simple thing that no one's mentioned is to put the piece of paper on top of and underneath the book when you drop them, rather than side by side. If they actually fall at different speeds, they would separate in one case. They'll fall together in both though, because the paper is shielded from air resistance effects by the book. If air resistance wasn't what was causing the difference, then this wouldn't matter.",null,0,cdjem5e,1r37tg,askscience,new,16
IAmMe1,"The key difference here is between force and acceleration. You won't really be able to get away with avoiding inertia to explain this.

See, the gravitational *force* is indeed stronger for an object with more mass. However, that object also has more mass and thus more inertia. This means that it takes a larger force to reach the same *acceleration* for that object. It just so happens that the dependence on mass cancels out for gravity; every object experiences the same *acceleration* due to gravity.

However, the *force* of air resistance does not depend on mass; it depends on the shape of the object (and its speed). The result is that, while all objects of the same shape (moving at the same speed) experience the same *force* of air resistance, due to different amounts of inertia (mass) they experience different *accelerations* due to air resistance. In particular, since the heavy object has more inertia, the same force produces less acceleration than on a light object.

To summarize: gravity produces more force on a heavier object than a light one but the same acceleration, while air resistance produces the same force on each but less acceleration for the heavy one.",null,1,cdj7jf3,1r37tg,askscience,new,7
Weed_O_Whirler,"So, to answer this you have to understand the difference between ""force"" and ""acceleration"" (I know, you're thinking- of course I do! But, your explanation shows a misunderstanding between them). Gravity attempts to *accelerate* everything at the same rate- and it does this by pulling on heavier things with more *force.* 

A nine year old might not be able to fully understand Newton's second law (F = m\*a if you need a refresher) but you can probably explain it to him pretty well. Tell him to imagine a rocket hooked up to a car, and how that rocket can make the car go fast. Now imagine that instead of a car, it is a big truck. The rocket, which puts out the same force regardless, will push the truck slower than the car. And now hook that rocket up to a train, and the train might not move at all. Or if it does, it will move slowly. This is the basics of Newton's second law- if you apply the same force to objects, the heavier ones will move slower than the light ones. You can do this experiment by trying to push a book across the table, or a stack of books- you'll have to push harder for the stack. 

OK, so gravity isn't a rocket. Gravity pulls harder on things which are heavier. In fact, if you double the mass of the object, gravity will pull twice as hard. So that is like if you made a car twice as heavy, but also attached two rockets- the acceleration would be the same regardless. So, gravity provides *twice the force* on an object twice as heavy, but due to Newton's second law, that is *the same acceleration.*

So now, add in air. Air resistance comes from the object having to move air molecules out of the way as it falls. So, it makes sense that the force of air resistance would be dependent on two things- the ""surface area"" or shape of the object in the direction it is falling and the speed at which it falls. The larger the surface area (again, only in the direction of falling, a book turned up on its spine would have less air resistance than a book lying flat), the more air molecules it has to move out of the way. Also, the faster it is falling, it will hit more air molecules it has to move. So, the larger those things are, the larger the force of air resistance. 

Thus, as you might expect, a book and a single sheet of paper should have the same air resistance (at least, when it first starts to fall- eventually the book will have more because it is moving faster). So, they have the same force pushing up on them- but that *force* causes more *acceleration* on the lighter object (the sheet of paper) than the heavy one (the book). 

Playing with some numbers (we'll choose easy ones). Imagine you have a 1 kg book and a 2 kg book, and we'll say the acceleration due to gravity is 10 m/s^(2). The, using Newton's 2nd law we can see that the force due to gravity on the first book is 10 Newtons (A Newton is the SI equivalent to a pound, it has units of force) and the second book at a force of 20 N. But now imagine each of them have 5 Newton's of air resistance acting on them. So, the total force acting on book 1 is 5 N, and the total force acting on book 2 in 15 N (Forces add- and since they are in opposite directions you are getting 10-5 and 20-5). So now, we can use Newton's second law to calculate their acceleration:

&gt; a = F/m (just re-arranged) 

&gt; a1 = 5N/1kg = 5 m/s^2

&gt; a2 = 15N/2kg = 7.5 m/s^2

So, the book that weighs more (but has the same air resistance) accelerates faster. ",null,0,cdj932w,1r37tg,askscience,new,4
DanielSank,"Please, please please show your son this:

Hold the paper and book up in the air shoulder width apart and drop them at the same time. The book hits the floor first.

Now place the paper on top of the book. Make sure it's pretty flat and hugs the top of the book as best you can get it (edges of the paper must not extend past edges of book). Now drop the book. They fall together.

If nothing else this will astound him enough that he won't forget it and will continue to seek answers.",null,0,cdjh47y,1r37tg,askscience,new,4
dampew,"Smart kid.  Tells it like it is.

Wind resistance is proportional to the area of the object.  But heavier objects are heavier and push down harder on the air.  So if two objects of the same size are falling through the air, the heavier object will fall faster.",null,0,cdjgo0j,1r37tg,askscience,new,3
ww-shen,"Little OFF: If you explain something to your kid (or any kids) it is a common wish for them to understand it. Sometimes explain the matter more advanced way (without the simplification). This way the kid will learn that there are things he cannot understand, and in time, that many things you don't understand either. It will open his perspective about the nature of knowledge.",null,0,cdji8zp,1r37tg,askscience,new,3
Surf_Science,"Dropping things may not be the way to go. You should try rolling them down a hill. I think this was the way Galileo went about some of his experiments (though not a hill and more precise). I think the only time you'd get into a problem would be with weird shapes so you should be able to do a bit better experiment with different weighted balls. 

With respect to the paper book issue. Isn't the paper loosing part of its downward speed from gravity by moving in a lateral direction? That may be easier for a kid to understand.  ",null,0,cdj7jdk,1r37tg,askscience,new,2
jayman419,"For an simple unscientific demonstration, you can [build a parachute out of a plastic bag, some tape, and some yarn](http://www.wikihow.com/Build-a-Plastic-Parachute) and have him experiment with different weights, to show that wind resistance is a limited thing that only slows objects by a certain rate.

Like hook a toy action figure up to it, and drop it from the balcony (if you have one) and then hook up something heavier, and he can see that the same parachute and the same air affect objects differently. You could even hook up the book to the parachute and race it against the sheet of paper.

That way you don't have to get into the maths of wind resistance, and you can show him the difference in practical terms.

EDIT: There's also the Apollo 15 video (one example of it is here: http://www.youtube.com/watch?v=5C5_dOEyAfk) where they dropped a hammer and a feather on the Moon, you can let him see what happens when there's no air.",null,0,cdj7oyv,1r37tg,askscience,new,2
natty_dread,"The thing is, that gravity **does** pull with different force on different things.

The actual constant value is *acceleration* not *force*. The sheet of paper and the book are being accelerated equally towards the ground. Since their shape is the same, the force of air resistance should be roughly the same too. However, since the book has more mass, the earth pulls harder on the book, thus making it fall faster.


The mathematical description is as follows: (This is meant to give **you** additional insight in order to give you well funded knowledge to share with your son)

 
Newton's law of gravity states that F_Gravity=G * m * M/r^2 (G being the gravitational constant, M&amp;m the masses and r the distance between them)

Now, Newtons Axiom states that the movement of a mass under the influence of a force is given by F=m*a

If we substitute F with F_Gravity we get G * m * M/r^2 = m * a.

As you can see, m can be canceled out of the equation.

This leaves us with a = g = G*M/r^2 .
This equation shows, that the acceleration of all masses in a gravitational field is, indeed, equal.

Then why are some things heavier than others?

To answer this question, we have to ask ourselves, what weight is. Weight is the force with which an object is pulled towards the earth.
Since the force, is given by F=m * a, and the mass of different object differs, the force which objects are pulled towards the ground is does not necessarily have to be constant.

",null,2,cdj81fn,1r37tg,askscience,new,4
joshhinz,"I always found the dropping two objects experiment misleading and unintuitive, because of the very problem you encountered! It simply isn't true in many cases on earth because there is air resistance on earth and even buoyancy forces (most evident with helium balloons). Maybe try to explain to him this fact to some degree then show him the [Apollo 15 Hammer and Feather Drop](http://www.youtube.com/watch?v=-4_rceVPVSY) ",null,0,cdj8gkl,1r37tg,askscience,new,2
tagaragawa,"I think one of the first mind-blowing things one learns in life is when you are taught that *everything* attracts *everything else*. Please include this in your discussions. It is not obvious at all, and only nowadays do we have experiments sensitive enough to measure the influence of, say, a solid sphere upon another test object. You can however easily convey this by pointing out that the earth moves around the sun, and the moon around the earth. The simplest explanation is that both the sun and the earth 'exert gravity', and from there that everything 'exerts gravity'.

Related: does anyone know when this idea was conceived? Did the early inventors of heliocentric models immediately make this implication, or was it not until Hooke and Newton that is was fully realized?",null,0,cdj95vn,1r37tg,askscience,new,2
iorgfeflkd,It is indeed held together by gravity. The collective gravity of the sun is a lot stronger than that of Earth.,null,0,cdj8lbv,1r38kq,askscience,new,6
__Pers,"For the most part, gravity keeps the sun together. That said, the solar atmosphere (the chromosphere and corona) is composed of hot--hundreds of thousands of Kelvin to millions of Kelvin plasma--that indeed floats off into space. This hot, expanding plasma is called the solar wind.

Edit: typo",null,1,cdj8onh,1r38kq,askscience,new,4
ooburai,"It's not definitive, but The Straight Dope has this article on the topic.

http://www.straightdope.com/columns/read/619/why-do-we-nod-our-heads-for-yes-and-shake-them-for-no

Also this: http://en.wikipedia.org/wiki/Nod_(gesture)

It seems that most people nod for yes, but there are some specific exceptions.  Bulgaria and Albania are cited.  That would seem to imply that it's not completely innate, or possibly that if it is that it can be overridden by culturally learned behaviour.",null,0,cdjb1d3,1r38ut,askscience,new,5
snusmumrikan,"[Yes it appears so](http://onlinelibrary.wiley.com/doi/10.1002/%28SICI%291096-8644%28199808%29106:4%3C483::AID-AJPA4%3E3.0.CO;2-K/pdf)

If you don't have access through an academic institution to that article this is the reference: American Journal of Physical Anthropology, 106: 483–503.

It's all on Wikipedia.

(Edit spelling)",null,0,cdj67pa,1r390m,askscience,new,3
gilgoomesh,"I don't know what measure of signal strength you're looking at or what WiFi hardware you're using so there could be 100 different answers. Three possibilities stand out in my mind:

a) When idle, your connection's sync rate will sit on a ""best guess"". When you're actually downloading, it can use the data sent and the error rate on that data to measure the *real* signal strength and it turns out that it's much lower than the guess so it needs to lower the sync rate.

b) The router antenna increases its power when sending data. Maybe this causes the power supply to create more interference.

c) You're getting a lot of multi path interference and your own WiFi signal bouncing off walls in the room is interfering with itself.",null,0,cdj90vh,1r3eju,askscience,new,3
superAL1394,"For a more complete answer, input from an EE or a physicist is necessary, but in general wifi strength is a 'best guess' estimate by the operating system based on its reception of packets from the router. When transmitting, you might be able to hear the router just fine, but your packets aren't being heard by the router. This is often the case if the power on your router has been increased, or you are using a lower power device (like an iPhone).

When you receive data, you will transmit back acknowledgements as defined by the IP protocol. This helps the system determine how fast it can send packets based on the number of collisions and dropped packets. If you are hearing the router just fine at idle, it will show a high strength, but as soon as you start receiving and the acknowledgements aren't being received by the router, the operating system will revise its guess on the signal strength to reflect your inability to effectively communicate back. The operating system will know the router isn't receiving acknowledgements because it will be receiving packets multiple times as the router will assume transmission failed and resend the packet.",null,0,cdjxq13,1r3eju,askscience,new,3
arumbar,"Enzymes are proteins - what happens when you ingest proteins?  They are digested into their substituent components and lose any structure/function they once had.  This is why replenishing insulin in diabetic patients orally does not work - they need injections instead.  The one scenario that I know of where we give people oral enzymes is for pancreatic insufficiency.  In this case, they are missing digestive enzymes that would normally be secreted into the gut, so artificially adding some in works well.",null,0,cdj6qmf,1r3em7,askscience,new,7
KarlOskar12,"The enzyme isn't missing from the blood, it is missing from lysosomes. Tay-Sachs is a *lysosomal storage disorder*. The problem with giving supplemental enzyme to someone with Tay-Sachs disease is that [HEXA](http://www.news-medical.net/health/Tay-Sachs-Disease-Research.aspx) is too large to pass the blood brain barrier where it is needed to break down GM2 gangliosides.",null,1,cdj9tei,1r3em7,askscience,new,5
iorgfeflkd,"If you want to look at it that way, you can find the average kinetic energy per molecule (sort of, it's a bit more complicated) by multiplying the temperature by Boltzmann's constant (which is equal to the ideal gas constant divided by Avogardro's number). However, the average molecular energy isn't the best definition of temperature; it actually relates to how energy changes with entropy.",null,1,cdj6gu5,1r3ez4,askscience,new,12
__Pers,"In some subfields of physics such as plasma physics, temperature is indeed quoted in units of energy, typically electron Volts (eV) or kilo-electron Volts (keV), referring to the average kinetic energy a particle from the ensemble would have.",null,1,cdj77de,1r3ez4,askscience,new,4
natty_dread,"Temperature and Energy are connected by the so called Boltzmann constant.

That means, that there is a fixed ""exchange rate"" between temperature and energy. You could choose your system of units in a way that will result in the constant being equal to 1. This will render temperature and energy equal in terms of value.",null,2,cdj880a,1r3ez4,askscience,new,2
iorgfeflkd,"If the planet passes in front of the star periodically, its radius can be determined by how much the star dims as the planet passes in front, while the mass can be determined by the planet's gravitational effect on the star's velocity, which is based on the stars chemical spectrum changing with the Doppler shift. Combining these, a density can be calculated.",null,1,cdj6ius,1r3ezh,askscience,new,4
quality_is_god,"This would depend on the nature of the restriction. 

Only if the restriction were isentropic would the pressure at C not be affected by the upstream restriction. 

Isentropic restrictions are very specific devices (like a nozzle and diffuser) but under the vast majority of restrictions, there will be unrecoverable pressure losses that propagate downstream.",null,0,cdje3hg,1r3f2v,askscience,new,2
202024,"Yes it will, the restriction will have a pressure drop, depending on the type some of it will be recovered, but there will still be a permanent pressure drop caused by the restriction.

http://en.wikipedia.org/wiki/Orifice_plate is a good place to start if you want some more technical information.",null,0,cdjn8dk,1r3f2v,askscience,new,1
Randomaway,"Your pump will adjust to provide a constant flow rate.  Let's call point C some kind of spray nozzle.  That spray nozzle will require a certain pressure to produce the desired flow rate.  Introducing a restriction (B)  upstream will not affect the required pressure at C.  Point B  does introduce extra pressure drop, so the pressure at point A will have to be higher to produce the required pressure at point C.",null,0,cdkmxmc,1r3f2v,askscience,new,1
whatsup4,"Point C should not see any difference theoretically. I say theoretically because if for some reason the blockage at B caused some kind of turbulence at point C it could alter what is happening downstream. Something like a vortex upstream forces your fluid to vortex downstream. But if that is not the case or C is far enough away from B then no there shouldn't be a difference.

Here's one way to look at it if you have a pipe with a certain diameter and a certain back pressure from the piping after it then there is only one pressure it can be for a certain flow rate. If it's pressure increased its flow would need to increase if its pressure decreased its flow would need to decrease. Since its flow must remain constant so must its pressure.",null,0,cdlj3wc,1r3f2v,askscience,new,1
evertjm,"Yes, pressure is caused by a resistance to flow. As you have stated that point B will restrict flow, if the diameter of A and C are equal then C will have less flow and pressure.

Simple test is to kink your garden hose while the tap is on fully. If you kink the hose at the halfway point, and the end of the hose is unrestricted, then you will notice a considerable drop in flow and pressure.",null,1,cdjb2g4,1r3f2v,askscience,new,1
__Jay,"One very important aspect that has gone unmentioned is your Reynolds number.  Loosely speaking, an indication of smooth (laminar) vs turbulent flow.  Your Reynolds number is a function of distance, i.e., as distance increases your flow will become less stable and inevitable losses are incured which call for pressure increases to maintain flow. 

 In technical terms, you have boundary layer separation with distance.  A mechanical energy balance will show, any losses with kinetic energy (fluid velocity) lead to a neccessary pressure increase to maintain fluid velocity at constant cross section (pipe diameter).",null,1,cdjdbut,1r3f2v,askscience,new,1
iorgfeflkd,"We don't need particularly fancy telescopes to detect the cosmic microwave background, and recent missions like WMAP have given us pictures of it with extraordinary resolution.

Too see farther back than the CMB (about 400,000 years after the big bang), we can not longer use light (or any EM radiation) because the universe was opaque. Neutrino or gravitatioonal wave observatories could bypass this, but our capability in those areas is much much worse than with light telescopes.",null,1,cdj6d9b,1r3fak,askscience,new,15
s8nlovesme,"""Gray hair, is simply hair with less melanin, and white hair has no melanin at all. Genes control this lack of deposition of melanin, too. In some families, many members' hair turns white in their 20s. Generally speaking, among Caucasians 50 percent are 50 percent gray by age 50. There is, however, wide variation. This number differs for other ethnic groups, again demonstrating the effect of genetic control.""",null,0,cdjazwp,1r3fe1,askscience,new,1
Platypuskeeper,"It's a common misconception, from the [Bohr model of the atom](http://images.tutorvista.com/cms/images/38/bohr-atom-model.JPG), that there'd be any 'empty space' between the atom and nucleus. Electrons are quantum-mechanical particles, which means they don't normally occupy a precise location, nor radius from the nucleus, they're sort of 'smeared out' in space. Atoms actually 'look' more [like this](http://kaffee.50webs.com/Science/images/orbitals/1s.gif), with a diffuse 'cloud' of electrons around them, where the density represents the probability of finding an electron in the vicinity of a point. The density is actually _highest_ near the nucleus, and drops off exponentially as you move away from it.

So there's no real 'empty space' there, but some parts of space are emptier than others. (That said, there's still a concept of the vacuum in physics). 

One thing that could be said to be true of the Bohr model is that it shows that the 'size' of an atom is the 'size' of the volume which its electrons occupy. Although that's a diffuse 'cloud', it's still 4-5 orders of magnitude larger than the 'cloud' of the nucleus. 

Electrons and protons are not very 'solid'. An electron has no radius of its own, and the protons (which are composite particles) are also spread out. Due to this quantum smearing-out, electrons (if they have opposite 'spins') can occupy the same location, and the electron and proton can be in the same location, which wouldn't be possible for classical point-charges, as it would lead to infinite energies. 

The classical concepts of things 'touching' and having surfaces and volumes don't apply here. You have to flip it around: _Because_ the electron density drops off so rapidly as you move away from the nucleus, the electrical forces between two adjacent atoms get enormous within a short distance, once their electron clouds start pushing into each other. Things push back with more force than you can exert over an imperceptibly small distance, which is why things appear 'solid' to you at the large-scale, macroscopic, level. At the atomic level, nothing is solid though.

",null,5,cdjbguz,1r3ju6,askscience,new,13
r2k,"We can't ""see""  atoms or subatomic particles. To properly resolve atoms,  we need to use electrons or other particles which have a smaller wavelength on the same order as the atoms. Electrons are considered point particles, and the nucleus of an atom is composed of protons and neutrons. These subatomic particles are themselves composed of things called quarks. 

You can't probe a proton/neutron to see how hard it is -  because you would have to use something much smaller than it in the first place. In reality,  it is not a sphere,  and scientists are still trying to determine what the fundamental building block of matter is. String theory proposes that ultimately,  everything is composed of bundles of energy. 

The space between a nucleus and the orbiting Electrons is the same space that exists between the moon and Earth. It really is nothing. ",null,10,cdjarhu,1r3ju6,askscience,new,4
ModernTarantula," you have to imagine yourself smaller that what you are building and building it vertical not horizontal. Next watch a spider make it's web. You will see how it happens. Next is speculation from that. They need to have a central hub to make the spokes from . They follow the old line to the middle and then can hold up a string out with a leg to make the next radial. The contehcntric circles are made as they walk around the radials like a racetrack. at each pint connecting to the radial. 
so spokes and a tight spiral is simpler than measuring out a grid in the air.",null,0,cdjh1ch,1r3lv4,askscience,new,1
brawnkowsky,"depends specifically on what you're allergic too and whether that allergy exists on that particular animal.  but most cat allergies are caused by a specific protein (Fel d 1) that is produced by cats and released via sebaceous glands on skin.  this protein interacts with IgE and IgG4 to create an immune response.

research shows that this protein is present on some big cats, so if this is the protein you are allergic too, then chances are you will have a reaction when coming in contact with any cats.  but there is a chance you are not allergic to this protein.

http://www.ncbi.nlm.nih.gov/pubmed/1695231

Edit: research shows that a Fel d 1-like molecule is present.  same results, different cause",null,0,cdjg5om,1r3lwr,askscience,new,4
justin3003,"I hate to keep doing this as a means of answering questions here, but I wrote a blog post on this exact topic a few months ago. The short answer, based on my research, is ""kind of."" Big cats are genetically related but distinct from house cats (as well as distinct from each other) and have different forms of the allergenic protein Fel d 1. The big cat version of the protein can cause an acute allergic reaction in humans but not all to the same extent as Fel d 1 does. I cite the same paper as brawnkowsky does; in it they have a chart of histamine release vs. concentration of proteins as well as relative release of the relevant immunoglobulins from big cats compared with Fel d 1. The lion, for example, has a significantly lower reactivity in a cat allergic patient than both a tiger and Fel d 1 cause (and fel d 1 &gt; tiger). The chart also shows dramatic (at least 50%) lower immunoglobulin release during exposure for big cat dander vs. Fel d 1/house cats. While the conclusions presented in the abstract are technically correct, since they can/do elicit an immune response, the actual data within the paper much more clearly demonstrates the nuances of their conclusion: while many are still allergenic to some extent, Fel d 1 has a much higher activity than any protein present in the big cats by a fairly wide margin for both IgE and IgG4. 
Link to my blog post: http://theweeklypaperblog.com/2013/02/13/lions-and-tigers-and-bears-oh-my-allergies-and-cross-species-issues/
Link to the ScienceDirect site (if you have institutional access to papers, sorry I can't post it directly for you): http://www.sciencedirect.com.libproxy.usc.edu/science/article/pii/S0091674905801307#",null,0,cdksxlm,1r3lwr,askscience,new,1
datums,"Taking a photograph on film is a chemical reaction. When light hits film, it causes chemical changes in the film itself, and the pattern of these changes makes up the image.   
  
Like any chemical reaction, it takes a particular amount of time. If I expose a particular film to light, it might take 1/24 of a second for the chemical reaction to occur, as is the case for standard movie film.   
  
If I want to increase the frame rate with that film to 1/48 of a second, I am going to have a problem. The same chemical reaction will have to happen in half of the time. A potential solution? Double the amount of light to compensate.    
  
Another way to analogize this is that is takes a particular number of photons to record an image. If I want to capture images more quickly, I can simply increase the number of photons per second, ie. the brightness, to compensate. ",null,1,cdjajxm,1r3nxd,askscience,new,7
johnwilkesbandwith,"Running at 24 FPS the shutter, a physical spinning disk, exposes the film at 1/48 rotations per second with a 180 degree shutter opening. This means every second half of the oval blocks the light and half exposes the film as it runs through the gate. 

The gate is a plate system that holds the physical medium in place. At this speed, with the proper F/stop, the film is exposed properly.

When you increase the frame rate, you need more light because the speed that the film is going through the gate is getting the proper exposure.

You can compensate for a higher frame-rate by either switching to a higher shutter speed, which can effect the motion blur of the image or you can open your lens up to a lower F/Stop. 

If you don't have the available light, you can compensate with the shutter speed but it does effect the visual aesthetic of the image.

The image gets dimmer because the film loses about a stop/multiplication of speed that you run the film. 

This concept applies to digital filmmaking, but its effects on 3D photography are more complex.",null,0,cdji3t2,1r3nxd,askscience,new,1
Das_Mime,"Particles certainly exist, although on a quantum level everything exhibits particle/wave duality. Particles, instead of being the hard pellets that we sometimes envision, are actually described by a probability function which tells you how likely the particle is to be in any given location. So in that sense the particle can be thought of as a probability field.

In the Standard Model (which is the more or less unanimously accepted model of particle physics), mass is caused by particles interacting with the Higgs field, whose existence is supported by the discovery over the last few years of the Higgs Boson.",null,1,cdjk1ri,1r3pfa,askscience,new,4
null,null,null,0,cdja5vr,1r3pfa,askscience,new,1
Claclink,"ive tested this in the lab with a spectrometer and it depends on the color and person, but in optimal conditions you can detect a change of about 2nm.",null,1,cdjet36,1r3qnw,askscience,new,5
Greyswandir,Human eyes are surprisingly sensitive to changes in wavelength.  My research used to involve a colorimetric assay (one where the main output is a color changed) which involved me both taking pictures of the samples and measuring their spectrum.  A relatively small change in the shape of the spectrum (~20nm of peak broadening) could change the perceived color of the sample from green to orange.,null,0,cdjqi6j,1r3qnw,askscience,new,1
Bbrhuft,"There would be no difference. Riffling causes the bullet to spin, it acts like a gyroscope, giving it extra stability that keeps it from tumbling randomly and deviating from its intended target due to variable wind drag.

But there's no air in space, no wind drag, so whether a bullet tumbles or not, it will still follow the same path. 

And indeed when calculating the orbits of asteroids there's no need to take into account their rotation period.",null,9,cdjdg2i,1r3sde,askscience,new,44
TangentialThreat,"It depends on whether you need the bullet to hit pointy-end first.

Many bullets have either a hollow point so it fragments on impact and does more damage, or a hardened armor-piercing tip. Rotation helps keep this end towards the enemy. A bullet that hits sideways or backwards is still pretty lethal on Earth, but you are presumably shooting at either your fellow kevlar-suited cosmonauts or battling an unknown alien menace. The terminal ballistics will be much less predictable and this could be a problem.

It should be noted that firing any gun in space is probably a rather delicate skill due to conservation of momentum. Rifling will make this even more complicated. Best of luck out there and try to make Earth look good.

[Loose wads of nitrocellulose](http://www.youtube.com/watch?v=QnDZ_cO5Ln4) refuse to burn in vacuum.

[Double-action revolvers](http://www.youtube.com/watch?v=hUdkIn7C9fA) do appear to work, which I would not have expected.",null,3,cdjhdea,1r3sde,askscience,new,9
Evomon,Wouldnt it be impossible to shoot a gun in space as the conventional  bullet wont have o2 to ignite ? ,null,10,cdjdkgf,1r3sde,askscience,new,9
CatalyticDragon,"Yes. All the objects you have listed contain the same elements but in different total amounts and ratios. And they all behave according to the same universal physics.

Here is a quick page on the makeup of the earth, stars, and even interstellar dust clouds. You'll see all of this ""stuff"" is made up of the same ""stuff"" because it all comes from the same place. Exploding stars;

- http://spiff.rit.edu/classes/phys240/lectures/elements/elements.html",null,0,cdjbbg6,1r3v70,askscience,new,3
iorgfeflkd,"There are only 90 or so elements that can be found in nature. Asteroids and planets are all made from them, in different abundances.",null,0,cdjczor,1r3v70,askscience,new,2
baloo_the_bear,"Electronegativity is how strongly a particular atom attracts electrons. 

To understand what that means, you need to know what attracts electrons, and what factors can affect that.

Electron orbitals have very specific shapes in 3 dimensions, and can be thought of as a waveform. These waveforms are most stable when filled with an appropriate number of electrons. The first valence shell is *s* and the first level of the *s* orbital prefers 2 electrons, this makes it most stable. Hydrogen is nothing more than a proton and an electron, but it is very stable when it can fill its valence shell with 2 electrons, hence a high electronegativity and why it is so weird. Helium has 2 protons and 2 electrons (and 2 neutrons, but they don't matter for the moment), so its valence shell is full at 2 electrons, and it does not attract electrons to fill and sort of void, and therefore has a low electronegativity. 

Now, we talk about ionic forces.

Electrons are negatively charged, and as such they are attracted to positive charges, such as the nucleus of an atom. The force of ionic attraction is proportional to the charges of the two objects, and inverse of the square of the distance between them. The higher the positive charge of an atomic nucleus, the stronger the force of attraction, and the smaller the distance between those charges, the stronger the force of attraction.

As you move from the left to the right of the periodic table, you have more and more protons, which means an increasing positive charge in the nucleus. This increasing positive charge exerts an increasing force of attraction on the electron orbitals. This causes the **size** of the orbital to **decrease** as you move across the periodic table. This is why the **electronegativity increases going from left to right.** 

Now to add another concept.

Valence shells exist in orbitals that have different levels of energy. The fact that energy is discrete (dividable down to quanta) means that the orbitals have discrete levels, or layers. Not only do more layers increase the distance between the nucleus and the electron it is attracting, but those layers are all negatively charged and will act to repel another negatively charged electron. 
This is why as you **move down** the periodic table (increasing levels of valence shells), the **electronegativity reduces.**

Now remember that we talked about valence shells being most stable with certain numbers of electrons? The next 'magic number' of valence electrons that make the orbitals stable is 8. Now count over from left to right on the table. Florine has....**7** electrons in its valance shell, just one short. It strongly attracts that last electron not only because it is small and has a large charge in its nucleus, but because gaining another electron makes it have a **more stable** valence structure. 

OK, so we've talked about what electronegativity means, and what factors have an affect on it, but why do we care?

We stated that a stable valence shell has 8 electrons. This is why Carbon, with 4 electrons in its valence shell, will make 4 covalent bonds. Covalent bonds, as their name suggests, are when 2 atoms **share** an electron so that **both** atoms can have a stable valence shell. in the case of a covalent bond between atoms that are the same, the electron is shared equally, because the **electronegativity** of each atom is the same. However, if one atom in the covalent bond has a higher electronegativity, the electron is attracted more to that side of the bond. What happens when you're attracted to something? You want to spend more time there. Because of this unequal sharing, the bond becomes **polar**, in that one side of the bond has a slightly negative charge, and one side has a slightly positive charge. 

This matters in incredibly significant ways. Water, for example, is a polar molecule. Because of this water is liquid at room temperature, held together by hydrogen bonds (a consequence of polar molecules). DNA is also held together to their complementary strands by hydrogen bonds. 

edit: the stability of 8 valence shell electrons are also why the noble gases are very unreactive. They do not need to share electrons to be stable. The low reactivity of this group of elements is why they are called 'noble', as nobility kept to themselves.
",null,27,cdjc2e8,1r3wa9,askscience,new,126
MJ81,"I'm not sure there's a ""rigorous"" explanation of electronegativity - there are quantitative ways to calculate electronegativity (see [the Wiki article](http://en.wikipedia.org/wiki/Electronegativity)), after a fashion, but it's primarily a way to formalize available experimental data &amp; observed trends.",null,3,cdjeduy,1r3wa9,askscience,new,6
M4rkusD,"In short. Hydrogen is so weird because, in contrary to the other elements, its nucleus is just a proton. That means its properties (and especially those of its H+ ion, essentially a naked proton) are partly different from those of other molecules.

The next element, helium, which has 2 protons already needs 2 neutrons to stabilise its nucleus.",null,6,cdjl4ss,1r3wa9,askscience,new,4
ThePizar,"Atoms want to fill their valance shell (s &amp; p sub-shells) and to do that they need an electron. As you go across a period atoms get closer to having a full valance shell and thus they want that next electron. Going down a group atoms get bigger and the valance shell is farther from the nucleus and thus they have a weaker pull in each other. This leads the atom to not want the electron as much. 

You may notice a drop after the Cu/Ag/Au group. That is because the next group has all its existing outer sub-shells (s &amp; d) filled. 

Edit: Lewis Dot structure is a simple way of showing how molecules interact. Actually molecules are in 3d space. There are two main ways of showing/describing interactions between molecules. [Valance bond theory](http://en.wikipedia.org/wiki/Valence_Bond_Theory) and [Molecular orbital theory](http://en.wikipedia.org/wiki/Molecular_orbital_theory). 

Source: AP Chemistry class",null,9,cdjblqa,1r3wa9,askscience,new,4
fishify,"When a particle interacts with its environment, the state of the two-particle system can change, disentangling the previous entangled state.

Entanglement simply means that a two- (or more) particle state cannot be written as a product of separate one-particle states.  If one of the particles interacts with its environment so its state becomes definite, then it is no longer entangled with its one-time partner.",null,1,cdjd7fq,1r41ug,askscience,new,7
KerSan,"I think /u/fishify's answer is incomplete. If systems A and B are entangled, why should interaction with system C diminish the entanglement between A and B?

It turns out that [entanglement is monogamous](http://arxiv.org/abs/quant-ph/9907047). If systems A and B are perfectly entangled, it turns out that they cannot be entangled (or even correllated) with any other system C. More generally, the amount of entanglement between A and B places a limit on the amount of entanglement that either system, or both considered together, can share with C. The right way to count entanglement is still an open question, but this odd behaviour is the reason why particles become un-entangled.

The most important thing to realize about entanglement is that it *cannot arise without interaction*. That is to say, systems that do not directly interact cannot become entangled. Therefore, a particle P that is entangled with a system S generally becomes less entangled with S if P interacts with another system R.

This of course begs an interesting question: is there such a thing as an un-entangled particle? All particles might be entangled with *something*. This comes down to an interpretation of quantum theory, which I don't think is worth trying to discuss on reddit. Those kinds of discussions are mostly fruitless anyway. Emphasis on 'mostly'.",null,1,cdk2z6l,1r41ug,askscience,new,1
rupert1920,"Would this not depend on how much water there is in a bucket, or the size of the bucket?

What scientific principle are you trying to elucidate here?",null,1,cdjduam,1r43z8,askscience,new,4
endocytosis,"Depends on how angry you are…just kidding.
I doubt you'd get hypothermia, but it wouldn't be pleasant.  Your head will transfer some thermal energy to the ice water, raising its temperature slightly.  Your head will lose the thermal energy, causing among other things, blood vessels to contract, cellular metabolism to slow, and you to get a splitting headache (long story but tried it once on a dare-wouldn't recommend it).  You may feel the initial effects of hypothermia, uncontrollable shivering, confusion, disorientation, etc., mainly because your brain is having difficulty getting enough oxygen, nutrients, etc. and is chilled, but your heart is still pumping warmer blood to your brain to keep it going, assuming it's a reasonably warm (70 deg F, 25 deg C) day.  Edit: grammar",null,0,cdjm5il,1r43z8,askscience,new,1
king_of_the_universe,"Yes, more heat will be trapped, but this heat is irrelevant in comparison to the heat that your hands are able to feel when they touch the screen: That heat is *not* caused by the light itself but by the mechanism that creates it / by other components of the screen.",null,0,cdjj1v5,1r49bc,askscience,new,1
Mossman11,"I can't comment on why a cheaper high octane fuel isn't available, but considering the relatively small market and the fact that 93 octane for cars is $3.50, $6 for 100LL doesn't seem ridiculous to me.  Check out race fuels for cars, they're upwards of $10/gallon.

I do know a thing or two about motors, and octane, and compression so I can clear some stuff up there.  The 100 in 100LL is the motor octane rating.  This is a measure of how resistant to knock the fuel is, and how slowly it burns.  Especially in high compression engines, if a fuel is too volatile and its knock resistance is not sufficient, the fuel can pre-ignite due to compression alone.  When combustion happens while the piston is still on the way up, the piston now has to be fight through the high combustion pressures generated, as opposed to using them to propel it down again (the power stroke).  This is called knock, or ping, and is really bad for engine internals.  I know on some older aircraft the pilot had to control timing advance.  This would be equivalent to running too much timing advance all the time, except you can't push a lever to rectify it.  I've never flown a Cessna but I imagine if your motor grenades mid-air, that's not a good thing.  

As for why aircraft use high compression engines I would imagine it's to make the most power from the smallest/lightest engine.  Also the fact that you're flying in thin air means that you're sucking in less oxygen,nitrogen,etc. at altitude and in order to have sufficient power at altitude you'd need a high compression, knock prone engine at sea level.  

Also, I found this on wikipedia which seems like a nice feature of 100LL that possibly the proposed replacements don't have: 
""Avgas has a lower and more uniform vapor pressure than automotive gasoline so it remains in the liquid state despite the reduced atmospheric pressure at high altitude, thus preventing vapor lock""
",null,1,cdjo282,1r4coe,askscience,new,4
Minifig81,"It's something they call a stroboscopic effect and it's something that you commonly see in Western movies on television, so they also call it the wagon wheel effect. It's actually just an optical illusion in which the spoke wheel just appears to rotate differently than the true rotation. Actually it may look like it's rotating faster or not at all or even in the opposite direction.

It's illusion that's very similar to what we see with camera strobing that we might see in dance club or a party, when you have a flashing light that kind of freezes dances in a series of images.

If we think of the wagon wheel and we imagine that one of the spokes is painted white and it's pointed straight up - so like at like 12:00 on a clock - if we rotate that wheel at once per second and then we take images once every second, in our minds, it's going to look like that spoke never moved, because it's always going to be pointing at the 12.

If we were to suddenly now start capturing those images faster than once a second, so faster than it's rotating, that white spoke that's pointing up at 12, the next time we see an image, it's not going to make it all the way around, so it's going to be pointing at 11. And then the next image that we take, it's going to be pointing at 10. And so we're going to imagine that it's going to be moving in this counter-clockwise direction instead of the clockwise direction. ",null,1,cdjhxsk,1r4dw4,askscience,new,12
Criticalist,"In 2009 a group of researchers published a paper in the New England Journal of Medicine in which they reported on direct measurements of blood oxygen levels as they ascended to the summit of Mount Everest. At this altitude, 8800m, the partial pressure of oxygen is thought to be at the lower limit of what acclimatised humans can tolerate- according to the article, under 4% of people who climb Everest do so without supplemental oxygen.

The researchers took samples from their femoral arteries (located in the groin) at various altitudes as they ascended to the summit; the samples were transported down to the base camp for analysis. Above 7300m, the researchers used supplemental oxygen, but took it off for the sampling process.

The mean pO2 value, which is the partial pressure of oxygen dissolved in blood, was around 20mmHg at 8000m - about one fifth of normal.
However, the arterial oxygen content, which reflects the amount of oxygen bound to haemoglobin, was not nearly as reduced; it was 150ml/l, compared to a normal value of 200ml/l. This reflects the degree of acclimitisation the researchers had - non acclimaitised subjects exposed to that degree of low oxygen rapidly lose consciousness, whereas the climbers were functioning reasonably normally.

The pO2 levels recorded in this study are amongst the lowest recorded in healthy humans  and probably represent the lowest limit of tolerance.

[Link to the paper](http://www.nejm.org/doi/pdf/10.1056/NEJMoa0801581)",null,5,cdjk2e3,1r4egz,askscience,new,6
SmellyRaghead,"The amount you absorb is proportional to the partial pressure. At 1 atm., ppO2 is about o.21 atm. At 2 at., 0.42 atm, etc.

With a lower partial pressure, the less mass transfer of oxygen occurrs, obviously causing a lack of oxygen in the blood.

The opposite is also true - when diving beyond certain pressure thresholds, the ppO2 becomes far too high and can cause serious damage, as oxgen in too great a concentration is quite toxic.

In short, we are most 'comfortable' at sea level and 1 atm of total pressure.",null,0,cdjjkmo,1r4egz,askscience,new,2
irreligiosity,"It's complicated.. Your body adjusts to it's climate's oxygen levels to assure the appropriate intake of oxygen is met for cellular respiration. Hemoglobin supplies oxygen to your body. People who live at high elevation with little oxygen can have twice as much hemoglobin as those at lower elevations. This happens to ensure that there is a higher uptake of oxygen because there is so little available in the air. This is also why people have to wait at base-camp for 3 months before climbing Everest - so their body adjusts to the appropriate hemoglobin levels. If they do not wait, and your body does not get enough oxygen then the electron transport chain that is used to form ATP (energy for your body) is disrupted. This is hard on your body, and there are other effects of lack of oxygen also. 

With too much oxygen in the air your body receives too much oxygen. This does various things that are unwanted to the body. Oxygen can bind to things in your body, and it is used in important cell regulating. It is possible for the oxygen molecules to bind to protons forming a hydroxyl group which is undesirable. Too much oxygen it stressful for your bodies normal cellular functioning. ",null,3,cdjjs85,1r4egz,askscience,new,4
TangentialThreat,"They're called [fulgurites](http://en.wikipedia.org/wiki/Fulgurite).

Cool, huh?",null,1,cdjhga1,1r4em9,askscience,new,17
slumberprojekt,"You get a piece of fulgurite if you're lucky.

http://en.wikipedia.org/wiki/Fulgurite",null,1,cdjhow8,1r4em9,askscience,new,8
Problem119V-0800,"More force, yes. The light should exert twice as much pressure (aka transfer of momentum) on the reflecting surface as the black one, assuming other things are equal.

Looks like the first experimental measurement of this was in the early 1900s.

(Note that Crookes radiometers— those light powered spinning things in glass tubes that people sometimes have as toys / science demonstrators— actually work by thermal expansion of the air near the heated vane, not by photon pressure. Photon pressure is a much smaller effect. and would turn the vanes the opposite direction anyway.)

[a mildly interesting article on the subject (pdf)](http://dujs.dartmouth.edu/2002S/pressureoflight.pdf)",null,17,cdjixs9,1r4g5u,askscience,new,120
do_od,"A [Nichols radiometer](http://en.wikipedia.org/wiki/Nichols_radiometer) is essentially a small scale that measure the inertial force of light, or radiation pressure. It uses a pair of mirrors.

A perfectly reflecting scale would register a pressure twice as high as a perfectly absorbing, pitch black one. This is because of conservation of momentum. In total reflection the momentum of the light is reversed, while in absorption the momentum is transferred into the surface.",null,6,cdjj4sd,1r4g5u,askscience,new,20
SproutsCrayons,"Yes it would. The absorbed light, would be reemitted in a random direction or absorbed as heat. In both cases the net force is bigger for the mirror. 
If you are thinking about these things the gravity wave detectors, e.g. GEO600 and LISA, might interest you as they deal with these kind of problems. It might also be interesting to look in to [optical tweezers] (http://en.wikipedia.org/wiki/Optical_tweezers) , [laser cooling] (http://en.wikipedia.org/wiki/Laser_cooling) and [cavity optomechanics](http://en.wikipedia.org/wiki/Cavity_optomechanics). 

",null,1,cdjk3l8,1r4g5u,askscience,new,6
null,null,null,27,cdjo3q8,1r4g5u,askscience,new,6
LoyalSol,"Opening the window at best will make your apartment as humid as the outside, but Canada in general is a pretty dry area on the relative humidity scale so not really sure how much more opening the window will do.  ",null,1,cdjnz18,1r4geg,askscience,new,2
FatSquirrels,"That somewhat depends on what humidity you are talking about.

Absolute humidity is how much water is in the air.  Opening the window will equalize the absolute humidity between inside and outside.  Hot air can hold more moisture, but that does not mean that it necessarily has more water in it.  If it is dry outside then likely your enclosed space inside with showering, respirating humans/animals, cooking and such is actually more humid.

Relative humidity is the amount of water in the air relative to how much it can hold.  I think this is more relevant to humans as it determines the perceived temperature and how close the atmosphere is to precipitation.  Warm air can hold more water, so if you increase the temperature inside by opening a window the relative humidity will go down (assuming equal absolute humidity inside and out).  Opposite if it is colder outside.

Really what this means is that you need to know both the temperature and the relative or absolute humidity both in and outside to make this call.  Likely, your best bet is to keep your windows closed and buy a humidifier.  They aren't very expensive and they could help you out if the dry air is bothering you.",null,1,cdjs7zk,1r4geg,askscience,new,2
Dannei,"Yes, GPS jamming is possible - as with any radio communications, you just need to fill the relevant frequencies with enough ""junk"" to drown out the actual signal.

It's also possible to spoof GPS signals, by creating a much more powerful signal that overrides the normal one. This can be used to convince a receiver that it is somewhere else than it actually is - examples of possible uses include misdirecting planes/ships to another destination, by carefully changing the signals you send it over time.

In both cases, yes, the area you cover would be limited by your transmission power.",null,2,cdjmmtx,1r4orq,askscience,new,11
zerbey,"You didn't ask, but it's important from a historical point of view if you're interested in GPS.  When GPS was first deployed, there was a function known as Selective Availability, which means that the US Government could intentionally block the GPS signal to all users except those who had authorised receivers.  The basic idea was that it would cause all ""civilian"" receivers to be inaccurate, so it'd be useless for tracking.  This functionality was turned off permanently during the Clinton administration and new Satellites are incapable of using it.
",null,0,cdjsx0o,1r4orq,askscience,new,3
Bradm77,Dannei gave a good answer.  I'm an engineer and I used to work on anti-jamming devices for the military.  We built devices that would prevent enemy jamming equipment from effecting our military's GPS devices.  ,null,1,cdjpp0x,1r4orq,askscience,new,3
haikuginger,"I can talk about two things that were factors in the long development process of color TV: backwards compatibility and display technology.

These two topics are heavily linked together. For example, during the early days of color TV development, one system, CBS Color, was a filter that spun in front of the screen, sequentially showing red, green, and blue. With that system, each frame was transmitted three times sequentially; one time each as red, green, and blue. The TV would time the spinning of the disk to its reception of each colored frame; the red frame would be drawn on the screen while the red filter was in front, and so on.

However, to see why that wouldn't work very well, we need to take a step backwards to see how black and white TV worked. Reduced to a basic level, a stream of electrons was fired at the inside of a glass screen coated in phosphorus. When electrons struck the phosphorus, it lit up. By using electromagnets, that single beam of electrons could be moved to point at any part of the screen. 

So, to form a picture on the screen, the electron beam would sweep across the screen, line by line, with the intensity of the beam determined by the amplitude of the TV signal arriving in the set at any given moment.

So, if you fed a CBS color signal into a black-and-white TV, it simply wouldn't work. TV at that point relied on the signals coming down exactly as the set expected them to, and on being able to ""line up"" those signals so that the first bit of a frame arrived at the TV when the electron beam was pointing at the top-left corner of the screen. CBS color just couldn't do that.

So, an alternate route needed to be taken. Color TV couldn't use an additive color space (composed of separate red, green, and blue signals), it needed a subtractive color space- one that started out as a complete monochrome picture (with equal amounts of red, green, and blue) that any black-and-white TV could read, but that had bits of extra information carried alongside it that could be used to turn that monochrome picture into color.

This introduced a new challenge, though. TVs at that point in time didn't have any ""memory""- they were sending the signal from an antenna or cable directly to the picture tube. If what was coming in didn't get drawn on the screen immediately, it wasn't getting drawn at all. And, with a subtractive color space, all the color was coming in at once. Unlike with CBS color, a TV couldn't use a mechanical trick like a spinning wheel to alternate between colors being drawn by a single electron beam- it needed to draw all the colors at once.

So, the logical answer to that problem was to go from using one vacuum tube producing an electron beam to three; one for each of red, green, and blue. Each tube would fire only when aimed at a bit of phosophor designated specially for it; each bit of phosphor would have a tiny filter in front of it so that, from the front, it would appear to glow red, green or blue.

As you might guess, this arrangement introduced a new problem. Before, it didn't matter if your electron beam was aligned exactly right or if it was particularly fine; the entire tube of your TV was coated in phosphor, and the picture would show up no matter what, even if the electron beam was angled over to the left a bit.

With the new arrangement, the problem was that the electron beams weren't fine enough and couldn't be aligned exactly right 100% of the time. This meant that ""blue"" phosphor dots were being lit up by the ""red"" electron beam- and the same for every other color. A reliable color picture wasn't being produced.

That's where Werner Flechsig's shadow mask comes in. The concept is simple, but brilliant. Take a metal sheet, and poke thousands of tiny holes in it. Then, put it directly behind the screen, which has your pattern of differently-colored phosphor dots on it. Now, because you're using three different electron beams, they're each coming from a different location- and each will enter any particular hole in the shadow mask, as the metal sheet is called, at a different angle. This allows you to restrict where the electrons from each beam land on the screen.

You can try it yourself. Take a sheet of paper and poke a hole in it. Then, holding the paper still, look through that hole from different angles. You'll see different things through it. Similarly, even if the electron beams were too big to focus and align down to the sizes they needed to be, shooting them through a tiny hole from slightly different places resulted in precise control over which points on the screen they struck.

Those are the things that were necessary for color TV to take off. We needed a way to send a color TV signal that could be viewed by people with black-and-white TVs. We needed a way to put each component color of that color signal on the screen simultaneously. And we needed a way to  keep each color at the point on the screen it needed to be.",null,0,cdjv089,1r4qud,askscience,new,11
cheeseynacho42,"Well, a huge obstacle towards broadcasting colour television was the massive amount of bandwidth it took. This went away slowly with time, as we got better and better at transmitting radio signals. 

As far as the actual television goes, there were a lot of ideas as to how you could make a TV colour. There was combining three monochrome images (red, green, and blue ones) to make a colour one, and shooting electrons at a phosphor plate (knows as ""Telechrome""), but probably the best-known one is Technicolour.

Technicolour evolved a lot over its lifespan, and while itw as eventually replaed, throughout most of the late 40s and 50s it was the dominant way of filming in colour and broadcasting in colour. Early Looney Toons used Technicolour, and the Wizard of Oz was filmed using it. 

Also, the way you talk about black-and-white TV makes it sound like we could have filmed in colour, but chose not to. That's not the case at all - it was very hard to figure out how to properly capture colour on film, much harder than filming black and white images. They weren't stripping the colour, or changing it, that was just what the film could capture. It's like asking the fish that live in the Abyssal zone why, when there's so much in the world to see, they only have light-detecting discs as eyes. It's the best they can do. ",null,0,cdjs5gp,1r4qud,askscience,new,1
PorchPhysics,"All atoms have small motion from the kinetic energy at an atomic level.  When you put matter under pressure, you increase the number of collisions between neighboring atoms which leads to more chaotic motion of atoms and constant transferring of this energy.  Planets are constantly applying pressure to their cores due to the outer layers pushing inward, which adds more energy and so on.

it can get much more detailed than this, but that is the general idea.",null,1,cdjqyq1,1r4r9g,askscience,new,7
chrisbaird,Gravitational potential energy is converted to heat when the gravity of an astronomical body crushes it to a higher pressure.,null,0,cdjsju8,1r4r9g,askscience,new,5
goldistastey,"The original theory was that the heat comes from the leftover formation energy of the planet. After all, for heat to diffuse through the entirety of the earth into space would take billions of years. The calculations, however, found that this does not account for all the heat, so now scientists think that it's a mix of the contributions of radioactive decay and formation energy. Consensus is that there is no fusion in the Earth I believe, btw.",null,0,cdjza2l,1r4r9g,askscience,new,1
The_Serious_Account,"Let's take the Riemann hypothesis. It's statement about *all* primes. The naive approach is to let a computer run through all primes, one by one, and check if the hypothesis is correct for that prime.

You know how many primes there are? It's a lot. Like a lot a lot. Infinitely many primes, in fact. Such a computer would need to do infinitely many calculations. Neither normal or quantum computers are capable of such a thing. 

So unless we discover more exotic forms of computation, this approach will never work. ",null,1,cdjqhva,1r4x1p,askscience,new,15
Vietoris,"How do you think a computer work ?

Computers are not magical creatures. They can do only what we teach them to do. They cannot solve problems that humans cannot solve.


The only thing computers can do better than us is doing very large computations in a very short amount of time. But it is important to understand that any computations that can be done by a computer could be done by a human using paper and pen. It will just take the human much more time.

Now the last piece of thing you need to understand is that almost all interesting unsolved maths problems are not computational problems, but theoretical ones. It's not the computational power that we lack, but the method to solve the problem.",null,5,cdjrs1v,1r4x1p,askscience,new,15
farhaven,"Because these math problems are not about calculating some large number. They are about fundamental properties of numbers and their relations. It takes a human mind to analyze these relations, form theories and finally proove them. Computers can aid in this, for example by doing prime factorization of very large numbers, but they are only a tool, never the solution.",null,1,cdjqfii,1r4x1p,askscience,new,4
selfification,"All proofs come down to starting with a fixed set of rules, a set of start points and a goal and then figuring out a way to use the set of rules you have to somehow transform the start points into the goal.  It's like being given a maze, a starting position and set of simple rules like ""you can go forward by 2 spaces"" or ""you can turn left, but only if you've already visited the square or your right"" or stuff like that.  If the maze is fairly limited, then a computer can just exhaustively search it faster than a human can intuit his way through it.  If the maze is extremely large, then exhaustive searches are out.  A computer stills needs to be taught heuristics - how to search through a large search space without doing unnecessary work.  These heuristics are human heuristics and are limited by how clever we are.  We do have machine learning algorithms and expert learning systems that try to emulate human pattern matching and inference skills, but these are still programmed to emulate some fixed, limited subset of human intuitions and skills.

That said, if we do have a certain intuitive ""inkling"" as to how a proof needs to proceed, we can easily program computers to exhaustively verify the parts that are impractical for a human to do.  See http://en.wikipedia.org/wiki/Four_color_theorem .  The first proof of this was based on a *massive* computer generated proof that generated 2000 or so possible cases and then solved every one of them mechanically using one set of techniques.  No human could have walked through all of those quickly or correctly.  The output proof was virtually uncheckable by humans.  One could verify that the computer code was correct, but the proof itself was too voluminous to verify by hand.  That's one example where computer step in to *assist* humans in tasks that we're not good at.",null,0,cdk367m,1r4x1p,askscience,new,2
Glimt,"Actually, we can, but it will take very long time. We can define formally when a string of letters is a proof of a given sentence with a given set of axioms (first-order logic). It is then easy to write a computer program to check if we have a proof.

Now all we need is for a computer to check all strings, in some order, and check if any of them is a proof of a given conjecture, or of its negation. If the conjecture is not undecidable, the program will surely halt.

This method is, of course, completely non-realistic, due to the number of strings that need to be checked.",null,0,cdkeomw,1r4x1p,askscience,new,2
jjandre,"From a layman's perspective, computers are programmed with math, the math to make those promblems machine solvable has to be written. You'd have to either already have the solution, or be pretty confident you were already headed pretty deep in the right direction of a solution just to write the code. Sounds like more of a job for AI. ",null,0,cdjzuof,1r4x1p,askscience,new,1
winduken,"A typical use of computers to solve an interesting math problem takes two steps. First, analyze the problem to boil down to some possible large but finite number of configurations that can be checked for correctness. Then, either check each configuration by hand or write a computer program to do the checking instead. The 4-color theorem for planar graph (http://en.wikipedia.org/wiki/Four_color_theorem) was solved this way. Unfortunately, it isn't obvious how to do the first step for most interesting math problems.

Incidentally, the P = NP problem (http://en.wikipedia.org/wiki/P_versus_NP_problem) in computer science is one the Millenium problems. It asks a simple question: Given a combinatorial problem, if every guessed solution can be checked for correctness by a computer in a number of steps bounded by some polynomial, is there an actual algorithm to come up with a solution in a number of steps bounded by a (possibly different) polynomial?",null,0,cdk83hh,1r4x1p,askscience,new,1
rapist1,"I am surprised to find that all the comments this far do not answer your question correctly, so I will. 
You are right to think that a computer can try to brute force search for a proof of any statement in your axiomatic system, however if your axiomatic system can express 0,1,2,3... etc, then you will never stop searching for true statements. That is sort of redundant however, but it gets worse; even if you pick a specific sentence you want to search for a proof or disproof of, the search may never halt because some sentences are ""independent"", and cannot be proven true or false in those axioms. It is stated in the second paragraph of [Godel's incompleteness theorems](http://en.wikipedia.org/wiki/G%C3%B6del's_incompleteness_theorems)",null,0,cdn9tdl,1r4x1p,askscience,new,1
wishfulthinkin,"You feel with your sensory neurons, and the vast majority of your sensory neurons are concentrated on your skin and especially at your extremities (finger tips, toes, face).  As such, there are very few sensory neurons in your internal organs, so it's very difficult to localize feeling in your intestines.  However, there is an abundance of nociceptors (pain receptors) in your internal organs as well as your skin, in case of injury.


That is because the purpose of pain is to alert the brain about bodily damage, and to condition you to avoid pain causing activities in the future.  Damage can be caused everywhere, but light touch and pressure sensing is only necessary on your skin and extremities.",null,0,cdjxo4r,1r4yaq,askscience,new,2
assay,"Heya mate, you can feel pain in your intestines, it just doesn't localize well.  Your spleen and liver pain localizes a bit better, but what you're feeling as sharp pinpointable pain is due to irritation of the peritoneum (a lining in your abdominal cavity that has somatic (and not visceral) pain receptors.  Take another example: appendicitis.  It often starts out as pain in the mid epigastric area above the belly button, but once the infected appendix touches the peritoneum lining the abdominal cavity, the pain becomes sharp and very localized to the lower right quadrant.",null,1,cdjsp6m,1r4yaq,askscience,new,2
Dannei,"I would argue that globular clusters *are* part of the galaxies they're orbiting - galaxies have a lot of structure below their large-scale size! There's no real reason for the clusters to break up - like the Solar System, they are bound much more strongly to themselves than to the galaxy they are part of. They also spend much of their time very far away from anything else that is very massive, such as other clusters or molecular clouds, and so don't get disrupted. However, we do see evidence of globulars being disrupted as they pass through the disc of the MW on their orbits - these tend to form tidal streams of stars that precede/follow the cluster in its orbit, as well as ""stealing"" some.

Clusters (both globular and open) are generally thought to be made by the collapse of massive clouds of gas. These form large amounts of stars in a small volume of space, and so the stars remain gravitationally bound to each other. Specifically talking about globulars, they are suspected to have been made during the formation of the galaxy, collapsing to form stars before the rest of the Milky Way had fully formed - hence why their orbits do not resemble those of stars in the disc, which were formed at a later stage.

For reference, open clusters are those that have formed, and are still forming, in the disc - these do tend to break up quite quickly, firstly as they become unstable once the gas cloud they are forming from dissipates, and also due to gravitational interactions with other structures in the disc.",null,0,cdjmj5f,1r4ym5,askscience,new,4
Davecasa,"A 200 meter telescope operating at the diffraction limit would be able to resolve objects about 1 meter apart on the surface of the moon using visible light. One meter resolution makes a lunar lander about 4 effective pixels wide. Probably not quite good enough to actually confirm what you're seeing, but I'll call 200 meters a lower bound. The largest optical telescope on Earth has a 10.4 meter primary mirror.

The Lunar Reconnaissance Orbiter has a 0.195 meter primary mirror, and photographed Apollo landing sites from an altitude of 31 km. This should give it a resolution of about 10 cm, but the stated resolution at this altitude is actually 50 cm, so diffraction on the primary mirror is not directly a limiting factor on resolution. [Here's an example image of the Apollo 11 landing site](http://featured-sites.lroc.asu.edu/view_site/14).",null,1,cdjtjo8,1r4z3k,askscience,new,5
PorchPhysics,"I don't know about being able to see them from Earth, and I actually doubt it.  HOWEVER, i do know that the LRO (Lunar Reconnaissance Orbiter) did manage to photograph the landing site of Apollo 11 in enough detail to make out the small paths left over of the footprints surrounding the remains of the LEM as well as a small excursion that Armstrong took to a nearby crater.",null,0,cdjrmu4,1r4z3k,askscience,new,2
SigmaStigma,"The basis of electrogenesis is derived from a salt pump. These pumps generate ion gradients with an electrical potential. It uses sodium (Na^+) and potassium (K^+) ions, which are actively pumped in or out, to generate en electrical potential. Remove K^+ and you'll get a negative charge inside of a membrane. [This article](https://en.wikipedia.org/wiki/Resting_potential) goes into depth on that topic.

Some animals have evolved mechanisms that use these ion gradients in pretty interesting ways in what are called electric organs. You can think of them as essentially batteries, which they are constantly charging. These battery-like muscles store a charge, and the animal discharges them for whatever purpose they use them for. Some use it to paralyze prey, others as defense, and others as communication and navigation.

Electric eels aren't the only ones with electric organs. [Atlantic stargazers](https://en.wikipedia.org/wiki/Atlantic_stargazer) have electric organs derived from the sonic muscles. In other fishes the sonic muscles produce vibrations on the gas bladder. The croaker, or drum, is a good example of a fish that uses sonic muscles. Another stargazer species has its electric organ derived from eye muscles.

The [elephant fish](https://en.wikipedia.org/wiki/Mormyridae) use them in very turbid, or cloudy, waters to navigate.

And on a related note, some animals have electro-recepting organs. Sharks, for example, have what are called [ampullae of Lorenzini](https://en.wikipedia.org/wiki/Ampullae_of_Lorenzini), with which they can detect the muscle movements of animals. This is helpful for burrowing in the sand or mud for food that may be hiding.",null,1,cdk3zgw,1r4zeu,askscience,new,4
then_and_again,"bad breath is caused by bacteria in your mouth. One of the mouth's best ways of cleaning itself is saliva flushing, where saliva carries the bacteria down your throat and into the stomach. When you sleep, saliva production generally slows down. No saliva, no cleaning, and therefore, bad breath.",null,0,cdjub41,1r509m,askscience,new,12
then_and_again,"It's most likely due to the timed expression of different genes. Not all genes are expressed at once; at certain points your body will activate genes that code for the expression of, say, a mustache or increased bone growth, etc. Most likely the hazel and dark brown genes were not being expressed until you hit a certain age. Since brown hair and hazel are caused by increased melatonin expression you wouldnt even need to 'turn off' the blond/blue gene, just upregulate the melatonin gene. Hope that made sense...",null,0,cdjuk2g,1r5110,askscience,new,1
chrisbaird,"Spacetime itself contracts and not just the objects in spacetime. The size of objects, atoms, etc. *relative to their own spacetime* stays constant. In other words, if a spaceship contracts in half due to high velocity, a ruler in the rest frame of the spaceship will also contract in half, so measuring the length of the spaceship using that ruler will give you the same number, no matter what frame you are in. This is important, because the laws of physics must be the same in all frames even despite relativistic effects. ",null,1,cdjt5fg,1r513d,askscience,new,9
NGC2467,"A Planck length is just a unit of measurement. It has no special significance. (Quantum field theory is often said to ""break down"" at the Planck length, but you could just as easily say it breaks down at two times the Planck length, or pi times the Planck length, or 0.12345 times the Planck length) What is a Planck length-long stick in a comoving frame will not be a Planck length-long stick in another, the same way that a meter-long stick will not be a meter-long stick in a different reference frame.",null,3,cdjtr82,1r513d,askscience,new,6
KerSan,"First, I want to be clear about a misconception you share with many professionals. Objects do not appear to shrink at relativistic speeds so much as they rotate in space-time. The ""Terrell rotation"" is pretty well explained by [this YouTube video](http://www.youtube.com/watch?v=JQnHTKZBTI4) (bad quality, sorry). Other replies mention this.

As for your question about the effect on various particles, you should remember that elementary particles are point-like -- they don't really have an extent in space beyond the spread of their position-basis wavefunction. The position-basis wavefunction can indeed undergo relativistic effects, but the more important consideration is the energy of the particles.

Finally, your question about the Planck length. The Planck length is just a unit of distance, like a meter or an inch. If you run really fast, do you think that the inch changes? No, it's a definition. The number three also doesn't change if you run really fast, either. The Planck length is an abstract quantity, not a physical thing.",null,1,cdk2gae,1r513d,askscience,new,2
null,null,null,4,cdjqxe2,1r513d,askscience,new,3
abstrusey,"Veterinaran here: I know of no evidence to support that idea.

I was unable to find the source, but there have been some instances of different species that were so phenotypically similar (I believe the distinction was visual patterns in birds or moths) that they confused other non-same species as a member of their own species. It resulted in sufficient unsuccessful matings that one of the species was ultimately wiped out (or could no longer be found). 

Ultimately, it begs the question of defining ""mating preferences"" more clearly. Not many species mate for pleasure. Also, it appears that they are biologically driven to reproduce because they are wired that way, but it's not evident that they have any conscious ""thought"" in the decision of whether to mate or not to mate with another animal.",null,0,cdl10a3,1r52dn,askscience,new,3
wishfulthinkin,"Clots are created by platelets and fibrinogen, which form fibrin, a web of solids that clots consist of.  Normally, this happens when blood pools at the location of a cut, forming a scab to prevent infection and allow for the formation of new skin or scar tissue.  The fibrin cannot be formed when blood is circulating at normal speeds, as any beginnings of a web will be destroyed by the moving fluids around it.


This has many benefits but some disastrous consequences.  If our blood did not behave in this way, every time we got a cut (on our skin, or internally, anywhere) we would slowly bleed out until we died, if we didn't die from infection first.  However, sometimes this will result in fatal blood clots, aneurysms, etc.",null,1,cdjwdjx,1r52wz,askscience,new,6
S7R4nG3,"You should check out [Newton's Law of Cooling](http://www.ugrad.math.ubc.ca/coursedoc/math100/notes/diffeqs/cool.html). Its a fairly simple differential equation that has solutions as an exponential increase or decay that, in your case, would be dependent on the thermal properties of the cups.

You end up with an equation like:
T (t) = Ta + (To - Ta) e ^ -kt. 

-Where Ta is the surrounding temp. of the room.

-To is the initial temp. of the water in the cup.

-k is a unitless constant based on the thermal properties of the cup. The negative value is due to the solution of the original differential equation.

-t is the amount of time the cups are left in the room.

-T(t) is the temp. the cup falls to after time t, has elapsed.

Obviously to find one value, you would need to know the other three.

Edit: I can't read the link I posted.
",null,1,cdjrze0,1r53ot,askscience,new,4
SonOfOnett,"Since it's equally valid to model the situation as heat dispersing or cold dispersing with the diffusion equation, in an ideal situation they will both reach room temperature at the same time.

In actuality the thermal conductivity of the water and cup will actually vary with temperature, meaning that it might be easier to transfer heat into or out of the cup at higher/lower temperature. Therefore the answer actually depends on the equilibrium temperature you are trying to reach.

Another complicating factor is phase transitions, but I think you want both the cups to be full of liquid water at and the room to be at standard temperature and pressure.
",null,4,cdjt7hb,1r53ot,askscience,new,5
cmuadamson,"The amount of energy required to change the temp of a liquid by 1 degree does depend on the starting temperature.  Imagine you have a pipe from a steam plant that it used to heat buildings (This is a real scenario, there are underground steam pipes all over Pittsburgh, to the hospitals, universities, etc)  Suppose the steam starts at 500 degrees and goes into building 1, comes out at 400 degrees, then goes into building 2, and comes out at 300 degrees.

Both customers seemed to take 100 degrees out of the steam. If you bill customer 1 the same as customer 2, you're cheating customer 2, because 1 got more energy out of the steam. This is the ""enthalpy"" of the steam. No, I did not *mithpell* that, it is [enthalpy](http://en.wikipedia.org/wiki/Enthalpy).

Similarly, if you look at the energy difference between room temp and your cold cup and hot cup, the hot cup has a bigger enthalpy difference, and has to radiate more energy to get to room temp  than the cold cup has to absorb to get to room temp. So I would say the hot cup will take longer.
",null,0,cdjx5gp,1r53ot,askscience,new,1
null,null,null,2,cdjqppn,1r53ot,askscience,new,1
zmil,"Think of the human genome like a really long set of beads on a string. About 3 billion beads, give or take. The beads come in four colors. We'll call them bases. When we sequence a genome, we're finding out the sequence of those bases on that string. 

Now, in any given person, the sequence of bases will in fact be unique, but unique doesn't mean completely different. In fact, if you lined up the sequences from any two people on the planet, something like 99% of the bases would be the same. You would see long stretches of identical bases, but every once in a while you'd see a mismatch, where one person has one color and one person has another. In some spots you might see bigger regions that don't match at all, sometimes hundreds or thousands of bases long, but in a 3 billion base sequence they don't add up to much. 

edit 2: I was wrong, it ain't a consensus, it's a mosaic! I had always assumed that when they said the reference genome was a combination of sequences from multiple people, that they made a consensus sequence, but in fact, any given stretch of DNA sequence in the reference comes from a single person. They combined stretches form different people to make the whole genome. TIL the reference genome is even crappier than I thought. They are planning to change it to something closer to a real consensus in the very near future. My explanation of consensus sequences below was just ahead of its time! But it's definitely not how they produced the original genome sequence.

If you line up a bunch of different people's genome sequences, you can compare them all to each other. You'll find that the vast majority of beads in each sequence will be the same in everybody, but, as when we just compared two sequences, we'll see differences. Some of those differences will be unique to a single person- everybody else has one color of bead at a certain position, but this guy has a different color. Some of the differences will be more widespread, sometimes half the people will have a bead of one color, and the other half will have a bead of another color. What we can do with this set of lined up sequences is create a *consensus* sequence, which is just the most frequent base at every position in that 3 billion base sequence alignment. And that is basically what they did in the initial mapping of the human genome. That consensus sequence is known as the reference genome. When other people's genomes are sequenced, we line them up to the reference genome to see all the differences, in the hope that those differences will tell us something interesting. 

As you can see, however, the reference genome is just an average genome*; it doesn't tell us anything about all the differences between people. That's the job of a lot of other projects, many of them ongoing, to sequence lots and lots of people so we can know more about what differences are present in people, and how frequent those differences are. One of those studies is the 1000 Genomes Project, which, as you might guess, is sequencing the genomes of a thousand (well, more like two thousand now I think) people of diverse ethnic backgrounds. 

*It's not even a very good average, honestly. They only used 8 people (edit: 7, originally, and the current reference uses 13.), and there are spots where the reference genome sequence doesn't actually have the most common base in a given position. Also, there are spots in the genome that are extra hard to sequence, long stretches where the sequence repeats itself over and over; many of those stretches have not yet been fully mapped, and possibly never will be.

edit 1: I should also add that, once they made the reference sequence, there was still work to be done- a lot of analysis was performed on that sequence to figure out where genes are, and what those genes do. We already knew the sequence of many human genes, and often had a rough idea of their position on the genome, but sequencing the entire thing allowed us to see exactly where each gene was on each chromosome, what's nearby, and so on. In addition to confirming known sequences, it allowed scientists to predict the presence of many previously unknown genes, which could then be studied in more detail. Of course, 98% of the genome *isn't* genes, and they sequenced that as well -some scientists thought this was a waste of time, but I'm grateful the genome folks ignored them, because that 98% is what I study, and there's all sorts of cool stuff in there, like ancient viral sequences and whatnot.

edit 3: Thanks for the gold! Funny, this is the second time I've gotten gold, and both times it's been for a post that turned out to be wrong, or partly wrong anyway...oh well.",null,198,cdjoort,1r54d1,askscience,new,1053
Chl0eeeeeee,"Even though everyone has unique DNA, genes still would occur in the same location in the genome (exclusive of any mutations that would add/delete a nucleotide). Basically what genome mapping does is look at multiple samples of DNA from different people. It aims to understand what regions are coding versus non-coding, and to annotate the genome (see what the coding genes control). This has been done for other species. ",null,17,cdjopps,1r54d1,askscience,new,77
Tass237,"Complete mapping of what sections apply to what.  A redhead and a blonde both have a gene for hair color, and the location of that hair color gene can be mapped.  The fact that they have *different alleles* doesn't mean it's a different gene or in a different location.",null,7,cdjq7af,1r54d1,askscience,new,29
zedrdave,"In addition to other answers in this thread, one important clarification: when one says that a person's DNA is unique, that's still no more than somewhere around a 0.01% difference, out of the entire sequence, between two individuals.

Most nucleotides (the small bricks that make the DNA sequence) are the same for all individual of the same species (humans, for instance), with a very few single nucleotides changing here and there (these changes are called [SNPs](http://en.wikipedia.org/wiki/Single-nucleotide_polymorphism)). Just the same way that moving a single cog in a complex mechanism, or modifying a single byte in a computer program, will give out a completely different result, that single nucleotide modification can have huge consequences on the person's appearance, health etc.

Mapping the first genome, meant mapping *a* genome (with its specific SNPs), with the implicit idea that we were first interested in the parts that were common to everybody. Now that sequencing is a lot cheaper and more widespread, there are a number of efforts to map genomes for a number of individuals, in order to figure out more specifically which positions in the sequence can occasionally differ (see ""1000 genome project"").

**Edit:** I should have also mentioned that, while some SNP variations have huge effects on the resulting organism, other SNP mutations are completely silent (""synonymous mutations""), thanks to the redundancy of the DNA-Amino Acid transcription code (i.e. different triplets of DNA can end up coding for the same AA). Because such silent mutations do not affect fitness (and therefore are more likely to be passed down), they are a lot more common than you would expect from pure chance.",null,1,cdjud86,1r54d1,askscience,new,9
tdcarlo,"Each person's DNA is unique, that is true.  But the difference between you an me is incredibly small.  

DNA is made up of nucleotides.  There are four kinds of nucleotides.  Think of nucleotides as legos each kind being a different color....let's say Aqua, Green, Cyan, and Teal.  A gene is composed of nucleotides in particular order.  Imagine stacking legos.  Using the first letter of the colors from the legos, the insulin gene is 450 nucleotides long and looks like this.

Aqua Green Cyan Cyan Cyan Teal Cyan Aqua GGACAGGCTGCATCAGAAGAGGCCATCAAGCAGATCACTGTCC
TTCTGCCATGGCCCTGTGGATGCGCCTCCTGCCCCTGCTGGCGCTGCTGGCCCTCTGGGGACCTGACCCAGCCGCAGCCTTTGTGAACCAACACCTGTGCGGCTCACACCTGGTGGAAGCTCTCTACCTAGTGTGCGGGGAACGAGGCTTCTTCTACACACCCAAGACCCGCCGGGAGGCAGAGGACCTGCAGGTGGGGCAGGTGGAGCTGGGCGGGGGCCCTGGTGCAGGCAGCCTGCAGCCCTTGGCCCTGGAGGGGTCCCTGCAGAAGCGTGGCATTGTGGAACAATGCTGTACCAGCATCTGCTCCCTCTACCAGCTGGAGAACTACTGCAACTAGACGCAGCCCGCAGGCAGCCCCACACCCGCCGCCTCCTGCACCGAGAGAGATGGAATAAAGCCCTTGAACCAGCAAAA

So we know what a gene is...the next thing to understand is a chromosome.  A chromosome is a long stack of DNA that contains numerous genes.  There are 23 chromosomes in the human genome.  The longest human chromosome is about 250 million nucleotides long the shortest is around 50 million nucleotides.  Each chromosome contains hundreds of genes along with some other ""accessory"" DNA that is beyond the scope of this explanation.    The entire size of the human genome is around 3 billion nucleotides.

Human being the clever types have been able to determine the precise order of all of the nucleotides in each human chromosome and have identified most if not all of the genes on it.  So each chromosome has the location of each gene mapped. Pretty amazing.

Your DNA is unique but the percentage of the 3 billion nucleotides that are different than mine is less than 0.0001% and most of the differences will be in the so called ""accessory"" DNA.  ",null,0,cdjtoip,1r54d1,askscience,new,8
nanoakron,"I feel the need to write this because whilst all the previous commenters have gone into great depths to explain the science behind genes and genomes, they have failed to address a fundamental misunderstanding the OP has:

Your DNA is **NOT** unique. Only about 0.1% of it is. You are somewhere around 99.5-99.9% **genetically identical to every other human on the planet**.

You're also 98.8% **identical** to every chimpanzee, 98.4% **identical** to every gorilla, 88% to every mouse, 65% to each chicken and 47% genetically **identical** to a fruit fly.

This means you have the exact same codes (give or take a letter) for the most essential 'housekeeping' functions - the ones that process energy in your cells, allow your cells to reproduce, build cell walls, cell skeletons and the other basic stuff all multicellular life needs to do. As a side note, this is very strong evidence that these abilities evolved only once in a distant ancestor, and then because they were so successful compared to all species around at *their* time, they outcompeted them and all their descendants now share those genes.

The closer you get to a human in genetic relatedness, the similarities extend beyond simple housekeeping genes to those which allow us to be 4-limbed, air-breathing, visually-dominant omnivores. Cows are 4 limbed - we share the same genes which switch on in embryonic development which cause 4 limbs to develop. We *also* share these with fish - after all, these are the genes which were first used to make fins, they were just 'repurposed' to make limbs through mutation and natural selection.

And so on with all 30,000 genes that make us human. We're not even genetically the best at doing many things in the animal kingdom - plants 'eat' sunshine, some bacteria detoxify alcohol better than we can, and as for our radiation susceptibility, we're pathetic. We just so happen to carry the baggage of every creature that came before us that was able to reproduce.",null,3,cdk283p,1r54d1,askscience,new,10
knobtwiddler,"I work in genetic informatics and we sequence and analyze human genomes.  ""complete mapping,"" rather optimistically, means is that we have assembled a reference genome of a number of pooled humans' gene sequences, so we know where a typical human's sequences fall in the chromosomes from beginning to end (around 50 billion base pairs).  This assembly is used as a reference to compare against.  Currently we are using a reference genome sequence called HG19. HG20 (human genome v20) is coming
out soon.   It's an ongoing process.


From this reference genome we can align pieces of sequenced dna from samples in an effort to to say where those pieces of dna came from in the genome.


This is far from an exact science, and there are large portions of the genome for which we have no clue about their function.  However we have identified around 56,000 protein-coding genes (the exome) and a large number of ""intronic"" non-protein-coding regions which do code for RNA (lncRNA), some of which are functional, most of which we don't know anything about (previously referred to as ""junk dna""). 


believe me though, as far as understanding the function of all these genes, let alone the non-coding regions, the process is far from complete.",null,0,cdjqpyk,1r54d1,askscience,new,7
BillieHayez,"How interesting that you ask this question today. Fred Sanger, a pioneer in the mapping of the human genome, aged 95, and winner of two Nobel prizes has just passed. Maybe you were tuned in this morning, as well.",null,0,cdjqzu4,1r54d1,askscience,new,6
jams2014,"Think of the Genome like the spec sheet for a car, except it's been broken up into 46 text files and compressed so that the data is all mashed together into 46 strings, and somewhat difficult to parse out. Somebody didn't comment their code. If we were just trying to read the strings, and infer what they mean, we would fail. But luckily! there's also an automatic, computer-controlled factory that reads the strings and builds stuff! (Cells in the body.)

In the simplest sense, genome mapping is about making the factory build from *parts* of these strings, so that we can see what they do.  Imagine that you run your fictional automatic car factory like normal - it builds you a hot little red Corvette. Now imagine that you take part of the instruction string and copy/paste/copy/paste that part until you've made that section repeat a bunch of times. When you run the factory again, the car comes out a *deep, vivid red* instead of the ordinary red from before. 

You've found a gene for the paintjob, but you don't know for sure whether you've found the gene for red paint only, or for the whole thing. Now, that section might be a little bit different in someone else - like, maybe it's a different color. If you enhanced that section in someone else's instruction sheet, maybe you'd go from blue to a more vivid blue (if all of the color selection is in that part). Or maybe you would just add red, so that someone's purple paint would approach pink.

Anyway, what you've found is the meaning of a section of the instruction sheet, but it can be difficult to determine exactly which of the machines are activated by each string. Sometimes the instructions trigger other instructions, and wind up causing lots of parts to move. Sometimes they trigger something very tiny - like spinning a part of one machine. And sometimes they don't do anything at all (like bits of commented-out code). And sometimes they do something, but don't appear to unless certain conditions are met - imagine instructions to turn on or off some safety feature on the factory floor.

- EDIT - 

To perfect the analogy - we're not talking here about running the whole apparatus to create new cars. That would be like making changes to an embryo's genes and letting them grow up, which is unethical.

It's more like flipping switches in the factory while the assembly line is down, just to see which machines start to spin, or spray paint. ",null,1,cdjw201,1r54d1,askscience,new,4
tsacian,"The best way to understand what scientists are doing with the human genome, it is best to look at a much smaller and simpler genome (such as the **Japanese Rice Genome Project**).  It is simpler because the rice being mapped only has 9 chromosomes, whereas humans have much more.

http://rgp.dna.affrc.go.jp/E/GenomeSeq.html

Here you can click on a chromosome and literally see the sequences which have been directly mapped.  The difference is the wealth of knowledge already learned from this project due to its ""simplicity"", such as finding genes responsible for specific proteins and tracing them all the way back to the base pair patterns.  You can search through the big discoveries, and even look for specific proteins.

Click on chromosome 1 and then click the link for the first accession.  This first set has 31,687 base pairs (bp) (think ATCG).  You can then click on a gene and see the sequence that scientists believe is responsible for a gene.  The reason it is a ""gene"" is because it has the correct properties for coding of a gene, including a start sequence (a pattern they look for that is typical for the beginning of a gene), and a stop sequence (called codons).

Additionally, you can click and see a specific pattern of base pairs responsible for coding an mRNA and even specific proteins.  Using these ""Maps"", scientists can study each chromosome and find which genes are responsible for specific attributes of the organism.  We can find which sections of DNA are responsible for specific proteins, and use that to find mutations that result in the absense or mutation of a protein that causes harm in an organism.  There is really a wealth of information.",null,0,cdjwez6,1r54d1,askscience,new,3
XSlayerALE,"Mapping the Human Genome is like identifying the parts of a car. Sure, a wheel can be Pirelli, Firestone, Goodyear or whot not but we know its a wheel and its not the axle or the brakes or that funny triangle sign on your dashboard that no one really knows what it does....
",null,0,cdk6q2y,1r54d1,askscience,new,3
futuregp,"simply speaking, think that all humans have the same genes that have specific functions (and every human being needs these to be considered human)

but each gene can have different traits (blue eyes, brown eyes etc)

complete mapping of the human genome is to identify all those functional parts of our DNA (most of our DNA is technically not 'functional' and doesn't play a part in protein synthesis)

Each functional part ('functional gene') would have different traits, and every human being is composed of permutations/combinations of these millions of gene traits combined (e.g. let's say we only have 2 genes, A/B. Gene A has 2 traits - male (m) or female (f). Gene B has 2 traits - tall (t) or short (s).

I'm a short male. I would have A(m), and B(s) genes. You are tall and female. You would have A(f), and B(t) genes.
We're both unique, but that doesn't mean you have to map both of us to realize that there are 2 genes.

By mapping a single human being, you can map all the genes of the human genome. The uniqueness comes not from which 'gene' you have but which 'trait' of the gene you have.",null,0,cdjot9c,1r54d1,askscience,new,3
Drfilthymcnasty,"I may be wrong, but I think a complete ""mapping"" means a complete understanding of all the functional genes in our DNA. So while we may know the general sequence of nucleotides, our understanding of how/why certain segments get translated into proteins is not yet complete. Also we still have a long way to go understanding epigenetic changes and controls.",null,3,cdjouzf,1r54d1,askscience,new,6
liteerl,"Each person's DNA is unique, but their DNA differs from others at certain genes. You could record the common variants of each gene (these variants are called alleles), although certainly you would be very like to miss some of the variation. Individual's DNA sequences are not all completely random, but differ from each other in predictable ways.",null,0,cdjrxb1,1r54d1,askscience,new,2
shanebonanno,"Everyone's DNA is unique, however, nearly all of it is shared with every human on the planet. Only a very small part is unique. When scientists talk about the genome of any given species, they basically mean a list of the genes in the DNA of the species and eventually what they do.",null,0,cdjuwzx,1r54d1,askscience,new,2
dreamhunters,"Or think about it this way: it is not some much about the content but about the placement. The genes are somewhere in the genome, their position is much more fixed that the genes themselves. That is why we use mapping, because as with a map it is about location. ",null,0,cdk9y90,1r54d1,askscience,new,2
Hillsbottom,"I am a biology teacher and I use the following analogy. 

Think of the genome as a recipe to make bread. A recipe is basically a list of instructions that need to be followed in a particular order to get the desired result. These instructions are analogous to genes. 

Bread is not all the same; you get white, brown, wholemeal granary, bananana, pumpkin etc. These differences are due to slight changes in the instructions to the recipe eg putting white flour in instead of brown. The instructions are basically the same they are just different versions of it (in genectics these are called alleles; different versions of the same gene).

What scientists have done is got lots recipes (genomes) for many differents type of bread (people, including Ozzie Osbourne!) and worked out the order the instructions (genes) go in. They have created a map of how to make a bready human.

The instructions you have as a human are almost indentical to all other humans however the the combinition of which type of instructions you have is unquie to you (with a few exceptions).

So now we have this massive recipe of how to make a human that we can compare with indivdual humans and look for difference and similarities. 

",null,4,cdkdx9l,1r54d1,askscience,new,4
Sherm1,"I don't think that a true complete map could ever exist, because it would have to know how each sequence of code would react to every given environmental stimulus, including what its surrounding genes might be. How do we know that some of these stretches of ""junk dna"" don't become active when bombarded by some special radioactive waves that aliens can emit from their space ships? 

Point is, we need to not only know what the sequences are, but also what they do, and what they do is always determined by their interaction with the environment.",null,1,cdjoxib,1r54d1,askscience,new,3
the_sex_kitten,"Although each sequence is unique, there are still common gene codes that exist in each of us. By mapping the genome, they are able to locate these codes. For example, the gene for cystic fibrosis is located [here], and since we know that we are able to specifically look [here] for that gene. CF is way more complicated than that because there are a number of different genes that can be mutated, but that's just one example. Basically it allows us to determine the relative location of where potential mutations can occur. Apologies for the lack of sources and simplicity in my response. And please anyone feel free to correct me if I'm wrong!",null,1,cdjrcly,1r54d1,askscience,new,2
smfdeivis,"Only around 0.1% of the DNA between humans is different! So 99.9% genomic human DNA is the same. That 0.1% accounts for observable characteristics (phenotypes) like hair,eye, skin colors, and many others. Complete mapping of the human genome is basically mapping these conserved 99.9% of the DNA which codes for various essential peptides that make up proteins that give rise to tissues. There is a new project on the way called, ""the real human genome project"" Prof. Erick Lander gave a great summary of it on youtube!",null,0,cdjsquj,1r54d1,askscience,new,1
civilizedanimal,"This really depends on what definition you are using. Strictly speaking, mapping a genome is marking out where genes are located on the chromosome. Again, we are talking genes, or chunks of DNA that code for something. Most frequently, when people talk about mapping the human genome what they are actually referring to is sequencing the human genome. Sequencing the human genome is simply recording the sequence of nucleotides in a complete set of human DNA. They do this by sequencing more than one person's DNA and then averaging it. In order to map the genes, they would need to do a lot more research. When we finally get all the genes mapped, we will know what portions of human chromosome code for something. Even after mapping out all the genes it still takes a long time before you can determine what genes code for what.",null,0,cdjtir5,1r54d1,askscience,new,1
DLove82,"Mapping tells us the relative location of stretches of DNA that actually encode something (genes). This arrangement is very very similar between individuals (rarely, duplication, deletion, or transposition events can add, move, or delete a region of DNA, but that is uncommon), even if the genes themselves differ slightly on occasion. The genes are arranged in a group of 23 different unique chromosomes, or HUUUUGE stretches of DNA that are wrapped up really tight.

Mapping tell us the location of one gene relative to another in one dimension (along a line). (EDIT: 3-dimensional genome sequence is all the rage now - it actually looks in 3D at which stretches of DNA are in contact or close to which others - this is very important because those local interactions between genes REALLY far away have turned out to really impact gene function) Each of these genes is composed of a sequence of building blocks, or nucleotides, of which there are four - A, T, C, G (each is a slightly different molecule). The sequence of these nucleotides in a gene determine almost everything about its function - when it turns on and off, what it makes, what cells it's active in. Between individuals, the sequence of these genes is nearly identical, because the products of most genes (proteins) only function if they are composed of precisely the correct sequence of molecules (amino acids). Some, however, can work to varying degrees when the sequences are slightly different. If these occur in more than 1% of the population, they're called ""polymorphisms."" If they occur in less than 1% of the population, they're regarded as ""mutant"" forms of a ""wild-type"" (or normal) gene. 

So, in fact, mapping a bunch of individuals genomes actually allows researchers to come up with a heat map of the building block changes that occur in individuals. Genomic mapping is actually what tells us specifically what areas of the genome are unique between individuals. This can be immensely helpful in disease research where large regions of chromosomes are duplicated, lost, or moved. By mapping genomes, we can say which genes specifically are lost in a certain disease, narrowing down the number of genes which might cause the disease. For example, Down syndrome is caused by an entire extra copy of a chromosome (I think it's 21). That means these individuals have an extra copy of ALL the genes on that chromosome. And since we've mapped where all the genes in the genome are, we can identify which genes might be involved in Down syndrome (this is just an example, it's not really all that practical since the chromosome encodes THOUSANDS of genes).

tl;dr: The unique components of a person's genome are very few relative to the HUGE size and homogeneity (""sameness"") of the genome as a whole between individuals. For the most part, we all have the same number of chromosomes, each with the same number of genes in the same orientation. Complete mapping of the human genome allows us to build up a heat map of the few little areas where genes actually are unique, and see how common those changes are; if they're associated with disease, etc.",null,0,cdjtw4g,1r54d1,askscience,new,1
SMURGwastaken,"It means we've sequenced all of a person's DNA and worked out what each part codes for - whether it be amelase for digesting simple carbohydrates or amelogenins for producing tooth enamel, or the homeobox genes for deciding which organs and body sections go where. Since all humans are essentially identical in terms of how they work, all humans will have the genes for these things. Only about 0.1% of your genes are different to another human, and you'd be surprised at how little the difference between you and any other vertebrate (or even any other eukaryotic organism) is.",null,0,cdjuvtu,1r54d1,askscience,new,1
EvOllj,"There are differences on individual DNA that get completely ignored/lost when they are read, because the reading mechanism is very error tolerant. And there ate a LOT of differences that never get read.

And the differences in appearances are so small compared to the whole genome, that the genome of all humans is basically the same, all genes do the same thing, some are just more active and rarely a few barely important genes are disabled or damaged.",null,0,cdk5fgr,1r54d1,askscience,new,1
__Pers,"One of the hypotheses consistent with the data (at least back in the 1990s, when I last followed this field) is that gases like N2 and O2 dissociate during collapse, form different chemicals (e.g., peroxide), and thus are lost to the water during implosion. Noble gases don't undergo such chemical changes, meaning then when one has trace amounts of such gases there's a surfeit of argon or xenon left behind to compress and radiate. ",null,0,cdjxnxa,1r55qg,askscience,new,2
Davecasa,"Bone mass decreases, but the bones don't actually get much smaller, just less dense. The height increase is mostly due to lengthening of the spinal column due to not fighting against gravity... the same happens every night when you sleep, to a lesser extent.

Some more info:  
http://www.nasa.gov/mission_pages/station/research/news/spinal_ultrasound.html#.Uo4-hMR9te5  
http://science1.nasa.gov/science-news/science-at-nasa/2001/ast01oct_1/",null,0,cdjqu8k,1r57d7,askscience,new,10
dkreat,"There is a constant gravitational force on your entire body; this is something that your bones, ligaments, cartilage, muscles and joints all have to oppose and overcome to produce the common movements and actions you perform daily. This competition creates strain, and the cartilage and bones over time compress and shrink due to the act of holding your body up.

As /u/Davecasa said, your spinal column becomes decompressed in space and your bones decrease in density. This can compensate for a good portion of your height since the spinal column counts for nearly 25% of the average person's height. The cartilaginous discs that intercalate with our vertebrae can change shape and compress to allow for weight bearing to shift from the bones themselves to the cartilage in certain positions. 

In space, and when you aren't walking bipedally, the weight bearing role of your spinal column decreases, so the cartilage can return to its resting shape and density, and your height increases, even if by a little bit.

Source: I'm in medical school",null,0,cdk5ak7,1r57d7,askscience,new,1
Surf_Science,"It varies, even with Salmonella for example a few bacteria (10-20) can cause illness in the right circumstances where in other cases it will take millions. 1 Bacterium could cause illness but it is unlikely. ",null,2,cdjrf7z,1r57f2,askscience,new,10
wishfulthinkin,"As stated before in this thread, it varies depending on the strain and species of the bacteria.  It is theoretically possible for an individual to become ill from a single bacterium, but depending on the strength of the individual's immune system relative to the aggression of the bacterial strain.  In the vast majority of cases, a single bacterium would be destroyed by the immune system before it got a chance to reproduce and further infect the body.",null,0,cdjvsro,1r57f2,askscience,new,2
endocytosis,"As others mentioned, yes.  With [TB](http://iai.asm.org/content/73/10/6467.full) it is widely hypothesized that a single bacterium can successfully infect someone.  This does not necessarily mean that infection will happen every time, but the that it *can* happen.  The journal article is from a scientific group that studied *Mycobacterium bovis*, a related strain of bacteria that was also used to make the BCG [vaccine](http://en.wikipedia.org/wiki/BCG_vaccine).",null,0,cdjznhe,1r57f2,askscience,new,1
TheSkyPirate,"ELY5: 

1. Opiates make you feel good, so you want to keep taking them. 

2. Your body uses natural opiates to regulate feelings if discomfort. You are constantly experiences a ton of slightly painful stimuli, and your body uses natural opiates to block these and keep you feeling ""normal.""  Taking artificial opiates causes down-regulation of these natural opiates and the receptors that they bind to, so when you're off them your body is less able to compensate for pain. You end up feeling nauseous and achy because your body can't properly regulate discomfort.  

This is different than normal addiction. You don't just crave opiates, you're consciously aware the you need them to not be in pain, and you want to take them in order to feel happy. ",null,1,cdjz371,1r5a61,askscience,new,5
Javi2639,"Narcotics are opiates, which are chemicals that imitate natural endogenous painkillers like endorphins, enkephalins, and dynorphins. The body releases these in life or death situations, such as when you are running from a predator. If you break your ankle doingso, you will collapse in pain and die without these painkillers. The brain focuses on survival first and repair later in these situations. However, if there is a chemical that can bind to the same opioid receptors as these endogenous painkillers, the receptors will become overstimulated. The brain adapts to the constant stimulus, but upon stopping the opiates, the brain thinks it is in pain, so you take more. This is where the addiction potential comes in. ",null,3,cdjwd3x,1r5a61,askscience,new,4
Takagi,"Percocet is an opioid which increases dopamine levels in synapses (Online edition of Harrison's Principles of Internal Medicine, 18e), and dopamine is a chemical associated with rewards and ""feeling good"". That's the neuro part of why it makes us feel good. There's withdrawal that comes with people who use percocet which can develop between 6-8 weeks of chronic use (ibid).",null,1,cdjswxf,1r5a61,askscience,new,1
PorchPhysics,"I can answer this from a physics standpoint, but I might be uninformed on a particular topic:

No.

Nuclei are held together by the strong nuclear force, if it were not for this force, then all the protons would repel one another greatly due to the electric force between two charges.  Protons are all positive and would want to repel away.  

If the two nuclei you propose are close enough for the strong nuclear force to keep them from repelling significantly, they would be merged, as the drop-off in strength of the strong nuclear force is extremely rapid over all but the smallest of distances.",null,1,cdjrjz7,1r5bzl,askscience,new,5
Platypuskeeper,"Yes, [halo nuclei](http://en.wikipedia.org/wiki/Halo_nucleus).",null,3,cdjx255,1r5bzl,askscience,new,4
hotshot_sawyer,"Check this out: [antiprotonic helium](http://en.wikipedia.org/wiki/Antiprotonic_helium). It's an electron and an antiproton orbiting a helium nucleus. They make it in 3% yield by simply shooting antiprotons into helium. It lasts for microseconds.

The antiproton is bound in an orbital with n around 38, which is different from normal atoms where you fill up orbitals from n=1 and up with very few exceptions. This seems interesting but I'm pretty helpless with quantum stuff and it would be great if someone else could unpack this aspect a little.

There's also [positronium](http://en.wikipedia.org/wiki/Positronium), which is a bound electron-positron pair that behaves a lot like hydrogen but doesn't last very long.",null,0,cdlzl7r,1r5bzl,askscience,new,1
owaisofspades,"yup, eventually the receptors for it on your tongue will get down regulated with continued exposure. 

Also, it's slightly toxic, so it might even lead to tolerance by damaging the cells that contain the receptors themselves, but I'm not entirely sure ",null,1,cdjx7uz,1r5c17,askscience,new,2
FatSquirrels,"It definitely falls into the definition of a molecule, but we tend not to think of it that way when talking about polymers.

When working with [macromolecules](http://en.wikipedia.org/wiki/Macromolecule) we will instead break things up into repeat units or subgroups.  For polymers we look at the repeat units, for copolymers we look at the individual pieces/blocks and for proteins we look at the amino acid sequence.  When looking at something small, a single picture of the molecule is all we need to define, but for things where one molecule is sometimes arbitrarily large we determine the properties of the substance by analyzing smaller pieces and we don't even think about the term ""molecule.""

However, the fact that a piece of vulcanized rubber might be one big molecule is very important to its material properties.  You could theoretically melt it or dissolve the whole thing but it would remain completely connected to itself, just one big connected molecule that has very strange properties when compared to ""normal"" sized molecules.",null,0,cdjrxb3,1r5ckr,askscience,new,6
Brewe,"It is one molecule, but it's usually not treated as such. It's like with crystal structures that are also one big molecule (at least as long as we're talking about a perfect crystal), here we look at a unit cell, the smallest part of the crystal that explains the whole structure. The same is done with polymers, where f.x. a polymer could be shown as [-CH2-CH2-]n, where n is a large number.

Polymers, crossslinked polymers, crystals etc. are technically one very large molecule, but they are not treated as such.",null,2,cdjr831,1r5ckr,askscience,new,4
then_and_again,"It does refract light, the refraction index of the vitrous humor is 1.336, water's around 1.333 if i remember correctly. We don't have fish eye vision because your brain is the organ that 'sees' not your eyes. What you see is just your brain's interpretation of certain light signals, so yes the light is refracted, and that refraction causes your brain to see it as 'normal'. if you look through an lens that corrects this refraction, your brain will interpret it as a different image and your vision will be distorted. Basically, your brain is used to light being refracted, so that's the basal image",null,0,cdjvfca,1r5d2l,askscience,new,3
wishfulthinkin,"The vitreous humor does refract light, but the brain is able to counter this by processing the image before you ""see"" it.  Likewise, the image of the world that reaches your brain via the optic nerve is upside down, and the brain flips it right side up.  Similarly, your brain hides the small blind spot that is created by the complete lack of rods and cones on your optic nerve.  When you see anything, it's actually your brain's interpretation of the real image of the world.",null,0,cdjvw0i,1r5d2l,askscience,new,2
TrainerGary,"The concept is called [proprioception](http://en.wikipedia.org/wiki/Proprioception). 

I'm sure someone will give a more in depth answer, but essentially there are multiple body maps in your brain, and these maps are used in conjunction with your peripheral nervous system in order to determine body position. 

For example, proprioceptors are found in skeletal muscles. The relative stretching/compression of these proprioreceptors gives information about the position of the limb.",null,3,cdju34l,1r5l7s,askscience,new,26
Lillelyse,"As already mentioned this is called proprioception. To expand a bit on how sensors in the muscles tell you about limb position: The body's sense of its position in space is determined by stretch receptors in the muscles, joints and tendons. Muscle stretch receptors, or muscle spindles, are specialised muscle fibres in a fibrous capsule. These detect changes in muscle length.

Joint receptors are mechanosensitive axons in the connective tissue of joints. They are good at detecting the movement of the joint, but not so useful when the joint is kept still. 

Finally, at the junction of a muscle and a tendon we find Golgi tendon organs. These register muscle tension, or force of contraction. The information from all of these sensors come together to tell us where our limbs are. ",null,1,cdjxdl4,1r5l7s,askscience,new,9
SpacemanSpiff1222,"Without getting too in depth, your body will use proprioceptive sense organs to obtain this information. In the muscle itself, your muscle spindles will pick up how fast and how intense a contraction. Muscle spindles exist all throughout the muscles and in different numbers in different muscles. For example, the sub occipital muscles have a much higher amount of muscle spindles than, say, the biceps femoris. Inside the tendonous attachments to the bones you have Golgi Tendon Organs which will pick up information regarding tension. The information will then travel to the spinal cord through peripheral sensory nerves, in to the spinal cord in the medial lemniscal system. From here it will travel up through the medulla and decussate (cross) at the nuclei gracilis and cuneatus as internal arcuate fibers. The tracts will then head on up to the thalamus (VPM and VPL) and out to the post central gyrus in the cortex of the cerebrum. Skipping a lot of stuff, but that is the basics of it. 

ninja edit: This is purely proprioceptive. Your body will also use the vestibular system to track the position of the head. Also, as simple as it sounds, your body will keep track of your segments using your vision as well. ",null,0,cdk15wc,1r5l7s,askscience,new,2
lengendscrary,"Like it was mentioned before visual stimuli seems to be important in creating your body map. If you're interested in getting a better understanding of proprioception, you can read a short story called the Disembodied Lady by neurologist Oliver Sacks. It is sort of a case study of what happens when you lose your sense of proprioception. It's pretty cool because the lady would not know what her limbs were doing and where they were unless she was looking directly at them.",null,0,cdkj53i,1r5l7s,askscience,new,1
ModernTarantula,The [cerebellum](http://neuroscience.uth.tmc.edu/s3/chapter05.html) may be responsible for most of the body map. It is not part of consciousness. Such that you don't have to think exactly about the movement necessary to touch your knee. I heard tell that if you hold a stick it will become part of the cerebellar map.,null,2,cdkc572,1r5l7s,askscience,new,1
darksingularity1,"I'm assuming you mean the relative positions of your limbs and not just the relative position of your entire body(well just your head really) in space. The latter deals with the vestibular system near your ears. The way I like to think of it is that it covers the three planes of space (x,y,z) with its three orthogonal ""pipes"". 

Now to the issue of knowing where your hand is at any given moment. There are two ways that we can generally figure it out. Or at least there are two theories. So theoretically, in order to find the arms position, the brain can just ""ask"" it or if the arm just moved somewhere it can calculate the new position from the old position and the force/energy/etc that was used to get it there. 

First of all, the general sense of position is called proprioception. We have different sensors on our body to figure out our positions. Most importantly, we have things called muscle spindles within out muscles. These give the brain an idea of how contracted/relaxed the muscle is.
Also, when messages are sent to muscles to do something, they sometimes send something back, called an efference copy, which kinda gives the brain more info on position and stuff.
The brain also uses the traditional senses for this as well. Vision plays a huge part in this. Seeing where your arm is gives you a great idea of where it is (amazing right?!). ",null,8,cdjxkkv,1r5l7s,askscience,new,3
wishfulthinkin,"Improvements in dental care have significantly extended the lifespan of humans.  The wealthy in pre-modern-medicine times very frequently died from cavities, as only the wealthy were able to afford sugar.  In fact even nowadays, there are instances where people get cavities in a molar that eventually extends back through the mandible and into the brain, causing infection and later, death.",null,4,cdjw217,1r5lfe,askscience,new,10
Javi2639,"Differing intermolecular forces. Milk is mainly water, which is polar and can form strong hydrogen bonds with other water molecules. Chocolate is mostly fat, which only have van der Waals interactions. This will cause the two to interact with each other and exclude the other, separating them. ",null,0,cdjvy8k,1r5lx8,askscience,new,5
Overunderrated,"Good question!

Intuitively, if you have a rigid container which contains a tube at equilibrium, if you then create a vacuum within that tube, you'll be increasing the pressure of the rest of the container.

But earth's atmosphere isn't a rigid box; it exponentially decays towards zero density far from the surface. The hydrostatic pressure you feel at the surface of the earth is due to gravity acting on all the air above you. Now, gravitational attraction decreases with height... and if you were to create a vacuum chamber at the surface, this would have the effect of pushing air up, towards higher altitude, where gravity is weaker. So I could see that creating a vacuum chamber at the surface of the earth might actually *lower* the ambient atmospheric pressure. Analogous to the notion that if Earth's radius doubled, but the total mass of air stayed the same, the ambient pressure would be much lower.


I'm just spitballing here, so corrections/refutations/alternatives welcome.",null,2,cdjv1la,1r5n2t,askscience,new,13
Javi2639,"Soap is the salt of a carboxylic acid with a long hydrocarbon tail. The ion head will interact strongly with polar water, while the hydrocarbon tails will interact strongly with each other. This forms a bilayer with the tails pointing inward towards each other and the heads pointed outwards towards the water, forming a sphere. It is the same principle that creates the cell membrane of all living creatures. ",null,1,cdjw2rs,1r5o2u,askscience,new,3
slartibartfastfive,"Presence of surfactants; water is a racist.

Water is very cohesive because it's small and can form networks of hydrogen bonds. This property leads to surface tension, which is the force that opposes the formation of bubbles--the water doesn't want to be spread out in a thin surface touching the air, it wants to be huddled up in a ball with as many water molecules as possible touching each other.

Surfactants, like soaps or other amphiphilic molecules, have a structure where one part of the molecule is quite polar (water-loving, hydrophilic) and one part is less polar (""oil""-like, hydrophobic, hates water). The surfactant molecules arrange themselves at the air-water interface with the nonpolar parts touching the air. The nonpolar parts of the surfactant are much happier in contact with air than water would be, and the polar parts are pretty happy touching water, so this arrangement lowers the interfacial tension at the bubble surface, allowing it to grow beyond a certain size.

Bubbles will form in soapy water or milk because of this. Mayonnaise and salad dressing are ""emulsions"" of oil and water stabilized by similar surfactants.",null,0,cdl2lc5,1r5o2u,askscience,new,1
wishfulthinkin,"We have adapted to live in this pressure.  Reduced pressure is not equivalent to increased health, as evidenced by the death of many deep sea fish and crustaceans when they are brought too close to the surface.  Their bodies have evolved for life in the high pressure of the sea floor, and upon losing the pressure, their bodies lose structural integrity and they simultaneously increase in size and decrease in stiffness.  The blobfish picture everyone has seen is a good example.  In its natural habitat, the blobfish looks similar to most other fish, but when it is brought up to the surface, a low pressure area for it, its body collapses and it looks like a gelatinous blob.",null,6,cdjw6hn,1r5qol,askscience,new,16
goldistastey,"The atmospheric pressure is balanced by our internal pressure which is just due to us being a bunch of compressed stuff, and our cells and tissues have evolved this pressure to match the atmospheric pressure. That's why in space, with no atmospheric pressure, our internal pressure would kill us.",null,2,cdjz0eb,1r5qol,askscience,new,6
adamhstevens,"Mainly because we are mostly composed of incompressible materials (i.e. water). In fact, the airspaces in the body (i.e. lungs, stomach, sinuses, ears) are generally in equilibrium with the atmosphere, so air moves freely in and out, preventing a pressure difference.

Problems actually occur in certain situations (e.g. diving) where these airspaces are blocked from equilibrating and the pressure difference can cause tissues to rupture, but most of your body is unaffected.",null,1,cdkgsba,1r5qol,askscience,new,3
PepperJack_delicacy,"The other poster is completely right but since you're teaching a science class I thought maybe your students would appreciate a slightly more detailed answer.


Alcohol is metabolized predominantly in the liver and there are **3** important enzymes that you should be aware of. First, alcohol is converted to acetaldehyde by **alcohol dehydrogenase** or **cytochrome P-450**. In turn, acetaldehyde is converted to acetate by **acetaldehyde dehydrogenase**.

Both alcohol dehydrogenase and acetaldehyde dehydrogenase create molecules of **NADH** during these reactions and NADH will inhibit gluconeogenesis and fatty acid oxidation. The important thing to take away from this is that **this leads to a very fatty liver when you chronically abuse alcohol**.

Also, an alcoholic will have an increased amount of cytochrome P-450 (this is because the body tries to adapt to the increased intake of alcohol). Cytochrome P-450 handles many other reactions and will cause the body to create an excess amount of **reactive oxygen species**. Having a lot of reactive oxygen species around creates unnecessary inflammation, cell death, and fibrosis. 

*So overall, an alcoholic will have a fatty liver and an excess of reactive oxygen species. Together, this creates a lot of damage in the liver tissue, which over a long period of time will lead to cirrhosis.* 

I hope this answers your question. Let me know if you want me to clarify anything. 


Source: http://www.clevelandclinicmeded.com/medicalpubs/diseasemanagement/hepatology/alcoholic-liver-disease/",null,2,cdjxr3r,1r5rxu,askscience,new,14
then_and_again,"the body processes ethanol by oxidizing it to acetylaldehyde, and then to acetyl-CoA. a lot of the enzymes needed are in the liver, and the liver takes the brunt of alcohol metabolism. Why some people get cirrhosis is still unclear, we know that the liver undergoes oxidative stress, which causes apoptosis and eventually fibrosis. What we don't know is why this only happens to some people, most people get hepatitis instead.  ",null,2,cdjv9o2,1r5rxu,askscience,new,7
HexagonalClosePacked,"Not an aerospace guy, but it likely has to do with fault/failure tolerance.  Each propeller on a helocopter represents a single point of failure, meaning that it is a single part or component that causes the entire vehicle to become inoperative if it fails.  A helicopter cannot fly properly/safely if either one of its propellors is disabled.  Contrast this with a plane, which due to the lift generated by air moving over the wings, can keep flying if it loses one or more engines (even if it loses all of them it can glide for a while).

Say you have a helicopter with two props.  That means there are two points on the vehicle where a malfunction will cause a major accident/crash.  If we increase the number of propellors to 6, as is common in some RC helicopters, we have now tripled the number of points of failure, since if any one of these 6 props are disabled the aircraft will become unstable.  Someone might be able to correct me, but I'm assuming it would be much more difficult to design a 6 prop helicopter that could keep in the air after losing one propeller than it is to design a 6 engine plane that can do the same.",null,5,cdjxbul,1r5tcm,askscience,new,12
jvs_nz,"There are a few reasons why multicopters have not been brought to life size yet. I can see it being feasible one day, but improbable.

* Most multicopters you see (4,6 and 8 rotors) control their pitch, altitude, and yaw by varying the speed of the blades. They can do this because the inertia of the blades is quite small, so the motors don't have a big job of changing the speed. This wouldn't be the case for full size blades needed to lift a multi-ton craft. 
This can however be offset by using variable pitch blades, much the same as on a normal helicopter - this obviously increases complexity however and also the number of components that can fail.
* 4 rotor crafts will crash if a prop fails. All 4 are fundemental to having controlled lift. 6 and 8 rotor crafts can survive quite easily if 1 prop or motor fails (provided the failure doesn't take any of the other motors out, like a blade flying through neighbouring motors). 
* The electronics and systems needed to keep multicopters afloat are still very new - only in the past 5-8 years has the technology matured enough to make such a thing feasible. 
* Cost. A single engine, single rotor heli is expensive enough. Now times that by 6 - and for what? There are very few scenarios where a traditional helicopter can't do a job a multi could. Now times the running costs by 6 as well. (I say 6 because you wouldn't build a 4 rotor due to above mentioned safety/redundancy reasons).

The reason multi rotors have taken off in the hobby and small scale world is because traditional single rotor model helicopters have been very difficult to control and learn to use - as well as horribly dangerous. Because multi rotors are controlled by software - it's easy enough to make that software user friendly, so it doesn't take much to pick up and learn. They are also cheaper in a lot of the cases - you can build a multi rotor for a couple of hundred $. 
They can also be quieter. 

I wouldn't rule out full size multi's in the future, I just don't think it'll be very common for a long time, if not ever.",null,2,cdk2y6y,1r5tcm,askscience,new,9
Farnswirth,"A lot of people here are arguing that more rotors = more complexity.  While this is true, it does not mean that more rotors = worse design.  Take the B-52 for example, it has 8 jet engines.  Or the V-22, it has two engines built on rotating pivots.  Complexity is not always bad, sometimes you are merely trading simplicity for functionality.  In the case of the B-52 you are gaining redundancy and power.  In the case of the V-22 you are gaining speed and VTOL capability.  

There is no reason you couldn't build a full sized multi-copter.  I am sure there are companies working on one right now.  The problem with multicopters is not necessarily the complexity of the *design* but the complexity of the software and control systems needed to build a full scale one.  For example, if you loose one engine on a B-52 and the flight computer fails, a pilot can still fly the plane manually with no problem.  If the flight computer fails on a multicopter - you are screwed, especially if a rotor fails as well.  A pilot simply can't react fast enough.  We've seen examples of unstable aircraft with extensive flight control systems: the F-16, the [F-22](http://www.youtube.com/watch?v=faB5bIdksi8), [V-22](http://www.youtube.com/watch?v=n3lbKqStvHI), [B-2](http://www.youtube.com/watch?v=_ZCp5h1gK2Q), F-117, etc.  Consequently, most of these planes have had horrific crashes where the flight computer fails and the pilot can't react fast enough - that's just the nature of the plane.  An unstable helicopter is just an added level of danger and complexity, it's just asking for trouble.  That's not to say it's not possible, it's just very challenging.  The technology is new.  It's maturing.  

[People have done it](http://www.youtube.com/watch?v=L75ESD9PBOw).  [The military has played around with it.](http://en.wikipedia.org/wiki/Curtiss-Wright_VZ-7) Commercial and modern military versions are likely on their way.  It's just a matter of time.  ",null,0,cdkggl3,1r5tcm,askscience,new,3
SitnaltaPhix,"Because single-rotor RC helicopters are **extremely difficult** to learn how to fly. Not only do aerodynamics work differently on small aircraft (and therefor make them very unstable), but you are not operating in the same reference frame as a regular pilot would be. Even experienced RC pilots crash all the time. That's rather disconcerting to someone who just wants to take aerial shots with their camera.

The advent of Lithium polymer batteries, more efficient motor controllers, and better computing power have allowed multi-rotor RC helicopters to be possible. Allowing not particularly experienced pilots to fly.

Larger aircraft are just inherently more stable. Their rotors have more air molecules to push against and more inertia to keep everything in line. They don't really need extra stability, making the engineering feat of designing gas-powered engines driving multiple propellers not economically viable.",null,2,cdkakpm,1r5tcm,askscience,new,2
good_n_plenty,"So... not an expert of this but my understanding is that one of the advantages of having two rotors rather than one is that the rotors spin in opposite directions, so their torques cancel each other out. With only one main rotor, you need a tail rotor to control the angle of the helicopter and keep it from spinning in the opposite direction as the rotors are moving. That's why helicopters with two main rotors don't have a tail rotor. Having 3 (or 5, or 7) main rotors would defeat the purpose of having multiple rotors. I don't know about 4. You should ask r/helicopters",null,3,cdk1dic,1r5tcm,askscience,new,2
baloo_the_bear,"A person has 2 types of sweat glands, apocrine and eccrine. Eccrine glands are present all over your body and act to aid cooling. Apocrine sweat glands are present only certain areas like in the axilla (armpits) and groin. They develop during puberty and though they also contribute to cooling the body, they also excrete a small about of cytoplasm when they do so. This leads to the body smell. ",null,1,cdjxdqo,1r5y1m,askscience,new,22
lemons47,"Body odor generally starts becoming noticeable on a person when they reach puberty. Sweat glands in the groin and armpits become surrounded by dense, thick hairs around this time which provides a better environment for the growth of bacteria. Part of the smell is a combination of the bacteria producing odious chemicals from their own biological processes as well as the breakdown of molecules in sweat into more volatile chemical constituents. Because of this, your diet can sometimes affect your body odor because the chemical composition of secreted sweat can change which changes the volatile breakdown products produced by your skin microbiota.",null,0,cdkzlkd,1r5y1m,askscience,new,1
antpuncher,"In the first generation of stars, the only elements are hydrogen and helium, and they suck at cooling.  So when stuff collapses, it stays very hot.  That means there's a lot of available pressure, so for a blob of hydrogen to collapse by its own gravity, it has to be huge.  The first estimates are hundreds or thousands of solar masses, more recent estimates permit stars as small as 10-20 solar masses.  That's still _way_ bigger than a gas giant, which 10^-3 solar masses.

(The alternate scenario is that the rocky core forms first, but that won't work because rocks are silicon, carbon, and iron, and you need to make stars first, before you can make S,C, and FE.)",null,0,cdk0nqx,1r68w8,askscience,new,5
redit_,"It produces lightning in basically the same way that thunderstorms do. It's a release of the accumulated static charge. Volcanic ""ash"" is actually a large cloud of rock dust. These particles bump against each other until they build up enough charge to release a spark. It's the same as when you rub your feet on the carpet except for a much larger spark.

And snowstorms can produce lightning, it's just very rare. They actually have a name for it; thundersnow. It's really, really cool to experience. It almost sounds like an avalanche because the reverberations are muffled by the snow. You don't hear a sharp crack and the echos. It's more of a deep rumble from out of nowhere.",null,1,cdk5nyf,1r6ag9,askscience,new,7
Sannish,"If there is lightning there will be thunder, but we might not hear it over the sound of an erupting volcano.

In typical thunderstorms it is the collision of ice and water that creates the charge separation that results in lightning.  We do know that there is charge separation in some volcanic plumes that produces lightning but it is currently unknown what is causing the charge separation.

It could be ash particles, it could be water in the plume, or it could be something else entirely.  Part of the problem is that it is very hard to take measurements inside of a volcanic ash plume.",null,0,cdkiq6w,1r6ag9,askscience,new,2
Baloroth,"t'=t/sqrt(1-(v^2 /c^2 )). So for a v of 370,000 m/s, that gives a rate of 1.000000760556423223785 seconds in our frame for every second in the ""stationary"" frame. So, time in our frame is passing at 99.9999239% the rate of the CMB stationary frame.

Note: that is with respect to an observer in the CMB frame. In our frame, it's the CMB frame that is moving through time slower (hence, relativity), by the exact same amount.",null,1,cdk7px4,1r6e8s,askscience,new,5
adamsolomon,"You phrased your question in two subtly different ways. The correct way, at the end of your post, ""how slowly are we experiencing time, relative to an observer who is stationary to [the] CMB?"". That's a valid question and has been answered already.

But in the title of your post you asked how slowly we're experiencing time relative to the CMB radiation itself. And that's actually *not* a valid question, since radiation doesn't experience time!",null,0,cdkdxq2,1r6e8s,askscience,new,2
Platypuskeeper,"Without experiment, a guess from homology is the best you can do. ",null,1,cdk7kj3,1r6g8q,askscience,new,4
Siny_AML,I think the next question to ask would be the possible nature of the enzyme. You would have to test the Kd values of the reaction with and without the protein of interest to see whether the rate of the reaction either stays constant or changes. I may be wrong but I don't think that this is something that you would be able to test without using some kind of in-vitro system. Relying on computational methods would only give you a theoretical system with no biological basis.,null,0,cdk7ltk,1r6g8q,askscience,new,2
edge000,"I think this question is usually approached from the opposite direction. Biologists tend to be interested in processes, so I would think people wouldn't typically investigate a random protein unless there was some interesting reason to be looking at it. A scientists is more likely to be investigating some phenomenon and investigate what causes it. 

Genetic knockout experiments are carried out to elucidate a metabolic or signalling pathway. Basically, someone will identify that a suite of genes controls some pathway and will start [silencing genes](https://en.wikipedia.org/wiki/Gene_silencing) for enzymes along the pathway and see what substrate builds up in the pathway. 


After doing these types of experiments enough times we can build the database that people can search like you mentioned. ",null,0,cdkipub,1r6g8q,askscience,new,2
zopamine,"Forget everything you've ever heard about right brain/left brain. For the most part, it's a myth.

However, functions can be localized to certain hemispheres of the brain. For example, when we see human faces or pictures of faces, that process has been said to take place mostly in the right occipitotemporal (fusiform) gyrus, which is above your right ear and a little further back. 

Basically, there are cognitive functions that are localized to each hemisphere of the brain, but the whole being more left-brained (colloquially logical) or right-brained (colloquially creative) just isn't true. We use both hemispheres equally. 

Source: Nielsen, Zielinski, Ferguson, Lainhart, &amp; Anderson (2013) 

It's on PLOS One, so it's widely accessible if you're interested. I'd link it but I'm on my phone. Hope that was helpful!",null,1,cdk98lr,1r6hfl,askscience,new,4
3asternJam,Most famous example of left/right differences in brains is speech - Wernicke's area (speech/writing comprehension) and Broca's area (speech production) are only found in the left hemisphere (generally speaking; there are exceptions). The corresponding areas in the other hemispheres are more general sensory/premotor areas respectively.,null,1,cdkg8e2,1r6hfl,askscience,new,2
Hiddencamper,"Nuclear engineer here.

The RBMK is graphite moderated. This means that the cooling medium and the moderation medium are separated. 

Under normal operation, water enters the bottom of the reactor, and flows upward. The water heats up on its way through the core, and the amount of steam voids by volume increases on your way up through the core. 

Steam is drastically less dense than liquid water, and is virtually transparent to neutrons when compared to liquid water. As such, an increase in voids means that my water is absorbing less neutrons. It also means that my neutrons will have a longer mean free path length, and ultimately means more neutrons will be able to get to my moderator. tl;dr, Increase in steam voids = increase in moderation = increase in power.

Inherently this has stability issues. As I increase my heat output, I'm going to increase the amount of voiding I have. This will then increase power, which further increases voiding. Active control systems which adjust control rods can come down to compensate for this. The control rods for this reactor design drop in from the top, which makes sense as the top is where the highest neutron flux is likely to be seen. Active control rod motion suppresses any power excursions and maintains the reactor in a stable operating state.

At high power levels, you are producing a lot of steam flow, and as a result, you have a high flow of water through the reactor. With high flow rates through a reactor, your boiling boundary remains fairly constant, and it takes quite a bit to have a runaway excursion, as the forced flow of water into your reactor tends to push voids out quickly and ensure cooling water gets to where it needs to be. Additionally at high flow high power conditions, the voids do not have a dominiant contribution to reactivity, meaning small changes in voids have small changes in reactivity.

When you are at low power, you are in a situation where you have low flow. Your boiling boundary is higher. Your voids have a much larger contribution to reactivity in the reactor. At low flow conditions with low control rod density conditions in boiling reactors, we observe the boiling boundary is somewhat unstable. Steam voids take longer to get out of the reactor, and they begin to have a stronger impact on reactivity. As such, anything that changes your boiling boundary even a little bit is going to have an amplified effect on your neutorn flux and power output. Little things like random noise in your pump flow can start these oscillations, causing the boiling boundary to move, which starts causing power oscillations. In the RBMK, to respond to the power oscillations, the control rods will start moving in and out to try and stabilize this. Having graphite tipped rods combined with having a control system with a response time constant in seconds (which is similar to the fuel's thermal time constant) just means that the control rods are going to be trying to catch a power change, but will have trouble keping up because it will be causing some of its own problems. 

All of these things together will drive power oscillations in the core. Under a worst case condition it can drive a runaway condition requiring a reactor scram. When operated appropriately, the reactor scram (even with the graphite tips) will have sufficient margin to preclude a steam explosion. When not operated appropriately (with nearly all control rods out), a power excursion can occur which leads to a steam explosion and loss of the unit.

The reason this is an issue at low power is due to the way the boiling boundary behaves at low flow conditions with low control rod density. The voids have too much contribution to the core's reactivity under these conditions, and small changes in voids drive large changes in flux. Because the voids contribute so much to reactivity under these conditions and the possibility of an instability can occur, RBMKs have a safety limit which requires a minimum number of control rods to be inserted at all times, to ensure that the voids do not carry enough reactivity to drive a core damaging power excursion. At Chernobyl they removed these rods past the safety limit because they were in the xenon pit, and trying to reach a specific power level on their reactor.

I'm more familiar with BWRs for instabilities. Standard BWRs like those in the US have stability issues as well at low flow low control rod density conditions, however the because the moderator and coolant are the same, they tend to be self limiting and are not capable of undergoing a power excursion/steam explosion. Most BWRs also have a system (called OPRM) that detects core stability issues and initiates an automatic reactor scram. What we see in BWRs is an oscillation with a time constant that is usually 1-2 seconds. We will start seeing small oscillations, then the oscillations will start growing. Oscillations in a BWR grow slowly. If the plant has an OPRM, it will scram the reactor prior to it increasing past certain limits. If the plant does not have an OPRM, it will grow over several minutes until the reactor hits either the high or low flux scram setpoints. The main danger in BWR type reactors is that you can cause localized thermal stress and plastic strain on the fuel (localized fuel cladding damage), but no power excursion.

For BWR light water reactors that have it equipped (all US BWRs have this) the OPRM (Oscillation Power Range Monitor) looks for counts (how many oscillations am I getting in a row), period (are the oscillations in the right time constant that is indicative of thermalhydraulic instabililty or is it just random noise), and growth (is my oscillation diverging with a &gt;1.0 decay ratio). There is also a confirmation density algorithm which uses a factor that looks at the above factors across the core to anticipate these factors before they start. If the OPRM gets enough counts, on the right period, with growth, it will initiate a scram immediately. US BWRs are forbidden from entering the region where core thermalhydraulic oscillations exist, and are required to insert an immediate reactor scram if they enter the region.

Generally, the only time a BWR enters the instability region is if they have an inadvertent loss of their reactor cooling pumps. 

Another poster mentions xenon. Xenon can cause long term issues which force you into a low rod density low flow situation, however xenon will not cause the instabilities that created the chernobyl event. Xenon does not respond fast enough (hours), while the boiling boundary response is in seconds. He's not entirely wrong, xenon transients can force you into an instable core operating region, but they do not cause the instability. Proper reactivity management can ensure that you pass around the instability region without going into it. RBMK reactors have a very tough time with the xenon pit and are more likely to put themselves into an instability, while other BWR type reactors can deal with it just fine.

I hope this helps. If you have any other questions please let me know.",null,0,cdks0mi,1r6hig,askscience,new,3
grillkohle,"Are you referring to the Chernobyl incident? I will try to explain the problems without looking at that incident.  
Controlling nuclear reactors is all about controlling (slow) neutrons. The slow neutrons basically are the chain reaction.  
A big problem with reactors in low power mode is Xenon 135. It is known as a neutron poison, because it has a high probability for absorbing neutrons (and thereby reacting to the stable Xenon 136). It is formed by decay of Iodine with a half life of 6.6 hours. When the reactor is running in normal operation mode, there is a balance between the ""production"" and the ""reduction"" of Xenon 135.  
If you lower the power output, there is more production of Xenon 135 because of the delay (decay from Iodine). Hence, it is more difficult to control the reactor, you can't increase the reactivity, because the Xenon will absorb your neutrons. Usually you have to shut it down and wait until most of the Xenon is decayed itself (half life 10 hours).  
But if you try to increase the reactivity (by pulling out control rods for example), you increase the reduction of Xenon (obviously the production stays at a lower level for some time because it takes some time for the ""production"" of the Iodine and its decay), which further increases your reactivity because you have more slow neutrons because less neutrons are absorbed by the Xenon. This happens to all nuclear reactors.   
The next problem is the positive void coefficient, which basically means: more thermal activity (more reactivity) =&gt; less cooling water (because more steam (much lower density!), more bubbles) =&gt; more thermal activity because of the void coefficient [because of the graphite still moderating at higher temperatures].  
These 2 effects support each other and make controlling the reactor in a low power state quite difficult.  
In a water moderated reactor (negative Void coefficient) for example the chain reaction stops or slows down if the water vaporizes too quickly, because there is less moderator to slow neutrons down. It is sort of ""self stabilizing"".  
I don't know how much you know about reactors, but I hope I could explain it to you. I know that stuff because I took a course at the university about nuclear reactors.
There are some other factors regarding the Chernobyl incident which lead to the explosion.
",null,0,cdkehwb,1r6hig,askscience,new,2
rupert1920,Check out the Wikipedia article on [Roche limit](http://en.wikipedia.org/wiki/Roche_limit).,null,1,cdk78nz,1r6kn2,askscience,new,4
Drunk-Scientist,"Interesting question. The above answer on Roche Limits is correct; within a certain radius (governed by how strongly held together each planet is), the gravity from each would rip them both apart. This is why [Saturn has rings](http://www.astro.washington.edu/users/smith/Astro150/Tutorials/Roche/) and why [Comet ISON might not make it through](http://www.asterism.org/tutorials/tut25-1.htm) it's close interaction with the Sun.

Much of the material from this interaction would collide and form a [disc of material](http://news.bbcimg.co.uk/media/images/63566000/jpg/_63566727_r3400571-artwork_showing_the_moon_s_formation-spl.jpg) around the point where the planets came nearest. Some material would also likely be thrown off into the solar system. This disc would then re-accrete back into a (much larger) planet. In fact that outcome is similar to [what happened to Earth 4.5 billion years ago](http://en.wikipedia.org/wiki/Giant_impact_hypothesis) when a Mars-sized body collided with us and the resulting debris disc coallesced into the Moon.

HOWEVER, if you ignore the 'planets' and 'dense atmospheres' part of your question, then such an occurrence can and does happen in the universe. [Contact Binary Stars](http://en.wikipedia.org/wiki/Contact_binary) are stars that orbit so tightly that their atmospheres are connected. Mass and energy are transferred between the stars and eventually the smaller companion may even become swallowed entirely by the larger, but they may last happily together for hundreds of millions of years.

Going back to the question in hand though: What would actually happen if these planets could touch atmospheres without being ripped to shreds (eg, assume they are made of adamantium). Well, as the gases mix some of the molecules would become gravitationally bound by the planet above and cross the gap to the other, effectively. It would also cause huge atmospheric disruption on both planets, possibly with supersonic winds created that might blow around the planet for days.

However, such a scenario would be unlikely to last for more than a couple of orbits: the atmospheric drag on both would reel in their aphelion until, rather than just kissing atmospheres, they were slamming into each other head-first. But I guess if these really are indestructable planets they would just form a strange bow-tie shaped world; a bit like a [Contact Binary Asteroid](http://en.wikipedia.org/wiki/Contact_binary_(asteroid).",null,0,cdn0yzy,1r6kn2,askscience,new,1
rupert1920,"The existence of a permanent dipole moment is independent of whether you put it in the microwave or not.

Some functional groups in olive will have dipole moments along their bonds - for example, the C=O double bond - but there are also large extents of the molecule that doesn't have a strong dipole moment - such as the hydrocarbon chain. The end result is that the molecule as a whole has a small dipole moment.

This, combined with the size and weight of the molecule (i.e., high moment of inertia), makes it much less responsive to microwaves than, say, a water molecule.",null,0,cdk6p7b,1r6liv,askscience,new,5
walluwe,"No. The oil is made up of several fatty (hydrocarbon) chains connected to carboxylic acids. While the carboxylic acids themselves have a dipole moment, the molecule on a whole is not really affected by that, just due to the very long hydrocarbon chains (the carboxylic acids are so much smaller than the fatty side chains). The water molecules themselves will vibrate, but the fatty acid will not be affected by this radiation. ",null,3,cdk5m1p,1r6liv,askscience,new,2
codyish,"I'd be interested to see a source confirming your idea. My understanding is that capsaicin is fat soluble, so milk washes it away more effectively than most drinks, and that olive oil would do an even better job. ",null,0,cdkai3o,1r6lk9,askscience,new,2
GProteins,"Okay-- so I'm going to assume that you're talking about things like cuts/abrasions/etc. 

The first thing that happens is your blood vessels release a variety of cells, some to eat bacteria, some to help close the hole by a) plugging it and b) secreting chemicals that help plug it in a more stable manner (that thing we call a 'scab'). 

Then, your body rushes to fill the gap with really fibrous tissue-- mostly collagen and some elastic fibers. This is your initial scar. It's red and leaky and not terribly strong, but it's better than nothing. Your body then goes through and slowly replaces that fast-placed fibrous tissue with the normal cells that belong in the area. Eventually, the fibrous tissue gets replaced (sometimes it doesn't entirely go away), but the tissue that's there is a LITTLE smaller than the tissue that was originally in the wound area, which is why scars can feel ""tight"". 

Basically, when you're talking more severe wounds, you're just seeing more fibrous tissue, meaning a longer time for the body to replace it with normal cells. And sometimes when you have such fast cell proliferation (even though it's slow to you, it's fast for your body), the cells ""forget"" things and you get little mutations here or there-- darker or lighter skin sometimes. And you can lose architecture that was there already (hair cells, moles, etc).",null,0,cdn9ep2,1r6o58,askscience,new,2
Proxymace,"Potentially yes. But any orbit through the solar system undergoes changes from all of the planets gravities. These are likely to have a much greater effect than a solar flare which does not have much impact mass, even if it does contact directly with solar material",null,0,cdmjdr7,1r6p3q,askscience,new,2
Drunk-Scientist,"Adding to Proxymace's answer; another thing that can alter a comet or asteroids trajectory is simply the sunlight itself. [Radiation pressure](http://en.wikipedia.org/wiki/Radiation_pressure) from the photons colliding with an object can lead to minute changes in orbits that add up over time. In fact, one suggested technique to shift an asteroid headed for Earth would be to paint one half of it white and let the radiation pressure from the Sun do it's business.

However it should be noted that both this effect and that of Solar Flares is orders of magnitude less than those due to gravity, for example by close interactions with Jupiter. So unless a comet remains in a totally undisturbed orbit close to the Sun for many millennia (an unlikely state of affairs for eccentric comets), the effects of the solar wind and radiation pressure will be negligible.",null,0,cdn1ib2,1r6p3q,askscience,new,2
thetango,"When a bottle says aged 12 year they mean 12 year in a barrel not the bottle.   Liquor, for example whiskey, picks up flavor from the cask/keg it is barreled in.   The cask/keg can impart very subtle textures and flavors to a whiskey.

Wine, on the other hand, ages differently than liquor.  As wine ages, a chemical called 'tannins' breaks down and the result is a ""smoother"" tasting wine.",null,1,cdki31j,1r6pfu,askscience,new,4
NightmareOfLagrange,"I'm gonna go out on a limb here with this answer, but hopefully my simplifications won't be too offensive:

Soda is mostly water, sugar, and carbonation.  Based on their molecular structures, although water has decent surface tension (and can form bubbles), based on interaction with the rest of the water molecules and mutual attraction, neither of those are very good at holding a structure like a bubble.

Ice cream introduces lipids into the solution of your float, which, based on their structure, are more likely to retain bubbles (like how you can make bubbles with soap but ice cream tastes a lot better).",null,0,cdklmbz,1r6pjl,askscience,new,1
tigerhobs,"No, human cells do not, but surprisingly your gut microbiota can produce it if a yeast gets into and persists in your intestines.  Your intestines are largely anaerobic, and these yeast will ferment sugars to make alcohol, and you can get drunk.  It doesn't sound like a very pleasant condition, but it should be easily curable with anti-fungal agents.  [I first heard about it on NPR, click me!](http://www.npr.org/blogs/thesalt/2013/09/17/223345977/auto-brewery-syndrome-apparently-you-can-make-beer-in-your-gut) .  The literature does not have much on this relatively rare condition, but it seems to be possible.

To reiterate- your own cells cannot, but your gut, supplemented with alcohol producing yeasts, might.",null,4,cdkb7by,1r6pmw,askscience,new,15
null,null,null,6,cdka124,1r6pmw,askscience,new,6
Gargatua13013,"Nope

Spider and scorpion are arachnids (and indeed 8 limbed)

Crabs are crustaceans, together with lobsters and barnacles (and actually 10 limbed)

The closest marine relatives of spiders are  Pycnogonids (see: http://en.wikipedia.org/wiki/Sea_spider). Although pycnogonids are *not* spiders, or even arachnids, they are a somewhat related class.",null,0,cdkl2xw,1r6shx,askscience,new,2
thenotoriousFIG,"Yes, they are both Arthropods. http://en.wikipedia.org/wiki/Arthropoda",null,0,cdk76p7,1r6shx,askscience,new,1
cmuadamson,"It's an interesting question.  I can think of a number of conditions that would have to be met, but none of them rule out the possibility completely. Perhaps someone else can think of an impossible requirement.

*  The planet would have to be slow rotating, so that the synchronous orbital distance is far enough away that the two bodies don't tidally rip each other apart.

* The planet would have to be extremely more massive than the moon, so that their center of mass (the barycenter), about which they both are truly orbiting, is very close to the center of mass of the planet.

* The moon would have to have been captured, not formed by normal gravitational collapse of a gaseous field. Otherwise, the clumps falling together to form the moon would not have had a chance to ""sweep"" through the area of its orbit. Someone may be able to argue down this point.

* The moon must be orbitting directly in the sidereal plane of the planet, i.e. directly above the equator, which is unlikely to occur. If its orbit has any inclination, it is no longer a ""stationary"" orbit, but could be a ""synchronous"" orbit.

* Similarly, the moon's orbit must be circular. Any elongation means the moon's speed changes throughout its orbit, also changing it from a ""stationary"" orbit to a ""synchronous"" orbit. This is also very unlikely to happen coincidentally.
",null,0,cdk7lmj,1r6sqm,askscience,new,3
KarlOskar12,"Well all we can really do is look at what has worked in the past for organisms and what success they have had. As it currently stands the most successful organisms are single celled prokaryotes. They have been around for billions of years and their DNA replication has many errors in it. They mutate very quickly as a result and as a result they have been able to adapt to literally every environment on this planet. They have been around far longer than anyone else, and they will probably be the last surviving organisms on this planet. So you could easily make an argument that a higher rate of error is optimal in a DNA replication system.",null,0,cdk7wao,1r6tl1,askscience,new,1
snusmumrikan,"Agh the paper I'm trying to find is eluding me! It's from this month though.

The potential for variation is selected for evolutionarily. Without the potential of random mutation, a species will not be able to evolve and respond to varying or novel selection pressures in their environment (which is always changing, and on an evolutionary timescale the environment can change significantly quite quickly).

If you are 'stuck' without any change in the genome then a species will be at a significant disadvantage as there will be no way for the gene pool to adapt. As such it is actually preferable for the DNA machinery to have a background level of error, otherwise the species would die out. 

If anyone can find and link the article I mean, I think it is Nature this month, something like 'The capacity for variation is selected for...' I would be very grateful. It was also on many normal news sites as a side note article",null,0,cdkl6hf,1r6tl1,askscience,new,1
UncertainHeisenberg,"[This USGS FAQ](http://www.usgs.gov/faq/?q=categories/9830/3353) does a good job of explaining why larger ruptures can't be prevented using smaller, harmless, ""controlled"" earthquakes. 

The main reason is that you need thousands of smaller quakes to release the equivalent energy. For example, 32000 M3 earthquakes release equivalent energy to one M6 event. This scales to around *one million* M3 events if you want the equivalent energy of an M7 earthquake!

Secondly, how do you guarantee that the earthquakes you trigger will be minor enough to cause no damage? Now guarantee that for every one of the many thousands of quakes you will need to trigger.",null,83,cdkcb4a,1r6u2f,askscience,new,448
null,null,null,2,cdkaw9q,1r6u2f,askscience,new,28
MrsWerf,"Earthquakes also don't universally decrease stress in the surrounding area. Seismologists are now able to model the changes in stress ([Coulomb failure stress - the bottom of the page](http://www.geology.um.maine.edu/geodynamics/AnalogWebsite/UndergradProjects2010/PeterStrand/html/Introduction.html)) and show that some areas, particularly the tips of the ruptured fault, are areas with increased stress after an earthquake. ",null,4,cdkczko,1r6u2f,askscience,new,26
Anomander82,It's not really viable method of reducing stress at tectonic boundaries if one simply considers the scale of energy involved. The energy dispersed from the well during hydraulic fracturing is typically confined to a radius of a few metres to a few tens of metres from the wellbore. The commonly cited incident in the UK in 2011 is really an isolated occurrence. The amount of energy required to relieve large-scale tectonic stress is orders of magnitude greater than what would typically be applied to an unconventional hydrocarbon play. Also consider that there are numerous risks associated with the drilling process in a tectonic province with a high level of activity. Overall it's simply not feasible.,null,0,cdkgr5m,1r6u2f,askscience,new,5
ripitupandstartagain,"You can't state that hydraulic fracturing only creates small earthquakes, just that it seems to be responsible for increased seismic activity and that the events attributed to this cause have so far been small. 

The frequency of different magnitudes of earthquakes in a set area obeys the principles of chaos theory (ie the frequency of the event is inversely proportional to the energy released). There will be a stress point reached that will trigger a quake but the size of the quake produced is pretty random based on the odds of a certain amount of energy being released. For example a magnitude 6 earthquake releases approximately 32 times the energy of a magnitude 5 so over a set area you would expect magnitude 5 earthquake to be about 32 times more frequent than a magnitude 6 quake. 

Rather than causing small quakes, fracing can be said to be loading an area with stress past its trigger point. The earthquakes recorded at hydraulic fracturing sites tend to be on the order of about 3. So one could reasonably expect a magnitude 6 quake to occur within the time taken to create 16000 magnitude 3 quakes (32x32x32/2 for average). 
Now if a mag 3 quake is happening once a week (which seems to be very unlikely) that would mean a mag 6 would be expected after around 2300 years (obviously it could happen at any tipping point but the cumulative odd suggest about then) or about 70 years for a mag 5.

This is based on fracing being the major cause of the earthquakes which would be the case for the majority of sites as they are away from active faults (such as with the Blackpool quakes which halted UK fracing the other year). ",null,1,cdkhvgu,1r6u2f,askscience,new,5
Male_Rikku,"This idea is actually similar to a concept in plasma physics whereby you steadily bleed off free energy at the plasma-wall boundary by inducing magnetic oscillations. It's used to keep large events that could damage the wall from happening. In *practice* we may not have a good way to do this with geophysical problems, but in *principle* a way could exist if we could model everything well enough and had a lot of control over the ""shape"" of our perturbation.",null,3,cdkgjjj,1r6u2f,askscience,new,5
classycactus,"Also, induced seismicity is more closely linked to fluid injection. Particularly salt water/waste water disposal wells (class II), the induced earthquake that are being the cause of controversy are those causing earthquakes in parts of the crust that are fairly tectonically dead (like the mid west US). Also the earthquakes that cause real damage(~magnitude 6.5+) are much much much larger then anything you might get from fluid injection(IIRC the usually the earthquakes are from ~2.0-4.5, which is a fraction of 6.5, as the Richter scale is a log scale)",null,0,cdkkjej,1r6u2f,askscience,new,2
astazangasta,"At last, a good use for pie charts!

[Here](http://feww.files.wordpress.com/2009/07/total-seismic-moment-released-by-earthquakes-1906-2005.jpg?w=420&amp;h=361) is a graph showing total seismic release divided by magnitude of events. As you can see, a single event like the SF 1906 earthquake is a small fraction of the total, even though it had a huge impact. Meanwhile, there are single events that dominate this chart, implying it would take MANY SF-scale events to relieve the stress of one of the big movements. [Here](http://scienceblogs.com/greengabbro/wp-content/blogs.dir/265/files/2012/04/i-51c95bfa0685b39b350ab449ef2bd73e-2004-2005-seismic-energy.png) is another image of 2004-2005 events, showing how total energy release from a single event is much larger than many many small-scale events put together.",null,0,cdkkbue,1r6u2f,askscience,new,1
OrbitalPete,"There are several problems with forecasting reversals. Firstly, they do not occur at regular intervals. 

I was going to post a big explanation with nice figures, but in searching for nice figures I found this, which does the job brilliantly. http://all-geo.org/highlyallochthonous/2009/02/is-the-earths-magnetic-field-about-to-flip/

The long and short is we can go for millions of years without reversals, and a reversal is not an instantaneous thing but can take decades to millenia to occur.  

The causes are due to changes in convection direction in the outer core - convecting a fluid around a rotating sphere leads to instabilities. Then the bulk fluid direction changes, so does the field polarity.

It's worth noting that earth's field is currently not a true dipole either, there are significant positive and negative magnetic anomalies (particularly one over the South Atlantic), and if you look at the magnetic field at depth we are currently in a fairly multipolar condition, with several North and South magnetic poles. This may or may not be an indication that the poles are in the process of reversing. ",null,0,cdkd57i,1r6vce,askscience,new,3
McBented,"Some background: 

1) Reversals do not occur on regular intervals. It could happen after 100k years, and then not happen for a million years after that. 

2) Sometimes, the dynamo also tries to reverse, but end up the same polarity after the ""reversal"". These are called ""excursions"".

3) The reversal process itself usually takes about ~20,000 years, so it's very gradual to the time-scale we live in.

There are some people in the field who believe that we are at the beginning of a reversal right now. In my opinion it's pretty much impossible to know what the dynamo is trying to do until we're well into the reversal, so I find it pointless to speculate.

Dynamo reversals occur due to nonlinear behavior of the convecting fluids in Earth's iron-core, and its interactions with the magnetic field it generates. We can now simulate these fluids by solving the appropriate equations with a computer. In our models, we do get spontaneous reversals for geodynamo models. Since these equations are non-linear, they are not predictive for very far into the future (just like how weather forecast models are only good for the next few hours/days or so).

Interestingly, we can also model a cyclic dynamo with predictable reversals with the right parameters, like the Sun's dynamo.",null,0,cdkjlp1,1r6vce,askscience,new,1
galinstan,Some iron **is** oxidized to form iron(II) oxide in the slag and fume. Table III in the link provided by the OP mentions that 3.2 % of the charge is lost to slag and fume formation.,null,12,cdk8pey,1r6vha,askscience,new,49
SmellyRaghead,"Iron oxide will be destroyed at temperatures that high. The oxygen would be free until it encountered silicon or other elements with high melting point oxides.

When you weld (say) mild steel, you are relying on the fact that the surface coating of oxide melts at a lower temprature than the bulk material.

I can't remember the exact temp but certain iron oxides are quite easy to destroy with heat. ",null,11,cdkafnj,1r6vha,askscience,new,31
dmd53,"In short, this occurs because of the relative free energies of the various oxidation reactions at such elevated temperatures. The creation of SiO2 is far more energetically favorable than the creation of, say, Fe2O3, and as such the oxygen will selectively bind with the Si and prevent rust formation. Elements like Si are intentionally added to the steel melt to scavenge oxygen and prevent rust formation (as well as to form ceramic inclusions which increase the strength/hardness of the steel); these elements are known as [deoxidizers](http://en.wikipedia.org/wiki/Deoxidizer).

",null,1,cdkh6c9,1r6vha,askscience,new,9
ShoutyCrackers,"At the heat level used (3000°F +), most impurities are destroyed. Medieval blacksmiths referred to this as ""crucible steel"" and for a long time the only place to get it was the Middle East until the Vikings found a route to Iran from the Volta. There's a great NOVA (PBS) piece about it called ""Secrets of the Viking Sword"" that tells the whole story. Its on Netflix.",null,16,cdkatrq,1r6vha,askscience,new,20
372xpg,"If you look at a chart of the oxidation potentials of these metals and the temperature you can see that most of these metals/elements oxidize more readily than iron especially carbon, the big difference between pig iron and steel. Its a handy property, however some impurities do not like to oxidize this way.

An example of a similar process where iron is oxidized(with air) before another metal is in nickel converting.",null,0,cdkkyn4,1r6vha,askscience,new,2
mjwaters,"I think the answer you are looking for is that there is a lot of carbon in the molten iron at this point (http://en.wikipedia.org/wiki/Pig_iron). When they blow the oxygen in, it will preferentially react with carbon forming carbon monoxide and releasing a bunch of heat. With good mixing from the oxygen lance and good exhaust pull from above, the CO is stirred and pulled out. Silicon also reacts with oxygen before iron, but the primary reaction is combustion of carbon. If there was no carbon in the iron, you would see very fast oxidation into iron oxide. Source: I am a materials scientist and I have read this for fun http://www.amazon.com/Making-Shaping-Treating-Steel-Refining/dp/0930767020 ",null,1,cdklt4m,1r6vha,askscience,new,2
ImpossiblePossom,"As with all things in chemistry and thermodynamics: The reactions and resulting mixture allow for a lower Gibbs Free Energy (GFE) at the conditions of reaction. 

At room temperature and in the right environment, iron reacts with oxygen to form a few different iron oxides we commonly consider rust. One could assume that this would also occur in the furnace during the production of steel from pig iron.  However iron oxides that make up rust do not end up forming at high levels because other reactions occur instead. These reaction(s) are known as decarburization reactions.  In the case of steel making the exact reactions are O + C -&gt; CO and CO + O -&gt; CO2 (per OP’s link). Some iron oxide are formed in the BOS process (FEO). However these are not the same oxide’s present in rust (Iron III Oxide and Iron III Hydroxide). The FeO that formed is either blown out the top of the furnace’s effluent gas stream or forms a separate phase in the form the slag that can be removed.

I think the crux of the question is: Why do some reactions (decarburization) occur preferentially and others do not (Iron Oxide formation). This is where Gibb’s Free Energy comes in.  At the conditions in the furnace the decarburization reactions and resulting products require an overall lower Gibb’s Free Energy.  Since Gibb’s Free Energy is always minimized at equilibrium, the reactions and resulting mixtures that require a lower GFE will occur preferentially. This is why the majority of the carbon in the pig iron reacts to form CO, and a much smaller fraction or carbon forms CO2, and an even smaller fraction of Iron Oxide (FeO) is formed. 

So why doesn’t this happen at room temperature too? Well at room temperature Iron-O2-Environment system changes dramatically such that rust formation gives a lower GFE and the Iron Oxides in rust are preferentially formed.

Thanks for the question. It really challenged me to answer it in an accurate, concise, and understandable way.

Source: I’m a chemical engineer who specializes in the production process of advanced materials.

Link to wiki on decarburization:
http://en.wikipedia.org/wiki/Decarburization

Link to wiki on BOS Process (IMO it’s a bit less technical than the link OP posted)
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to the wiki on the Bessemer process (because why not! it’s a fun read but not really necessary to answer this question):
http://en.wikipedia.org/wiki/Basic_oxygen_steelmaking

Link to wiki on GFE (in case you need help sleeping tonight):
http://en.wikipedia.org/wiki/Gibbs_free_energy
",null,0,cdkma57,1r6vha,askscience,new,1
ramk13,"It's true iron and oxygen can combine to form rust (Fe2O3/FeOOH), but that doesn't mean that the two will fully react in that combination. The practical extent (how much forms) of the reaction depends on the thermodynamics of the reaction, the environmental conditions (temperature, pressure) and the kinetics (speed) of the reaction.

In this case, as a couple other people have said. Rust is less stable at high temperatures (thermodynamic) and the oxygen prefers to react with other compounds first (both thermodynamic and kinetic).",null,0,cdknkde,1r6vha,askscience,new,1
NightmareOfLagrange,"Although the original question regarding formation of impurities has already been answered, I'd just like to add that formation of rust specifically is a reaction where iron is oxidized, but rust is not just iron oxide.  Rust is either the hydrated oxide, or a hydroxide where water and oxygen are reduced to form hydroxide ions with electrons from what is assumed to be the iron oxidation.  
",null,1,cdkkvdq,1r6vha,askscience,new,1
joshhinz,"Hello. I'm a mechanical engineering student, I noticed you have not had any responses yet, so although this is not strictly my area of expertise, I will share my thoughts on your question with no pretense of absolute certainty...If you were to model an arm or leg for example, as a set of mechanical elements, it might be reasonable to model the bone as a cylindrical beam, and the muscles and tendons as a set of parallel springs and dampers on different sides of the beam. Additionally, it might be reasonably to say that: at rest these springs are likely in a small amount of tension(putting the bones in a small amount of compression). Also, the human body is only able to actuate these springs to either relax them or to put them into greater tension. Therefore my conclusion is this: Yes! muscles and tendons provide strain relief especially in tension because the muscles are able to ""share the load"" in tension and only a minimal amount (if any) in compression because the muscles are not likely to be brought past their relaxed (small tension) state into a state of much compression, the body is simply not able to physically actuate muscles in this way. ",null,0,cdkadb9,1r6vj0,askscience,new,3
NightmareOfLagrange,"Interesting question.  Muscles and tendons actually apply stress to the bones they are attached to.  Bone structure can be thought of as a composite material composed of collagen and mineral.  Structure of this composite is what gives bone its mechanical properties, and it's been shown that stresses acting on bone at a stage of incomplete secondary remodeling can affect the resulting structure.  In a way, you could say that yes, the stresses applied to the bone will help it develop into a design better suited to bear those loads, and physiologically other types of tissue do help in impact absorption to relieve stress on the skeletal system.",null,0,cdkldls,1r6vj0,askscience,new,1
Lost_Wandering,"The strain to failure will not increase due to the tendons/muscles since it is a property of the material not the biomechanical system. They will however increase the amount if load that can be applied to the system since some of the stress will be transferred to the soft tissue. Increasing the strain to failure would happen if composite composition changed, the collagen content is increased  (and consequently decreasing the brittle hydroxyapatite phase) creating a more viscoelastic material response, but decreasing the ultimate strength.

Source: I do computational material modeling of bone-like biocomposites",null,0,cdl9vnq,1r6vj0,askscience,new,1
EdwardDeathBlack,"Here is my understanding. Right right after the big bang, the universe was overall an electrically conductive material. EM waves can not propagate very far in a conductive material, so there is no remaining radiation from that immediate time. It couldn't ""propagate""

A little while later, matter starts forming hydrogen, and the universe becomes transparent, allowing radiation to propagate. We can see the radiation that existed at the point of time in the universe by looking ""far enough"". It is known as the [cosmic microwave background](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation) . 

My understanding is the universe was [~400,000 years old](http://en.wikipedia.org/wiki/Cosmic_microwave_background_radiation#Features) when it happened. So out of the 13.798±0.037 billion years of the universe, we can pretty much see all the way up to 13.798 10^9 - 400,000 ~ 13.797 10^9 years...so pretty far back. ",null,4,cdk7pk0,1r6xdz,askscience,new,11
MaskedEngineer,"Not only is it possible, it's easy. Or used to be. If you turn on an old-school analog TV, connect an antenna, and tune to an unused channel, much of the static you see is from the [Cosmic Microwave Background radiation](http://en.wikipedia.org/wiki/Cosmic_background_radiation). These are photons that are red-shifted so far that they've become radio signals. They're essentially coming from the edge of the discernable universe.",null,1,cdkl46l,1r6xdz,askscience,new,1
Blacklightzero,"As said before, we can see nearly as far back as the big bang.  They are working on a map of the universe using the Hubble right now.

And, if you look at the map so far, you can see that we are right smack dab in the center of the universe.  We can see equally distant in all directions all the way back to when things started emitting light.  And also that the oldest objects we can see look just like the ones that we can see right next door.  Curious, isn' it?",null,1,cdklhth,1r6xdz,askscience,new,1
LegateDamar,"For your example, no. Increasing temperature will not reduce the pressure needed to form diamond. Here is a phase diagram for carbon. Notice that diamond will only be formed at extremely high pressures and increasing temperature will just melt or sublimate the graphite. In fact, higher temperature increase the pressure needed to form diamond. 

http://www2.chemistry.msu.edu/courses/cem152/snl_cem152_SS12/_images/carbonphase.png

In the general case however, yes there are minerals whose crystal structures can be changed through heat. An example would be quartz. Here is a phase diagram for quartz. 

http://www.geo.arizona.edu/xtal/geos306/silica_phase_diagram_large.gif",null,1,cdkajxr,1r6xvg,askscience,new,3
Das_Mime,"There's no evidence that the fields or radio emissions from alternating current have any negative health effects, attempts to detect things like claimed [electromagnetic sensitivity syndrome](http://en.wikipedia.org/wiki/Electromagnetic_hypersensitivity) (not a recognized medical condition) have all turned up the result that there is no detectable effect on people.

Any conductor will stop a radio wave. Some electronics are shielded inside what are known as [Faraday cages](http://en.wikipedia.org/wiki/Faraday_cage), which is basically where you surround it with a mesh of conducting wire. This is commonly used near radio telescopes, where the radio waves from electronics might otherwise interfere with the telescope.",null,0,cdka5rl,1r6ymz,askscience,new,6
Mazetron,"Electromagnetic radiation can harm you, it can be harmless.  For example: 
-UV radiation in sunlight causes sunburn
-gamma radiation from space can cause cancer
-X-rays can cause problems if the dose is too high

But visible light, radio waves, and lower energy waves are harmless.

Also, electromagnetic fields (emf) are completely harmless.

And both DC currents and AC currents produce magnetic fields, the difference is an AC current will have a changing field while a DC current will have an unchanging field

Things stop electromagnetic radiation all the time: whenever light hits an opaque object.  For blocking high energy radiation that typically passes through objects that visible light cannot, dense materials like lead are used.  Lead pads can block x rays and gamma rays.

Electric blankets and power lines are harmless.

EDIT: Alright, technically you could kill someone with a magnet or burn their face off with visible light.  You can also die from drinking too much water.  The dosage of visible light and magnetism you find in everyday objects is way too small to harm you.",null,1,cdk7wsr,1r6ymz,askscience,new,6
ohsohigh,"Electromagnetic radiation can be dangerous. If it is a high enough frequency it can ionize molecules in your body which can adversely affect DNA and cause cancer. This requires high frequency radiation in the UV or X-ray region of the spectrum. Lower frequency radiation can transfer heat to your body, which can be bad for you if you get a sufficiently powerful dose all at once; this is how a microwave oven works. AC running through power lines isn't going to do either of these things. It is not going to produce high frequency ionizing radiation and if it was putting out radiation at sufficient power to burn you it would render the transmission of electricity incredibly inefficient not to mention the fact that that would be impossible for anyone to miss as you would be able to feel the heat. I am not aware of any other mechanism by which electromagnetic radiation can have detrimental effects on human health.",null,0,cdk7y5q,1r6ymz,askscience,new,1
iorgfeflkd,"The speed of sound in any real material can't exceed the speed of light.

The speed of sound in the early universe, confirmed through observations of the large scale structure of universe, is believed to be the speed of light divided by the square root of three. (1.7x10^8 m/s).",null,0,cdk8flo,1r70oa,askscience,new,13
Daegs,"iorgfeflkd is of course right, but the question itself shows a misunderstanding of relativity.

The speed of light is not simply a ""speed"", it is a property of our universe that literally changes reality around us to remain true. Both space and time change (length contraction / time dilation) in order to ensure the speed of light remains both constant and unattainable for anything to travel faster.

",null,0,cdku9h9,1r70oa,askscience,new,1
blakemerkes,"Not too sure, and not my area of expertise. But I do know that many cows are being fed Corn in farms to fatten them up. And one of the side effects is the buildup of pathogens in their alimentary canal. But I believe that is because corn contains much less fibre than grass.",null,3,cdkagdg,1r70u0,askscience,new,2
henriquetk,"bubbles are a delicate equilibrium of forces. It problaby bursts when other force is applied to it( example the contact between the bubble and other surface).The bubble can also burst when it's surface dehydrate and can't no longer sustain it self.
I'm not sure, any scientist can correct me.
Sorry for the poor english, I hope I've helped you. ",null,0,cdkdwvm,1r7118,askscience,new,2
Trill-Nye,"Bubbles are usually characterized by a force balance between the pressure of the gas inside the bubble and the surface tension of the material comprising the bubble film. The pressure pushes outwards on this surface, which would make the bubble expand. However, the surface itself resists this expansion, because it would require stretching of the film. This is similar to a balloon, the more air you blow into it (increasing the internal pressure) the more the surface stretches. 

Anything that affects the mechanical properties of the bubble surface, such as heat or contact with another surface, can cause the bubble to pop. The real source of the popping is usually either a decrease in the strength of the surface material or an event that causes the escape of the gas inside, similar to the popping of a balloon.",null,0,cdki6x8,1r7118,askscience,new,2
PepperJack_delicacy,"To understand prolapses, you'll need to know just a little bit about the anatomy of the pelvis. Basically, all the pelvic organs (bladder, intestines, uterus, vagina, etc.) are held in place by a sheet of muscles and ligaments referred to as the **pelvic floor**. Here is a very short clip that gives you a 3D view of what's going on down there: http://www.youtube.com/watch?v=cWZdRebxsdM
If you take a look, you'll see that it pretty much acts like a ""net"" holding the organs in place and has passageways that let the anus (and in women, the vagina) pass through. 

The immediate cause of a rectal prolapse is a weakening of these muscles and ligaments. **Once the weight bearing down exceeds the strength of the supports, you'll get a prolapse**. What exactly causes the weakness though? The important thing to keep in mind is that we don't know the **exact** causes but there are several conditions that can increase the chances of you getting a prolapse. I'll try to outline some of the biggest factors.

* **Straining**: If you have a long history of straining while pooping because of chronic constipation, you're more likely to wear out the muscles.
* **Old age**: As you get older, your muscles get weaker in general. The same applies to the pelvic floor muscles.
* **Nerve damage**: If the nerves that supply the pelvic floor muscles don't work properly, the muscles will have a harder time supporting the organs.

Again, since we don't know the exact causes of a prolapse, it's a bit hard to say for certain why women experience it more often. The most likely explanation is that women experience pregnancy and child birth, which absolutely do weaken the pelvic muscles. 

If you're worried about getting a prolapse, remember to have enough fiber in your diet and to drink adequate amounts of water. If you notice that you're constipated a lot and having to strain, have it checked out by a doctor. 

Let me know if you want me to clarify anything.

Sources: 
http://www.emedicinehealth.com/rectal_prolapse/page12_em.htm#prevention
http://my.clevelandclinic.org/disorders/rectal_prolapse/hic-rectal-prolapse.aspx",null,0,cdkmnbq,1r72is,askscience,new,3
GoThirdParty,"There is a canvas of muscles called the levator ani. Think of it as a basket on the bottom of your pelvis.

The other thing that holds them in is that your guts are on a mesentery. This is basically a membrane that the colon hangs on and is anchored to the posterior intraabdominal space.

EDIT: [Here is a video.](http://www.youtube.com/watch?feature=player_detailpage&amp;v=Qw-97RU2NFs#t=182) The mesentery is the flappy looking thing the colon is on.",null,0,cdkmhiq,1r72is,askscience,new,1
temuchan,"The book ""Why we get sick"" by Nesse and Williams discusses this in one of the chapters.  Natural selection selects for genes that increase an organism's ability to pass on their own genes (reproduce and make sure your offspring reproduce), regardless of whether or not the individual desires the trait associated with that gene.  In humans, miscarriage occurs in a fairly high percentage of pregnancies, often without the mother ever knowing she was pregnant (This is why IVF clinics implant fairly large numbers (~7) of fertilized eggs).  Nesse and Williams discuss that myopia (near sightedness) may decrease the percentage of pregnancies that result in miscarriages.  So even though someone with myopia may not want myopia, the trait will be selected for if it means that person will have more offspring than a non-myopic person.  Edit: Also, having myopia today is not as life threatening as it may have been for cavemen.  If you live in New York you are unlikely to be eaten by a tiger regardless of how good your vision is!

Another example of this is Huntington's disease.  ~~The fact that it has not disappeared yet means there is some selective pressure for keeping those genes.~~ (Edit: See Darkaardvark's comment)  Huntington's symptoms tend to begin around age 40.  Since many people reproduce before age 40, the debilitating effects of the disease do not decrease the ability to reproduce and the gene is not selected against.

Edit: More food for thought, if a hypothetical mutation appeared that caused exceptional vision, but also made the person sterile, it would never be passed on, no matter how desirable eagle eyes are.",null,3,cdkcm8o,1r72ls,askscience,new,19
Klinefelter,"optometrist here: one of the theories for the increase in myopia is that an increased amount of near-work increases myopia. so for instance, studying in school have been shown to make you more nearsighted. There has also been shown to be a genetic component for refractive error. Since glasses can correct for refractive error, poor uncorrected eye sight does not make you less likely to be able to reproduce so poor eye sight is still prominent in our society. ",null,0,cdkfzcw,1r72ls,askscience,new,6
atomfullerene,"Myopia is a disease of modern life, like diabetes, asthma, and obesity.  In premodern societies, it is very rare.  Even in most modern societies, it has vastly increased in prevalence due to environmental condition in the recent past.

It is _not_ the case that selection doesn't select against nearsightedness.  Nearsightedness has historically been a huge fitness disadvantage, and only within the last few generations (since the spread of eyeglasses) has this decreased.  That's not enough time to cause eyesight to degenerate, since when selective pressure is removed from a trait, it tends to remain the same, not degenerate (except over very long timespans)
",null,0,cdkmq7q,1r72ls,askscience,new,1
bdlxb3,"I'm willing to bet you start to lose your eyesight around the time you lose the ability to reproduce. Evolutionarily speaking you are a waste of space after you turn 30 or 40. Yes I know there are advantages to having grandparents alive to care for their young, but I don't think Cro-Magnon cared too much about his 401K. People with truly horrible eyesight are able to survive thanks to modern science, just like diabetics, pussies allergic to gluten, and fat-asses with bad thyroids. Still a human with sub-par sight has better sight than other animals. 
",null,5,cdkza88,1r72ls,askscience,new,4
expertunderachiever,"You can see forever.

What you mean to ask his how finitely of an arc can I resolve at a given distance.  

The definition of 20/20 vision for instance [see http://en.wikipedia.org/wiki/Visual_acuity] means that at 20 ft you can tell two lines that are 1 arc minute [about 1.75mm] apart.  So you could see something 100ft away but it would have to be separated by at least 89mm.  At 1km it would be 290mm, etc and so on.

So given enough brightness you could see two lights from 100,000km away if they were separated by about 30km.
",null,2,cdkfhfg,1r72q2,askscience,new,15
RetraRoyale,You'd be able to see to the edge of the universe. All it takes for you to see something is for light to enter your eye. The only reason you don't see that far in some directions is because there are objects in the way.,null,1,cdk9y4b,1r72q2,askscience,new,2
erlegreer,"Absent of haze (atmosphere) and other objects, you would be able to see objects at least up to millions of light years away.
source: Normal healthy naked human eyes can see the Andromeda Galaxy, which is about 2.54 million light years from Earth.",null,0,cdka0uz,1r72q2,askscience,new,1
I30T,"It depends on what you want to observe.
Technically you can observe as far as an object can give light to. Stars can be observe from light years away. However if you want to observe another person then (remembering from 7 years ago) you can only see about 300 miles.

VSauce has done a video on this.",null,0,cdkdxly,1r72q2,askscience,new,1
Dominus_,"Theoretically, to the end of the observable universe (14 Gigaparsec, or 45 billion lightyears), but in reality, your eye cannot see anything that takes up less than 0.07° of your field of vision (average). 

This is equivalent to about 1.2 mm at a meters distance. So at one and a half kilometer, you would just barely be able to see an ~average person. ",null,0,cdkewbt,1r72q2,askscience,new,1
tehgreatblade,Why does any body part make cracking noises? Is this a sign of injury or is it benign?,null,0,cdkue60,1r72q9,askscience,new,2
Zangetsux20a,"Between the joints in your body are little pockets of synovial fluid. These pockets contain gases like nitrogen and CO2 bubbles.They're there to prevent your bones from rubbing up against each other when your move around. The neck has these joints as well, and the same synovial fluid pockets. When you ""crack"" a joint, you deform the shape of the pocket. This increases the pressure on the bubbles in the synovial fluid, with make a popping sound. ",null,0,cdlxta9,1r72q9,askscience,new,1
Local_Motion,"Interesting question. Antibiotics can disrupt the normal gut flora. This can be seen with penicillin-based antibiotics and cephalosporins, among others. 

What's more interesting to me is the end result. You are treating one infection, but in the end can cause another ""infection"". These antibiotics, when they disrupt certain gut flora, allow the excessive proliferation of other gut flora, specifically Clostridium difficile. C. diff can overgrow since the competition has been reduced, and with it's toxins, it can produce pseudomembranous colitis and some bad diarrhea. 

So what do you do? Treat it with another antibiotic, namely, metronidazole. Makes the differences in the bacteria structure/virulence and antibiotic mechanism of action an interesting study.

Source: A lowly med student",null,5,cdkgzr3,1r72qt,askscience,new,22
null,null,null,22,cdkad4i,1r72qt,askscience,new,32
HushaiTheArchite,"One of the proposed functions of the [appendix](http://en.wikipedia.org/wiki/Vermiform_appendix#Maintaining_gut_flora) is preventing this kind of gut flora-pocalypse from having a long term impact. That said, gut bacteria can get very far out of whack and have been thought to contribute to some illnesses of the colon. You're interested, look up stool transplants. They're simultaneously disgusting and a really fascinating example of the interaction between humans and their bacteria.",null,0,cdki6j5,1r72qt,askscience,new,4
ModernTarantula,Most digestion does not involve bacteria. So you will still get calories and proteins. But diarrhea is a common complication of antibiotics. Most absorption of antibiotics takes place in the mostly sterile small intestine.  The colon mainly absorbs water and is the locale of bacteria.  A serious compliction is altering the gut flora to have more serious and infectious bacteria (*Clostridia dificile*) ,null,1,cdkc31q,1r72qt,askscience,new,4
null,null,moderator,2,cdkjwbi,1r72qt,askscience,new,2
codyish,"It can and does.  Not enough to kill it all off but enough to inhibit digestion of some food that rely heavily on gut fauna. AFAIK, this is one reason why it's recommended to not consume dairy while on a antibiotics. ",null,10,cdkabfh,1r72qt,askscience,new,10
iorgfeflkd,"The quantity that is often cited as being the best measured is the magnetic moment of an electron (related to how an electron responds to magnetic fields). It is very close to two, and half the deviation from 2 has been measured as 0.00115965218073(28), where the brackets indicate uncertainty on the final digits. This means the magnetic moment has been measured to a part per trillion.

The theoretical value, based on quantum electrodynamics, is .001159652180{85(76)}, where I put curly brackets where it deviates from the experiment (within the uncertainties). This is the most accurate verified prediction in science.",null,3,cdk98m9,1r734s,askscience,new,11
Trill-Nye,"A very [recent experiment](http://arxiv.org/pdf/0709.2996.pdf) managed to make a measurement with an accuracy approaching the Heisenberg uncertainty limit. This means that the uncertainty associated with this measurement is roughly as small as is physically possible, because quantum mechanical considerations preclude precise knowledge of the state of a system at higher accuracies. 

This measurement was of the interference of two photon that were only slightly out of phase with one another.",null,0,cdkifpr,1r734s,askscience,new,1
proule,"Animals like cats raise their fur with the help of arrector pili muscles in each hair follicle. This serves a myriad of purposes, from thermal insulation (less movement of air close to the animal's skin) to intimidation (hair stands up, animal looks bigger).

Humans actually have this arrector pili muscle, but it's strongly reduced in function. Humans do actually raise their hair, but the main thing you'd notice is goosebumps. You'll get goosebumps in response to cold (again, to reduce air flow around the skin), but since our hair isn't very thick in most places, this largely isn't effective anymore.

The arrector pili muscle in humans is thus largely vestigial (a remnant of a structure that served a purpose that isn't necessary anymore). 

This weakness in our arrector pili means that the hair on your head just largely will not move, even with these muscles engaged. They are, however, still present in the scalp, and if you get goosebumps you can feel the action of your arrector pili on your scalp. You know the sensation, it's just not as readily obvious as when a cat's hair stands up.",null,2,cdkmc4v,1r737x,askscience,new,5
Platypuskeeper,"There's a closed-form solution for a Dirac potential, with a bound state. (and for the double-Dirac potential as well)

I don't really see the connection. Our knowing about a closed form solution doesn't make those stable, bound states any more special than the stable, bound states of potentials that can't be solved analytically. Bertrand's theorem is inherently linked to trajectories - which quantum particles don't have (or at least not in that sense), and conservation of angular momentum - but the ground state solution to the Schrödinger equation for a central potential is always l=0. 

",null,2,cdk9tp8,1r74bj,askscience,new,4
Lillelyse,"Yes it does. The time it takes for signals to reach our brain depends on the length of the axons involved. The speed of an action potential, depending on the nerve, is generally very fast, so delays are in the ms range. 


Taller people, who need longer axons to connect one part of the body to another, will take longer to perform for example a knee jerk reflex as the signals will have to travel a longer distance. When it comes to things like vision and hearing, I don't think there will be much difference between pathway lengths from person to person (unless you have a massive or abnormally small head), so any differences will be miniscule. This does of course not apply to people with diseases that affect neurone ability to propagate action potentials, such as MS. ",null,0,cdketnw,1r7531,askscience,new,3
clockwerx,"You may also be interested in learning about blindsight (http://en.wikipedia.org/wiki/Blindsight), which suggests ""comprehension"" of an image takes place very differently from many of the earlier stages; or why we have certain frame rates (http://en.wikipedia.org/wiki/Frame_rate#Background); which imply our brains are stitching together inputs into a seamless experience.

Interestingly, that stitching together of information that our brains do is far from perfect - Optical illusions (http://en.wikipedia.org/wiki/Optical_illusion) demonstrate we've got a few helpful rendering optimisations that work for most, but not all scenarios.

When you think about it, it suggests we live in a slightly laggy simulation of reality just kind of guessing our way through it all. 

It works well enough to avoid being eaten by large animals or falling off cliffs, so after that the pressure is effectively off for our bodies to get any better at this sort of thing.
",null,2,cdkf3gv,1r7531,askscience,new,6
batmantis25,"Our brains actively compensate for this ""lag"". Effectively, we generate images before we actually ""see"" them. I've seen sources which suggest a lapse range amongst individuals but I'm having trouble finding that at the moment.
http://www.livescience.com/4950-key-optical-illusions-discovered.html",null,0,cdkhcqq,1r7531,askscience,new,3
SimpleBen,"OK, some numbers. 
  
Sense of touch: Light touch  
Finger to brain: 20-30 msec  
Toe to brain: 50-60 msec  
  
Sense of burning pain  
Finger to brain: 1 second  
Toe to brain: 2 seconds  
  
Itch:
Finger to brain: 2 seconds  
Toe to brain: 4 seconds (ever wonder why that mosquito seems to get such a good head start??)  
  
Sense of vision to brain: 30 msec or so  
  
Sense of hearing: Ear to brain
About 10 msec
  
These time lags will vary from person to person, but not that much. Light touch, for example, may have a 5 msec range across people.",null,2,cdkinxn,1r7531,askscience,new,4
afcagroo,"In addition to the large sensory lag, there's a small lag that ensures that we are always living in the past.  If you have a CPU that executes instructions at a rate of 600 MHz, that means that it completes one instruction every 1.67 nanoseconds. Which is how much time it takes light to get to you from a computer monitor 1/2 meter away. ",null,0,cdki8c8,1r7531,askscience,new,2
Jyesss,"Yes, the action potential must travel from the nerve through the spinal cord to the brain, the brain must process the information, and then send an effector response back. The rate that the signal can travel along the nerve can be increased by increasing the diameter of the nerve. This is important for larger organisms, and a lot of what we know about nervous transmission was learned by studying octopus' nerve. They are huge and can be studied under just a light microscope. Mammals developed another way to expedite nerve transmission via myelination. This helps to insulate the nerve signal and increase rate. There is a theory proposed that posits that athletes with fast response times is due to the body further myelinating the appropriate nerves to convey the action potential faster. ",null,0,cdme9lo,1r7531,askscience,new,1
DEATH-OF_RATS,"What others have said is more or less correct. There are a few stipulations, though they're tangential.

The knee jerk reflex only changes with length of the nerves between muscles around the knee and where they they hit the spinal cord. It's a locally generated spinal reflex, so the brain has no part in it (this is a general definition of a reflex - that it's generated in the spinal cord).

In addition to some variance of axon size across people, different sensory inputs have different axon conduction speeds. Proprioception is a very fast sense, where vision is relatively slow. /u/SimpleBen summarized delay times nicely. As /u/batmantis25 pointed out, the brain also compensates for the lag by extrapolating current vision to what we should expect to see next.

The same thing happens in the sensory system with tactile and proprioceptive inputs. The brain predicts what future tactile and proprioceptive input it expects to receive based on the current tactile and proprioceptive inputs in combination with the current motor output.

This sharing of motor output is called ""efference copy"" - the motor cortex sends commands directly to sensory cortex, as well as to muscles. The sensory cortex can use that information to predict what it should next feel, allowing much better control than systems that are purely reactive. This is one problem you run into with rudimentary robotics - they adjust activity to compensate for sensed (past) inputs, instead of having a good generative model of what should come next.

What this all means at a higher level is that, in very trained movements (like an olympic wrestler executing a take-down during a match), your brain knows how to predict what will come next based on current input, because it has a very good model of how the system acts. Here, ""the system"" includes the player's own body, the floor, the opponent... everything that interacts. As a result, the athlete knows quite well what move their opponent is doing once the opponent starts the move, which means they know what the future state of the opponent will be, and can respond to that future instead of the present, which is really the past. A novice wrestler does not yet have a good model built and relies only on reaction time (usually in the 200-400ms range), which is usually not enough to compete against someone with training.

The idea of ""compiled"" actions also comes into play, and are the reason for repetitive drills. Instead of having to process each step in a sequence of moves, a trained person can initiate the sequence and the body automatically completes it; this is the idea behind muscle memory. This is why you don't have to think much about a practiced movement (like tying your shoes).",null,0,cdmjahz,1r7531,askscience,new,1
Astromike23,"First off, MAVEN does *not* have the ability to detect methane. There were plans to include an instrument that could do so, but it was removed due to budget cuts. Such an  instrument will likely be included on the 2016 ""Trace Gas Orbiter"", though.

With that said, it's still an important measurement to make with a spacecraft. Over the past few years, we think we may have detected methane on Mars using Earth-based telescopes - but this is a very hot topic, and very controversial. There are quite a lot of people in the planetary science community that are not convinced by the evidence, and yet also quite a lot who have based their research on it being true.

The biggest problem here is that it's an exceptionally difficult measurement to make from Earth, because our own atmosphere's methane gets in the way. When looking at a spectrum, it's hard to tell exactly what's coming from our planet, and what's coming from Mars. By measuring this from outside Earth's atmosphere, we can instantly remove that source of confusion from the data. 

The current data taken from Earth is pretty ragged and down in the noise. It's equally suspicious that the methane seems to come and go in both location and time, with only some groups detecting it, and others not able to do so (the Curiosity rover, so far, has not). Maybe it's an exciting dynamic process that suggests Mars is not a dead planet...or maybe it's just imprecise measurements. 

Either way, it's an important enough result that it bears checking with greater precision by a spectrometer outside the Earth's atmosphere. The Hubble would have a difficult time doing this - the easiest place to measure methane is in the mid-infrared (methane has strong spectral lines at 3.3 and 7.8 micron wavelengths), which is outside the range of Hubble's spectrographs.

**TL;DR**: MAVEN cannot sense methane due to budget cuts, which is a shame. From Earth, methane is very difficult to detect on Mars because our own methane gets in the way of the observation.",null,3,cdkdp3s,1r75ya,askscience,new,6
adamhstevens,"MAVEN isn't going to measure methane at all. Its entire science goal is to measure water loss in the upper atmosphere. I'm pretty sure it couldn't measure methane if it tried. However, the recently launched [Indian mission](http://en.wikipedia.org/wiki/Mars_Orbiter_Mission) and the European [Trace Gas Orbiter](http://en.wikipedia.org/wiki/ExoMars_Trace_Gas_Orbiter) both have instruments essentially devoted to detecting methane.

As to your actual question, we can do it from Earth and it has been done, and actually terrestrial results are the most convincing. (e.g. [Mike Mumma's results](http://images.spaceref.com/news/2009/Mumma_et_al_Methane_Mars_wSOM_accepted2.pdf)). However, some people (quite rightly) question terrestrial spectra. This is because there's an awful lot of methane in the Earth's atmosphere (roughly a part per million - ppm) compared to Mars' atmosphere (if there is any - it's around a part per billion - ppb) so you have to 'correct' for the terrestrial atmosphere. This can be done, but only to a certain level of accuracy (since the atmosphere gets stirred up a lot), which turns out to be about accuracy you need to be able to detect methane on Mars at all. However, there's a bit of leeway given by the fact that spectra taken from Earth of Mars are either slightly red-shifted or blue-shifted depending on the planets' relative motion. This means you should be able to distinguish martian methane by a slight shift in the methane absorbance. Unfortunately there are other issues that mean this isn't quite as straightforward, either.

So basically this all means that we need to send instruments to Mars so they can image the atmosphere without interference. Curiosity's in-situ experiments set the level of methane as having to be very low (they didn't detect any, so it must either not have been there or be there at a level below the instrument's precision) but that was only measured at one place on the planet at one time, and the measurements we do have suggest that any methane is highly variable in the atmosphere, so really we need long term observations on a planet-wide basis, which is what orbital spectroscopy allows us to do!

Anyway, this is actually what I'm doing my PhD on so if you have any more questions, feel free to ask.",null,1,cdkdppm,1r75ya,askscience,new,6
neha_is_sitting_down,"No idea, but if we haven't found it yet, odds are that we can't detect it from here using regular techniques (maybe it is very scarce. Or maybe we don't have the right conditions to use spectrography) and so the only way to be sure is to go there and measure directly.

Just thinking about this, the only way to do spectrography at that distance would be to have a light source behind the planet (correct me if I'm wrong about this). Because mars is outside our orbit, it will never be between us and the sun, so we will never have that light source.",null,4,cdkcp54,1r75ya,askscience,new,2
arumbar,"There is a 'minimum' lung volume - this is known as the [residual volume](http://www.admit-online.info/fileadmin/materials/images/cd_rom/1221_lung_volume.gif).  This is the volume of air that remains in the lungs after maximal expiration.  You can see some of the other typically referenced volumes in that figure there - the narrow band in the middle (tidal volume) represents air movement during regular breathing, while the peak (inspiratory reserve) represents how much additional air intake occurs during maximal inspiration.  Similarly, expiratory reserve tells you how much additional air you can blow out to reach residual volume.  These values are often clinically very significant in describing pathologies of various lung diseases (for example, someone with restrictive lung disease such as pulmonary fibrosis typically has reduced volumes due to inability to fully expand the lungs, while someone with obstructive disease like COPD/asthma may have larger volumes due to air trapping).  We also use [spirometery](http://en.wikipedia.org/wiki/Spirometry) to further characterize how air moves in and out of lungs.

Outside of intentionally breathing out to collapse a lung, this can also happen with a pneumothorax, where the pressure gradient normally created by the pleural space is lost (eg due to a puncture wound to the chest).  This can result in collapse of the [lung on one side of the chest](http://www.daviddarling.info/images/pneumothorax_radiograph.gif) - note the loss of pulmonary markings on the right side of the chest (left side of the picture).",null,0,cdkejtb,1r77hw,askscience,new,7
patchgrabber,"Cats are [obligate carnivores](http://en.wikipedia.org/wiki/Carnivore#Obligate_carnivores), meaning that they need to get all their nutrition from animal sources, and they also lack the digestive enzymes to break down plant material. Kibble can be used as a food source because it contains some animal meat and likely an extraneous source of taurine, which is very important for cats.

Dogs, however, are also carnivores. Their short intestines and colon are not well-suited to digesting plant matter either, as most digestion for dogs is in the stomach and they lack the long intestines required for proper fermentation of plant matter by intestinal microbes, although they do have higher levels of amylase as [this](http://news.sciencemag.org/plants-animals/2013/01/diet-shaped-dog-domestication) study found. This means that over time having been domesticated and fed starchy foods, they have somewhat adapted to be able to process these foods a little better. This is similar to people; those from Europe and Asia have more amylase production than people from Africa. It is not advisable to only feed your dog vegetables, as they are still carnivores like their wolf counterparts. Dogs' stomachs are also different than a feline's. Dogs have strong muscles that can push large and confusing things through their digestive system. An old veterinarian friend of mine would tell me stories of socks and other assorted weird things being passed through a dog's system.",null,3,cdkeiwk,1r7cxp,askscience,new,20
mrsix,"Cats are [Obligate carnivores](http://en.wikipedia.org/wiki/Obligate_carnivore#Obligate_carnivores) - their metabolisms are unable to produce a few essential nutrients out of vegetable matter, and therefore take it in directly from animal protein. A very important one specifically for cats is [Taurine](http://www.vcahospitals.com/main/pet-health-information/article/animal-health/taurine-in-cats/3857), which [some of the first 'cat food' in the 50s-70s lacked](http://www.onlynaturalpet.com/knowledgebase/knowledgebasedetail.aspx?articleid=113) which unfortunately lead to many cat's premature deaths until about the 80s. Most animals are able to produce it from methionine which is commonly found in plant material, whereas cats simply ingest it from other animals.",null,3,cdkehqp,1r7cxp,askscience,new,7
hideous-bike,"A differential allows for the driving wheels to spin at different rotations per minute therefore allowing a car to turn corners. This is because the inside wheel in a turn has less distance to cover than the oudside wheel on the vehicle in the same turn. 

Think plastic coffe cup on it's side. It will roll in a circle / turn because the bottom is narrower than the dinking end of the cup. What a differential essentially does is give power to the wheel with the least resistance. This is a desirable trait in wheels on a road surface, because otherwise the driving wheels start to squirm and bounce. 

In mud however this is not, because when a drive wheel looses traction and starts to spin the power of the engine will just be sent to the spinning wheel and therefore not adding to the traction of the vehicle. To overcome this some cars can lock their differentials to ensure that both wheels will start churning at the same speed giving a lot more traction to the vehicle.

On a side note: don't lock your diffs on a road surface (if your car has the option). That is, unless you really like your mechanic, and like to give him, or her, fist fulls of money.     

",null,3,cdkdzw6,1r7e4c,askscience,new,24
stairwaytoheaven57,"Where did you hear this info?  Locking differentials are almost always paired with four wheel drive.  It certainly does get more traction in addition to 4x4, but a locking differential without 4x4 wouldn't be as effective.  With four wheel drive you have power going to every wheel unless you lose traction in one wheel from the front and one wheel from the back, but until then you're pulling with all the tread so it's less likely that you'll get in a situation where you lose traction.  With just a locking differential you would have to lose traction to both back wheels which is more likely.  First, the back generally has less weight on it, second, you can steer the front wheels back and forth to try to grab onto the sides of the hole, and lastly, without 4x4 the front wheels would be introducing drag instead of pulling. ",null,3,cdkeg17,1r7e4c,askscience,new,23
MrPickleCoppter,"Locking differentials are best suited for off-road or low traction environments. With a locked differential both drive wheels will turn at the same speed making turning more difficult if you have full traction.  This will give more wear and tear on your tires on asphalt and your tires will chirp as you go around a turn.

I have just installed an auto locker in to the front differential of my Jeep.  Its is only active if I put it in 4 wheel drive and if I apply gas.  If 4 wheel drive is not engaged it will still preform like an open differential but with a ratcheting mechanism.

[install process of the locker](http://imgur.com/a/HlDGq) 
 ",null,0,cdkfdva,1r7e4c,askscience,new,8
LibertyBill,"A locking diff isn't always necessarily better than 4x4. 
Not only that but 4x4 accompanied with a limited slip rear diff is much better suited to everyday driving due to the ""flexibility"" of a limited slip diff vs a locked diff.

A locking diff would have very limited use in a primarily road vehicle. This is mostly due to the fact that the tires will spin at the same speed even when taking turning into consideration. On pavement this will cause ""wheel hop"" and a lot of strain is put at the diff wearing it down/damaging it. Wheel hop will occur in vehicles with 4x4 with open or limited slip diffs as well but these options are more ""giving"" in terms of stress on the diffs.

Also, it's a myth that 4x4 means all four tires have ""power"" going to them.  The only case this would apply to is if both the front and rear diff are locked. I believe this option exists in some Hummer models. In many ""All-wheel"" and 4x4 designs there is usually one or two ""open"" diffs involved. An example would be my Tacoma. It has 4x4 but both the front and rear diff are open. This means (unfortunately) the energy will be routed to the tires that give the least resistance. I say unfortunate as these tires will spin in place. To clarify this point even more I actually got my truck stuck in 4x4. Both my front left and rear left tire were on slippery ground.  I was however able to get out by cycling the steering wheel back and forth.

So to answer your question: Given all the possible scenarios of ""4x4"" I'd still take the 4x4, even with both front and rear open diffs, over just a locked rear dif any day.",null,0,cdkkbbr,1r7e4c,askscience,new,2
pbae,"Most cars have ""Open Differentials"" which already has been explained, let's the outside wheel turn faster than the inside wheel when a car is making a turn.

A MAJOR disadvantage of an ""Open Diff"" is that the wheel with the LEAST amount of traction gets the MOST power provided by the engine.  For example, if you take a car with an open diff and jack it up on either the left or right side of the car and then rev the engine, the wheel that's in the air is the one that is going to spin, not the wheel that is touching the ground.

Now imagine being off-road.  If one of your driven wheels gets stuck in the air, you would be stuck because your car or truck isn't going anywhere because the only wheel getting power is the one in the air.  You would be able to get out of this situation if you had a locking diff since you could lock the diffs and send power to both wheels at the same time.

And to answer the OP's question, a rear-locking diff is better than four wheel drive (4WD) for the reason explained above because some 4WDs have two open diffs.  The best would be 4WD with a locking diff.

",null,0,cdkq885,1r7e4c,askscience,new,1
DarylHannahMontana,"One thing to note here is that the *definition* of an integral is ""the area between the function and the x-axis"" (ok, the real definition is a little more technical than that, but the main idea is the same; if you remember Riemann sums from your calculus class, *that* is the real definition of the integral^\*).

It is a *fortunate coincidence* (i.e. a theorem) that the integral of a function is related to its antiderivative; this is the fundamental theorem of calculus (and more generally, there is something called [Stokes' theorem](http://en.wikipedia.org/wiki/Stokes%27_theorem), of which FToC is a specific example).

So, if you can find an antiderivative for a function, that gives you an easy way to calculate an integral ""by hand"" (and, as others have said, there always *is* an antiderivative, but it may not be ""elementary"" (see JoshuaZ1's post)).

But if you can't find an antiderivative (and usually, you can't), you can just integrate using the definition (adding up the area of very narrow rectangles or [trapezoids](http://en.wikipedia.org/wiki/Trapezoidal_rule)) and get a numerical value that is as precise as needed.

That is, is some sense, using an antiderivative is the exceptional way of computing an integral, and numerical integration is the ""standard"" method.

\*: there's also a Lebesgue integral which is defined differently, but this is not the time to discuss it fully.

EDIT: misspelled 'Lebesgue', and added wikipedia link to Stokes' thm",null,1,cdki2kc,1r7hb8,askscience,new,11
JoshuaZ1,"The issue simply put is that there are functions without an elementary anti-derivative. For our purposes, a function is [elementary](https://en.wikipedia.org/wiki/Elementary_function) if it a combination of x, log, constants, exponentials, closed under addition, subtraction, division, multiplication, and composition. Note that we get trig functions from using exponentials with complex numbers, but if you want to stick to the reals you can instead throw in the basic trig functions and inverse trig functions. You'll get essentially the same set for the totally real functions but it turns out this is a harder definition to  work with.

[Liouville's Theorem](https://en.wikipedia.org/wiki/Liouville%27s_theorem_(differential_algebra\)) severely restricts what anti-derivatives can look like, and can be used to prove that there are elementary functions that don't have elementary anti-derivatives. Now, one can extend what one is working with to a larger set of functions. For example, one can also throw in the [Lambert W function](https://en.wikipedia.org/wiki/Lambert_W_function). But these functions only help so much, and they are about as hard to approximate as simply doing the numeric approximation of the integral you care about. But at another level, this isn't so bad, you are in practice going to need approximations even for elementary functions for practical purposes, so this is a comparatively minor inconvenience for engineers. Generally,  it is mildly inconvenient for physicists, and is substantially more inconvenient for mathematicians who may care about the exact behavior of a function, and may have to get that way from analyzing its derivative rather than the function directly. 
",null,2,cdkgcbp,1r7hb8,askscience,new,11
ohsohigh,"The approach we take to solving integrals is based on the fact that integrals are antiderivatives. To oversimplify quite a bit, we basically look at a function and say this looks like the derivative of this other function, so that must  be the integral. If a function doesn't look like the derivative of some other function then we don't know how to integrate it analytically and have to use a computer to approximate the answer.",null,2,cdkf9ma,1r7hb8,askscience,new,5
Sirkkus,"What exactly constitutes an analytical solution or closed-form solution is subjective and arbitrary. Take for example the differential equation dy/dx = y. Most people will say that this equation has an analytical solution, the exponential function. But what exactly *is* the exponential function? It can be defined in a number of ways, but ultimately if you want to calculate it's value you have to use some numerical definition. The only reason that the exponential gets to be considered an analytic solution is that it's very common and most people are very comfortable with it's properties.

Some integrals may not have solutions *in terms of other functions that we're familiar with*, and that's all we mean by not having an analytic solution. If a particular integral comes up a lot you could give it's solution a name and include it in your new definition of elementary functions. This could be useful later if you find that some other complicated integral can be written in terms of this one, meaning you may not need to write a new numerical algorithm to compute it.",null,2,cdki7sj,1r7hb8,askscience,new,6
doctorbong,"In addition to the good answers already here, I'd like to point out [this thread](http://mathoverflow.net/questions/66377/why-is-differentiating-mechanics-and-integration-art) on MathOverflow. In particular, I like Terry Tao's answer. In summary, integration is a global operation: In fact, when one defines a Lebesgue integral, a pointwise definition of a function isn't even needed. On the other hand, the derivative is an *extremely* local property of a function - it only deals with limits over arbitrarily small neighbourhoods of a point.",null,1,cdl0l9k,1r7hb8,askscience,new,5
GruntingButtNugget,"The Rings are Saturn are much more dense than the asteroid belt. The rings themselves are actually seven sets of concentric circles that circle the planet. There are gaps between each set of rings big enough that Cassini was able to fly between two sets of rings on its way closer to the planet. Flying THROUGH a ring would be similar to how you would probably have originally pictured the asteroid belt

Edit: spelling is hard on a phone",null,0,cdkjzau,1r7hch,askscience,new,3
EdwardDeathBlack,"The positive/negative temperature scales of Celsius and Fahrenheit are arbitrary. The more common scientific scale is the [Kelvin](http://en.wikipedia.org/wiki/Kelvin). The Kelvin never goes negative. It goes towards zero, the famous ""absolute zero"", the temperature at which ""jiggling"" is at its absolute minimum (! But not zero jiggling , see [""zero point energy""](http://en.wikipedia.org/wiki/Zero-point_energy)). Absolute zero is -273.15 Celsisus, -459.67 Fahrenheit and of course, 0 Kelvin. 

As for why life isn't very fond of negative temperature, one place to look at is of course the freezing of water. We are mostly made of water, so freezing isn't very desirable. At atmospheric pressure, water freezes at 0C, 32F or 273.15K. Below those, things get trickier (but not impossible, plenty of life forms have evolved to live in freezing weather). 

There are other places to look to for reasons why life on earth doesn't strive in very cold weather , chemistry kinetics for exemple. But we can get to that if you have follow up questions. ",null,1,cdkjjky,1r7i1w,askscience,new,9
spookyjeff,"Besides what others have mentioned here, at low temperatures proteins can undergo ""cold denaturaiton."" Hydrogen bonding and the hydrophobic effect are dependent on temperature, they are also responsible for the shapes of macromolecules in the body. At low (and high) temperatures these molecules can lose their shape and function.",null,0,cdkrgt2,1r7i1w,askscience,new,2
ramk13,"Freezing is a major (but not the only) problem. Life depends on the diffusion or active transport of chemicals across concentration gradient. For example, if there is more sugar outside a cell than inside, some can diffuse in. When you freeze the cell, mass transport grinds to a halt. Chemical reactions also slow down as temperature decreases, but not all at the same rate. Also, freezing water expands and can rip apart cellular structure. The combination of all those things disrupts most of the basic processes in a cell (or organism).",null,0,cdkogih,1r7i1w,askscience,new,1
Das_Mime,"Tidal locking means that the same face of the Moon always points toward Earth.

This occurs because tidal forces--the difference in gravitational tug from the Earth between different parts of the Moon-- cause small distortions and stretches in the shape of the Moon. These distortions dissipate the moon's rotational energy, slowing its rotation until eventually ~~it stops.~~ its rotational period is equal to its orbital period.",null,5,cdkfbfz,1r7ivu,askscience,new,16
MayContainNugat,"It feels cold precisely *because* it conducts heat so well. If the metal is at 20C, and your body temperature is 37C, then the metal will very efficiently conduct heat away from your body, and that feels cold. It does this much more efficiently than wood or plastic. ",null,9,cdkejkr,1r7l11,askscience,new,92
ww-shen,"Metals generally are good conductors, but they are good capacitors aswell. So they can 'take' many heat 'fast' from your hand. The cooling process will last as long as the temperature of the  metal is equal with your hand's. The metals inner temperature will be almost equal (becouse of the good coductivity) and will last 'long' to warm it (becouse of the capacitivy). Ceramics for example are good capacitors and bad conductors. It takes longer time to warm up, but they will radiate the heat longer. (coclkle stove). Plastics and air (and vacuum) are bad in both, so they can be used in heat insulations.",null,12,cdket3p,1r7l11,askscience,new,2
rupert1920,"Check out [pair production](http://en.wikipedia.org/wiki/Pair_production), where a particle and its antiparticle can be produced from a photon. This is essentially the reverse of [annihilation](http://en.wikipedia.org/wiki/Annihilation).

I'll also add that ""energy"" used to create ""mass"" is a bit of a an awkward wording, as both of these are properties of a system. It's akin to saying can ""height"" be used to create ""width"". In the example above, it's a case of particles with no rest mass creating particles with rest mass.",null,27,cdkftg9,1r7mfl,askscience,new,144
50bmg,"Absolutely. If you add enough energy to a particle, and collide it with another, you can create new particles. This happens in particle accelerators all the time, and it is the method by which we create and study antimatter (usually by bouncing protons off iridium and creating antiprotons). However, it does take a LOT of energy to do, which is why antimatter is the most expensive material (per gram) every created. ",null,8,cdkg0ub,1r7mfl,askscience,new,35
technogeeky,"I'm surprised to not see a direct and relevant answer.

**Yes** and this is exactly what high energy accelerators do. Really. All of them. Stanford's Linear Collider. The LEP. The LHC. The Tevatron (before it was closed).

You could ask yourself: If the [Large Electron-Positron Collider](http://en.wikipedia.org/wiki/Large_Electron%E2%80%93Positron_Collider) collided electrons and positrons (which weigh 1/1836 as much as a proton), how could that machine have possibly seen the W and Z bosons (which weigh ~80 and ~90 times more than the proton)? Combining the two, where did this extra ~150,000 electron masses come from? There isn't anywhere near enough mass to create these particles in the collision!

The point of particle colliders is to provide the particles with enough extra energy that they can produce interesting particles as outputs. Extra might be understating it a bit: the vast majority of the energy of colliding particles in these machines is the kinetic (motion) energy of the particles, and not the rest mass of the particles.

In other words, if a lot of energy **could not** be used to create mass, then particle physics would not exist at all.",null,3,cdkm8h5,1r7mfl,askscience,new,14
yinz_n-at,"Yep they're equivalent. But in the nuclear world, energy is very big and mass is very small (from our point of view). This is why nuclear power works (convert a small amount of mass into equivalent energy). So going in reverse will take a tremendous amount of energy but is still possible (particle accelerators). 

Look at Binding Energy per nucleon for the ranges of fission and fusion. About iron is the most stable nucleus 

Source: I'm a Nuclear Eng. ",null,2,cdkkfnm,1r7mfl,askscience,new,7
QCD-uctdsb,"In any experiment used to determine mass, pure energy (photons/gluons) is actually indistinguishable from matter. Imagine you have a mirrored box full of photons, and let's say that the total energy of all these photons is 9x10^17 joules. If you measure the mass of this box (say you put it on a scale) then from your mass-energy equation the box will have a mass of 10 kg. 

As another example, up and down quarks have a mass of roughly 3 MeV. You have 3 of these in a proton, so you'd guess that the proton would have a mass of 9 MeV. But nope!.. the measured mass is actually 938 MeV. This is because protons have extra ""binding"" energy (mostly in the form of gluons) which contribute to the total proton mass.

To answer your question then... yes, it happens all the time. If you wanted to create *matter*, well that's a different question. You would need some mechanism (pair production) and experimental setup (particle accelerators) for creating quarks and leptons from photons and gluons.",null,0,cdkli31,1r7mfl,askscience,new,1
JonathanFeinberg,"Indeed yes. A fast thing has more mass than a slow thing. A compressed spring has more mass than a relaxed spring. Of course, under ordinary circumstances, this differences are vanishingly small. But, as other answers have pointed out, you can actually create new particles from the kinetic energy of a collision of existing particles.",null,0,cdkms66,1r7mfl,askscience,new,1
_ridden,"To put it shortly, yes. The addition of energy into a system can be used to create mass, this is why in the Large Hadron Collider (LHC @ CERN) particles are sped up (thus giving them energy) and colliding two particles together, creating new particles which may overall have a greater mass then before.",null,0,cdkpi9s,1r7mfl,askscience,new,1
roh8880,"You're talking about matter to energy/energy to matter transference. 

The issue that has arisen during this research is in order to construct any material object is that you have to build each molecule and have them stabilize. It's the stabilization that is the problem, if I recall the article I read correctly.",null,0,cdkydl1,1r7mfl,askscience,new,1
jakkes12,This is what happens in CERN. As the particle that's being accelerated it gains energy.The energy is transformed into kinetic energy until it reaches a speed close to the speed of light. Nothing can go faster than light therefore the energy added cannot be transformed into kinetic energy at this point. Instead the energy is transformed into mass. The particle being accelerated will start growing.,null,0,cdl5a12,1r7mfl,askscience,new,1
null,null,null,16,cdkhys9,1r7mfl,askscience,new,6
TehMulbnief,"Right up my alley! I worked at a food science company for a few months earlier this year and wouldn't you know it, we were developing caramel colorings!

I'm not too sure why ammonia would be added, or I should say, we never used ammonia in any of our colorings, but sulfites are used as a preservative. In fact, if you punch sodium sulfite into [wikipedia](http://en.wikipedia.org/wiki/Sodium_sulfite), one of the uses listed is a preservative.

Since sodium sulfite isn't exactly tasteless, there is a lot of pressure from consumers to eliminate its use in certain foodstuffs. Also, with the ever growing presence of chemophobia regarding food, consumers want less stuff added. However, sodium sulfite doesn't just limit bacteria growth.

The caramel coloring I was developing was actually based on a particular strain of corn starch. To extract the starch completely from the raw kernels, the corn was soaked in a dilute solution of sodium sulfite overnight. This helped loosen the intermolecular connections between the starch and cellulose present in each kernel. Higher yields of starch means more coloring which means lower prices for our buyers and their customers.

Let me know if you have any followup questions!",null,0,cdmxea3,1r7o3s,askscience,new,2
ArmyOfFluoride,"This is a really big question you're asking, but I'll try to give a bit of a foundation for you.   If you've ever taken a class in the life sciences you've likely heard the phrase ""structure determines function"".  I first learned this in the context of proteins: the amino acid structure of a protein determines its biochemical function.  This is also true for tissues however.  The way cells are organized in your heart is what allows it to work as the core of your circulatory system.  Your brain is no different.  The structure of the tissue in your brain is (as far as we know) what determines its functioning.    You also likely know that the structure of your body is determined by both your environment and your genetics.  ""Instinct"" then, can be thought of as the behavior that arises from the brain structure that can be attributed to genetics, as opposed to the environment.  This is where things get messy, as ascribing any trait as complex as behavior to entirely a genetic or environmental cause is impossible, because we all have both environmental and genetic histories.  Does that help?",null,3,cdklzjv,1r7q41,askscience,new,18
bags_of_geckos,"
I think it’s easier to use the term ‘innate behavior’ rather than ‘instincts’ because the latter has a lot of more metaphysical connotations in humans, at least. I can describe an innate behavior in a simpler organism, like a fly. 

Fly mating behavior is what we call ‘stereotyped’- given certain stimuli (like a young female fly or something that smells like one) a male fly will initiate a series of behaviors that are pretty much the same every time: a little wing dance, an attempt to mount her, etc. The neurons that sense the odor of the female fly connect to a few parts of the fly brain, including an area called the lateral horn. It’s a little oversimplified, but neurons in that region connect with downstream neurons that control the fly’s muscle movements that cause them to do the dance, and more complex series of movements that include mounting. It’s a circuit of neurons that starts with a sensory stimulus and ends in a behavior, with a little processing in the middle. 

So where to genes come in? Genes are what tell the brain how to structure itself in the first place. In each neuron there are a series of genes transcribed that tell the neuron how to develop, what kind of neuron to be, where to migrate to in the brain during development. The neurons in the lateral horn that bridge the gap between the smell of a female (sensory stimulus) and the mating (behavior) are where they are, and fire when they fire, because of the history of genetic switches that got them there. 

I should add, there is no one gene for this behavior or that, every single cell is the product of thousands of genes turned on or off throughout the organism’s development. Cells (such as neurons) are the sum of their genetic history, and behaviors are the sum of the neural activity that was elicited by a stimulus. 

You can check out the work of [Vanessa Ruta](http://www.rockefeller.edu/research/faculty/labheads/VanessaRuta/), for a more in-depth explanation. I am probably butchering my description her work but you might find more of what you are looking for there.

Most of human behavior is learned, though we can suck as infants and grasp things, most of what we do during the day we had to learn at some point. We don’t have a specific brain structure devoted to drinking coffee that is the same for all humans. Less complex organisms tend to have a much greater percentage of their brains devoted to innate behaviors than we do.




",null,2,cdknup8,1r7q41,askscience,new,15
zzerrp,"Yeah this is a pretty huge field of research, involving people working on a variety of brain circuits and the genetics that underlie their function.  A couple examples: Hopi Hoekstra studies how genetics affect the shapes of the burrows that mice decide to build (simplification; see her [lab website](http://www.oeb.harvard.edu/faculty/hoekstra/projects_behavioral_genetics.html) ).  And all the people who study [FOXP2](http://en.wikipedia.org/wiki/FOXP2) and its associated gene regulatory networks, which are involved in innate ability to acquire language and vocalize.  It's a very cool field of research, but it's tricky.  One big reason is that there is a lot more to the story than just the sequence of bases in the DNA.  There are [some 250,000 exons and a similar number of introns](http://www.ncbi.nlm.nih.gov/pubmed/15217358) known in the human genome. That gives a rough idea of how many distinct genes our bodies have to work with -- half a million.  Compare that with the over 100 BILLION (prob closer to a trillion) neurons that are in the human brain.  There isn't anywhere even close to enough genes to specify the properties of each neuron individually.  So it's all about how the body uses the products of these genes in combinations, and the program of development that is set up by those combinations, such that a functional network of neurons grows.  Innate behaviors are driven largely by the particular forms of the connections that have a tendency to keep arising out of that biological program of development.  ",null,0,cdkwlqa,1r7q41,askscience,new,2
null,null,null,9,cdkmeh4,1r7q41,askscience,new,5
Cherrysquid,"The ball will land back in the boat if the boat is moving at a constant velocity. When you throw the ball straight up the ball not only has your vertical velocity that you gave it with your hand, but also the same horizontal velocity the boat has. The only acceleration acting on the ball is vertical, gravitational acceleration. With no horizontal acceleration the ball will maintain the same horizontal velocity (velocity of the boat) until it lands.",null,0,cdkg2oq,1r7qcs,askscience,new,13
AleccMG,"Start by considering that you, the ball, and the boat are all moving at the same velocity (speed and direction).  In the frame of reference of the boat, your velocity is zero (as is the ball before you throw it).

When you throw the ball straight into the air, you give it an initial vertical velocity.  With respect to the boat, it has no horizontal velocity.  The ball's motion is now entirely determined by the initial conditions (how you threw it), and the forces acting on the ball.  Since we are neglecting air resistance, the only force remaining is gravity which acts towards the boat.

Since we have no force in the horizontal direction, and your throw imparted no horizontal velocity, the ball will not move horizontally from the perspective of you on the boat.  If you don't catch the ball, you'll likely end up with a good knot on your head!",null,0,cdkhifr,1r7qcs,askscience,new,3
snusmumrikan,"Realistically there is no difference, which the anti-GMO lobby refuses to admit. The development of hardier and more productive crop strains has been done for thousands of years through crossing plants and hybridisation, along with the development of larger load-bearing horses and farm animals.

Our ability to do it from an informed position and alter specific genes through 'genetic engineering' only makes it more precise and targeted and less likely to have the Brassicoraphanus problems that Karpechenko had in the 1920s; where he tried to cross radishes and cabbages to get to root of a raddish and the head of a cabbage. Unfortunately he got the useless root of a cabbage and the useless head of a raddish in one plant.

For you paper are you considering talking about Armand Pusztai and his flawed release of information regarding GMO potatoes poisoning rats? - refuted endlessly by reliable peer-reviewed studies and yet still one of the major reasons GMOs have a bad name. Also Prince Charles and his irrational vehement fight against GMO without any expertise in the area itself?",null,0,cdkks93,1r7qtk,askscience,new,2
ToThink,What Borlaug essentially did was he maximized the growing efficiency of wheat as well as increasing the number of disease resistance genes in wheat to increase overall crop efficiency of wheat. He did this on the macro scale by breeding several varieties of wheat together and selected for the best phenotype. An example would be how he bred wheat which were more likely to resist cold temperatures together for many generations. He kept doing this with other environmental conditions until the wheat became very resistant to any extreme environmental conditions altogether. He did genetic engineering on the macro scale by actively selecting favourable traits (selecting for the best trait by sexual selection) over several generations of wheat.,null,0,cdkko4q,1r7qtk,askscience,new,1
Surf_Science,"So an antidote can vary widely but I'm not aware of any 'traditional' antidotes that might think of from the movies. 

In the case of venoms we often use antibodies generated by exposing animals to the venom. These antibodies bind the venom, may prevent it from acting and may help our body remove it (antibody opsonization). 

In the same way an antibody will bind a dangerous substance we will sometimes use chelators to bind other molecules that may be problems (heavy metals, iron). 

In some other cases the antidote may be using something that competes with the dangerous substance for a receptor. In the case of ethylene glycol poisoning for example (and this is what happens when a dog for example drinks antifreeze) ethanol can be used as a treatment. Ethanol will compete for the target receptor with the ethylene glycol preventing the action of ethylene glycol (ethanol has a higher affinity for the receptor).  ",null,0,cdkjlta,1r7rep,askscience,new,3
alex199119,"I guess there are two main ways that drugs (which could be an antidote) are discovered. 

It's possible to test a large number of chemicals in a number of different systems and see what effect they have and if any of those effects are positive or beneficial. So upon testing a certain chemical, you may find that the chemical in question works as an antidote for a certain chemical poisoning. 

Or it's possible to look biologically at the effect a poision has on a body, and then try to 'engineer' a solution to combat that poison. So I suppose in someways you could say the latter method derives the antidote from the posion, but not from the actual presence of the poison but from the way the poison has an effect on the body and an understanding of this.",null,0,cdktj6s,1r7rep,askscience,new,1
Whisket,"Scientist discovered the [chemical reaction series](http://www.google.com/imgres?imgurl=http://www.theozonehole.com/images/ozoned43.jpg&amp;imgrefurl=http://www.theozonehole.com/ozonedestruction.htm&amp;h=438&amp;w=580&amp;sz=29&amp;tbnid=2B5I5wnJEI6IvM:&amp;tbnh=90&amp;tbnw=119&amp;zoom=1&amp;usg=__VSGqa4LvGkOQioh7a5z697w7CNA=&amp;docid=n7L0JaEnpXluyM&amp;sa=X&amp;ei=5pCPUsSTKKW42wXn0IA4&amp;sqi=2&amp;ved=0CDQQ9QEwBA) where chlorine acted as a catalyst for ozone depletion.

CFC stands for chloro-fluoro carbons. For those without O-chem knowledge,  carbon atoms can bond with up to 4 other atoms. In CFC's, there are at least one chlorine and one flourine atoms bonded to the central carbon atom, with the rest being hydrogen atoms (there are multiple variations based on numbers and positions of these atoms). When these CFC's get into the atmosphere, they start to break down due to UV radiation, and chlorine atoms are released. These chlorine atoms then proceed to catalyse the chemical reaction series I linked earlier.

[Here's a link from the US EPA with more detail](http://www.epa.gov/ozone/science/process.html)

A quote from the article: ""It is estimated that one chlorine atom can destroy over 100,000 ozone molecules before it is removed from the stratosphere """,null,0,cdkk6rk,1r7s9w,askscience,new,3
richard_woodhouse,"It's a bit of a long process but there were 4 major breakthroughs associated with the discovery of the Ozone Hole:

1.)  **The Fate of CFCs:**  Mario Molina was a postdoc working with Professor Sherwood Rowland at UC Riverside in the 70's.  His project was to determine the fate of CFCs in the atmosphere.  CFCs were used as refrigerants for a lot of industrial processes because they were incredibly stable.  They happen to be so stable that nothing in the troposphere can break them down, so no one really knew what happened to them when they were released.  Molina eventually discovered that the CFCs would finally break down high in the stratosphere (~20-40km) when they absorb high-energy UV light (~175-220nm).  This releases Cl radicals into the stratosphere.  Their famous Science paper [here](http://www.nature.com/nature/journal/v249/n5460/abs/249810a0.html) (with a great typo in the title, see the [pdf](http://www.nature.com/nature/journal/v249/n5460/pdf/249810a0.pdf)) presented this initial warning that CFCs could deplete ozone.  However, this reaction needed observational evidence now.

2.) **Observations of enhanced ClO:**  In 1976 James Anderson of Harvard lead a field campaign to the south pole and made in situ observations of enhanced ClO.  This confirmed that chlorine was present in the stratosphere and verified Molina's proposed CFC fate.

3.) **Observations of the Ozone Hole:**  This one's kinda funny in hindsight, in 1985 Joe Farman published a paper in Nature ([here](http://www.nature.com/nature/journal/v315/n6016/abs/315207a0.html)) showing dramatic ozone loss in the austral spring.  This was in direct contrast to satellite observations over the south pole at the time.  It turns out that the satellite data was being filtered out because concentrations were so low.  Essentially, the scientists had assumed the satellite measurements must have been noisy and were throwing out data that was ""unrealistic""!

4.) **Clouds and Crazy Chemistry:**  The original chemical reaction chain that Molina proposed was not the cause of the ozone hole (the source of chlorine was right but the catalytic reaction mechanism was not).  The ozone depletion that Farman noticed occurred in the spring at first light in the south pole.  There's three reasons for this depletion:  (1) the polar vortex keeps the south pole somewhat isolated from the surrounding regions, (2) typically the chlorine can be ""locked up"" in reservoir species such as chlorine-nitrate, however Susan Soloman discovered that [polar stratospheric clouds](http://en.wikipedia.org/wiki/Polar_stratospheric_cloud) can form due to the extremely cold temperatures and provide a surface for heterogeneous chemistry to occur.  The chlorine nitrate can react with HCl to yield Cl_2 + HNO_3 and the HNO_3 will dry deposit over the winter.  So in the spring there's a lot of Cl_2 that will photolyze and no nitrogen to lock it up.  (3)  In 1986 [Luisa and Mario Molina discovered](http://pubs.acs.org/doi/abs/10.1021/j100286a035) that ClO can self react: ClO+ClO -&gt; ClOOCl and that the dimer will break at the Cl-O bond, not the weaker O-O bond.  This means that ClO self reactions can produce radical chlorine and deplete ozone without radical oxygen atoms!  

In summary, Mario Molina proposed that CFCs could deplete ozone in the stratosphere.  His original paper correctly pointed out the source of chlorine in the stratosphere but didn't identify the actual reaction that would cause the ozone hole.  2 years later Jim Anderson then verified that enhanced chlorine was present in the stratosphere.  10 years later Joe Farman noticed the ozone hole.  3 years later Susan Soloman, Luisa Molina, and Mario Molina discovered the crazy set of chemical reactions that lead to the ozone hole.

Edit:  Added a note about the typo in the Science paper title.",null,0,cdm3m9e,1r7s9w,askscience,new,3
Son_of_Thomas,"The different blood types refer to the different types of antigens on the surface of our red blood cells. There are 3 main ones: A, B, and D. A and B determine the type of blood you have (A, B, AB, or O type) and D determines whether your blood is positive or negative- the presence of the D antigen (also known as the Rh factor mentioned in another comment) means you are positive. For example, having the antigens A and D make your blood A+. If you have A and B but no D, your blood is AB-. Having only the D antigen without A or B means you have O+ blood, and a lack of all 3 antigens means you have O- blood. 

The reason this is a problem during blood transfusions is because if you have a certain antigen, it means you have an antibody for the other antigens you dont have. For example, if you have type A+ blood, meaning you have the A and D antigens, you have a B antibody. If your A+ blood were to come into contact with a B type blood, the B antibodies in your blood would react with the B antigens in the B blood, causing the B blood to coagulate and virtually be unusable. 

This is why type O- is the ""universal donor"", because it has no antigens for anybody else's antibodies to react with. This is also why AB+ is the ""universal acceptor"" because it has no antibodies to coagulate any type of donor blood. 

also, I do not know *why* we have different types. 

EDIT: clarification

EDIT 2: gold?! success! thank you, kind stranger. ",null,541,cdkkjeh,1r7v5g,askscience,new,2024
Yes_That_Guy,"Have had this saved after answering this question a few times.  Much better summary than I could write up with the time I have this morning.  Edited for content.  Credit to /u/Its_the_bees_knees

**If you have any other specific questions after reading this please let me know.  I feel this should satisfy most if not all of your questions**

**Antigen**- The tag that identifies something. These antigens exist, to help our body distinguish molecules as ""self"" or ""non-self."" Our immune system will recognize these antigens and react, or wont react. When there is an antigen-antibody reaction that is what causes the destruction of these cells and a massive clot forms.

*Edited expansion*- In my analogy. The antigens are the Locks. And the Antibodies are the keys. When your body encounters a Lock that doesnt belong to you, it tries to destroy it (open it) Once your body encounters this lock, it has now seen it and has now seen its design. After encountering a foreign lock, your body will now make keys to open and destroy these locks. Some of these locks are small (ABO blood group) and the keys can be made immediately and the reaction occurs immediately. However the locks of Blood type of +/- (explained more detailed below) are relatively big and can take a while for the appropriate keys to be made. So you constantly have these keys floating around in your blood constantly looking for a compatible lock. When a key matches a lock, that's what activates your immune response, destroys the lock and in turn causes all the negative effects of a transfusion reaction (explained more below)

**Why are there different types of antigens?** This boils down to their molecular structure. All blood types have the same ""core"" structure, but what differentiates the types is a Polysaccharide molecule that sticks out from the molecule. This polysaccharide (or Sugar in simpler terms) is what is the antigen and helps our body identify them as different blood types.


**Antibodies**- The main part of our immune system. Antigen-antibody interactions follow a lock (antigen) and key (antibody) When the key fits the lock that's what causes and immune reaction which ends up destroying the Red Blood Cell (RBC)

The locks are pre-determined due to the sugar ""tag."" Now one thing people dont realize is that certain blood types will have the specific antigen, but WILL HAVE THE OPPOSITE ANTIBODY.


**What does that mean?**

*Blood Type A*: Has A Antigen, and B Antibody

*Blood Type B*: Has B Antigen, and A Antibody

*Blood Type AB*: Has A and B antigens, No antibodies

*Blood Type O*: Doesn't have A and B, Has both A and B antibodies 

*edit*- (as pointed out by nearquincy below, Blood Type O actually contains H antigen, which is the precursor antigen to both antigen A and antigen B.
There are other antigens that is present on the surface of the blood type O itself although they are not significant in ABO system.""






**What does that translate into the real world?**

*Blood Type A*: They only have 1 antibody-Type B, so if that antibody finds its partner antigen (found in Type B or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type B*: They only have 1 antibody-Type A, so if that antibody finds its partner antigen (found in Type A or type AB transfusion) then it will cause a reaction and essentially self-destruct.

*Blood Type AB*: Has both antigens, but no Antibodies. (Because if it had any antibodies to either A or B, the person wouldn't exist in the first place, because the Antibodies would keep finding their Locks, and self-destructing the RBC's) Because blood type AB has no Antibodies, they are known as **Universal Recipients** Meaning that because there are no keys, there will never be keys to correspond with the locks.

*Blood type O*: Has no antigens, but does have both types of antibodies. Because of the lack of antigens, they are known as **universal donors** meaning their blood can be transfused/mixed with any other blood type and there will not be a reaction.





**Below an even simpler version of what this means**

*Type A blood*- Can Receive type A or Type O Blood. Will react with type B or type AB.

*Type B blood*- Can receive Type B or Type O Blood. Will react with type A or type AB.

*Type AB blood*- Can receive all types of blood- Type A, B, AB, or O. Will not react with any.

*Type O blood*- Can only receive type O blood. But can be donated to any blood type person. Will react with types A, AB, and B (when type O is the type of the receipient)

*edit*- cheat sheet for blood types http://i.imgur.com/fTw8AIj.png




**What is Positive and Negative Blood?**

Well this refers to the presence or the lack of another type of antigen, known as the Rh Factor or also known as the D antigen.



**Why is being positive or negative important?**

Well, this is most important for pregnancies. Specifically a mother's **second pregnancy**. It occurs in a Rh(-) Mother and an Rh(+)Baby. What does being Rh (-) or (+) mean? Well like I said it depends on whether or not you have the Antigen or you don't. During the 1st pregnancy the fetal blood will mix with the mothers blood. Because the mother has no Rh factor, but the fetal blood does- the immune system will recognize these particles as foreign and will in turn start making antibodies to it. Anything your immune system recognizes as foreign, it assumes its an enemy and will start to attack it.



**Why did you mention 2nd pregnancy and not first?**

Well these antibodies belong to a different class than the ones that react with blood typing. These antibodies take MUCH longer to form. Hence it wont affect the first pregnancy, but can affect the second pregnancy.


**Also, can a A- receive A+ blood?**

A+ can receive blood from a A- Negative donor, or A+ donor.

A- can only receive blood from A-.

Its the antigens that make up the difference. Negative means you have no antigen, so even if - donor blood present inside of a + person's body, the body doesn't recognize it as foreign, so its okay.

The problem exists when - person receives + blood. + Blood does contain the antigen, so when - Persons body sees the + blood, it sees the antigen, recognizes it as foreign and attacks it. So this blood transfusion is incompatible


**What happens when you have an incompatible blood type?** 

You get-

Fever

Jaundice

Decreased blood pressure

Increased heart rate

Increased breathing rate

Acute kidney failure (if severe enough)

Blood in urine (after it reaches the kidney failure stage)

Shock ( if severe, and untreated)

Death ( if severe and untreated due to the sudden loss of blood pressure and the shock)

Basically I described it as self destruct, because the host antibodies attack the foreign antigens and cause these red blood cells to be destroyed. When your red blood cells are destroyed they release a molecule know as hemoglobin. Hemoglobin is normally a good molecule which is responsible for oxygen transport, but that is only of its attached to/inside of the Red blood cell. When there is too much free floating hemoglobin in the blood- that is what causes all of the above symptoms.


**Can transfusion reactions be treated? And is it symptomatic treatment or does it require another full transfusion?**
Yeah it's basically sympomatic treatment.

But remember, that symptoms that appear are really variable and dependent on how severe the reaction is. 
But for a complete overall treatment of the above symptoms you would require-

Treating and anticipating the Shock (Which would also treat the low blood pressure and fast heart rate

Treating the kidney failure (Dialysis to filter out the hemoglobin) (If the reaction is severe enough, it may require a complete re-transfusion)

Treat the subsequent clot that will form. (When there is a transfusion reaction there will also be a massive blood clot that forms due to all of the now destroyed Red Blood cells)

**Certain blood types have shown greater incidence with certain diseases/infections. But there is no increased susceptibility**

Blood Type A: Hepatitis, Small Pox

Blood Type O: Black Plague, other digestive system infections, Autoimmune disorders(when your body attacks itself)

Blood Types A, B, AB: Clots in your veins


**Also some other interesting things to note is that-**

ABO (+/-) Blood typing will fit and categorize about 99.97% of the worlds population. But there is still that 0.03% who's blood has neither A/B Antigens nor the A/B Antibodies.

For these special people we have to to EXTREMELY careful when transfusing any type of blood, also we have to use other systems to type/categorize their blood

When a Blood Type O patient, receives blood from a blood type A patient. This causes the most severe of the transfusion reactions.

About 40% of the population is Type O. Type AB is the rarest.

About 80% of the population is +. So being AB- is the rarest of blood types


**Are there any benefits to having either A, B, AB, or O blood types?**

Type O can donate to all types of blood.

Type AB can receive all types of blood.

Types- A, AB, B dont have a distinct advantage over the other. *edit*- as pointed out by /u/hammurarbisan "" both A and B types routinely have elevated platelet counts versus the other blood types - most especially female A types - and thus more frequently tested for platelet counts and targeted for apheresis collection. Platelets are commonly used for transfusion during surgery.""

Being + however is an advantage over being negative. + Can receive from + or - people. Negative people however can only receive from Negative people.

*edit*- Type AB can donate PLASMA to all people, since they dont have any antibodies (the donation rules for plasma and whole blood are opposite in terms of antigens and antibodies)
",null,23,cdkmrvv,1r7v5g,askscience,new,143
swamp14,"A lot of people have been asking about the ""why"" and rightly so because it was asked in the OP but not yet answered. 

WHY humans have different blood types is akin to why some humans have more or less of a specific protein that helps break down alcohol, why some humans tend to be taller or shorter than others, why some humans have different proportions in arm/leg length... etc. It's simply genetic variation. Different genes being expressed because of genetic diversity in the human species, amplified by sexual reproduction.

To all the people asking what evolutionary advantage different blood types offer - just because a species has a particular trait doesn't mean that trait helps the species survive and reproduce more than if the species lacked that trait. Traits can be passed on simply because of luck or a number of other factors. Evolution works because traits that tend to be favorable for survival and reproduction tend to get passed on and thus become more prevalent in the species. But that doesn't mean disadvantageous traits don't get passed on. They're just less likely.

""Isn't it more evolutionarily advantageous to have a single blood type?""
Perhaps. I can't give a straight answer because I don't know. But this question indicates a misunderstanding of evolution. It's like asking ""Isn't it more more evolutionarily advantageous for humans to be stronger, faster, and smarter?"" So, even if the answer is yes, it doesn't mean it should be true. 

What this tells us is that evolution is not perfect. It is the process that results from random mutations and natural selection over long periods of time. We have different types of blood because of random mutations and those blood types got passed on. Whether or not those different blood types contributed to our survival and reproduction, I cannot tell you.

But what I can tell you is: 1) traits are not perfect, and asking why they are not ""better"" usually demonstrates a misunderstanding of evolution; and 2) traits do not necessarily need to contribute to survival and reproduction in order to be passed on, and assuming that they MUST be is not reasonable. Yes, traits usually are advantageous because if they weren't advantageous, they'd be less likely to be passed on. But again, this does not mean they absolutely need to be.

Edit - Thanks so much for the gold~

",null,15,cdkkr1g,1r7v5g,askscience,new,79
cracked_chemist,"In terms of evolution:

Reading a bit the article ""ABO blood group glycans modulate sialic acid recognition on erythrocytes"" (Cohen, M, et al. Blood, 2009), they suggest that the different blood type ""antigens found on human erythrocytes modulate the specific interactions of 3 sialic acid-recognizing proteins...  with sialylated glycans on the same cell surface."" These antigens ""modulate sialic acid-mediated interaction of pathogens such as Plasmodium falciparum malarial parasite."" Thus the different blood types may affect the host-pathogen interaction. To be fair hematology is not my field, but their model seems plausible. ",null,17,cdkmgly,1r7v5g,askscience,new,81
Andrenator,"And [an interesting thing](http://andrenator.tumblr.com/post/66202105636/detenebrate-0xymoronic-shitarianasays) about the B antigen (and a little of the A antigen),

is that during the black plague, the bacteria mimicked the B antigen, so if people had B type blood, they would have virtually no defense against it.  A type was a little better, people could recover from it.  Type O blood developed in small villages where accidental marrying of distant cousins happened, and O blood reacted violently to the plague.

So that's why B blood is so rare now, A is a little more common, and O type blood is so common.",null,7,cdknftz,1r7v5g,askscience,new,30
mobilehypo,"Interestingly, some blood types do give resistance to disease. The lack of the Duffy antigen in the majority of Black Africans has been shown to give resistance to two species of malarial parasites. This blood group system has been studied closely and we are finding that specific combinations of this antigen on the surface of red blood cells might have impacts on other diseases. [This part of the Duffy article](http://en.wikipedia.org/wiki/Duffy_antigen_system#Clinical_significance) on Wikipedia gives a rundown of what we have found so far.

This question has come up before, here's a set of search results that might help further:

* http://www.reddit.com/r/askscience/search?q=blood+type&amp;restrict_sr=on&amp;sort=top&amp;t=all

Here are some posts that address some whys:

* http://www.reddit.com/r/askscience/comments/iepiv/why_did_we_evolve_with_different_blood_types/

* http://www.reddit.com/r/askscience/comments/f19p4/how_or_why_did_blood_types_evolve_is_there_any/

",null,1,cdkjtyc,1r7v5g,askscience,new,26
Cabin_Sandwich,"And what connection, if any, does this have to ""eating for your blood type""?  I have a hippy friend who is always telling me I need to eat according to my blood type, how the blood types arouse from different groups of people who were eating different things and therefore I should eat what my ancestors ate.  I think it's nonsense.",null,6,cdkkrn4,1r7v5g,askscience,new,16
Chl0eeeeeee,"Differences in blood types arise from different protein expression on the red blood cell. So, first with the Rh factor (named so because it was first discovered in Rhesus monkeys). You can be Rh + or -, which means that your red blood cells have or don't have this certain protein on the surface. Problems can arise when a mother is Rh - and is carrying a child who is Rh+, as an immune reaction can occur (because the mothers body sees the Rh+ proteins as being foreign) if there is blood exchange between the two.  Usually the first child is fine, but when there's blood exchange in child birth, the mother builds up anti-Rh antibodies. To help with this, all Rh- mothers are treated with Rhogam to suppress that response. 

Now, on to the A/B/O blood type. Again, this is code for the type of proteins that are on the surface of the blood cell. The differences arise from genetic inheritance with one gene determining ABO inheritance. So, without making it too complicated... The presence of at least one ""A allele"" will cause the body to make specific glycoproteins, while the absence causes anti-A antibodies. The same goes for the B allele. If a person has one A and one B, then they are type AB. Because they are AB, they don't produce either antibodies. If they have two As, they are type A and likewise for B. In this gene, both the A and B genes are co-dominant. Type O arises when the person has neither an A allele nor a B allele, and instead has two recessive alleles. This person would not have any glycoproteins on the cell, and would produce anti-A and anti-B antibodies. This is why they are the universal donor (transfusion of blood won't set off an immune reaction, especially if they are O-). AB blood types are considered the universal receiver because they don't have the antibodies for any of the blood types, and thus can receive any blood. 

Hope this helps!",null,2,cdklo01,1r7v5g,askscience,new,11
Fazaman,"There are several great explainations in this thread as to what the different blood types are and how they interact with each other, but the question is *why* do we have these different types (just evolutionary/genetic differences?), are there any effects to having different types of blood (Say, A is more prone to getting disease X, or some such).",null,3,cdkq47v,1r7v5g,askscience,new,7
billyvnilly,"A,B,AB,O are designations for cells that express antigens.  D is also an antigen, and its significance is that it's reactive with antibodies as much as A and B.

We have evolved to develop these blood groups.  When Red Blood Cells (RBCs) are made they express a carbohydrate chain (millions) which is called H.  We have evolved over time to have different enzymes that modify this carbohydrate chain.  Most people believe B was the original enzyme.  A is speculated to happen afterwards.  So enzyme A or B act on the carbohydrate chain H, and modify it.  The modified chain is thus termed A or B.  If neither enzyme acted on the H chain, its termed O.  The enzyme A and B behave totally differently (enzyme activity and thus amount of H chains they convert), so if you have enzymes A and B, its not an equal split (thats a minor point, doesn't matter for this).  So these are just two examples of antigens on RBCs.  D is a third antigen that is clinically significant and very common, that is why people know about it.  What they don't know is D is a combo of many antigens.  D,C,c,E, and e.  but we always say if you're D+ then your blood is positive.  there is no d antigen.  
What most people don't know is that there are tons of antigens (100s) on red blood cells.  Some are there from the development of the RBCs' nucleus, and some are adsorbed from secreted antigens circulating in the blood stream.  
Why are all these important?  Well as your body makes antigens, it is also recognizing those antigens as ""self"" ... our bodies have the ability to then recognize ""not-self"".  So if you only have enzyme A, and not make any RBCs with B on their surface, you make antibodies to B, but not to A.  People with type A blood cant receive type B or AB blood because they have antibodies to B.  
Why do we have different blood types?  Well a good example is a blood group called Duffy.  The blood group has two antigens: Fya and Fyb.  And again these antigens come from two enzymes.  If you lack both enzymes, you lack the antigens.  And if you lack both antigens you're termed Fy(a-b-).  Why is this evolutionary?  Well because Plasmodium vivax (malaria) uses the antigen site to enter cells.  So if you lack the antigen, malaria cannot enter the cell (evolutionary!  This is why the majority of African-Americans are Fy(a-b-).",null,2,cdkm1ad,1r7v5g,askscience,new,9
kroxldyphivian,"Everyone is answering *what* the different blood types are. And while that's really informative and helpful, the original question in the post title (and the much more interesting topic imo) is *why* we have different blood types. Can anyone shed some light on this?",null,1,cdkobtd,1r7v5g,askscience,new,5
arumbar,"Blood antigen variation is thought to be a response to pathogens like viruses spreading from person to person and carrying some component of membrane protein with them, such that individuals who created an antibody response to foreign ABO markers had more success fighting off infections.  [wiki explanation here](http://en.wikipedia.org/wiki/ABO_blood_group_system#Origin_theories), [journal article here](http://www.ncbi.nlm.nih.gov/pubmed/15293861)",null,0,cdl3nvf,1r7v5g,askscience,new,5
Mumma_Sooz,"I've been lurking for a while but this has had me step out of the shadows.. I run a blood donor centre in Western Australia.  I encourage and applaud anyone who donates blood, be it whole blood, plasma or platelets.  People do it for so many reasons and in oz they get nothing tangible in return.  1 in 3 people will need a blood transfusion but only 1 in 30 donates.  If you have ever considered it but are hesitant, please don't be.  I can tell you from experience that it's nothing to worry about and the feeling of having contributed to saving lives is amazing.  ",null,0,cdl40ig,1r7v5g,askscience,new,4
pleasantliving,"I am in the Rare Donor Program and I never really understood what it means. All I know is I get a letter every few months making sure my address hasn't changed. On my donor card, my blood type says things like K:-1 and Jk(b-). Can anyone explain this to me? Why do some people need this rare blood? Does this mean I need rare blood? ",null,0,cdkzjdh,1r7v5g,askscience,new,4
OldMarmalade,"http://www.dnalc.org/view/15404-Chromosome-9-gene-for-blood-group-Matt-Ridley.html

AB - Very resistant to cholera

O - Susceptible to cholera

Among other things this may be one of the evolutionary pressures that explains variation in bloodtype. If you want to create the maximum number of ABs you'd have a population that was ~41% A and ~41% B to randomly generate ~18% AB supermen (in the time of cholera). This is indeed what you see in many populations http://www.blood.co.uk/images/content/pie-charts.jpg ",null,0,cdklqsz,1r7v5g,askscience,new,3
WasIsMitDenKohlen,"I haven't seen an answer to the first two questions. WHY are there different blood types. Why was there a need to have those distinct blood types. 

Would it make more sense evolutionary to have one blood type only, why bother preserving those distinct mutations, and everything that goes along with maintaining those? Or if not, why are there 3 distinct ones, and not 1000? What is the driving force to create 3 blood types? ",null,2,cdkpw6a,1r7v5g,askscience,new,4
redplate12,A clarification of the discussion of 'antigens' that represent the various groups. An O type does have an antigen structure but nobody has antibodies to it. The O antigen is a chain of 4 six carbon sugars. The A antigen has an added six carbon sugar (N-acetyl-galactosamine bound to second sugar in a 'branch) while the B antigen has an additional six carbon sugar (galactose) that is found bound to the fourth six carbon sugar core. An AB has both the A and B  five sugar chains present. Thus the difference is the presence of a single sugar moiety added to the core 4 sugar chain.,null,0,cdkswmc,1r7v5g,askscience,new,2
slakist,"I have no knowledge on the subject but have always been interested so this thread was great. I see some people have answered as to why we have different blood types, as well as explaining the difference between them. 

I would like to know if there is any disadvantage to having a specific blood type. For instance, I am O-; am I more susceptible to different kinds of diseases and disorders based on my blood type alone, more than a person with say, blood A? ",null,3,cdktzqz,1r7v5g,askscience,new,5
beliefinphilosophy,"Bonus factoid:

The different blood group antigens (or surface markers) are
Different kinds of sugars, (glucos, carbohydrate) and the +/- is a protein.





For example, the antigens of the ABO blood group are sugars. They are produced by a series of reactions in which enzymes catalyze the transfer of sugar units. A person's DNA determines the type of enzymes they have, and, therefore, the type of sugar antigens that end up on their red blood cells.
In contrast, the antigens of the Rh blood group are proteins. A person's DNA holds the information for producing the protein antigens. The RhD gene encodes the D antigen, which is a large protein on the red blood cell membrane. Some people have a version of the gene that does not produce D antigen, and therefore the RhD protein is absent from their red blood cells.

More [here](http://www.ncbi.nlm.nih.gov/books/NBK2264/)",null,0,cdkubis,1r7v5g,askscience,new,2
12and32,"I don't think the question of ""why we have different surface antigens"" has been answered. 

So why do we have differing surface and blood borne antigens? Are the genes that code for them related in any way, say by a few SNPs, or even just a single point mutation? I would think that they're related if we can predict heredity through simple Mendelian genetics, and because they do seem to have a high degree of compatibility (hemolytic newborn disease aside). There doesn't seem to be any particular reason for the Rh factor, nor does there seem to be a reason for antibodies that can only attack the A and B antigens.",null,0,cdkw1vb,1r7v5g,askscience,new,2
Jrj84105,"The different blood groups reflect differences in antigens present on the red blood cell surface. Because some of these antigenic variants confer resistance to infections by organisms such as malaria, certain blood groups are more common in different regions of the world. For the most part though, prior to the advent of blood transfusions, these differences were largely of no impact, essentially benign polymorphisms.

These kind of benign variations aren't just limited to our red cells but are present throughout our bodies. As we continue to challenge our bodies with new medical therapies, there turn out to be lots of previously irrelevant benign differences between people that now are seen as increasingly important factors in response to therapy and risk of side effects. ""Personalized medicine"" is largely an attempt to tailor therapy not just to a person's disease but also to the way in which any individual's response to therapies is a product of their unique set of variations.",null,0,cdl12gq,1r7v5g,askscience,new,2
MrGrow,"Our red cells consist of many different antigens (something that can trigger an immune response). Think of them like land mines. What people have can vary a ton (Kidd antigens, Duffy, Rh, Lewis, and so on). You simply make what you don't have. If I am A+ (very common) I am going to make the antibody to B. If you were to give me B blood, because I don't have that antigen on my own cells, my body will recognize that as foreign. What will more than likely happen is antibodies will coat the B blood cells, and when they are taken to the spleen, they will be ripped up and destroyed. This is called a transfusion reaction. There are different kinds of these and they each have there own symptoms. Why do humans have different types? Good question.     ",null,0,cdl68l5,1r7v5g,askscience,new,2
bigdaddymatty,"Whenever I donated blood the first time I became really interested in blood types and wondered this same question. I gave blood and didn't know my blood type before, but found out after that it was O- by looking on the donation companies website and logging in to see my donation session.   
I expect to receive a phone call each quarter now and every time I donate blood that ask me to do double red blood cells since I'm the universal donor! Giving blood is definitely worth it though",null,0,cdl6fqd,1r7v5g,askscience,new,2
laika84,"I checked comments and did not see this brought up as far as what the blood types ""actually mean.""  People have mentioned antigens and that these antigens are glycoproteins, which is correct.  However, I think it's also interesting/important to go a little further into how these different antigens are made and their significance, in terms of the ABO portion.

ABO bloods types are distinguished by the individual alleles that encode an enzyme.  The different enzymes have long names and essentially add different sugars via O-linked glycosylation to what could be a ""base"" sugar chain that is added first.  This ""base"" sugar chain exists on our RBCs and receives a fucose residue by fucosyl-transferase and having this enzyme alone constitutes the ""O"" phenotype/antigen.  The ""A"" and ""B"" enyzmes each add a different sugar to this base carb chain with the fucose and that creates the different antigens that we see.  You can think of it as the ""A"" enzyme adding the ""A"" sugar/antigen (N-acetylgalactosamine) and the ""B"" enzyme adding the ""B"" sugar/antigen (galactose).  It may seem trivial, but I thought it was interesting when I learned it and enhanced my understanding of what these antigens ""looked like"" on a biochemical level.

For a really cool tangent, do a search for the ""Bombay"" phenotype!",null,0,cdl02x8,1r7v5g,askscience,new,2
Shekho,"I don't remember where I heard this but i'll try to be nice, and simple....

Lets say a disease came along and wiped out all people with (x) blood type because they don't have a certain antibody that protects them from disease then other people with (y) blood type would live because they naturally can protect themselves..

If we all had the same blood type, wouldn't that mean we could easily be wiped out as a race from diseases? 

Please correct me if I'm wrong.",null,1,cdl33v2,1r7v5g,askscience,new,2
quiktom,"I was told that the blood types evolved over time. The O being the original and oldest when we ate anything we could find. The A type evolved when we got into agriculture and the B type when we started living in concentrated cities. Apparently AB is only 600 or so years old. AB blood can accept any blood before it (since those tpes were around when it evolved and essentially are a part of it) but no other type can accept AB, whereas at the opposite end O can be accepted by any but can accept none. All the carbons, sugars and whatnot are the details of how the evolution took place.

Apparently this affects what we should eat according to our blood types but I'm an O type and as much as I'd like to eat steak and nuts and raw veg it'd get really expensive without bread or pasta to fill me up.",null,3,cdl4ckm,1r7v5g,askscience,new,4
SparklePonyBoy,"I'm surprised that no one mentioned CMV, cytomegalovirus.  When blood is tested, spun, etc., they also test for this because it can kill immunocompromised individuals, also babies.  I know this because I used to donate blood every 8 weeks and they told me I am O- CMV- and that each donation can save up to 5 babies' lives.  Supposedly up to half of the general healthy population may be walking around with CMV and not even know.

http://www.cdc.gov/cmv/overview.html",null,0,cdl4r5d,1r7v5g,askscience,new,1
A_Soggy_Sheep,"This is real pseudoscience, but i would hazard a guess that it is beneficial for the human race to have different blood types for survival, incase some kind of disease was incredibly deadly to a specific blood type...

A bit like [diversify your bonds](http://www.youtube.com/watch?v=FTsNEUZx8v8) .

As i said earlier i may be totally wrong, so feel free to patronise me into oblivion. ",null,1,cdkkwwv,1r7v5g,askscience,new,2
Marsdreamer,"We breath oxygen today primarily because of organisms called cyanobacteria which created organic compounds from H2O and CO2; turning it into O2. Before this time there was no or very little atmospheric oxygen and it was actually toxic to most other organisms. This catapulted the Oxygen Revolution and is largely responsible for shaping the world as we know it today. 

Fast forward to your more pertinent question, which the answer is The Electron Transport Chain. You see, in order for our bodies to generate chemical energy in the form of ATP we metabolize organic compounds. Through some very lengthy and complex chemistry electrons are stripped from atoms and other molecules to generate an electrochemical gradient responsible for powering the generation and packaging of dense energy molecules known as ATP. 

During this process of ripping negative charges (Electrons) from organic compounds a build up of electrons takes place and they *must* go somewhere. 

Enter Oxygen. Oxygen loves electrons and it is *highly* electronegative. Because of this our bodies have adapted to harvest oxygen for the use of dumping the excess electrons generated from the Electron Transport Chain. Oxygen takes the electrons, binds with Carbon, and then is exhaled. 

*Gen Bio was a long time for me, but this is what I remember. I'm going to double check my sources now and would also appreciate other biologists to check my explanation as well.*",null,1,cdkjwie,1r7w97,askscience,new,16
Jetamors,"Marsdreams does a great job of answering the Why Oxygen question. You also asked Why Not Nitrogen: the answer is because most nitrogenases (enzymes that break down N2) are inhibited by oxygen; if there's any oxygen around, they won't work. If you're a bacterium or a plant with roots in the soil, you have areas where you can use those enzymes, but if you're an animal living in the air, there's no way to avoid oxygen, so nitrogenases aren't going to be useful for you.",null,1,cdklcvn,1r7w97,askscience,new,6
proule,"Many organisms actually use nitrogen as a terminal electron acceptor (essentially ""breathing"" it in the same way we breath oxygen). The difference is that these organisms are very small and can live on significantly less energy than higher multicellular organisms.

Oxygen is significantly more electronegative than nitrogen (especially considering nitrogen in the atmospheric form N2 is already satisfied with its electron configuration and won't accept more electrons). This difference in electronegativity results in a significant increase in energy liberated from giving oxygen electrons. 

You can think of giving oxygen electrons as a starting point and an end point (your electron donor, and your electron acceptor, which is oxygen). If you simply give oxygen the electron right away then you're losing a bunch of energy to heat. So, between the oxygen and the electron you essentially put up walls for the electron to push (walls being an analogy for cellular work). Oxygen is so powerfully attracting the electron that it pulls it in despite these walls, with the movement of the walls performing work in the cell.

Nitrogen is less electronegative than oxygen, meaning that nitrogen can't pull electrons through as many walls. Giving an electron to nitrogen just doesn't result in as much energy because it's not pulling on the electron as hard.

Higher organisms, mammals especially, have tremendous energy consumption. From a single glucose molecule, a fermenting organism like yeast will produce two molecules of ATP (the energy currency of any cell). The only way for complex multicellular organisms to evolve was to get more energy out of a single molecule of glucose. With oxygen and the electron transport chain we get more like 38 molecules of ATP. If multicellular organisms extracted energy from glucose as inefficiently as fermenting organisms, we'd have to (very simplified) basically consume 19 times more food to get the same energy.

Animals are all nitrogen dependent, but luckily, consuming other organisms yields plenty of nitrogen, because it's the base of amino acids that make up proteins. Trees fix nitrogen in their roots (with the help of symbiotic relationships to bacteria; they don't do this themselves) because they get their energy from the sun, and light obviously can't provide the tree with nitrogen.",null,1,cdkm17n,1r7w97,askscience,new,5
BizQuit,"3d printers that use DLP projectors are more expensive because they require a powerful projector to initiate polymerization of photosensitive resin. 

The more common hobbyist 3d printers are simply melting plastic and drawing lines with it requiring little expense as they are only slightly more than a hot glue gun. These common printers are very limited in just how fine a line they can draw, and they must create their work very slowly drawing out the entire parts layer.

With that understanding, It is easiest to think of a Dlp based 3d printer as a photographic process. Each layer is ""drawn"" not line by line but entire layer at once. An image of each layer is projected for a few seconds. Areas where pixels are dark remain fluid and uncured. Pixels that are illuminated cause a chemical reaction which causes solidification of the resin. The printer must then advance in only one direction UP or down depending on the design of the printer, allowing new resin to flow, and another layer can be ""exposed"". 

The tradeoff here comes in resolution and maximum part size. In the melt and deposit printers where you are limited by the fineness of the drawn line (minimum features of 200-400 microns at its best) you can, within the boundaries of the mechanical construction of your printer print parts of any size. The fineness of motor movement can make these parts seem quite smooth, but the ""line thickness"" can significantly limit small features.

Whereas the DLP printer has a fixed resolution in general no greater than 1920X1080.  If you fix your projector at a focus in a similar size to the line of an extrusion printer you only get to print (@200microns 15.1X8.5 inches) and you do not get the smoothing effect of a motor gliding along. BUT unlike these melt and draw printers, a DLP printer can attain VERY fine focus creating individual pixels 50 microns, 20 microns, a few have successfully managed as fine as 5 microns with off the shelf projectors. But each step down shrinks the total field printed. @20 microns you only get 1.5X.85 inches but the resulting part is amazingly detailed as a result. 

All that said, Photopolymer is more expensive than filament. The projector adds expense. And for many hobbyists the resolution is not worth the tradeoff in build size. If you are looking to make large objects it is not the right technology. If you are looking to make fine jewelry, intricate small scale models, dental work (yep labs are making dental crowns with these printers) right now the speed and resolution cannot be beat by any other hobbyist attainable technology. ",null,0,cdl6044,1r7wig,askscience,new,1
ozone_one,"Cell towers generally have a relatively low power signal (a few miles to tens of miles of range), and are meant to cover a specific area. When you get close to the border of a cell you are handed off to the next cell.

You would not be likely to receive a cell signal in a plane at 35,000 feet for a couple of reasons...  1) the vast majority of the country doesn't really have much cell service.  Look at a coverage map - all of those white areas have no coverage.  and 2) You are moving so fast in the plane that your phone is not able to lock on to a particular tower, and if it does lock on it will be unable to hand you off to the next tower.
",null,0,cdkk8fo,1r7xrc,askscience,new,8
800gpm,"Considering the radius of a bottle opening, the additional thickness is not a minuscule factor. I can't measure right now, but estimating the radius at 2cm and the thickness of a towel under pressure at 2mm, the moment of force would increase by 10% (assuming the same force). 

Another factor may be that the towel makes it less painful and thus lets you apply more force before reaching your pain limit.",null,0,cdkkmii,1r833i,askscience,new,3
shavera,"Okay so imagine you have the simplest circuit. Current flows around a simple loop of wire. Now, you cut the wire. The electrons are still in motion, so one end of the cut gets a little bit depleted of electrons, and the other one gets an electron pile-up. But this won't last too long, since the depleted end now has a positive charge, and the pile-up end is negatively charged, the electrons will be pulled in the opposite direction somewhat.

A simple analogy is like a circular pipe of water that's got water flowing around through it. Imagine you were to cut the pipe *and* seal the ends simultaneously. There would be a back pressure at one end, and a low pressure at the other end, causing the flow to slow down and stop. (neglecting some other effects we'll get to in a bit like ""ringing"")

So what if you wanted the effect to last longer, to take a bit longer before the circuit slowed to a halt, what could you do? Well you could take the ends of each wire and add more and more metal to them, giving the electrons more space to spread out on the pile-up end, and more electrons to draw from on the depletion end. (we'll call the pile-up end the negative plate, and the depletion end the positive plate from now on). And if those plates were closer together, the electrons would ""see"" the opposite plate more easily and so not feel the effect as strongly (ie, electrons travelling toward the negative plate and being repelled would also be attracted to the positive plate just slightly beyond it, and so the overall repulsion would be relatively less). 

So that describes our basic ""parallel plate capacitor:"" The amount of charge you can ""store"" in the capacitor, its 'capacitance' is proportional to the area and inversely proportional to the distance between the plates. C = k A / d (where k is just a constant to show proportionality).

---

Now let's look at the next step. We have these plates of charge, one positive and one negative. The current has stopped. well since we have a potential difference, that's going to drive a current in the opposite direction, no? The electrons on the negative plate are going to be pushed off of it and pulled onto the positive plate. But since they can't cross the gap between the plates (deal with this a bit later) they have to go the opposite way of the current that charged the plates up; they are 'discharging.'

---

So now let's put a little more into it. Clearly if you just pushed the electrons harder, you could push more electrons onto the plate, so we can also make a new equation Q=CV, where the charge Q is given by the structure of the capacitor itself (how it distributes charges) and the Voltage (how strongly charges are being pushed onto the plate).

Furthermore, we'd like to see how these plates change over time, right? Well, the amount of charge being added or removed from the plates is going to relate to how much charge is already on the plates, right? If they're really charged up, there are going to be very few new charges added to them (if we're charging) or a *lot* of charges leaving them (if we're discharging). And if we want something where the rate of change is proportional to the value at that moment, we want an exponential. Some kind of e^t function. 

Well let's consider discharging full plates. They start with some charge Q0 and get rid of a lot quickly. But as time goes on, there's less pressure to push new charges out, so they lose less charge. So if we look at e^t, we see that it goes from 0 at t=-infinity, passes through ""1"" at t=0 and then climbs high after that. What we'd like to do is flip that graph around the t=0 axis, right? So that it starts with some value and trends toward 0 after very long times. Flipping about that axis takes t to -t. So we can say that Q(t) = Q0 * e^-t . 

Now let's think of charging. Again we know that to start there's no charge, so we want rapid change. But we also want it to level off to some constant value, where the ""back-pressure"" from the charge on the plates is equal to the ""forward-pressure"" we're using to charge it with. So we look at our e^t again. Well that won't work since it goes up to infinity, which isn't reality. We try flipping it about the t=0 axis again (e^-t ), since that at least levels off. But this graph is decreasing charge, and we want increasing. So we multiply the whole thing by a -1, giving us -e^-t . Well the charge is increasing from a negative 1 to zero... that's not right either, since it starts at 0 and goes to some steady value. So let's add 1 to it, and we get 1 - e^-t . It starts at 0 and increases to 1 in a way that's proportional to the amount that's on the plate. That's nearly what we're looking for. If we just multiply the whole thing by its steady constant charge (Q0) then we'll have it. Q(t) = Q0 * (1 - e^-t ). ",null,5,cdkkfxd,1r841l,askscience,new,45
njaard,"Water metaphor:

Imagine a sphere in which right in the middle is a membrane made of rubber, and tubes go out on either end:

           ___
    ======( | )======
           ---

If you push water on one side, the rubber membrane starts stretching in the other direction. If you release that pressure, the membrane pushes water back.

Now replace ""water"" with ""current"" and ""push"" with ""apply voltage"".",null,2,cdkjvwd,1r841l,askscience,new,7
PorchPhysics,"A lot of the examples given here are great, but capacitors have another use in AC circuits.

Capacitors in AC circuits have decreased impedance at higher frequencies.  Impedance is effectively like resistance, but is more dependent on other factors in the circuit. 

One place where this comes in handy is for tweeters in speakers, they want only the high frequency sounds to come out so that the lower sounds are not overpowering.  Hook up a capacitor in series and it will filter out lower frequencies by resisting them more than the higher ones.  Since lower frequencies are already overpowering when coming out of a subwoofer, they don't need a similiar filter (but you could in theory use an inductor to the same effect on higher frequencies).",null,2,cdklbkp,1r841l,askscience,new,5
flawless-contempt,"The capacitor for a hydraulic system would be your accumulator and it basically does the same things.  Stores ""presure/voltage"" until a need arises, and creates a shock absorber for the system in case any irregularities occur as not to damage the system.  It has a very fast discharge rate and both are equally dangerous when not handled with proper care and due respect. 

Edit: Also a capacitor will only block direct current (dc)  it allows alternating current (ac)  to pass through. 
An accumulator does this by tying into a single direction flow. Meaning it only has one line in and will push its charge directly into a single line of the system there by absorbing impacts to the system or similarly ultimately supplying power.   Hydraulic systems and basic electrical systems are very identical. ",null,1,cdkl7qd,1r841l,askscience,new,2
Lost_Afropick,"I was using some at work yesterday for power factor correction.

You can read about it [here](http://www.kwsaving.co.uk/Business/pfc/pfc-simple.htm)

But I had to supply an inductive load, a big transformer with a certain amount of amps and because of it's impedence that would take a certain amount of volts to do.

Only my supply doesn't have that kind of V/A power and that current is slightly higher than what my supply regulator will allow (it's windings may burn).   So I used capacitors to get me a few extra amps.  

Use the link i put above to see how.  But In a purely resistive circuit your current will be in phase (rise and fall at the same point of a cycle) as your volts.  With an inductive load like my transformer, the current lags by 90degrees.  With a capacitive circuit the current leads the volts by 90degrees.  So adjusting the capacitance lets you play around with the angle of the current in this RLC circuit.  The current the transformer sees is still it's high rated current but I'm drawing less amps from my supply.  

[Here's more](http://en.wikipedia.org/wiki/Power_factor#Power_factor_correction_of_linear_loads)",null,1,cdklepb,1r841l,askscience,new,2
Mathness,"A simple system that will show the effect of charge and discharge is an astable multivibrator with two bulbs/LEDs, the time each bulb is on and off can be set by the capacitors (and resistors). Also fairly simple to calculate the times and show that the theory fits reality.

Another is an AM radio, which can be build with very few components.

Capacitors have a lot different uses, to list some basic uses. Block AC and let DC pass (and vice versa), change the frequency response of a system (filtering sound/noise for instance) and generating a specific frequency (for example in a radio).",null,1,cdkma0z,1r841l,askscience,new,2
SkyDolphin99,"Thanks for your replies everyone. I appreciate your effort to explain it to me, but I really can't understand I'm afraid. I would just like you to go slower in terms of explaining. :)",null,0,cdkrrep,1r841l,askscience,new,1
kajarago,"Other folks here have discussed the physical theory of the capacitor.  This explanation outlines some of the uses of capacitors in a circuit.

The function of the capacitor will depend on the type of input voltage (alternating or direct) as well as the placement of the component relative to the circuit.

The capacitor is very useful in a circuit because its impedance (electrical ""resistance"") is a function of the capacitance and the frequency of the signal as Z = 1/(2*pi*f*C).  This means that a system can be tuned to a certain band of frequencies or can be used as a filter depending on the application.  Take this simple circuit as an example:

http://upload.wikimedia.org/wikipedia/commons/e/e0/1st_Order_Lowpass_Filter_RC.svg

The capacitor will ""hold"" the lower-frequency components of the signal so you will see DC signals have the full* amplitude of the input signal, and the amplitude will roll off to zero as the signal frequency component increases to infinity.

In other more extreme cases, the amount of charge stored in a capacitor is very high and can be released to power devices like a camera's flash or an electric car.",null,0,cdksko5,1r841l,askscience,new,1
fourpenguins,[Related question](http://www.reddit.com/r/AskElectronics/comments/1pru9j/what_does_a_capacitor_do_what_are_they_used_for/) from three weeks ago in /r/AskElectronics,null,0,cdky9te,1r841l,askscience,new,1
knflrpn,"One of the useful properties of capacitors that I don't think anyone has [directly] mentioned yet is that the current through them can change instantaneously, while the voltage across them can not.


This is why they're used to ""smooth out"" voltage.  If the current in some device needs to change very quickly (as in, for example, a computer's CPU), but the power supply is relatively far away (e.g. somewhere else on the motherboard) then capacitors nearby will prevent the voltage from changing at the device (or at least mitigate it).


The reason that being ""far away"" matters is the corollary to the capacitor: the inductor.  For an inductor, the voltage across it can change instantaneously but the current can not.  Wires have inductance, so if the power supply is somewhere else, the inductance of the wires can cause problems with fast changes in current.",null,1,cdkzi8u,1r841l,askscience,new,1
whatzefuk,"best to test and check it out on a oscilloscope and you will understand it fully.
it pretty much packs electrons and releases them when its full , and you can influence curbs and yeah theres plenty of uses like Mathness said , you can pulse dc , use it as a filter , to turn from ac to dc its a diode bridge you will use , you dont have to use a cap but its highly recommnended to filter out.

same goes for neon lights , balasts you need a jolt top turn on the chemical reaction in the neon light but after that it can run on very low power , to create that jolt your gonna use caps.

also since caps hold on electrons after putting something on power be advised that it might hold a charge , small farad caps you can worry too much but once you get to the big boys it can be lethal , turning off the power and time will have it decay its charge.

Some capacitors have poles also , in my course it was always funny to hear a gunshot somewhere in the class you knew someone plugged his cap on the wrong pole.
",null,4,cdknkys,1r841l,askscience,new,3
shavera,"well it does always travel at the same speed. And if the space between two points is expanding, you could imagine that it might be that light would appear *as if* the light was being slowed, since it wasn't covering the right amount of distance in time. 

What **actually** happens is that the light is stretched out while it travels, so that the speed stays the same, even though distance is added between things. 

The other thing maybe you're getting confused about (I'm not sure) is that both wavelength is elongated and frequency is reduced. If you work out the wave equations, they still work for speed of light = c",null,3,cdkjmnc,1r845k,askscience,new,6
bohr_exciton,"The red shift of light is a consequence of the fact that energy is not Lorentz invariant. In simpler terms, an object will be perceived as having different energies depending on your relative motion with respect to it. For instance, take a basketball on a bus. In the stationary frame of reference of an observer on the sidewalk, the ball has a kinetic energy due to its motion with the bus, but in the frame of reference of someone on the bus, the ball would appear stationary and have no kinetic energy.

Now think of light (or photons) as the basketball. The energy of the light is given by h*f, where h is Plank's constant and f is the frequency. Two observers moving relative to one another will observe the photon to be moving with the same speed. However, at the same time, they will measure different energies for the light, which due to the relation above means that they will observe different frequencies/wavelengths. 

Since there is a known relationship between the change in apparent frequency and the relative motion between two reference frames, we can use this information to calculate the relative motion of say an object emitting that light towards us assuming we know the energy of the light in the frame of reference of the body emitting it. ",null,1,cdklf0z,1r845k,askscience,new,3
jimustanguitar,"Does the wavelength shift along with time and space, and I shouldn't be thinking of speed and wavelength as being locked together?",null,0,cdkiz30,1r845k,askscience,new,1
LaLife,"**EDIT:** *It turns out that it's both: there is both Doppler (related to the motion of the point of light emission) and Cosmological (related to space expansion during travel) redshifts, and these both play into the redshift component. The funny thing is that redshift at relativistic velocities can happen even when the objects are not moving away from each other, due to time-dilation effects (Ives-Stilwell experiment).*

To put it simply: the wavelength of the light is a different quantity than the velocity. Any given photon travels at constant velocity c. It's the wavelength of the photon that is 'stretched' at the point and at the time of emission of the photon, due to the relative speed of the light source to the observer.

The redshift is not a measure of the expansion of the universe, it's a measure of the velocity of the light source. We know the universe is expanding by virtue of the fact that the furthest a galaxy is, the faster it's moving away from us.",null,0,cdkred8,1r845k,askscience,new,1
Spiralofourdiv,"I see a lot of kinda complex answers here, so let me take a crack a a simple one:

Light is indeed a fixed speed for all reverence frames, etc. but the color of light has nothing to do with speed, it only has to do with frequency.

Dopler shift **doesn't** ""speed up"" or ""slow down"" light, it's actually entirely dependent on the fact that neither can happen! Since light moves at a fixed speed for all observers, if the source and/or target are moving, then the wavelength needs to adjust correspondingly so that there can be n/2 wavelengths between the objects (that's the ""quantum"" part of quantum mechanics, no ""23.1495 wavelengths"" allowed, only 1/2, 1, 3/2, 3, ...). When the wavelength is ""pulled out"" because the targets are moving away from each other, the light is red. It's not slower light, it's simply lower wavelength/frequency light. 

This actually makes some intuitive sense if you consider that frequency is a measure of energy. The following is a bit naive, but I think it's fair. It makes sense that light has to travel farther to reach targets that are moving away, and traveling farther takes more energy, so the light we perceive is lower frequency/energy. Similarly, objects moving towards each other ""give"" energy to the light since it's reaching it's target in shorter distance. The energy can't disappear, so the frequency is higher.",null,0,cdkwrch,1r845k,askscience,new,1
-Rookery-,While the speed of light is still the same certain aspects of the light change. As the wavelength of the light increases it red shifts. To ensure that the speed of light is the same (c=wavelength * frequency) the frequency of the photon needs to decrease.,null,0,cdlpoek,1r845k,askscience,new,1
proule,"Simply put: Genes come in different versions referred to as ""alleles"". Different alleles show different degrees of dominance over each other, and on top of that, some alleles may show incomplete dominance when expressed along with a certain other allele. 

In the dominant-recessive gene interaction, one gene's visible end effect dominates and is all you see. In incomplete dominance, two alleles are expressed where neither fully dominates the other.

In reference to the eye colour question: Eye colour tends to be more of a dominant-recessive relationship. The allele for brown eyes dominates that for blue, so we don't get any sort of brown-blue intermediate. This works similarly for green and blue.

Skin pigmentation is the result of several (I believe around 6?) separate genes relating to the amount of pigment produced, the ability for pigment to be shipped to the proper place in the body, stuff like that. In this case, the myriad of different genes contributing to skin pigmentation result in expression that mirrors incomplete dominance (but is actually slightly more complicated than the normal single-gene dominance question).

There are some concrete reasons that some traits mix and others don't. People with blue eyes have significantly reduced melanin in their irises, so any allele for eye colour that isn't blue will just overwrite blue eyes, since the trait is due to little pigmentation (so it can just be overwritten essentially, by the presence of said pigment). The reasons for other allelic interactions vary by case, and there's likely no satisfactory answer that's all encompassing.",null,0,cdklnkc,1r847u,askscience,new,6
ToThink,"I'm only in my 3rd year studying genetics, so I hope I can get this through properly :)

First we have to establish the fact that within a gene, there are alleles of that trait. Alleles are just different forms of a gene, so for eye colour there is a ""green eye"" allele and there is a ""blue eye"" allele. I put the quotations there because eye colour isn't due to a specialised pigment which reflects that colour of light (e.g., blue eyes aren't due to a pigment which reflects blue lights), rather eye colour is dependent on the amount of melanin in the eye. So blue eyes = very low melanin
Similarly with skin colour, the amount of melanin in your skin determines how dark you are.
Currently, one of the explanations behind why a parent with blue eyes and a parent with green eyes don't make a turquoise eye baby is because of dominance of alleles. The child will most likely have green eyes because the green eye allele is ""dominant"" to the blue eye allele. This isn't completely true but eye colour works in a ""dominance-recessiveness"" sort of manner.
There are new studies which reveal this may not be the case, rather some new studies reveal eye colour is dependent on many different genes. (I tried looking for them, couldn't find them).",null,3,cdkkf68,1r847u,askscience,new,6
Fignot,"Codominance can be a funny thing. There's a type of codominance that is unique to genes located on the X-chromosome, because women develop as genetic chimera's of themselves.

Another example is in blood where the A and B are codominate. This is because most genes you have are being expressed, even the recessive ones. In the case of the blood genes though any amount of antibody production is enough to make certain types of blood incompatible with you. Since people with the AB blood type are producing A and B antibodies, they will react with blood that contains the relevant antigens.

On the other hand though you have traits like hair colour. Say you have the blond hair allele, and the brown hair allele. You'll make the pigments of both colours, but the brown will overpower the blonde. So you'll still have brown hair, but maybe you'll have a slightly lighter shade of brown, or some blond highlights.

*source: I went to school for Bioinformatics.",null,1,cdkrzvy,1r847u,askscience,new,2
KarlOskar12,"Mythbusters actually demonstrate how much a stomach can stretch before bursting in this [clip](http://www.youtube.com/watch?v=93vjY9RY4-k). However, one technique competitive eaters use is to use abdominal muscle contractions to force food from the stomach into the small intestines creating more room for them to continue eating. Normally this would likely cause one to vomit, but through practice they have been able to bypass the reflex. Their bowel movements must be massive after a hot dog eating competition, although it would probably feel better if they just vomit a lot of it up. And they drink plenty of fluids so they don't get dehydrated.",null,0,cdkyz6w,1r86rk,askscience,new,2
afranius,"Bitcoin and peer-to-peer data sharing are about equally anonymous. When people say bitcoin is anonymous, they don't mean that it's impossible to find out who paid for what, they mean that you don't have to use a bank account or credit card number (which usually requires supplying your real name, DOB, etc and stores a history of your transactions). You still need to use a computer to transmit the information, which exposes an IP address, which can in principle be traced back to you. The same is true for peer-to-peer data sharing. You can mitigate this by not using a connection registered to you (internet cafe, public wifi) and taking other steps to anonymize your traffic (proxies, etc.), and that applies equally to both bitcoin and peer-to-peer traffic.

To summarize, if the FBI wants to prove that you paid for something with bitcoin, they can show that the payment data came from your computer, just as they can show that your computer was involved in peer-to-peer file sharing. Methods to obfuscate your identity are equally applicable to both.",null,0,cdknq1u,1r87g0,askscience,new,8
Daegs,"While other posters are correct that bitcoin isn't anonymous, often transactions are made through the Tor network, which makes it anonymous.

The reason for this working for bitcoin and not P2P, is that the Tor network has a lot of overhead, leading to high latencies and very small bandwidth. 

For transmitting a bitcoin transaction, only a few ID's and keys need to be passed around, so even on extremely slow connection, it will be very fast. For P2P, this might make a 5 min download take a day or more. ",null,0,cdkp7yv,1r87g0,askscience,new,3
null,null,null,1,cdknxni,1r87g0,askscience,new,1
polandpower,"Yes, it's not a ""strong law"" in that you could, theoretically, observe the reverse happening. However, the chance of heat flowing from cold to hot on human observable time scales is so *astronomically* low that in practice the law is never violated.

If you look for instance at a gas, you can compute the microstates and see just how gigantically more likely heat is to flow from hot to cold. 

If you're interested in reading more about the topic, I definitely recommend going to your (University) library or Amazon to read [""Thermal Physics"" by Daniel V Schroeder](http://www.amazon.com/Introduction-Thermal-Physics-Daniel-Schroeder/dp/0201380277). It's very well written and pretty readable if you've had high school physics. 

",null,0,cdkp8za,1r88ue,askscience,new,6
Mooslletoe,"Everything goes from high to low, unless you do work. That is the second law of thermodynamics. The hot and cold try to equalize and reach equilibrium unless you do work on the system. This also applies to grades in school (assuming you start with an A), your grade will drop unless you do work. ",null,11,cdklifh,1r88ue,askscience,new,3
iorgfeflkd,"Yes, that's why you get two-slit interference patterns when you do the experiment with electrons.",null,3,cdkkuxr,1r89ry,askscience,new,9
nanopoop,Another example is neutron scattering.,null,1,cdkl3y0,1r89ry,askscience,new,4
selfification,Also consider bonding and anti-bonding orbitals.  That's literally electron wave functions overlapping in phase and out of phase.,null,1,cdkopgo,1r89ry,askscience,new,4
Trill-Nye,"The two most prominent examples, diffraction of electrons and neutrons, have been mentioned. To expand on this, all particles can be made to interfere if suitable conditions are imposed. This is easiest to do with subatomic particles. By shooting energetic electrons or neutrons at some kind of grating for which the spacing of the grates are on the order of the wavelength of the particles used, one can produce a sort of ""many-slit"" experiment. 

Crystalline materials are good for this, because they have regular atomic spacings, such that the atoms scatter incoming particles, generating quasi-point sources of scattered particles similar to the slits in a double slit experiment. If you then have detectors set up to measure where these particles end up, you will see an interference pattern where certain places get many hits (where constructive interference of the wave-like particles occurs) and other places get few or none (where destructive interference occurs).

Interestingly, diffraction using [x-rays](http://en.wikipedia.org/wiki/X-ray_diffraction#Overview_of_single-crystal_X-ray_diffraction) (photons) gives similar results to [neutron](http://en.wikipedia.org/wiki/Neutron_diffraction) and [electron](http://en.wikipedia.org/wiki/Electron_diffraction) diffraction, because all three behave as waves in a scattering experiment, even though the latter two are particles.",null,0,cdklu16,1r89ry,askscience,new,3
bertrussell,"Destructive interference doesn't mean that the matter disappears or cancels itself out.

Destructive interference means that the there is an interference pattern in the position/momentum pattern for the objects.

When light undergoes destructive interference in one location, there is necessarily constructive interference in another location. This means that the light is more likely to interact in the constructive interference location than in the destructive interference location. The same is true for matter that interferes.",null,0,cdkrgwe,1r89ry,askscience,new,2
Brodken,"I think your question have been answered already, but I will contribute with an example I personally like a lot. 

You can macroscopically see matter interference with Bose-Einstein condensates. Around 1995-1996 there started to be a lot of experiments of interference with this condensates (formed by millions of particles). All this macroscopic system *behaves* like one giant quantum particle, and as such, it behaves as a huge wave. This is what we call a giant matter wave. 

I find this so incredibly awesome, the fact that we can actually see in a experiment, in a direct way, the inteference of two macroscopic clouds of atoms.",null,0,cdkuce8,1r89ry,askscience,new,2
clade_nade,"One not-yet-posted example is [Anderson localization](http://en.wikipedia.org/wiki/Anderson_localization), which is basically the result of electron wavefunctions canceling themselves out in disordered solids.*

*Actually I'm not sure if this has been experimentally observed for electrons, but the theory is quite solid and it's worked for photons, so...",null,0,cdl8gli,1r89ry,askscience,new,2
NightmareOfLagrange,"Stimulation of epithelial cells in the upper airway from tobacco smoke (both first and secondhand) has been shown to result in both an immediate inflammatory response (SEVERELY exacerbated by any concurrent allergic reactions, I might add) as well as long-term remodeling of the tissue.  Without getting too technical, constant damage and stimulation of the airway tissue causes the tissue to remodel in an attempt to repair and acclimate to the smoke.",null,0,cdkl5ck,1r8ahf,askscience,new,1
strummingmusic,"Even though there are no ""crows"" in South America, there are plenty of other corvids. Different types of jays and such in the genuses Calocitta, Cyanocorax, Cyanolycra, etc. Here's a photo I took of a Green Jay in Venezuela some years ago: http://imgur.com/aVoI4Lp

Maybe over time the range of crows will extend down into there, who knows - we're looking at such a short timespan when you really think about things on an evolutional level. ",null,39,cdkorbl,1r8ake,askscience,new,209
carolnuts,"But we have corvids here in Brazil! We call them ""gralhas"" down here.  
The most common is the azure ray ( gralha-azul) , who lives in southern  Brazil. [Link](http://farm5.staticflickr.com/4087/5176353885_f94fbabfc1_z.jpg)


But we also have the white-naped jay ( gralha cancā ) , who lives in the Northeast semi arid region.  [Link](http://www.criadourosonhomeu.com.br/sonhomeu/images/gralhacanca.jpg)",null,13,cdksjqx,1r8ake,askscience,new,73
AshRandom,"This might seem overly simplistic, but the short answer is that there are a vast number of bird species in south america which out-compete the north american crow. 

Also, it's possible that it is a question of the motivation for continental movement which triggers the spread of species. Crows in the southern parts of their range appear to be resident and not migratory. This tendency might also contribute to the explanation for why they have so far failed to take up residency in the southern continent. 

Partial Source: *Dr. Kevin J. McGowan, Cornell Lab of Ornithology.*",null,9,cdkp7e8,1r8ake,askscience,new,65
AcaAwkward,The migration pattern observed in crows excludes both New Zealand and southern parts of South America. There is no concensus on the reason behind this,null,0,cdknamr,1r8ake,askscience,new,4
null,null,null,1,cdkn03v,1r8ake,askscience,new,1
wazoheat,"**tl;dr: Wind gusts are usually very shallow, only within a kilometer (0.6 miles) or so of the ground ([simulation video](http://www.youtube.com/watch?v=G7aOwKigyTk)), and are due to turbulence.**

Sudden wind gusts are caused by a few different phenomenon, but the most common is just plain old turbulence. Through most of the depth of the atmosphere, wind speeds are relatively constant over short periods of time. Wind speed and direction above the ground changes all the time, but over the course of hours, unlike the ""gusts"" we experience which only last a few seconds. Friction in the [planetary boundary layer](https://en.wikipedia.org/wiki/Planetary_boundary_layer) (the layer of air closest to the ground) means that the wind above Earth's surface is almost always going to be stronger than the wind near the surface, where people spend the majority of their time. This means that there is a region of high wind speed flowing over a region of relatively low wind speed near the ground. The boundary between these two regions is inherently unstable, which results in turbulence. This turbulence has the consequence of sending some areas of high-velocity wind down towards the ground

[Here is a simulation that probably gives the best visualization of this phenomenon](http://www.youtube.com/watch?v=G7aOwKigyTk). In that video, high winds are marked in red, and calm winds in green. You can see that as time goes on, areas of high winds are brought down to the surface due to the turbulent motions of the boundary layer, causing what we know as a ""gust"" of wind. So to answer your initial question, a cross-section of a gust would look like one of those green areas in the video; it does not extend throughout the atmosphere. The depth of the boundary layer is usually [around 1 km (0.6 mi or 3300 feet)](http://www.met.rdg.ac.uk/~swrhgnrj/teaching/MT36E/MT36E_BL_lecture_notes.pdf), so this is about how ""tall"" a gust would be.",null,0,cdkni9i,1r8apk,askscience,new,16
NAG3LT,"Well, almost all 3D stuff that is made today is Stereoscopic 3D. It means that two images are created with a slight offset in camera position. When one image is shown to one eye, while the other eye sees the slightly different picture we perceive it as a single 3D image. So to display such 3D content, your TV must be capable to display two different images to the two eyes at the same time.


Your standard TV is likely an LCD panel with 50 or 60 Hz refresh rate. Your both eyes see the same image on TV, which is slightly shifted between eyes and you perceive it at the same distance as the TV is. To allow your eyes to see different images you need some additional tricks. The simplest solution is to use special glasses with filters capable of filtering different stuff. 


There is one type of 3D your TV can show with no issues - those 2 colour [Anaglyph images](http://en.wikipedia.org/wiki/Anaglyph_3D). You then use glasses with red and cyan filters (or other colour combo) and one eye sees only red and other only the cyan image. The main issue of this method is colour reproduction, which is awful, so it works best with black and white content. Some colour choices are less bad, but there is no perfect choice for this method. Another type of 3D that normal TV can display is [active shutter 3D] (http://en.wikipedia.org/wiki/Active_shutter_3D_system). You use a glasses with LCD shutters which show 1 frame to right eye while covering the left, second to the left and so on. TV meanwhile displays frames in pairs: R-L-R-L-... , which you also perceive as a 3D view. The refresh rate you see is effectively halved and the smaller it is the more perceptible the flickering is. Using it on a 60 Hz display is possible, but far from comfortable, so TV's using it have 100 Hz or higher refresh rates to get 50 Hz of more perceived 3D refresh rate. 


Another interesting property of light used for 3D displays, but mostly with projectors is the polarisation. The light can come in two different polarisations, like linear vertical and linear horizontal or CW circular and CCW circular. These are just some of possible combinations, but what is important that they are exclusive. You can use a special polariser to pass only CW polarisation while completely blocking CCW. Polarisation is also independent from colour and can be filtered by a passive filter, making glasses with polarised filters relatively cheap. So by showing image for one eye in CW polarisation and other image in CCW, glasses with corresponding filters can allow you to see a 3D film. This technique simply requires the special hardware to show different polarisations and wont work on a normal TV. BTW, all LCD displays emit a linearly polarised light, but only in one polarisation. 


If you don't want to use glasses there are some solutions, but they are less reliable so far and less used. Some TVs use micro lenses to show some pixels only from a specific angle of view. This allows them to show different images to your eyes, which look at TV with a slight difference in angle. Their main problem is the fact that you can only view them from specific spots, otherwise the 3D effect breaks down. 


There are other methods as well, but all of them still require you to have additional hardware to show separate images to each eye. The methods with the highest quality require hardware solutions that normal non-3D TVs almost always lack.",null,0,cdkn5i6,1r8ayj,askscience,new,5
threegigs,"3D TVs have alternating pixels which are polarized horizontally and vertically, allowing it to display one vertically polarized image and one horizontally polarized image, which are then selectively filtered out by 3D glasses (one lens horizontal, one vertical).

Alternately, active shutter glasses can be used to make any TV a 3D TV, by presenting alternating images for the left/right eyes, and by using special glasses which alternate turning each lens opaque, thus presenting different images to each eye.",null,1,cdkn8xx,1r8ayj,askscience,new,3
StringOfLights,"Crocodylians, which include alligators and crocodiles, are not dinosaurs. They are the closest living relatives of dinosaurs, however (because birds *are* theropod dinosaurs).

Dinosauria is a group that was originally defined by anatomist [Richard Owen](http://www.nhm.ac.uk/nature-online/science-of-natural-history/biographies/richard-owen/) based on a few described taxa, including [*Iguanodon*](http://en.wikipedia.org/wiki/Iguanodon) and [*Megalosaurus*](http://en.wikipedia.org/wiki/Megalosaurus). There are a few more technical ways to define the group, but no matter what it falls out being comprised of two smaller groups: [Ornithischia](http://en.wikipedia.org/wiki/Ornithischia) and [Saurischia](http://en.wikipedia.org/wiki/Saurischia), although these groups were not recognized at the time. Ornithischia includes dinosaurs like *Triceratops*, *Iguanodon*, and ankylosaurs. Saurischia includes sauropods and theropods.

Crocodylians, dinosaurs, and a couple other groups (including pterosaurs) are [archosaurs](http://archosaurmusings.wordpress.com/what-are-archosaurs/) (side note: people often refer to pterosaurs as dinosaurs, but they're actually not). 

To get more at the heart of your question: Crocodylians are widely perceived as these unchanging, prehistoric animals. They're really not. Crown-group crocodylians (that is, the group consisting of the common ancestor of all living species and all of the descendents of that ancestor) first show up in the Late Cretaceous, around 84 million years ago. This actually isn't a terribly long time ago, and it overlaps with the non-avian dinosaurs for about 20 million years. For reference, the [oldest known placental mammal](http://www.livescience.com/15734-oldest-placental-mammal.html) is 160 million years old. 

It is true that crocodylians do have relatives that extended back much further, because archosaurs started to diversify in the Triassic some 250 million years ago, but the crocs you see today are highly derived, not long-forgotten vestiges of the Mesozoic. It's true that some have had a fairly stable body plan, but it's also a body plan that has cropped up multiple times in vertebrate evolution, including in [temnospondyl amphibians](http://en.wikipedia.org/wiki/Prionosuchus) some 270 million years ago. In a lot of these cases it has evolved independently.

The major radiation of archosaurs that includes modern crocodylians is known as Pseudosuchia, and it [first shows up about 250 million years ago](http://openi.nlm.nih.gov/detailedresult.php?img=3194824_pone.0025693.g012&amp;req=4). These early [relatives of crocs](http://web.missouri.edu/~hollidayca/Croc_epipterygoid/Fig%202.jpg) looked more like [this](http://en.wikipedia.org/wiki/Hesperosuchus) (in that cladogram Crurotarsi = Pseudosuchia). Nothing like a modern croc. 

Even as we move up the tree towards Crocodylia, early crocodyliforms looked like [this](http://en.wikipedia.org/wiki/Protosuchus). These were fairly gracile, terrestrial animals. Again, a similar croc body plan pops up in a few lineages, like in the [phytosaurs](http://en.wikipedia.org/wiki/Phytosaur), which are likely a basal pseudosuchian but not closely related to crocodylians.

[Mesoeucrocodylians](http://en.wikipedia.org/wiki/Mesoeucrocodylia), a grade of crocodyliforms that isn't a valid taxon but useful for referring to groups outside the crown group, often look more like the body plan associated with typical crocodylians, but they also show significantly more morphological diversity than that. Pholidosaurs (like [*Sarcosuchus*](http://en.wikipedia.org/wiki/Sarcosuchus)) and dyrosaurs have a similar body plan. Metriorhynchids like [*Metriorhynchus*](http://en.wikipedia.org/wiki/Metriorhynchus_superciliosus) were marine and had flippers. Notosuchians like [*Simosuchus*](http://en.wikipedia.org/wiki/Simosuchus) are very different. *Simosuchus* probably wasn't even carnivorous. It was also pretty adorable. 

The oldest members of crown-group Crocodylia are more morphologically similar to extant crocodylians. However, you still have morphological variation within Crocodylia, such as the [pristichampsids](http://en.wikipedia.org/wiki/Pristichampsidae), which were terrestrial. Terrestriality shows up again even in the family Crocodylidae (with the Mekosuchinae, including *Quinkana*).

The oldest members definitely attributable the genus *Crocodylus* [date to the Late Miocene](http://www.bioone.org/doi/abs/10.1643/0045-8511%282000%29000%5B0657:PRADTO%5D2.0.CO%3B2) (paywalled, sorry), and the genus probably diverged in the last 10 million years or so. That's pretty recent in the grand scheme of things, and some 55 million years after the non-avian dinosaurs went extinct.

",null,0,cdkrh86,1r8b8y,askscience,new,15
null,null,null,2,cdkow4g,1r8b8y,askscience,new,2
Astrokiwi,"That's exactly it: you can't! This is actually a big part of the theory of relativity: there is no universal concept of ""not moving"". You can say things are moving or not moving relative to something (as you mentioned), but to simply say that something is ""not moving"" does not actually make sense.",null,1,cdklc3q,1r8bbn,askscience,new,7
Naf623,"We can't tell if something is absolutely moving; only if it's position is changing relative to us. So we still wouldn't know for sure if we're both moving or just one, or which. 
One method is red shift if light based on the Doppler effect. Similar to how sounds are distorted as an object moves (sirens coming closer then further away) so is the light from/reflected off it. If the light is more red then the object is getting further away; more blue ; it's approaching. ",null,0,cdklyi0,1r8bbn,askscience,new,1
JoolNoret,"Sea level is the average level of the water in the ocean between tides.

The Netherlands is below sea level (hence the name). So is New Orleans, which resulted in flooding when the levees broke during Katrina.

Just because an area is below sea level doesn't mean the water will climb over a hill to fill it up.",null,2,cdklqri,1r8bxo,askscience,new,8
robged,"Before I answer the question, I need to describe a speakers construction: an electromagnetic coil produces magnetic fields when electricity is applied. A rare earth magnet attached to a flexible cone moves in response to this magnetic field. The flexible cone compresses air generating sound. Bass is low frequency sound which is generally between 32 Hz at 512 Hz. The overall range of human hearing is between 20 Hz and 20,000 Hz. 

Speakers are less efficient at producing low frequency sounds than high frequency sounds, as during the low frequency cycle, air has time to equalibriate. To get around this inefficiency, it is possible to build the speaker larger and having it travel farther. Fortunately, it only needs to vibrate at lower frequency than high frequency speakers, so this is possible. Also, note that not only can high frequency speakers be smaller, but they also *must* be smaller and have less travel, because if they were as big as bass speakers the high amplitude combined with high travel would rip them apart.

Now, knowing that, it becomes clear why there is such a thing as a crossover circuit in good quality speakers. A crossover circuit is a low-pass filter before the bass speaker, and a high-pass filter before the treble, such that the small high frequency speaker doesn't get the large amplitude bass signals which will cause it to rip, and the low frequency speaker doesn't get the high frequency signals which will make a distorted sound as it is too large to respond to the high frequency signals.

Sound quality is *defined* as the ability of a system to produce an even response across all frequencies, at high volume, without distortion. If you like more bass, on a good system, you can always use an equalizer to add more bass without distortion.

EDIT: The energy of sound waves is independent of frequency (I get confused with light) so I had to delete a sentence.

TL;DR? The big speakers are for bass, medium speakers for mid range, the small speakers are for treble.",null,1,cdkm3ga,1r8c16,askscience,new,7
Daegs,"Sharpening is basically scraping off material until it resembles the shape you want (in this case, a thin edge).

To start sharpening, a large grit is used, which translates to very deep ""trenches"" and irregularities in the resulting material, these are refined with finer grits to even these trenches (scratches) out. Doing this progressively leads to a smoother finish which approximates the shape you want (again, a low angle straight edge).

",null,1,cdkp9xn,1r8fth,askscience,new,6
Bbrhuft,"Tsunamis are caused when the sea floor is displaced up or down within just a few seconds or minutes, this is typically caused by a megathrust earthquake, where oceanic plate is forced under continental plate, creating a violent earthquake and sea floor displacement. 

In the largest megathrust earthquakes, 10,000 to 100,000 km2 or more of sea floor can be displaced upwards several metres in only a few minutes, displacing vast quantities of ocean water. This displaced water radiates away and eventually hits land as a tsunami.

Mud volcanoes, on the other hand, are much smaller and happen over a much longer time frame, days to even years. When the mud is erupted to the surface, the surrounding land surface or ocean bottom may subside slightly. But none of these changes are ever on a large enough scale or fast enough to create a tsunami.",null,0,cdkpc7x,1r8g1s,askscience,new,6
GreenAdept,"Landslides and impacts can actually cause what is know as a Megatsunami, with potentially greater impact than the typical earthquake tsunami. I'm not very familiar  with mud volcanoes (last I knew they were fairly poorly understood) but it's reasonable to surmise if one were to ""erupt"" in an area that could cause a massive landslide it could be responsible. http://en.wikipedia.org/wiki/Megatsunami",null,0,cdkxyky,1r8g1s,askscience,new,5
TheCrazyOrange,"Because sound is just compression waves traveling through the air, and our ears are highly sensitive to the frequencies in the range of human voices.

But when you blow air, your attempting to physically move the air with enough velocity that it will reach the person. Contrary to what it seems, air has mass, and thus the air separating you can diffuse and block the air you blow our.",null,2,cdkn173,1r8gcu,askscience,new,9
selfification,You can't throw a handfull of water that far without a lot of effort but you can create a small splash on a pond and have the ripples travel for a long long distance.,null,3,cdkoctf,1r8gcu,askscience,new,5
therationalpi,"I feel like you haven't gotten a satisfying answer yet, so I'm gonna try my hand at this.

To start, let's imagine what the air is like at rest. All of the air particles are bouncing around randomly, but *for the most part* they are all bouncing around at equilibrium. All of the air particles are where they want to be.

Now, let's say I blow on the air. I'm introducing a slug of fast moving air, with a good deal of kinetic energy. That air has not found it's equilibrium point, so it's going to carry forward. In the process, the air is slamming against the air around it that was formerly at equilibrium, imparting kinetic energy, and carrying it forward. A moment after the air first comes in, you now have a larger slug of slower moving air all trying to find an equilibrium position. The process then repeats, with more and more air going slower and slower. Eventually, the air stops and finds its place.

Now let's look at sound. For sound, you have the same transfer of kinetic energy when the molecules slam into eachother. The big difference is that after they slam into the next group, they *bounce back* and end up right where they started.

As a result of going back to the old equilibrium position, none (or at least very little) of the kinetic energy ends up permanently stored in potential energy. When you blow, part of the energy goes towards moving the next cluster of air molecules, and part of it goes towards choosing a new equilibrium position. For a sound wave, pretty much all of the energy is able to keep moving forward. Look at the first figure on [this page](http://www.acs.psu.edu/drussell/Demos/waves/wavemotion.html), paying careful attention to how the red dot isn't really going anywhere.

Hopefully that answers your questions, but if you would like any clarification please don't hesitate to ask!",null,0,cdkus3i,1r8gcu,askscience,new,2
iorgfeflkd,"More energetic photons are actually more likely to interact with materials (above a certain threshold), because it increases the likelihood of pair production.

[This is essentially a graph of how likely a photon is to interact, vs energy](http://en.wikipedia.org/wiki/File:Attenuation_Coefficient_Iron.svg).",null,4,cdkoadv,1r8gmf,askscience,new,11
StarshipEngineer,"Conservation of energy prohibits a photon from interacting with vacuum itself, and so pair production is restricted to the vicinity of relatively heavy atoms. Therefore, interaction with vacuum and an electron-positron cascade is probably not something that can occur; if it could, such an event would likely have spread across the observable universe long ago. (Unless, of course, photons of such energy are exceedingly rare. However, even one photon would be enough to trigger such a cascade, so it is unlikely that it is possible at all.)

It is true that, from the perspective of particle physics, even a pure vacuum is filled with ""virtual"" particles born of quantum fluctuations. What makes them virtual however is that they appear and disappear in such a short time that their brief existence does not violate the uncertainty principle, and under ordinary circumstances, they are not directly detectable. If a photon were to interact with such a particle, it would have to reduce the energy of the photon at least enough to account for the particle's mass-energy, in order to obey conservation of energy. This would prohibit such a runaway cascade.",null,0,cdkq03p,1r8gmf,askscience,new,2
D0ct0rJ,"I think a photon of sufficient energy will interact with free space strongly enough to prefer to spew out electrons and positrons, which will radiate as they experience forces in material, leading to more high energy photons, and so on: an electron-positron cascade. Gamma rays interact with heavy metals due to pair production, or so the current theory goes.",null,3,cdknpx7,1r8gmf,askscience,new,2
Pallidium,"Yes. This would not be innate though, and would result from the person learning to identify themselves. The ""higher"" brain regions, such as the prefrontal and parietal cortices, would register it as ""me"" or ""my face,"" and in turn use this to alter activity in ""lower"" regions. There probably would not inherently be any any mechanism in the visual cortex or fusiform gyrus (a brain region heavily implicated in facial recognition) for a person identifying their own faces, but these regions activity would be modulated by prefrontal and parietal input, which could lead to different activity viewing oneself as compared to others faces. [Here is a study](http://www.ncbi.nlm.nih.gov/pubmed/18656465) which shows difference between self-recognition and other-recognition, and shows fusiform involvement. I'd like to restate that this is NOT an innate ability of the fusiform and probably results from modulation by higher brain centers. The related citations section of that pubmed abstract (right hand side) has some other abstracts about the neural correlates of self-recognition.",null,2,cdkydec,1r8hem,askscience,new,11
temuchan,"Nucleotides can be synthesized ""de novo"" from precursor molecules (obtained from the breakdown of food, for example).  The major organ involved in this process is the liver.  However, nucleotides can also be [recycled](http://en.wikipedia.org/wiki/Nucleotide_salvage) through a process that synthesizes nucleotides from the components of degraded nucleotides.",null,1,cdkqro4,1r8p24,askscience,new,6
sphenopalatine,"The new nucleotides are synthesized from a large number of other precursors, such as folic acid, glutamine, glycine, etc. The method of synthesis differs between purines (A and G) and pyrimidines (T and C). 

The purine synthesis pathway is [quite long](http://gallus.reactome.org/figures/denovo_IMP_synthesis.jpg), but can be summed up as resulting in the end product inosine monophosphate (IMP). This can be interconverted to GMP or AMP. Two more phosphate groups are added on to give the triphosphate tail of a nucleotide. These ribonucleotides (NTPs) are then converted to deoxyribonucleotides (dNTPs) using [Ribonucleotide Reductase](https://en.wikipedia.org/wiki/Ribonucleotide_reductase) and a dNTP is born.

The pyrimidine synthesis pathway is of [similar length](http://gallus.reactome.org/figures/denovo_UMP_synthesis.jpg) and gives uridine monophosphate (UMP), which is then converted to UTP (used in RNA synthesis). UTP can be interconverted with CTP and TTP. Ribonucleotide Reductase once again converts the NTPs into dNTPs.",null,0,cdkriwp,1r8p24,askscience,new,3
oxymoron1629,"The same mechanism that creates energy from your food has a built in arm that takes the energy in food and instead of creating energy for later use, it uses the energy from food to create the bases needed for DNA replication. But it only does this when the cell decides to replicate so most of the time, it just stores energy from food. ",null,0,cdksxgh,1r8p24,askscience,new,2
TangentialThreat,"Yes. Well, [sort of](http://www.the-scientist.com/?articles.view/articleNo/32997/title/Electrical-Bacteria/).

*Desulfobulbaceae* are forming living wires that connect electron-rich upper sediment with the electron-poor deeper sediment. It is taking advantage of a natural electrochemical potential - in other words, a living wire that's feeding off a battery. Tell me that's not cool. [Study](http://www.nature.com/nature/journal/v491/n7423/full/nature11586.html)

Injecting synthetic ATP into your arm is probably a bad idea. There is never much free ATP in the body except after a major injury. This may cause several body systems to freak out, [including your heart](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3265710/).

",null,1,cdkw2ds,1r8phn,askscience,new,10
JimmyGroove,"I can think of no theoretical reason why such a bacteria couldn't exist.  The practical reason for why one doesn't exist already is that free charge differentials aren't very common in the natural world, don't tend to last terribly long, and don't occur over distances that would allow a bacteria to travel from one to another when their first electrical source evens out.",null,0,cdktc9s,1r8phn,askscience,new,2
JeremyJBarr,"Actually there is a whole scientific field called ""Microbial Electrochemisty"" utilizing this phenomenon! I spent a little part of my PhD working on a microbial fuel cell (MFC) where the basic premise is that you feed a microbial community a waste product in an enclosed reactor operating as an anode, that is connected to a separate reactor which operates as the  cathode. [Google images picture](http://www.technologyreview.com/sites/default/files/legacy/hydrogen_x600.jpg)

In these systems waste products are used as the food source for mixed microbial communities in the anode reactor. These bacterial communities degrade the organic components present in the waste product, while utilizing the anode as an electron sink, and dump protons into the surrounding media. The electrons are then sent via a conductor across to a separate cathode reactor generating current! The protons generate in the anode then flow across to the cathode via a specific membrane that permits their transport, while keeping the microbial communities separate. 

Microbial fuel cells (MFC) were once suggested as a renewable source of electricity from waste products (typically wastewater). However, current MFC designs do not produce sufficient electrical current to make them sustainable. But a recent suggested use for them has been to pump electrons into the MFC, essentially feeding the microbial communities electrons, and force them to produce bioproducts of extreme value. There is lots of research going on in this field investigating novel MFC designs to generate more current, and the formation of high-value bioproducts from wastewater.

Some sources [1](http://www.sciencemag.org/content/337/6095/686.full) [2](http://link.springer.com/article/10.1023/A:1025484009367) [3](http://www.nature.com/ismej/journal/v1/n1/full/ismej20074a.html)",null,1,cdlc8gu,1r8phn,askscience,new,3
arumbar,"Maternal and fetal blood do not normally mix.  Maternal blood is pumped through the [maternal blood vessels in the placenta](http://www.biog1445.org/media/placenta.jpg), where nutrients and oxygen are allowed to diffuse into the fetal bloodstream due to the close proximity of fetal vessels.  There are a number of [fetal anatomic features](http://img.docstoccdn.com/thumb/orig/107478990.png) that work together to make this system work, as the fetus will be obtaining oxygen from the mother rather than its lungs.  The expansion in maternal blood volume and cardiac output is simply a consequence of having to perfuse not only her normal organs but also divert a significant amount of bloodflow to perfuse the uterus and placenta, which then feed into fetal circulation - but they do remain separate.

Typically having a baby with a different blood type is not an issue.  However, a Rh negative mother (eg O-, AB-, etc) can become sensitized to the Rh antigen if her fetus is Rh positive, resulting in complications for future pregnancies.  This usually occurs as tiny volumes of fetal blood (&lt;0.1mL) enter maternal circulation and trigger formation of anti-Rh IgG antibiodies, which can then cross the placental barrier in subsequent pregnancies and cause hemolysis (destruction of fetal red blood cells).  ABO alloimmunization is less commonly an issue - for a variety of reasons the antibodies associated with Rh factor are more prone to cross the placenta and cause disease (they are IgG rather than IgM, and fetal rbcs express more of the Rh antigen).  There are a few other high-risk antibodies associated with fetal hemolysis (eg anti-Kell).",null,3,cdkqj3h,1r8r2g,askscience,new,29
cryptorchidism,"There is a [maternal-fetal barrier](https://en.wikipedia.org/wiki/Placenta#Placental_circulation) in the placenta, similar in function to the [blood-brain barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93brain_barrier) (or for that matter the [blood-testis barrier](https://en.wikipedia.org/wiki/Blood%E2%80%93testis_barrier)/blood follicle barrier). It prevents large molecules from passing, like pathogens, immune cells, and blood antigens, the last of which determine blood type.

^(Thanks, now ""blood"" looks funny.)",null,3,cdkpyc2,1r8r2g,askscience,new,14
NassT,"No, mothers and babies have separate circulatory systems, but the mother's blood does have to carry all of the oxygen, nutrients, etc. for the baby as well.  Pregnant women also tend to put on weight in addition to that of the baby, which also stresses the heart.",null,2,cdkq3u4,1r8r2g,askscience,new,5
Naf623,"No; the mother's heart pumps blood through the placenta, which makes for a small increase in where it needs to be pumped. The baby has it's own completely separate blood circulatory system which also goes through the placenta. 
In the placenta the blood vessels get very small, thin walled &amp; close so that nutrients can be transferred between them. 
I'm very surprised by the 30-50% increase figure; ",null,0,cdkq3hb,1r8r2g,askscience,new,3
mutatron,"Past performance is no guarantee of future results.

Overall fertility has been declining for a long time, and is expected to continue to decline. The figure of 10 billion is arrived at by looking at this decline in fertility.

Replacement rate fertility is 2.1 children per woman, or 21 children for every 10 women. Many countries, mostly developed ones, are now below replacement rate, some as low as 1.3 children per woman. Mexico has dropped from around 4 children per woman to 2.3 in just a couple of decades, and other less developed countries are expected to follow suit as poverty declines, healthcare improves, and education becomes ubiquitous, especially among women. Studies have shown that the most effective deterrent to fertility is the education of women.",null,19,cdkrdzc,1r8tbm,askscience,new,80
4698458973,"Look for a torrent of a video by Hans Rosling called, [Don't panic: the truth about population](http://www.bbc.co.uk/programmes/b03h8r1j). He is a fairly famous statistician, and he's a great speaker.

The short answer is that birth rates have fallen worldwide, in some places like they fell off a cliff. Population growth is still happening because we have at least fifty years left of people getting older: fifty years ago, women were having more babies than women are now, and those babies are going to be around for a while -- and, themselves, have babies. But the next generation will be barely at replacement level for most of the world.",null,3,cdkys9c,1r8tbm,askscience,new,18
AshRandom,"It's only predicted to stabilize at 10 billion by the most conservative of estimates. Dr. Michio Kaku gives a lecture where he talks of the coming super abundance of the future. Should the fusion power plant designed by [ITER](http://en.wikipedia.org/wiki/ITER) successfully create a [Tokamak](http://en.wikipedia.org/wiki/Tokamak) reactor capable of converting plasma directly into electricity (without all the fuss of boiling water and turning a big turbine steam engine) the cost of electricity would plummet. Not only would the cost be many thousands of times lower, the amount of energy available would be many thousands of times greater. 

Cheap and abundant electrical energy combined with modern desalination water processing would turn every bay and every inch of oceanic coastline into a fresh water river. Vast amounts of currently unfarmable land would become viable. Additionally, the ability to build, light, and water indoor hydroponics farms would be possible at tremendously reduced costs. This would make skyscraper greenhouse projects highly profitable, where they are currently cost prohibitive. 

Clean energy, fresh water, and a super abundance of food would have obvious consequences on the expansion of future populations. And should the vast uninhabited stretches of the world's surface become utterly filled with cityscapes, moving underground would then further magnify the square footage available for hydroponic farms and human habitation. Pushing Earth's capacity for human population into the trillions would not be unthinkable. 

Partial Source: *Dr. Michio Kaku N.Y.U. Institute for Advanced Study*
",null,6,cdl6b92,1r8tbm,askscience,new,9
OctoRock33,"As countries progress through development they go through multiple stages of birth, industry, and death. In the final progression of development Stage 4. The birth rate stays at or below the death rate, which could lead to either a stable population or a slowly decreasing population. 
Source: I took a college course on Human Geography
",null,0,cdl6pyd,1r8tbm,askscience,new,2
Zedred,"Predictions calculate there is a tipping point at which insufficient arible land and water exists to sustain the population, factoring in the amount of land and water required produce enough food to sustain each human and the livestock they require.  That number stands at about 24 or 25 acres per human given today's technology and weather models.

Theoretically at a population of somewhere between 4 and 16 billion ( average 10 billion) every single acre would have to be in use for farming  or housing given today's technology and water availability and assumes air quality is sufficient.  Before that limit is reached, population grosth would start to decline due to econonimic factors.  UN calculations no doubt take all these factors into account.

http://en.wikipedia.org/wiki/Human_overpopulation

 Global warming, nuclear disaster, war, or unforseen causes of crop failure could result in less food due to less farmable land, more sea water, and less fresh water, further constraining population growth.  Now facter in the potential for a plague disaster along with antibiotic resistence in a large population, and natural changes in fertility rates due to pollution or declining economic conditions that prevent access to birth control. 

It is not hard to understand that a planet with finite limits on the resources required to sustain human life will sustain only a finite maximum of people.  Technology  improvements and wiser land/water/air use could raise that maximum number, but it will stabilize again at the higher number unless additional resources are introduced into the food and energy production  system from other planets. 

This upper limit on population on earth is why space exploration is so critical to to the future of humanity.   Lets also hope China is smart enough not to launch a population  boom by lifting their one child policy amidst their current economic expansion, lest we reach that upper limit faster than technology can solve the problems that would cause.",null,1,cdl73zt,1r8tbm,askscience,new,2
TheMuslinCrow,"Each species has a population threshold for the environment it's in, known as the carrying capacity (K). This is determined by species requirements, as well as the environmental constraints on these requirements (food, space, etc). When a species population goes above this level, there's usually a setback in the form of disease or famine, and the resultant deaths bring the population back near the carrying capacity.

In the case of humans, we are able to artificially increase the size of our K through technology, medicine, urban planning and such. However, this planet has finite resources and space. So what is the carrying capacity for humans on Earth? We really won't know until we reach it. Some places such, as Japan and parts of Europe, seem to be nearing their K for their respective environments.

Source: I'm a zoologist.

EDIT: Am I being downvoted for providing a biology based answer about population, because this is an earth sciences subreddit? There's too much segregation in science, and that holds us back.",null,5,cdl76cg,1r8tbm,askscience,new,7
Filipinolurve,"Hans Rosling: Religions and babies

https://www.google.com/search?q=ted+talks+hans+rosling+religion+and+babies&amp;ie=UTF-8&amp;oe=UTF-8&amp;hl=en&amp;client=safari

OP I tried to copy and paste the link above (I'm on my phone) anyways this stats guy on TED talks explains how the population trend will go and why, the video starts off talking about if religion and does it affect the population growth but it'll def answer your question.",null,2,cdl6cjs,1r8tbm,askscience,new,2
oxymoron1629,"The Sickle Cell gene is incompletely dominant with the wild type gene. That means that both will be expressed and will create an intermediate phenotype. One who is heterozygous for the Sickle Cell gene is considered to express the Sickle Cell trait, but is not suffering from Sickle Cell Anemia. A heterozygous person will have some normal, some sickle shapes and some in between. Since a red blood cell had many hemoglobin molecules, when both genes are expressed each red blood cell gets some proportion wild type hemoglobin and some mutant ones. The amount of the mutant ones determines how bent the red blood cell will look. 

Tl;dr Not exactly a 50/50 split, but some completely normal, some completely sickle shaped, and many in between. ",null,2,cdkstzy,1r8w0j,askscience,new,6
meaningless_name,"&gt;And how does this help fight against the malaria plasmodium?

[A relevant paper on this topic](http://www.nature.com/news/sickle-cell-mystery-solved-1.9342)

Basically plasmodium, after infecting a red blood cell as part of its life cycle, hijacks RBC actin (a naturally occurring cell-strucure protein), to help it transport a protein of its own (adherin) to the RBC surface to make it stick to surfaces and other RBCs, which helps the plasmodium.

For sickle cell individuals, the mutant RBC actin can polymerize into long, stiff ""rods"" that distort the shape of the RBC and make it ""clog"" in capillaries. 

However, the plasmodium cannot effectively ""hijack"" the mutant RBC actin, which is the source of sickle-cell mediated malaria resistance.

Sickle cell homozygotes have mostly sickle-cell RBCs, which causes the clinical symptoms of sickle cell anemia.

Sickle cell heterozygotes have a mixture of sickle cell and non-sickle cell RBCs (which is not quite 50/50, as oxymoron 1629 explained), meaning they retain the malaria resistance while avoiding the worst of the anemia.",null,2,cdkuzei,1r8w0j,askscience,new,5
pravl,"No, they aren't.  The predominant form of hemoglobin in normal individuals, HbA, consists of two alpha-globin chains and two beta-globin chains.  People with sickle cell disease have only mutated beta-globin, which pairs with alpha-globin to form HbS.  The mutated beta-globin chains pair to alpha-globin with less affinity than normal beta-globin, so individuals who have both normal and mutated beta-globin, i.e. people with sickle cell trait, end up having slightly more HbA than HbS.  Usually around 60% HbA, 40% HbS.  And that is the hemoglobin itself, not the percentage of actual sickled red blood cells.  If not otherwise sick/stressed, people with sickle trait usually have no or very few sickled cells on a blood smear.",null,0,cdl1ovy,1r8w0j,askscience,new,2
Farnswirth,"Heat engines require a heat differential to function, by definition.  

However, what /u/JimmyGroove said was correct as well, and is merely another way of stating the first law, which is:  The change in total internal energy in a system (in this case an engine) is equal to the heat in, minus the heat out, minus the work done by the engine.  This is one of the most fundamental principles of thermodynamics, [the first law.](http://en.wikipedia.org/wiki/First_law_of_thermodynamics)

For instance, you could have a very energetic system with no heat differential (eg. a cold car with a full tank of gasoline).  In this case, the heat inside the system (the car) would be at thermal equilibrium with the outside air because you haven't started the engine yet.  In this case, there would be no heat differential in the beginning.  The net reaction for the car as a system would be a conversion of internal energy (the chemical energy of the gasoline) into work.  *It's all a question of how you define your system.*  

So to answer your question - a heat engine requires a heat differential, by definition, because that's how a heat engine works.  But depending on how you define your ""engine"" (thermodynamic system) - you don't necessarily need a heat differential- for work to be produced.  But the energy must come from somewhere, and it must have somewhere to go.

Edit: Another good example of a system that does not require a heat differential to do work is a fuel cell, which directly converts chemical energy into electrical energy with no heat differential.  Obviously this isn't a heat engine, but you could definite it as an ""engine"", and it is used as such in some vehicles. ",null,1,cdkwnan,1r90aa,askscience,new,8
JimmyGroove,"You only get a net flow of heat from areas of high temperature to low, so a heat engine could only work off ambient heat if the ambient temperature was different from that of the engine (or can be manipulated into being different, the most commonly used way being to change the pressure of a gas).",null,0,cdkt7bu,1r90aa,askscience,new,3
theoreoman,"Law of thermodynamics says that heat can only flow from hot to cold, so for work to happen the heat needs to flow from hot to cold, the larger the differential the more work that can be done.  Quantum mechanics might have a different answer that I'm not aware of",null,0,cdkwkpo,1r90aa,askscience,new,1
Jonex,"Not really answering your question - as it's already answered - but an interesting addition:
You can extract ambient heat energy without having a heat differential - if you add more energy than you extract. This is used in earth heating systems where energy is taken from the ground at a few degres to add to the indoor heating at around 20 degrees.

To to this you need an heat pump, powered by for instance elecricity. There are physical limitations of efficiancy. But it's a popular way to increase the efficiancy of electricaly powered heating.

I'm a bit tired so not an awesome explanation, someone who has done their thermodynamics more recently can hopefully expand and clear things up a bit.",null,0,cdl4g2a,1r90aa,askscience,new,1
Updatebjarni,"Consider the fact that not only are the electrons in the extension cord running back and forth since the cord carries AC, but it's also running down one conductor while it is running up the other inside the cord. So regardless of exactly how electrons behave with respect to gravity in a conductor, that would seem to answer the question.

Same with the battery; there is one conductor carrying electrons from the negative terminal of the battery to ground, and one carrying them from ground to the positive terminal.
",null,1,cdl15ac,1r933v,askscience,new,12
iorgfeflkd,The voltage required to move an electron through a gravitational field is very small but nonzero. It is about 5x10^-11 Volts/meter.,null,2,cdl1mfs,1r933v,askscience,new,10
sporclesam,"If you mean [Sulphate-reducing bacteria](http://en.wikipedia.org/wiki/Sulfate-reducing_bacteria) (which use sulphate and not sulphur as terminal acceptor) then, yes, but not like us but more like plants/algae. These anaerobes use dissimilatory sulfate reduction to obtain sulphide as waste (somewhat similar to plants releasing oxygen from water)

Check out links on [chemosynthesis] (http://en.wikipedia.org/wiki/Chemosynthesis) *vis-a-vis* photosynthesis. ",null,0,cdl3l9h,1r99pm,askscience,new,2
Platypuskeeper,"Not really, they reduce SOx to H2S, while we reduce O2 to H2O. H2S doesn't perform any functions beyond that, while we use water for quite a lot of other things. Sulfur-reducing bacteria still use water for all those other things.

It's also a very different enzyme and mechanism. 
",null,1,cdl64y4,1r99pm,askscience,new,2
dudley_love,"Great question, putting the finger on the limit of the SF theory. 

One variation of the SF is [Yanagida](http://www.ncbi.nlm.nih.gov/pubmed/2082730)'s, where the actin-myosin crossbridge is not a solid-ish state, but a ""loose"", transient one. 

So instead of a ladder with strong actin ""hands"", you have a ratchet with potential for slippage.",null,0,cdmjgkh,1r9a7m,askscience,new,4
KarlOskar12,"[Eccentric contraction](http://medical-dictionary.thefreedictionary.com/eccentric+contraction) happens when you apply force to the muscle through its range of motion while the muscles contractile force is less than the applied for causing the muscle to stretch. The farther you stretch  the [muscle](http://puu.sh/5qXWU.jpg), the less myosin heads will be able to bind to the thick filaments and the amount of force the muscle can produce rapidly decreases as the stretch progresses.

This [article](http://muscle.ucsd.edu/musintro/contractions.shtml) explains it pretty well.",null,2,cdl90c6,1r9a7m,askscience,new,2
Daegs,"no one says it ""must"" be. 

There are 4 forces: strong, weak, electromagnetic and gravity.

The first 3 are all very similar, and in fact the weak / electromagnetic have been joined into a single force, the electro-weak. It *seems* that the strong also fits in very well mathematically, and it is expected in the coming years / decades that we could actually integrate all 3 (strong, weak, electric) into a single force. This is called ""Grand Unification"":

http://en.wikipedia.org/wiki/Grand_Unified_Theory

So the problem is asking why are 3 out of 4 of the forces so similar to the point where they can actually be shown to be the same underlying force, while gravity is such an oddball and so much weaker. 

One *possible* explanation is that all 4 of the forces are equal in strength, and could be unified into a single force, while gravity is special because it ""leaks"" its force to an unseen dimension. This would allow it, in theory, to be unified with the other 3 forces.

There are other explanations, including that gravity is an emergent property of our universe (such as holographic universe theory) rather than a fundamental force.

Or, it could simply be that gravity is fundamentally different from the other 3 forces, which would just be troubling for physicist.

Without an explanation such as extra dimensions or more exotic behavior we haven't seen, there isn't going to be a way to unify gravity with the other 3 forces.",null,4,cdkxnpf,1r9c24,askscience,new,17
iorgfeflkd,"I'm not sure I understand your question. Did your teacher say that gravity needs to be as strong as the others, despite not being?

It is thought that at extremely short distances, there is so much energy from interactions of other forces (consider two electrons very close together for example) that the energy itself starts to gravitate. At these scales (typically, Planck scales), gravity would overwhelm other interactions. However, we don't have any way of testing this.",null,0,cdkwv4t,1r9c24,askscience,new,2
theoreoman,"Gravity is a very weak force and  it takes an entire planet to counteract an electrical charge on a small object.  This thing is no one knows why exactly why yet, there are theories but one of the holy grails of physics it to figure out how gravity relates mathematically to the other forces ",null,1,cdkwzlp,1r9c24,askscience,new,2
OverlordQuasar,"Essentially, it seems out of place. We have 2 forces that we have demonstrated via experimentation to be part of one underlying force (electromagnetism and the weak force becoming the electroweak force), another that has been mathematically shown to be part of that, which is awaiting experimental confirmation which requires a bit higher energy particle accelerators (strong force). All these are part of one thing, but gravity isn't. It is so much weaker, and refuses all efforts to unify it with the others mathematically without resorting to things on the level of higher dimensions.

TL;DR It's out of place to have 3 forces that are easy to unify and one that is much weaker and appears to be completely seperate.",null,0,cdl2y71,1r9c24,askscience,new,1
nairebis,"That's all in the BIOS (Basic Input/Output System), which is a program stored in a special chip on the motherboard. That provides a standardized interface between the hardware and the operating system, which is why you can load Windows on any Brand-X motherboard. OS/X has the same concept, though of course Apple only officially supports certain motherboards.

How much power it uses would be dependent on the motherboard design, but if it uses electronic starting, then it uses some small amount of power to monitor the button. I believe all modern motherboards do it this way. In the relatively distant past, they used to use a mechanical on-off switch, but there were a lot of advantages to electronic power on/off, so that became standard.

Whether you can use a different key, etc, is whether that feature is programmed into the BIOS. Here is a page that describes typical BIOS settings:

http://www.tomshardware.com/reviews/bios-beginners,1126-8.html

BIOS settings are usually accessed by pressing a special key when you start up the machine, but before it starts loading the operating system. I've typically seen the DEL key, the F2 key and the F8 key. I don't know how Apple does it. If it works like how Apple does other things, then they picked some arbitrary key that no one else uses. :)

Edit: As haikuginger pointed out, the more modern version of BIOS is EFI, which allows changing some settings from within the Operation System (particularly in the case of Macs). But the principle is the same, the controlling program here comes from the motherboard firmware.",null,0,cdlach2,1r9c6s,askscience,new,4
glarn48,"Great question! There's been a lot of investigation into biological differences related to depression; much of this work (at least that I'm familiar with) is related to hormonal differences. However, you're asking specifically about structural differences, so I'll give you an example from morphometry, though this admittedly is potentially related to hormone dysregulation.

Many studies have shown morphological differences is in the size of the anterior cingulate cortex and amygdala, among some other areas (see meta-analysis http://www.sciencedirect.com/science/article/pii/S0165032711001480). The ACC is involved in affect regulation and motivation, two areas which are impaired in major depressive disorder (MDD). The amygdala is an important area for emotional learning as well as fear and aggression. 

The decreased size of these areas may be due to dysregulation of the HPA axis which controls the release of gluccocorticoids, a hormone associated with stress. Past studies have demonstrated a link between early childhood stressors, adult brain morphometry, and the course of MDD (see http://www.sciencedirect.com/science/article/pii/S0022395610000154 and http://www.ncbi.nlm.nih.gov/pubmed/16616722). 

It's important to think about the ontology of MDD then not as someone simply having a different brain, though that may be the case. Rather the course of MDD may be dependent on a number of biological (e.g. genetic), developmental, and situational factors which interact to bring about the disorder. One must consider factors like early childhood experiences, genetic predispositions, and recent traumas, which may lead to hormonal dysregulation (say, of the HPA axis), which may culminate in structural differences.",null,4,cdl2rih,1r9dom,askscience,new,18
RelativisticMechanic,"Note that when dealing with roots and complex numbers, you actually get multiple results. For example, -1^(1/2) actually has two values:

1. i
2. -i

Similarly, -1^(1/3) has three values:

1. -1
2. cos(π/3) + i\*sin(π/3)
3. cos(π/3) - i\*sin(π/3)

Finally, note that 1/2.5 = 2/5. Thus, (-1)^(1/2.5) = (-1)^(2/5) = 1^(1/5). We therefore need to find the fifth roots of 1. There are five of them, and they are

1. 1
2. cos(2π/5) + i\*sin(2π/5)
3. cos(2π/5) - i\*sin(2π/5)
4. cos(4π/5) + i\*sin(4π/5)
5. cos(4π/5) - i\*sin(4π/5)

Any of these numbers satisfies x^(5/2) = -1 (and x^(5/2) = 1), since they all satisfy x^5 = 1, and -1 satisfies -1^2 = 1.",null,44,cdkxquq,1r9elk,askscience,new,334
Tsien,"To elaborate on what RelativisticMechanic wrote, the roots come from Euler's formula: e^(ix) = cosx + isinx. If you're familiar with Taylor series, this formula can be derived from taking Taylor expansions around x = 0 for e^(ix), cosx, and sinx:  

e^(ix) = 1 + ix - x^2 /2! - ix^3 /3! + x^4 /4! + ix^5 /5! - ...  
cosx   = 1 - x^2 /2! + x^4 /4! - x^6 /6! + ...  
isinx   = ix - ix^3 /3! + ix^5 /5! - ix^7 /7! + ...  

So, using this formula, we know that e^i\*2kpi = 1 where k is an integer. So 1^(1/5) becomes

e^i\*2kpi/5 = cos(2kpi/5) + isin(2kpi/5)

Using k = 0, 1, -1, 2, -2 (in that order) gives RelativisticMechanic's answers. Notice that since cos and sin are 2-pi periodic, if you try to use other values of k, you'll end up getting one of the 5 answers already listed.",null,3,cdkyeo1,1r9elk,askscience,new,15
regnirps,"This should help you out: [Graph of (-1)^x in Wolfram Alpha.](http://www.wolframalpha.com/input/?i=%28-1%29%5Ex)  This graph show the real vs. imaginary parts of the powers of (-1).  Notice that (-1)^(1/2.5) is in the part of the graph with nonzero real *and* nonzero imaginary parts!

In general, I think your question has to do with the properties of the function f(x) = a^x, but I'm not exactly sure what explanation you want for ""why"" beyond the graph linked above.",null,5,cdkx2fk,1r9elk,askscience,new,17
SidusObscurus,"For this question, you have to compare an inverse function, as defined by convention, with the solutions to an equation involving a function that is not one-to-one (invertible).

In order to define a function, you need exactly one output for each input. For possible multivalued functions, we select one solution completely by convention (sqrt function, inverse trig functions, and many others).

Solving x^2 = -1 has two answers, +i and -i, both imaginary.
Solving x^3 = -1 has three answers, e^(i*pi/3), -1 = e^(i*pi/3 +2*pi/3), and e^(i*pi/3 +4*pi/3).

To define the inverse functions of these equations, we can only pick one of these answers, so we pick one answer entirely based on mathematical convention and general agreement. Typically we pick the real number solutions first if we can (ex. cube root), positive part solutions after this if we can (ex. sqrt), and if it is still multivalued pick the solution set that intersects 0 if we can (ex. inverse trig functions). This defines the inverse functions x^(1/2) and x^(1/3). If we cannot do any of those, it's less clear what we pick, and mathematicians will usually explicitly state the solution branch they are using for their math, so no one is confused.

What about x^(2.5)? Once you have defined all the integer rational roots and powers, the standard convention is to interpret x^(5/2) as either of the two equal expressions,
sqrt(x^5) = sqrt(x)^5.
There is no ambiguity in these expressions. We could also solve
x^(2/5) = -1, which would have multiple solutions. Our solutions would be
e^(i*pi/2.5 + 2*pi*k/2.5) for each integer k.
See [Roots of Unity](http://en.wikipedia.org/wiki/Root_of_unity) and [Euler's Identity](http://en.wikipedia.org/wiki/Euler%27s_Identity) for more information. In this case, we would only have 5 total solutions, due to the 5th root.

For irrational numbers, everything gets a little more complicated, as irrational powers of negative numbers aren't very well defined. But that's another issue entirely.

*Edit: Corrected a really silly mistake of mine.*",null,0,cdl8h2z,1r9elk,askscience,new,3
Borlaug,"When the exponent is a fraction, they want you to find the root of the coefficient using the denominator as the radical. 

For the first one, no known number when multipled by itself results to -1. In other words, the square root of -1 is imaginary. This imaginary is represented by the letter i. 

The second one is asking for the cube root of -1, which is -1. i. e. -1*-1 *-1=-1",null,3,cdl6se0,1r9elk,askscience,new,5
ekohfa,"Solar photovoltaic panels create current due to the [photoelectric effect](http://en.wikipedia.org/wiki/Photoelectric_effect).  Incoming photons cause electrons to jump across the energy ""band gap"" in a semiconductor, typically made of silicon.  The silicon consists of a positively doped layer and a negatively doped layer, just like a diode.  Metal leads are applied to each side to allow electrical current to flow to an external circuit.

Edit: To be clear, the metal leads are not oxidized; they are present only to carry the current created in the semiconductor.",null,2,cdkynzl,1r9es2,askscience,new,4
hal2k1,"&gt; Where are these electrons sourced from?

This is a misconception. Electrical current is charge flowing in a [circuit](http://en.wikipedia.org/wiki/Electrical_circuit).

&gt; An electrical circuit is a network consisting of a closed loop, giving a return path for the current.

A circuit is a [**loop** of conductor](http://en.wikipedia.org/wiki/File:Ohm%27s_Law_with_Voltage_source_TeX.svg). 

The carriers of charge are normally electrons (as your question suggests). The electrons are already part of the conductor. A current is merely a flow, or movement, of charge (electrons) around the loop of conductor (the circuit).

The solar panel merely converts photons into a ""push"" for the charge carriers (electrons), like a pump does for a fluid. The solar panel then ""pushes"" the electrons (which are already in the conductors) around the circuit. In electrical circuits, we do not call this pushing force ""pressure"", but rather we use the term ""voltage"".",null,0,cdl4dv3,1r9es2,askscience,new,1
jayd42,"My understanding of current is that the 'flow' is actually photons moving between electrons as the electrons change energy levels, gaining photons to raise in energy and losing photons to lower in energy, and not the physical movement of electrons through a material.

From this understanding, solar cells become very easy to understand from a non technical point of view. The cell gets hit with photons raising the energy level of the electrons in the cell and instead of reflecting the photons back as light the photons are redirected into an electric circuit as current.

I'm sure that's not 'exactly' right but close enough is good for me.
",null,0,cdl8979,1r9es2,askscience,new,1
iam_sancho2,"A solar cell is made of layers of different materials. The top layer will be some kind of anti-reflection surface. The next region is a thin layer of n-type material followed by a larger region of p-type material. The juxtaposition of these two differing materials creates an internal electric field. 

When a photon with sufficient energy enters the inner material, it generates electron-hole pairs, which are immediately swept to apart from each other by the internal electric field. With the electrons going one way and the holes going another, a photocurrent is generated within the solar cell. 


There is no net loss of electrons in the material. ",null,0,cdlif43,1r9es2,askscience,new,1
clever_cuttlefish,"I think you're mistaken about the current.

Electrical current is a flow of electrons, but no electrons are created or lost in the panel. The energy carried from the photons creates a voltage, which causes the electrons to move around, but none are created.

Think of it like this: All the electrons are happily bound to their atoms, but the photon can bump into one and knock it out of place. However, all the atoms and electrons this has happened to would much rather that another electron comes to fill it's place. So electrons move across the panel to fill in the gaps. This flow of electrons is the current, and is what we get the energy from.",null,3,cdl029v,1r9es2,askscience,new,3
Tacomelt,"I can give you a short run down.

Light emits photons, the photons are captured and piled up on the plate causing a voltage.  A current is then formed from the resulting voltage producing electricity.  The electricity is stored into a capacitor or battery system.

The electricity is a direct current, most systems go through an alternator changing the dc to ac.  It is now capable to power your home.

",null,4,cdkyc5n,1r9es2,askscience,new,2
iorgfeflkd,"Protons and neutrons are made of quarks, held together by gluons. Electrons and quarks, to the best of our knowledge, are fundamental, and so are neutrinos. With better experiments this understanding can change, but right now as far as we know, certain particles are fundamental.",null,2,cdkxbap,1r9g54,askscience,new,14
frogdude2004,"A balloon is inflated when the pressure inside the balloon is greater than the outside: the air inside the balloon's walls stretches it while it pushes back. This continues until the combined force of the balloon and the air outside the balloon equals the force from the air inside the balloon. If the pressure inside the balloon equals the pressure of the outside of the balloon to start, then the balloon exerts no force: it is 'deflated'. If you were to put a balloon that was filled to a certain pressure into a box with the same pressure and then let the balloon open, the sum of the balloon and outside air force would be greater than the force inside, and it would then contract until deflated. I hope this answers your question. If it wasn't clear, please let me know.",null,0,cdl8ze2,1r9hf4,askscience,new,3
Naf623,"No, it cannot stay inflated without something blocking the neck. The rubber skin if the balloon will always be stretched and exerting a force to expel air. 
There is one exception; if the neck on the ballon is open to a source of gas, but the rest of the balloon is placed at a sufficiently lower pressure, then the balloon could be inflated by suction. ",null,0,cdlaggw,1r9hf4,askscience,new,2
the_dan_man,"http://en.wikipedia.org/wiki/Tunica_media

Muscle cells are arranged in a circular fashion around the vessel's interior. When these cells contract, the lumen (inside space) of the vessel grows smaller. It's a radial-ish contraction.",null,1,cdl4a5l,1r9ilc,askscience,new,2
the_dan_man,... the same way any cultivated crop survives predation. Human intervention.,null,1,cdl45l4,1r9omj,askscience,new,2
glittercheese,"People taking statins (cholesterol lowering medications) are often told to avoid grapefruit and grapefruit juice because consuming grapefruit can cause higher-than-expected levels of the drugs in the blood. Statins are metabolized in the liver by the same type if enzymes that metabolize grapefruit. If these enzymes are busy processing grapefruit, they are unable to metabolize the drugs, leading to more of the drug circulating in the body. This can cause an increase in adverse side effects of the drugs. 

I think the reason other fruits don't cause the same effects is that they are not metabolized by the same liver enzymes. 

This isn't overly scientific but I am a nurse and I wrote a short pamphlet on this subject when I was in nursing school. ",null,2,cdlao57,1r9qsz,askscience,new,8
justin3003,"Glittercheese gives a good basic overview of how the process works. However, to add, a substance called bergamottin and the related 6,7-dihydroxybergamottin are thought to be the culprits present within grapefruit specifically. 

These chemicals are potent inhibitors of CYP3A4, which is part of the cytochrome P450 family of enzymes. These enzymes are responsible for liver detoxification of substances in the blood (generally by making them more soluble for excretion in urine or bile). The exact mechanism isn't important, but what is important is that these guys are potent inhibitors of CYP3A4 and thus prevent CYP3A4 from detoxifying a whole host of important things, including a large number of pharmaceutical drugs. Given that many drugs have a fairly narrow therapeutic index (blood concentration range at which they are safe and have an effect), inhibiting their liver metabolism can cause blood levels to rise to toxic or even deadly levels quickly. Also, while they aren't the strongest inhibitors of CYP3A4, they are the most readily available, common, and seemingly innocuous and thus potentially dangerous.  

There is a good list of things CYP3A4 metabolizes at the bottom of this wikipedia article (under substrates): http://en.wikipedia.org/wiki/CYP3A4",null,0,cdli08l,1r9qsz,askscience,new,2
iorgfeflkd,"You'd have to sum up the contribution from each height.

If you assume a constant density and cross section ( which isn't exactly true for a person, but you could make it more complicated if you want) would be the integral from r=6380 km to r=6580 km (or whatever) of GMdA/r^2 by dr where G is the gravitational constant, M is the mass of Earth, d is the density, A is the cross sectional area, and r is the radial position.

For heights much less than the diameter of the Earth, just using mg isn't too too wrong.",null,0,cdl94jj,1r9r7m,askscience,new,4
goingforth,"If we're going off of your theoretical proposition, the solution would be rather simple. If only half of the person's mass is influenced by gravity, then they would consequently have half their original weight. However, that concrete of a border doesn't exist. The above comments describe the mathematics of the issue, but the conclusion will be that, assuming the person's head is around low earth orbit, their weight will only change by a very small amount, as acceleration due to gravity doesn't change very appreciably from the surface to orbit (about 9.7 rather than 9.8) This effect will, of course, be amplified if the person's head is, say, on the moon, and will thus require definitive calculations in order to come to an accurate conclusion. ",null,0,cdlq4ba,1r9r7m,askscience,new,1
NotFreeAdvice,"hmmm...well, it depends on the reaction that you are interested in.  Multiple things can happen.

1) You can just denature proteins at high/low pH.  It is *very* common to ""cook"" fish in acids (like lemon juice) or bases (such as lye).  These foods would be called ceviche or lutefisk, respectively.  
2) You can hydrolyze bonds that are holding in the flesh together, in which the acid or base acts as a catalyst for this reaction.  

There are probably more reactions of interest, but as a chemist, these are what immediately comes to mind.  ",null,0,cdl81pd,1r9zbg,askscience,new,3
Jyesss,"I think the question that you are getting at is ""what happens at the sub-cellular, or protein level, when a strong acid breaks down flesh?"" Proteins are made up of amino acids, and the way that these amino acids fold in 3-dimensional shape largely determines their function and appearance. Their shape depends on the stabilizing interactions between positive and negative charges on the amino acids, on hydrophobic interactions, and on hydrogen bonding. A strong acid increases the concentration of hydrogen ions and thus affects the protonation/deprotonation status of the side chains on the amino acids. The correct folding of the protein will be interrupted if a group that is normally deprotonated at physiological pH is changed to being protonated, thus resulting in the change in appearance. This same thing also happens when the temperature is increased, increasing the vibrational energy in the bonds which can eventually overcome the stabilizing forces at play. This is why an egg looks different when cooked. ",null,0,cdlf7c8,1r9zbg,askscience,new,1
inmate992,"Self renewal is the ability of cells to continually replicate - preserving genetic information in all subsequent daughter cells. Stem cell reservoirs are usually tightly regulated in the human body to prevent them becoming cancerous.

As for the difference between Autologous and Embryonic, embryonic stem cells are much more pluripotent ie given the right external environment they can assume any cell type (eg cardiac, neurons, immune cells etc).
Autologous stem cells on the other hand are often precursor cells, for example OPC (oligodendrocyte precursor cells) cells are cells that are able to differentiate into immune cells, but their cellular fate is already decided, so I suppose they lose their pluripotency to an extent as their fate is already decided.

I hope this makes sense!",null,0,cdlgfhw,1ra41i,askscience,new,2
Osymandius,Could you clarify your question slightly? Partially differentiated haematopoietic stem cells in my hands happily divide lineally for 50 rounds of divisions in culture with no change in genotype or phenotype. eSCs are similarly capable. ,null,0,cdlayhh,1ra41i,askscience,new,1
bohr_exciton,"That's actually a tricky question. The most direct explanation is that ice is slippery because it behaves as being wet, i.e. that there is liquid water between the bulk of the ice and an object gliding on the surface. However, what causes ice to be wet with respect to objects moving on it is still under debate. There was a Physics Today [article](http://scitation.aip.org/content/aip/magazine/physicstoday/article/58/12/10.1063/1.2169444) that came out a few years ago that neatly describes possible explanations. To briefly summarize them, they are as follows:

1) Pressure melting. Due to the fact that ice (at least the common form) is less dense than water, applying pressure reduces the melting point of the ice. To apply this example to say a skater, the idea is that a skate bay locally increase the pressure to a sufficient extent that the ice can melt, making it slippery locally and the water then refreezes after the skate passes. This is the most common explanation that has been invoked historically, but the problem is that it's not clear whether this additional pressure can be enough to melt the ice.

2) Frictional heating. When objects move across a surface there is (virtually) always some friction which results in local heating. Again, this heat may be enough to melt the ice.  This explanation, however, can't really explain why ice is slippery even for stationary objects immediately before moving. 

3) Ice may be intrinsically wet. In describing solids, we often tend to ignore surfaces to a first approximation, because this simplifies the description, but surfaces can behave very differently from the bulk. In the case of ice, it's been speculated that the different local environment of the first few layers of water molecules may result in this molecules being less strongly bound to the bulk than is the case for molecules within the body of this crystal. Because of this, these surface layers may behave as being liquid, which would then cause the ice to be slippery under all circumstances. ",null,1,cdl7ngd,1ra493,askscience,new,8
stillealles,"Because either 
a) a thin amount of water is on top (in the case of ice skating and such)
Or 
B) when you apply pressure to ice, it causes it to melt because of properties of water (this also applies to ice skating and such, but is easier to think about if you have a piece of ice on the floor and step on it and slip) 
Veritasium has a good video on it ",null,0,cdl8vq4,1ra493,askscience,new,1
jericho,"This is one of those great questions that results in arguments around the water cooler in the Physics Dept. Short answer; we don't know. [Here is a nytimes article that cover some bases.](https://www.google.ca/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=8&amp;cad=rja&amp;ved=0CFwQFjAH&amp;url=http%3A%2F%2Fwww.nytimes.com%2F2006%2F02%2F21%2Fscience%2F21ice.html&amp;ei=nPKQUt78OoXMqgHX2oGwCw&amp;usg=AFQjCNH9Hpt9VbWa5zPA0g36hCt64d71cQ&amp;bvm=bv.56988011,d.aWM)
",null,0,cdlac1j,1ra493,askscience,new,1
Ejb90,"The auroras depend on two main things:
1) Solar activity
2) latitude

Solar activity produces the solar wind - a stream of charged particles from the sun. These interact with the Earth's magnetic field, being drawn up along the field lines to where they intersect the Earth, at the poles. The interaction of these particles produces the lights. There are more particles when the activity is greatest. At the moment we are nearing a sunspot maximum, so the activity should be greater.
The Forster North/south the more field lines, so the more prominent the lights are.

The lights fluctuate daily, but there is a good chance of seeing them. Going in winter doesn't make too much of a difference, though it is darker for longer so they will be clearer for longer.",null,0,cdli1qs,1ra4dm,askscience,new,2
now_you_listen_here,"I'm going to start by explaining it very simply, and then if you want more details I can expand on it!



As far as the part about being non-reactive to CMV (cytomegalovirus), that simply means that you do not have antibodies to the virus.  This means that you have not been exposed to CMV before.  It doesn't mean that your blood ""type"" is non-reactive; we don't speak in terms of blood *types* being reactive or non-reactive to a virus.


As far as your O(-) status, I'm afraid she gave you some misinformation.  That does not mean that your blood is ""good"" for newborns.  It doesn't mean it is ""bad,"" either.  The (+) or (-), as you probably know, is referring to the presence or absence of Rh factor, which is a protein on the outside of red blood cells (you are either Rh+ or Rh-).  You DON'T have it.  Therefore, if your blood was exposed to blood that DID have it, your immune system could form antibodies against it, because it is recognized as something that is ""foreign"" to your body.


The importance of this would be if you would become impregnated by a male who is Rh(+).  In that case, the baby might inherit the gene from the father and also be Rh(+).  Therefore, if your blood (which is Rh-) was exposed to the baby's blood, whether during birth or some event during the pregnancy where bleeding occurred, your immune system could recognize the Rh factor on the baby's red blood cells and form antibodies, which would allow for an immune response at sometime in the future.  Your first baby would be just fine (the immune response takes too long that first time for it to put that baby in danger), but any babies you have after that are also Rh+ would be at risk of your immune system attacking their red blood cells, something we call [hemolytic disease of the newborn](http://en.wikipedia.org/wiki/Erythroblastosis_fetalis). 


There is good news, though!  We have developed something called Rhogam, which can prevent all this!  It's an injection that basically suppresses the mother's immune response to the Rh factor.  We can give it to pregnant women during their pregnancy and prevent the bad stuff that might have happened otherwise.  Science is great, isn't it?
",null,0,cdl8582,1ra6bd,askscience,new,6
abbe-normal1,"I'm going to expand on /u/now_you_listen_here since there are a few points I believe they didn't explain as well as could be.  CMV non-reactive as previously stated means you haven't been exposed to CMV.  The reason this is important is because CMV while causing a relatively minor infection in healthy adults is considerably more worrisome for immunocompromised individuals and babies.  See more information [here](http://www.mayoclinic.com/health/cmv/DS00938/DSECTION=symptoms).  The reason your blood is good for these individuals is that you can transmit the virus to them through a blood transfusion.  See [here](http://www.mayomedicallaboratories.com/test-catalog/Clinical+and+Interpretive/62067) again for more information.

As for O-, again as stated previously you lack the Rh factor that can cause an immune response in a recipient of a blood transfusion.  Your blood isn't just 'good for babies' (that's the CMV- part) you are actually known as the universal donor.  Your blood can be given to anyone regardless of blood type because you not only lack the Rh factor, but you also won't react with A or B blood because you being O lack those antigens as well.  Therefore your blood can be given to anyone A, B, AB or O without a life threatening reaction from their body.  In an emergency O- is given to a patient when there isn't time to check their blood type or until cross matched blood is available.  Also, others with your blood type can only receive O- so blood is harder to get because it is rarer.  

TLDR:  GIVE BLOOD!  O- blood is always in need and you would do a great service by regularly donating your blood to help others! ",null,0,cdl8f7o,1ra6bd,askscience,new,3
user31415926535,"""CMV non-reactive"" means that your body has never developed antibodies to cytomegalovirus (CMV) - that is, negative for both CMV-[IgM](http://en.wikipedia.org/wiki/Immunoglobulin_M) and CMV-[IgG](http://en.wikipedia.org/wiki/Immunoglobulin_G). A positive CMV-IgM test would mean you have a current CMV infection. A positive CMV-IgG test would mean that you had been infected some time in the past. What's important is that if you have been infected some time in the past, you still have some level of the virus in your body; a negative CMV-IgM test doesn't mean that you are entirely free of the virus. 

[CMV is an extremely common virus](http://www.cdc.gov/CMV/index.html) in humans; worldwide, 40% of adults have antibodies to CMV. [It's not particularly dangerous to healthy adults](http://www.mayoclinic.com/health/cmv/DS00938) (though it is implicated in some cancers and rare syndromes). If you notice an infection at all, it's usually similar to [mono](http://en.wikipedia.org/wiki/Infectious_mononucleosis). But to humans with undeveloped immune systems - newborn infants or immunocompromised people - it can be deadly. [In infants, CMV can cause blindness, deafness, neurological deficits, even death.](http://en.wikipedia.org/wiki/Congenital_cytomegalovirus_infection) 

So we need to be sure that blood used for infants has no trace of CMV in it. Hence, we look to people like you as a source of CMV-negative blood. 

[*Source: I'm not a medical professional, rather I'm an immunocompromised adult who has to know these things to survive.*]",null,0,cdl8jtj,1ra6bd,askscience,new,1
Izawwlgood,"To your general question, yes! A number of groups are working on culturing induced pluripotent stem cells into whole organs, by either decellurizing pig equivalents and seeding the collagen matrix with the iPSCs, or causing the tissues to grow onto some other matrix. It's pretty cool and exciting work!

But as to why some transplants need to be grafted to a person; the host body will facilitate the vascularization of the tissue, and 'feed' it, which will allow it to grow and remain healthy. At a later date, when the graft is ready, surgeons will move it. 

Ever seen someone with a crush wound have their hand stitched to their chest or side? It's to promote blood flow into the damaged tissue. Similar idea.",null,0,cdl8bvk,1ra6bi,askscience,new,6
KarlOskar12,"The reason they grew the nose/facial skin on the person is because a complex medium is required to grow human tissue. It *can* be done in a lab, but it is much easier to attach it to the person and use their blood supply to get all the required nutrients for growth.

As to your main question: we will absolutely be able to have organ farms. Take this [mouse](http://en.wikipedia.org/wiki/Vacanti_mouse) for example. A live host is - for now - the easiest way to do it but that most definitely does not have to be the case.",null,0,cdl8szw,1ra6bi,askscience,new,2
iorgfeflkd,"Matter doesn't contain gravitons, and if gravitons are used to describe gravity they are neutral particles that are their own anti-particles.

Anti-matter is expected to behave normally in gravitational fields, although it's hard to get enough of it in one place to test this. There is an experiment at CERN working on it, [here is their 1990s looking website](http://aegis.web.cern.ch/aegis/).",null,1,cdl9d7n,1ra79d,askscience,new,14
stevenstevenstevenst,"As explained, antimatter will behave classically with respect to gravity.  If, however, you want something with the repulsive properties described, you need negative energy (or, correspondingly negative mass).  While this is a rather abstract idea, negative energy has in fact been observed-notably in the Casimir effect.  

In another example, lasers are reported to produce energy in an oscillation of positive and negative energy.  Assuming this is correct, it is easy to imagine a series of rotating mirror which could then separate the negative and positive energy.  This is the beginning of a discussion on how to expand a singularity in a rotation wormhole blahblahblah time travel etc.  For this process it would be necessary to isolate negative energy.

http://en.wikipedia.org/wiki/Casimir_effect",null,0,cdmfqwz,1ra79d,askscience,new,1
dirtyburger8,"Let's first take a look at what the definition of an organ is. ""An organ can be defined as is a collection of tissues joined in a structural unit to serve a common function."" The skin serves to protect our body from bacteria and infection. Now let's take a look at the individual layers of the skin. There is the stratum corneum. This layer is the outer ""dead"" layer of skin. This can be thinned or shed naturally or by scrubbing your skin. Then there is the epidermis (outer layer), dermis (middle layer), and hypodermis (deep, inner layer that lays next to the muscle tissue). The epidermis contains 4 different layers and contains many immune cells to protect from the outside, melanin for skin color, but mainly to protect from the harsh environment and exposures. The dermis layer is the layer that contains collagen and elastin. These proteins are responsible for the support and elasticity that we see with our skin. When someone gains a large amount of weight, the fat stretches the skin, but the amount of elastin stays the same. The fat / elastin combination allows the skin to stay ""normal."" When someone loses a large amount of weight, there isn't enough elastin to support the amount of skin that has been produced due to the fat stretching it out. The skin isn't good at producing elastin, neither is the body. Hence all the skin products that claim ""elastin"" will help restore the natural beauty of your skin. Elastin is a large protein which has difficulty penetrating the skin and being absorbed. 

TL;DR: elastin is a protein responsible for stretching of the skin. Our body sucks at making more. You stretch your skin when you get fat, you lose weight and there isn't enough elastin to allow it be tight and form to your body.",null,7,cdl9art,1rae6m,askscience,new,20
null,null,null,7,cdl88u6,1rae6m,askscience,new,20
Phunky_Munkey,"soo many links.. basically, skin as an organ was not built for rapid weight gain(stretch marks) or weight loss(droopy flesh).. Your skin is engineered to stretch over a slowly growing skeletal system.  It does eventually reform itself but at a much slower scale.. that of simple body maturation(you stop growing in your mid-late teens). It does do this through shedding of epithelial layers but that again is a lengthy process and new skin cells can only be formed on that layer which is elastic and retracts to body size but very slowly so.",null,0,cdlksij,1rae6m,askscience,new,1
null,null,null,3,cdlc4wt,1rae6m,askscience,new,1
LukeSkyWRx,"Powders with a spherical morphology that can slide past one another will behave like a liquid even with a rather large particle size. I have some spray dried silicon nitride powders at work that are ~30-40 um spherical agglomerates and you would think I was pouring liquid if you saw it come out of the bottle.

The problem when you grind is that you get coarse/angular particles that do not flow well, in addition as the particles get smaller and smaller the surface interaction become so strong that they start to stick together and agglomerate really badly. This agglomeration and self attraction is a big hurdle for commercial nanotechnology. In addition powder flow behavior is a very big deal for ceramic processing, if you are dry pressing parts you want the powder to flow into your mold well but not fall apart when you press it so some balance is needed when engineering your powder system.",null,843,cdlaovd,1raftj,askscience,new,2816
some_generic_dude,"This is already done with sand ground for glass. They call the product ""flour"" and a bucket of it flows and jiggles like a liquid when you shake it. 

You must wear special breathing protection when you handle it, because of the silicosis hazard.  It is both fine and, under a microscope, sharp. It gets around your body's particle protection(cilia in your bronchial tubes) because it's so tiny, and/or cuts its way through. When it gets into your lungs, it starts cutting the sacs in your lungs, and you eventually die either of hypoxia or exhaustion from struggling so hard to catch your breath.

EDIT: You can go to a waterproofing supply place and buy a bag of Quick-Gel brand bentonite, which is a mix of ground Fuller's Earth and fine silica, and see the behavior for yourself. Just wear good breathing protection. Those flimsy surgical masks or rubber-band white masks that they sell for construction will not suffice. You need the kind that gets a good seal on your face, the kind that usually offers organic vapor protection. They either have particle protection by default, in addition to the vapor protection, or you can slip a little pad into the filter chamber. 

Don't take it lightly. My brother-in-law works at a plant where they make this stuff, and, over the years, he has known a dozen or so people who have died this way, by going into a room full of the dust without their protection. Sometimes it kills in hours, sometimes months of agony. Nothing short of a full lung transplant can save you once you get a lungful in you.

EDIT2: udser=under, king=kind",null,22,cdl8mfx,1raftj,askscience,new,117
Primal_Pastry,"Chemical Engineer here, a way you can make certain granulated chemicals behave like a fluid (for reaction purposes anyway) is with a fluidized tank reactor. Essentially, you pump a gas through the bottom of the particles and the flow counter acts gravity, allowing the particles to flow around similar to a liquid.

http://faculty.washington.edu/finlayso/Fluidized_Bed/FBR_Fluid_Mech/packed_beds_scroll.htm",null,14,cdlb9od,1raftj,askscience,new,75
Oznog99,"The weirdest solid I know of is glass microballoons used as epoxy filler.  They're literally microscopic glass balloons.  I have a clear plastic gallon tub of them and the container feels empty.  

Shake the tub and the contents not only forms waves that ""ripple"", once you stop shaking, it takes about an extra sec or so for the waves to stop rippling back and forth and it all comes to a stop.  

They do have friction against one another and that makes it lossy and limits how minor a motion can be before it can't push the pieces out of place.  So it ""freezes"" in place and a ripple stops abruptly once it's too small, rather that displaying seemingly infinitely smaller ripple motions like water.


",null,7,cdlic1w,1raftj,askscience,new,31
TheTrevorGuy,"This is youtube video with a university professor explaining such an experiment. (they used very fine glass beads to represent sand)

[Granular Jets (slow motion)](http://www.youtube.com/watch?v=Nt4jzVUEJjo)

as you can see it behaves as a liquid to an extend. However due to lack of surface tension it will not have fluid like properties.

I hope this helps, because the comments here are making me cringe.",null,2,cdl9sks,1raftj,askscience,new,19
cohesive_friction,"Chemically, sand particles will not act as a liquid, but mechanically they can. There is an entire field of modeling for Computational Fluid Dynamics (CFD) for granular materials. Basically if your domain is very large as compared to your particle size, you can model granular material as a liquid with cohesion and frictional properties.

https://www.youtube.com/watch?v=ejdh9Ye9IDM",null,5,cdldi1w,1raftj,askscience,new,13
BroscientistsHateHim,"isn't one of the fundamental principles of something being liquid that its particles follow a random walk even when free of external force.

Lots of folks here are saying it is possible, but I've never heard of a solid being so finely ground that its particles do random walks. Convection would be almost nonexistant as well which is pretty important for liquids.",null,0,cdl8ok4,1raftj,askscience,new,7
yikes_itsme,"Generally, no.  ""Sand"" is primarily considered to be a polymeric mass of silicon dioxide chains, essentially chemically same as common glass.  If you reduced the particle size enough, it would turn from sand into a very fine powder.

To see what happens when you reduce the particle size further, you have to turn to chemistry.  Side note:  you can't just grind solids straight into a liquid; the two are different phases of matter which occur at distinct temperatures and pressures, and so you usually have to go through a phase transition...unless you're doing a thought experiment like we are.

What you might imagine you'd end up at the end of your size reduction is [silicic acid](http://en.wikipedia.org/wiki/Silicon_hydroxide), which is a single unit of what forms the silica glass which makes up sand.  I believe this might act as a proper liquid, but this material quickly polymerizes into a solid through condensation reactions, so in a normal environment you wouldn't be able to simply reach a state where you have liquid sand.  Even with very small pieces of silicon dioxide, the material will still act as a solid (c.f. fumed silica size 50-500A).

I sense that your question might be more about what makes a substance form a liquid versus a solid, but that's all I have for now.",null,4,cdl8sip,1raftj,askscience,new,6
nofivehole,"Lots of people are saying now and that is true if you are just grinding up the solid. However, just by adding air current you can 'fluidize' a particle bed and basically make it appear to have 'fluid'-like properties. Look up fluidized bed. I think the problem is that the solid particles themselves would have too much friction between them, but with just a little space added, which is easy if the particles are small and with a little gas blowing through it, the solid would spread out and start acting much like a fluid. ",null,0,cdl9183,1raftj,askscience,new,3
polyquaternium10,"One way to explore this is to use a rigid body dynamics model applied to a large number of particles then observe the system's behavior. For this video I was more interested in simulating with forces between the grains of sand. Adding slight attractive force between grains (with no friction) behaved like a viscous liquid:
http://www.youtube.com/watch?v=zsfm4xlm6cA",null,1,cdlb3sf,1raftj,askscience,new,4
whiskey_and_cigars,"I didn't see this posted in here, but sand DOES behave like a fluid under certain conditions.  Notably, during a seismic event.  This is an effect known as liquifaction and can be devastating to any structures built on top of or above areas where liquifaction occurs.  This is a major component of structural engineering and foundation design, especially for tall or heavy structures and in high seismic zones.",null,0,cdlc9m6,1raftj,askscience,new,3
Ub3rN00b,"Finer particles flow more poorly due to surface electrostatic and Van Der Walls forces.   Powder flow is a significant issue for consideration for making tablets for medicinal purposes.   Generally the more finely you grind a medicine, the more rapidly it will release, but the more difficult it becomes to compress into tablets since the flow properties and compression properties become worse.    The best flowing powders will typically be spherical, and about 200 to 300 microns in size.       ",null,1,cdlia9a,1raftj,askscience,new,4
lowrads,"The way a substance, or in this case a fluid behaves, is due to intermolecular forces.  Water molecules tend to like stick together under a certain range of conditions, which is why only a tiny portion of them volatilizes and goes zinging off at room temperature and pressure.

Sand becomes silt and then becomes clay.  Clay has interesting properties owing to its crystalline structure.  If you poured some of each of those differentiated silicates into water, the sand would settle out first, followed by the clay.  The middle group, silt, would actually stay in suspension for longer.

The reason for this is charged surfaces.  The silicate materials form in sheets.  The sheet as a whole tend to have a charge, especially as components of the repeating structure are often displaced by differently charged metal ions.  Consequently, the tiny fragments of sheets tend to stick together due to opposing charges.  

Ordinarily, the charges are too weak between larger particles.  The surface area to mass ratio isn't favorable.  As the surface area ratio shifts, surface charge starts to be more significant, whether as an aggregate, or a solution.  Additionally, as the material is ground down, the rate of disintegration slows down exponentially.  You would think this was odd or inverted, given that surface area ratio seems to approach infinity as particle size becomes vanishingly small.  The force of mass available for collisions changes in a non-linear fashion with the diameter of the particle.  ",null,1,cdlavot,1raftj,askscience,new,4
HairySquid68,"you use progressively finer silicas in the metal casting process, and while it never becomes like a liquid, the super fine stuff does become very similar to a liquid when you vibrate or agitate it gently.  you vibrate tubs of it to help stick pieces into the silica so you don't break the mold just shoving it in.

there is also a physical therapy technique where people put their affected body part into vibrating, fine sand, and when motion/air is applied to it, it becomes fairly easy to move around it.

edit *move around in",null,1,cdlonsl,1raftj,askscience,new,3
SirJohannvonRocktown,"Assuming the particles that make up the substance are sphere, the short answer is it depends on the mass, volume, and number of particles in the substance. 

So how do you determine if it's valid to disregard the discrete particles that make up the fluid and model a substance as if it's infinitely divisible? 

This is referred to as the **continuum approximation** and there are mathematical ways to determine if it's a valid assumption.

The whole idea behind this is that we can average the random thermal motion of the molecules if the number of molecules are large and close enough. There's a lot that goes into this, but here's the gist.

If we have a fluid and we take a small enough volume of that fluid (say ⌂V_l for lower bound), we'll notice that at that volume of the substance, the statistical average or any property is meaningless because there is an insufficient number of particles contained in that substance at any given time. see: 

http://pillars.che.pitt.edu/files/course_10/figures/density_oscillate.gif

similarly, there is an upper bound (⌂V_u) due to non trivial spatial variation in the fluid properties. In other words, the density will increase non-linearly.

Assuming ⌂V_l &lt;&lt; ⌂V_u, we can define the continuum limit of the mass density at a point in a fluid is defined as,

rho = lim (as ⌂V -&gt; ⌂V_l) [⌂m/⌂V_l]

This is a good place to say that the density of a fluid can also be modeled as

rho = m*n

were m is the mass of the molecule and n is the number of molecules for a given volume.

The interesting thing here is that it's pretty much meaningless until you look at it's geometric context. Are these particles inside of a pipe, flowing past a wing, or doing something else?

The reason this is important is because fluids might or might not behave differently when a property or two are changed. For example, it might be turbulent or laminar. It might be very efficient on imparting and transferring energy to it's surroundings, or it might act as a damping mechanism. 

This is getting way too long, so I'll just try to finish up here.

Since we can't know how a particles behaves at all times and under all conditions, we have to determine whether the statistical mean is significant or not. A dust particle 200 miles above the earth can't be treated as if it's in a fluid, where as a baseball in a wind tunnel can.",null,2,cdl8n5d,1raftj,askscience,new,4
shapu,"No, for several reasons.

First, beach sand is a collection of lots of different things (rock, seashells, large particles of some solids), and so you'd have to have something that was a relatively pure sample.  So, you'd need, say, EDIT relatively pure quartz sand (silicon dioxide is what makes up most sand as we think about things like inland dunes).

Secondly, what makes a particle round is not necessarily how it is milled; once you get down to a certain very small size, it's about intermolecular interactions and binding.  Most rocks - which again, make up sand - tend towards tetrahedral binding, which by and large forces very small pieces into cubes or other non-round shapes.

Finally, those non-round shapes, because they have flat faces with large (relatively speaking) surface area, tend to exhibit strong binding thanks to things like hydrogen bonding, which makes them behave like...well, like solids, and not like liquids.

So I suppose if you could mill down a rock that had very weak electron interaction between the particles, and that formed less tetrahedral and more polyhedral shapes in the aggregate, then yes, it would behave like a liquid.  But you wouldn't be using sand to do that.",null,7,cdl9v5f,1raftj,askscience,new,7
null,null,null,0,cdl8egp,1raftj,askscience,new,1
rupert1920,"Disclaimer: I'm not an expert in protein folding, and would love to be educated more on the matter.

Check out [Levinthal's paradox](http://en.wikipedia.org/wiki/Levinthal's_paradox), which states that the sheer degrees of freedom a protein has makes it highly unlikely to spontaneously fold into the energetically stable conformation. Which means that there must be other effects - other than thermal sampling - that ""guide"" the protein into the proper conformation. This could be chaperones, or stable intermediates.",null,1,cdl9son,1raob3,askscience,new,5
Osymandius,"As /u/rupert1920 has said, Levinthal's paradox states that it would take longer than the age of the universe for a polypeptide of 100 residues to fold into the correct configuration by ""trying"" all phi/psi angles. Anfinsen et al (Anfinsen, CB et al (1961) ProcNatAcadSci 47, 1309-1314) showed that primary structure directly determines tertiary fold, therefore trying all the possible angles is not required - as was evident by proteins folding on a biological time scale and life existing to begin with!

Your question, therefore, is a very good one: if primary structure does determine tertiary structure, why bother with chaperones?

Ken Dill answers this nicely in an excellent review [here](http://www.nature.com/nsmb/journal/v4/n1/abs/nsb0197-10.html). He encourages you not to think of protein folding pathways, rather protein folding tunnels where there exist multiple routes to the most stable configuration (i.e. the lowest energy). Through these multiple routes can exist ""energy traps"" - local energetic minima which require energy input to overcome such that the polypeptide can reach the final fold. This is where chaperones come in. You can sort of think of them as proteins which recognise improper folds - say extensive hydrophobic stretches facing the surface of a protein - pull them apart and say ""try folding again"". This is why we don't get trapped in an infinite loop: who folds the chaperones? The chaperones aren't specific to any one protein, rather they recognise common folding mistakes.

Edit: His review really is rather good - if it's trapped behind a pay wall, reply and I'll get it for you - he explains it much better than I do.",null,1,cdlatkk,1raob3,askscience,new,5
gredders,"Neutrons and protons are arranged in 'shells' in the nucleus in a way that is analogous to the way that electrons are arranged in shells around the nucleus. 

[Have a look here](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). If you scroll down a little you can see a diagram of the shell closures.

3H has two neutrons which forms a closed shell, making it pretty stable. 

4H has three neutrons, one of which (the 'valence' neutron) must sit above this shell closure, making it highly unstable. ",null,1,cdlah66,1rapuj,askscience,new,15
iorgfeflkd,"This is only a partial difference, but one way to look at it is in terms of energy differences. Between H^3 and He^3 , the difference in nuclear energy is very small: less than a tenth the mass of an electron. For H^4 , the difference between that and H^3 is about ten times the mass of an electron. The energy benefit for H4 decaying is over 100 times greater than for H3 decaying.  Decay rates are related to the energy difference between mother and daughter states. Bigger energy difference, faster decay.

Between H3 and H4, the decay rates differ by a factor of about 10^30. The energies differ by about a factor of 300.",null,0,cdlbj6h,1rapuj,askscience,new,5
Chandley54,"Yes, in veterinary medicine we can categorise hyperthyroidism based on where the tumour is within the hypothalamic-pituitary-thyroid axis. It slightly alters our treatment options for it, but as far as I know, hypothalamic/pituitary surgery is almost never carried out in animals clinically, so many general practice vets will simply treat the general hyperthyroidism. I imagine in human medicine where surgical removal of the tumour is a possibility they are a lot more rigorous with determining where the cause is in all cases!",null,3,cdlcihm,1raq8w,askscience,new,6
mklevitt,"theoretically, yes, but i don't know that one has ever been proven or written up in the human scientific literature. secretory (hormone-producing) tumors of the hypothalamus are rare in general, and to find one that specifically comes from clonal expansion of TRH neurons, and then actually secretes? i couldn't find an example among the human literature. a much more common (albeit still very rare) cause of hyperthyroidism is TSH-secreting pituitary tumors, like you said. most hypothalamic tumors cause dysfunction by damaging/suppressing 'normal' functioning hypothalamic nuclei. thus common hypothalamic tumors like craniopharyngiomas can cause hypopituitarism (from suppression of nuclei that stimulate the pituitary) but not hyperpituitarism.",null,0,cdmppa5,1raq8w,askscience,new,1
Syphon8,"Yes. 

Not far into the bands, but there is various among people. The blue cones in your eyes are the most sensitive to ultraviolet light, and IIRC babies can usually see a very short way into UV. The cornea blocks out most UV, and were you to have your eye unlensed for some reason, you would see UV.

Aside from that though, the sensitivity of everyone in their cones is different, or else colour vision deficiencies wouldn't occur. Some women are tetrachromatic, and can see 4 primary colours because they have 2 different type of red-sensitive cones, for instance.",null,4,cdlgzhy,1raqf6,askscience,new,9
EdwardDeathBlack,"On a related but slightly different note, it is quite possible some women are actually [tetrachromats](http://en.wikipedia.org/wiki/Tetrachromacy#Possibility_of_human_tetrachromats) and can distinguish between colors that are absolutely identical to mere trichromats. 

Here is another [link](http://www.dailymail.co.uk/health/article-2161402/Gabriele-Jordan-British-scientist-claims-woman-superhuman-vision.html) . ",null,1,cdloi6w,1raqf6,askscience,new,2
iorgfeflkd,"The way it would manifest itself is through a change in density, but because water isn't very compressible, its density only changes by like 3% even at the bottom of the ocean, so the transmission of light from a source at that depth isn't too much different.",null,2,cdlclzj,1rarw1,askscience,new,5
PepperJack_delicacy,"Smoking cigarettes essentially speeds up the aging process of the skin, which leads to wrinkles. The main reason this happens is because **nicotine** is a **vasoconstrictor**, meaning that it narrows the blood vessels that supply the skin. When you impair blood flow, it has a harder time getting oxygen and absorbing nutrients such as **Vitamin A**, which normally keeps the skin hydrated and protects it from oxidative damage. Furthermore, it will have a harder time repairing wounds and synthesizing a protein called **collagen**, which keeps the structure of the skin intact. 

In summary: 

*Smoking cigarettes ==&gt; decreased blood flow to skin ==&gt; skin gets less oxygen and nutrients ==&gt; skin has a harder time protecting itself from damage and repairing wounds.* 

Sources:

http://dermnetnz.org/reactions/smoking.html

http://www.mayoclinic.com/health/smoking/AN00644",null,1,cdll8tr,1rav3s,askscience,new,7
ColdWaterEnthusiast,"Good question. It is almost pointless to try and find out whether the Sahara is growing or shrinking, because of the sheer size of the desert (as well as demarcating what exactly constitutes 'desert'). In the late 90s to mid 2000s, the thought was that the Sahara desert was expanding southwards by a certain extent each year. This was of course somewhat exaggerated. On the other hand, so is the perception that the deserts are 'in retreat' as these articles seem to imply.

The thing is, between the late 70s to late 80s, there was a significantly dry period in the Sahel region (the transition zone between the Sahara and the savanna) which exacerbated the effects of desertification, leading to the perception in the 1990s-2000s that the desert was indeed expanding. However, over the last 15-20 years in terms of precipitation, the region has been in a comparatively very wet period. Relative to the significant drought the area previously experienced, it may seem that the deserts are in 'retreat' but arguably that is essentially what is expected in terms of how vegetation has responded (it gets a bit more complicated because some of the previous mesic vegetation has been replaced by xeric vegetation in certain areas so while it is greener, it is not quite the same)

I hope this help. I could go into more detail but this should give you an idea of how complicated it is to understand.

Source - My dissertation research has broadly to do with understanding how vegetation responds to moisture events",null,1,cdlfsdd,1ravcx,askscience,new,6
ucstruct,"Not exactly my specialty, but I've worked in bioenergetics which is a central part to this story. The short answer is that there was likely a precursor that came before both of them, but then fungi came before plants. The evolution of eukaryotes was an extremely fascinating and important event in evolutionary history, and one that isn't extremely well understood. One theory is the so-called [endosymbiotic theory](http://en.wikipedia.org/wiki/Endosymbiotic_theory), where an ancient prokarytotic organism engulfs another and co-opts it to become a source of useful energy. It is likely that mitochondria were the first organelles to be formed this way, making the critical event to make eukaryotes. Plants likely formed when an early cell engulfed a cyanobacteria, which are photosynthetic bacteria, at a later time, but someone who specializes in plant biology and evolution will have to tell you more here.",null,2,cdlh7r4,1ravg2,askscience,new,15
redmeansTGA,"I'm only going to discuss the fungi, because you've asked a really complicated, fascinating question that touches on a lot of different fields. 

Firstly, a quick note on fungi. Most people think of fungi as things like mushrooms and bracket fungi- which belong to a phylum called the Basidiomycota. The other major group of fungi most people are aware of are the Ascomycota, which includes molds like aspergillus, the yeasts (such as Saccharomyces cerevisiae) and a weird assortment of other things you might recognize from the forest floor. 

The fungi also contain a bunch of other, less familiar things as well, such as  the [microsporidia](http://en.wikipedia.org/wiki/Microsporidia) and [Chytridiomycota](http://en.wikipedia.org/wiki/Chytridiomycota). Some of these are really fascinating, and truly push the envelope when it comes to eukaryote biology. 

The oldest described fungi is a filamentous microfossil called Tappania , which was dated to 1,430Ma (Butterfield, 2005). This unicellular fungi likely lived in shallow water (Butterfield, 2005). The oldest ascomycete has been dated from 400mya, and interestingly was found in association with an early lycopod plant (Taylor, et al., 1999). More modern fossilized fungi have been found from the Cretaceous, which resemble yeasts.

 Aside from this, there is scant fossil evidence- fungi don't have hard parts that readily fossilize. Using molecular clocks is another way to measure the age of a taxon. Berbee et al.,(2010) did this and found an estimate date of divergence between the fungi and animals around ~1,600Ma. Molecular clocks have dated the origin of the hemiascomycetous yeasts to around ~100Ma, which was probably due to co-evolution between fermenting yeasts and fruiting plants (Piškur, et al., 2006). 

The fungi are a part of a large group of eukaryotes called the Opisthokonts, which includes the animals, as well as a couple of smaller groups of unicellular organisms. The opisthokonts, and their relatives (part of a larger group of eukaryotes called the Unikonts) diverged from the rest of the eukaryotes a *very* long time ago, and possibly represent the earliest divergence (Stechmann &amp; Cavalier-Smith, 2003). The lifestyle, morphology and genome architecture of these earliest eukaryotes is a contentious, though fascinating subject that I don't have time to go into. 

Plants evolution is just as fascinating. Very briefly, the earliest plants entered the land around ~500 million years ago. Probably around the same time as the earliest fungi came onto land. Plants and fungi likely co-evolved very early on- the earliest ascomycota fungi was found together with a lycopod plant. Ever since, plants and fungi have been doing interesting things together (and earlier, remembering lichens). 

Anyway, to sum up, fungi as a traditional kingdom are much older than plants, being perhaps some 1.6 billion years old. Plants date back perhaps 1 billion years (older if you count some related algae that I didn't discuss). Recognizably modern groups of both fungi and plants didn't arise until much later, however. 

Refs:

Berbee, M.L., Taylor, J.W.
Dating the molecular clock in fungi - how close are we?
(2010) Fungal Biology Reviews, 24 (1-2), pp. 1-16.

Taylor, T.N., Hass, H., Kerp, H.
The oldest fossil ascomycetes [8]
(1999) Nature, 399 (6737), p. 648.

Piškur, J., Rozpedowska, E., Polakova, S., Merico, A., Compagno, C.
How did Saccharomyces evolve to become a good brewer?
(2006) Trends in Genetics, 22 (4), pp. 183-186.

Butterfield, N.J.
Probable proterozoic fungi
(2005) Paleobiology, 31 (1), pp. 165-182.

Stechmann, A., Cavalier-Smith, T.
The root of the eukaryote tree pinpointed
(2003) Current Biology, 13 (17), pp. R665-R666.",null,2,cdls0u3,1ravg2,askscience,new,8
Sunscorch,"Ok.

For a second, forget that the Earth and Moon are orbiting, and picture the Earth falling straight down towards a stationary Moon. At the very start of our thought-experiment, the Earth is also stationary and its oceans are equally spread out across the entire surface.

The Earth then begins to accelerate towards the Moon, as is begins to fall. The ocean nearest the Moon experiences the greatest acceleration, because it is closer and therefore is exposed to the greatest force. Likewise, the ocean furthest from the Moon experiences the lowest acceleration for opposite reasons. The Earth itself, of course, experiences an average acceleration.

So! The Moon-side ocean begins to move away from the surface of the Earth, as it is accelerating faster than everything else, creating the bulge that is easiest to understand. The ocean on the other side, however, gets ""left behind"" because it is accelerating slower than the Earth. Essentially, the Earth is moving away from it, which creates the opposing bulge on the far side from the Moon.

That is how it works in our thought-experiment, but exactly the same thing happens in orbit because the Earth and Moon are essentially falling towards each other, and are constantly accelerating because of the constant change in direction. That is why there is a tide on each side of the Earth.",null,2,cdlgab2,1ravmk,askscience,new,12
thumbs55,"Excellent question:

[Sixty symbols did a video on it.](http://www.youtube.com/watch?v=YO3eDYzFp8Y)

[In this image](http://hendrix2.uoregon.edu/~imamura/123cs/lecture-2/tides.jpg)

Thinking in vectors the first image is of 5 vector forces acting toward the moon. Note that the farther they are the weaker they are and the top and bottom are not parrallel to the other three.

But we dont live in space we live on the earth so if the earth moves we move with it and dont notice so for that reason in the second image the middle vector is subtracted.

Giving a zero vector in the middle since anything minus itself is zero.

The top and bottom vector are pointing in since they were already pointing a little bit in.

The vector nearest to the moon is a lot shorter but still pointing toward the moon.

And most interestingly the vector farthest from the moon is actually pointing away from the moon.",null,2,cdlgink,1ravmk,askscience,new,7
SingleMonad,"What you're asking has been done.  It called a pulsed laser.  You are imagining making little pulses of red light no more than a few femtoseconds long.  The output is not red.  It is *white*.  The light has a broad spectrum, centered about red, the width is inversely proportional to the pulse duration.

Wiki ulatrafast, supercontinuum, frequency comb, laser.  If you disperse the output, you will see the individual colors in the laser.

http://grad.physics.sunysb.edu/~meardley/fiber/weiss5.jpeg",null,0,cdloqib,1rawb8,askscience,new,10
null,null,null,15,cdlfycp,1rawb8,askscience,new,8
onyablock,"People can become immunocompromised through various ways including pregnancy, viral infection, steroid treatment etc. etc.

The reason it is important when considering vaccinations is that being immunocompromised can increase your risk of obtaining the infection to which you're being immunized against (if the vaccine is 'live' virus) or being vaccinated can be pointless as no immunity will actually be gained from the vaccine.

The effects of an immune deficiency on vaccines varies greatly depending on the vaccine and its application regiment. For example it could be recommended that you receive multiple booster shoots or don't receive the vaccine at all if you are immunocompromised. 

In the case of the flu shot, obtaining the killed-virus vaccine won't allow you to actually contract the flu, however the chances of you generating good and long lasting immunity to the flu is reduced if you are immunocompromised. Obtaining the 'live' attenuated vaccine would not be recommended for immunocompromised patients as there is potential for infection, hence why the box is there.

I hope that makes sense.

Source: 4th year immunology student.",null,2,cdlcf5d,1rawff,askscience,new,17
tthershey,"Live vaccines are generally contraindicated in immunosuppressed patients because these patients will not be able to mount a sufficient immune response to the vaccine.  Consequently, the live vaccine could induce an infection.

Inactivated or component vaccines won't put immunosuppressed patients at risk, but the patients might not gain much protection from the vaccine.  This is because immunosuppressed patient's won't be able to generate antibodies to help fight off future infections.

Immunosuppressed patients most certainly need to get vaccinated because they are at greater risk for getting serious complications from infections.  It's important for health care providers to know a patient's immune state in order to deliver the right kind of vaccine.",null,0,cdlk7wj,1rawff,askscience,new,3
Chandley54,"There're three reasons why you need to be immunocompetent when vaccinated. 

1. It is rare nowadays for a vaccination to be live &amp; pose a direct threat, some of them may revert to being harmful, for example attenuated vaccines commonly have a harmful gene removed before they are given to the host so they can be dealt with by the immune system without any significant risk. In some cases the process may not be perfect and so some live unattenuated virus may get into the vaccines, so although in theory the virus should not be able to establish itself and replicate, it sometimes can and is therefore much more hazardous to immunocompromised patients. The same is true for killed virus vaccines - in theory all of the virus should be dead, but whatever process they use to kill the virus (e.g. exposure to UV light) may be ineffective, so again, live virus may end up being present in the vaccine and lead to an active viral infection.
2. If a patient is immunocompromised, there is a possibility that the body will not be able to respond effectively and generate the necessary memory lymphocytes for the vaccine to be effective. This would mean that should the person then encounter a live version of the virus, the vaccine would not have provided them with any protection at all as the memory cells were not generated at the initial vaccination. so are not available to respond.
3. Although the vaccine you received was killed, the form you filled out was probably written by some legal team at some point in history when they were using a different type of vaccine, so they're basically just covering their own backs, and if may be expensive for them to change the documentation.

I would imagine it is likely a combination of the above 3 reasons!
Hope this was helpful.

Source: Veterinary Surgeon/Anatomical Pathologist",null,2,cdlcbxz,1rawff,askscience,new,3
Urgullibl,"The point of a vaccine is to stimulate your immune system into producing antibodies against whatever it is you're being vaccinated against. If your immune system is suppressed, there is no point in vaccinating, as the reaction would not result in protection from infection.

In case of the flu vaccine, we're talking about a dead vaccine, i.e. there are no attenuated whole viruses in it, hence there is no risk of getting the flu from it. In case of vaccines containing whole attenuated viruses, an immunosuppressed patient might get sick from such a vaccine.",null,1,cdlk4zy,1rawff,askscience,new,2
killer_alien,basically there are two types of vaccines: antibodies and dead virus or w/e cells. Anti bodies is a short term vaccine which basically grands you immunity whereas dead cell ones stimulates your body to create antibodies which is a long lasting treatment. (This is the most basic i can put it w/o making to wrong and confusing),null,0,cdmicxd,1rawff,askscience,new,1
invariance,"No. It is simply inconclusive, because there exist series which diverge and series which converge for which the ratio test gives 1. For the series a_n = 1/n, the sum diverges. For the series a_n = 1/n^2, the sum converges.

The ratios are not 1 in magnitude, except in the limit. Note also that both ratios converge to 1 from below. A refined version of the ratio test will tell you that if |a_{n+1}/a_n| &gt;= 1 for sufficiently large n (so the ratios converge to 1 from above), the series will diverge. The proof for this follows from what you have already said. So in fact, the only inconclusive case is if the limit of the ratios converges to 1 from below.

So the short answer is that if the limit of the ratios converges to 1 from below, the test is inconclusive because there are examples which converge and examples which diverge. ",null,2,cdlud5b,1rawis,askscience,new,7
iCookBaconShirtless,"The issue is not which direction that the limit is approached, as you conjecture.  While approaching 1 from above assures divergence, approaching from below does not assure convergence.  A simple example is the harmonic series.

They key to understanding the ratio test is to more precisely understand this statement that you made:  

&gt; I understand what happens when the limit is smaller than 1 (every element of the series is smaller than the previous by a factor L, hence the series tend to stabilize and converge).

The fact that every element of the sequence is smaller than the previous by a factor of L (at least in an asymptotic sense) implies that the sequence converges to zero **exponentially fast**.  Basically, it looks like a geometric series at large n.  This exponential convergence of terms is enough to ensure that the series converges, but it's strictly stronger than what is needed.  Plenty of series have terms that converge more slowly than exponentially, but still converge as a series.  Any p-series with p&gt;1 for example (e.g., 1/n^2 ).

tl;dr Ratio test determines exponential convergence of terms, which is more than is needed for convergence of series.",null,0,cdlv91r,1rawis,askscience,new,4
wgunther,"In order to understand the ratio test you have to understand the proof. The proof is if the ratio a_{n+1}/a_n goes to L&lt;1 then eventually the of consecutive terms is less than 1-epsilon for some small but positive epsilon, and therefore, the series is smaller than the geometric series a(1-epsilon)^n for some suitable a. Thus it converges by direct comparison. 

If L&gt;1 then you can do the same thing but with 1+epsilon. 

The problem is this proof doesn't work if L=1. The ratio test will only work when the sequence of the summand of the series converges *faster* than something geometric whose series converges or *slower* than something geometric whose series diverges. In the case when L=1, one can not compare the series to a geometric series that converges or diverges. ",null,0,cdmmc8x,1rawis,askscience,new,2
snusmumrikan,"[This paper](http://www.academia.edu/372962/Giants_on_the_landscape_modelling_the_abundance_of_megaherbivorous_dinosaurs_of_the_Morrison_Formation_Late_Jurassic_western_USA_) discusses it for herbivorous large dinosaurs and says a few tens of each per sq km. 

[This one](http://earth.geology.yale.edu/~ajs/1993/11.1993.06Farlow.pdf) discusses the limiting factors in population density of large carnivores and the balance between food availability and having enough of each species to ensure a mating fequency high enough to avoid extinction.

It seems your question can't be answered reliably as so much depends upon the preservation of dinosaur remains for that. Looking at the variables and comparing it to modern-day predators might be the best option?",null,3,cdlea2h,1raxgi,askscience,new,9
Ruiner,Only by the amount of mass it consumed. ,null,0,cdlt15h,1rb1bz,askscience,new,4
Infinite_Ambiguity,"Steven Hawking has shown that black holes also radiate energy because of quantum effects near the event horizon.  Consequently, black holes might increase in mass/energy by the amount of mass/energy consumed, but they are also radiating mass and energy (equivalent by e=mc-squared) and thereby also simultaneously evaporating to some degree. .  ",null,0,cdltx5v,1rb1bz,askscience,new,2
Ejb90,"I think you're misunderstanding the structure of an atom. The ""shells"" the electrons occupy aren't determined by their distance from the nucleus, but by the relative energies of the electrons in each. Why this happens is explained by quantum mechanics.
In the classical case, the ""radius"" of the first shell would simply expand with the nucleus, though that would never be a real problem - the nucleus is 10^-15m across and the atom 10^-10 - that's 10,000 times bigger, so any atom that big would be inherently unstable.
However in reality the electrons aren't hard point of mass whiz zing around the nucleus, they're a ""cloud"" of delocalised charge with certain characteristics, also described by quantum mechanics. If you want to know more there are loads of online resources about this.",null,0,cdlgd0k,1rb1wp,askscience,new,7
Platypuskeeper,"Why would it 'force electrons out of the lowest shell'?
",null,1,cdlg0vx,1rb1wp,askscience,new,3
thumbs55,"Basically no.

If you have a very small object (less than the DeBrogle wavelength of the electron ~ 10nm) then the electrons in the outer shell of this object may have descrete energy levels and be treated as a giant atom.

This is refferd to as a [Quantum dot.](http://en.wikipedia.org/wiki/Quantum_dot)

So these quantum dots are much larger than the lowest shell of a Hydrogen atom but still dsiplay quantum properties.

Sure the nucleus is like 15 orders of magnitude smaller than the electron shells but if we had a nutron star and some how made it positevly charged, and placed an electron to see if it would orbit it:

Then we would find that the electron and the proton in the star join together to create a neutron due to all of that pesky gravity.

If the system is changed such that the lowest shell no longer exists then the next lowest shell by definition becomes the lowest. This is just simantics and is a bad argument since the lowest energy is an s shell and the next lowest is a p shell and behave measurable differently. And if you did get rid of the first s shell then the new lowest shell would be an s shell at a higher energy.",null,0,cdlgc3l,1rb1wp,askscience,new,1
miczajkj,"In fact this is something, that can happen but not for electrons. 

You may know, that there are three generations of quarks and leptons, while our universe consists mostly from the first generation (up- and down-quarks + electrons and electron-neutrinos). But there are certain natural processes, that produce particles from higher generation, for example the cosmic radiation in the earth's athmosphere. 
The electron equivalent in the second generation is the muon - and if you construct muon-atoms your question gets important!

Like the others already mentioned, 'orbiting' electrons (or muons) don't really orbit the nukleus: their position gets described by a probability density; those densities are the absolute squares of the particles wave function. 
If you calculate those wave functions you find, that the probability density of the ground state has it's maximum at the classical Bohr radius, that can be calculated by using a semi-classical force approach (for muonic hydrogenium): 

Let the Coulomb-Force (ℏαc/r^2) equal the centripetal force (p²/mr). Also keep in mind the the uncertainty principle: pr ~ ℏ. It follows:

ℏαc/r^2 = ℏ^2 /mr^3 
r = ℏ/αmc

Now, because the mass of the muon is 200 times bigger than the mass of the electron, therefore it's Bohr Radius is 200 times smaller and if you also consider the finite circumference of the proton it is possible, that the muon may have finite probability to be found inside of the nukleus, especially if you talk about heavier nuklei. 

The main consequence is a displacement of the energy niveaus. This fact was used, to find the much used formula

r = 1.2 fm * A^(1/3) 

for the radius of the nukleus depending on the Mass number. 

Another consequence is the increased probability of decays like the K-capture, where a muon from the K-shell reacts with a proton:

µ^- + p -&gt; n + ν_µ",null,0,cdlsooe,1rb1wp,askscience,new,1
ShwinMan,"Short answer: no

LADEE is a small spacecraft, it's only 2.37m high and if it were visible then so should all the other spacecraft there now as well, including the Apollo descent stages, lunar rovers, debris etc. Even Hubble wouldn't be able to make it out.",null,0,cdmxh0z,1rb6hn,askscience,new,1
sharp12180,"The force of gravity due to an object with mass is never zero. It can be very small if the object has very low mass or you are far away from the object. For a galaxy, which is very massive, you can get far enough away where the force of gravity acting on you is negligible but it will never be zero. In fact, the gravitational force is proportional to the inverse-square of the distance between two objects, meaning if you double your distance from an object the force of gravity decreases by a factor of four. Still, this force will never be zero.  ",null,0,cdm0ted,1rb6tc,askscience,new,3
Ejb90,"In a way, no - the gravitational field produced by mass is infinite, so the field has an effect throughout the universe.
In another way, yes - there exist points called Lagrangian points where the gravitational fields of objects cancel to produce zero net acceleration. These are quite common, there are several around earth, being utilised for their stability. Though here the rotational effects of the entire galaxy have been ignored and considers the Sun-Earth system as relatively stationary.",null,0,cdm0vay,1rb6tc,askscience,new,2
PepperJack_delicacy,"The ""pins and needles"" feeling is referred to as **paresthesia**, which occurs when a nerve and the arteries supplying the nerve are compressed. This prevents the nerve from carrying electrical impulses that transmit the sense of touch, which you feel as ""pins and needles"". 

It's harmless when it occurs transiently (like after you fall asleep on your hand) because once the pressure is removed, blood supply to the nerve will be restored. However, there are certain chronic cases that are indicative of neurological disease or more traumatic nerve injury, which are more serious.

In the case of a blood clot, though, you are created a plug in an artery that prevents blood flow to a tissue. There is nothing you personally can do (such as switch body positions) that will remove the clot. Furthermore, in the case of a heart attack or stroke, you are preventing blood supply to the heart or brain--two of the most important organs in the body, which absolutely need blood for you to survive. 

So overall, it's true that ""pins and needles"", strokes, and heart attacks are caused by circulation problems. However, in the case of ""pins and needles"", the blockage is readily reversible and the organ that is losing blood supply is no where near as important as the brain or heart. 

Sources:

http://www.ninds.nih.gov/disorders/paresthesia/paresthesia.htm

http://www.urmc.rochester.edu/encyclopedia/content.aspx?ContentTypeID=1&amp;ContentID=58",null,0,cdm065z,1rb75x,askscience,new,2
brawnkowsky,"'pins and needles', which is called Paresthesia, is caused by pressure applied to a nerve.  This inhibits its ability to conduct a signal and eventually leads to a limb 'falling asleep' (a dead leg).  a lack of blood circulation does not cause this.

lack of circulation (specifically tissue perfusion) results in a failure to deliver oxygen to systematic cells and to remove metabolic waste.  This lack of oxygen causes cells to create energy through alternative pathways that create acidic products (lactic acid is common), causing acidosis.  Eventually, the cells will die because they fail to maintain the pH needed to function; this is called Ischemia.  Ischemia in organs can lead to organ failure, which will kill a person.  ",null,0,cdm0add,1rb75x,askscience,new,1
adoarns,"Pinch a nerve long enough, and it becomes permanently damaged. The nerve fibers will wither back, and you will lose sensation until the nerve grows back (about 1 mm/day). Even then you may expect that not all the withered fibers will find their way back to their proper locations.

Heart muscle and brain tissue are much more metabolically active, and take much less time without proper blood flow to be permanently damaged.

Unlike peripheral nerves, brain tissue and heart muscle does not grow back. You lose it, and it's gone for good.",null,0,cdmc3ds,1rb75x,askscience,new,1
_Momotsuki,"If you squeeze just one vessel of your arm, there are many collateral vessels to take up the slack and perfuse the rest of the arm. This principle applies to your heart because there are only a few main vessels that supply the heart (with great variation between individuals, and very simplistically, there's the left and right coronary arteries with the left splitting very early to become the left anterior descending and circumflex artery). Any blockage in one of those 3 vessels will cause ischaemia and lead to death of the tissue areas that are meant to be supplied. Indeed, there are collaterals present in the coronary circulation. However, these are usually functionally non-patent and can't really help with distributing blood because they have such a small lumen. This is especially the case when there is ischaemia due to a sudden thrombo-embolic event. With that said, in *some* cases where there is a slow build up of atherosclerotic plaque within arteries, you can get slow opening of these collateral vessels to help perfuse the heart.",null,0,cdn9eo8,1rb75x,askscience,new,1
StarshipEngineer,"There is no such thing as terminal velocity in an airless environment. It doesn't matter what the terminal velocity of an object in air is, if there is no air for the object to interact with through friction, the object will keep accelerating as it falls until it hits a solid surface.",null,1,cdlfu8p,1rb77o,askscience,new,12
Smoothened,"Microorganisms are the main cause of spoilage, but there are other ways in which food can go bad. This varies both with the content of the food and the environment it is exposed to. For example, food that is partially composed of water can dry out. Protein and other molecules in the food would undergo degradation, which will result in changes in both texture and taste. Another thing that can happen is the separation of ingredients in different phases. These changes would be less obvious in foods that are homogeneous and mainly composed of simple molecules such as sugars. All in all, the changes would generally occur more slowly than in the presence of microorganisms. Most likely the resulting food wouldn't make you as sick as if you ate food spoiled by bacteria or fungi. ",null,0,cdmc7d1,1rb7jz,askscience,new,2
stevenstevenstevenst,"I do not know a lot about what the atmosphere is actually like within the ISS, but I can tell you that in a pure oxygen environment, as was used on some NASA flights, those on board have reported that it is difficult to hear one another.  This is perhaps related to the pressures in these vessels more than atmosphere itself, as a denser atmosphere transmit sound waves more readily.  

Kind of just a curiosity, so I apologize if your question was not answered in its entirety.  The atmosphere being thicker or thinner is related more to pressurization than the content- although content does play a role.",null,0,cdmgh7o,1rb86r,askscience,new,2
Proxymace,"At high concentrations oxygen is toxic to organisms, at 100% humans get a ""high"" from breathing it. This tends to make people calmer and is why planes deploy oxygen masks and not just air masks. For the iss I imaging its due to weight limits on supply ships. Carrying a load of nitrogen that you don't need isn't very good.",null,1,cdmjb36,1rb86r,askscience,new,3
chrisbaird,"""I saw somewhere that in the ISS and other stations that they have a 100% oxygen environment"" You saw wrong. The composition of air on the ISS is definitely not 100% oxygen, and is in fact intentionally regulated to match the composition of air on earth's surface, with about 21% oxygen:

http://www.nasa.gov/missions/highlights/webcasts/shuttle/sts112/iss-qa_prt.htm

There are a couple of reasons for this:

- Pure oxygen is highly flammable. NASA unfortunately learned this the hard way with the Apollo 1 accident
- High oxygen concentrations are unhealthy to humans as our bodies have evolved to work most efficiently with the oxygen levels common at earth's surface.",null,0,cdmutcf,1rb86r,askscience,new,2
owaisofspades,"a-helix and b-sheets are due to hydrogen bonding. Random coil is due to hydrophobic reactions if I remember correctly. There's also di-sulfide bridges, which only cysteine can form. 

Whether a protein will form an a-helix or a b-sheet depends on the sequence (the amount of residues between the two interacting residues determines which one will form)",null,0,cdmbq05,1rb8il,askscience,new,2
reddishpanda,"Short answer: the types of interactions needed to produce a secondary structure can be made by a variety of amino acids. 

Helices and sheets are made by the backbone interactions between the amino acids of a proteins, so almost anything goes (except for proline, which would be too geometrically limited to form a helix or sheet and is limited to loops and random coils). You can use many combinations of different amino acids to make one structure or another, but interactions among the side chains of amino acids (where you will find hydrogen bonding and hydrophobic interactions as well as salt bridges between say glutamate and lysine). 


It might be information overload, but try playing around with a protein database like [RCSB](http://www.rcsb.org). If you just come up with an amino acid sequence of your own creation, you can use [Phyre2](http://www.sbg.bio.ic.ac.uk/phyre2/html/page.cgi?id=index) to get a prediction of what your protein might look like and the secondary structures that might form it. ",null,0,cdmedut,1rb8il,askscience,new,2
Polyknikes,"I don't think anyone will have an exact answer to your question because it would depend on which cell you are talking about, how well stocked they were beginning the fasting period, and how much energy they are being asked to expend over a given amount of time.  As an alternate answer I will discuss what happens during starvation which hopefully will answer your question.

In the normal course of starvation we first burn carbohydrates which basically refers to glucose.  Glucose is stored in many cells, but particularly in the liver hepatocytes, in the form of glycogen.  The breakdown of glycogen is referred to as glycogenolysis which releases glucose into glycolysis for energy production.

After all the glycogen is used up the body begins catabolizing (burning) proteins.  Protein catabolism involves the breakdown of bodily proteins into amino acids for use in synthesizing more glucose in a process called gluconeogenesis (which takes place in the liver).

Sometime during protein catabolism your body will begin the process of lipolysis which is the breakdown of triglycerides into fatty acids which can be oxidized into multiple units of Acetyl-CoA and fed into the TCA cycle for energy production, bypassing glycolysis.  High rates of fatty acid oxidation will lead to ketogenesis, or the creation of ketone bodies.  Ketone bodies are another form of high-energy molecule like glucose which can be metabolized by many tissues and are especially important for the brain during the starvation state as it has high energy demands and cannot directly metabolize fatty acids.

TLDR: During starvation your cells first use glycogen (stored glucose), then catabolize proteins into glucose, then burn fats in the form of ketone bodies.

Hope this answer helps.
",null,0,cdlx6pc,1rb8zj,askscience,new,2
AbouBenAdhem,"When the temperature of a gas changes, its density changes. When its density changes, its index of refraction changes. When light passes from one substance to another substance with a different index of refraction, it travels at a different speed; and when it meets the interface between two such substances at an angle, it bends.",null,2,cdlghd7,1rbb96,askscience,new,21
iorgfeflkd,"Freefall doesn't get rid of tidal gravitational fields. The difference in Earth's gravitational field between the front and back of the ship could be detected with precise instruments, and would be absent in intergalactic space.",null,1,cdm16ju,1rbd4j,askscience,new,4
owaisofspades,"It's a bit complicated, but I can give you the general idea.

During glucose metabolism you go through glycolyis, which gives you pyruvate. Pyruvate then gets converted to acetyl CoA. Now here's where it gets tricky. If you need energy, your cells are going to send the acetyl CoA through the Citric acid cycle to make loads of ATP. If you don't need energy, the acetyl CoA gets shunted to fatty acid synthesis.

YOu have some enzymes involved that activate the acetyl CoA into malonyl CoA, and this allows you to add an acetyl CoA to it, forming a chain. After a certain number of extensions you get palmitate, which is a freefatty acid, which can then be modified to form other fats or phospholipids, or which can be esterified with a glycerol molecule to form mono-, di-, triglycerides.

This takes place in the liver. Your liver then packages the TAGs into lipid containers (chylomicrons) and then put them into the blood stream. Then lipoprotein lipase on the surface of adipocytes grabs the chylomicron and pulls the TAGs out of them.

Now for the second part of your question. fatty acids don't get converted back into glucose as far as I know, but into acetyl CoA through a process known as beta oxidation (for short and medium chains, I don't remember the long-chain degradation process ATM). The acetyl CoA is for the liver to use (it's not water soluble so can't get transported in blood). For other tissues, the liver converts the acetyl CoA from Beta oxidation into ketone boies, which are water soluble and can be transported to other organs such as the brain through the bloodstream.

Hope this helps, feel free to ask for clarification. As a med student with my biochem final coming up soon i'm trying to keep my knowledge from disappearing haha",null,0,cdmbnot,1rbdz6,askscience,new,2
sharp12180,"Electrons are bound to an atom and his bond has a certain amount of energy. If the is an incident photon with energy greater than or equal to this bond energy, it can cause the electron to become unbound. When enough electrons become unbound, you have a current. In a solar panel, some of the photons from the sun have the right energy to dislodge electrons in the panel which creates usable electricity. ",null,0,cdm0x32,1rberh,askscience,new,2
rupert1920,"You should also take note that it is the [photovoltaic effect](http://en.wikipedia.org/wiki/Photovoltaic_effect), not the photoelectric effect, at play here (hence a ""photovoltaic cell"").",null,0,cdm8iuh,1rberh,askscience,new,1
meerkatmreow,"Black both absorbs and radiates better.  The net result can be a lower equilibrium temperature.  For example, the SR-71 was originally not painted black.  However, it turned out that the black paint lowered the temperature enough to allow the structure to be lighter even with the extra weight of paint, resulting in a net savings.",null,0,cdm5fnf,1rbfb3,askscience,new,2
miltoniousbastard,If you have ever touched a tinted window you will notice that it is noticeably hotter than non tinted windows on the same vehicle. The tinted window is absorbing most of the light (energy) which keeps it from being transferred to your cars interior. The heat on the window is radiated to the surrounding outside air vs. the heat radiating off your seats/dash/whatever else is in your cars interior.,null,0,cdm0fzy,1rbfb3,askscience,new,1
Greyswandir,"I think perhaps this is a good lesson that aesthetic concerns sometimes (often?) trump engineering ones.  I think the real answer to your question is that many people don't want other people peering into their cars, and as you pointed out, other coatings are more expensive.  I imagine that concerns about heat probably weren't too much a part of the design process.

And, as plenty of other people have pointed out, having the light absorbed by your windows which are in contact with the outside and have a high surface area, keeps the heat from instead being trapped inside your car.",null,0,cdmvmx1,1rbfb3,askscience,new,1
bohr_exciton,"Air is not completely transparent, however we perceive it as such for two reasons:

1) The various molecular and atomic species that make up the atmosphere can only absorb light at specific frequencies. If you look at the absorption spectra of the species making up thee majority of the atmosphere, such as nitrogen (N2), you will see that for most of them there will only be negligible absorption in the visible part of the spectrum. 

2) The density of air is so low that we can't perceive the minimal absorption that does take place. The best example of this is water. Water actually has a blue color, which you can see by say looking at the sea. However water only absorbs weakly in the visible, such that to observe this color light needs to pass through meters of water before we can observe its absorption. If you just look at a glass of water, it will just look transparent. Now the density of water vapor in the atmosphere is much much lower than in the glass, and therefore you can't possible see this effect in air with your bare eyes.

Finally, as for why you can see the atmosphere from space, or why the sky looks blue, the answer is that while the atmospheric gases do not significantly absorb visible light, they can scatter the light. Moreover, the major scattering mechanism is so-called Rayleigh scattering, which occurs more strongly for higher energies. Because of this blue light is scattered more than red light, which makes the atmosphere look blue. ",null,1,cdlvjoo,1rbhiu,askscience,new,4
chrisbaird,"Air is not perfectly invisible. Look at the sky during the day. That blue color is air. Look at a distant mountain on a clear day and compare it to a very close hill. The distant mountain will seem to be covered with a whitish haze. That haze is air. Clean air is composed of very small molecules that are very far apart. For this reason, you need a lot of air in order to see it with your naked eye.",null,0,cdmul9m,1rbhiu,askscience,new,2
Qvanta,"Materia excerts light ""color"" only if it has absorbed and emited the light struck by it. Here is the catch, each materia has a specific amount of energy in form of light it accepts. If the light shining on a materia is below or above the requierd amount of energy it needs to possess, light will just pass by.

The atmospheric light comes from the scattering of the blue spectrum when the light passes through our atmosphere. So you actually dont see any color only the smudging of the suns blue light.  ",null,2,cdluxwu,1rbhiu,askscience,new,1
DeadVirus0,"Earth's solstices and equinoxes are based exclusively on its 23.5 degree axial tilt.

For example, our next perihelion will be on January 4th, 2014 while the upcoming winter solstice will be on December 21st. This means that the northern hemisphere's longest night is 2 weeks away from the Earth's closest orbital position to the Sun. [Source: US Naval Observatory](http://aa.usno.navy.mil/data/docs/EarthSeasons.php)

Additionally, 4 Vesta is, relative to other celestial bodies, small and amorphous.

So, I guess what I'm trying to say is that your assumption that solstice/equinox are related to perihelion/aphelion is flawed.",null,0,cdm19tt,1rbj3h,askscience,new,1
rupert1920,"Ash is material is leftover material that cannot undergo further combustion - in the case of complete combustion. This is the white ash you often see when something is completely burnt. An example of this would be [wood ash](http://en.wikipedia.org/wiki/Wood_ash), which consists of the trace mineral compounds in wood that don't combust, such as calcium and potassium (which comes from the word [potash](http://en.wikipedia.org/wiki/Potash), with a similar etymology in how it was made historically - from ash).

Since different compounds will have different amounts of these non-combustible elements, they will naturally have different ashes.

This concept also has direct impact in [gravimetric analysis](http://en.wikipedia.org/wiki/Gravimetric_analysis) - which relies on very careful measurements of weight of a compound before and after some process. You can attempt to weigh out your analyte, then completely burn it at high temperatures, and weigh out the ash that is left over in order to find the chemical composition of the analyte (for example, finding the stoichiometric ratio can give you oxidation states). You'll also find filter papers used for this purpose to be ""ashless"" - it produces purely gaseous products under combustion so it won't skew the results of such measurements.",null,1,cdlzjby,1rbk7h,askscience,new,3
Platypuskeeper,"Alcohol (ethanol) and water are miscible fluids because the -OH group of the ethanol molecules forms [hydrogen bonds](http://en.wikipedia.org/wiki/Hydrogen_bond) with water, just as water does with itself. 

Hydrocarbons, as in oils, only bond negligibly with water molecules, which means it costs energy to separate the water molecules from each other and stick a hydrocarbon molecule between them. The lowest energy situation is if you lump all the hydrocarbon molecules together and minimize the contact area with the water, that is, form an oil phase.
",null,3,cdm0b7z,1rbl1i,askscience,new,5
LoyalSol,Density has nothing to do with it unless the two materials don't mix.  In the case of oil and water there is a mismatch in how the molecules interact with each other so they prefer to stay separated.   ,null,1,cdm0l07,1rbl1i,askscience,new,2
TehMulbnief,"Couple of reasons. The most direct reason is that alcohol and water are miscible. They mix together very nicely, so much so that you can't tell them apart once they are mixed. The resulting solution is sort of like a metallic alloy. When you look at a stick of bronze, you don't see copper and iron, you just see nice, homogenous bronze.

The other less obvious (and maybe a bit niggly) reason is the Second Law of Thermodynamics. This law introduces the idea of ""entropy"" or randomness of a system. When you add water to alcohol, the tendency is going to be for the two to mix, until the alcohol molecules are perfectly and uniformly distributed amongst the water molecules and vice versa. Once you're at this point, the system is going to stay that way because that's the most stable way for the system to be.",null,0,cdmx3br,1rbl1i,askscience,new,1
Farnswirth,"You can absolutely see one atom thick graphene sheets, this is one of the things that makes it such a remarkable material.  Just look at [This picture](http://3.bp.blogspot.com/-2UU-zkkrxm0/UPXIX6at0RI/AAAAAAAACNc/LsODcsw_1Oo/s1600/High-Speed+Graphene+Circuits.jpg), [the bottom of image a in this picture](http://www.nature.com/srep/2012/120921/srep00682/images/srep00682-f2.jpg), and [image c at the bottom left in this one which shows two single-atom graphene sheets layered on top of eachother on a PET film](http://www.nature.com/nnano/journal/v5/n8/images/nnano.2010.132-f2.jpg).  

We can see graphene when it is only one atom thick because it is exceptionally good at absorbing light.  Graphite and graphene are extraordinarily good at absorbing light mainly because the individual sheets of graphene have an extremely low [band gap](http://en.wikipedia.org/wiki/Band_gap) (pretty much 0eV).  This is also one of the reasons it is such a good electrical conductor as well.  Notice diamond has a very high band gap, which makes it transparant to visible light and a poor electrical conductor.  Finally, take a look at the [physicochemical properties of Boron Nitride](http://en.wikipedia.org/wiki/Boron_nitride#Physical).  While its structure is remarkably similar to graphene, it has a high band gap, which makes it appear white or transparent.  ",null,2,cdlranh,1rbqyc,askscience,new,7
organiker,"If your application calls for multiple layers, then you grow it as multiple layers.

If you're making electronic devices, you lay it flat on a substrate like glass, silicon or plastic, then add electrodes.

If you're making a coating, you layer it on whatever you want to coat. Probably with a protective layer on top.

If you're making batteries or capacitors, you mix it with your electrode materials.


",null,0,cdlo8z4,1rbqyc,askscience,new,1
EdwardDeathBlack,"You incorporate it into another matrix. You layer it in a sandwich of other materials...all that to exploit either its electrical or mechanical properties. 

It is not a first...here is a [TEM](http://large.stanford.edu/courses/2007/ap272/kimej1/images/f2.jpg)  of a gate oxide (a semiconductor term about one of the common layer in modern electronics) that is only a few atoms thick. Or here is  a [STEM](http://origin-ars.els-cdn.com/content/image/1-s2.0-S0038109805008914-gr1.jpg) of a quantum well of the type often use in vcsel and the like. Also atoms thick...

In a world where ""everything"" is made of atoms, all we have is made of ""stacks"" of single atom. The art is to stack them in the right order to make new properties arise that are useful to us. ",null,2,cdlnz09,1rbqyc,askscience,new,2
Jyesss,"Antibiotics target certain essential enzymes and proteins that bacteria have but humans do not, thus giving their specificity. These proteins are coded from genes, so in a round about way, yes, antibiotics are based off of genetic targeting in that they disrupt the proteins that those genes create. Bacteria generally do not become antibiotic resistant by changing the protein the antibiotic targets because this would probably kill the organism in the process. Instead, they evolve new genes that code for enzymes that will break down the antibiotic before it can harm the bacteria. ",null,0,cdme16k,1rbr4k,askscience,new,1
leftnuttriedtokillme,"There are a couple of hurdles.  For one thing, you can't target ""just bacterial DNA/RNA"", since the actual structure isn't any different from human DNA/RNA.  You can go after the proteins that form the nucleic acids, because those are slightly different.  And there are some antibiotics that do exactly that.  But it's rather difficult to target bacterial nucleic acids themselves specifically.

There has been some research into using artificial nucleic acids to target specific segments of a genome and basically turn it off, but I don't think they've been able to get it into a practical form that could be used in medicine quite yet.

There has, however, been success in using DNA to determine what antibiotics a particular organism is susceptible to.  Currently one of the normal steps in treating a bacterial infection is to culture and ID the organisms involved, and also to perform a susceptibility test on them, which determines the effectiveness of a number of common antibiotics.  

Traditionally this was done by exposing them to the antibiotic at certain concentrations and seeing whether or not it grows.  The new process allows us to look for the genes responsible for certain types of antibiotic resistance.

The best example of how this is useful would be for MRSA.  If a doctor suspects a MRSA infection, he would traditionally do a culture, which would take 1-3 days to tell him anything.  The new genetic method could tell him if it was *Staph. aureus* in a few hours, and whether or not it was the resistant form at the same time.",null,0,cdmhr6b,1rbr4k,askscience,new,1
abstrusey,"""Normal"" is usually defined by sampling lots of apparently healthy individuals and then using statistics to calculate an expected range into which the large majority will fall. This range is often referred to as a ""reference interval."" For aspects of physiology (e.g. heart rate, respiratory rate, temperature, blood pressure, etc.), these ranges are typically set as a standard that you read in a book and/or memorize. For test values (e.g. sodium/potassium/pH value of the blood, red blood cell count, etc.), they are often established individually by the testing facility, and they typically print that range next to the value of the sample they analyzed. 

A reference interval can be established by collecting test results from at least 50 healthy adult animals/humans. In this example, therefore, the range would only represents adults. At least 50 juveniles (and you'd have to define the age range) would have to be collected to have a new ""normal"" range determined. Two statistical analysis techniques are used, based on the distribution of the data. For normal distribution (also called gaussian distribution), there is a ""bell curve"". The data has ""variance,"" which is a representation of how widely scattered most of the animals are when compared to the average of all of them (e.g. they may all be tightly clustered near the average (low variance), or they could be very high and very low away from the average (high variance)). This variance is represented by a number called the ""standard deviation."" In gaussian and non-gaussian distributions, you can use the average ± 2 standard deviations to select for the ""middle"" 95% of the data. The lowest and highest numbers that you collected are now your range, and putting a dash between them makes it a reference interval. In vet med, we have reference intervals for most parameters for dogs, cats, horses, cattle, goats and alpacas, and many exotic species as well. 

It is very important to remember, though, that if the range only represents 95% of the samples -- 100% of which were apparently healthy -- then you should expect about 5 of 100 individuals to have values outside of the range and *still* be okay.",null,0,cdlm8h3,1rbtd9,askscience,new,10
gettingoldernotwiser,"Another way of defining ""abnormal"" is increased risk of death/disease.  People with high blood pressures can have an increased risk of cardiovascular disease, stroke or death compared to those with normal blood pressure.

Similarly, abnormal lab values (glucose, cholesterol) confer a higher risk of disease than normal ones. ",null,0,cdlrvxj,1rbtd9,askscience,new,1
Asrat,"In hospitals and other medical facilities, we typically take an individuals blood pressure every 4 to 8 hours and start generating a baseline for the individual. Your primary care physician does this as well to identify any changes. We also ask what you normally run if you are competent enough to answer. Using that information, for example, we can tell if you normally run 95/55 and today your pressure is 125/75, something is wrong and we need to identify that.
",null,0,cdlsaif,1rbtd9,askscience,new,1
patchgrabber,"Copper is toxic to algae and bacteria in moderate to high concentrations. A copper plate should have an antimicrobial effect, yes; brass doorknobs are known to have antibiotic properties and sterilize faster than, say, an aluminum doorknob.",null,0,cdm5yoq,1rbtxa,askscience,new,2
blacksheep998,"I encountered [this study](http://www.ncbi.nlm.nih.gov/pubmed/12042333) a few years back about whales. They found that the energy demands of accelerating their huge bodies to lunge-feeding speeds to fill their massive mouths with seawater and krill are extremely high.

Massive whales are up against the law of diminishing returns, where each unit they increase in size gives them less and less of a return on that investment.

More info: http://www.americanscientist.org/issues/issue.aspx?id=8779&amp;y=0&amp;no=&amp;content=true&amp;page=2&amp;css=print",null,1,cdlt3h1,1rbu2s,askscience,new,6
Izawwlgood,"Some of it has to do with what other organisms are doing. For example, one of the precipitous drops in insect size was due to the development of birds, which are superior fliers. Bison twice as large may be too cumbersome to evade a pack of wolves, for example.",null,1,cdlxbjl,1rbu2s,askscience,new,3
promega,"The largest discovered organism on earth is actually a fungus.  

""The discovery of this giant Armillaria ostoyae in 1998 heralded a new record holder for the title of the world's largest known organism, believed by most to be the 110-foot- (33.5-meter-) long, 200-ton blue whale. Based on its current growth rate, the fungus is estimated to be 2,400 years old but could be as ancient as 8,650 years, which would earn it a place among the oldest living organisms as well.""

Source: http://www.scientificamerican.com/article.cfm?id=strange-but-true-largest-organism-is-fungus

In theory such an organism could continue to grow until it exhausted one of its resources.",null,0,cdmqwxw,1rbu2s,askscience,new,1
null,null,null,3,cdls9rb,1rbu2s,askscience,new,1
deruch,"You need to be more careful with your terms.  How are you defining size?  By mass, volume, area, etc.?  Do you really mean ""animal"" instead of ""organism""?  In terms of organisms, I can make the claim that the largest one is an [aspen forest](https://en.wikipedia.org/wiki/Pando_%28tree%29) in Utah, or maybe a [fungus colony](https://en.wikipedia.org/wiki/Largest_organisms#Fungi) in Oregon.",null,17,cdlse7h,1rbu2s,askscience,new,11
aerugino,"Well, the short answer here is: Defensins. These are small proteins that are found in your saliva that kill bacteria, and serve to protect the inside of your mouth from getting infected when there's a cut. Most of your body's mucous membranes produce large quantities of these defensins in order to protect themselves. They're really quite fascinating proteins

http://www.ncbi.nlm.nih.gov/pubmed/17979749",null,14,cdlq378,1rbu6q,askscience,new,107
laika84,"There are many immunological components that exist in the areas of our bodies that are constantly exposed to microbes - respiratory surfaces, and gut mucosa which in a way includes everything from the mouth to the anus.  There are specialized immune structures in back of the mouth and form what is called ""Waldeyer's Ring"", (consisting of adenoids, palatine, and lingual tonsils,) that are believed to be sampling antigens and serving as a sentinel system for immune response.  In the lower gut there are Peyer's patches, which like the tonsils are essentially unencapsulated lymph-nodes that sample the gut environment for antigens.

However, that's only part of the story.  Sampling for antigens is important to initiate a response, but the true ""magic"" of immunology is that the cells of the adaptive immune response, (B and T-lymphocytes,) are selected, in a fashion similar to evolution.  In the bone marrow, (B-lymphocytes,) and thymus, (T-lymphocytes,) the cells are ""trained.""  There they learn, through the processes of positive and negative selection, how to distinguish self from non-self antigens.  Those lymphocytes that can recognize, *yet not severely react against*, self-cells go on to progress eventually into immature lymphocytes who then wait to be activated when an appropriate antigen interaction + costimulatory event occurs.

So essentially, there's this system where the body can modulate what it reacts to through selection of lymphocytes and I would think it's within the realm of possibility that the lymphocytes ""learn"" to not react against commensal bacteria.  

There are other pieces that come into play as well.  The innate immunity reacts to antigenic determinants that are common to many invasive/parasitic microbes, and not commensal bacteria.  Their receptors are encoded in our DNA and do not recombine like the adaptive immunity, so they react the same time every way a given antigen is encountered.  These include Toll-like receptors, Nod-like receptors, the alternative complement activation pathway, scavenger receptors, etc.  The take home message from this is that these mechanisms are quickly recruited due to the lymph node-like structures that can initiate an innate immune response, which then goes on to set the stage for the adaptive response.  I believe defensins fall into this.

Then you get your specialized antibody classes for mucosal surfaces, (IgA) and many other things that we don't even know that essentially due a balancing act of defending us from microbes that we don't want while not being activated by those we need. ",null,0,cdlthyi,1rbu6q,askscience,new,10
Polyknikes,"OP, I wanted to address the part of your question about ""why can it heal properly when its not dry"" with an example.

Dry areas of the mouth are actually more prone to infection than those covered with saliva.  Since we have discussed how saliva is protective against microbes this makes sense but a good example is dental caries (cavities).

People with xerostomia (dryness of the mouth) are much more prone to getting dental cavities.  For example, Sjorgren's syndrome is an autoimmune disorder where your body develops a sensitivity to your own salivary glands and attacks them leading to decreased salivary production.  These people are much more likely to develop dental caries!  So you can see how important saliva is to maintaining oral hygiene.",null,2,cdlxkeu,1rbu6q,askscience,new,5
chewgl,"Histatins found in saliva also promote wound healing, and seem to do it differently from the defensins mentioned by aerugino. They may actually be more relevant to the mouth microenvironment.

http://www.ncbi.nlm.nih.gov/pubmed/?term=18650243",null,0,cdlvhe5,1rbu6q,askscience,new,3
Spazyak,not all bacteria are harmful and some that are are only harmful in large amounts. a cut in mouth is actually healed more quickly and better thanks to some of these bacteria. Not all bacteria are bad and even good. salkavia and snot contain more bacteria that is good then is bad or even human cells.,null,0,cdmkg5v,1rbu6q,askscience,new,2
iorgfeflkd,"The sun's power is distributed evenly over an area. When you are focusing it with a magnifying glass, you are essentially taking all the light that reaches an area the size of the lens and combining it to a region the size of the focus.",null,2,cdlpmaf,1rbw5f,askscience,new,3
Mazetron,The sun is a powerful source of light on many wavelengths.  The mafnifying glass just focuses light.  You could burn something with an electric light and a magnifying glass of the light was poweful enough.  I have done it with a laser pointer.,null,0,cdlqqmx,1rbw5f,askscience,new,1
chrisbaird,"Light carries energy. Energy causes damage to materials when it is absorbed in a given area faster than it can be dissipated from that area. The ability of energy (and therefore light) to damage materials therefore is a result of a high energy delivered per unit area per unit time, which we call power density. A lens does not create energy, it just focuses the energy so that there is a high power density in one small region and lower power densities in surrounding regions. If the power density is high enough in the focal region, the material heats up faster than it can cool to its surroundings, and its temperature steadily rises. With high enough temperature, materials will melt, burn, ignite, etc.

Light can be focused because it is a wave and waves can be bent (refracted) at the interface between two optically dissimilar materials (such as glass and air). ",null,0,cdmufp9,1rbw5f,askscience,new,1
neverlupus16,"It's the infrared light of the sun. The fusion reactions of the sun release electromagnetic radiation across the spectrum. You have infrared photons, which explains why you feel heat. You have visible light, which explains how we can see things using sunlight. And we lastly have ultraviolet light, which is how our skin burns and tans.

The wave nature of light allows it to be refracted by traveling through a medium such as glass. By focusing the wave (and all of it's energy) to a single point, you change the energy from being diffuse and spread out to being concentrated in one small region. It is now easy for the infrared energy to be transferred to another object. If the energy is transferred faster than it can be dissipated, the object will increase in temperature and possibly reach ignition.",null,8,cdlon4k,1rbw5f,askscience,new,1
therationalpi,"Common misconception about sonic booms: You don't just create a boom at the moment when you ""break the sound barrier."" In truth, for the entire duration that an object is traveling faster than the speed of sound, it generates a shock front. This is more apparent when you look at an image of the effect. Here's some [Schlieren photography](http://library.thinkquest.org/12228/Page4.html) of a supersonic jet. The dark colored bow shocks that start in front of the plane are the ""sonic booms"" that it's creating.

So, in answer to your question, nothing particularly special happens when you reach 2 or 3 times the speed of sound. Indeed, you will still be creating sonic booms at those speeds, but you would likewise be continuously generating booms if you were traveling at 1.2x the speed of sound.

Hope that helps!",null,5,cdlnxzg,1rbw8z,askscience,new,24
rocketsocks,"You don't create a sonic boom only when you pass the speed of sound.

When you are traveling at or above the speed of sound you trail a cone shaped shock front which travels at the speed of sound. You usually only here the sonic boom once as an observer because the shock front only passes over you once.",null,2,cdlo7lt,1rbw8z,askscience,new,5
neverlupus16,"The question here is phrased incorrectly: when it's a cold morning and you breathe out, you see the water in your breath. But it's not water VAPOR. Water vapor is gaseous water. When it is cooled sufficiently, it will condense into liquid water. 

That condensation is what is actually happening when you see your breath. You see VERY tiny droplets of liquid water suspended in the air. They are so small that the current of air from your lungs will suspend and move them in front of you AS IF they were a gas.",null,1,cdlokzd,1rbyel,askscience,new,18
MarkWW,"Supersaturation is the point at which you can no longer stir sugar into your tea, because the water - at that temperature - can no longer dissolve solids into it. Cool it further and more of the sugar comes out as a solid.

The same happens with cold air &amp; foggy breath. Think of other forms of condensation - water appearing on the outside of a cold glass, or on a window. This is water vapor (gas) from the air turning into a liquid, because the surface is so cold that it turns the gas into a liquid.

The same is true for the breath you can see on cold day. The air outside your body is so cold, that the warm, moisture laden air inside your body instantly turns into liquid - albeit, in very tiny droplets.

Something similar happens when you create rock candy &amp; I encourage you to make rock candy with your brother... Because, yum.

Basically warm gas (or liquid) can support lots of gaseous liquid (or solid) in it because it's so chaotic and full of energy that it can keep the liquid (or solid) a gas (or liquid).

As it cools down, it loses energy and more of the matter that's right at the edge of being a gas/liquid reverts to the state you normally associate it with at that temperature. In other words, water isn't always a liquid between 0C and 100C - it's in a constant state of flux, with more water being gaseous the warmer it gets.

For more mind blowing facts - research Swamp Coolers, which cool the air by adding water to it.",null,0,cdlr7a9,1rbyel,askscience,new,4
chrisbaird,"Water vapor always comes out of your mouth in gas form (water vapor) when you breath. Water comes out of your mouth in liquid form when the air is cold enough to condense the gaseous water that you always breath out into small drops of liquid water, which we see as a white cloud of steam. ",null,0,cdmu6un,1rbyel,askscience,new,1
goingforth,"It is likely a combination of both, but your former suggestion likely has the largest effect. Moving clouds tend to maintain a consistent shape over relatively short periods of time, and the distortions that do occur are often just that, and don't involve the addition or subtraction of parts of the clouds (again, this is on a short time frame) Likewise, clouds tend to move faster with higher wind speeds, suggesting a correlation. Your second suggestion is responsible for clouds forming, changing shape, and disappearing, but not as much for the movement of clouds.",null,0,cdlptxo,1rbymp,askscience,new,2
EdwardDeathBlack,"I am not sure you are asking a ""clean"" question. 

First,  let us assume room temperature (293K) and atmospheric pressure (101300Pa or so) for most/all of our discussion. Under different condition, water cohesion can change drastically. 

In the absence of gravity, water will easily for an orb the size of a baseball. SO water cohesion

In the presence of gravity, water reacts like a viscous material. It means the rate of deformation of the water is proportional to the stress applied. Note that a viscous material will always deform in the presence of an arbitrarily small stress.  So if I consider this, you are asking how to make water into an elastic material, instead of a viscous material. Freezing seems the obvious answer. 

There are also polymeric materials that you can add to the water to bind it in place and make it an elastic material, [Jell-O](http://en.wikipedia.org/wiki/Jell-O) being particularly well known. 

No sure I answered your question, but as I said, I am not sure I completely grok'ed your intent...",null,0,cdlo75d,1rbyp8,askscience,new,1
SmellyRaghead,"Yes, you can alter the surface tension by using surfactants. As for making a giant ball of it, probably not, unless you were in zero gravity.",null,0,cdlr0fy,1rbyp8,askscience,new,1
dreemqueen,"If you dissolve ionic compounds like NaCl into water, the water becomes more polar and cohesive.  If you dissolve sucrose which is an organic covalently bonded molecule in water,  the water becomes less polar and less cohesive.  You can see the difference if you measure the wetting or contact angle.   Salt increases the contact angle, sugar decreases it.  This is the best way to measure relative surface tension.",null,0,cdmqcd4,1rbyp8,askscience,new,1
SingleMonad,"You don't have everything pinned down in your question.  Namely you need to know how big an ice cube.  Given a 500 ml glass of water, it may well be impossible with what would be considered a conventional ice cube.  

Assuming that no heat is lost to the environment, setting the heat lost from the water equal to the heat gained by the ice, using values in Wikipedia for specific heat and heat of fusion for water, and assuming the final temperature is 0 c, I get that the original ice cube warms by the following amount:

**Delta T = 200 M/m** (degrees c), where M is water mass, and m is ice mass.  Since the final temperature (0 c) is 271 degrees above absolute zero, it had better be a pretty big cube.

Disclaimer:  my arithmetic sucks.  Don't bet anything important on the basis of my answer.

Edit:  also assumed the water was initially at room temp (21 c).",null,1,cdlodsp,1rbyvm,askscience,new,9
Farnswirth,"Due to the conflict between /u/just_helping and /u/InexplicableContent results I did my own calculations, which came out to:

167000+2100(Initial water temperature, C) = 576.03(mass of ice) - 2.11(mass of ice)*(temperature of ice, K)

with all masses in grams.

When the initial water temperature is 25C and the temperature of the ice is absolute zero, the required mass of ice needed to freeze the whole cup is: **381g, which agrees with** /u/just_helping 

With an initial water temperature of 21C, and ice temperature of absolute zero, the mass of the ice is: **336g**

For a more realistic scenario, you could assume the initial water temperature is at 1C, and the ice has been cooled with liquid helium (4K).  This gives an ice mass of: **298g**  For liquid nitrogen (77K) the ice mass is: **409g**",null,2,cdlr4kj,1rbyvm,askscience,new,5
null,null,null,0,cdlpv6f,1rbyvm,askscience,new,2
tysongrey,What temperature is the room?,null,0,cdlnj0m,1rbyvm,askscience,new,1
Gradri,"That depends on the initial temperature of the water (probably about 21 °C), and the mass of the ice cube.",null,0,cdlnxxe,1rbyvm,askscience,new,1
EdwardDeathBlack,"You are asking two questions. The first one is what do thermal fluctuations look like in a  DNA molecule. The second is can DNA melt. 

First, the double helix of DNA is indeed held together by Hydrogen bonds. Above a certain temperature, the energy is high enough to overcome those bonds. The double helix melts and the two molecules separate. This has been extensively [studied](http://en.wikipedia.org/wiki/DNA_melting) and is an essential tool of biotechnology (and especially of [PCR](http://en.wikipedia.org/wiki/Polymerase_chain_reaction) ). If you have any background in the thermodynamics of how, say, water, freezes and melts, you will find it very similar to look at DNA melting. 

Now, to address the first point, what does a hot (but not melted) DNA molecule look like. First, a DNA molecule will [""ball up""](http://en.wikipedia.org/wiki/Random_coil), it doesn't stay a nice stretched thing. That little ball jiggles and wiggles along with the solvent, exhibiting [Brownian motion](http://en.wikipedia.org/wiki/Brownian_motion) . It will also have [thermal phonons](http://en.wikipedia.org/wiki/Phonon), of the [1-dimensional kind](http://en.wikipedia.org/wiki/Phonon#One_dimensional_lattice). 

If the temperature is high enough, but not enough to melt completely the DNA, there will also be [""bubbles""](http://www.bu.edu/meller/research_bubbles.html) forming alongside the DNA of partially melted areas.. These will have a limited lifetime, will occur predominantly in areas of weak Hydrogren bonding (AT rich areas), and have a lot of roles to play in living organism. 

Anyway, all that is already a lot. Maybe I'll let you read what I linked to and ask more questions rather than drone on...",null,0,cdloddj,1rbz3z,askscience,new,2
owaisofspades,"yup, you have about 10 layers and 5 different types of nerve cells  on your retina, and the photoreceptors are one of them. They have pigments on them (rhodopsin for rods and iodopsin (?) for cones) which respond to light, and transmit the signal through the rest of the layers into your optic nerve",null,1,cdlos9c,1rc0vh,askscience,new,4
dakami,"Genes (OPN1SW, OPN1MW, and OPN1LW) express one of three proteins, called opsins.  When an opsin is hit by a photon, it has some chance of isomerizing.  This process causes an electron to be released, and the chance is related to two things:

1) The wavelength of the photon
2) Which opsin is hit

Your ""red"" opsins are more likely to be isomerize in response to ""red"" light.  (This is an extreme oversimplification.)

As you can imagine, nerves are quite good at responding to electrical signals, and creating complex computational cascades.  So vision stops being about light really, really early.

Once isomerized, the spent opsin is recycled.  Your retina is actually among the most (if not the most) metabolically active portions of the body.  Really, your eyes work a lot more like living film than CCD/CMOS silicon.",null,0,cdlqugn,1rc0vh,askscience,new,2
brawnkowsky,"not all of it is rinsed, some of it passes into the epithelial cells.  once it passes through the epithelium, it is able to inhibit cyclooxygenase, an enzyme necessary for the creation of prostalgandins.  prostaglandins are involved in pain and vasodilation, two major components of inflammation, which is what a 'breakout' is

source: wikipedia, student of medicine",null,1,cdm050e,1rc303,askscience,new,2
Quant_Liz_Lemon,"You need to be awake during brain surgery in order to ensure that nothing important is damaged during the procedure. This is especially important if surgery is being conducted near functional areas of the brain. Otherwise, you might risk permanent brain damage. Depending on what area of the brain you're near, a surgeon might ask you to make specific movements, count, say specific phrases, etc, while performing the surgical procedure. 

[source: Mayo Clinic](http://www.mayoclinic.org/awake-brain-surgery/about.html)",null,0,cdluba1,1rc3d0,askscience,new,7
U235EU,"I work for a medical device company, one of our products is used to treat movement disorders by deep brain stimulation. The patients are conscious during the implant so that the doctor can insure the proper location of the stimulating leads by direct feedback from the patient, and by neurological monitoring. See this video:

http://www.youtube.com/watch?v=lUG8iFxukig",null,0,cdm1wfl,1rc3d0,askscience,new,1
Polyknikes,"Some surgeons still perform operations while the patient is awake but a more modern technique is to use various functional brain imaging techniques prior to the surgery while asking the patient to perform certain tasks, seeing which areas of the brain light up near the tumor, and then avoiding those areas during the surgery.  With this technique the patient can be fully sedated during surgery.",null,2,cdlxqlz,1rc3d0,askscience,new,1
Ruiner,"I'm having trouble understanding exactly what you mean. But let me try to answer:
(First, keep in mind that if there is a liquid inside, things are a lot more complicated because of convection, which makes thermalization faster than if there was only conduction)

You start heating the bottom of the point. At this point, that surface gains heat through the source and loses heat through conduction, fine. If you leave the source on, it will at some point reach a steady state. This state is not really thermal equilibrium, but it's a state in which you have a constant flux of heat from the bottom to the top, so there is a temperature gradient. You know that you reached the steady state because the temperature of everything inside the pot is no longer increasing, but all the energy coming in is compensated by energy going out. (btw, if you're trying to boil a pot of water, you never get to see the steady state, since the average temperature keeps on increasing).

Once you turn off the source, the steady state will now be approximately thermal equilibrium. It would be thermal equilibrium of the pot was a closed system, but since it interacts, it will still have a temperature gradient - the coldest parts being those that interact the most with the outside. ",null,1,cdlt6gm,1rc3z2,askscience,new,2
PENIS_VAGINA,"Interesting question. I'll try to answer this. 


First off sugar does not neutralize the acidity. However you are correct that sourness is based on H+ ion mediated receptors on the tongue (TRP family receptors). 


I don't believe the sugar changes our tongues ability to taste sour because sweet and sour receptors are distinct from each other so there should not be competitive inhibition of sour receptors when a substance that activates sweet receptors is present. In fact, it is possible that the low pH from the sour substance is enhancing ligand binding to sweet receptors. This is what happens when you ""Taste Trip"" with miraculin. 

My thought is that you are experiencing both tastes simultaneously and therefore your brain in an attempt to process both tastes is not pronouncing the sour taste as strongly as it would if sour was the only taste happening. 

I am looking for a source to confirm this and will update if I can find something. 

Edit: May as well add (because its a common misconception) that the classic ""taste map"" that shows different areas of the tongue to have varying densities of different kinds of taste receptors is FALSE.",null,0,cdlxpl2,1rc6y9,askscience,new,1
zalaesseo,"Stationary electrons generate electric fields. No current

Moving electrons generate magnetic fields. Constant current

Accelerating electrons generate electromagnetic fields. Changing current.

Oscillating circuits generate changing currents, and thus electromagnetic fields. 

Then we can either amplitude or frequency  modulate signals into the carrier wave.",null,0,cdludhm,1rc957,askscience,new,3
drzowie,"zalaesseo gave a nice answer.  Another, perhaps even more simple, is:

Shaking electric charge produces electromagnetic waves, just as shaking objects in air produces sound waves (the physics is different but the fundamental waveness is the same).  We make electromagnetic signals by shaking electric charges.  

Every time you switch on or off a circuit, you create electromagnetic waves - so there's a lot of electromagnetic noise all around us.  To punch through that interference, devices that signal each other (like radios, or phones, or wifi units, or whatever) pick a particular frequency and shake electric charges at almost exactly that frequency.  That's like cutting through the noise of a large room full of drunk people, with a particular clear tone (say, from a flute).  You can discern the flute even though it's not any louder than the people, because all the energy is concentrated into a single tone.

Small variations in the strength or frequency of the electromagnetic signal carry the information people want to transmit.  For an old-style AM radio, the strength of the signal indicates where the speaker cone of the receiver should go.  The station gets ""louder"" and ""softer"" very rapidly to move your receiver's speaker cone around, forming sound waves.  FM radios use small changes in the pitch of the radiofrequency tone to control where the speaker cone should go.  Digital radios like wifi or modern cell phones use variants on those two strategies to communicate streams of bits.  Those bits encode the sounds and internet packets that are being transmitted.
",null,0,cdlw28q,1rc957,askscience,new,2
chrisbaird,"Causing electric charges to oscillate (bump, jiggle, shake, collide, change energy levels, transition between states, etc.) *always* creates electromagnetic waves, and not just in fancy circuits. The chair you are sitting on is emitting electromagnetic waves right now (mostly in the infrared) because its electrons are slamming together due to thermal motion. There are many ways to oscillate electric charges, and so there are many ways to create EM waves:

- heat them up so they collide more (incandescence)
- shake them up and down a wire using applied voltages (antenna radiation)
- excite electrons into different states and then have them transition back down (lasers, fluorescence, phosphorescence, gas discharge, chemiluminescse)
- send electrons passed a system of magnets that makes them wiggle or circle quickly (free electron laser, cyclotron radiation)
- smash a charged particle at high speed into a material (Brehmstrahlung)

If you want to send a signal on an EM wave, then you need to precisely control the shape of the EM wave. You must therefore precisely control the movement of electric charges. Electric circuits come in handy for that.
",null,0,cdmu1if,1rc957,askscience,new,2
JohnSmith1800,"There's sort of two features going on here, the macro and microscopic, so I'll detail them both.

The light first passes through the pupil and lens, which focus it in particular on a small patch of the retina known as the fovea. This is where the concentration of cone cells (those which detect colour) is highest. You also have a lot of rod cells (just detect light generally, more sensitive in dark settings) here, but they're more common as you move away from the fovea. Collectively rod and cone cells are photoreceptors. These photoreceptors are hooked up to bipolar cells, horizontal cells and ganglion cells, which together do some initial ""processing"" of the image before it passes down the optic nerve to the brain. Interestingly, because the horizontal, bipolar and ganglion cells are actually between the retina and the pupil, the optic nerve has to travel through the retina, which creates a blind spot in each eye, about 15degrees off-centre (which your brain lies to you to fill in).

On the microscopic level, photons travel through the eye to the retina, where some encounter either rod or cone cells. In rod cells there is a stack of ""plates"" which are coated in an enzyme called rhodopsin. When a photon is absorbed by rhodopsin, it changes conformation and can activate another protein known as transducin. Transducin is what is known as a ""G-Protein"", when activated it in turn activates another protein in turn, which then changes cGMP (a small second messenger) into 5'-cGMP. This leads to a closure of Na^+ channels. This hyperpolarises (makes more negative) the cell. Neurons only release neurotransmitter when they depolarise, so this reduces the release of neurotransmitter. I'm not familiar with the exact pathway in cone cells, it is photopsin rather than rhodopsin which absorbs the photon, but otherwise I believe it to be generally similar. This whole process actually takes quite a while, about 200ms from memory between when the photon hits and when your photoreceptor's sodium channels close. This is because of the ""protein cascade"" which occurs, it takes time. However, it does greatly increase your sensitivity to light: A single rhodopsin can activate ~500 transducins, which will in turn do ~500 cGMP's each. This will close ~100 sodium channels, stopping 10^~11 ions, and hyperpolarising the cell by almost a mV. That is to say, your eyes are actually very sensitive to light.

Intriguingly photoreceptors are actually inhibitory neurons, they release a neurotransmitter which hyperpolarises other neurons. As such, when a photon is absorbed their rate of firing decreases, which increases the firing rate of bipolar cells. The rest of the neural pathways are way over my head, but it involves ""receptor fields"" and the other accessory neurons.

Edit: 
Source: I'm a 2nd year physiology student / L. Sherwood ""Human Physiology: From Cells to Systems"" 8th Edn.
Also, I've left out a protein or two in the signalling pathway, but in terms of answering the question I think it's sufficient?",null,0,cdlu2z4,1rc9cl,askscience,new,2
rupert1920,"You can read about it in the Wikipedia article for [phototransduction](http://en.wikipedia.org/wiki/Phototransduction).

Basically, it involves the photoisomerization of a molecule of [retinal](http://en.wikipedia.org/wiki/All-trans_retinal) - the incoming photon excites an electron in that molecule, and allows for a rotation of one of the double bonds. This causes a conformational change in the protein that houses it - [rhodopsin](http://en.wikipedia.org/wiki/Rhodopsin) - which then sets of a cascade that leads to the nerve signal.",null,0,cdlu45k,1rc9cl,askscience,new,1
do_od,"Mountains on Earth will never be much taller than they are now. That is  because at some point the weight of the mountain causes such enormous pressure that the base of the mountain will start to liquify and deform.  

Planets with lower gravity can have higher mountains. A prime example is [Olympus Mons](http://en.wikipedia.org/wiki/Olympus_mons), the tallest mountain on Mars at 22 km. The surface gravity on Earth is about 2.7 times that of Mars. Because pressure scales linearly with height, we can expect the tallest mountains on Mars to be about 2.7 times taller than the tallest on Earth. 2.7 * 8.9 km = 24 km, is in that ballpark.  

Wether or not Mt Everest is higher than the atmosphere depends on your definition. You could say that it is because humans can't survive there for very long.",null,0,cdm3ptj,1rc9ea,askscience,new,3
soylentblueissmurfs,"One of the reaons inbreeding can be harmful is you run a much higher risk of recessive genetic disease since your relatives are more likely to carry the same damaged alleles. However, if you inbreed enough those mutations will be weeded out so the answer is basically: they are SO inbred it's a small problem.",null,0,cdlut33,1rc9t0,askscience,new,5
mak484,"I'll handle the follow-up question. Mice are an ideal model organism for many reasons- they have a relatively short gestational period, produce many offspring, and reach sexual maturity very quickly. All of these factors lead to incredibly short generation times with a large exponential increase in population size with each generation. This allows scientists to very easily weed out deleterious recessive alleles, leaving breeding stocks with very uniform and well-understood genotypes.Compare mice to humans- it takes 12-16 years to reach sexual maturity, and females can only give birth (naturally) to 1-2 offspring per year. Factor in a high level of allelic heterozygocity, and you can see where aliens would have a difficult time creating a genetically uniform breeding stock.*I got a little morbid below, sorry if this disturbs anyone*Now, if I were the aliens, assuming I had unlimited resources and appropriate technology, I would find brother-sister pairs of paternal twins, and harvest their sperm and eggs. I would then fertilize all of the eggs simultaneously, and begin genetic testing once the fetuses reached several weeks. The offspring with the highest levels of homozygocity would be selected for the F1 generation of breeding, where I would repeat the process. After maybe 5-10 generations complete homozygocity and lack of deleterious genes could be reached. Since I started this process with numerous genetically diverse brother-sister pairs, I could develop multiple lines of humans that carry whatever combination of genes I want. ",null,0,cdlvtyw,1rc9t0,askscience,new,4
Platypuskeeper,"&gt; What exactly is an oil? [the requirements to call something oil]

Typically something with a lot of long-chain hydrocarbons in it, but in the broadest sense (e.g. essential oils, vegetable oils) it could really be any liquid composed of non-polar (oily) compounds. It's not a precise (or 'technical') term as far as chemistry is concerned.

&gt; Are there oils which do not have carbon in it?

[Silicone oil](http://en.wikipedia.org/wiki/Silicone_oil), although the term 'oil' there is more because of its use as a lubricant than its chemical composition.

&gt; Is it true that kerosene is not technically an oil? And why?

I don't see what usable definition of 'oil' would exclude kerosene. It's not _crude oil_, it contains a more limited subset of hydrocarbons. But it's still a hydrocarbon mixture.


",null,1,cdm06t8,1rc9wk,askscience,new,4
dapwnsauce,"Most oils do not dissolve in water as they have two characterizing features, a polar(hydrophobic) and non polar end(hydrophobic).  An example is a [Micelle](http://upload.wikimedia.org/wikipedia/commons/thumb/4/4d/Micelle_scheme-en.svg/532px-Micelle_scheme-en.svg.png).  Kerosene is an really long hydrocarbon chain in comparison to others which are used as **fuels**(although there are some which are longer).  Kerosene does not have a polar end, it is in other words a potential backbone to an oil.  Considering this, it would be classified more as a **fuel** rather than an oil.

Silicone oil does contain carbons and it maintains its ""oil"" status due to the polarity of the Si-O bond.  Non-polar end being the carbon groups that are attached to it.  The design of silicones have multiple chemical uses which sets them apart from conventional oils.  
  
The fluidity of an oil really depends on the hydrocarbon backbone.  The arrangement of the molecules can determine whether they become more rigid and less viscous or more fluid.  When oils are able to tightly pack together, they tend to form more viscous structures and even solid structures (ie butter/shortening).   When they have rigid backbones (ie kinks in the chain) they tend to form more fluid structures such as olive oil.  

Are there any oils that do not have any carbons in them whatsoever?  Theoretically there should be some that exist, though none come across my mind at the moment.

Hope that helps.",null,0,cdmkda9,1rc9wk,askscience,new,2
weinerjuicer,"without drawing energy from the environment, it seems like it should in principle be possible to exchange kinetic energy due to translational motion for gravitational potential energy: if they are going slower horizontally after the dive-down-then-pull-up move it may not violate conservation of energy.",null,2,cdlurjr,1rc9xn,askscience,new,6
drzowie,"Without some effect to bring energy to the hang-glider, diving and rising will always cause the hang-glider to go down.  That will always happen faster than if the hang-glider pilot just flew along at his best-sink-rate speed and attitude (which should be close to his best-glide-ratio speed and attitude)

That said, energy is not particularly well conserved in the hang-glider's system.  An experienced hang-glider pilot can make use of many counterintuitive effects, mostly involving wind shear or vertical winds, to scrub energy from the environment.  

Hang-gliders near Torrey Pines in San Diego, CA can fly all day without any thermals at all due to the vertical wind break at the Torrey Pines cliffs.
",null,2,cdlw5tk,1rc9xn,askscience,new,4
zlatan08,"According to conservation of energy, the total energy, which is the sum of gravitational potential energy, kinetic energy, chemical potential, magnetic potential etc.., must remain the same. In this case, let's consider just kinetic and gravitational potential energy. At any point in time, as long as no outside forces act on the glider (i.e. thermals), their sum must be constant. When the glider dips down, it trades gravitational potential energy for kinetic; when it rises back up, it trades the kinetic for gravitational potential. If it tries to rise to high, the kinetic will come close to zero and the glider will stall. In real life, if the glider keeps trying to dip down, rise, stall and then dip down again, drag forces on the surfaces on the glider will cause it to lose energy and every time the glider stalls, its height will be lower than the previous time. Drag would be considered the outside force in this case and energy would not be conserved.",null,1,cdlv4jy,1rc9xn,askscience,new,2
_Jordan,"I have heard that competition gliders, under the rules of the competition get towed to a certain height, and see how long they can stay in the air.

A trick they use, is to carry a tank of water with them when they get towed to the starting altitude. They immediately dive down to the ground (trading altitude for speed), then dump the water low to the ground, and go up (trading speed for altitude). Using this trick, they can rise up above the starting height, and stay in the air for longer.

If you see a hang glider diving off of something high, and then rising higher than they started, look to see if they dropped anything at the bottom of their dive.",null,1,cdm1ln5,1rc9xn,askscience,new,1
Ruiner,"This is a cool question with a complicated answer, simply because there is no framework in which you can actually sit down and calculate an answer for this question.

The reason why know that photons travel at ""c"" is because they are massless. Well, but a photon is not really a particle in the classical sense, like a billiard ball. A photon is actually a quantized excitation of the electromagnetic field: it's like a ripple that propagates in the EM field.

When we say that a field excitation is massless, it means that if you remove all the interactions, the propagation is described by a wave equation in which the flux is conserved - this is something that you don't understand now but you will once you learn further mathematics. And once the field excitation obeys this wave equation, you can immediately derive the speed of propagation - which in this case is ""c"".

If you add a mass, then the speed of propagation chances with the energy that you put in. But what happens if you add interactions? 

The answer is this: classically, you could in principle try to compute it, and for sure the interaction would change the speed of propagation. But quantum mechanically, it's impossible to say exactly what happens ""during"" an interaction, since the framework we have for calculating processes can only give us ""perturbative"" answers, i.e.: you start with states that are non-interacting, and you treat interactions as a perturbation on top of these. And all the answers we get are those relating the 'in' with the 'out' states, they never tell us anything about the intermediate states of the theory - when the interaction is switched on.",null,209,cdlyfi3,1rccq1,askscience,new,1149
DanielSank,"/u/Ruiner's answer is great but maybe got a little bit too technical for OP's current level. I'll try to add to that great post.

Think of what happens when you dip your finger in a pool of water. You see ripples propagate outward from where you dipped your finger. Those ripples move at a certain speed, and occupy a reasonably well defined region of space.

Photons are the same. The water in that case is ""the electromagnetic field"". The ""photons"" are the ripples in the water. They don't accelerate. The water itself has certain physical properties (density, etc.) that cause any of its waves to move at a specific speed. The water waves are not a single object in the usual sense... they're displacements of something else. You should think of ""photons"" the same way.

Does that help?",null,84,cdlsqys,1rccq1,askscience,new,465
miczajkj,"Because a photon is an massless particle it always travels through space at a speed of c. 
In quantum field theory the photon is described by a certain disturbation in the photon field and this disturbation just travels at c, regardless from what it is caused. 

This doesn't mean, that you can't talk about photons in different movement states: in relativistic (quantum)-mechanics you need to expand on the definition of momentum. It follows, that even particles with the same speed can have different momentum, depending on their total energy. ",null,20,cdlusqz,1rccq1,askscience,new,40
dronesinspace,"In addition, why can light be 'bent' around massive objects?

To my knowledge, light bends around objects like black holes and stars because they're on a straight path, and that the path is 'bent' by the object's gravity well.

Related question - if that is true, then photons that are bent around a star would at some point be moving along the gravitational field's equipotential lines, right? Or do they? Can photons just move between equipotential lines freely because they're massless?",null,10,cdlwc5k,1rccq1,askscience,new,18
ArabianNightmare,"Photon is just a way to 'quantify' the electromagnetic wave in ""space"".

The wave always moves with the speed of c.

A photon is just a way to try to convert the wave notation to classical mechanical-physics notation.  That is why it has 'iffy' qualities, such as not having mass while it is a particle, etc.

Try not to get confused by how it is taught, and go drop a few pebbles into a nearby fountain.

*edit: typos.",null,13,cdlxd5j,1rccq1,askscience,new,18
robjtede,"A Level Physicist's point of view...

The photon would be created with an instantaneous velocity of 'c':

My premise here is that photons cannot be described in the classical model using F = ma or the like. They are neither particles nor waves and behave in ways that we do not yet fully understand.
It's like when a photon is being pulled towards an event horizon, does it accelerate beyond 'c'? No, it is simply blue-shifted so that it has a higher energy with the same speed.

To me, this means that a photons must ALWAYS have a speed of 'c'.",null,10,cdlvk3k,1rccq1,askscience,new,17
cougar2013,"If I'm not mistaken, virtual photons don't necessarily travel at c, but real photons do. This is looking at photons from a quantum field theory perspective. Obviously, there is no bright-line difference between real and virtual particles, but disturbances in the electromagnetic field that propagate at c are said to be real because they can go on infinitely, whereas virtual photons are not stable.",null,7,cdlz5z8,1rccq1,askscience,new,14
Plowplowplow,"Quantum mechanics is not well-developed enough to answer such a question; what happens during the release of a photon is outside the bounds of modern science; we simply do not know.

There IS something that happens right before a photon is emitted that we simply aren't sophisticated enough to have modeled.

It's just like we don't know exactly what happens when an electron drops or raises an energy level; we understand broad implications, like the change in total energy, and other such factors, and there are atto-second measurements being made today in 2013 that are revealing these interactions little by little, so every year we will have a better and more in-depth explanation of how fundamental particles interact, and thus we will slowly begin to be able to answer your question with more and more precision; but today, really, your question just asks about something that happens during a time-frame that our instrumentation cannot handle (like sub-attosecond interactions, etc)",null,10,cdlushn,1rccq1,askscience,new,16
jgemeigh,"Alternative question I would love to have answered--what happens to photons that are observed by the observey-things in our eye? Is any of that light (or whatever it is) transferred Into information, or is 100% of it reflected/refracted/lost?",null,10,cdlz6us,1rccq1,askscience,new,15
ThatInternetGuy,"Infinite acceleration. If photon had finite acceleration, at some point in the fastest timescale, you would be clocking/observing the photon traveling slower than the c speed of light, and that would violate general relativity. Remember, a massless particle has to travel at the speed of light in all frame references. Wait for it...

Here's the kicker: Everything travels at the speed of light, according to the tried and true theory of special relativity. You, I and all the planes in the sky get that same energy to travel at this cosmic 'c' constant speed, but we who have mass travel in time dimension in addition to space dimension. You don't notice you're traveling at 'c' speed because 'time' passing by at near 'c' speed is a common sense and native to you since you're born. To the massless photons, they travel at 'c' speed in only space dimensions, and they don't experience time at all. Remember, space and time are just dimensions. It's proven time and time again in special relativity tests. What we don't understand is why time dimension moves uniformly to one direction, not reversed.

More info: http://physics.stackexchange.com/questions/33840/why-are-objects-at-rest-in-motion-through-spacetime-at-the-speed-of-light",null,11,cdlzvzx,1rccq1,askscience,new,14
JohnPombrio,"There simply is no time reference to the photons and neutrinos so there is no speed to measure. To the photon, it leaves one atom and strikes another instantly, whether that atom is next to the emitting atom or across several galaxies. To US, there seems to be a finite speed but that easily changes by going from one material to another (vacuum to air to water to air to the eye for Sunlight for example). The photon also smears out like an ink blot on paper as it travels only to be locked into a particular place when it is used, viewed or measured. Truly is a strange place, the subatomic.",null,11,cdm5uag,1rccq1,askscience,new,14
mhd-hbd,"Well... We have a clash of intuitions here.

Photons are quantum objects. They don't have a point-shaped location nor a vector-shaped momentum the way that we think about classical particles.

Strictly speaking, all of physics is state-less. In any given physical system there is exactly one answer to what happens next. Put plainly any physical system that contains photons demand they move at the speed of light.

It simply cannot be any other way.

You might say that it ""instantly"" accelerates or some such and it might be true in some ways, but it still conveys the wrong idea.

Photons propagate at the speed of light. Always and ever. Acceleration implies that it changes in speed.",null,12,cdmelu9,1rccq1,askscience,new,15
Thalesian,"The simple answer is that it leaves the photon source and reaches its destination at the same 'time'. But let's walk through it:

Einstein said a couple of funny things with his theory of relativity. First, that E = MC2. E is energy, M is mass, and C is the speed of light. He also said that space and time were the same thing - they could be characterized as a space-time continuum. The implication of this was that if you have mass, then for you to cross a distance, you would also have to cross time. Look around you - for you to walk to a wall or a chair would require you to travel both space and time. 

But he didn't call it relativity for nothing. The concepts of distance and space are not universals. Pretend that you get in a spaceship that can travel 99.9% the speed of light. You can't go the speed of light because you have mass, and with mass comes a speed limit. But let's pretend Apple built a fancy spaceship, then Samsung made a copy called the Galaxy SS, and you get to take it for a drive. You hop in and journey for the stars, traveling 99.99% the speed of light. Your twin brother/sister stays on worth to watch over things. However, after a year you realize that you can't live without reddit because ಠ_ಠ, and turn back for Earth, again at 99.99% the speed of light. How much time has passed for you? Easy answer, 2 years. But much more time has passed on earth, hundreds to thousands of years, depending on how close to light speed you approach. Your twin brother/sister is either old, or long gone. The effect is known as Time Dilation. 

This phenomenon is weird. The faster you go relative to another person your respective perceptions of time diverge. But you can't go the speed of light because you have mass. For a photon, which is massless, the speed of light is possible. But, if time slows down for you relative to folks on Earth as you move in a spaceship, how much time passes for a massless photon? 0. In Einstein's view of physics, the speed of light is a constant, both space and time are relative experiences for particles with mass.

This is a profoundly weird view of the world. We describe light as traveling at a set velocity of 299,792,458 meters per second. We even define distances by the amount of time it takes for light to travel at this speed. Proxima Centauri is 4.24 light years from Earth, meaning light takes that long to reach your eyes. But to light, no time passes, and no distance is crossed. A photon leaves the star and enters your eye at the same time. There is no acceleration to the speed of light, it is the speed that exists when you have no mass.  

Incidentally, this is why the wavelength idea of light, while useful for mathematical predictions, is incorrect. A wavelength requires a length, and photons don't have a length anymore than they have an experience like time. You may hear about folks who have slowed lights to (almost) a stop, but all they have done is change the speed of light relative to us by adding obstacles like cooled Rubidium atoms. As photons take a long path (in our frame of reference) through multiple electron shells between atoms, it seems to take longer for them to cross a distance. But, at the end of the day, they move at the speed of light.

We can create photons, and when you see them you are destroying them in your eye. In fact, the very detector destroys the photons it measures. Strictly speaking (and if I'm wrong on this, correct me), a photon has yet to be observed before its point of annihilation. The idea of acceleration doesn't work right because that assumes there was a position of rest. Rather, think about photons as constantly in motion at the speed of light until annihilation. Without M, there is only E = C2. 
",null,9,cdlyunp,1rccq1,askscience,new,13
riotisgay,"Mass doesnt get created when a photon does, and massless particles naturally travel at the speed of light, like a particle with mass travels at 0 speed without energy. It would be as weird to say that a particle with mass deccelerates from light speed to 0, as to say a particle without mass accelerates from 0 to light speed when being created.",null,9,cdm1s0i,1rccq1,askscience,new,12
sstults,"It might help to think about what's happening with the photon just prior to the photon emission. It's already emitting a field which propagates at the speed of light. Then suddenly it ""moves"". It's still emitting a field at c, but the change itself is also propagating at c. That thar is a photon.",null,9,cdm2bky,1rccq1,askscience,new,12
SnickeringBear,"Several decent answers have been given, but one significant part of the interaction that generates photons has not been covered.  Remember than the law of conservation of mass/energy applies, it is not possible to create or destroy mass/energy. (with a bunch of caveats, mostly having to do with ""information"" going places it can't be retrieved from!)

A photon is generated at the point in time/space that an electron changes energy state.  When an electron has been excited by an energy source, it rises higher in the electron shells around the atom's nucleus.  At this higher energy point, an opening in a lower shell is available.  The electron falls into this lower energy shell and must in the process lose energy to stay there.  The ""pressure"" developed as the electron transfers has to be released in the form of a photon.  The number of shells the electron drops determines the total energy dumped into the photon.  The photon inherently cannot exist at anything other than the speed of light.  Therefore, it always travels at the speed of light.

There is much much more that is not understandable or explainable in this process without the use of quantum mechanics.",null,9,cdm8vt9,1rccq1,askscience,new,12
bloonail,"A photon can be modeled in the classical sense somewhat like a kink in the electric field that has become detached from its source as the source retreated. So a rotating electric charge can emit photons because the electric field cannot collapse back on the moving charge as the charge recedes. That portion of the field that is withheld from collapsing by relativity is released as a photon. 

However more accurately the electromagnetic field is maintained by photons. It only exists through them as a mediating particle. The field measured at any point in an electromagnetic field is measured in photons. In the situation of a static non-moving charge the photons are in a 1/r2 relationship through radio waves to their point of origin, but those photons do spread out infinitely at the speed of light from that point.

The ""kink"" idea is an unsatisfying 1930's [model](http://m.eet.com/media/1141968/82251f5.pdf) but it hints to some degree how the photon is released at the speed of light. It is by nature at the speed of light, at least in this model, because it is energy that has separated away due to kinda getting lost in space and unable to retreat back onto its charge. It is lost because the electric field is expanding at the speed of light. 

Its a weak model. Its useful mostly for showing how high energy photons are created by sudden acceleration changes. It explains antennas at a very basic level. The photons exist as a field at all times, they become higher energy photons through accelerations.

I like the notion that all photons are the same. It is really only our reference frame that changes their energy.

As for the question of whether they accelerate. Its sort of related to the permittivity and permeability of free space. These can be complex numbers or tensors, and as they compose the speed of light the speed of light varies. The speed of light in some [crystals] (http://www.sciencedaily.com/releases/2013/08/130813201436.htm) is different for different directions and all are different from what it is in free space.

However in no sense do they accelerate to light speed in the way a Mercedes might accelerate on the autobahn (*like I know).. They're at the speed of light in that medium, always. Their acceleration is more akin to their changing wavelengths. They gain energy by becoming associated with a reference frame that is different. So for example gamma rays hitting us from gamma ray bursts, in the old style classical viewpoint somewhere that gamma ray was a radio wave... emitted from something that is going very close to the speed of light relative to us. Its not an accurate description - but the truer descriptions are moderately dense tensor calculus and quantum theory.",null,9,cdm4wev,1rccq1,askscience,new,12
null,null,null,10,cdmg52h,1rccq1,askscience,new,12
Zeakk1,"So I am going to keep it simple - it does not accelerate. It always travels at c.
I think it's great you're interested in physics. I can recommend a good book written for lay people that describes photons and wave particle duality. Schroedinger's Kittens and the Search for Reality by John Gribbin. 
http://www.amazon.com/Schrodingers-Kittens-Search-Reality-Mysteries/dp/0316328197",null,2,cdmh8d8,1rccq1,askscience,new,4
mcM4rk,"I think that instantly reaches that speed, because light travels at c at any given moment, and it will not slow down. (Einstein theory of relativity) If that is correct, then the photon, which is the light, will travel at c immediatly.

(If this is incorrect please tell me, because then i might have to take another look at the theory of relativity)",null,9,cdm4kp8,1rccq1,askscience,new,11
Platypuskeeper,"Ice behaves like a normal solid, the [density increases with lower temperature.](http://en.wikipedia.org/wiki/File:Density_of_ice_and_water_%28en%29.svg). 

Nothing particularly interesting happens at the molecular level, unless you have a phase change. The average distance between molecules decreases because they vibrate less at lower temperatures.
",null,1,cdltrht,1rcdt3,askscience,new,2
Wrathchilde,"There are many forms of solid water [ice](http://www1.lsbu.ac.uk/water/ice.html), and which form exists is a function of temperature and pressure.  Some forms, as described in the link above, are more dense than liquid water.

However, since you did not include pressure changes in your question, let's assume a constant 1 atm.  This [phase diagram](http://en.wikipedia.org/wiki/File:Phase_diagram_of_water.svg) shows that below about 70k ""normal"" ice will change to ice-XI, a more structured crystal.  However, as the first link describes, the density of ice-XI is pretty much the same.
",null,0,cdlufvc,1rcdt3,askscience,new,1
NotAStructrlBiologst,"I hope this gives you a better picture of what is going on at the molecular level.

Even in the crystal/solid state molecules may not move but their atoms continue to vibrate. Bonds contract/expand and angles wobble ever so slightly. Continuing to cool something will also decrease these motions, increasing the order",null,0,cdm0rnu,1rcdt3,askscience,new,1
Ruiner,"First, kudos for doing your own experiment and trying to interpret results.

Now, my suggestion is that you should try striking with two coins instead of one. And afterwards, use a coin that's a bit heavier than the others.

After you're done with it, try reading the physics explanation on this [page](http://en.wikipedia.org/wiki/Newton's_cradle).",null,1,cdlsz1r,1rcgby,askscience,new,6
Ruiner,"If you were in a planet without atmosphere, then it would, but since we have air friction, the bullet would land with its terminal velocity.

correction.: if its terminal velocity is bigger than its speed when it is fired. Otherwise, it will land with a smaller velocity, given the energy it lost to friction.",null,3,cdlt0kg,1rch1e,askscience,new,19
meerkatsrgay,"so, I have never been full body immersed in -90. However, I have ineracted with huge standup freezers that cool biological samples to -86C (I have seen -89 once on the readout) Generally when interacting with this environment you wear protective gloves. Grabbing a cooled piece of metal can be dangerous if held for more than a few seconds as it will freeze the moisture on your hand. But, through my interaction with these freezers I would assume that standing in an environment like that naked you would be fine for even up to 15mins provided you only contacted anything solid through your feet (and had shoes) AND there was absolutely no wind. In an environment with no air movement your body is able to build up a very tiny layer of warm air close to the skin. This is the same reason why holding dry ice or sticking your hand into liquid nitrogen is fine, the evaporated gas that is instantly created between you and the material acts as an insulator. 

Either way, I wouldn't recommend going outside naked in -90C weather because you prolly don't look as good as you think and no one wants to see you naked.",null,2,cdm6hb4,1rchgw,askscience,new,11
shiningPate,"See the information on the South Pole station [300 Club](http://en.wikipedia.org/wiki/300_Club). To get into the club, you have to run naked from the 200 degree sauna, down the access tunnel, outside 20 feet to the south pole marker, touch it, and return back to the sauna. The dash has to be done when the temperature outside is -100 F. ",null,5,cdm1xw7,1rchgw,askscience,new,10
null,null,null,2,cdm2oo5,1rchgw,askscience,new,4
Polyknikes,"Short answer: Stimulation of the vagus nerve (CN-X) which induces bradycardia.

Long answer:

I had not heard the term apneic pause before but from googling it you are referring to a cessation of breathing for at least 10 seconds, commonly referenced in relation to sleep apnea.  I didn't know the term but I do understand cardiac physiology.

In 10 seconds your oxygen saturation levels do not drop by a measurable level.  Try holding your breath while using an oxygen saturation meter and see if you can even get it to go down by 1% - it's really difficult even if you can hold your breath for several minutes.  So I don't believe it is related to lack of oxygen unless we are talking about a much longer duration apneic pause.

During normal inspiration and expiration the heart rate increases and decreases, respectively.  This is thought to be induced by changes in vagal nerve tone (activity) but the mechanism by which breathing influences vagal tone is not understood.  The vagus nerve acts to reduce heart rate by hyperpolarizing the intrinsic pacemaking cells of the heart.  

Stimulating the vagus nerve would decrease heart rate and this can be accomplished by various means including hypoventilation, hypoxemia, respiratory acidosis, or vigorous inspiratory effort against a closed airway known as the Mueller's maneuver.  In the case of obstructive sleep apnea a person may be attempting to inspire but cannot due to a blockage of their airway (usually seen in obesity) which could stimulate the vagus nerve by the Mueller's maneuver mechanism.  But again, the exact mechanism by which the vagus nerve is stimulated by these pressure changes is not known, it has simply been observed indirectly.

I hope this at least partially answers your question!",null,0,cdlwfmx,1rci0v,askscience,new,3
Platypuskeeper,"There is no motion in a classical sense. Electrons have kinetic energy and that, but they do not follow specific trajectories. The probability of knowing where you might find the electron is all you've got. For specific energy states, these probabilities don't change with time. Electrons have no size of their own, their position-probability distribution in space is basically where the electron is.

The wave function/probability distribution, which for a single particle has [solutions like this](http://chemwiki.ucdavis.edu/@api/deki/files/8855/Single_electron_orbitals.jpg), is analogous to a standing wave in three dimensions. The angle-dependent part of the functions are the spherical harmonics.

Photons and electrons are particles, they both have energy but they have more than that as well. They carry both linear and angular momentum, for instance.
",null,3,cdlufy4,1rcj7e,askscience,new,5
The_Serious_Account,"&gt; So do electron move like those diagrams of standing waves? Or do they not wave like that, but whiz around like a particle? If I was reduced in size, is the electron a hard ball or is more of a packet of energy like a photon?


I think the most sensible thing to do is to give up the concept of particles. There's no such thing as a particle. Just drop the concept entirely. We keep it in language, but as a mental picture it's dangerous and misleading. 

The electron doesn't move around within the standing wave. The electron *is* the standing wave. The word electron should refer to the standing wave and nothing else. It's an extremely hard concept to accept, but repeat it to yourself. There is no particle. There is no particle. There's only the wave function. ",null,2,cdlucvm,1rcj7e,askscience,new,3
biffym,"It wasn't to make them believe it was real so much as to make it feel more real. They knew it was an experiment and that being arrested was part of it, but it adds to the feeling that they aren't there by choice. If they'd walked in of their own free will the jail would have felt less oppressive.",null,0,cdltsfb,1rcjmy,askscience,new,3
DougWC,"What is most interesting about the experiments and others like it is not the obvious.  It's the implications for ""real world"" situations of authority and subordination - that they are no more truly legitimate than are contrived ones.  They are all contrived and all should be seen through.",null,0,cdm1916,1rcjmy,askscience,new,1
cromonolith,"The only thing stopping this from being intuitive to you is what ""bigger"" means. 

You're used to judging the sizes of things by counting each of them and comparing the numbers. That is, if I gave you two bunches of apples, you might count and see that there are 10 apples in the first bunch and 14 in the second, and conclude that the second bunch is bigger. 

That's fine, but it's not a good way of measuring the size of infinite collections. So here's a better way of comparing the size of two collections: make a pairing between the collections, and the one that has stuff left over is bigger. 

As an example, let's say we had a huge auditorium and a huge crowd of people who want to see a show there. What's the best way to see if we have the same number of seats as people? You can count the seats and the people, but that's dumb. The smart thing to do is to tell everyone to sit down. As long as no one sits down stupidly (two people in the same chair), then you can easily check. If there are empty seats left over there are more seats than people. If there are people left standing there are more people than seats. 

What does this have to do with infinities? Well, this kind of pairing (like pairing people with seats) is a function. If no two people sit in the same seat, the function is called ""injective"" (or ""one-to-one""). If no seats are left unoccupied, the function is called ""surjective"" (or ""onto""). If a function is both injective and surjective (one person to a seat and all the seats are filled) the function is called ""bijective"". 

Bijective functions are what we use to compare the sizes of all sets, including infinite ones. Two sets are *defined* to be the same size if there exists a bijective function between them. So for example, the set of all natural numbers **N** = {1, 2, 3, 4, ....} is the same size as the set of even numbers 2**N** = {2, 4, 6, 8, ...} because, as you can check, multiplication by two is a bijective function from the former to the latter. That is, the function f where f(n) = 2n hits every even number and never sends two numbers to the same even number. 

On the other hand, you might want to compare the set of natural numbers as above to the set **R** of real numbers (the whole number line). This isn't obvious, but there's a [relatively straightforward proof](http://www.mathpages.com/home/kmath371.htm) that no matter how you try, it's impossible to make a bijective function from **N** to **R**, so they can't have the same size. Since **N** is actually a subset of **R**, we say the size of **N** is smaller than the size of **R**. ",null,1,cdlzcjv,1rcjr9,askscience,new,18
rlee89,"There are several notions of size when it comes to infinity.  The most common is *cardinality* which in lay terms asks whether one infinity can be fit (or more formally, mapped) into another.

For example, if you have a list of the positive integers, there is a way to place a rational number next to each positive integer in the list in such a way that each rational number is next to some positive integer, and vice versa.  We would formally call this a bijection between the positive integers and the rational numbers, and it would demonstrate that the two are the same cardinality.

[Cantor's diagonal argument](http://en.wikipedia.org/wiki/Cantor%27s_diagonal_argument) shows that any attempt to match up the positive integers with the real numbers will necessarily miss at least one real number.  Thus the of real numbers won't fit into the poisitive integers, and thus have a larger cardinality than the positive integers and are a bigger infinity.",null,1,cdlznix,1rcjr9,askscience,new,2
protocol_7,"Here's another way of thinking about it: Any natural number can be represented by a finite amount of information. (For example, you can write it down in base 2 as a finite string of 0's and 1's.)

However, a real number has an *infinite* string of digits past the decimal point, so there's no guarantee that any particular real number can be expressed using a finite amount of information. (And, in fact, most real numbers *cannot* be uniquely identified by any finite amount of information.)

So, intuitively, it's the difference between finitely many versus infinitely many ""degrees of freedom"". Since specifying a single real number can involve an infinite number of choices of digit, there are vastly more real numbers than natural numbers.

This is just a vague intuition, though. You can make it precise using [information theory](https://en.wikipedia.org/wiki/Information_theory), but that's rather technical. Instead, here's how to see very quickly that some infinite sets are larger than others — in particular, that given any set (finite or infinite), there's another set that's larger.

By definition, two sets have the same [cardinality](https://en.wikipedia.org/wiki/Cardinality) (basically, size) if they can be put into [one-to-one correspondence](https://en.wikipedia.org/wiki/Bijection) with each other — that is, if you can pair up elements of the sets so that each element of one set is paired up with a unique element of the other set.

**Theorem** ([Cantor](https://en.wikipedia.org/wiki/Cantor%27s_theorem))**.** Let S be any set, and let P(S) be the [set of all subsets of S](https://en.wikipedia.org/wiki/Power_set). Then S and P(S) do not have the same cardinality.

*Proof.* Let f be any function from S to P(S), that is, for each element x in S, we assign a unique element f(x) in P(S). Since each f(x) is a subset of S, we can ask whether x is an element of f(x). In particular, let T be the set of all elements x of S such that x is *not* an element of f(x).

Now we can ask, is there some element y in S such that f(y) = T? Suppose there was: then we can ask whether y is an element of T. If it is, then y is an element of f(y), so by the definition of T, y is not an element of f(y) — a contradiction. If it isn't, then y is not an element of f(y), so by the definition of T, y *is* an element of f(y) — again a contradiction. Therefore, it's impossible for such an element y to exist. So no element of S is paired up with T.

Thus, *any* attempt to pair up elements of S and elements of P(S) fails, which means that S and P(S) don't have the same cardinality. ∎

But we can embed S inside P(S) by sending each element x in S to the [singleton set](https://en.wikipedia.org/wiki/Singleton_%28mathematics%29) {x}, a subset of P(S). So, in fact, S is strictly smaller than P(S).

Notice that there's no mention of whether S is finite or infinite. (If S is finite with n elements, then P(S) has 2^n elements.) So the proof is valid regardless, meaning that it's true for infinite sets, too.

For instance, if we denote the set of natural numbers by **N**, then P(**N**) is strictly bigger than **N**. (Actually, it turns out to be the same size as the set of real numbers.) And the set P(P(**N**)) is bigger than that, and so on.",null,1,cdlzsre,1rcjr9,askscience,new,2
null,null,null,3,cdlz5a1,1rcjr9,askscience,new,2
shiningPate,"First, this effect only occurs in a vacuum. In air, the gas molecules rapidly absorb the emitted electrons. Basically the reason xrays are emitted is because of the static electricity generated by separating the tape film from the surface below it. As the tape is pulled away, an electrical field is formed at the point of separation. As the roll rotates away, the film separates from the layer below it, reducing the strength of the field between the point that just separated, and the point at which it was previously touching. Meanwhile a new point has just separated from the roll, and higher strength field is formed at that point. The result is an electrical field at a gradient intensity in the wedge between the roll and film. At the point of separation, the field strength is so high electrons are ripped away from the film material. The film material was also electrically charged as part of the manufacturing process to get it to roll up smoothly. Electrons emitted into the gradient electrical field will begin accelerating along the gradient. Accelerating electrons emit xrays",null,0,cdm2gu7,1rcmj7,askscience,new,3
dampew,"http://www.nature.com/news/2008/012345/full/news.2008.1185.html

""The leading explanation posits that when a crystal is crushed or split, the process separates opposite charges. When these charges are neutralized, they release a burst of energy in the form of light.""",null,0,cdm0glp,1rcmj7,askscience,new,2
KerSan,"Energy is a consequence of a symmetry in the laws of physics. This is a deep point, so let me try to explain.

It is first worth explaining probably the most important concept in physics, momentum. Early on in your education, you are told that it is simply mass times velocity. This turns out not to be true generally, because momentum is really what is called the 'generator' of translations.

Consider a point particle that has a definite position in space. That position is actually an arbitrary label, since it refers to your chosen co-ordinate system rather than some actual property of the universe. You could just as easily switch co-ordinate systems by, say, moving the origin of your co-ordinates somewhere else. This action of shifting co-ordinates is called 'translation'.

Now let us suppose that the particle is moving (relative to you, the observer). No matter which co-ordinates you chose, you will notice that the position of the particle is changing over time. If you want to predict where you will see the particle next, you need to translate the last known co-ordinates of the particle by some amount. That translation is determined by the momentum of the particle. Keep in mind that the momentum is not changed when you translate your co-ordinate system. The momentum is, in a technical sense, dual to position.

What does this have to do with energy? Well, energy happens to be the generator of translations in time. This is because *energy is defined to be that quantity which remains invariant under translations in time*, much as momentum is that quantity which is invariant under translations in position. Your co-ordinates for time are just as arbitrary as your co-ordinates for position, because your choice of starting time is your choice and not some property of the universe.

Energy therefore tells you something about how quickly the properties of an object change over time. The more energy, the faster changes can happen. I could write essays explaining this further, but I don't want to lose you.

Work is the addition or subtraction of energy from a system. That energy can be added in a variety of ways, but it is common in early physics education to think in terms of force rather than energy. Energy is actually more fundamental than force, so I'll explain the force times displacement rule by first explaining force in terms of energy.

Force is the derivative of potential energy with respect to position. If you think of the potential energy of a particle as a function of the position of the particle (for instance, gravitational potential varies linearly in the height of the object), then force experience by the particle at a given position is the slope of the potential function at that position. In other words, force is the negative change in potential energy (i.e. the work) divided by displacement (in an infinitesimal sense). Multiply both sides by displacement and you have the relation work = force * displacement.

Hope that helps.",null,2,cdlzluj,1rcojz,askscience,new,7
miczajkj,"What is energy?

This is a very interesting question - but we we don't have an answer to this. We formulate physics in equations and stuff and there is this certain quantity that shows up in some of them and seems to be conserved under certain circumstances. 

The first kinds of energy you encounter, when you start to think about physics are the kinetic energy and the potential energy.
Assume a body at the position x that is exposed to a force F(x). Following Newton, you get the differential equation

m*x°° = F(x) 

The second derivative of the bodies displacement with respect to time times its mass is equal to the force on the particle.  
If it is possible to write the force as the derivative of a potential, namely F(x) = -V'(x) you see, that the following quantity is conserved for every solution of the mentioned equation: 

E = 1/2 mx°² + V(x) 

If you take it's derivative, you see: 
E° = m*x°*x°° + V'(x)*x° = x°*(mx°°-F(x))

So if the differential equation is fulfilled, then E' = 0 follows directly. 

You can see: because of the form of the differential equation that describes the movement, there is a certain quantity, that won't change - we call it a conserved quantity. 

If you dive deeper into the mathematical foundament of physics, you find that symmetries are responsible for the most conserved quantities. The energy for example, is only conserved, if the physical laws describing the process are time independent. 

All in all: energy is just a number. A phrase like 'pure energy' is nonsense. Energy is just a quantity, that appears in our equations. 

Work is defined as the change in energy you need to apply to a system, to move it from one state to another. In most cases this movement is a spatial movement from one place to another - then work is just the difference in potential energies, 

W = V_1 - V_2

or because V'(x) = F(x) 

W = integral F(x) dx

For forces that don't depend on position, you get 

W = F*x


I guess that was a whole lot of different concepts - maybe you've never heard of differentation, integration or newton's 2nd law: but I can't come up with an easier way to describe energy, that is not too simplified or wrong. ",null,0,cdlztxi,1rcojz,askscience,new,2
dampew,"Work has units of energy -- they're basically the same thing.  For instance, if you want to know how much work you've done, the answer would be in units of energy (joules, calories, whatever).

Why is it force times distance?  Well, you do more work if you have to push something harder through a larger distance.  I'm not really sure how to answer that question -- it's just a name for something, really...",null,0,cdm0kyk,1rcojz,askscience,new,1
FeckSakeLads,"work is the amount of energy you must add using a force of a certain strength to move an object with mass a certain distance in a certain direction (the displacement). the equation is:



work = force (joules) x displacement (metres) x cos(theta) (theta being the angle between the direction of the force relative to the direction of displacement).



See [this](http://www.physicsclassroom.com/calcpad/energy/) for more.",null,3,cdlzbtd,1rcojz,askscience,new,1
therationalpi,"Sound waves definitely *are* affected by the wind. Since sound waves travel through a medium, and wind is a bulk flow of the medium, the sound speed in a windy environment (which is normally the same in all directions) suddenly becomes direction dependent. Specifically, the speed of the wave becomes the speed of the wind in that direction plus the speed of sound at rest. Moreover, since wind tends to move faster the higher you move up from the ground, there is usually an effective sound speed gradient as well. In the presence of a sound speed gradient, sound waves tend to refract towards regions of lower sound speed. As a result, sounds sent out against the wind will tend to refract upwards, and sounds made with the wind will tend to refract downwards. Sounds made cross-wind will tend to refract downwind and up. And since your listeners tend to be near the ground (relatively speaking) the net effect is that sounds carry further with the wind than against it.

As for electromagnetic waves (light/radio), I don't believe there is any notable effect, but I would wait for verification from someone with more experience in the field.",null,0,cdlz4kd,1rcooj,askscience,new,7
ignorant_,"While we're waiting on someone with better credentials, I'll throw in that for every 4 inches in height over 5ft, a person has a 16% increased risk of cancer. So converse to your statement, it is my understanding that more cells, thus more cell divisions, means greater risk of cancer development. ",null,1,cdlztvc,1rcpmh,askscience,new,5
Aniridia,"Obesity does increase the risk of several cancers. The inverse, does being ""skinny"" lower the risk of cancers, is less clear, and I'm not sure if it has been directly studied. There's a [Lancet article](http://www.thelancet.com/journals/lancet/article/PIIS0140-6736\(11\)60814-3/fulltext#article_upsell) that deals with many of the health risks of obesity, including cancers. (The article is free, but you must log in.) Here is a [PDF PowerPoint type presentation](http://www.mhsimulations.co.uk/Documents/WangC.pdf) of the article from the author. ",null,0,cdmr14g,1rcpmh,askscience,new,3
Azurity,"If you're up for a bit of fun and history, here's an ancient (1961) paper that originally investigated various mechanisms of polypeptide assembly: [ASSEMBLY OF THE PEPTIDE CHAINS OF HEMOGLOBIN](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC221568/pdf/pnas00219-0005.pdf)

It's actually a fun article to read if you've got an hour or three and you feel like working out a molecular puzzle using 50-year-old methods and logic. Scientists weren't sure if proteins were synthesized from one end to the other, or started at both ends, or if there was actually a giant cellular ""stamping"" machine that knit every amino acid of a protein together at once! Essentially, they used a series of radiolabeling and quenching experiments and froze moments in time as proteins were being made to mathematically derive a mechanism of N-terminus to C-terminus translation. Cool stuff!",null,0,cdm5qng,1rcqnv,askscience,new,6
PENIS_VAGINA,"Well read the section about Crick's contribution to molecular biology here: 


http://en.wikipedia.org/wiki/Francis_Crick#1951.E2.80.931953:_DNA_structure


I'm not sure that there was ONE definitive experiment that determined the mechanism. Perhaps there was, but I can't seem to find it.


If you are wondering how you can prove it now, a simple example would be using GFP to follow a DNA sequence to an mRNA transcript and then to a fluorescing protein. There are other experiments that rely so heavily on mRNA as the transcript of DNA that it basically 100% accepted as the mechanism. ",null,3,cdlytmo,1rcqnv,askscience,new,4
quantum_lotus,"It seems like you want the actual experiments that led to our understanding of the triplet nature of the genetic code.  I'll offer you two resources that explain the experimental procedures (and the logic behind them) that led to our current understanding.  Both are at a basic undergraduate level, so I doubt you'll have trouble following.

The first is from the Nobel Prize website called[ ""Crack the Code""](http://www.nobelprize.org/educational/medicine/gene-code/history.html).  I'd read the historic background, but the explaination of the experiment starts with the ""A Clever Experiment"" section a little further down the page.

The second is as [PDF](http://basic.shsmu.edu.cn/jpkc/cellbiota/resource/exper/11.pdf) I found with a basic google search (for ""cracking the tRNA code"") that is unattributed, but hosted on a site from the Shanghai Jaio Tong University.  It appears to be from an undergraduate level textbook and gives a more in-depth look at the same history and experiments.",null,0,cdn4cyp,1rcqnv,askscience,new,1
Das_Mime,"The expansion of the universe and the speed of light have different units, therefore you can't compare them.

The Hubble Constant is the fraction by which a given parcel of space will grow in a given amount of time. It has units of inverse time, s^(-1). The speed of light, of course, has units of distance/time, m s^(-1).

The Hubble Constant is usually given in units of kilometers per second per megaparsec, but the two distance units just cancel out and you get the result that the universe expands by about 0.00000000000000002% per second.

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

There will eventually come a time when there are no other visible galaxies in our observable universe (except for nearby ones that are gravitationally bound to us).",null,3,cdm01m4,1rcr2g,askscience,new,10
IAmMe1,"We in fact know that far-away parts of the universe are receding from us faster than the speed of light. However, this is not a problem. It's better to think about the expansion of the universe as an change in space itself rather than the motion of the things in that space; think of it as extra distance appearing between far-off objects. In this way, nothing is moving faster than light in the sense of any actual motion; instead, the distance between us and such an object increases faster than light can traverse that distance (i.e. more than 1 light-year of distance is added per year).

&gt;Also, if that happened, would we at one point not be able to see any other galaxies because they're receding from us faster than their light can reach us (assuming we'll be here, of course)?

Yes indeed. It will be a dark and dismal universe that day far, far into the future!",null,1,cdlyqtu,1rcr2g,askscience,new,7
Luminarie,"Based on what we know about physics right now, we have no reason to believe it won't happen. Lawrence Krauss puts it quite succintly: ""Nothing can move faster than light in empty space, but space itself can to whatever the hell it wants"".

And yes, that's what would happen. At some point, the space between galaxies will be expanding faster than light, and at that point they will disappear from our region of the universe, as light would need to be faster than the expansion to be able to get to us. Therefore we will be causally disconnected from the rest of the universe.

Beyond the point where it accelerates faster than light, extrapolations based on an unchanging acceleration end in a [Big Rip](http://en.wikipedia.org/wiki/Big_Rip). Basically, the increasing speed of expansion overcomes all physical forces, and the universe would seem to end in a singularity (the scalar factor that defines expansion becomes infinite) at the moment when this happens.",null,0,cdlz409,1rcr2g,askscience,new,2
ofeykk,"I am going to attempt to translate, as best as I can, your problem into a mathematical question. I suspect you are making a bunch of assumptions here which I will try to make more formal.

First, with the view of simplifying as much as possible yet retaining the crux of the original problem, you can make the following assumptions (removing each would yield a slightly different problem to solve):

1. Look for curves not in 3-space but in 2-space.
2. Simplify curvature of earth to be flat — seek planar curves.
3. Simplify sun to be on this flat plane as a point.
4. Simplify yourself to be a point on the sought curve.
5. 180 degrees view is equivalent to dividing the plane by the tangent to the curve at your location (point).
6. Choose an orientation for the curve. This helps fix what it means for a point to be in your field of view or equivalently, to be in the ""correct"" half space of the tangent to the point (you) on the curve.
7. Parametrize looping around the curve to be traveling along the unit interval [0, 1] with the end points {0} and {1} identified — essentially a fancy way of saying that the end points of the interval are glued.
8. Assume that the sun is fixed relative to the time taken to loop around the curve once.
9. Finally is the curve sought smooth or not ? (A circle is a smooth curve whereas a triangle isn't one.) I believe it's easier (at least for me) to imagine smooth curves. Also, will exclude wild curves like space filling curves simply because I am not comfortable dealing with those !!

Now, the question is to find a curve that maximizes the view of the sun when you loop around once.

It appears to me that solution would depend on whether you would wish to make a further assumption about whether your plane is unbounded or not. 

If the plane is unbounded, the answer is simple — any straight line not through the point representing the sun would do.

If the domain on which the curve is sought is compact (or technically if its closure is compact) — think finite if you wish, like a square plot or a circular plot of land — then it depends on where the sun is located relative to this domain. Some examples that come off the top of my mind are as follows:

1. Circular region with the sun in the center: Take any diameter and take a tube around the diameter. This tubular region would have a boundary curve composed of two straight lines (chords of the circle) with a small part of the circular's plot's boundary (two of these actually). You can make this tubular region as small as you please and would provide you with a curve for which the sun is visible for as long as you please. (In other words, give me a number for which you wish the sun to be visible, say 99.99%, and I can give you a *width* of this tubular region for which it would be realized. Make it 99.9999999% and I'll give you a different number for the width and so on.)

2. A circle with the sun not in the center: Use the same idea as in example 1. Drop a radius from the center of the circular plot to the boundary that passes through the sun. Measure the distance, say r, from the sun to the boundary of the circle. Draw a smaller circle within the larger circular plot with radius r and centered at the sun. Repeat example 1.

3. A square with the sun at the intersection of the diagonals: Repeat example 1 with the a circle of length equal to the distance from the sun to any one of the vertices of the square.

4. A square with the sun not at the intersection of the diagonals. Easier to say that one should fall back to example 3 but rather simply draw a circle centered at the sun with a radius equal to the shortest distance from the sun to the boundary of the square.

Can go on but would stop here. I have to say that I did this as a fun Sunday morning exercise, and tried to reason mathematically which may or may not have been what you were looking for ! (I enjoyed it though !) :-)",null,0,cdlztop,1rctma,askscience,new,4
Bondator,"Human walking speed is roughly 6 km/h so if you circle around north or south pole at 23km radius, you'll do a full circle in 24 hours, keeping the sun in front of you 100% of the loop.

As for your triangle, you didn't go deep enough. Don't do an equilateral triangle, do an isosceles triangle. Mathematically expressing, if we mark the short side with x, and the equal sides with y, and choose the orientation in such a way that x is the part where you don't face the sun, then uptime of sun in face is lim(x-&gt;0) 2y/(2y+x) = 1.",null,0,cdm9rcs,1rctma,askscience,new,2
musubk,"I once drove an 18 hour loop in Alaska in the summer with the Sun shining on the left side of my face for all but about an hour of it.  I suppose if you go above the Arctic Circle at the right time of year and just walk toward the Sun at a constant speed, you'll end up where you started 24 hours later making a complete loop.",null,0,cdnb6l9,1rctma,askscience,new,1
shiningPate,"The original computer architectures used different circuitry for retrieving bytes  assembling into word sizes matching the register size in the CPU. When you were looking at the memory sequentially, independent from the CPU, you needed to know which way the CPU assembled the data into register values to understand why your calculations where coming out wrong",null,0,cdm27j3,1rcvso,askscience,new,2
bellcrank,Pretty sure you could get away with a plane parallel approximation in this scenario.,null,0,cdmksb0,1rcvv4,askscience,new,2
adamhstevens,"If you're talking about long wave radiation from the Earth, I think this is a fairly standard textbook problem. I'll try and look it up when I get home... if I remember!",null,1,cdmildz,1rcvv4,askscience,new,2
Farnswirth,"It's actually very simple.  Pure silver is softer and more malleable than silver alloys.  Just like how pure gold is much more malleable than gold alloys.

http://en.wikipedia.org/wiki/Mohs_scale_of_mineral_hardness#Hardness_.28Vickers.29",null,0,cdm0s7d,1rcwaq,askscience,new,5
PENIS_VAGINA,"90% of the blood flow leaving the glomerulus through the efferent arterioles perfuses the cortex (10% to the medulla) under normal conditions. The main purpose of this is to keep the medulla interstitial fluid hypertonic so that concentrated urine can be produced. I suppose that vasculature changes (i.e. arteriolar constriction) could reroute some of that 90% of blood flow into the medulla to aid in decreasing the hypertonicity of the medulla. 


I'm not 100% sure though. I do know that in normal conditions it is hypertonic to aid in urine concentration (your original question). ",null,0,cdlxyv5,1rcwz4,askscience,new,1
xtxylophone,"If you want to use a computer to put an image onto a video, you pretty much have to do it frame by frame. Modern software can speed this up a lot but sometimes you just want to change a background or something.

So you pick a colour that you know will not be in your frame, make sure its evenly lit up. Then you have some software that will replace the green in the video with whatever you want. For this, a light green is usually chosen.",null,7,cdm155s,1rcx3o,askscience,new,24
DorkmanScott,"VFX professional here. Greenscreen compositing is part of an overall technique called chromakey. You effectively tell the computer a color it should isolate, and it selects that very narrow wavelength of color from the image and makes it transparent. Depending on the algorithm (keyer) you're using you then have various ways to expand the range of hue/saturation/brightness the keyer will consider. 

Any color can be used, but green or blue are typically used because most of the time you're dealing with human subjects, and human skin tones are mostly red, so subtracting the screen won't tend to affect the character. Bluescreen used to be the more popular color, as it responded better to the optical extraction techniques of the pre-digital age. Green has become more prevalent since the dawn of digital, as digital sensors respond more strongly to green light, but the keying algorithms are so advanced at this point that it's really down to personal preference and/or a particular restriction -- e.g. if you have a character like Superman who wears blue, or Peter Pan who wears green, that will dictate the necessity for the opposite screen color. It's also typically easier to extract light-colored hair from bluescreen and dark colored hair from greenscreen, since there's more contrast. 

The way it USED to work in the optical days is MUCH more interesting, involving progressively filtering wavelengths of light to produce high-contrast isolations (mattes) of the screen, and a negative-image isolation of everything else, which were then used effectively as stencils on foreground and background so they could be cleanly double-exposed together without overlapping. Because this process had to go through several generations, the edges of both stencils would tend to get soft, which is why in pre-digital effects films you will see the telltale black outlines around things which have been extracted and layered over the background. ",null,2,cdm8ztu,1rcx3o,askscience,new,14
sexgott,"Why read these comments when you can [watch Stu Maschwitz replicate the way it used to be done with film](http://prolost.com/blog/2011/10/13/real-men-comp-with-film.html).

It's very fascinating, and you get to see both how it's done digitally and how they did it with real film and color filters.",null,0,cdmcfwj,1rcx3o,askscience,new,3
suprasamus,"There are two types of Green Screen. I'll explain the 'simplest' of the two for you because that's the one I know about. 

Green Screen is basically a Green Wall. That's all it is in essence. It's a plan, flat background most popularly in the colour green or blue. 

Now due to this background being such a solid colour it stands out when a subject stands in front of them (unless they are wearing green which is a big no-no as the process will not work properly). The background is then selected and an image is projected onto it. As only the background is selected and not the subject the subject appears in front of the projected image. Nothing complicated. 

There is a different type of green screen that works in the same way but instead of a little green wall a green light is emitted onto a crystal line background but hopefully someone else will explain that to you in simpler terms.   ",null,1,cdm4414,1rcx3o,askscience,new,1
TITS_ME_UR_PM_PLS,"The Moon is not a homogeneous rock any more than the Earth is. Plus, we only have a handful of sites from which we have been able to get samples. However, [the crust is mostly anorthosite and gabbro.](https://www.uwgb.edu/dutchs/planets/moon.htm) The [""maria""](http://en.wikipedia.org/wiki/List_of_maria_on_the_Moon) (seas) are mostly basalt flows.

[Anorthosite](http://en.wikipedia.org/wiki/Anorthosite)

[Gabbro](http://en.wikipedia.org/wiki/Gabbro)

[Basalt](http://en.wikipedia.org/wiki/Basalt)

Some interesting trivia:

[Armalcolite](http://en.wikipedia.org/wiki/Armalcolite) was discovered on the Moon before (tiny) quantities were found on the Earth. The name comes from the three members of Apollo 11, Armstrong, Aldrin, and Collins. Two other minerals, [tranquilityite](http://en.wikipedia.org/wiki/Tranquillityite) and [peroxyferroite](http://en.wikipedia.org/wiki/Pyroxferroite) were also discovered on the Moon before found here on Earth.

All of the lunar samples have been painstakingly documented; here's one random page- lunar sample [65015,](http://curator.jsc.nasa.gov/lunar/lsc/65015.pdf) just to name one rock. (I can't find the really interesting half-spherical sample that Apollo... 15, I think it was, discovered on the ground by the drill site.)

The Apollo astronauts trained extensively on Earth; one of the geologists that took part is [Leon Silver,](http://en.wikipedia.org/wiki/Leon_Silver) granduncle of Nate Silver, the [statistician and journalist.](http://www.forbes.com/sites/quora/2012/11/07/how-accurate-were-nate-silvers-predictions-for-the-2012-presidential-election/)

[Harrison Schmidtt](http://en.wikipedia.org/wiki/Harrison_Schmitt) was the sole professional geologist that went to the Moon, and the last of the astronauts to walk there.

Interestingly, the Soviets had some landers that retrieved lunar samples. [Luna 16](http://en.wikipedia.org/wiki/Luna_16) brought back 101 grams; [Luna 20](http://en.wikipedia.org/wiki/Luna_20) returned 55 grams; [Luna 24](http://en.wikipedia.org/wiki/Luna_24) brought back 124 grams. 8 other Soviet missions to return samples from the Moon failed.

Apollo missions brought back 22 kilos (Apollo 11), 34 kilos (Apollo 12), 43 kilos (Apollo 14), 77 kilos (Apollo 15), 95 kilos (Apollo 16), and 111 kilos (Apollo 17).",null,13,cdm3qw8,1rcxbu,askscience,new,49
oloshan,"Interestingly, the main difference in rock type on the larger scale is that the moon is almost entirely formed of igneous rocks. This is because, in the absence of plate tectonics, there are no large-scale geological processes on the moon that would contribute to the formation of either sedimentary or metamorphic rocks.

The only exception, and it's a technical one, is the lunar regolith (or lunar ""soil""). Although it is basically made from the pulverized remains of the typical igneous lunar rocks, its deposition is secondary and one could argue that this aspect makes it a kind of pseudo-sedimentary rock. An analogy might be something like a tuffaceous sandstone or an aeolian deposit on Earth (if the grains were wind-blown pieces of igneous rock).",null,1,cdmfwqv,1rcxbu,askscience,new,2
miczajkj,"If you talk about two different charged particles, that only interact as a closed system (so no external magnetic or electric fields) the problem is equivalent to the [Hyrdogen atom](http://en.wikipedia.org/wiki/Hydrogen_atom). 

Therefore there are quantized stable orbits and a radiation of photons is not allowed without a time-dependent perturbation.",null,0,cdlym8i,1rcxw0,askscience,new,1
Platypuskeeper,"Classically, if you're accelerating a charged particle (and a particle moving in a circular pattern is being accelerated constantly), then you will give off radiation. Obviously if you had one particle orbiting another, you would need some kind of outside energy to sustain this, or the thing would give off all its energy and spiral into the other particle.

[Synchrotron light](http://www.iop.org/publications/iop/2011/page_47511.html), which is up in the X-ray range, is produced by moving electrons around at relativistic velocities.


",null,1,cdlyxjg,1rcxw0,askscience,new,2
KerSan,"This is *precisely* the problem that made physicists develop quantum mechanics. The answer to your question is 'no', because otherwise the particles would lose energy and crash into each other. Unless the particles are going to merge or something, this is a violation of the Heisenberg Uncertainty Principle because then you would know too much about both the position and momentum of each particle.",null,0,cdlz5t5,1rcxw0,askscience,new,1
skleats,"Cells receive and respond to survival/apoptotic signals independently, so the senescence or death of one cell does not directly impact those around it. This is key since [controlled apoptosis is a normal part of embryonic development](http://people.ucalgary.ca/~browder/apoptosis.html) in multicellular organisms. However, a multicellular organism relies on coordination of activities between its many cells, so having a large proportion of senescent or apoptotic cells would be likely to impact the ability of those cells to contribute to survival of the organism. [This article](http://www.ncbi.nlm.nih.gov/m/pubmed/15265523/) describes an *in vitro* model which mimicked chronological aging and showed reduced coordination between cells as they aged.",null,0,cdm6ky2,1rd0kh,askscience,new,2
redmeansTGA,"First off, let’s look at this from an ecosystems perspective. Coral reefs and coastal forests close to the impact site were probably completely annihilated. Other ecosystems; wetlands, tropical forests, woodlands, and so on would have suffered the nuclear winter, microwave summer, firestorms, tsunamis and shockwaves to varying degrees. Aside from Chemolithotrophic bacteria and archaea living in deep within the crust, nowhere on Earth would have escaped unaffected.


The deep sea, far from being safe, was significantly affected by the K/T impact. A decrease in species richness and abundance is observed. The specific mechanism of the extinction event in the deep sea, along with the rest of the oceans, remains unknown- although two hypothesis have been proposed; either 1) marine primary productivity was hit hard, and the oceans 'died' as the bottom of the food chain was taken out, or 2) rapid acidification wiped out calciferous plankton, which broke down the oceans [biological pump](http://en.wikipedia.org/wiki/Biological_pump).  Either way, the deep oceans (including communities living in trenches) starved. 


I don’t know much about cave ecosystems from the Cretaceous, however we do know that modern caves (and K caves wouldn’t be different) receive what’s called resource subsidies- that is resources from the outside world are moved into the cave, via insects, streams or other means, and cave animals then depend on those resources. The destruction of outside ecosystems would surely adversely affect this flow of resources, and cave ecosystems probably suffered mass extinctions too. 


Remote islands probably wouldn’t have been a great place to be. To begin with, the K/T extinction caused massive tsunamis that would have devastated low lying atolls. Secondly, island ecosystems are relatively small, and generally don’t have a whole lot of redundancy so climatic change can hit them hard. Thirdly, islands don’t often stay around a long time. Many oceanic islands are doomed to sink back under the waves.


So to answer one part of your question, there were probably no pockets that survived unaffected. However, let’s look at things from a different perspective. 


The late cretaceous contained a lot of flora and fauna that we are familiar with today, as many dominate species emerged during the mid-Cretaceous. There were some notably absences, for example open savannahs and steppe dominated by the grass family (poaceae). The superabundant passerine (‘perching’) birds didn’t evolve to the early cenozoic either. Temperate deciduous forests also didn’t exist until the Earth cooled during the mid-cenozoic. But for the most part, Cretaceous landscapes would have been full of species we would recognize- social insects like bees and ants, butterflies, birds, frogs, lizards, snakes and crocodiles. The rivers and lakes would have had many modern types of fishes. The forests would been full of palm trees, cycads, tree ferns, and tropical hardwoods, with diversity of flowers and fruits. There were no large mammals, and dinosaurs (et al) still roamed around, but large animals are only a tiny proportion of species anyway. 


Looking at it from that perspective, it’s clear that large chunks of extant ecosystems bear similarities to Cretaceous ecosystems. 65 million years of evolutionary innovation has introduced new elements, of course, but successful lineages and ecosystem interactions not only survived the aftermath of K/T, but they prospered. We live in a world still dominated by Cretaceous survivors. 
",null,1,cdm4phv,1rd168,askscience,new,7
xtxylophone,"Well all life today has survived to this day since the dawn of life. heh :)

But no new life 'formed' about that time, only new species arise. There are some species alive today that have not changed much since that time like sharks or crocodiles to think of a few.

But if you are after dinosaurs yes and no. Birds are descended from dinosaurs so they are literally dinosaurs. All non avian dinosaurs are extinct though.",null,5,cdm18dh,1rd168,askscience,new,8
meerkatsrgay,"The answer is almost certainly NO for any multicellular or non hibernative organism.....and YES for individual organisms

There are 2 reasons why we get to say YES. 
First is bacteria! 
Very ancient bacteria have been found inhabiting ancient salt beds deposited by historic seas. 
http://news.google.com/newspapers?nid=1928&amp;dat=19880816&amp;id=QO4pAAAAIBAJ&amp;sjid=GWUFAAAAIBAJ&amp;pg=3231,2859428

Ancient frozen bacteria may also be found in frozen areas of the globe.
These examples may be unsatisfying because they lasted this long due to a ""hibernative"" state with little to no metabolism. However, you would be hard pressed to find a scientist to tell you that a non hibernative organism (especially a multicellular one) has been surviving that long.

Second, is because viruses!
so...these are different. Its still a debate as to whether you can even call a virus an organism or even a ""life form"". However, it is actually quite likely that there are still individual viruses  still around form 65m years ago. They could be in your back yard right now, or even IN YOUR BODY! yes! virus can integrate themselves into an organisms genome and wait multiple generations before exiting. It is very unlikely that they have escaped mutation all this time, but still possible.
",null,0,cdm63kl,1rd168,askscience,new,2
TangentialThreat,"Do cockroaches count?

Also, sharks and bees. Many forms of life have not changed much over very long spans of time.

If you are hoping for undiscovered dinosaurs, then no. Large animals tend to be very noticeable and easy to find. Even things like giant squid got caught in nets or washed up dead once in a while. Thanks to satellites and helicopters, we are also running out of large unexplored islands and plateaus to explore. There are still deep caves but organisms in cave ecosystems tend to be small and low-energy.

There have been a few species that were known from fossils before they were found alive, such as the coelacanth.",null,1,cdlzs2l,1rd168,askscience,new,2
TITS_ME_UR_PM_PLS,"[Triops.](http://en.wikipedia.org/wiki/Triops_cancriformis) You can even buy eggs on eBay and hatch them yourself.

There are other examples of such [living fossils,](http://en.wikipedia.org/wiki/Living_fossil) but few come in packet form in the mail like triops.",null,2,cdm48t8,1rd168,askscience,new,3
bjornostman,"Ants and other insects were around back then. And of course birds were too, in fact going way further back. You can use [timetree.org](http://timetree.org/index.php?found_taxon_a=91788%7Ctoucan&amp;found_taxon_b=9160%7Csparrow) to see that sparrows and toucans share a common ancestor about 93 million years ago, for example.",null,2,cdm0xp9,1rd168,askscience,new,2
deadlywoodlouse,"Just so you know, those aren't actually spiders, they're [Opiliones](https://en.wikipedia.org/wiki/Opiliones), also known as Daddy Longlegs or Harvestmen. [This](https://www.youtube.com/watch?v=0JK2dR8ei5E) video clears up both what they are, and any confusion the name causes (since there are other animals also known as Daddy Longlegs).

Other than that, I can't help you sorry, I'm don't know much about biology.",null,8,cdm5t2m,1rd2z5,askscience,new,34
skinnyhobo,"Many species of harvestmen easily tolerate members of their own species, with aggregations of many individuals often found at protected sites near water. These aggregations may number 200 animals in the Laniatores, but more than 70,000 in certain Eupnoi. This behavior is likely a strategy against climatic odds, but also against predators, combining the effect of scent secretions, and reducing the probability of any particular individual of being eaten. - Wikipedia

[Here's a video of a large mass of Opiliones in a tree.](http://www.youtube.com/watch?v=OWASwBWyUXI)

",null,13,cdm7450,1rd2z5,askscience,new,33
cladocerans,"No one knows exactly why Daddy Longlegs cluster together. It's a fall time behavior, though. Here are two hypotheses from Harvestmen: The Biology of Opiliones.

It could be for moisture--they need a moist place to hibernate to keep from drying out, and the congregating is just a side effect of having few suitable nooks &amp; crannies.

Alternatively, it could be for defense. Daddy Longlegs/Harvestmen all produce defensive chemicals against predation. Gathering together may increase the impact of their defense.",null,14,cdm746b,1rd2z5,askscience,new,28
MarineLife42,"Biologist here, yes those are Opiliones (well done /u/deadlywoodlouse). May I ask in what country/state this pic was taken?  
If it weren't for the high temperature, I'd assume they prepare for winter rest. Otherwise, I am clueless. ",null,10,cdm7h7l,1rd2z5,askscience,new,18
rossk10,"In my realm (structural engineering), wind tunnels are used to simulate and predict expected wind loading to structures during specified gusts.  Smaller, to-scale models of buildings are built, hooked up with load sensors, and then placed in a wind tunnel that simulates a design storm and provides load data at critical points.

As for your specific question regarding smoke trails with cars, understand that my fluid knowledge comes from two fluid dynamics classes in undergrad.  I think that these trails are used to demonstrate how particles travel over the surface of a car, giving useful information about the aerodynamics and drag coefficient of the car.",null,1,cdm0p63,1rd4yo,askscience,new,4
null,null,null,0,cdmcdq3,1rd4yo,askscience,new,3
meerkatmreow,"The data from the smoke is a type of qualitative flow visualization.  Based on the behavior of the smoke, conclusions about laminar v. turbulent flow can be drawn.


The data from wind tunnel tests can come in many forms depending on what you're trying to do.  Full field quantitative measurements (using something like Particle Image Velocimetry or Pressure/Temperature Sensitive Paint) can be useful for exploring the entire flowfield.  Point measurements using pressure transducers can provide the needed data if you're interested in a certain area.  Data such as overall forces and moments on the model may be what you're after.  Qualitative measurements such as flow visualization uses smoke lines or laser induced fluorescence can help identify areas where additional investigation would be beneficial (ie, separate flow).

What you want to measure and how you measure it are very tightly coupled.  When you do a wind tunnel test you can often choose how you measure things by what you're interested in rather than using a one size fits all approach.",null,1,cdm59n4,1rd4yo,askscience,new,2
AbsolutePwnage,"The smoke shows were the air flow is laminar and where it starts becoming turbulent and therefore, where parasitic drag starts appearing. It also looks cool, which is why they show it very often in ads and other media.",null,0,cdojkoc,1rd4yo,askscience,new,1
burninatingpeasants,"At least from my experience, smoke trails are not used to gather numerical data.  Instead, they may be used to gather more fundamental, conceptual data: ""is the airflow separating from this section of the wing?"", ""Is this section of airflow turbulent or smooth"", etc.

For 3D models (such as a scale model of a full airplane), most of the data is gathered by the actual device used to hold the airplane.  The mounting device is rigged with sensors that can detect forces and moments, so you can get an exact measurement of how much force the wind is putting on the model (and therefore, the mounting device).

Another method sometimes used is a series of ""pressure taps"", which are small holes drilled in the surface of a model to which pressure lines are connected.  By measuring the pressures recorded at each tap, you can use math to determine how much force is exerted on the entire model (or at least that section of the model).  This is used more when trying to get a detailed view of the airflow over a certain component, rather than the overall force acting on the entire component as a whole.",null,0,cdpw20l,1rd4yo,askscience,new,1
Lost_Wandering,With today's digital imaging capabilities it has gone beyond qualitative to actual quantitative analysis possible from smoke trails in wind tunnel. Digital particle image velocimetry allows for tracking smoke particles in space and can be used to do such things as validate or adjust computational flow dynamics models.,null,0,cdpztz6,1rd4yo,askscience,new,1
user2097,"3rd year aerospace engineering student here. Wind tunnels are used largely for models to mimic equivalent flow conditions, and the data from the testing includes qualitative and quantitative data.

Sometimes your test is performed to verify dynamic stability, examine stall characteristics of aeroplanes), examine flow condition (separation, turbulence, mixing...), etc. Other tests will produce data based off sensors attached to the model or tunnel such as force on a wing, dynamic response to an input, measure location of separation with hot wires, etc. ",null,1,cdm4stc,1rd4yo,askscience,new,1
null,null,null,31,cdm2wxo,1rd53a,askscience,new,103
ryannayr140," Mythbusters did something similar to what you original question you asked, I highly recommend watching it.  In a non theoretical world one car is going to be lighter than the other.  The lighter car is going to receive much more damage than the heavier car.  Does anyone know if hitting a car that weights twice as much as you head on at 30 is worse than hitting an immovable object at 60, another car at 60?",null,4,cdm5qvb,1rd53a,askscience,new,24
testingthelimits,"It seems like lots of people in the comments are reading ""car"" and thinking ""object"". Modern cars have crumple zones. Also, your definition of ""damage"" is essential to the problem. I'm going to assume ""passenger damage"". 

A head on impact between two 30 MPH cars should be better than a 60MPH impact with one car and a wall. Because in the instance with two cars there are two crumple zones, providing more opportunity for a  gradual de-acceleration. 

A head on impact between a 60MPH car and a stationary car would look similar to the 30 vs. 30 MPH instance. I would generally expect a more favorable outcome. There are other factors such as the brakes/skidding of the stationary car also providing additional opportunities for gradual de-acceleration, but without substantially more detailed information the problem is pretty general.

If you are interested in cars crashing, the [NHTSA website](http://www-nrd.nhtsa.dot.gov/database/veh/veh.htm) (National Highway Traffic Safety Administration) has crash test results available for download (includes videos, report, photos.. etc). 

If ""car"" was replaced with ""object of mass ""x"" "" it might be possible to have an answer that meets the ""no speculation"" guidelines. 
",null,9,cdmepc8,1rd53a,askscience,new,19
null,null,null,11,cdm3al3,1rd53a,askscience,new,18
zdavis1987,"IIRC, in a perfect experiment with two identical cars impacting head on, both traveling 60 mph, each car would experience the same amount of force as if that car had impacted a solid object at 60 mph, not 120 mph. The combined velocity of the cars is 120 mph, but there are now two cars to spread the force through. So in your case, two cars impacting head on at 30 mph would be the same as one car impacting a solid object at 30 mph. It's probably safe to say that impacting a solid object at 60 mph would do more damage.",null,1,cdmcgu0,1rd53a,askscience,new,6
claireauriga,"In the collision, the kinetic energy of the moving vehicle needs to go somewhere. If it goes into your body, then you are going to get hurt. I don't know numbers, but I can discuss some of the relevant factors. 

**First up: two identical cars, each at 30 mph, in a head-on collision.** They're going to spin a bit, but we can think of it as hitting each other and coming to a complete halt. All the kinetic energy of each car (0.5 x mass x velocity^2) needs to be converted into some other form. Some of this energy will be used to crush and deform the car bodies. The purpose of crumple zones is so that there are lots of bits to crumple and take up the energy, while the bit protecting your body stays strong. The rest of the car's kinetic energy will go into sound, heat, and doing unpleasant things to your body. 

**One car at 60 mph hitting a car that is stationary but able to slide:** The moving car has a lot more kinetic energy than the two 30 mph cars combined, because kinetic energy = 0.5mv^2 as mentioned above. However, some of its energy will go into crumpling the cars, and some will go into accelerating the stationary car for a bit, as it pushes it along, and some will stay in the moving car, as it doesn't stop completely. I don't know enough to tell you if the energy left over to go into your body is more or less than in the first case. 

**One car at 60 mph, hitting an immovable object:** This could get nasty. The one car has a lot of kinetic energy, and it all needs to be used up. The car will deform, the immovable object might, and there will be heat and sound, but still ... there's probably a lot of energy left over to be absorbed by your soft, vulnerable body. ",null,10,cdmivwc,1rd53a,askscience,new,14
U235EU,"Assuming both cars end up at ""0"" mph the 60 mile per hour collision will be much more violent and damaging. The formula for kinetic energy is one half the mass multiplied by the square of the velocity. The 60 mile per hour car will have 4 times the kinetic energy of the 30 mile per hour car. ",null,7,cdm1z7x,1rd53a,askscience,new,9
ttifiblog,"This question is all about energy, not momentum.  Energy goes with the square of velocity and 60^2 is a lot more than 2*30^2.  Not only that, but cars can deform and have energy absorbing crumple zones.  A solid object is not going to have that. So in terms of energy transference to the driver or passengers, hitting a tree at 60 is much much much worse than hitting another car at 30. ",null,12,cdm59cb,1rd53a,askscience,new,14
nerys71,hitting a solid object. because while the initial impact energy is similar in the case of the head on the two cars are both (relatively speaking) squishy so less energy will transfer (over time) to the passengers than one car at 60 hitting something solid (less squishy),null,10,cdm9dy3,1rd53a,askscience,new,11
tstneon,Definitely one car hitting a solid object at 60 mph would cause more damage. Both the cars traveling at 30mph would sustain damage and split the energy between the two cars. They would both be similarly damaged. Where as the one car traveling at 60 mph is the only object that is taking the energy and taking all the damage. ,null,1,cdme9ac,1rd53a,askscience,new,2
PublicallyViewable,"Other people answered this question, but I'll put it into terms that are easier to visualize.

Visualize a car from the side driving left to right hitting an immovable wall head on at 30 mph. You'll see that the car comes to a complete stop very quickly, and never moves past the surface of the wall (to the right).

Now visualize the same car hitting an identical car head on at 30 miles per hour, that is, replace the collision of the wall in the previous visualization with a collision of the two cars at the same position. Again, you'll see that the car on the left side does not move past the collision point. Which means the two damages must be equal.

Like others have also said, it's acceleration that does damage. Since the two situations have the same point of collision, and neither car moves past the collision point, the must have the same acceleration.",null,0,cdmibrd,1rd53a,askscience,new,1
UnquietTinkerer,"If the ""solid object"" is a parked car then the two collisions are essentially the same.  In the head-on case the two cars end up at rest (at higher speeds they might disintegrate and send debris flying everywhere, but 30mph is slow enough that the cars could just crumple).  In the other case, the 60mph car would hit the parked car and the combined wreckage would continue moving at approximately 30mph down the road until friction or some other force stopped it.  In both cases the change in momentum for the passengers and the total kinetic energy released in the collision would be identical.

If the ""solid object"" is something like a brick wall then it could stop the car abruptly, resulting in a much greater change in momentum and release of kinetic energy.  This would be much more damaging to the car and its passengers.  I don't see the profit in this comparison though.  A more interesting question is whether it's whether it's better to hit a car head-on (both traveling at *60mph*) vs. a solid wall.  In both cases the you would end up stopping abruptly, but hitting the wall releases less kinetic energy and so would be less damaging.",null,0,cdmijd2,1rd53a,askscience,new,1
bjornartl,"Look aside from the energy in each vehicle (physics-vise), take into account that two cars head to head would have two deformation zones. 

This deformation would not just dampen the impact but it would also allow the two cars to twist around each other and spin off and to some degree continue in the same direction their energy is projected, allowing friction over hopefully a longer distance to stop the vehicles. 

Hitting straight into a wall however forces the vehicle to come to a halt there and then. All the energy will be projected straight into this solid mass. It can be even worse when this solid/grounded mass presents a lower surface area, like a lamp pole, giving it more penetrating power. The pole will dig itself right into the core of the car. ",null,0,cdmio88,1rd53a,askscience,new,1
null,null,null,22,cdm2cds,1rd53a,askscience,new,16
tthershey,"&gt; Dr. Harper explained in her presentation that the cervical cancer risk in the U.S. is already extremely low, and that vaccinations are unlikely to have any effect upon the rate of cervical cancer in the United States.  In fact, 70% of all HPV infections resolve themselves without treatment in a year, and the number rises to well over 90% in two years.

While it is true that the chances of getting cervical cancer are low, the vaccine does prevent a cancer, which is amazing.   Very few cancers have the potential of being eradicated like this.  Not all strains of HPV are covered by the vaccine, and not all strains of HPV cause cancer.  So on the plus side, those scary statistics about how prevalent HPV infections are can be misleading because the actual incidence of cervical cancer is low even among those who get infected with HPV.

Anogenital warts are mostly caused by HPV 6 and 11.  This lesion is usually benign (not cancerous).  Most cervical cancer is caused by HPV 16 and 18, but there are some other, less common strains of HPV that can also cause cervical cancer.  Gardasil protects against HPV 16 and 18, which prevents 70% of cervical cancers.

&gt; All trials of the vaccines were done on children aged 15 and above, despite them currently being marketed for 9-year-olds.

Not true, here's an example: http://www.ncbi.nlm.nih.gov/pubmed/23971122

&gt; So far, 15,037 girls have reported adverse side effects from Gardasil™ alone to the Vaccine Adverse Event Reporting System (VAERS), and this number only reflects parents who underwent the hurdles required for reporting adverse reactions.  At the time of writing, 44 girls are officially known to have died from these vaccines.  The reported side effects include Guillian Barré Syndrome (paralysis lasting for years, or permanently — sometimes eventually causing suffocation), lupus, seizures, blood clots, and brain inflammation.

I would have to see the source for this claim to make any specific comments, but in general I can say vaccines are tested very vigorously for their safety.  It has to be expected that some people will suffer health consequences after receiving a vaccine.  Many of these people might have suffered those consequences whether or not they had received the vaccine because they had some pre-existing conditions, and some might have rare diseases that make them more susceptible to complications.  But serious complications from the vaccine are rare.

&gt; Studies have proven “there is no demonstrated relationship between the condition being vaccinated for and the rare cancers that the vaccine might prevent, but it is marketed to do that nonetheless.  In fact, there is no actual evidence that the vaccine can prevent any cancer.  From the manufacturers own admissions, the vaccine only works on 4 strains out of 40 for a specific venereal disease that dies on its own in a relatively short period, so the chance of it actually helping an individual is about about the same as the chance of her being struck by a meteorite.”

This is simply not true.  The vaccine has been proven to prevent HPV 16 and 18, which prevents 70% of cervical cancers.  The CDC is a reputable source for information on this: http://www.cdc.gov/STD/HPV/

Some more info:

3 key genes in HPV 16 and 18 are E2, E6, and E7.  E6 and E7, when activated, disrupt cellular defense mechanisms that kill off cells that might become cancerous.  E6 and E7 are normally repressed by E2.  HPV infects cells by integrating the viral DNA into the host cell (human) DNA.  HPV can insert itself into the human DNA in many different positions, and where it inserts itself is, as far as we know, random.  If HPV inserts itself in a way that disrupts the E2 gene, then E6 and E7 are free to disrupt the host cell's defense mechanisms, leading to cancer.

So, if you get an HPV infection, you might get a strain that doesn't cause cancer.  Or, you might get a strain that does cause cancer, but the HPV inserts itself in a way that does not result in cancer.  But you could be one of the unlucky people who gets HPV 16 or 18 that integrates in such a way that causes the cancer.  So yes, getting cervical cancer from HPV is rare, but you don't know if you are going to be one of the unlucky ones or not.",null,7,cdm2tjm,1rd56j,askscience,new,44
dreitones,"If you do a quick google search you will see that Dr. Diane Harper doesn't in fact work for Gardasil -as the article claims- this immediately throws into question the validity and truth of any claim the article made. I wouldn't trust this article's claims. 

also, here is an article from that counters the claim made in your article: http://www.skepticalraptor.com/skepticalraptorblog.php/gardasil-researcher-against-vaccine-myth-debunked/

edit: grammar
",null,2,cdm1c02,1rd56j,askscience,new,19
housebrickstocking,"Bit busy or I'd pass you a lot of links...


The HPV vaccine has been associated with a hysterical response pattern globally, all symptoms being ""faint"", ""disorientated"", and other hard to quantify BS. The fact that it is being re-reported over and over as if the risk of fainting somehow offsets the risk of having ones' cervix become militant and attempt to encroach on other organs.


Stepping back however, HPV vaccine is one of the safer ones according to unwanted effect studies, with most of the effects listed being related to the injection itself NOT the vaccine.


The anti-vax mobs break risk management rules, let us say that there is ""one in one hundred chance of unwanted effect, with a one in ten thousand chance of a catastrophic effect"" - that is not the same as one in a hundred chance of unwanted effect, the worst being catastrophic"", however in any case the catastrophic effect is still probably preferable to being dead due to measles or suffer a life of disability due to rubella.",null,0,cdmbzxq,1rd56j,askscience,new,3
dontgothatway123,"At the end of the day [high-risk HPV types (16, 18, 31, 33, 35, 39, 45, 51, 52, 56, 58, 59, 68, 69, 73, 82) are found in over 99% of the cases of cervical cancer](http://www.cdc.gov/vaccines/pubs/pinkbook/hpv.html).  Guardasil obviously doesn't vaccinate for all of those but as stated in another reply HPV 16 and 18 account for 70% alone.

Therefore, in many ways, cervical cancer can be thought of as an STD.  ",null,0,cdmjdr3,1rd56j,askscience,new,1
caitdrum,"As of May 13, 2013, VAERS had received 29,686 reports of adverse events following HPV vaccinations, including 136 reports of death, as well as 922 reports of disability, and 550 life-threatening adverse events. The vast majority of adverse reactions don't go reported.

The fact is 1/4 of all VAERS reports are now HPV vaccine related, this is extremely high considering the vaccine has been on the market less than 10 years.  

This astonishingly high incidence of adverse reactions is clear indication of over-prescription and profiteering.  Be careful.  I would go on to talk about immune system optimization and diet but i'll probably be labeled ""anti-science.""
",null,3,cdmgfyw,1rd56j,askscience,new,3
xtxylophone,"Aside from the comparatively 'busy' time around the Earth's formation, nothing has changed. They are just infrequent and the evidence they leave lasts a long time.

Check out: http://en.wikipedia.org/wiki/Impact_event

Impacts that can change geography are about on the scale of the length of Human civilisation. Don't take the data for one being 'due' though. ",null,1,cdm0yuw,1rd5c8,askscience,new,8
null,null,null,32,cdm23mb,1rd5mf,askscience,new,128
dontgothatway123,"There are multiple known changes of people sleeping on their right or left lateral sides.  Whether or not this correlates with a disease state or with long-term benefits I believe the evidence is still out. 

What we know:

- There are known changes in cardiac outputs depending on your positioning (supine, prone, left lateral, right lateral) suggesting that [sleeping on your right side improves cardiac output](http://www.ncbi.nlm.nih.gov/pubmed/9768796) but the studies are inconsistent and sample sizes are small.  The perceived implications are primarily for those in low cardiac output states.

- Sleeping on your left lateral side helps decrease *symptoms* of GERD because the body of your stomach rests in a way that allows acidic stomach contents to 'pool' there decreasing the chance they re-enter your esophagus.  However this position reduces gastric emptying; the food contents will remain in your stomach.  

- Sleeping on your right lateral side helps *increase gastric emptying* because the pyloric sphincter that separates your stomach from small intestine opens towards the right.  Food will leave your stomach more quickly laying on your right versus

- Sleeping with the head of the bed elevated (usually on bricks or phone books) 10-15 degrees or more has the most impact on gastric reflux according to the research.  Broad recommendations to elevate the head of the bed for people with GERD are generally made as a first line recc. in combination with other things (smoking cessation, meal timing, food triggers, etc)

- If you have a unilaterally diseased lung for whatever reason then sleeping with the good lung 'down' will increase blood oxygenation.  This is because the lung on the bottom (the good lung in this case) gets more blood perfusion and therefore more oxygenation occurs.

- Sleeping on either side versus your back is suggested in sleep apnea.  This is because the soft palate and tongue fall back and occlude your airway during sleep when in the supine position.  This is also a similar but slightly different reason why we place unconscious people in the 'rescue' side lying position.  To help keep their airway clear.

- Infants seem to have a reduction in the rate of SIDS when placed 'back to bed' meaning a supine position. 

There are more examples than I've listed, I'm sure.  An important thing to remember is that in medical science just because there is a change does not necessarily mean there is a benefit or detriment significant enough and with enough evidence behind it to make broad recommendations.  Consider that.",null,22,cdmhzt8,1rd5mf,askscience,new,91
null,null,null,9,cdm6gn5,1rd5mf,askscience,new,14
null,null,null,30,cdmgdmn,1rd5mf,askscience,new,20
tin_can_conspiracy,"There are still trace amounts of bacteria. Heating the caviar is not enough since bacteria can get into the container when filling. Now unless they used a hot fill (putting the food product into its container at 180 degrees Fahrenheit, and forming a vacuum to ensure as little oxygen as possible) there is still enough bacteria in there to replicate enough that the food's quality or safety is compromised. ",null,0,cdm258n,1rd5rx,askscience,new,5
GeneralKrakus,"Shelf life can relate to off-flavors as well, not just yeast/mold/bacteria. Even if something is pasteurized and sealed, the flavor of the food/beverage can still change over time. This can be from oxidation, volatile loss (smells/flavors escaping the food/beverage into the headspace), or separation/destabilization of the food/beverage matrix.  
  
Side note: shelf life is typically just the ""quality guaranteed by"" date. You can usually consume most foods after the shelf life date, but each food is different (I wouldn't recommend drinking old milk). If it smells/looks funny, don't eat it",null,1,cdm95un,1rd5rx,askscience,new,5
housebrickstocking,"Aseptic packaging and handling is only half the battle, even without acetobac and yeast munching into the food it is subject to other reactions, settling, half life on preservatives...

In short - because it is aseptically in a can/jar doesn't mean it is held in stasis.",null,0,cdmdtw1,1rd5rx,askscience,new,1
lengendscrary,"Pasteurization doesn't kill all the bacteria it kills most of them. It is a process that kills most of the noxious ones, including yeasts . It involves heating food to a high temp and holding that temp for a few seconds. So milk,for instance, is a breeding ground for bacteria and can only last a few weeks after this process. Caviar, however, is salted so its not a good or inviting place for bacteria to grow and has a shelf life for 2 years.",null,0,cdmfv6y,1rd5rx,askscience,new,1
endocytosis,"There's a good [Wikipedia](http://en.wikipedia.org/wiki/Pasteurization) article on it.  Basically, as others mentioned, it doesn't kill all bacteria, just most of the bacteria that can cause spoilage and typically all of the harmful pathogenic bacteria.  The Wikipedia article discusses milk, but there's multiple types and ways something can be pasteurized, such as flash pasteurizing (briefly heat something really hot, not from concentrate orange juice is also done with this method), or Ultra-high temperature (heat something really hot for a while, the half-and-half containers or milk cartons that don't need to be refrigerated are done using this, note as soon as they're opened bacteria/yeast/mold can enter so they must be refrigerated).  

A quick google search showed that unpasteurized caviar apparently is more expensive and desired because the flavors are more intact, but unpasteurized caviar is also extremely perishable.  This makes sense, there's a trade-off: even if you're extremely careful harvesting and preparing it, the micro-organisms are still there and will readily go to work breaking down the caviar (spoiling it), refrigeration/preservatives will only slow the process down, pasteurization will wipe out *most* of them, but a few will remain, and after 2 years, while it may or may not be spoiled, the flavor will definitely not be the same.",null,0,cdmppru,1rd5rx,askscience,new,1
null,null,null,2,cdm2rd8,1rd5rx,askscience,new,1
iorgfeflkd,"tl;dr: If the laws of physics don't depend on location, momentum is conserved.

Noether's theorem says that for every symmetry in a process, there is a conserved quantity. For things that are translationally symmetric, that conserved quantity is momentum. This means that if you consider a collision on a highway, and then the same collision a couple of miles down the highway (translation), if they behave the same (where on the highway it is didn't matter), then momentum is conserved.

That's fairly complicated, another but less rigorous way of looking at it is that momentum changes when a force is applied, and if no force is applied then the change in momentum is zero, so in the absence of external forces the total change in momentum is zero.",null,1,cdm156w,1rd5ys,askscience,new,9
rupert1920,"Plastics are long polymers, and can undergo [polymer crystallization](http://en.wikipedia.org/wiki/Polymer_crystallization) when stressed. It is the formation of these ordered structures that causes scattering in the material - which is why it looks white.

In some plastics this process can be reversed by heating the plastic (for example, boiling it in water for a few minutes).",null,1,cdm8ncc,1rd67w,askscience,new,4
bohr_exciton,"The most probable explanation is that by bending the material you are creating defects, i.e. inhomogeneities in structure, density, etc. These defect sites can then act as scattering centers, which in turn reduces the transparency. This is a similar effect to scratching the surface of ice, for example.",null,1,cdm3rsa,1rd67w,askscience,new,4
ultimatety,"The answer to this is actually more complicated than you would think.  It all boils down to the fact that the surface layer of the ice underneath the object is partially melted.  However, the reason for how this top layer melts is somewhat of a scientific controversy.  People used to believe that the pressure exerted causes the ice to melt, however, this appears to be false.  
The two current theories are that: 
1) The friction of the moving object causes the top layer of the ice to melt
or 2) The top layer of water molecules are unable to bind correctly to the layers underneath and thus stay in a quasi water-like state.

TL;DR There is a little bit of liquid water on top of that ice, and liquid on top of something smooth makes it slippery.",null,3,cdm1crb,1rd6cm,askscience,new,19
ace425,"Adding on to this, why doesn't waters ability to form hydrogen bonds affect the slipperiness of ice? It seems like since water likes to form hydrogen bonds that ice would not be slippery but instead have a lot of traction, but this obviously isn't the case. Can someone expand on this please?",null,1,cdmjimu,1rd6cm,askscience,new,1
sharp12180,"When you step on ice, you apply pressure to the ice directly below you. This pressure decreases the freezing point of ice and so there is a thin layer of liquid water formed between your feet and the ice. Its this difference that causes ice to so slippery.
http://www.youtube.com/watch?v=Stx6kLd9dYI",null,20,cdm153f,1rd6cm,askscience,new,6
incognegro76,"You can graphically illustrate a line with this equation but it will form an asymptote very rapidly to zero.

y = 2^-x",null,1,cdm6f0c,1rd6ok,askscience,new,3
rlee89,"y=-ln(1-t)/ln(2) seem a good place to look.

For a runner running at velocity 1, y is the number of terms you have added to get the runner's distance at time t.  It is rather obvious that no matter how many terms you add, you will never reach the runners distance for any time after t=1.

This is derived from the closed form of the partial sum 1/2 +1/4 + 1/8 ... 1/2^n = 1-2^(-n).",null,0,cdmam7a,1rd6ok,askscience,new,2
Tidurious,"It's not so much the altitude as it is proximity to large cities and prevailing wind patterns.  There aren't a lot of large cities with manufacturing and chemical processing plants near the French Alps, for example, and the higher you go, the smaller the population is - therefore, the air is much cleaner.  

In Hawaii, some of ""most pure"" air in the world is blown in from the Pacific, because although these winds originate in China, they travel over the pacific for approximately 3 weeks before making landfall in Hawaii which allows all the pollution to settle out.",null,20,cdmdtdm,1rd6th,askscience,new,42
ww-shen,"There are many type of 'pollution', different components in air. The O2, CO or CO2 level are tolerable in certain interval, it just gives you a headache. But there can be different chemicals, becteria, viruses, dust, heavy metals, or even hazardous waste or radiation carried by the dust.
The air cleaning 'things' are different too.  Rain cleans dust and phisical substances, plants refreshes CO2 to O2 (daytime), UV light will kill bacteria and viruses, and some things heavyer than the 'air' (CO, Butane, dust, etc) will just sweep out in the calm air. Lighter gasses will pass to upper atmosphere (freons). And there are other special cases, like CO2 or suplhur can dissolve in water, even rainwater. Carbonic-acid &gt; light type of acid rain or suplhur &gt; acid rain.

So, when the suplhur and dust pollution is high coused by the coal firing (London, 60 years ago) Red snow or acid rain can be fall elsewhere (Sweden's high mountains.)",null,8,cdmh86v,1rd6th,askscience,new,13
perso_nel_mondo,"The least polluted air I've ever seen is in the Antarctic. It is so remote there's nothing in it (besides the usual). There are so few particles that breath doesn't even condense out: You know how you can see your breath when it's cold? Sometimes, you don't see the condensation because the air is so clean.

Then again, ""polluted"" is relative. The Appalachian mountains in the TN valley and SW Virginia get dangerously bad, and it's caused by what trees emit mixing with what's blown in from cites.",null,0,cdmxemi,1rd6th,askscience,new,2
Hagenaar,The other feature of mountains (at least the ones which are not involved in heavy industry) is often an abundance of trees. [Trees/forests are able to reduce airborne particulate quite well.](http://cen.acs.org/articles/91/web/2013/11/Trees-Capture-Particulate-Matter-Road.html),null,0,cdmqdkm,1rd6th,askscience,new,1
Deeger,"The least polluted air is where it is filtered by the Amazonian rain forest. http://www.sciencemag.org/content/329/5998/1513

Cold air feels cleaner, and often *is* cleaner, due to its lack of water content. Water vapor is often a sponge, picking up all sorts of other particulate. ",null,0,cdmtp6j,1rd6th,askscience,new,1
instalockyi,"Think about a seesaw. A fat kid sitting halfway across and a skinny kid sitting at the very end may very well be balanced--this seems intuitive. The same thing happens with, say, spinning a ball on a string. A larger mass on a shorter string is easier to spin around than a small mass with a long string.

So, imagine that cylinders rolling down a slope as masses rotating around an axis in the center. Assuming they are the same mass, the hollow cylinder is essentially like the fat kid sitting at the very end--it takes a lot to move him. The solid cylinder is more like a few light kids distributed across the radius.",null,11,cdm74bu,1rd6yw,askscience,new,15
lukehashj,"If the cylinder is full of liquid, it rotates more slowly because the liquid is slipping past itself as the cylinder rotates, and some of the kinetic energy is transferred into friction. What's also interesting is that once the cylinder is at the bottom of the hill, you can stop it and the liquid inside will stay spinning. You could then place the cylinder back down and it will begin to roll again - even uphill if possible!

The higher the viscosity of the liquid, the stronger the effect.

edit: I've seen this in person with a large can of syrup. When placed on a ramp, the can looked basically ""stuck"" because it hardly moved. Upon reaching the bottom, the professor turned the can around and it rolled about halfway up the ramp. So why is my answer being downvoted? What do I not understand?",null,6,cdm99w3,1rd6yw,askscience,new,6
YaMeanCoitus,"If the cylinder is FULL of liquid it will roll down faster than an empty cylinder for the reasons mentioned in the other comments.  However, if the cylinder is partially filled with water, it will roll down slower.  This is caused by turbulent flow in the cylinder.  Think of how its much easier to splash around mouthwash when your mouth has a bit of air in it.  This turbulent flow allows a transfer of macroscopic kinetic energy to microscopic energy (turbulence and heat).",null,5,cdm43xg,1rd6yw,askscience,new,3
dampew,"Look up ""moment of inertia"" for a full explanation.",null,18,cdm72ew,1rd6yw,askscience,new,13
samloveshummus,"A solid cylinder has a higher moment of inertia than a hollow cylinder - this means that it is more resistant to angular acceleration, the same way that an object with greater mass is more resistant to (linear) acceleration. Therefore the hollow cylinder can pick up a fast speed more quickly than the solid cylinder can.",null,10,cdm1wjv,1rd6yw,askscience,new,6
patchgrabber,"Well, kelp are basically algae, so they are quite different from land plants in pretty much every way except photosynthesis. Although their holdfasts resemble, and may be a primitive form of, plant roots, kelp are fundamentally different. 

In most land plants, while very limited photosynthesis may occur in the stalk of the plant, most of its photosynthetic activity is in the leaves. Kelp, in contrast, photosynthesize in every part of the organism (although different parts have different levels of photosynthetic ability depending on age), allowing for more and making light less of a limitation than it is in land plants.

The environment the kelp lives in is also a big factor. Since it is under water, light is attenuated differently than above water. Due to the large amount of particulates, blue light is attenuated rapidly in coastal waters, and blue light is much more valuable than red light that can penetrate deeper at a higher intensity.

While there are products out there that purport to use kelp in them to make plants grow faster, I'm thinking this is only because of the nutrients, not any special property that is linked to kelp growth. I cannot think of any way at present to genetically transfer this quality to land plants; their limitations are different, their environments are different, and they are just fundamentally different organisms. Kelp would be much better used as fertilizer, as you suggest, than as a source of genetic information, although in the future that may change.",null,0,cdmahyq,1rd7fs,askscience,new,2
MarineLife42,"As /u/patchgrabber said, Algae are very different from plants.  
Here, the main difference is that (higher) plants grow, i.e. create new tissue only at specific regions on their body. Usually this is at the tip of the plant or leaf, the apex. In grasses (grains) it happens at the nodes too.  
Kelp, on the other hand, creates new tissue along the entire length of its thallus (the big leaf) which is why in grows so fast.  
Another big difference is that the thallus doesn't have much, if any speciation; it is composed of more or less the same kind of cell. Higher plants, in contrast, have an internal structure of xylem, phloem, bark etc. that requires many different specialized cell types.  
Both these differences work together to prevent us from simply transplanting this ability into our crop plants. 
BTW - some bamboo species can also grow very fast, up to 10cm a day or so but there is trickery involved. In fact the plant tissue has been created at the usual speed beforehand, but compressed. During the elongation phase, the plant sucks up much water and fills the cells so it telescopes upwards. ",null,0,cdn4dss,1rd7fs,askscience,new,1
therationalpi,"Basically, it's because multiple sources together are louder than a single source. You are probably familiar with constructive and deconstructive wave interference, where two waves on top of each other can either add or subtract based on phase. As it turns out, if you have sounds at different frequencies, or if the phase relationship varies randomly over time (as it would when you have two people yelling), then you get interference which is mostly positive. The math would be that the squares of the pressure add.

A good rule of thumb is that the sound pressure goes up by 3 dB every time you double the number of people. Likewise, if the distance to the source is much greater than the size of the source, then the loudness will drop by 6 dB for every doubling of distance. Additionally, there is also sound damping that becomes important at long distances. This is highly dependent on temperature, humidity, and frequency, but let's just ballpark it at about 6 dB per kilometer.

So, let's suppose you could clearly hear someone yelling 100 meters away when it's fairly quiet. If I went 1 mile away (approximately 1600 meters), then that sound would need to be 34 dB louder (24 dB from doubling the distance 4 times + ~10 dB from 1600 m worth of sound absorption). From here, we simply solve to see how many people we would need in the soccer stadium to increase the source strength by 34 dB. In this case, we would need to double the crowd 11.3 times, which means you need about 2500 people. Naturally, the more people beyond that you have, the louder it will be when it reaches you.

Hope that answers your question!",null,64,cdm1wqf,1rd7qj,askscience,new,414
bobevans1,as a followup: how much does it depend on the weather - things like humidity and wind direction?,null,8,cdm942z,1rd7qj,askscience,new,16
brawnkowsky,"different ethnicities will have different genes that express proteins differently.  For example, [degrees of lactose intolerance vary between regions, from 5% in north europe to 90% in some african and asian countries](http://www.scielo.br/scielo.php?script=sci_arttext&amp;pid=S0100-879X2007001100004&amp;lng=en&amp;nrm=iso&amp;tlng=en).  this is simply because of altered protein expression (lactase in this example), which is a factor in all protein expression in our body.  also, people will have varying levels of gut microorganisms depending on their environment, what they eat, and their own immune strength; this natural flora is important in digestion.

",null,0,cdnfwoy,1rd803,askscience,new,2
skleats,"The approximate age of a person can be determined a number of ways (prortion of naive T cells, growth plate presence in bone, etc.), but these approximations are all based on average values across many humans, so there isn't a way to get exact birthdate - usually you'd be looking at a 2-5 year window of age.",null,91,cdm5xi6,1rd8ip,askscience,new,428
carl_888,"Atmospheric nuclear testing from the 1950s caused a worldwide spike in the background level of several radioactive elements, including some that are incorporated into [human tissues](http://en.wikipedia.org/wiki/Baby_Tooth_Survey), eg Strontium 90. It should therefore be possible to determine an individual's birthdate by measuring the amount of particular isotopes in their tissues, against a standard curve.

edit: [Here's](http://www.pnas.org/content/early/2013/06/26/1302226110.abstract) a reference where this method is used.",null,21,cdmcfye,1rd8ip,askscience,new,116
mckulty,"From about age 30 to 60 the flexibility of the crystalline lens (""amplitude of accommodation"") declines in a fairly predictable fashion. Refractionists learn a table of values for supplemental optical correction that predicts age pretty well between the ages 40 and 50. The [scatter becomes smaller with age](http://web.ncf.ca/aa456/misc/cataracts/accommodationVsAge.png), and reaches a [nonzero endpoint](http://www.scielo.br/img/fbpe/abo/v63n6/9618f1.gif) that is probably due to optical depth-of-field.

",null,15,cdm8mi8,1rd8ip,askscience,new,53
TheSynsear,"There are also patterns in dental records. Each Tooth enamel goes through a daily cycle where it accelerates, and slows down during a 24 hour period. These can be observed under an electric microscope. When observed these teeth patterns will develop into long strands that each cycle creates a bead on. If you count the number of beads you can tally the days that the enamel has been forming, give or take the teeth development time of newborn babies. This of course proves more difficult in adults due to the loss of early teeth. This method also works on fossilized teeth, and the teeth of any enamel based organism.",null,13,cdmc6qv,1rd8ip,askscience,new,36
null,null,null,8,cdm5g8r,1rd8ip,askscience,new,15
arachtivix,"If you could test a person's upper hearing range (highest frequency they can hear for example), this can infer a range for their age.  Here's a study that shows high frequency hearing ability is highly correlated with age.  

http://occmed.oxfordjournals.org/content/51/4/245.full.pdf",null,15,cdmduex,1rd8ip,askscience,new,24
xerberos,"In the Scandinavian countries, the immigration authorities x-ray teeth and wrists to determine the age of immigrants who claim to be under the age of 18. The reason is that it is (obviously) easier for children without parents to get asylum, so some lie about their age. I have tried to find out exactly what it is they check, but haven't found any good info.",null,10,cdmi1ua,1rd8ip,askscience,new,18
archaeosaurus,"In terms of archaeological skeletons the most common macroscopical ways to assign age are through teeth eruption and fusion of different skeletal elements - but these only are really useful for individuals up to early 20s, when all teeth are erupted and bones are fused.

Older individuals can be aged to within around 10 years by tooth wear, the state of cranial sutures, the fusion pattern of the pubic symphysis and auricular surface of the pelvis and the sternal end of some ribs. All degenerate/change with age.

Of course, all of these depend on good preservation and can only give you a range. And only once they're dead! For more information Byers' Introduction to Forensic Anthropology is pretty good.",null,8,cdmjyvn,1rd8ip,askscience,new,13
Philosophisation,"It may be possible to determine age via analysis of bone marrow. The amount of wbc undergoing mitosis at any given time should be lower over time, however this isn't accurate at all. The most common methods used by doctors is not telomere analysis, which is far too specific, rather growth plate analysis.",null,10,cdmg9o4,1rd8ip,askscience,new,13
null,null,null,9,cdm64ib,1rd8ip,askscience,new,11
null,null,null,0,cdmm6j2,1rd8ip,askscience,new,1
bopplegurp,No one mentioned this paper that recently came out claiming that age can be measured by DNA methylation.  http://genomebiology.com/2013/14/10/r115,null,0,cdnvr5h,1rd8ip,askscience,new,1
iorgfeflkd,"Protons and neutrons are held together by the strong nuclear force (or a residual form of it, sort of the equivalent of van der Waals forces for nucleons), which in stable nuclei is much stronger than the electrostatic repulsion between protons. If a nucleus has too few neutrons then the repulsion will break it up.",null,1,cdm1vxx,1rd8yh,askscience,new,5
iorgfeflkd,"Beta decay involves a neutron turning into a proton and emitting an electron (beta particle) and an antineutrino. Static electricity involves movement of pre-existing electrons. Nuclear reactions generally involve much higher energies than electronic or atomic. For example, beta particles from potassium decay in bananas have as much energy as if they went through a 1.5 million volt potential, and static discharge is typically in the thousands. However, static discharged generally involves a lot more electrons compared to most radioactive sources.",null,1,cdm28la,1rd953,askscience,new,7
zalaesseo,"When Benjamin Franklin said that Charge can only be collected and lost, he really meant it. When you discharge electricity, electrons just moves to the metal object.

Until beta decay. Beta decay literally creates a new proton electron pair and an antineutrino^irrelevant. You're not collecting charges, you're MAKING charges appear from nothing.   ",null,0,cdm2ovh,1rd953,askscience,new,3
cosmicosmo4,"When you get a static shock, you're typically experiencing millions-billions of electrons being transferred over thousands or tens of thousands of volts, and they're only doing that because the recipient object (be it you or the metal railing) has a positive charge, meaning there are places for those electrons to settle once they get there.

When a beta particle is emitted, it comes with an energy in the range of millions of volts, and there's no predesignated spot for it to settle, meaning it will fly straight through things until it happens to find a spot to settle, often by displacing some other electron. This is what makes it ionizing radiation.",null,1,cdm357l,1rd953,askscience,new,4
owaisofspades,"Your thyroid hormone is responsible for regulating the metabolic rate of most of your body. When you have thyroid insufficiency, your metabolic rate drops and your body no longer functions at full effectiveness. The concentration problems are likely a secondary effect of the lethargy and weakness that are caused by hypothyroidism",null,0,cdmb0lv,1rdckt,askscience,new,2
s3c7i0n,"As a basic reply, dogs, like cats, have a reflective coating at the back of their eye, which helps them see in low light situations. The color of the coating is based on the color of the eye, which has some evolutionary benefits having to do with common colors in various environments, but the gist of It is that the colors are caused by the iris colors. 

(edit) the blue eye is red due to a lack of pigment in the reflective layer, so you're actually seeing the reflection of the blood vessels in her retina. ",null,1,cdm3dji,1rdd4v,askscience,new,3
Ejb90,"This revolves around the maximum power transfer theorem. There are two ways to look at it.
Firstly from a circuit-theory point of view, when power energy is transferred from one component to another, the maximum is transferred if they are the same resistance. Impedance is the more generalised, complex form of resistance.  This means that if they are matched in resistance (impedance) then the most power is transferred, which is most efficient.
The second way to look at isn't is from a waves point of view. At the frequencies you get in a transfer cable the currents can be modelled as electromagnetic pulses. When they reach a boundary some are reflected and some are transmitted, just like light is when it passes between air and water. When the two mediums either side of the boundary have the same ""resistance"" to the wave, more of the wave propagates through, as it's almost as if there is no boundary, so the maximum energy is transferred, as expected.",null,1,cdm4hjj,1rdd6o,askscience,new,6
SwedishBoatlover,"You should *really* watch this [video](http://youtu.be/DovunOxlY1k) from Bell labs, where the host use a wave machine to visually show how waves work. You can actually get an intuitive feel for what the impedence matching does, it's very interesting!",null,0,cdm9s27,1rdd6o,askscience,new,3
rat_poison,"This is the distilled wisdom of my Microwave Networks experience regarding impedance.

The most important defining feature of the transmission line is its characteristic impedance. It is affected by the shape of the transmission line and materials that make it up. Generally, for a TL extending to the z direction, we can divide the whole length in small parts of length Δz. Τhose parts can be arbitrarily small: so much so that we can ignore any radiating properties within that Δz. We can therefore make a lumped-circuit equivalent of that Δz length of the tramsission line.
Movement along the length of the transmission line will mean some ohmic resistance (R) and some inductance (L). The neighboring of metal surfaces will cause capacitance (C) and the material between them will cause dielectric losses based on a conductance (G).

As the electromagnetic wave travels through the transmission line along direction of propagation J, we can generally define functions I(z) = I+(z) + I-(z) and V(z) = V+(z) + V-(z)

So current and voltage are made up of two constituents: the first (+) representing movement along the direction of propagation and the second (-) representing the part of the current and voltage that are caused by reflection and therefore are moving opposite the direction of propagation.

Characteristic Impedance is the ratio Z_0(z) = V+(z) / I+(z)

for the length of Δz I have described earlier, it is calculated as Z_0 = sqrt((R+jωL)/G+jωC)) (j = sqrt(-1))

In most cases, trasmission lines are uniform in the z direction, or they are made up of a cascade of uniform parts. Either way, for every uniform TL, the characteristic impedance is the same no matter which Δz I choose within it (as long as it's small), so that's why it's such a defining property of a TL.

We can then define another quantity, Γ(z)=V-(z)/V+(z), called the reflection coefficient. This tells us what is the ratio of reflected and forward waves. Its amplitude is 0 if there are no reflections, 1 if the reflections and the forward waves have the same energy therefore leading to standing waves not able to propagate energy, or an-inbetween state for the other values in between.

If z=l, then we have calculated that Γ_Load = (Ζ_L-Z_0)/(Ζ_L+Z_0). For a lossless transmission line, this will have constant amplitude throughout its length.

Now we want to minimize energy lost in reflections. So Γ should be 0.

If you look closely at my last equation, you'll see that this can only be true if Z_L = Z_0.

Regarding the part about the return line.

When dealing with high frequency circuits, a return line is not necessary. BUT NOT because of impedance matching. If the outer shell or the inner wire of a coaxial cable didn't exist, it wouldn't posses the geometrical properties that induce the field to operate in the desired way. There wouldn't be two points with different potentials along the direction of propagation around which the EM energy could oscillate back and forth. In fact the concepts I have just described break down. BUT there are transmission lines that don't have a return wire: these are waveguides. The wave DOES oscillate back and forth, but the points are not as strictly defined as in two-wire TL's or coaxial cables. Instead, we have modes: depending on the ratio of the wavelength and the dimensions of the waveguide, there are (possibly several) nulls and peaks at the transversal plane, that are defined by how many half-waves fit into that dimension. These nulls and peaks are now the places around which the energy fluctuates in order to propagate forward. Waveguides are the reason you should be careful when using current and voltage concepts on microwave circuits. Therefore you should just stop thinking about TLs in terms of a phase line and a return line, but a single structure which guides the waves along a direction and (maybe) causes reflections along the opposite direction.
",null,0,cdmbm4q,1rdd6o,askscience,new,2
selfification,"http://www.youtube.com/watch?v=DovunOxlY1k

This is a classic that explains all phenomena.  Standing waves, interference, impedance matching, refraction, reflection...  everything.  All in one video.",null,0,cdmcdgz,1rdd6o,askscience,new,2
ece_option_chair,"By the way, an interesting historical fact is that the first use of transmission lines (and impedance matching) was with telegraph cables where intersymbol interference occurred and the solution was to INCREASE inductive loading (normally one thinks of reducing inductive loading if the bandwidth is not high enough).  [LINK](https://en.wikipedia.org/wiki/Heaviside_condition).  Heaviside doesn't get enough credit for all the things he invented/re-invented/simplified.",null,0,cdpbbbq,1rdd6o,askscience,new,1
fastparticles,The event would melt most of Earth and put the upper mantle into orbit around Earth. At this point the moon is thought to come from Earth because they are so isotopically similar. The compositions of the moon and Earth do differ especially in terms of volatile elements (the moon for example is relatively depleted in potassium). ,null,0,cdm6y13,1rdhkw,askscience,new,2
ProfEntropy,"Postmortem fluid and tissue toxicology is able to quantify both the drugs and alcohol present at the time the samples were taken.

Connecting that back to the amounts present at the time of death can sometimes be difficult. For example, many drugs are known to partition into different parts of the body after death. Knowledge of this, and sampling tissues and fluids from the proper place will help get more accurate measurements.

Many other factors must be considered when looking at ethanol concentration. See [this article](http://www.sciencedirect.com/science/article/pii/S0379073806002891) for a good review of postmortem alcohol concentrations and how they relate to BAC at time of death.",null,2,cdm82ym,1rdosg,askscience,new,9
Smoothened,"The machinery behind X-inactivation specifically targets the X chromosome as opposed to any chromosome. For example, the gene XIST is located on the X chromosome and is required for its inactivation. When this gene is expressed, its transcript, a long noncoding RNA (Xist) coats the respective chromosome, becoming involved in its silencing. A chromosome lacking XIST would not undergo inactivation. If you insert the XIST gene in an autosomal chromosome, that chromosome can then be inactivated. 

A more interesting question is how is inactivation targeted to one of the chromosomes in each cell. That question is not entirely answered, but it is believed that an autosomal gene encodes a blocking factor that prevents one X chromosomes from being inactivated. Interestingly, even when there's more than 2 X-chromosomes present, only one remains active in each cell. ",null,0,cdmbqmg,1rdsv9,askscience,new,5
Platypuskeeper,"Sea salt is from evaporating seawater, table salt either comes from the sea or from salt mines. 

When you say ""table salt"" you're referring to one single compound: Sodium chloride. The vast bulk of the sea salt, and virtually all of what's 'table salt' is sodium chloride. Sea salt has some other salts in it, how much and what depends on where it's from. Table salt is often [iodized](http://en.wikipedia.org/wiki/Iodised_salt), meaning they've added some iodine salts as a dietary supplement. (Lack of iodine causes developmental disorders and thyroid problems) Depending on the salt it might have small amounts of stuff to avoid it caking together too, which aren't usually added to the stuff marketed as 'sea salt'. (I don't believe there's any _requirements_ on this though)

So sea salt has some other minerals in it, but it's such a small part and you use so little salt, that it probably doesn't make a significant impact on your overall mineral intake. The iodization of salt has had a measurable impact on iodine deficiency-related stuff since it was introduced in the 1920s. For the individual any health effects would depend on whether you get enough iodine from other sources. 

The biggest differences are really taste and texture more than anything, though.
",null,2,cdm93o8,1rdu42,askscience,new,6
225274,"Sea salt is the salt produced by evaporating sea water. Table salt is the same thing, just crushed into a fine powder, with fewer impurities of other salts like KCl, and is often iodized, i.e. has added iodine salts. 

Table salt is healthier as iodine is not so commonly available in our diet, but is an essential mineral. 

",null,2,cdm8et4,1rdu42,askscience,new,4
chuck10470,"The difference between table and sea salt is the iodine. Fresh from the factory it contains 50 ppm iodine. That's all. 50 ppm. As it ages, the iodine evaporates out, losing half every 40 days.

All salt comes from the sea. Or a lake. Mined salt has precipitated out over thousands of years and built up thick beds that became buried through mountain building. Ironically, most of these mines are today quite some distance from the ocean. The Swiss city of Salzburg has several salt mines,  though it is hundreds of miles from the ocean today. 

And some of this mined salt is quite old. But whether it precipitated out at the bottom of the Tethys Sea 65 million years ago or last week in Sardinia, it's still 99.9% NaCl and 0.1% other minerals. Most of the table salt, industrial salt, road salt, animal feed salt, etc, is mined. Sea salt is precipitated out in huge evaporation ponds. It should be noted that sea salt can be iodized and become table salt, and much of it is. ""Sea salt"" is a marketing name given to un-iodized salt produced by evaporation. It supposedly has better taste, but since it's nearly impossible to determine which minerals give it a specific favor profile, maybe it does, maybe it doesn't. It depends on where it's from. The expensive $10/lb culinary salt is usually sea salt, but with additives like smoke or truffles. The various types of salt available at the grocery store differ mostly in the iodine content and the shape of the crystals. That's it. You can use regular iodized table salt for nearly every application you have,  except for canning. The iodine turns some stuff brown.",null,0,cdmf52z,1rdu42,askscience,new,2
Truck43,"The lighter works because the butane is a liquid under pressure, opening the valve lets it spray out and be ignited by the flint. When it's very cold, it contracts, reducing the pressure in the fuel vessel, and it's less volatile, this reduces the amount of fuel that is expelled.  ",null,3,cdm9hep,1rduf4,askscience,new,8
adlermann,butane's(what bic lighters use for fuel) vapor pressure drops to near zero at atmospheric pressure about 40F not enough gas is released to fuel a flame.  That is why natural gas and propane are used for heating despite butane's higher energy potential,null,2,cdm9ieg,1rduf4,askscience,new,4
Platypuskeeper,"&gt; I'm assuming the lighter fluid has less energy therefore it's lazy.

That's one way of putting. A more formal but roughly equivalent way would be to say that the pressure over the liquid butane in it, is lower when at a lower temperature. The equilibrium is shifted towards more liquid and lower pressure at lower temperatures, higher pressure and less liquid at higher temperatures. 
",null,2,cdmae2f,1rduf4,askscience,new,4
cass314,"In both beer and soda, the bubbles are caused by carbon dioxide coming out of solution.  The big difference is what's there to ""catch"" the bubbles and hold them.  In soda, there's not much at all.  In beer, there are proteins.

Soda is mostly water, sugar, salt, and acid.  There's not a lot to give structure, so the bubbles die out quickly, and after a few minutes you can hardly tell there was ever any foam.  Beer, however, has proteins leftover from both the mash (wheat or barley, usually) and the yeasts that did the fermenting, and it's the proteins that give beer such an interesting head. Proteins, especially hydrophobic proteins (they ""like"" oil better than water) and denatured proteins with their inner hydrophobic parts exposed, tend to clump together into structures (many to avoid interacting with water).  These structures can trap air bubbles.  

You can think of it like a less extreme example of whisking sugar water vs. whisking sugar and egg whites.  If you whisk or shake water, you'll get bubbles, but they'll pop very quickly after you stop.  If you whisk egg whites long enough, you'll get meringue.  ",null,0,cdmczfs,1rdy9r,askscience,new,3
fishify,"The energy of the initial and final states in beta decay, as in other processes, have the same energy. The W boson that appeas as an intermediate particle in the standard desecription of the process is a so-called virtual particle. In particle physics, our calculational scheme known as perturbation theory tells us that we can calculate what happens using intermediate states known as virtual particles which have the energy, momentum, and other conserved quantities you'd expect; but this also means they have the 'wrong' mass.  We say they are *off mass-shell*.

These virtual particle that appear in calculations are never actually observed. Any W boson you actually detect will have the expected mass of 80.4 GeV/c^(2), or just under 86 proton masses.",null,1,cdmfttr,1re0d7,askscience,new,3
fishify,"Depending on your background, this article might be helpful to you:

""The Pumping of a Swing from the Standing Position."" William B. Case, American Journal of Physics, 64, 215 (1996).


",null,1,cdmg0m4,1re182,askscience,new,3
Shitler,"As I understand it, motion happens because the swinger shifts their center of gravity, causing gravity to have to recenter the pendulum. However, as is in the nature of pendulums, gravity overdoes it and the swinger ends up on the other side of equilibrium, at which point they shift their center of gravity again. And so on.

Energy is introduced into the pendulum when the swinger shifts their center of gravity by extending or contracting their legs.",null,1,cdmgk9l,1re182,askscience,new,3
jofwu,"I'm just going to describe the process...

When swinging forward you lean back, stick your legs out, and pull on the chains. *By leaning your torso back and kicking your legs out you apply torque to your body.* This torque is balanced by pulling on the chains. Imagine trying to perform this action without holding on to or pushing off of something- you can't. Note that the chains bend where you hold them. The line of action of the tension in the chains is *behind* your center of mass. This is where the balance in torque comes from: force (tension in chains) x distance (between force's line of action and your center of mass). On the backswing, everything is the opposite. You pull your torso forward and bend your knees back in, and to balance this out you need a torque in the opposite direction. So you *push* forward on the chains, and the line of action of the chains is *in front* of your center of mass.

*Making these transitions leading up to the peak of your swing is the key.* The movements don't do anything if they aren't timed right. By performing the forward swinging motions, you add some gravitational potential energy at the top of the front of your swing. The back swinging motions add energy at the back end of the swing. *The energy gained is thanks to that little distance you create between you and the chains' line of action- putting you a little bit higher from the ground than if you had just swung freely like a pendulum.* Of course this gravitational potential energy results in more speed/momentum at the bottom. *And I think it's worth mentioning that you don't conjure this extra energy from nowhere. It comes from you body.* The gravitational potential energy you add wouldn't be possible without applying a torque to your body. 

In the end, it's not that much different from swinging on parallel bars. Rather than balancing your torque by pushing/pulling on a chain, you apply a counter torque directly to the bar you hold (with a firm grip).",null,0,cdmmc2l,1re182,askscience,new,2
GlowInTheDarkDonkey,"My understanding, as a (uh oh) layman, is that a person on a swing is basically taking advantage of angular momentum in the same way a figure skater tightening their limbs in a spin makes them spin faster.

A shortening of the total length of the swinging body on the upward swing means gravity is being applied to a total body that has less distance to travel (is a shorter swing-arm), and then on the downward swing the thrusting of legs outwards allows gravity to work on a longer swinging body... which again is then shortened on the upward movement.

Some of the angular momentum of the legs themselves also adds to the total forces being shifted around.

When someone is standing on a swing seat you'll notice they put all of their mass to the seat on the down-swing, and then they stand on the upswing.  This, similarly, means gravity is pulling a longer swing-arm (in terms of average mass distribution towards the outermost edge of the arm) on the downward stroke compared to the upward stroke.

I'm curious if someone in a white-coat finds this answer agreeable or not.",null,6,cdmdqxr,1re182,askscience,new,5
eliareyouserious,"A presynaptic (fibre) volley can be observed in extracellular field potential recordings. It is caused by activation of (several) presynaptic fibres (usually using a stimulation electrode), which in turn fire and activate their postsynaptic partner. A brief negative potential preceding EPSPs is indicative of presynaptic action potential(s) and is termed the ""presynaptic volley"". Fig.2C on page 92 in this book indicates the volley in a recording: http://books.google.ch/books?id=y_ucmaDffXsC&amp;dq=presynaptic+volley&amp;hl=de&amp;source=gbs_navlinks_s (The link to the book chapter also serves as reference here). ",null,0,cdnryhk,1re1qb,askscience,new,1
Criticalist,"Blood welling out of the mouth can either be coming from the stomach or digestive tract, in which case it is called haematemesis (vomiting blood), or from the lungs and respiratory tract, when it is termed haemoptosis (coughing blood). Another alternative is that the bleeding is from a structure inside the mouth, such as the tongue. So generally speaking, trauma to the abdomen may cause haemtaemesis, while trauma to the chest would be more likely to cause haemoptosis.

Its pretty unusual for an abdominal wound to cause a large amount of haematemesis, as an injury that damages a blood vessel inside the abdomen will cause the bleeding into the abdominal cavity, but not into the digestive tract itself. So, one might see a distended, tense abdomen, and a low blood pressure, but unless there was also a hole in the stomach or intestine, there may well be no bleeding from the mouth.

In contrast, damage to the lungs is much more likely to cause haemoptosis, as the lungs are full of blood vessels, and its very easy for blood to leak into the airways, and so be coughed up. A wound to one of the major pulmonary blood vessels can lead to massive, torrential bleeding from the mouth and can be very difficult to treat.",null,39,cdmh92b,1re305,askscience,new,244
meltingdiamond,"It is possible to bleed from the mouth if, for example, the wound caused a punctured lung. How close a fictional depiction is to reality really depends o0n what you are watching. An example of getting it right, according to an EMT friend, is the death of Miles Dyson in Terminator 2.",null,17,cdmgqdz,1re305,askscience,new,45
null,null,null,13,cdmjqhe,1re305,askscience,new,33
DieSchadenfreude,"Ugh, thank you for asking this question! It drives me nuts when people bleed out of the mouth from every stomach wound in movies. The stomach actually sits pretty high in the rib cage, so an injury would have to be pretty high to fill the stomach with blood enough to either cause vomiting or force blood up. A major artery would also have to be hit to have blood come up aggressively I would think. There are so many sphincters between intestines and mouth I don't think it's very likely a low injury would bring blood up. That and if you get hit in the lungs and cough up blood, it isn't all pretty and romantic-y like in the movies, it's frothy. A person coughing blood from injured lungs or trachea would be struggling to breathe, probably making weird noises, and have red foam coming up. ",null,8,cdmm74p,1re305,askscience,new,14
Cyno01,"It can happen, but not usually. The reason people tend to bleed from their mouths when critically injured in movies and television is because while it takes some effort to simulate a realistic wound, a blood capsule in the mouth is quite easy. A hole in a shirt with blood coming out of and some leaking from the characters mouth are simple enough visual cues to the audience without being overly graphic. ",null,0,cdn4lbj,1re305,askscience,new,2
pretendtrain,"During the Iranian riots following the ""electing"" of Ahmadinejad a couple of years ago, a video of a young woman being shot by the military was posted on YouTube. I saw the video, and you see blood coming out of her mouth as she dies. 

It is a terrible sight, but it was verified as real. So, for whatever reason, it does seem that it will happen. At least sometimes. ",null,11,cdmlgbm,1re305,askscience,new,12
jakin20,"I think we are all forgetting about Disseminated Intravascular Clotting (DIC). Basically what this is, is when the body's clotting factors and components are so used up the blood is thinned to a point that it starts to literally seep through the veins. causing bleeding from orifaces and ""purpura"".",null,9,cdmmbb3,1re305,askscience,new,11
mzyos,"So most of this is fiction, and it would be unlikely that most deaths via gunshot, or stab wound could cause this. However, there are two major possibilities; either the pulmonary artery, vein, or aorta get damaged at the same time as the trachea (wind pipe). As all these vessels are close (relatively) to the trachea or its offshoots (bronchi) then a connection may form, passing high pressure blood from the heart/lungs to the wind pipe, where it is coughed up. 

  Or, the other possibility is that the aorta and oesophagus are both damaged and the ""very high pressure"" blood from the aorta passes straight in to the oesophagus and is pushed up in to the mouth. 

  Both of these are still relatively unlikely, but I'm sure it could happen. As for DIC, that takes a while to develop, and is very unlikely to cause this immediately after a shot, or stab wound.",null,0,cdojleg,1re305,askscience,new,1
Platypuskeeper,"The electromagnetic field. It's everywhere.

Somebody is inevitably going to chime in here with virtual particles and whatnot, which are quantum-level descriptions of _how the field works_. But at the end of the day, the 'medium' is the same: Space itself.
",null,1,cdmdzfk,1re5f5,askscience,new,11
fishify,"Not every wave needs a medium other than the vacuum in which to travel. Nineteenth century physicists did not recognize this, and thus postulated that the universe was filled with a substance they called *the ether*, which would serve as the medium for light waves.

Einstein in 1905 showed there was no need for an ether. As we understand it today, light travels through space just as an electron does. One way to picture this is to remember that light is made of photons (particle of light), which readily travel through space and which form electromagnetic fields and waves.",null,1,cdmfxew,1re5f5,askscience,new,10
killer_alien,"Light is an electromagnetic wave, and therefore does not require a medim to propagate through. On the other hand, waves that need a medium are mechanical waves. Theses include longitidinal, transverse and torsional waves. e.g. sound waves are longitudinal waves",null,0,cdmi1mi,1re5f5,askscience,new,2
animationb,"When a field gets enough energy, it ""manifests"" as some fundamental particle. For the electromagnetic wave, energy creates a photon. In sort of the same way matter helps ""create"" (or comes with) a gluon, the fundamental particle for gravity.",null,2,cdmng0x,1re5f5,askscience,new,1
KarlOskar12,"That depends what you mean...The major regulators of the cell cycle are [p53 and p27](http://puu.sh/5sB2h.jpg). They both halt the cell cycle, p27 specifically does it by binding to and blocking the action of cyclin and CDK preventing the cell from entering the S phase of the cell cycle (DNA replication phase). Once the cell cycle is halted, the cell is either repaired (let's say for DNA damage). If repair is not possible or too costly, the cell is told to undergo apoptosis (kill itself). This is done by activating [Caspase 3](http://en.wikipedia.org/wiki/Caspase_3) which systematically breaks down the cell by expelling all the water, chopping the DNA up in an orderly manner, degrading the nuclear membrane, degrading the golgi apparatus, blebbing the cytoplasmic membrane, etc.",null,0,cdmen5i,1re5ig,askscience,new,2
StringOfLights,"It is not so much that terrestrial mammals were big back then, it's that they're small now. Mammals [increased in size following the Cretaceous-Paleogene extinction and maintained that large body size](http://www.sciencemag.org/content/330/6008/1216.short) for nearly 30 million years years.  Then there was an [extinction at the end of the Pleistocene](http://onlinelibrary.wiley.com/doi/10.1111/j.1469-185X.1991.tb01149.x/abstract). Most vertebrate taxa made it through this extinction, but a lot of large-bodied animals, and especially large-bodied mammals, were hit particularly hard. Some 150 genera of megafauna (defined as animals &gt;44 kg) existed 50,000 years ago; [97 of those were extinct by 10,000 years ago](http://www.sciencemag.org/content/306/5693/70.full):

Given how geologically recent these extinctions are, it's extremely unlikely that anything would have been able to fill the gaps left by the loss of megafaunal mammals, as there appears to be a [maximum rate](http://pnas.org/content/early/2012/01/26/1120774109.abstract) that mammals can increase in size. In that sense it's completely expected that a recent extinction event would leave a gap in body size. 

Also, in all of this discussion it's worth bearing in mind that we're generally talking about terrestrial mammals. There are plenty of large marine mammals still around (for the time being), including the blue whale!

**Edit:** Forgot something! In terms of dealing with cold weather, having a larger body size actually slows heat loss because it lowers the surface area to volume ratio. So while larger mammals had to eat more overall, they [spend less energy per unit of body mass](http://www.planta.cn/forum/files_planta/511_131.pdf) producing heat. This was the original logic behind [Bergmann's Rule](http://en.wikipedia.org/wiki/Bergmann%27s_rule).

",null,3,cdmdkaq,1re5lq,askscience,new,9
masiakasaurus,"StringOfLights hit the most important points but I'd like to stress that every animal is a different case and the more you look into a particular species you'll see different or additional reasons for the size they had. While an old source, 1968 Björn Kurtén's *Pleistocene Mammals of Europe* (and I guess its companion New World book, *Pleistocene Mammals of North America* by the same author, though I have not read it) makes a good recap on ice age mammals. Let's see some of the animals you cited.

First of all, **mammoths**. There were different species of mammoths, and they have been cursed with being described as gigantic in popular literature, but they were really not that big. The biggest species was the Eurasian steppe mammoth *Mammuthus trogontherii* (followed by *Mammuthus columbi*, its North American descendant) whose males topped at 4'5 meters (14'5+ feet) tall. The biggest African elephants are about 4 meters tall or a little more. *M. trogontherii* and *M. columbi* didn't live in the tundra, however, but in temperate grasslands to the south. What's more, *M. trogontherii* wasn't an 'ice' animal proper, as it was common during the Cromerian or Gunz-Mindel interglatial 780,000-450,000 years ago, a *warm* period between glatiations. 

*M. trogontherii*'s (Eurasian) descendant, the more famous woolly mammoth *M. primigenius* was adapted to the glatial steppe-tundra and thrived during the cold periods. It was a *smaller* animal than its ancestor, no bigger than modern elephants. Kurtén gives a height at the shoulder for the Late Pleistocene mammoths, who lived through one of the coldest periods of the Pleistocene, as 10 feet or less. There is no doubt, however, that the woolly mammoth *was* the biggest animal of the tundra, but it wasn't the giant mountain it is made out to be in popular imagination, and it is plausible that its *decrease* in size from its predecessor is a result of living in a poorer environment with less food.

**Dire wolves**, despite popular image as well, **were no bigger than grey wolves either**. This confusion stems from the fact that dire wolf jaws and teeth were larger and more adapted to bone-crushing than grey wolves, making them ecologically analogous to Old World hyenas. Indeed, while hyenas inhabited Eurasia and Africa during the Pleistocene (and coexisted with wolves), they never reached America, so the direwolves occupied their niche here, and in fact coexisted with grey wolves who looked after different prey than them. The grey wolf of Late Pleistocene Europe is slightly bigger than modern European wolves, but then again, modern wolves in North America are *also* on average larger than Europe's.

**Giant Sloths** are rather weird as they have no living equivalents and I'm not as familiar with them, but once again the biggest ones ever don't come from colder areas. Wikipedia (yes, I know) has *Eremotherium* (Georgia, Texas, Mexico) and *Megatherium* (subtropical South America) as 6 meters long. The ones that lived in colder areas were smaller: *Mylodon* (Patagonia) and *Megalonyx* (Central America to Alaska and Yukon) peak both at 3 meters, half the lenght of the others.

**Cave Bears** were not particularly related to brown bears (if anything they were closer to black bears) and coexisted briefly with brown bears who originated in Asia (cave bears are exclusive to Europe). Cave bears were mainly vegetarians and didn't live in tundra either, but in temperate forests, and became extinct as the continent became colder about 30,000 years ago (almost like the neanderthals). I don't have size data right now but from what I recall they aren't that big compared to brown or polar bears (someone correct me if I'm wrong please) and once again this seems to be a confusion born of the fact that they are more robust and massive looking. Brown bears in cold periods of the Pleistocene are on average bigger than brown bears in warm periods including modern brown bears, however.

**Cave lions** were, indeed, on average bigger than modern lions, but still the same species. While cave lions lived in the Pleistocene in Eurasia and Alaska modern looking lions (who are not descendants of the former according to DNA) were in Africa and probably the Middle East. The *American* cave lion was bigger than both but its taxonomy is in discussion and could have been a whole different beast altogether (hehe), more related to the jaguar. Like mammoths (and unlike wolves and bears), however, Eurasian cave lions are biggest in the Cromerian and become their smallest in colder periods, especially near the end of the Pleistocene. Some European lions get so small that it has even been suggested that the cave lion became extinct before it is commonly assumed to, and was replaced by modern lions from the Middle East belonging to or closely related to the modern Indian subspecies (one of the smallest living lions by the way). On the issue of cats, leopards (which lived in Europe through the Pleistocene) and cheetahs are also bigger in warm periods and get smaller in colder periods, while lynxes and wildcats do the opposite. I'm not sure why. I have to say, though, that the European wildcat of Late Pleistocene Europe is similar in size to the modern wildcats of southern Spain, which are bigger than other wildcats in the continent, yet are more southernly distributed than them. Confusing, eh?

**Aurochs** were, like cave bears, temperate animals that didn't live in the tundra-steppe, where they were replaced by bison, and are most common in interglatials. Wild aurochs don't seem to have been any smaller than they were in the Pleistocene: Julius Caesar said that aurochs were the size of *elephants*! Mind you, this is the extinct North African Elephant which was smaller than an Asian elephant and closer in size to a very big horse, but it gives an idea of how massive they were compared to its living descendant, domestic cattle. That's a key as to why they became smaller: domestication. Humans selected the smaller, better manageable aurochs and this is why the species decreased in size, with no relation to climate. Had cattle not been domesticated (or aurochs not been hunted to extinction, just like we have wolves and dogs living side by side today), we'd have aurochs bulls of 1'8 meters at the shoulders. The biggest living bovine, the gaur *Bos gaurus* of southeast Asia (not a descendant of the aurochs) [can surpass 2 meters](http://www.ultimateungulate.com/artiodactyla/bos_frontalis.html).

So it seems that in a lot of cases, your hunch is actually right and lack of food and other factors can trump Bergmann's Rule.",null,0,cdp8p80,1re5lq,askscience,new,2
stevenstevenstevenst,"One way it is possible to determine age of a material vs. when a tool of that material was crafted is to compare the age of the material (easily determined by any number of techniques, such a radiocarbon dating and other isotopic methods) and to compare the quantity of atmospheric carbon adsorbed to the surface of the tool.  Quantity of adsorbed surface carbon (also known as adventitious carbon) is proportional to the amount of time the surface has been exposed to atmospheric conditions, and thus a comparison of adventitious carbon quantity of a surface known to have been exposed in the manufacture of the instrument and the isotopically-determined age of the material is informative.  

Other techniques are possible, but various analysis of oxidation, carbon adsorption, or other surface chemical phenomena are generally utilized.",null,0,cdmdtae,1re8mg,askscience,new,2
jessickofya,"To date when the tool was used we would look at residue on the tool and date that. So for example, if we found a stone tool with blood we can use dating techniques to get a estimation on when the tool was used. Depending on what material you are dating - you would use one of many different techniques.

There are also ways to break down rock into a gas and estimate the age of formation. Archaeology is all about context too. If we found the tool with a hearth or camp we could look at dating other items and estimate the age of when the artifacts were used based on the dates of surrounding artifacts in the same area. We can even use tree rings, dendrochronology,  to estimate the age of the wood used in the site and assume the age would be similar",null,0,cdmdxqu,1re8mg,askscience,new,2
Pachacamac,"Someone else just asked a pretty similar question and I saw theirs first, so I answered it first, and left a pretty detailed response. [You should probably just take a look at it.](http://www.reddit.com/r/askscience/comments/1rectr/how_do_scientistsarchaeologists_carbondate_human/cdmfnvd)

Basically, with most types of stone we can't date the stone at all (so we don't know how old it is, expect by talking to geologists who tell us that it comes from a certain formation of a certain age. But we don't typically care about that). We figure out when the tool was made by assuming that it was made, used, and discarded within a relatively short period of time (a century can be ""short"" to us because of the error ranges that all the different dating methods have, but stone tools wear out and break quickly so anything was probably used and tossed away in the same year that it was made). So because it was discarded at a site, we assume that it is as old as the site itself, and we date the tool by association; i.e. it was found with other things that we can date directly (like charcoal on younger sites, or layer of volcanic ash for sites as old as the one in this article), so we assume that it is as old as those things. So the fact that the rock itself might be 400 million years old doesn't matter; we find a tool at a site that we can date to 280,000 years ago and we assume that the tool and the site are the same age, as long as there is no evidence to suggest otherwise.

Now, I said with most stone. Obsidian is different. There's a method called obsidian hydration dating that we can actually use to date obsidian tools, which are what was found at the site in the article. When you make a stone tool you are always chipping away and breaking the surface, so when the tool is brand new it will have a fresh surface. Obsidian weathers at a known rate so you can look at the surface and determine how old it is by how much weathering is on it. This isn't a perfect method and it can't really tell us exactly how old the tool is (because there's so much variation across regions), but it can tell you that one tool is older than the other. Maybe they can get actual calendar years for Ethiopian obsidian too, I don't know (I'm not familiar with the area).",null,0,cdmfyqt,1re8mg,askscience,new,2
humanino,"You can access the article here :  
[Thermoelectrically Pumped Light-Emitting Diodes Operating above Unity Efficiency (pdf)](http://dspace.mit.edu/openaccess-disseminate/1721.1/71563)

Please note that they have not broken any thermodynamical law. They have a device which uses electrical power, and converts this power into heat and light. The power emitted in the form of light is larger than than the part of the electrical power directly used to create light. That is because the other part of the electrical power, which created heat, has also been re-converted into more light. That is really neat and clever, and it does have potential applications, but the ""communication"" part might have been misleading. ",null,0,cdnyxu0,1reb7j,askscience,new,2
iorgfeflkd,"Yeah, for example a red dwarf orbiting a much brighter star. When the dwarf is transiting, there will be less total light coming from the system.

[Here](http://arxiv.org/pdf/1109.2055.pdf) is a paper where they tried to measure this loss of light from a red dwarf orbiting another star.",null,1,cdmfm8v,1reb7p,askscience,new,7
HV250,"You seem to be confusing voltage with current. Voltage is just the potential difference required for current to flow. How much current actually flows is what determines whether you have enough for all components. As the current is consumed, the voltage slowly dwindles over time, till a point where the potential difference is simply not sufficient to let the charges move. That's when you need to charge it.",null,0,cdmh1b3,1rebi1,askscience,new,10
kizzap,"There are a number of things that could be happening. 

First, it would be most likely be connected in *parallel* not in series, thus the processor will be getting the 3.7V as well. LEDs take such small current too that a single LED will run for quite some time off that battery.

Secondly, it is quite possible that there is a switching power supply in the controller, which changes a lot of things.

Third, not all LEDs are 3 volts... ",null,0,cdmicso,1rebi1,askscience,new,3
acidburnzdeleted,"Diesel needs a higher compression ratio in order to burn, compared to gasoline, meaning the engine block has to withstand far greater forces. Diesel engine blocks are usually built out of cast iron, which is a LOT heavier than the aluminium most gasoline engine blocks are built from. 
A heavier engine means a heavier car, and since most cars have the engine in the front, this would translate to hideous understeer, the more heavier the big lump in front of your car gets.
You can read more about these basic principles of automotive movement if you're not familiar with them already.
http://en.wikipedia.org/wiki/Understeer_and_oversteer
Purists would say the best sports car, ( if the weather and road conditions are ideal ) would have to be mid-engined, rear wheel drive, and naturally aspirated, even though the latter is debatable.",null,1,cdmhxbu,1rebxm,askscience,new,22
awdsns,"[Actually they have been used with great success in race cars](https://en.wikipedia.org/wiki/Diesel_automobile_racing) against Gasoline powered cars, most notably by Audi in Le Mans: [R10](https://en.wikipedia.org/wiki/Audi_R10_TDI) [R15](https://en.wikipedia.org/wiki/Audi_R15_TDI).

But I guess the other posters have already given good reasons why you don't see them much in commercial sports cars.
",null,9,cdmigug,1rebxm,askscience,new,23
TestarossaAutodrive,"Audi developed a successful diesel Le Mans car, and I have heard rumors of a TDI R8.

http://en.wikipedia.org/wiki/Audi_R10_TDI

http://en.wikipedia.org/wiki/Audi_R15_TDI

http://en.wikipedia.org/wiki/Audi_R18

http://www.autoblog.com/2008/01/13/detroit-2008-audi-unleashes-its-diesel-monster-the-r8-v12-tdi/
",null,0,cdmgh3g,1rebxm,askscience,new,11
twelveparsex,"Diesel engines don't rev high like gasoline engines do, they create lots of torque but relatively low horsepower, great for towing things but not necessarily for high acceleration; after a brief moment of high acceleration the engine begins to make less and less torque.  I believe this is due to flame propagation of diesel fuel vs gasoline...any chemist feel free to chime in",null,2,cdmi9z6,1rebxm,askscience,new,9
FW190,"Audi is using diesel engines in their le Mans wining prototype cars. They have become superior to petrol powered cars and are given more and more restrictions each year to get them in line with rest of the grid. Peugeot also won with diesel powered car in 2009. 

http://en.wikipedia.org/wiki/Audi_R18",null,0,cdmiy00,1rebxm,askscience,new,5
Oderdigg,"Lots of good answers already but I thought I'd mention that Mazda just won the Grand AM with a diesel.

http://www.grand-am.com/News/GA_News/tabid/141/Article/53994/mazda6-becomes-first-diesel-to-win-at-indianapolis-motor-speedway.aspx

http://www.youtube.com/watch?v=HbCLdWOHJBs

2.2L twin turbo diesel, 400BHP, 440FT/LBS TQ.",null,0,cdmp17m,1rebxm,askscience,new,2
Buy-theticket,"Never made it to production but there was a v12 diesel r8 a few years back at the car shows. 

Looks like there are rumors about it coming back again as a new model with a diesel/electric hybrid drive train: http://www.autoguide.com/auto-news/2012/11/audi-r8-tdi-planned-as-diesel-supercar.html",null,0,cdmjtss,1rebxm,askscience,new,1
muchachoburacho,"The top two points here are right, but they also they also miss out on the fact that diesel engines typically provide power in large gulps rather than across a larger spectrum of the RPM's it will be operating at. http://en.wikipedia.org/wiki/Power_band",null,0,cdmkuqk,1rebxm,askscience,new,1
chocapix,"The gear ratio that maximizes torque at the wheel for a given car speed is the one that puts in the engine at peak power.
If what you're looking for is pure acceleration, engine torque figures are irrelevant, you want power. As already pointed out, diesel engines tend to have poor power-to-weight ratio, compared to gasoline engines.

But besides engineering issues, sports cars are not just about performance, a successful sports car needs to appeal to potential buyers.
People who like sports cars tend to dislike diesel engines for more subjective reasons like:

* they don't sound good

* they smell

* they make a lot of smoke at full throttle

",null,1,cdmmuyj,1rebxm,askscience,new,2
socercrze,"Something else that is significant is the ability to change RPM very quickly. Diesel burns more slowly than gasoline, so valve timing and compression are much different. Throttle response on gasoline is much much quicker, an F1 is gasoline with some sexy additives but it's throttle response from 1krpm to 15krpm is less then a second. A diesel going from idle to full rpm is much longer because of the large compression ratio needed to detonate the fuel. This large compression is what makes the high torque at lower rpm, which i love in my jetta TDI. ",null,0,cdms4k1,1rebxm,askscience,new,1
Platypuskeeper,"&gt; Hybridization is a generally good theory, but it doesn't explain properties like magnetism.

Valence-bond theory actually explains the paramagnetism of oxygen, if that's what you're referring to. (and has since the start, it's in Pauling's ""The nature of the chemical bond) It's a common myth though, so anyway...

You have antibonding orbitals because of symmetry. Each 'even' (symmetric) state has a corresponding 'odd' (antisymmetric) state. Now, I don't expect you to get what that means, so I'll demonstrate:

Two hydrogen atoms get close, and their atomic 1s orbitals combine to a _molecular orbital_. (The 1s orbitals are spherical and have a wave function that's like exp(-r), if you neglect constants). We assume for the sake of this example, that they form a linear 'superposition'. The combined wave function is simply the sum of the functions times some constants. 

There are only two possible combinations here: which is 1s_1 + 1s_2 and 1s_1 - 1s_2. This is because the overall phase (sign) of the function doesn't matter. so -1s_1 - 1_s2 is the same thing as 1s_1 + 1s_2. 

In the first one 1s_1 + 1s_2, where they add up, then the electron density is above zero everywhere, since the 1s orbital is exp(-1) and above zero everywhere. So there must be electron density all the way between the two nuclei. It's a _bonding_ molecular orbital.

In the second molecular orbital these two can create, 1s_1 - 1s_2, there is a spot at the exact center between the two nuclei where 1s_1 and 1s_2 are the same (because it's the same 1s orbital and the same distance r from their respective nucleus). So the total wave function there is _zero_. There's a region between the nuclei that lacks electrons! This is an _antibonding_ orbital.

[It's easier to see the thing visualized](http://www.expertsmind.com/CMSImages/2087_bonding1.png)

The antibonding MO has higher energy than the bonding one (fortunately for chemistry). A visual rationalization for this is in there's a higher curvature of the antibonding MO. After all, from one nucleus to the other it has to pass through zero. In quantum mechanics, a higher curvature of the wave function (more tightly located electrons), means higher kinetic energy. So the kinetic energy is higher when you have a 'node' like this (nodes being these areas of zero density, as with where a wave is zero). 

All this holds true whichever orbitals you combine to form your MOs. An antibonding orbital is formed for each bonding one, and the antibonding one has higher energy. 

(Note that the 'formation' here, just as with hybridization, is really just a way describing things. MOs don't suddenly form at a particular distance, it's a seamless transition from AOs to MOs)
",null,0,cdmet34,1recfy,askscience,new,7
Pachacamac,"Actually you can't carbon date stone at all. Carbon dating needs organic materials with carbon-14 in them (an unstable isotope of carbon), so we need floral or faunal material. Burned seeds or charcoal are the best, but other organic materials can be dated. There are other dating methods that can date non-organic things and can date much older things that radiocarbon dating (which maxes out at 75,000-100,000 years), and some of these are useful to archaeologists/paleo-anthropologists, but radiocarbon dating is the most common method that archaeologists use. 

I'll mention here that there is one method, obsidian hydration dating, that can actually determine how long it's been since a piece of obsidian (volcanic glass commonly used for stone tools) was broken, which happens when the tool is being made (basically you start with a larger rock and chip away at it to shape it into what you want), but this method has a lot of problems and isn't always reliable. It's about the only way to directly date stone tools that I can think of, though.

So, we can't date the actual. How do we determine how old something like a stone tool is? We rely on one of the key assumptions in archaeology that things found together were probably made and used at roughly the same time (radiocarbon dating has an error range of 25-100 years anyway, so ""same time"" can mean same decade or same century). If you find a stone tool within a fire pit, say, then you assume that someone threw it in there during a fire, and the fire pit will have lots of organic material that we can date, So we date the pit and assume that the tool is as old as the pit. That is the most straightforward example I can think of, but the basic idea, dating by association, is how we get specific calendar dates for most of our sites. Same thing if we get a stone tool and a piece of charcoal at roughly the same depth in a site that we know has not been disturbed, you can date it by association.

Edit: just took a look at the article you linked. They've dated those tools to 280,000 years ago, not 85,000 years ago, so they would definitely not be using radiocarbon dating. I don't know what they used. The article is a bit hyperbolic but just keep in mind that, especially with those really early sites, there is a lot of room for error or unknown things complicating the picture, and a ton of room for interpretation, so the big claims that the article makes might be a bit presumptuous. As always, more research is required.",null,0,cdmfnvd,1rectr,askscience,new,2
Solivaga,"There's a wide range of radiometric dating techniques, but as /u/Pachacamac points out, you can't use radiocrbon dating on inorganic materials (such as stone), and radiocarbon dating is only really accurate back to around 50k BP, and completely fails much beyond 75k BP.

The short answer is that we use context and stratigraphy to securely sequence artefacts and features - in turn this allows us to identify material as being conteporaneous.  This enables us to date other material that's from the same phase of occupation or activity as the stone tools.

Dating techniques that stretch further back than C14 include Potassium Argon, Uranium Series, Fission Track, Electron Spin Resonance, abd Obsidian Hydration.  The problem with many of these is that they date natural events (including volcanic rock formation, formation of calcium-carbonate etc.), so often we'll be using these dates natural events to constrain the archaeological materials - i.e. we know that this palaeolithic site was occupied sometime between x and y.

",null,0,cdndjg2,1rectr,askscience,new,2
fishify,"Hybrids have both an internal combustion engine and an electric drive system, which enables them to achieve better efficiencies in a few ways. One is that they recapture energy that would otherwise be lost; regenerative breaking allows the energy lost to waste heat in a standard car to instead be used to store energy in the hybrid's batteries. Another is that the internal combustion engine can be smaller, and thus operate more efficiently more of the time, since the electric motor is available for peak demands. In addition, the internal combustion engine can be turned off in situations in which a car is idling.",null,0,cdmfog1,1recy4,askscience,new,2
bkkgirl,"Well nothing's stopping you from using it except that few people know how to use it, and very little has been translated to it.

Also, people with different accents would _write_ differently. This is critically important in languages such as Chinese, where the differences would render every dialect mutually unintelligible, and somehwat important in languages like English, becuz eugeniks an da lik wud mak ritin litrl spekn had. Written language preserves etymology, whereas the IPA, which would produce different forms for the same word, does not.

Additionally, what is transcribed in the IPA is not entirely uniform, so representations would be ambiguous even among speakers of the same dialect.

Since people usually read by identifying words as a whole, direct transcription of what was said would be counterproductive and difficult to follow, and since that's what the IPA is for, it would be too.

Disclaimer: I can't speak AAVE, so my transliteration is probably shitty as fuck.",null,0,cdmkd36,1refad,askscience,new,9
protestor,"A thing about phonetic alphabets is that often two different sounds are interpreted as being the same phoneme in a given language (they are [allophones](http://en.wikipedia.org/wiki/Allophone)), but on a different language they might be distinguished. On a given language the preferred allophone might depend on region, for example. The fact that two sounds may be interchangeable is called [free variation](http://en.wikipedia.org/wiki/Free_variation):

&gt; When phonemes are in free variation, speakers are sometimes strongly aware of the fact (especially where such variation is only visible across a dialectal or sociolectal divide), and will note, for example, that tomato is pronounced differently in British and American English, or that either has two pronunciations which are fairly randomly distributed.

[Each language has its own set of phonemes](http://en.wikipedia.org/wiki/Phoneme#Numbers_of_phonemes_in_different_languages). Some languages don't use tone to distinguish phonemes (but use them for other things), others use a lot.

This kind of non-uniformity may negate any advantage in uniformizing our writing system.

I also find the latin alphabet pretty convenient to type in a keyboard, but the IPA is less so, because it has too much symbols. (also, IPA is sometimes too specific - how to represent a word that we don't know how to pronounce?)

(ps: I suppose you're suggesting we use IPA to substitute alphabets already in use, instead of using IPA just for phonetic transcription)",null,0,cdmqbkr,1refad,askscience,new,2
drzowie,"A superadiabatic gradient is what *drives* convection -- the free energy that gets converted to mechanical flow comes from the positive difference between the gradient and the adiabatic lapse rate.  Convection will happen at *some* level with any nonzero excess in the lapse rate above the adiabatic rate, since the material is a fluid.

In practice, the actual lapse rate doesn't get driven exactly to the adiabatic rate, but it's pretty darned close.  The actual offset is driven by the balance between heat flux and (effective turbulent) viscosity in the fluid.  Since stellar plasmas aren't known for their high viscosity, and the scales are large, the offset turns out to miniscule (negligible by orders of magnitude) in nearly all cases -- so you can treat the adiabatic lapse rate as a strict limit, and be good to go.

Let's apply your example of 10^-6 superadiabaticity to the Sun.   [The convection zone spans about 6 orders of magnitude, or about 14 scale heights, in density](http://solarphysics.livingreviews.org/open?pubNo=lrsp-2009-2&amp;amp;page=articlese1.html).  If the lapse rate differs from adiabatic by 1 part in 10^6, that corresponds to a temperature differential factor of e^(14x2/3x1.000001) compared to e^(14x2/3) across the whole convection zone - so if you assumed the lapse rate was exactly adiabatic, but it was really 1+1x10^-6 times the adiabatic rate (and you knew the photospheric temperature exactly), your calculation of the temperature at the base of the convection zone would be off by a factor or (1+1x10^-5).  Other effects (like convective overshoot and dynamo action) enter at the 10^-3 level, so the superadiabaticity is negligible.",null,0,cdmo4bd,1reh81,askscience,new,1
LoyalSol,"There isn't really one universal answer since different materials will react differently with acids/bases, but a large majority of them dissolve because of either oxidation like in the case of metals or through catalyzed reactions (the acid/base speeds up a reaction that normally would occur slowly).

Oxidation is pretty straight forward.  The metals have electrons taken away by the acid and once that happens they form stable ions which can be freely dissolved into solution.  In catalytic reactions the acid/base comes in and binds to a functional group on a molecule (usually organic molecules) and stabilizes the molecule in a way that it can undergo further reactions.  

http://www.organic-chemistry.org/namedreactions/fischer-esterification.shtm

That's an example of the forward reaction, but the reverse reaction is similar.    In large scale a organic molecules such as proteins, each peptide in the chain is linked together by an functional ground (amide group for proteins, O=C-N) and the acid/base will attack these links causes the chain to break apart.  Which is why they are generally detrimental to biological organisms. ",null,0,cdmm2e5,1rehln,askscience,new,2
NotFreeAdvice,"Answering your second question, glass is often used for two reasons.  First, the Si-O bonds that are the structure of silica compounds (like glass) are relatively inert.  Thus, they do not like to be broken by other compounds/chemicals.  Second, it is amorphous, which adds both strength to the vessel and well as a reduction in reactivity that can occur at the edges of crystal faces.  Hence, the amorphous nature renders the glass less reactive than it would be if it were crystalline silica.  

There are some things that are not good to store in glass, however.  Potassium hydroxide will etch away the glass, and hydrofluoric acid will do the same.  These are just two examples, but there are a number of chemicals that are not inert, with respect to glass. 

Hope that helps!",null,1,cdmmctc,1rehln,askscience,new,2
SilentCastHD,"Well, first of all, you have to differentiate subtractive color from additive color.

In the first case, all the colors give you black, in the second, all the colors give you white.

So to make that clear: If you take a flashlight, and shine it onto a white paper, you see white light. - Duh...

If you take a red marker and mark the page, you strip away ""all the light"" that isn't red and absorb it, so only the red light reflects. The dye subtracts the [wavelengths](http://upload.wikimedia.org/wikipedia/commons/c/c4/Rendered_Spectrum.png) that don't correspond to red.

So you transform white light to red light using the filter ""red marker dye"".
Going forward, with blue and yellow, you strip awaay more and more of the light, until no light is relected anymore, leaving you with black.

The other way around, you [add up colored light](http://upload.wikimedia.org/wikipedia/commons/2/28/RGB_illumination.jpg) to make white light.

So you shine red light onto a white wall, the reflected light is red. If you overlay it with the other colors, you'll get white again.

(This is why green looks black in ""pure red light"", since there is [no refelection of red light on green leafs](http://1.bp.blogspot.com/-hHcuVK0TGHg/TyVDCInAFWI/AAAAAAAAAT0/3tU3h7p1Zbw/s1600/Lights%2B1%2B-%2BOriginal.jpg))

So with that out of the way: What is grey?

Grey is the achromatic color between black and white.

So, since you get the two different colour-systems now, you see that grey in [RGB](http://en.wikipedia.org/wiki/RGB) displays (additive color) has to be different from the grey in a printed picture in [CMYK](http://en.wikipedia.org/wiki/CMYK_color_model)(subtractive color)

So, as you can see [here](http://www.aksiom.net/rgb.html) at the bottom the RGB value for grey is always something where R=G=B, and the stronger the individual light gets, the more you go up to white.

I hope this helps :)",null,0,cdmjc3a,1rekov,askscience,new,6
svarogteuse,"
$60 is not worth spending on a telescope. You will end up with a very low end wobble device and be disappointed.  Buy a set of binoculars first. If you decide that you aren't that into astronomy later binoculars have other uses a telescope really doesn't. Next go hang out with the local Astronomical society. Look at what they are using get to know their equipment before you make a purchase more than binoculars.
The smallest scope regularly used in our society 10 years ago was an 6"", well above the $60 price tag and the 2-3"" of the ones you mentioned.

15x70s are huge binoculars. You are going to have problems keeping them steady unless you invest in some sort of [mount for them](http://www.telescope.com/Orion-Paragon-Plus-Binocular-Mount-and-Tripod/p/5379.uts).  With those binoculars you will be able to see the moons of Jupiter, the ears on Saturn, maybe Titan if you are in a good spot, and clusters. They aren't really designed to see galaxies except the brightest ones. The standard binoculars used are 10x50s. Light enough to hold steady, or balance on a chair but powerful enough to see binocular objects (bright clusters, comets, birds). We really don't use binoculars for planetary observing not enough detail. I would recommend a set of 10x50s before the 10x70s. I have never owned nor known anyone to own such large binoculars except for special purposes like comet hunting and defiantly not without a mount.

Neither of those scopes are really worth using for more than a causal, hey that's the moon kind of use. I used a 6"" for many years around 2000 and it was the smallest scope in the group. 8"" is a standard entry level amateur scope. What matters in a telescope is aperture the size of the main lens or mirror. The larger the aperture the more light is concentrated onto your eye, the fainter an object can be seen. You want to spend your money on aperture! Magnification doesn't matter, most observing is done with relatively low magnification but the higher aperture the better.  

Long time amateur astronomer (30+ years), previous president local astronomical society. ",null,0,cdmnyge,1reljs,askscience,new,2
_NW_,"Having both works better.  Do the binoculars have a tripod mount?  At that kind of magnification, it's going to be difficult to hold steady.  Also, after a few minutes, your arms are going to start getting really tired.  My first telescope was a 60 mm Tasco.  A good pair of binoculars worked better.  Years later, I bought a 6 inch reflector and finally got see all the things that I couldn't with the Tasco.  I have a pair of 10x50 binoculars that I use alone or with the telescope.  When looking for something in the sky, it helps to find it with binoculars first before using the telescope.  Or, sometimes I just go look with the binoculars just because it's easy.  Also, because it's more than enough to see several galaxies, star clusters, nebulas, and Jupiters moons.  Actually, the Andromeda, LMC, SMC, and a few other galaxies are visable even without binoculars.  Stop at a store and pick up a copy of ""Sky and Telescope"" or ""Astronomy"" Magazine.  Both have a star map that shows what you can see for that month.",null,0,cdmzcq2,1reljs,askscience,new,2
botanist2,"I do a bit of bird watching and very amateur stargazing, so I have some experience in this issue.  One of the biggest problems with using binoculars for anything like bird watching or stargazing is that your arms aren't very steady, which isn't that much of an issue at lower magnifications (e.g. looking at birds in the tree above you), but is really bad at higher magnifications (e.g. trying to look at ducks way out in the pond).  I would suggest getting a telescope with a tripod because you'll get a lot more stability and you'll be able to see things more clearly as a result.",null,0,cdmmve6,1reljs,askscience,new,1
drzowie,"Jovian interference.  The asteroids are near a couple of major resonances with Jupiter; that gives them enough of a nudge to prevent them from coalescing.  (Source:  while I am not a planetary scientist I work in a lab with a passel of 'em).

A bit more: Small-ratio resonance orbits with major bodies typically have nothing in them, because over time the larger body kicks the smaller ones out of that orbit.  Think of pushing a swing, or operating a cyclotron:  you can transfer a *lot* of energy to an oscillating body just by kicking it gently in some pattern with a harmonic relationship to the oscillation.  Major bodies typically clear out their own orbits over time due to the 1:1 resonance with anything else in that orbit -- anything at, say, 0.999 AU would eventually have a near encounter with Earth, and get ejected.  That effect is why Ceres and Pluto are considered ""dwarf planets"" and not ""planets"" -- the dynamical process is part of our modern definition of a planet.  The 2:1 and 4:1 resonances with Jupiter define reasonable approximate boundaries of the asteroid belt, and there are noticeable gaps near small-integer ratios of Jupiter's period between those values.
",null,0,cdmoip2,1remcu,askscience,new,3
The_Evil_Within,"Jupiter exerts enough of a disruptive force on the asteroid belt to keep it thinned out - and due to their relative motion, individual asteroids are at least as likely to smash into *more* debris than they are to coalesce into one bigger mass.

At least, so I was informed when I asked this question here a while ago.  The detailed explanation was kind of over my head, as you might expect.

Given that explanation, I still have trouble understanding how Ceres could form in the first place, yet still not be capable of 'finishing' and collecting the remaining mass of the asteroid belt.",null,1,cdmojfi,1remcu,askscience,new,3
GumbyTastic,"Well you have to look at it like this. Why does saturn have rings? Why doesn't all that mass floating around it just make a new moon? Usually the mass doesn't have enough force to coldine and make new objects or there's not enough force keeping the mass together. The asteroid belt (don't quote me) like a big ring like saturn. It's just full of rocks and debris that get caught up in the suns gravitational pull. It looses mass and gains mass when new objects are knocked out and sucked in. Correct me if I'm wrong on anything. I enjoy learning and never see much coverage, hell I never see anything about the asteroid belt!!",null,4,cdmj64t,1remcu,askscience,new,3
bobbycorwin123,"bah, I cant find any links to the exact reason. I believe its because of the rotations of mars and Jupiter and the way the gravity of the two bodies prevent stuff from gathering too much...

All I remember is that Jupiter and Saturn have a 2/1 rotation ratio...which helps not at all in this",null,6,cdmjcle,1remcu,askscience,new,3
I_Gargled_Jarate,"Gravity isnt strong enough to compress asteroids into larger planets. It takes a high velocity collision for asteroids to fuse together. Gravity does play a part by attracting large bodies which may be potentially travelling at very high velocities, but just sitting next to each other is not enough to form larger objects.",null,4,cdmk4yf,1remcu,askscience,new,1
wazoheat,"No. Food and drink go bad due to [spoilage](https://en.wikipedia.org/wiki/Food_spoilage), which is usually due to the growth of bacteria and/or fungus, none of which will grow in plain water.",null,0,cdmi5o5,1remei,askscience,new,3
ides_of_june,As wazoheat said water doesn't spoil. It's possible that the container that the water is stored in could undergo thermal degradation making the water unfit for consumption (or at least undesirab. Also if the water is stored open to the environment it can become contaminated though in an indoor environment it's unlikely to become unfit for consumption.,null,0,cdmo21b,1remei,askscience,new,1
ramk13,"Depending the temperature ranges you could breakdown the disinfectant residual (usually chlorine or monochloramine at ~1 mg/L) that is normally present in tap water. If the residual is gone, then organisms (e.g. algae) are much likely to grow in your water if spores are present. 

Also if the temperature gets high enough you can have interactions between the water and its container. Metals are more likely to corrode and leach, and probably more relevant, plastics can leach plasticizers into the water. I don't know that there are studies that have quantified whether there are documented effects in animals or humans for tap/drinking water stored in plastic bottles, but many people are concerned about it.

The water itself won't spoil.",null,0,cdmrq6r,1remei,askscience,new,1
kipz0r,"It would come down eventually due to drag. There was actually a bag of tools 'dropped' from the IIS, which came burning down 9 months later.  
[Link](http://en.wikipedia.org/wiki/Heidemarie_Stefanyshyn-Piper#Lost_tool_bag_during_spacewalk)

To see for yourself, try out [Kerbal Space Program](/r/kerbalspaceprogram), it's quite a silly game, yet it gives you a good idea on what orbit is and how much speed you need to de-orbit etc. ",null,16,cdml5fw,1rendq,askscience,new,55
_Jordan,"The [ISS required periodic boosting to keep in in orbit](http://en.wikipedia.org/wiki/International_Space_Station#Orbit_and_mission_control), as the orbit is low enough to the earth that it experiences a small amount of drag, and would eventually deorbit on its own.

Whether you threw an object really hard, or just gave it a little push, it would eventually deorbit on it's own. I suppose the direction and speed you threw it in might change how long it stays in orbit a little bit, but I suspect given the orbital velocity of the ISS (27,600 km/h) and the speed of a good throw (~100 km/h), you would make only a small difference in how long it would take.",null,0,cdmu6ch,1rendq,askscience,new,7
brickses,"I went ahead and [numerically solved the problem](http://i.imgur.com/NKKyxsI.png) (ignoring air resistance). You would need to throw your tomato over twice as fast as a good baseball pitch in order to get it to reach Earth, anything less, and it will undergo an elliptical orbit for a while, until the air resistance gets the better of it.",null,2,cdn4gfc,1rendq,askscience,new,5
kodran,"If you throw it from the ISS as it is right now (moving), it'll probably stay in orbit (at least for a while) because it'd start with the ISS's original speed, but if you are only considering the ISS altitude as reference but your hypothetical throwing is from a stationary point it'd probably fall back down to earth. Remember: orbiting an object is pretty much being in a constant state of freefall, but with a huge speed towards the side as /u/WrecksMundi pointed out; that is why the ISS stays in orbit, it ""doesn't get to fall down"" because it keeps moving sideways.",null,18,cdmjxba,1rendq,askscience,new,19
WrecksMundi,Gravity in low earth orbit is very close to what we experience down on the surface. The ISS would crash down to earth quite quickly were it not for the velocity at which it was moving while orbiting the earth. The speed you need to stay in orbit is approximately 8 kilometers per second. So a slight nudge in the opposite direction should just about do it. ,null,48,cdmjdes,1rendq,askscience,new,27
paolog,"The ""criss-cross"" distance between two points is called the Manhattan distance between the points, while the straight-line distance is called the Euclidean distance. What you're asking is whether the limit of the Manhattan distance as the grid gets finer is equal to the Euclidean distance. It's easy to show that this is not the case.

Let's take a 1 x 1 square. The Manhattan distance from one corner to the other is 2 (length of bottom edge + length of right edge, for example), while the Euclidean distance is, by Pythogoras' theorem, √2.

Now subdivide the square into a 2 x 2 grid of four squares. To get from one corner to another, we have to zigzag along four edges of the small squares, each of which is 1/2 a unit long. So the total distance is 4 x 1/2, or 2.

It's not hard to show that however we subdivide the square into smaller squares (or even rectangles), the shortest corner-to-corner distance measured along the edges of these squares will always be 2 and will never get anywhere near √2. Hence no matter how many turns we make, the Manhattan distance never equals the Euclidean distance.

So no, nothing changes as the resolution of the grid becomes finer. Furthermore, a diagonal is not imaginary - it is just different from walking along the edges.

EDIT: removed repetition",null,1,cdml7vm,1reu8x,askscience,new,21
Professor_Snuggles,"The fundamental point here is this: two curves can be visually similar yet have very different lengths. Imagine a bug taking inch long steps in a long zig-zag across the line. What you're doing is similar to saying that the bug can instead cut closer to the line and decrease the forward distance covered with each zig-zag. This could give a path that stays closer to the original line overall, but has the same length because it wiggles more.

The moral of the story is that curves that stay close to each other do not have to have lengths that stay close to each other. As for real life: yes there is a difference traveled if you take a small step up, then a small step right, etc. compared to directly walking the diagonal. This is easiest to see if you have a robot or something that you can guarantee will travel at a constant speed and a timer. A real life diagonal is not necessarily ""imaginary"", it's just that traveling near it in any way you want is not going to get the same results as traveling *on* it, or other paths that more accurately approximate doing so.",null,1,cdmrs35,1reu8x,askscience,new,7
jeff0,"The size of the grid doesn't matter. Say your rectangle is a 1 mile x 1 mile square. The length of the diagonal from A to B is sqrt(2) =~ 1.4 miles. If you instead alternate walking due north with walking due east, you end up walking a total of 1 mile east and 1 mile north = 2 miles total. The size of the grid will only effect the number of times that you turn.

The same idea underlies the [troll math](http://knowyourmeme.com/forums/meme-research/topics/8029-troll-math) meme.",null,0,cdmleqb,1reu8x,askscience,new,2
wgunther,"Just to prove the bit more formally instead of showing some examples: if you divide a 1x1 square into an n by n grid then the distance you are traveling in total n*(1/n+1/n); that is, imagining you are in the bottom left corner, you have to go up distance 1/n and right distance 1/n for each subdivision, and there's are n of them. Therefore, the distance you must travel is 2. 

Intuitively it makes sense: all your motion is either purely vertical or purely horizontal. You must move horizontally distance 1 and vertically distance 1. Therefore you must move distance 2. ",null,1,cdmmhao,1reu8x,askscience,new,3
ignorant_,"A diagonal line bi-sects a square at 45degrees. The question asked is regarding using horizontal and vertical lines to travel toward the opposite corner. These lines are at 90degrees. Suppose we used intermediate angles. 89 degrees, then 88 degrees, etc., and work our way down toward 45degrees. Wouldn't my distance begin to decrease as the angle approaches 45 degrees, and have a limit of square root of 2?",null,1,cdmw7ub,1reu8x,askscience,new,1
yeast_problem,"Quantum Mechanics would bring a limit to this, as the grid size gets smaller the uncertainty principle would mean your momentum could not be zero in the  direction perpendicular to travel. It would become impossible to say that you were actually travelling along the grid lines, at scales around 10^-34 meters.",null,4,cdmvy76,1reu8x,askscience,new,1
breadmaniowa,"The real reason you feel the need to breathe is because of the carbon dioxide building up in your blood. Taking in oxygen removes the dissolved carbon dioxide from your body. So basically, the real reason you can't hold your breath for very long is that you need to expel the carbon dioxide from your body. You actually have plenty of oxygen still in your blood when you feel the need to breathe.",null,5,cdmlr1p,1revb2,askscience,new,12
fazedx,"There are two drivers in the human body that tells it to breathe. The first one is concentration of carbon dioxide in the blood, and the second one (backup, if you will) is the concentration of oxygen.

Carbon dioxide (CO2) is allowed to pass the blood brain barrier. High concentrations of CO2 diffuse into your cerebral spinal fluid (CSF), dissociates into hydrogen ions and lowers your CSF pH. This is picked up by chemoreceptors and signals your central nervous system to increase ventilation. This is your central, or main control of breathing.

Peripheral control is based on pO2 in arterial blood. If it drops below a certain point, it will send signals to your brain to start breathing.

You can reduce the pain from holding your breathe by hyperventilating before you hold your breathe, thus reducing the buildup of acid and the prolonging the time it takes for your brain to signal to you to breathe.

http://www.winona.edu/biology/adam_ip/misc/assignmentfiles/respiratory/Control_of_Respiration.pdf is a good source/summary",null,0,cdmuuor,1revb2,askscience,new,3
bbqbollocks,"Because there are two ways a stm works. Constant current and constant height.

With constant current, the distance between the tip and the sample changes to keep the current flowing through the tip the same. This maps the topography of the surface. 

If the sample is flat enough then you can use the constant height mode. The constant height mode will keep the distance between the tip and sample fixed as it scans across the surface. So if you have 35 xenon atoms writhing range cor quantum tunneling to take place then a current flows where the atoms are. So no nickel atoms can be viewed. This mode looks at the density of states on the surface. ",null,0,cdmlwie,1revqt,askscience,new,11
katc102,"This is essentially the Hot Chocolate Effect. 

When you first start stirring the coffee air bubbles get trapped inside the coffee reducing the speed of sound in the it lowering the frequency. As the bubbles begin to get released from the coffee sound travels faster in the liquid and the frequency increases again.

Here is a short wikipedia article that goes into a bit more detail. http://en.wikipedia.org/wiki/Hot_chocolate_effect",null,27,cdmlm65,1rew42,askscience,new,170
rupert1920,"Check out [this big thread](http://www.reddit.com/r/askscience/comments/x4tdu/askscience_my_coffee_cup_has_me_puzzled_so_i/) about a year ago, on this exact topic.",null,10,cdmokr1,1rew42,askscience,new,21
NicholasCajun,"It's important to first recognize that the media will completely blow things out of proportion. Any Black Friday violence is good for their ratings, since people love to gawk and feel better about themselves. So if you're living outside the US, your opinion has to be shaped exclusively by what you see or hear from others.

Guess how many deaths you think Black Friday has caused over the past 7 years.

Does [this](http://blackfridaydeathcount.com/) number fall under that guess? I wouldn't be surprised if most people reading this guessed higher than that number.

As for the ""why"" of your question, as should be evident, most people aren't violent. People will certainly resort to being rude, underhanded, or impolite, but very rarely does it escalate to actual violence, and a lot of the violence that does happen is indirect (i.e. people dying because of stampedes - no one's intentionally trying to harm others when that happens). Very few deaths/injuries have been caused by a shopper being violent with intent to harm.",null,13,cdmpnrg,1rf1fn,askscience,new,36
badcaseofgauss,"I agree partly with u/NicholasCajun...however I also think it has to do with competition and competitive escalation.  The items people are trying to get are scarce therefore people must compete to get them.  The first part of this is waiting in line, you are competing with other's patience to see who will get tired of the cold and noise.  Next people run and rush to get an item first, again with the competition.  At this point they have invested a significant portion of their time to get an item which means they are committed.  Add in the peer pressure some people feel (due to materialistic concerns and society) to get the best/newest present for others and you can get a sort of arms race type of competitive conflict escalation.    They shove you as you go to the door, you shove back, they shoulder you out of the way, etc.  Slowly you escalate from more socially acceptable behaviors into those that are less socially acceptable, like violence.

[Escalation link](http://en.wikipedia.org/wiki/Escalation_of_commitment)

[Sunken Cost Fallacy](http://www.skepdic.com/sunkcost.html)

[Good Article on Scarcity vs. Competition](http://www.sciencedirect.com/science/article/pii/S0176268003000338)
",null,0,cdmvf0j,1rf1fn,askscience,new,3
null,null,null,11,cdmqy7f,1rf1fn,askscience,new,13
null,null,null,410,cdmn013,1rf2b3,askscience,new,1933
crazzle,"Heat does not rise. Hot air rises.

Hot air rises because hot air is air with molecules that have more energy, so they bounce around and collide with each other more, creating more space between them.  As a result the air that is less dense than cold air, so the less dense air is displaced by heavier cold air. 

That's a weight issue, which only exists in gravity.

In zero G you get heat radiating outward in a sphere. You also get spherical flames.

Source: I studied and ran experiments on zero-g fire in grad school.",null,275,cdmlcrf,1rf2b3,askscience,new,1453
barnacledoor,"Based on [this Straight Dope response](http://www.straightdope.com/columns/read/819/if-you-lit-a-match-in-zero-gravity-would-it-smother-in-its-own-smoke), no.  Heat rises because warm air is less dense so then it floats up to be replaced by the heavier cool air.

&gt;Convection works in normal gravity because warm air is less dense and thus lighter than cool air and so rises above it. But in a weightless environment the exhaust gases basically hang around the candle flame until all the oxygen in the immediate vicinity is exhausted, at which point the flame goes out.

This was an answer regarding flames in zero gravity.",null,40,cdmmor1,1rf2b3,askscience,new,182
ErasmoGnome,"Researchers in space have actually tested this. [Here's a picture of a candle in space!](http://upload.wikimedia.org/wikipedia/commons/6/63/Flame_in_space.gif)

[And here's a more detailed gif created using thumbnails](http://i.imgur.com/xwDsYw6.gif) from this picture: http://i.imgur.com/1xidPX7.jpg

Obviously, one can't see heat in that picture, but I think the flame gives a good idea. Because there is no ""up"" for the flame or heat to go in, it can't behave as it normally would. In a regular environment, heat (or rather hot air) rises because it becomes less dense, and therefore floats up. In space, things can't rise because of their density because there is really no such thing as rising.",null,6,cdmlmhf,1rf2b3,askscience,new,67
mochamocho,"Just a simple argument: If there is no asymmetry in your experiment (ie no direction of gravity), there cannot be a preferred direction on the macroscopic level. Having no asymmetry also means it makes no sense to speak of up/down or rising and falling.",null,10,cdmlptk,1rf2b3,askscience,new,53
TheGrim1,"Heat always moves in straight line away from it's source. No matter if there is or is not gravity.

The question I think the OP wants to ask is ""In a zero gravity environment, does hot air still rise?""

The answer is no.

Hot air is less dense than cooler air. Cooler air is more affected by gravity (on earth) so it sinks.

In a zero gravity environment, assuming a point as a heat source, the air temperature would be proportionately related to the distance from the heat source. 

As the air was heated it would attempt to expand. So, the air density would be less the closer you got to the heat source. Less dense air conducts heat less effectively (or actually, dense air impedes thermal conductivity more). So I would imagine that there would not be a linear temperature to distance ratio.",null,27,cdmrqlw,1rf2b3,askscience,new,53
Knight_of_r_noo,"With hundreds of comments I'm sure no one will see this but I want to make my statement. I'm not going to get into the 'there is no up or down in zero-G' argument. All the other comments are doing a good job of covering that topic. I'd just like to add this tidbit about astronauts sleeping in space:
&gt;Sleep spots need to be carefully chosen - somewhere in line with an ventilator fan is essential. The airflow may make for a draughty night's sleep but warm air does not rise in space so astronauts in badly-ventilated sections end up surrounded by a bubble of their own exhaled carbon dioxide. The result is oxygen starvation

This is from the [ESA website](http://www.esa.int/Our_Activities/Human_Spaceflight/Astronauts/Daily_life)",null,11,cdmpnpe,1rf2b3,askscience,new,22
logicaless,"OP, I really hope this comment doesn't get buried. Here is a visual example of what heat actually does in zero gravity:

A match lit in zero gravity - http://www.youtube.com/watch?v=Q58-la_yAB4

Notice it makes a sphere instead of a teardrop shape because there is no up for the flame to rise towards.",null,0,cdmmg33,1rf2b3,askscience,new,11
jananus,"Basically, no. 

Taking the example of a candle, the shape of the flame is caused by gravity (i.e. heat, in this case the hot gas which is the flame, rises) . If you light a candle in zero gravity conditions, you get a sphere.

An interesting little movie on the matter: http://www.youtube.com/watch?v=SauaMVAl-uo

",null,9,cdmlh6z,1rf2b3,askscience,new,18
Sack_Of_Motors,"Technically heat doesn't rise or sink. It transfers from hot to cold. The reason it can be thought of ""rising"" on Earth, as pointed out already, is due to convection and the difference in densities of fluids (liquid or gas) at different temperatures. Since gravity effects on fluids don't matter in space, the fluid does not separate due to difference in density.

However, you can still have convective heat transfer in space. It mostly depends on phase change for the heat transfer and capillary pressures for moving the working fluid. If you want more info, you can read about [heat pipes](http://en.wikipedia.org/wiki/Heat_pipe#Space_craft).",null,18,cdmm72d,1rf2b3,askscience,new,22
ThePnusMytier,"People have mentioned how it is effected, but here are a couple interesting videos to demonstrate how heat makes things move in microgravity:

water boiling: http://www.youtube.com/watch?v=fsgPjpzGgT4

Though the bubble of water vapor above boiling is significantly hotter, there is no gravity to cause any buoyancy effects, keeping it pretty much just where it is and growing as more water reaches the boiling temperature. there is no 'rise' or even really motion of it, just more water vaporizing.

flame in microgravity: http://www.youtube.com/watch?v=SZTl7oi05dQ

Since there again is no buoyancy, the hotter carbon dioxide isn't pushed away, and it's just a growing sphere of oxygen being eaten up and then the standing CO2 suffocating it. The hot air can't rise, or even be pushed out of the way due to heat or convection alone.",null,1,cdmmitt,1rf2b3,askscience,new,6
Apocellipse,"The simple answer is no, for the reasons others have said.  For an idea of how micro-gravity effects air flow differently in space than on Earth, on the ISS, every single module has its own constant air flow systems, not just to recycle CO2, but to just move and mix the air to maintain a constant temperature and mixture.  In space, without fans, CO2 can build up in a stagnant corner, or right in front of a sleeping astronauts face, and hotter or colder air could build up in the same way.  Fans and suction and exhaust are constant and noisily making up for the loss of gravity induced convection.",null,8,cdmqbvb,1rf2b3,askscience,new,13
f0rcedinducti0n,"Radiated heat doesn't rise, hot air rises because it is less dense than the surrounding air. Heat radiates away from the source in all directions, even under the effects of gravity, it's the air that the heat source warms up that rises (in the frame of reference you're familiar with - on Earth)... ",null,0,cdmtrhn,1rf2b3,askscience,new,5
ITRAINEDYOURMONKEY,"There are a lot of good answers posted, but one thing that's tripping up the discussion is language. People are using the word ""heat"" pretty wantonly.

*Heat* is thermal energy, which means particles are wiggling around (faster wiggling = higher temperature). Heat moves across a thermal gradient from higher temperatures to lower, which means that, on average, particles that are moving around quickly transfer energy to particles that they interact with which are moving more slowly. In solid objects, this has nothing to do with gravity.

*Hot air* is what rises. Or any fluid that does not have homogenous temperature (so the same thing happens in water). Just like everything else it has to do with most energetically/statistically favorable condition, but suffice it to say gravity makes the more dense fluid (colder air) end up on the bottom while the less dense fluid (warmer air) moves upward, until it ends up with air of the same density. This is specifically because of gravity.

*Heat from the sun* is not properly heat while it's traveling through space. It's electromagnetic radiation, which is not thermal energy. It's energy propagating in the form of an oscillating electromagnetic field. It becomes heat as soon as some piece of matter absorbs it.

/u/thedufer (top comment) said it very succinctly, but maybe some people will see this and be able to feel better about the ambiguous word usage throughout the thread.

Edit: after /u/tSparx's comment (thanks) I made the requisite wikipedia check. Heat apparently refers to *any process* that transfers thermal energy (convection, conduction, radiation) (unless you all are buggering the wiki page for heat right now). Which means the the definition is unhelpfully ambiguous. Though it also changes the nature of the answer to OP's question, to say that the different mechanisms of heat behave differently. Radiative heat (the point about the sun) doesn't give a shit about gravity. Conductive heat (my first point, simply labeled ""heat"") doesn't either. Convective heat (the ""hot air"" point) doesn't happen without it.",null,1,cdmn10w,1rf2b3,askscience,new,6
wesramm,"""Heat"" doesn't rise, buoyant fluids do.  A fluid becomes buoyant because a local mass of the fluid (air) has lower density than the surroundings.  The air becomes less dense because it gets heated, and this gives rise to buoyancy.  BUT; buoyancy is a function of gravity, so, no.",null,9,cdmpswn,1rf2b3,askscience,new,12
cxseven,"NASA burned candles in microgravity and found that they self-extinguished ([pic](http://www.nasa.gov/images/content/684056main_update2_226.jpg)). So, not only is there no preferred direction for heat to ""rise"" in a zero gravity environment, in this case the heat also did not produce enough of any sort of convection to keep the flame lit. [[source](http://www.nasa.gov/mission_pages/station/research/news/wklysumm_week_of_august20.html)]

This makes me wonder if astronauts in the space station start to feel exceptionally warm (at least in spots) if there's not enough air circulation.",null,10,cdmpwee,1rf2b3,askscience,new,12
kingfalconpunch,"Heat doesn't rise, it flows from high energy concentration to low concentration. Heat is just kinetic energy of particles. The reason people think that heat rises, is that hot air is less dense than cold air, and therefore rises. But heat ""flows"" from hot to cold.",null,9,cdmmr5h,1rf2b3,askscience,new,13
NEIGHTR0N,"There are two primary factors in the transfer of heat in open air. Either [radiant heating](http://en.wikipedia.org/wiki/Radiant_heating) or [convection heating](http://en.wikipedia.org/wiki/Convection_heater). There is also the difference in pressure between different temperatures, which we'll discuss as well.

Convection heating is basically just air blowing across a heat source like a fan behind a radiator, and isn't relevant to your question. However, radiant heat is relevant. Imagine a heater in a corner of a room with no fans blowing any air around in the room. Eventually the heater would warm up the molecules immediately next to it, and then the molecules next to those, and so on and so forth until eventually all the room is about the same temp. That is radiant heating.

There is also a difference in pressure which can been seen due to the [Ideal Gas Law](http://en.wikipedia.org/wiki/Ideal_gas_law). In this case, as temperature goes up so does the pressure. This is what causes heat to rise here on earth. Take a balloon for at two different temperatures: at both temperatures, the balloon has the same mass, but at the hotter temperature the pressure increases thus making the balloon take up more space, this is why heat rises on earth and would not have a significant impact in a space ship at zero gravity.

tl;dr: In zero gravity, I'm assuming in a space ship with air in it (not in a vacuum). The heat would radiate outwards in all directions. That is all.",null,1,cdmq96f,1rf2b3,askscience,new,4
Dullahan915,"Air is a gas.  A warm gas is less dense than a cooler gas.  Gravity will cause the denser gas to sink and the less dense gas to rise above the cooler gas.  

In a zero gravity environment, the  forces that cause these actions will not be present, so ""heat"" will not rise.",null,10,cdmr4on,1rf2b3,askscience,new,13
insulanus,"In zero-g, in a fluid (e.g. air), heat will expand out from its source, due to Brownian motion.

Note that convection can't happen, because there is no gravity to pull the denser, colder air in any particular direction, so it will propagate more slowly.

You might also want to look up ""heat"" transfer via radiation vs. conduction. It's very interesting, and explains a lot of the mysteries behind heat.",null,0,cdms4uv,1rf2b3,askscience,new,3
lusamu,"Heat does not rise anywhere. Increasing the thermal energy of matter, with rare exception, causes the density of the matter to decrease. In a fluid (such as air) in a gravity field, (such as on earth) less dense materials experience an upward force (buoyancy) caused by the surrounding denser matter causing the less dense matter to move away from the center of gravity of the global system (rise).

In gases on a macro scale the relationship between temperature and density can be described by the ideal gas law.
 density = (molar mass x pressure) / (constant x temperature)",null,0,cdmlwfv,1rf2b3,askscience,new,3
aquarx,"In a vacuum, there would be no air for convection so in space, heat transfer would be almost completely radiation. In a zero gravity environment with an atmosphere, convection would still not occur. Heat transfer by convection occurs due to density gradients between hotter and less dense fluids(liquids+gases) and colder and more dense fluids. In a zero gravity environment a density gradient would still be present. Particles near the heat source would spread out (become less dense) and therefore heat would spread out in a uniform manner. ",null,9,cdmmecq,1rf2b3,askscience,new,11
neurkin,"This is all a matter of heat transference which has multiple routes:
**Conduction, Convection,** and **Radiation**

**Conduction**: the transference of heat through the physical particles interacting with each other. e.g. electric stove tops, iron rod feeling hot when on end is in a fire, burning your hand through direct contact.

**Convection**: what a lot of people above have referred to is the affect of air becoming less dense as it gets hotter (hotter air causes the particles to move faster, increase in speed causes a decrease in density). In a gravity environment this causes the air to rise (less dense air is located farther away from the surface due to lesser gravitational forces).  

I would argue in the candle example you would still get some form of convection due to movement, decreases in pressure around the candle... it would just not follow the normal convective flow. As oxygen particles are used and surrounding air heated it could be less dense than surrounding material thus causing **diffusion** to still be a critical role in moving the air from high pressure gradients to lower (this, of course, all depends on a huge number of factors)

Finally we have **Radiation**, all particles radiate energy according to their internal temperature (in kelvins).  This is approximated by [black body curve](http://en.wikipedia.org/wiki/File:Black_body.svg), this curve estimates what energy is released based on your temperature.  For example: The sun transmits most of its energy in the visible spectrum due to the very high temperature.  The earth (average temperature 288K) also radiates almost exclusively in the infrared range due to its internal temperature being much lower.

These principals apply all the time in day to day activities. IR goggles for example because we radiate a thermal temperature in the form of radiation. When we stick our hand in hot water we experience conduction as the water particles come into contact with our own and transfer that heat through direct contact.  And finally all of these into play when we look are large earth systems such as weather.",null,3,cdmmxgj,1rf2b3,askscience,new,5
alchemy_index,"To expand on this question (since the general consensus is that the heat would radiate ""out"" from the source)... 

What would it look like if I lit a piece of paper on fire in a zero G environment? It's hard for me to imagine what flames would look like without ""rising""",null,9,cdmn15l,1rf2b3,askscience,new,11
wickedsteve,No. And it can be a problem for electronic devices like computers in orbit and microgravity. As you have already read from others there is no up to rise to. On earth surface we rely on gravity and fans to cool our computers. The gravity pulls on cold air more than hot air. That makes hot air rise and cold air fall. If the heat my computer generated were to just hang around and accumulate the temperature would climb but the heat would stick around. Eventually it would get so hot that it would be useless and or shut down. Ever seen what a monitor screen can do if the fans on a GPU fail and it starts heating up beyond tolerances?,null,1,cdmn6cx,1rf2b3,askscience,new,3
GravityTheory,"This question has been answered pretty completely- I'd just like to point out that there really isn't any ""zero gravity"" environment (except in a physics classroom). In reality in space there is micro gravity which results from the attractive force of every massive object (not necessarily large-things with mass). The sum of these force vectors would be the  ""down"" and heat would rise away as a result of density/buoyancy. ",null,9,cdmngbx,1rf2b3,askscience,new,11
flowshmoo,"No, hot air will not rise in a zero gravity environment. 

Explanation: in an environment with gravity, hot gasses rise because they are less dense than air -- this has nothing to do with what orientation is ""up"" or to what ""rise"" is relative to. Density is largely related to gravity in that a less dense substance is less affected by gravitational force than is a more dense substance. Thus, without a gravitational force, there is no external influence to cause less dense gasses to orient in any unique way relative to more dense gasses. ",null,9,cdmnh29,1rf2b3,askscience,new,11
Swifty_Sense,"No. The absence of gravity means the absence of ""up"" in a constant direction. Hot air (most carbon dioxide) rises because it becomes less dense, meaning per liter of space occupied it weighs less. The heavier air then falls to the bottom. With no gravity, there is no up or down. The hot air will move to where ever it was originally headed. ",null,9,cdmnzr4,1rf2b3,askscience,new,11
qazwsx127,I watched a video of the ISS that explained they used special modified laptops with better ventilation because otherwise the heat just builds up around the GPU and CPU.,null,0,cdmo61c,1rf2b3,askscience,new,2
DimensionalNet,"The answer is probably not. Directions like up and down are relative to gravity so without gravity you can't have a rising action. Also, I don't think you can have heat without at least a tiny amount of gravity since a temperature gradient requires a material medium which will then have mass.  If this mass is continuous throughout with a high enough density to interact, the hottest stuff will probably ""rise"" compared to the cooler matter and form a spherical gradient assuming there's enough gravity to hold it together at all.  This particulate matter will probably behave like a fluid and that combined with enough gravity for observable effects gives you at least a gas giant or quite possibly a star.  At this point, you have to deal with much more variability than temperature.

Back to the original question, consider why there is a rising effect with heat. A hotter form of the same substance is going to be lower density and then has a higher probability to diffuse upward compared to the more dense form since there's less mass per unit of volume.  The heavier cold air sinks compared to the hot air but without gravity, there's no weight difference so the fluid would diffuse into each other and likely average out to the same temperature.",null,9,cdmo9dv,1rf2b3,askscience,new,11
Rodbourn,"Heat is the transfer of thermal energy, and itself doesn't rise even in a 1g environment (think of heating a solid, heat itself doesn't rise).  When a fluid's temperature is increased generally its density decreases/[volume increases](http://en.wikipedia.org/wiki/Thermal_expansion).  Then [buoyant forces](http://en.wikipedia.org/wiki/Buoyancy) cause the fluid to rise.  As it rises it may cool again and then 'sink'.  This has a name and is called [Rayleigh–Bénard convection](http://en.wikipedia.org/wiki/Rayleigh%E2%80%93B%C3%A9nard_convection).  This all depends on body acceleration to drive a flow from the density difference.  So if you are in a non-accelerating frame in microgravity - no, you will just have an expanding fluid.  If you were to accelerate the frame (engine burn), the fluid would rise against the acceleration vector.

Mathematically you can see this in the [Navier Stokes Equations](http://en.wikipedia.org/wiki/Navier%E2%80%93Stokes_equations) if you look at the momentum equation.  There is a body force term, *f*, which is where the buoyancy forces would appear as rho  g.  In microgravity that term would be zero. Note that *f* could have other contributions for body forces (such as a magnetic field in a ferric fluid).

source: phd student studying cryogenics in microgravity numerically and experimentally.",null,0,cdmognz,1rf2b3,askscience,new,2
TheoQ99,"Nope, heat only rises due to a pressure/density differential caused by the settling of particles by gravity. Take away gravity and then all particles are able to more freely move in all directions, so the hotter particles have no advantage in any single direction. The best way to see this is that [candle flames are spherical](http://www.youtube.com/watch?v=IgzCMKdAYuI) in zero g. Heat does not rise, so a convection current is not set up, and the plasma is stuck in that shell of a sphere. ",null,1,cdmphjt,1rf2b3,askscience,new,3
DeathbyHappy,"Heat always expands outwards. In a standard setting, the heat is transferred to a local source of lower temperature. When it is transferred to the air, it rises. In a vacuum, the heat will dissipate in all directions evenly.",null,1,cdmqesq,1rf2b3,askscience,new,3
thebattlefish,"Heat rising is actually gases expanding to fill the space they are in. The less energy contained in the particles of the gas(heat) the less it is able to expand outward from the earth. In a zero gravity environment, the gases mix into one temperature by all spreading throughout their container(hot faster than cold) and transferring heat via molecular conduction. The hot gas expands faster, not higher, in this case.",null,1,cdmrfzp,1rf2b3,askscience,new,3
Zombies_hate_ninjas,"Now I'm questioning how the ISS maintains it's internal temperature. Without gravity, or at least in an environment with significantly reduced gravity; how do they heat or cool the interior?

Obviously the space station is well insulated, but wouldn't they have to balance the interior temperature some how?",null,1,cdmtzvb,1rf2b3,askscience,new,3
lordofthemists,"There's a lot of people talking about what happens to heat in zero G (it radiates outwardly in every direction equally).

 But since you said you're curious, there is a [great video](http://www.youtube.com/watch?v=BxxqCLxxY3M) out there that demonstrates the effect of nearly zero G on flames and how their shapes change because the convection currents don't behave the same as under the influence of gravity. I found the entire channel fascinating. 

 ",null,0,cdmv2h3,1rf2b3,askscience,new,2
JSArrakis,"Some things need to be defined here first.

1. The thing you are defining as heat is the convection of atomic excitement from the air molecules around you to the molecules that make up your skin/body.

2. Everything has gravity. There is no such thing as a zero-gravity environment. It is a misnomer and a buzz word that the media likes to propagate. There are gravitational environments that are diminished (or strengthened) based on your location of adjacency and current escape velocity in relation to the object in question. For example, when you see astronauts in space that seem to appear weightless, this is just a scientific trick that scientists devised by means of calculating the speed a person or a ship needs to be to be able to move both sideways and 'down' at a speed that allows the person/ship to fall sideways around the object. This constant freefall around the object or ""orbit"" allows the person to seem weightless. If you slowed down your sideways velocity, youd start falling toward the earth, if you increased it, youd reach an escape velocity and no longer be in orbit. If you stopped your lateral velocity entirely, youd fall like a rock. 
The same goes for the sun, and all other bodies within the solarsystem. If there was no Earth, and you suddenly stopped orbiting the sun, youd fall like a rock toward the sun. If the Earth was still there and you and the earth both stopped lateral velocity, first youd fall toward the earth, because of its closer proximity, and then the earth would fall toward the sun. 
Every piece of matter in the universe has some level of gravitational pull. If it has mass, even very very small mass, it has gravity and pulls on all the things around it. 

3. Im going to assume youre talking about 'heat' in the form of convection in gasses.

The answer: Barring there are no outside influences, both gravitational and not, and in a vacuum, the gas will form a sphere due to all of the gas molecules acting upon each other. The within the sphere, the more excited molecules (the hottest) will travel toward the surface, while the least excited molecules will sink toward the middle. 

Consequently, the friction of the molecules interacting each other in the ""core"" of the gas bubble will heat them, while the molecules that rose to the surface will see less interaction and cause them to reduce their excitement and become ""cool"" again, which will make a circular flow within the gas sphere. This same mechanic is what causes wind and high and low pressure systems in weather here on earth.

Edit: formatting",null,2,cdmw6gb,1rf2b3,askscience,new,4
123STAR,"Of course not. It doesn't. ""Rising"", in this context, strongly implies a direction related to gravity. In a zero-gravity environment where would it rise to?
Instead it will go around and mix with the cold air to converge to an average temperature faster than in presence of gravity.",null,0,cdmxpkh,1rf2b3,askscience,new,2
callmecooper13,"No, heat would not rise. Heat 'rises' through a process called Free Convection. The classical example of free convection is a heated wire in completely still air. Heat 'rises' from the wire in a sort of wake (just like a boat through water) but instead this wake consists of heated air flowing through cooler air.

The reason that free convection results in hot air 'rising' is because of the density difference between hot and cool air. Hot air is less dense than cool air, so gravity pulls more on the cool air than hot air, and the hot air floats to the top of the cool air. 

In space, the gravity that pulls more on cool air would not be present, so the heat would slowly expand from the surface in all directions away from the source of heat. This obviously has practical implications in that the heat collects around the source and can cause the source to overheat. Therefore it is necessary to mechanically push the air across the source of heat in order to generate the type of air flow that would normally be present when there were gravitational forces at work.*

*Gravitational forces are always at work in orbit, but can be assumed negligible due to the control volume being in constant freefall/constant acceleration/due to the frame of reference

EDIT: Source - Purdue University BSME '13",null,0,cdn0nh7,1rf2b3,askscience,new,2
reactance_impact,"Heat does not rise, it radiates in all directions.  It is heated air that rises due to its lower air density.  Heat in a vacuum will radiate in all directions.  Just like the sun's heat can be measured in all directions. Heat is energy not matter. Therefore, heat is not affected by gravity, but affected by what is around it, that is affected by gravity.",null,1,cdn5ejj,1rf2b3,askscience,new,3
MasterDefibrillator,"well it's not exactly heat that is rising is it. It's excited air molecules that are being heated up, the more heated they become, the less dense, and so we see that the less dense air rises above the more dense air. This is what we mean when we say that heat rises and no it would not occur in a zero g environment. What you would see is a general expansion in all directions due to the expansion of air, you can see this happening in videos such as [this](http://www.youtube.com/watch?v=Q58-la_yAB4).",null,0,cdn7tyb,1rf2b3,askscience,new,2
BiggerJ,"Heat rises because things tend to expand when they heat up. Hot air is less dense than cold air. As a result, it floats. Inronically, however, things float because of gravity pulling down on denser things, because the resultant downward force on the denser objects is greater. When there's no gravity (or rather, when there is negligible gravity, aka microgravity - all mass has gravity), this doesn't happen. The upward force is a reaction to a downward force. In order for there to be 'up', there must also be 'down'.",null,1,cdna4y1,1rf2b3,askscience,new,3
vivtho,"I remember one of the Apollo astronauts describing that they didn't need any blankets to sleep in zero-G. The heat from their bodies warmed the air immediately around them enough that they were very comfortable. The only problem was that any movement would immediately destroy this pocket of warm air. 

The astronauts onboard the ISS use sleeping bags, but these are more to prevent them floating away than for insulation.",null,0,cdmmxvh,1rf2b3,askscience,new,2
iPlaytheTpt,"It's also important to make the distinction between zero-gravity and zero-G. On a space station, you're still being affected by gravity and cold will be attracted to the center of gravity. Outside of the universe is the only true place with zero-gravity, where I'm going to assume directions don't exist.",null,10,cdmnnog,1rf2b3,askscience,new,11
fameistheproduct,"Heat doesn't technically rise. In simple terms it goes from where it's hot to where it's cold. Perhaps a better way to put it, it goes from where it's hot to where it's less hot.

Heat rising in the earth's atmosphere involves a number of phenomena causing hot air to rise (you did not ask if it was hot air but I guess that's the question) which causes us to observe that heat rises. 

Heat can transfer via conduction, radiation, and convection. And these will occur in zero gravity.",null,2,cdmo0ib,1rf2b3,askscience,new,3
hylandw,"Heat as energy propagates away from the source towards a less heated environment (Assuming the source is hotter than the space around it). Heated particles move as the particles would normally, but in a more excited state. Without gravity, the particles have nowhere to go ""up"" from, and thus simply stay where they are, following the laws governing their physical properties.

Although this generally applies, the material that is heated will behave a specific way. If nothing is heated, i.e. it is just heat, the heat moves to a less heated environment.",null,0,cdmo7gw,1rf2b3,askscience,new,1
Osymandius,"Contrary to the answers below ATP **is** produced within the chloroplast. ATP synthase is located in the thylakoid membrane/space and does make use of the proton motive force generated by either cyclic or non cyclic photophosphorylation. But - the ATP produced in the chloroplast just isn't enough to compared to the amount produced in the mitochondria. We move relatively minimal numbers of protons across the membrane during photosynthesis - the really important product of non-cyclic photophosphorylation is the generation of reducing equivalents (NADPH). This can then be used to fuel the Calvin cycle and the production of triose phosphates and sugar derivatives.

Once we have produced TP/sugars, these can be metabolised to produce NADH in the mitochondria. The proton motive force produced by the electron transport chain is considerably greater, and much more ATP can be generated than relying on chloroplasts alone.",null,0,cdmrffo,1rf3cf,askscience,new,3
quantum_lotus,"As /u/Osymandius says, both organelles can produce ATP (the most useful form of stored energy for a cell), but that mitochondria are much more efficient at making it.

But there is another consideration.  Evolutionary data and model point to chloroplasts being acquired *after* mitochondria.  So the cells that eventually became the plant lineage already had mitochondria in them before they captured chloroplasts.  ",null,0,cdn40cu,1rf3cf,askscience,new,2
botanist2,"No.  The purpose of the chloroplasts is to make the energy needed for respiration, they don't have the ""machinery"" necessary to put the energy in the most usable form like what happens in the mitochondria.  Your question is kind of like asking ""Why can't the gas tank run the car?""",null,2,cdmplym,1rf3cf,askscience,new,2
ramk13,"Though diffusion is slower at lower temperatures, lowered vapor pressure is a much bigger influence. Most odors are either small solid particles or vaporized compounds. The equilibrium vapor pressure of a compound is exponentially dependent on temperature, so when it's colder a lot less of the compound gets into the air. Since it doesn't vaporize as much you smell less of it.

Also, most of the stuff you smell is more likely to be transported by convection (movement by temperature induced density gradients) or advection (forced movement) than diffusion.

For an empirical relationship between vapor pressure and temperature, you can use the [Antoine Equation](http://en.wikipedia.org/wiki/Antoine_equation) which is derived from the principles of the [Clausius-Clapeyron relation](http://en.wikipedia.org/wiki/Clausius-Clapeyron_relation).",null,0,cdmrge3,1rf3jg,askscience,new,3
stevenstevenstevenst,"At lower temperatures, vibration of particles is decreased due to the decreased energy of the system.  As diffusion of gases relies upon random vibrational motion for the even dispersal of a compound, gaseous compounds (such as any odor) will spread increasingly more slowly with decreasing temperature.",null,0,cdmp1lq,1rf3jg,askscience,new,1
Daegs,"This is not a 3D gif. 

A 3D gif would either require:

* two stereoscopic panels which you could view by changing the focus of your eyes so that the panels merge

* A single panel using red / blue shading and 3D glasses

* A single panel and special display to work along with polarized glasses.

This **non-3D** gif simply give perspective by being displayed over the ""break"" and the changing focus which cues our brain that there is 3d information being presented.

In other words, there is nothing special about 2 breaks, 3 breaks, 4 breaks, whatever.... the breaks are just used so that the gun can go ""over"" something that we perceive as flat. ",null,1,cdmtkie,1rf5et,askscience,new,6
tigertealc,"Catalysis by definition is a process by which a substoichiometric reagent promotes a reaction by lowering the activation barrier of the reaction. So that would be the common denominator, I suppose. 

Working out the mechanism of a catalytic reaction is not always straightforward. Most often, mechanisms are proposed to follow mechanistic steps that have been determined for related systems, or using intuition. But a number of different control experiments must be run to differentiate between different possibilities. Often these experiments involve the kinetics of the reaction, whether it involves determining the rate law of the reaction or determining a kinetic isotope effect. Isotopically labeled reagents can also assist, by seeing where they end up in the product. Computation can certainly aid in the assignments of mechanisms, but empiricism is the main method. And of course, the exact experiments that one is able to run to elucidate the mechanism is largely dependent upon the specific reaction. 

If you have any specific questions about specific reactions, feel free to ask. ",null,0,cdmotdy,1rf5ro,askscience,new,4
Platypuskeeper,"There is no common denominator other than the fact that catalysts catalyze. One reaction might be catalyzed by acid, the presence of H^+ , which participate in the reaction but are released on a later step. Another reaction might be catalyzed by a Lewis base, where the base temporarily donates an electron pair to a reacting atom. Those two scenarios really have nothing in common other than that they fulfill the definition of 'catalyst'. The word describes a role something plays in a reaction, but the reactions can be as different as any chemical reactions. There's no general theory of reactions either.

",null,0,cdmt6k1,1rf5ro,askscience,new,4
stuthulhu,"&gt; once you pass the outer layers of our atmosphere you are weightless - why cant we achieve that speed?

Weightlessness is a state achieved when no force other than gravity is acting upon you. When a vehicle is accelerating/decelerating, that force will be acting upon you, and you will not feel weightless. You would feel pushed against the back of the vehicle by the force of the acceleration.

The shuttle must burn fuel to leave our inertial motion, and burn fuel to match that of its destination. Being likely far more massive, both become more expensive actions, and the more fuel required to do either action increases the weight even further. ",null,0,cdmtslm,1rf5vk,askscience,new,1
WendyMouse,"The shuttle is bigger.  A LOT bigger.

New Horizons is a very light spacecraft-- about the size of a grand piano, launched from a very powerful rocket.  It was the combination of the two that made it travel so fast, faster than anything else humanity had ever launched.  New Horizons does not have enough propellant to slow itself down to enter into Pluto's orbit.  The fuel to do that would be too heavy.


Escape velocity from Earth is everything.  Humanity hasn't mastered launching a bunch of things in pieces, merging them and having another separate launch in space yet. 

Just because something doesn't have weight, (you are not in zero gravity in space, you are in microgravity), doesn't mean it doesn't have mass or momentum.

",null,0,cdnst3v,1rf5vk,askscience,new,1
Daegs,"On earth you can see million miles away yourself, right now!!! Just look at the stars.

Remember the sun is ~93 million miles away, most of the stars you see are orders of magnitude further away. 

We can see the andromeda galaxy with our naked eyes, so that is 14,696,249,500,000,000,000 miles away!

In space, you wouldn't have the atmosphere filtering photons coming from stars, so you'd be able to see even more.

This is why we have the hubble telescope in space, to avoid earth's atmosphere. ",null,0,cdmtpir,1rf86r,askscience,new,6
king_of_the_universe,"http://www.uitti.net/stephen/astro/essays/farthest_naked_eye_object.shtml

says:

&gt; Bode's Galaxy (M81), at 12 million (12,000,000) light years has been spotted by several people. This [page at SEDS on M81](http://www.seds.org/messier/m/m081.html) has a description of how to see it.

&gt; The trouble is, at Magnitude 6.9, M81 is dimmer than most consider naked eye. It depends on whose eye it is, and also where the feet are standing. It has to be an exceptionally dark sky site, probably at some altitude, at the right time of year, etc.

WolframAlpha's answer to ""12,000,000 lightyears in miles"" is 7.054×10^19 miles, which is 70,540,000,000,000,000,000 miles. (Take that, Daegs! ;)

The text also says that there could be bright events like super novas that could even be visible with the naked eye from further away for a few days.",null,0,cdncboq,1rf86r,askscience,new,2
Platypuskeeper,"The [Golden Rule](http://en.wikipedia.org/wiki/Fermi%27s_golden_rule) says that transition probabilities depend on the overlap between the initial and final states. In a Rydberg atom, you're in a highly excited state where the electron is far from the nucleus, and its overlap with the ground state and lowest-energy states is quite poor. So direct transitions back down to there are improbable. 

",null,0,cdmr8jt,1rf8ta,askscience,new,3
amvakar,"The first (and inescapable) factor in the large size of source code compared to the compiled binaries is the lack of information density inherent in any plain-text format -- you've got to keep things human-readable, which means that you're restricted to the alphabet plus enough special characters for basic formatting and organization. Each operation will involve reasonably-descriptive names as opposed to the pointers to their location in memory that the processor will see. Documentation will also be present. In short: you're describing what the computer will do so a person could understand it, while the computer will only need to be told the bare minimum about the operation to complete it. To see this in action, run the source through any compression algorithm -- the size will go down significantly.

The second factor in large software projects is the presence of code that might never actually be used. For an operating system, you'll end up with drivers for devices you don't have or support for CPUs you're not using. For applications, you might have support for different APIs or just functionality you choose not to include in the finished product. For debugging purposes there may be tests and additional information so that problems can be tracked down, and in debugging builds optimization may be turned off. 

In short: source code is far more descriptive than binary for human purposes and includes a lot of things that you may never end up using in the final build.",null,2,cdmstpb,1rf8w9,askscience,new,14
LeoPanagiotopoulos,"The limit of the situation you're describing is a ratio of 1 where the planet and moon are indistinguishable because they're the same mass. It's unlikely but possible. You're correct in your suggestion that the distance from the 3rd, larger mass in the system is important. If The distance between our twin planets (or moons? or ploons? or [manets](http://2.bp.blogspot.com/_gJ6d5yFc7fw/TL72k9N-pqI/AAAAAAAAB_I/Gbu1fWmRxPU/s400/g013v_manet_lemon.jpg)?) is comparable to the distance to the larger object in the system, their orbits around each other will be unstable. 

[Consider reading about triple star systems](http://en.wikipedia.org/wiki/Star_system#Triple_star_systems). The situation we're talking about is labeled C on the linked diagram. It's true that interactions between stars that are very close to each other can be a little more complex that cold, non-fusing rocks (planets), but in most cases the dynamics are comparable. 

Almost forgot: the 3rd object is more often smaller and orbiting the two inner objects, which are orbiting each other. Still your situation is possible. ",null,0,cdnk8oc,1rf9zl,askscience,new,2
bohr_exciton,"&gt;If we know the wavelength of a polarized photon... then why cant we determine where exactly a given photon will interact with the resist? I'm guessing something here will touch upon wave-particle duality...

Right, specifically it's the wave aspect that sets the limit. Light passing through a specific aperture or lens will not arrive in one infinitely sharp point but in a disk (e.g. the so called Airy disk for circular apertures). For far-field light, the size of this disk will be determined by a number of factors such as diffraction and the aberrations in the imaging system. The best possible case using simple far field optics is to obtain the diffraction limited spot, which is on the order of half the wavelength of the incident light. 

&gt;Part2: If we've got vapor deposition for things like gold ... why can't we vapor deposit a single atom later of the resist ... again being able to do away with the complex mask?

I'm not really sure I understand this question. Under certain circumstances it's possible to deposit metals uniformly for a desired number of monolayers. However, you need a mask if want something other than a uniform layer, e.g. patterning for an integrated circuit. ",null,0,cdmsmcx,1rfapt,askscience,new,1
stevenstevenstevenst,"The most serious affect upon the body due to exposure to lower or zero gravity is atrophy of the muscles.  As you will weigh less or nothing at all, you muscles have to work much less and thus will begin to degrade.  This is the reason individuals on the ISS need to work out regularly by running on a treadmill or though other means.

As blood circulation is negatively affected by reduced gravity (due to the way this system has evolved to partially utilize gravity in its function), other health problems may potentially be associated with manned spaceflight, such a neurodegeneration- although this research is ongoing.",null,0,cdmo0v8,1rfbi7,askscience,new,2
mzyos,"There is some worry at NASA currently about Optic nerve atrophy. This is where the nerve carrying signals for sight from the eye starts to deteriorate. It seems that about a 3rd of astronauts have this, if they have experienced long bouts of zero G. They are studying on ISS at the moment using a goldmann tonometer which measures eye pressure. They don't really understand what is going on just yet, but it might be due to the lack of gravity causing some of the eye, and it's nerve's blood supply being slowed, or stopped in one way or another. ",null,0,cdojdvm,1rfbi7,askscience,new,1
Truck43,"That's really two questions, whether or not the shell will act as a faraday cage, I'll leave to another, but, the microwaves will induce currents in the case that will produce enough heat to start a pretty serious fire, and probably cause catastrophic failure in the battery. ",null,0,cdn3oke,1rfc8w,askscience,new,2
auralucario2,"From my limited experience in putting metal in microwaves, I think that the shell itself would begin sparking, due to the movement of electrons caused by the energy of the microwaves. As for the insides, it would probably escape direct harm from the microwaves, but the heat and electricity thrown off of the casing would probably do some serious damage.

Please don't try it though.",null,0,cdnv98v,1rfc8w,askscience,new,1
Manhigh,"The only mechanism for heat transfer from the space station is through radiation.  In general, all of the electrical components on a spacecraft and solar incidence (when in sunlight) produce excess heat which needs to be shed.  If you look at a picture of the space station you'll see a series of panels that are perpendicular to the solar panels.  While it is generally desirable that solar arrays always face the sun, it's generally desirable to have the radiators edge-on to the sun, facing deep space.

Coolant passing through the radiators is cooled and then passed back inside to keep removing heat from the station.  If you wanted to heat the station, you could have the radiators face the sun slightly.

In this photo, the radiators are the white  accordion-like structures:  http://milesobrien.files.wordpress.com/2010/08/iss1.jpg",null,0,cdmsvb7,1rfc91,askscience,new,6
steeeeve,"There's no 'up' and 'down'. However, your brain is somewhat accustomed to zero-g; it happens whenever we're falling. The fluid isn't really ""floating"" because to the best of my knowledge there's no air in the part of your ear that controls balance. Rather, there's hairs in the ear that will 'flex' under a current that is induced when you accelerate. ",null,0,cdmxa5d,1rfcei,askscience,new,1
jadiusatreu,"Great answers from the beekeepers. To add a little more information apart from honeybees, not all bees make a honeycomb. Bumblebees make honey pots in which they store their honey.  These bees make a cylindrical, sometimes round, pot. Just a little more information for you.",null,0,cdmtjth,1rfcsm,askscience,new,3
HCOOH,"There are so many wild-types of bees... they don't make nests.
And the ""normal"" honeybees make round shapes, but because of the melting of these round shapes thexy become hexagonal. The whole thing is more.. a succes through error",null,1,cdmp126,1rfcsm,askscience,new,2
proule,"Curiosity drives you to ask questions, which, in being answered, can improve your chances of surviving. This ingenuity is perhaps the most evolutionarily successful means of avoiding death due to outside influence. Other evolutionary tactics would include simply being bigger than anything that could otherwise hurt you.

In animals capable of higher learning, curiosity is fundamentally a desire to learn and understand the world you interact with. At the most basic level, curiosity is important to be able to accomplish the key tasks for each living being: Survive and produce offspring.
",null,0,cdnh79b,1rfd57,askscience,new,2
spryspring,"Curiosity is a behavior that has probably been selected for in some animals by natural selection, or at least has not been selected against. Suppose that a ""gene for curiosity"" (I'm sure in reality it's not nearly that simple) arose in the ancestors of cats. Proto-cats that had this gene tended to have more offspring than those who didn't (we might guess that they, in being more curious, found more food).

Or maybe it's totally a learned behavior, I don't know. But in any case that's how behaviors can arise.  ",null,2,cdn1stc,1rfd57,askscience,new,1
xenneract,"Sure. [You can hire a plant to do it for you.](https://en.wikipedia.org/wiki/Photosynthesis)

If that's not chemical enough for you, there is also active research in making [artificial photosynthetic cells](https://en.wikipedia.org/wiki/Artificial_photosynthesis) that perform the reaction you are describing.",null,0,cdmrnvi,1rfd6j,askscience,new,6
sodium_dodecyl,"We *can*, but it's not going to be terribly efficient (or necessarily fast, I don't have any kinects data). An example of a possible pathway: Reduce [Reduce CO2](http://en.wikipedia.org/wiki/Sabatier_reaction) to CH4 + H2O, then use electrolysis to split H2O --&gt; O2",null,0,cdmrezw,1rfd6j,askscience,new,1
steeeeve,"Yes, it is possible. However, the reason we make CO2 is because reacting carbon with oxygen to form CO2 bonds releases energy. The same amount of energy has to go into the bond to break it. Since power plants are not 100% efficient (and never can be) the re-separation will always cost more energy then we got from burining the fuel in the first place (assuming the fuel is almost all carbon, like in coal)",null,0,cdmxv3u,1rfd6j,askscience,new,1
kyaj21,"Technically, yes. CO2 is just 1 part carbon, 2 parts oxygen, as any school child who has taken introductory chemistry would be able to tell you. Yes, we could extract oxygen from the carbon dioxide, but the carbon would still be there. Reducing carbon emissions is a whole other matter, as in order to reduce carbon emissions, we would have to change the fuel sources or at the very minimum how we process them, and what we would do with the carbon once we extracted the oxygen from the carbon dioxide.",null,5,cdmrfta,1rfd6j,askscience,new,1
omgdonerkebab,"It's just a convex mirror.  The mirror is curved toward you, so that the rays of light that get to your eye come from a larger angle.  (Kind of like [this image](http://0.tqn.com/w/experts/Physics-1358/2009/06/Convex-Mirror.jpg), but with the directions of the arrows reversed.)  

This allows you to see a wider angle of stuff behind you, which has its obvious uses when driving.  But it also means that this larger angle is squished into a smaller area on the mirror, so the objects look smaller on the mirror.  Your brain might interpret it as the object being farther away, which would be wrong.  The object is closer than it appears to be.",null,1,cdmpcpr,1rfd8w,askscience,new,7
botanist2,"For the sake of reference [here's](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0078092) the original article about these stone-tipped spears that you're asking about.  To clarify, they're not talking about aliens using spears, they're talking about different species of *Homo* (e.g., we're *Homo sapiens*, they could be talking about *Homo erectus*).  These spear tips were dated from the substrate in which they were found, they weren't dating the specific material of the spear (which could be much older). 

There's lots of ways to date different materials and the choice depends on what you're trying to test.  Carbon-14 is used predominately for organic materials, the article in question used [argon-argon](http://en.wikipedia.org/wiki/40Ar/39Ar) dating that is good for dating metamorphic and igneous minerals.   

&gt;When it comes to radio active decay, does it magically start over when you shape the object?

Not when it comes to minerals.  Radioisotope dating methods for minerals can only give you an idea of how long its been since the last time they cooled below their closure temperature (the temperature at which its assumed that isotopes aren't flowing in and out).  They tested the age of the substrate where the spear points were found because they wanted to know how long it had been since the points came to rest in that spot (and presumably when they were last used by their owner), not the age of the stones that were used to create the spear points.",null,0,cdmtews,1rfd9c,askscience,new,5
descabezado,"For radiometric dating in general, the clock starts once the object stops exchanging atoms with its surroundings.  For rocks, this means when the minerals of interest crystallized; for organic remains, it means when the creature died and stopped taking in air.  So, what they probably mean here is that the spear handle is made of wood that died 500000 years ago.  You are correct that dating the stone spear head would not be useful.

An interesting consequence of this is that you have to be very clear about what you've dated.  If you date pages of a book to be 2500 years old, it means the paper is that old, not the writing on it.  If you date a sedimentary rock to be 200 million years old using U-Pb dating with zircons, it means that the zircons were eroded out of 200 Ma old crystalline rock, but the timing of their erosion and deposition (i.e., the age of the sedimentary rock) could be any time between 200 Ma and yesterday.",null,0,cdmt83q,1rfd9c,askscience,new,3
tin_can_conspiracy,"Artifacts such as these are usually only dated by how deep they're buried (similar to how we date the dinosaurs) or by carbon dating artifacts found in the same site as the stone tools (wooden spear handles, bones, and such.)",null,1,cdmpbgx,1rfd9c,askscience,new,2
iorgfeflkd,"In [this](http://iopscience.iop.org/0143-0807/16/4/005) paper, they measured how likely it is for toast to land on the buttered side down, and found it was 62% (with thousands of tests), significantly more likely than random chance.",null,3,cdmozzv,1rfdo6,askscience,new,8
null,null,null,0,cdmubzf,1rfdo6,askscience,new,1
atomfullerene,"Murphy's Law is usually phrased ""Anything that can go wrong, will go wrong, and at the worst possible moment"".

It's meant to be taken tongue-in-cheek, it's not a physical law, but somewhere between a joke and a superstition.  If it was literally true, we'd all be dead. But it does have some level of validity, especially in the engineering context it was invented for.  Complex machines have lots of parts, and often only work right if _all_ the parts work together properly.  The probabilities that each part will fail get multiplied, making it more likely that something will go wrong.  And parts are more likely to fail under stress, which means while the machine is operating--often the worst time.  Eg, it's much worse if the wings fall off your test plane in the air than if it's sitting on the ground.  ",null,0,cdngwpa,1rfdo6,askscience,new,1
Jetamors,"We've known about cancers for a very long time. [The oldest known description is Case 45 from this Egyptian papyrus from 1600 BC](http://archive.nlm.nih.gov/proj/ttp/flash/smith/smith.html), though I don't think it theorized about the cause. There's a great article about old Greco-Roman treatments [here](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC2820670/). According to the article, the oldest known theory about cancer (written by Galen) attributed tumors to an accumulation of black bile, due to the black veins that appear around many tumors. Galen was working off the [four humors theory](http://en.wikipedia.org/wiki/Four_humors), which was predominant in Western medicine from antiquity to about the 1800s.

Edit: I should correct myself, Galen's theory is the oldest one in the *Greco-Roman tradition*. I don't know much about medicine in other cultures, but I wouldn't be surprised if they (China particularly, but probably others as well) theorized about the origin of tumors at about that time or earlier.",null,0,cdmt0zr,1rfdsx,askscience,new,13
iorgfeflkd,"There's a way of approximating functions called a Taylor series, where you add up diminishing terms with higher and higher powers. For example, the cosine of x can be approximated as 1-x^2 /2 + x^4 /24 - x^6 /720 ...

The tangent is the ratio of the opposite and adjacent sides of a right triangle, and for a 45 degree angle the tangent is 1. 45 degrees in radians is Pi/4. This means that the inverse tangent of 1 is Pi/4.

That series for Pi is based on the Taylor series of the inverse tangent function, substituting x=1 so that it equals Pi/4 (x=1 greatly simplifies the math because 1^anything is 1).

So basically, it's another way of saying that the tangent of 45 degrees is 1.",null,0,cdmpzwi,1rfdy0,askscience,new,15
keithb,"You are short-sighted. I can tell becasue your glasses have ""negative"" lenses, which cause a beam of light passing through them to diverge, to spread out. You can see the light which has been diverted in the brighter halo around the shadow of your glasses. If you were long-sighted you would glasses with lenses which are ""positive"", or converging, and there would be a bright spot in the middle of the shadow of the lens rather than a bright rim. 

The soft shadow of the lens appears darker than the carpet around it because the light passing through the lens is spread out over a larger area. The lenses will absorb a little bit of the light passing through them, but mainly they redistribute the light. That's what lenses are for.",null,1,cdmqwmz,1rfepp,askscience,new,18
Infinite_Ambiguity,"If galaxies are close enough to start with (as in clustered together, relatively speaking), then there's sufficient gravitational force between them to bring them together and to overcome inflation/expansion.  

To use an extreme example, inflation/expansion doesn't tear the earth apart, or the solar system apart, our own galaxy apart, or any other individual galaxy because the gravitational fields win each such cluster is sufficient to keep everything together.  Same concept between galaxies that are relatively close together.  

Many cosmologies believe that, I. Billions of years, our night sky will be totally dark and telescopes will be insufficient to see anything, except for the galaxies in our own cluster (which, I think, total something like 36 total galaxies).  ",null,0,cdmovvo,1rff0z,askscience,new,5
DarkLather,"Galaxies exist in groups. Galaxies within the same group can be gravitationally bound to each other. They can orbit each other and collide. Our Milky Way and the Andromeda galaxy, both members of the ""Local Group"", are currently on a collision course. ",null,0,cdmqwqr,1rff0z,askscience,new,2
atomfullerene,"Height is  highly dependent on the amount and quality of food one receives as a child.  Poor farmers are often quite short.  People living in modern countries with plenty of food are taller.  Interestingly, skeletons of hunter gatherers before the dawn of agriculture were also often quite a bit taller than their immediate farmer descendants (though height does recover in the farmers somewhat over time) owing to the better nutrition of the hunter-gatherers as compared to the early farmers.

Farther back in prehistory early protohumans were often shorter than modern people.",null,0,cdmt3vc,1rfff0,askscience,new,12
dontgothatway123,"In a specific practical sense when actively measuring the cardiac output (CO) of a person it is important to factor in the persons size.  This makes the CO calculation more relative.  For instance the average CO for a adult male is 5.6L/min (the volume of blood ejected from the heart every minute).  Now we'll introduce two people. One man is 5'2"" (157cm) 105lbs (47kg) with a BSA of 1.44m^2.  The other is 6'6"" (198cm) 285lb (129kg) with a BSA of 2.67m^2.  If we just considered CO (stroke volume x heart rate) would it make intuitive sense that if both of these individuals had a CO of 5.6L/min that would be ok for both?  No, some form of individualization is necessary.  This is obtained by taking the CO and including the BSA into the calculation.  This measurement is called the cardiac index (CI).  Clinically/practically it serves a better purpose and indicator for monitoring hemodynamic states in controlled situations.  Using the examples above the first man would have a CI of 3.89L/min/m^2 and the second man would have a CI of 2.1L/min/m^2.  Considering the normal CI ranges from 2.6-4.2L/min/m^2 the man in the second scenario is about a hairs breadth away from cardiogenic shock.

Hopefully that helps shows the significance of BSA inclusion within a certain situation.  As for whether there are better alternative parameters I am unsure. In research you tend to see body measurement index (BMI), ideal body weight (IBW), lean muscle mass calculations, body fat percentage (BF%), and body surface area (BSA) measurements used a bit.  Each has their own benefits and pitfalls.",null,0,cdn0ahy,1rfg3b,askscience,new,2
Trill-Nye,"In this case, it's better to think of color as a result of light absorption and emission, rather than reflection. When light hits a gas, it can be absorbed by various processes. Visible light just happens to be the right energy to excite the electrons bound to atomic nuclei in some molecules, such as those making up chlorine gas. These excited electrons, which have been given energy by a photon, then relax to their original energies, giving off new photons of a particular wavelength (and therefore color).

Electrons are unusual in that, due to quantum effects, they can have only certain discrete energies. This is determined by the structure and composition of the atom, and its interactions with other nearby atoms. Gasses that are not colored do not have electron excitation mechanisms of the correct energy to be excited by visible light, then give off light of a specific color.

If a gas were black, it would have to absorb most incoming photons, then give off accumulated energy as something other than visible light, such as photons of a wavelength that cannot be observed by the human eye. ",null,0,cdmsy9a,1rfggp,askscience,new,4
AznInvasian,"In easier terms to understand:

     Light wave goes into gas atom, energizes an electron and pushes it to a higher energy orbital. The electron doesn't like this, and returns to its original orbital, emitting that same amount of energy it absorbed. This makes it glow this specific colour (corresponding to the wavelength of light it absorbed).",null,0,cdnddj3,1rfggp,askscience,new,2
CoryCA,"Only in that all life on Earth is related, and that they are both plants. (Though a mango stone reminds me more of a peach stone.)

A pumpkin is a squash variety of the species Cucurbita pepo of the family Cucurbitaceae. Acorn squash are also a C. pepo variety (the species is highly variable), zucchini are a different species in the same genus, and watermelons and cucumbers are part of the same family.

Mangos of genus Manifera of the family Anacardiaceae which also includes cashew, poison ivy, sumac and pistachio.",null,0,cdmrzhg,1rfgwc,askscience,new,2
proule,"There are many examples of ""convergent evolution"" in the world. That is, evolution that has caused very distantly related organisms to take on a similar appearance in some fashion. Another example of this is flying insects, birds and bats. Obviously you can see a large difference between insects, birds and bats, but birds and bats may seem like they're more related than not, right?

Birds are more closely related to reptiles than bats, which are mammals. If you look at the parts of their bodies specialized for flight, they *look* similar at a base level, but: A bird's wings are modified forelimbs (arms), and they still have distinct, separated feet. A bat's wings are a leathery extension of skin that stretches between the modified forelimbs, and actually reaches down to the legs. [Here's a picture to illustrate my point](http://upload.wikimedia.org/wikipedia/commons/3/38/Homology.jpg).

In biology, function is very tied to structure. Two structures can evolve to look very similar based on sharing the same end function, however, this does not necessarily imply a close relation.",null,0,cdnhdlw,1rfgwc,askscience,new,2
Weed_O_Whirler,"You would barely notice a difference. 

The main reason the magnet in the motor needs to keep being pushed isn't due to friction, but due to [Back EMF](http://en.wikipedia.org/wiki/Counter-electromotive_force) force. When spinning the magnet in the coil, a current is produced in the coil, and a counter-emf voltage opposes the current. These will always be of the same amount of energy- thus even without friction a magnet will very quickly slow down, as you will not be able to extract more energy from the magnet than you used to get it spinning in the first place. 

It is good to think of how these generators are not ""making"" energy, they just ""convert"" it. So, we burn stuff in order to move pistons, the moving piston spins a magnet, and the moving magnet makes electricity. Even without any friction or losses in the burning process, you'll never get more energy out of the generator than you put in. ",null,0,cdmqz7c,1rfhiw,askscience,new,7
florinandrei,"Any battery has an internal resistance. Any resistance, when a current passes through it, it heats up. Therefore, a battery will heat up (or at least become a bit warmer) any time you either charge it or discharge it.

The higher temperature of a charging battery is not an indicator of it charging, it merely indicates that some current is passing through it. But same would happen during discharge.",null,1,cdmrmjz,1rfi8e,askscience,new,4
Guanglais_disciple,"The chemical reaction is endothermic and then exothermic (li-ion for example) but the joule heating (current ^ 2 * resistance) usually dominates. Since joule heating isn't a function of current direction, you see heating in both cases. For very low current, though, the chemical reaction dominates and it cools slightly. ",null,1,cdmtg6y,1rfi8e,askscience,new,4
dudds4,"It would be interesting if that was the case, but no. It'll help to understand why there is heat produced.

Basically the transfer of energy into the battery is not perfectly efficient. some energy is lost. Where does it go though, ( energy can not be destroyed) ? Heat, among other things, is the answer. 

Imagine a flowing stream of water. Some of the water laps up on either side of the stream, and gets absorbed by the land. Not all ( although nearly all) of the water makes it down the stream. Here the water getting absorbed is what we observe as heat


Same goes for discharging energy, it's just another form of energy transfer, and not perfectly efficient

",null,0,cdn4tzd,1rfi8e,askscience,new,2
polkasalad,"On discharge the batteries heat up due to the internal resistance.  Internal resistance increases as temperature decreases as well, which is why batteries last longer near room temperature, so if it were to cool the battery would actually lose capacity faster as you used it. Consequently, heating up the battery too much will damage the cell.  

I'm sure someone can offer more info, I'm in a graduate course relating batteries to hybrid-electric cars right now which is where I got my info from. ",null,2,cdmpz1e,1rfi8e,askscience,new,2
botanist2,"III&gt;Trees and plants existed millions of years before the first oxygen producing creatures

Photosynthetic organisms (mostly cyanobacteria that form [stromatolites](http://en.wikipedia.org/wiki/Stromatolites)) existed millions of years before the first oxygen producing creatures, but trees and plants as we know them today didn't evolve until much, much later.  

As to the rest of your question, there are a lot of other ways to make CO2 than just living organisms and one of the most likely sources of CO2 was volcanic activity.",null,2,cdmq5zo,1rfia5,askscience,new,10
sparky_1966,"I think you meant before the first oxygen consuming creatures. Trees and other plants weren't around for a long time after the start of making oxygen. The first photosynthetic organisms were single celled. When photosynthesis started, the atmosphere was thought to be a reducing atmosphere, so the excess oxygen taken up by iron and made rust, and there was a lot of methane that UV light made into CO2. Carbon was not necessarily limiting, since all the carbonate (limestone) had yet to form, and the oxygen comes from splitting water molecules. The oxygen cycle today is not necessarily the oxygen cycle at the beginning. ",null,1,cdms085,1rfia5,askscience,new,2
foamster,"Well, volcanic activity alone 'produces' a *lot* of atmospheric CO2. 

My understanding was that the atmosphere had very little oxygen initially, but plenty of CO2 at around the time that photosynthesis began to take off. Animal life wasn't really able to develop until the atmospheric oxygen concentration was high enough to allow for sufficient metabolic rates -- oxygen produced almost exclusively by algal photosynthesis.",null,1,cdmscd2,1rfia5,askscience,new,1
iorgfeflkd,"It's an amorphous solid, which basically means that it behaves like a solid (as most people would interpret them) but the atoms aren't arranged in a crystal lattice. This makes a difference if you try to measure heat transfer through the material, for example, or look at the diffraction of x-rays through it.

[Diagram](http://www.steelguru.com/uploads/reports/sss1-29-08-2008.jpg)",null,0,cdmq2n8,1rfih8,askscience,new,11
gfpumpkins,This isn't really an answerable question. The normal bacteria found in humans is incredibly unlikely to be pathogenic to ants. ,null,0,cdmr746,1rfix6,askscience,new,3
proule,"Your question seems to be based around the assumption that humans are bigger than ants, thus, our bacteria must somehow be stronger than the bacteria that colonize ant bodies.

There's no fundamental difference between bacteria that colonize ant bodies and those that colonize human bodies. Human bacteria don't need to be ""stronger"" to colonize humans; they're adapted to colonize humans just as bacteria in ants are adapted to colonize ants.",null,0,cdnh2jd,1rfix6,askscience,new,2
Osymandius,"You're right - there are lots of ways to kill bacteria. Antibiotics are selective ""weapons"" against bacteria which is why they're so important. Because they're specific to bacterial components, they're safe to give to patients without destroying their own cells.

Let's take another example of a way to kill bacteria: heat. Most bacteria give up at about 50/60^o C, some thermostable bacteria (see T. aquaticus) are good for a bit more - up to 85/90^o C. Yes - all antibiotic resistant bacteria will be killed by a 100oC burst, but then you've got the put the patient through that! 

Take any method that will kill bacteria that isn't antibiotics, and it'll probably do some damage to the host. Irradiation, particulate disruption, salt membrane disruption, electrostatic membrane disruption, intense dehydration etc.",null,1,cdmradq,1rfk3g,askscience,new,10
thedveeeee,"There's actually only a fairly limited number of ways to kill bacteria. To list a few, you can kill them through targeting protein synthesis, targeting DNA replication, and using cell wall synthesis/growth inhibitors. Some newer antibiotics are being produced that target ATP synthase, an enzyme that produces ATP for the bacteria.

Unfortunately, the specificity in these antibiotics lies in the fact that we can't administer compounds that are toxic to human cells. Many antibiotics (like methicillin) are mildly toxic to us so they must be modified. That being said, it takes years and millions of dollars to come up with solutions to these problems. 

Edit: To touch on antibiotic resistance, and this is a very simple explanation; when bacteria are exposed to sublethal doses of antibiotic, selective pressure can cause a change in their genome, in which the most advantageous traits are passed on. This leads to strains of bacteria that are resistant to antibiotics, and these bacteria can pass their advantageous genes on to other bacteria. You may have heard of the incredibly famous MRSA group of bacteria; Methicillin Resistant Staph Aureus. This is a strain of Staph aureus (a natural flora of bacteria found on your skin; it's very common) that has evolved to resistant methicillin antibiotics. ",null,13,cdmsn6g,1rfk3g,askscience,new,21
justin3003,"The big problem is that there are only so many ways to attack bacteria effectively. Many of our antibiotics center on attacking replication or protein synthesis, two areas of significant difference between humans and bacteria. This makes most modern antibiotics much less toxic to humans than they are to bacteria. Also, some bacteria are totally resistant to many antibiotics simply by their biology (ie. the drug cannot interact with it, etc.), limiting the available options to only a few drugs.

Unfortunately, because we only have these limited points of difference, antibiotic use over time tends to lend itself to the selection of bacteria that are not able to be killed by these mechanisms. As these elements become more resistant, we have more and more limited options to further address this problem. It is further compounded by the fact that antibiotics are not specific to the pathogen you are trying to treat; to eliminate one infectious pathogen you bathe all of the other bacteria in your body with the same drug. Thus you don't just drive resistance of pathogenic bacteria but also harmless bacteria in your body that, under the right circumstances, may become harmful. 

So, to get to your question, that is why we are terribly worried about antibiotic resistance. Bacteria are a constant presence in the environment and evolving faster than we can create effective, tolerable treatments.",null,10,cdmsg9e,1rfk3g,askscience,new,12
fazedx,"The most difficult part of drug design and discovery is to kill the thing you want to kill without harming ""healthy"" cells in the body. Most anti-bacterials are beta-lacatam antibiotics. That means they work by interfering with the building of the cell wall of bacteria. To put it simply, it disrupts penicillin binding proteins that are necessary for cross-linking of bacterial cell walls (kind of like the mortar in brick and mortar - without the mortar, the wall would not hold). Without the ability to reconstruct and expand cell walls, bacteria cannot grow or reproduce.

beta-lactam antibiotics work because they have similar structure to the penicillin binding proteins, but do not actually hold cell walls together. The bacteria use these to make their cell walls, but because they don't hold, the cell wall breaks down. It's kind of like giving a bricklayer sand instead of mortar to build a house. 

Some bacteria can produce beta-lactamase, which cleaves the beta-lactam ring and renders it ineffective. ",null,0,cdmv9no,1rfk3g,askscience,new,2
foamerc,"The short answer is there are many ways to kill bacteria, but few that discriminate between bacterial and human cells. Bacteria are cells too, and  they share many similarities with human cells, and a few differences here and there. Antibiotics exploit such differences such as bacteria have a cell wall and human cells don't. 

When discussing about killing them after they've infected someone within the body, you're pretty much left with antibiotics, which there are many subtypes working in different manners, but for all intents and purposes are chemicals ingested/injected into a human for the purpose of killing specific bacteria.

In addition you don't want to indiscriminately kill off all bacteria because that's how you select for resistant organisms, kill off normal helpful bacteria, and some nasty ones grow in their place. Look up C. difficile infections - a relatively new cure is to eat processed shit of other people.",null,0,cdo6tkv,1rfk3g,askscience,new,1
iorgfeflkd,Its engine was cut off a long time ago. It is on a trajectory that takes it beyond the solar system.,null,2,cdmr7xs,1rfk5l,askscience,new,30
Gprime5,"I think you might have misinterpreted something because your description doesn't make sense.

Voyager 1 doesn't have any actual engines, only small thrusters that keep it pointed towards Earth. The craft is in a hyperbolic trajectory meaning it has enough velocity travelling away from the sun that it will never come back.",null,1,cdmrakq,1rfk5l,askscience,new,10
PorchPhysics,"http://www.jaymaron.com/asteroid/tour-l.jpg

As the others said, its on a hyperbolic path out of the solar system.  This means its its velocity is greater than or equal to the escape velocity required for the sun.  

As for your idea that ""we're always moving around something"" is not really true at all, but in the case of voyager, it is now and interstellar probe, no longer orbiting our sun or being considered part of our solar system, it now orbits the galactic center.",null,2,cdms1nn,1rfk5l,askscience,new,10
Ocaiman,"No, plants cannot survive without oxygen.  They respire on O2 just like any other living thing.  O2 is a byproduct of energy production using photosynthesis and plants eventually give off more O2 than they take in to breath.

To your question, a plant needs oxygen to germinate and grow until it begins photosynthesis.  They do not store O2, thus they cannot live in an atmosphere of CO2 or they would suffocate (the O2 would diffuse into the environment).  They can live in a clear sealed container with access to light, as they continuously reuse the CO2 and O2 that was sealed in with them, but they reach a limit in growth.",null,0,cdmsibb,1rfkeu,askscience,new,5
chrisbaird,"Not enough to notice. You can test whether gravity has any noticeable effect easily. Pluck a guitar string whole holding it upside down and see it sounds any different from when plucking it upright. The relevant force for these instruments is the tension in the strings and drum membranes, which is enough stronger than gravity that you can ignore gravitational effects. 

Note you only asked about lack of gravity. I am assuming you mean there is still normal air pressure provided by a pressurized compartment. If there were no air, or lower air pressure, than that would definitely effect sound propagation. ",null,0,cdmt7yy,1rfkjv,askscience,new,3
lvachon,"An acoustic guitar has been on the ISS for a while.  According to Cmdr Hadfield, the only thing that required changing was his play style since he no longer had the weight of his arm to help move down the fret board.

Source : http://www.youtube.com/watch?feature=player_detailpage&amp;v=gWTndmDHZQc#t=59",null,0,cdn9no2,1rfkjv,askscience,new,1
openLIKEeuchromatin,"The WHY part of the question:
First think of it in terms of fitness (this is always a good idea when navigating through these types of organismal biology questions). The number one goal for life from a biological perspective is to reproduce and pass on your alleles. With that in mind, try to think of why these birds have all grouped together and are ""chatting"" away. Keeping fitness in mind (#1 goal in life is to reproduce) you know that the grouping and social communication behavior of these birds must be important in order pass on their alleles. Since these behaviors (phenotype) are important to the survival of the crow species, then they must have evolved via natural selection.

The HOW part of the question:
Birds calls have evolved for millions of years acted on by natural selection. The chirps, coos and shrieks you hear everyday are a product of that. Many birds have developed a communication system that allows them to recognize individual calls within that population. Much like humans can tell the difference between each others voices. Look at it from the birds perspective. Birds have very sensitive ears and a respiratory system with many airways that allows them to make the complex calls. Try not to fall into the ""anthropomorphism trap"". A large crowd of crowing birds in the eyes of a crow is very different than in the eyes of a human. A ""noisy crowd conversation"" from a humans perspective is loud and hard to decipher what an individual is saying (i.e. sporting events, concerts, etc.). This is not the case for birds. Some birds are able to pick up on some of the slightest changes in frequency to hear exactly who is calling and what the call is about (i.e. food, mate, predators, etc.). Many birds have a critical period during development where they learn specific calls usually unique to that  population. Some bird call are even genetically ""hardwired"" and do not require learning. Not all birds are social though and communication does vary from species. 

Last point:
Calling and crowing is not the only way birds communicate. A wide range of behavioral displays are used in junction with the calls in order to send a complete message to another bird (the receiver). 
",null,0,cdn0pf1,1rfkwr,askscience,new,3
Doener_wa,"I can tell you something about the Langmuir isotherm. To get to this equation you have some asumptions to make: first is you have an isotherm system, which means your temperature is constant and second your gas which will be adsorbed formes a mono-layer on your surface (there are equations which involve multi-layer adsorption). Therefor you get the coverage of your surface and your Langmuir-isotherme can describe how much you may adsorb until your surface is fully covered. Also Langmuir isotherms are used to describe how well an adsorber adsorbs a specific gas or a mixture of gases (all will adrob differently). This is very useful because you are now able to characterize reactions which are done using a (heterogene) catalyst or to cunstruct an adsorber like it is used in many chromatographie-applications. 
To your Freundlich isotherm: I think this must be a similar concept just using other asumptions.
I don't want to go in detail now, if you have further questions, just ask on.
Source: I am a graduated chemical engineer and I am currently visiting a lecture about adsorption.",null,0,cdniulx,1rflnd,askscience,new,1
creepy_old_grampa,"Police Radar is tied to their speedometer and decremented from the total, Source, I used to convert old cop cars to taxis, and I could always find the speedometer signal wire spliced under the dashboard already when I went to put in a meter.",null,0,cdmttof,1rfltg,askscience,new,6
EpicEvslarg,"So a car is travelling at 100 km/h North

A police car is travelling at 100 km/h South

The relative velocity would be 200 km/h

So the police radar would either know what speed the police car is going at, and automatically calculate the velocity of the other car, or the policeman would have to do it in his head by looking at the radar, and his speedometer. 

In this example the radar would either say 100 km/h or 200 km/h, so it would be easy to calculate.

I hope I solved your question.",null,0,cdmsce3,1rfltg,askscience,new,3
Dyolf_Knip,"That's just a matter of subtracting out their own velocity.  The real problem is angles.  If you were traveling at 100 mph perpendicular to the beam of the radar gun, it would register your speed at basically zero, because it only measures relative velocity along the path of the beam.  Any deviation from that decreases the measured speed.  So what cops do is position themselves as much as possible such that are directly in the path of incoming traffic.  I.e., right at a bend in the road, on an overpass, etc.

Area radar systems get around that limitation by being smart.  The radio beam can't really tell you how fast something is going, but it can tell you where it is.  An attached computer says ""5 seconds ago it was there, now it's here, x miles away, ergo it's moving this fast"".",null,0,cdnjjys,1rfltg,askscience,new,1
steeeeve,"If you had a rigid bottle, a difference in pressure would build up as you travel further into the ocean. This difference in pressure will cause greater net forces on the water at the mouth of the bottle, causing it to enter the bottle more rapidly. The amount that this happens will depend on some complex fluid dynamics, as the air needs to leave the bottle as well as the water entering it. 

If the bottle were pressurized with air so that the pressure was at equilibrium between the inside and outside of the bottle at the bottom of the ocean, then only the difference in buoyancy will cause the bottle to fill, similar to the case of a few feet of water. In this case, the amount of time would be similar for both cases, though perhaps not exactly the same due to changes in the viscosity of the water and air at those pressures.

The collapse of an air-filled bottle would depend on what kind of bottle is being used. For a typical soda bottle, the bottle can be collapsed just by sucking the air out of it (say, using your lungs). This means that a pressure difference of less than one atmosphere will cause the bottle to begin to crumple. The pressure increases with depth at ~1atm/10m of depth, so the bottle would crumple long before reaching the bottom of the ocean.",null,0,cdmxqyd,1rfo1d,askscience,new,3
chrisbaird,"You seem to be confusing length (m) with acceleration (m/s^2) which are different things. If gravitational acceleration is very small, that does not imply there is anything in the system with a small length scale. It just means the gravity is very weak. Quite the opposite case is more important actually: quantum effects and gravitational effects should intersect when there is a large amount of gravity in a very small volume (such as in a black hole).",null,0,cdmt1ks,1rfo90,askscience,new,1
Surf_Science,"Everything in your cell is doing what is thermodynamically favourable. Proteins involved in transcription bind to a gene because that binding is favourable, they function because that is energetically favourable. The produced proteins bind each other causing actions that occur because those are also energetically favourable. ",null,0,cdms0zn,1rfooz,askscience,new,3
sparky_1966,"DNA alone can't determine what a cell does, you can think of it as storing information. That information can be turned into RNA, some of which regulates genes, proteins and a few specific reactions. The proteins handle most of the actual work.

So, in the simples example if a bacteria that can use multiple sugar types for energy is sitting in an environment that has no lactose sugar, it usually doesn't waste energy making enzymes to break it down. If suddenly lactose becomes available, a receptor protein can bind the lactose and either activate transcription of lactose digesting enzyme from the DNA, or more commonly in bacteria, change shape and fall off the DNA, allowing the gene to be expressed. As the enzymes break down all the lactose, eventually the receptor protein wont have any to bind to, and will switch shape again to turn off the gene. There are many other levels of regulation, but that's the simplest example.

As far as viruses, there are a number of different strategies they use to take over a cell. Almost never is it just a piece of naked DNA floating around, since the environment and cells are full of enzymes to destroy those fragments. Probably the easiest system to understand is a DNA virus with a protein coat. The protein coat protects the DNA, but also makes sure it gets delivered. The protein is usually shaped to bind to the bacteria or cell it infects. On binding, the proteins change shape and make a path through the cell membrane for the DNA to get in. There is energy stored in the shape of the protein and the winding of the DNA (taken from the last cell) that allows injection of the DNA without other sources of energy. Once in the cell, the DNA gets replicated and transcribed in to viral proteins and more viruses like any other DNA. That's the simple version, there are any number of different virus types, some use DNA, some RNA, some large viruses carry most of the proteins they need to begin replicating so they can shut down most of the hosts protein production, etc.  ",null,1,cdmsir9,1rfooz,askscience,new,1
darksingularity1,"Technically it's not DNA. That determines what a cell does. Think of it as a master blueprint for a house. It contains a great idea, but it's not actually contributing to the building of the house. The workers (proteins) are who/what do everything. The architect might be the direct liaison to the blueprint. He reads it and converts it into instructions for a worker function. Technically new workers are created in the analogy sense, but I'm sure you get what I mean. The proteins are what actually create changes in the cell. In fact, certain proteins even act on DNA to control the expression of other proteins. The DNA does nothing.",null,0,cdndkh6,1rfooz,askscience,new,1
aziridine86,"Wikipedia is an OK place to start, but I believe that the most basic answer to the 'why' question is this:

The hydrocarbons that we get from the earth come in a huge variety from gases like methane, ethane, and propane, all the way to thick waxes and tars. 

Because of the prevalence of internal combustion engines used in cars, trucks, planes, etc., we have a much higher demand for gasoline, diesel, and jet fuel than for other hydrocarbons which are heavier or lighter. 

Cracking is one way we can turn less desirable hydrocarbons like high-boiling petroleum into more desirable products such as those used in gasoline. 

If your talking specifically about using kerosene as the feed stock, then the products will contain larger amounts of small (C2-C5) products. For example, [this](http://pubs.acs.org/doi/abs/10.1021/i200024a026?journalCode=iepdaw) paper (full text not free) says that cracking of kerosene yielded significant amounts of ethene, propene, butene, and butadiene. 

These chemicals have many different uses, but a major use of this class of chemical (often called olefins) is to make plastics like polyethylene and polypropylene. ",null,0,cdnc9mz,1rfpcs,askscience,new,3
sf_torquatus,"The products of catalytic cracking are smaller hydrocarbons. The catalyst (usually a strong acid zeolite or precious metal) cleaves the C-C bond. You will find a distribution of products corresponding to the temperature, pressure, and catalyst. Kerosene itself is a product of catalytic cracking. One would want to crack it further to produce smaller hydrocarbons. 

Regarding the ""why"" - I'm a bit sketchier on these details, so I'd ask anyone with a better understanding to pitch in. Kerosene is used as jet fuel, and I found a patent that described cracking kerosene to yield gas-phase products, but I don't understand the advantages of such a process versus fuel injection, unless such a process improved the injection in some way.",null,0,cdmu5xz,1rfpcs,askscience,new,2
hikaruzero,"It's pretty simple -- photons alone aren't the cause of attraction/repulsion.  It is the *fields themselves* that cause charged objects to attract or repel eachother.  Photons are created when charges accelerate, but if you have a bunch of stationary charges and no actual photons, those charges will still begin to accelerate and attract or repel eachother without emitting or absorbing any photons amongst themselves.

In the context of perturbative theories, this effect can be explained by saying that the vacuum is filled with virtual photons, and that the virtual photons end up exerting a force on the stationary charges.  But virtual photons are not detectable the way real photons are, and also virtual particles do not appear in non-perturbative treatments of electrodynamics, so it is something of a matter of debate whether they even exist at all (indeed in the theory of the strong force, perturbative calculations frequently end up being *wrong*).  Virtual particles can be thought of as simply a mathematical tool for calculating approximate answers -- it's best to just say it is the *fields* that cause charges to accelerate.

Now, real photons themselves are *disturbances* of the fields, and if the fields change, that will cause a change to the acceleration of a charged object, so real (detectable) photons *do* accelerate charged objects, but strictly speaking it is the field that is ""doing the work,"" the presence of photons isn't necessary for attraction and repulsion -- it's not like there have to be a bunch of photons flying around from one particle to the next in order for charged objects to accelerate (though if they are flying around and being absorbed or emitted, they will change how those charged objects are moving via transfers of momentum).

So it's the fields that do the acceleration, whether you want to interpret fields as being made up of virtual particles is something of a matter of philosophy, and not something that experiment can tell us is definitely true or false.

Hope that helps.  Some further (but more technical) reading:  [Wikipedia:  Static forces and virtual-particle exchange](http://en.wikipedia.org/wiki/Static_forces_and_virtual-particle_exchange) and [Wikipedia: Force carrier (particle and field viewpoints)](http://en.wikipedia.org/wiki/Force_carriers#Particle_and_field_viewpoints)",null,0,cdmsq9j,1rfqqd,askscience,new,5
siliconlife,"Actually what you suggest does happen, but it's not called subduction because continental crust is too buoyant to descend into the mantle like cold ocean crust.

The Himalayan orogeny actually is so intense that a process called [underplating](http://www.sciencemag.org/content/325/5946/1371/F2.large.jpg) actually takes place. Underplating is the positioning of crust or magma beneath an overriding crust. In the case of the Himalayas, the Indian continental crust is being thrust so strongly that it ends up completely beneath the Eurasian crust. [Link to paper](http://www.sciencemag.org/content/325/5946/1371.abstract)",null,0,cdmvwch,1rfs95,askscience,new,6
oloshan,"The Indian plate is indeed being subducted under the Eurasian plate. The Himalayas are the uplift of Eurasian crust, not Indian crust - although their elevation is certainly enhanced by the effects of having the Indian plate shoved beneath the Eurasian at the same time. But not only was the Indian plate subducted, the speed of the collision may have actually driven it deeper than typically subducted plates (probably meaning that it has had less time to melt since being subducted, and so can still be discerned beneath the Eurasian plate).

In addition, a fair amount of lighter continental sediments were essentially ""scraped off"" onto Eurasia by the collision, during the initial phases when the Tethys Sea closed. A similar process happened along the Pacific plate margins as well, and has contributed to the formation of Alaska, Japan, and other ""accreted"" terranes.",null,0,cdo7rsl,1rfs95,askscience,new,2
fastparticles,"The Himalayas are being lifted at least in part by this collision, however we do not have a specific mechanism worked out for it. The difficulty with this collision is that this is a continent on continent collision, and both are very buoyant. When you think of a traditional subduction zone you have oceanic crust hitting continental crust, and the oceanic crust is denser and so it sinks. In this case both are continental crust so there is little/no density contrast and India can't just sink.",null,2,cdmvdf1,1rfs95,askscience,new,3
mthiem,"It depends where the observer is relative to the galaxy. The Milky Way is visible to the naked eye even from Earth's surface, despite atmospheric scattering. Conceivably, a starship located near a galaxy, but not in the galactic plane as Earth is, would be able to see its spiral structure with clarity.",null,3,cdmvjxo,1rfss1,askscience,new,23
wbeaty,"Look above, at Askscience logo background.  Starfield.

That's our galaxy, seen from inside.   Go outdoors and look up.   Does it look like that?  No, not even out in the country.  Well, maybe when using multispectral image intensifier.   Or, if you're way out in the country, wait fifteen minutes to dark-adapt your eyes, then you can see a bit of that photo (wo/colors though). 

But most of us just see an orange HID lamp glow up there, from parking lots.
",null,0,cdn6v25,1rfss1,askscience,new,5
Das_Mime,"&gt;I recall reading something along the lines of observing the orbit of any natural satellite of the object, but a more detailed explanation would be nice. 

If you can see a natural satellite of the object and you can reasonably assume the satellite to have much much lower mass than the planet*, then you can use mechanics to work out the host's mass. In this case I'll also assume a circular orbit, but you can also work out the mass from non-circular orbits.

The force of the planet's gravity on the moon is **F = G m*_p_* m*_m_* / r^(2)** where G is the gravitational constant, r is the orbital radius, and the m's are the masses of planet and moon. In the case of  circular motion, the force on the moon is equal to **F = m*_m_* v^(2) / r** where v is the orbital velocity of the moon. Set these forces equal to each other, and you get:

**G m*_p_* m*_m_* / r^(2) = m*_m_* v^(2) / r**

Canceling out common factors, you get

**m*_p_*  = v^(2) r / G** 

So if you know the distance of the planet and it has a moon (which for Solar System objects can be readily obtained via parallax methods), then you can directly calculate the planet's mass. 

Calculating the mass of a body without natural satellites is a bit more work. Prior to the Space Age, Venus and Mercury's masses were not well constrained, because the best way to measure mass is to measure its gravitational effect on other objects. Venus also exerts a gravitational influence on other planets such as the Earth, and so if you have sufficiently accurate position measurements of both bodies and if you know Earth's mass then you can calculate Venus's mass, but this is still not an ideal method.

Our best measurements of Venus' mass come primarily from spacecraft like the Mariners 2, 5, &amp; 10 (American) and Venera (Soviet) probes sent to Venus. [From analyzing their trajectories](http://adsabs.harvard.edu/full/1968AJS....73R.162A)--some of them were flybys, some were orbiters (e.g. Soviet Venera 15 &amp; 16, American Magellan and ESA *Venus Express*), and some have landed on the surface--you can determine Venus' mass to a high level of accuracy, but in the end this is essentially the same method as the first-- measuring the planet's tug on nearby objects. 

Finally, you can make guesses at the composition of Venus, and since its radius is easily measured with a telescope, you can get an estimate of its mass. This is much less accurate, of course, since it depend entirely on the accuracy of your guess about the composition. 

*true for all Solar System planet/moon pairs except the dwarf planet Pluto and its moon Charon, which is about 12% of Pluto's mass
",null,0,cdmyrqu,1rfuon,askscience,new,3
Ejb90,"Even for the simplified case of a planet-star system there are a few ways to find the mass of a planet. I'll describe a common one, [Astrometry](http://en.wikipedia.org/wiki/Astrometry).

From observations we can usually deduce the distance of the star from earth, the ""apparent magnitude"" (how bright it is to us) and the spectral class - what types of elements it's made up of by looking at the light received.
We can also find the period of orbit of the planet around the star by several methods - the transit method is most popular, though the Doppler shift method and others are used dependent on the circumstances).
The next part is the difficult part. The velocity of the star wobbling backwards and forwards must be measured. 
The planet doesn't actually orbit a stationary star - they both orbit their combined centre of gravity, though for the star, which is much more massive, this is relatively close to its centre of gravity. Hence the star itself wobbles backwards and forwards. This speed can be measured from earth via the doppler effect - the light when the star is shifting towards us is shifted slightly up in frequency and then when it is moving away gets shifted down slightly. This can be used to calculate the speed.

From the distance and apparent magnitude we can calculate the ""absolute magnitude"" - how bright it it from a standard distance (30ly IIRC). Using these and some hefty thermodynamics/ fluid mechanics/ stellar structure knowledge (or the simplified [mass-luminosity relation](http://en.wikipedia.org/wiki/Mass%E2%80%93luminosity_relation) or extrapolating roughly from the [Hertzsprung-Russell diagram](http://en.wikipedia.org/wiki/Hertzsprung%E2%80%93Russell_diagram)) to find the mass of the star.
Also from the period of or it we can use Kepler's third law to find the radius of orbit.
Now, the speed of the body can be calculated as the distance it travels and the time it takes is known, and the speed and the mass of the star are known.
Finally these can be used with the conservation of momentum to calculate the mass of the planet.

This technique has several issues. Firstly, this only gives a lower limit, as the orbit may not be head on, so the star may be moving faster than expected. Also, some of the measurements needed aren't possible in some cases. Also, it must be noted that Kepler's laws and the mass determination of the star isn't exact. Finally the issue of having more than one body in the system. Because there are a large amount of bodies in the system, the equations aren't analytically solvable, so there is some error in the determination process.

The mass of moons we can observe is calculated much in the same way, using the planet as the main mass.
I'm not sure what you read about observing moons of planets. This certainly isn't possible with exoplanets - we're only just on the verge of being able to see the very biggest exoplanets as off this year.
",null,0,cdmxscf,1rfuon,askscience,new,2
PeeSherman,"First let's explain the science behind a ""note"". A note is just a name given to a particular frequency of air vibrations, which is what gives that note its tone. For example, an A in the middle of the piano in standard tuning is nothing more than a vibration at 440 Hz, meaning when that key is pressed on the piano, a hammer strikes a string that naturally vibrates 440 times a second, which makes the air around it vibrate at 440 times a second - a vibration that propagates through the air to your ear.
Using that same A as an example, on the piano 12 keys to the right, there is another ""A"", this one an octave higher. It is an octave higher because that string naturally vibrates at twice the frequency (880 Hz - 880 times a second), which vibrates the air around it at 880 Hz, which is the vibration that reaches your ear.
To summarize: an octave is a relationship between two sound frequencies (or rates of vibration) in which the relationship is 2:1. A 200 Hz tone is the octave up from a 100 Hz tone.
Interestingly, 2:1 is the simplest geometric mathematical relationship, giving us the most innately stable/consonant musical tone relationship - the octave. Deriving further, 3:2 relationship between frequencies gives us the ""perfect fifth"", the second most innately stable tone relationship. Flipping the relationship (2:3) gives us the ""perfect fourth"" which is a perfect fifth in the opposite direction. 4:3 gives us the 3rd and 6th and so on. The tritone, an extremely dissonant interval that the Catholic Church actually banned at one point in time calling it the devil's interval, has a very ugly mathematical relationship that I cannot recall at the moment. And this is why I am an engineering student who loves music.",null,5,cdmv2r3,1rfwje,askscience,new,43
drzowie,"It all boils down to a mathematical concept called ""Fourier transformation"".  This guy named Fourier figured out how to turn any series of values (like the pressure in air at subsequent points in time) into a collection of pitches.  That turns out to be extremely useful for many things.  

One of the cool things about Fourier transformation is that any *repeating* waveform is just the sum of several pure tones *at integer multiples of the base frequency*.  A flute makes a pure(ish) tone, but a horn making the same note sounds quite different.  The difference is that the horn sound has the main tone mixed in with overtones at integer harmonics (2x the base frequency, 3x, 4x, etc.).   It's worth repeating:  **any complex waveform (a pitch with ""timbre"") is just the sum of pure pitches at integer multiples of a base frequency!**.

So your auditory system has adapted to treat multiple frequencies separated by an integer factor as parts of the same complex tone.  That's good, since it's usually true -- if you have a bunch of random noises around you, most of them won't happen to share any integer harmonics:  two notes that are exactly an integer multiple apart are almost certainly part of the same tone.

There are some exceptions to that rule.  In particular, some devilish fellow might be playing a *chord* on a musical instrument.  Chords are auditory puns.  For example, a C major chord is middle-C, middle-E, and middle-G.  Those notes happen to have the frequency ratio 1 : 5/4 : 3/2.  Multiply all those numbers by 4 and you get the sequence 4:5:6 -- all the notes in the C chord happen to be multiples of another note with a much lower tone!  Whoah. In this case, the lower tone happens to be C two octaves down.  Your auditory system identifies the chord as part of a single complex sound at the much lower pitch -- even if that pitch doesn't actually exist in the music.

That's the basic theory of chords and pitches mixing.  The pitch scale is a *logarithmic* scale -- each step up or down the scale *multiplies* frequency by a certain amount.  Going up or down an octave multiplies or divides by 2.  The reason that notes an octave apart sound like ""the same note"" is that they are so closely harmonically related -- practically every sound around you contains a base pitch and its second harmonic.  If you listen carefully, you can also get that same ""sameness"" from a note and the fifth-interval an octave up.  A fifth interval is a ratio of 3/2 in frequency, so a fifth and an octave gives you a ratio of 3.  Since it's an integer ratio (not a fraction), the two notes (say, C-below-middle, and middle-G) have a little of the ""sameness"" that you normally associate with octaves only.  But octaves have so much of that ""same"" sound that we give notes an octave apart the same name.

Now -- why are octaves ""octaves"", and why are there exactly 12 half-steps in an octave?  That's because of something called the ""circle of fifths"", which musicians frequently mutter about (and which you can google for more information if you're not one).  The easiest way to construct a scale is by starting with a base note somewhere (say, A-440, but any frequency will do), and then constructing third harmonics of it.  Each time you go up in frequency a factor of 3, you get a nice harmony (the octave-and-a-fifth).  Then you fold the new note downward by octaves until it is within a factor of 2 of the original frequency, and start over.  If you do that 12 times you'll create 12 separate notes, and arrive *almost* back where you started -- 1.36% higher in pitch than the original note.  That's really discordant if you play it next to the original note, but if you tweak each of your derived notes ever so slightly, you can sort of smooth things out so that all the frequencies work right to form new chords with one another.  You'l find that you created exactly 12 notes and defined the half-step scale.  But you had to fudge the frequencies, because you had to sweep the discord under the sonic rug somewhere.  This is reasonable not just for aesthetic reasons but because, if you didn't know the math, you might think you'd just screwed up the tripling step a tiny bit each time.  When people say the Western scale is based on a lie, this is the lie they mean: the circle of fifths cannot work perfectly, because no matter how many times you multiply your original frequency by 3, you will never arrive at a power of 2 -- but you can fudge it if you're close enough. 

Through the ages there have been several different ""temperaments"" used, in which people tweaked the notes of the 12 tone circle of fifths in various different ways, to try to make particular chords sound particularly good -- at the expense of other chords.  These days, we use an ""equal-tempered"" scale where each half step is exactly a factor of 2^1/12 above the previous one.  If you're playing a bendable instrument (like the flute, the trombone, the violin, or the human voice) and you are a good musician, you will unconsciously tune each note slightly higher or lower depending on the chordal context of your particular note, to harmonize better with the rest of the orchestra.  You *can't* bend the notes on a piano, which is why pianos have multiple strings singing each note -- it fuzzes out the resonance of each note, so it's harder for your ears to pick out the harmonic discrepancies.  (There are *three* strings so you can't hear the slightly-detuned strings beating, as you could if there were just *two*.  The bass bridge usually has two strings per note, but by the time you get down there the resonances are so cruddy that you can't really hear the beating anyway).

The 8 primary notes (A-G) you can get by stepping *once* forward on the circle of fifths and and *once* backward, to get three notes separated by fifth intervals (for example, F-below-middle, middle-C, and middle-G).  If you create major chords for each of those three notes (and fold all those new notes into a single octave), you'll find that there are 8 unique pitches, which are the pitches of the major scale.  That's why we call it an ""octave"" - oct for 8.  Since going down a fifth (and folding into the main octave) is the same as going up a fourth interval, you can immediately see why IV,V,I and similar chord progressions are so common in Western music -- they're the very basis of our musical scale.

Incidentally, not everyone agrees on that scale.  The equal-tempered Western scale can generate harmonic sequences up to 7/8 of the original (if you play a C7 chord with the low G and two lower C's, you are playing the 1, 2, 3, 4, 5, 6, and ~7 harmonics of the lowest C).  But any higher harmonics fall between the notes.  Middle-eastern and Indian music uses higher harmonics, and therefore has lots of quarter-step or smaller intervals that sound strange to our ears.  The German tradition calls that 7/8 harmonic of C by its own special name - 'H', as the next note after G, a fact Johann Sebastian Bach exploited by working his own name (BACH) into a counterpoint line in his last great composition.

**tl;dr**: What, I summarize 900 years of musical theory and you're complaining it's a wall of text?  F\*ck you, go back and read it.


",null,4,cdn6q1y,1rfwje,askscience,new,15
do_od,"Buoyancy is a force acting on a body as to oppose gravity when that body is immersed in a fluid. This force is proportional to the weight of the volume of fluid displaced. In zero gravity, the fluid has no weight and there is no direction in which buoyancy could act. Buoyancy requires gravity... or more generally a reference frame under acceleration. Example: If you spin a bucket of water on a string in outer space, a ball could be buoyant in the water. That would not be useful for propulsion though. ",null,0,cdmv0go,1rfxwf,askscience,new,10
blacksheep998,"Here's a good video answering your question. http://www.youtube.com/watch?v=bgC-ocnTTto

In it an astronaut places an alka-seltzer tablet into a spherical water drop. Without gravity the only major force affecting the bubbles is surface tension, which causes most of the bubbles to combine with each other and eventually form one large bubble in the middle of the water sphere.

There's also this video, http://www.youtube.com/watch?v=QPf5MJluhvo in which an astronaut injects an air bubble into a water sphere, and then injects small water droplets into the bubble.",null,0,cdn17rs,1rfxwf,askscience,new,4
AltoidNerd,"It's puffy.  If highly energetic, roughly spherical.  

You can get fireworks to discharge in predetermined shapes by the way you pack the explosives.   By analogy, the shape of a space flame would depend likewise on the shape of the source,  

Spherical enough of course to feel good about 4/3 π r^2 in a physics calculation.",null,0,cdn9gny,1rg4lz,askscience,new,2
Nicked777,"The fire will indeed be spherical, this has actually been tried in Space before, it looks pretty cool (I'm on my phone so I won't link it.)

The flame changes colour because the lack of convection causes diffusion to be the dominant transport mechanism. Compared to a terrestrial flame this means the flame burns with more complete combustion, with less soot. (Glowing hot soot is the reason most terrestrial flames are yellow.)

Edit: More information here: http://carambola.usc.edu/research/microgravity.html
",null,0,cdn9k0r,1rg4lz,askscience,new,1
therationalpi,"Basically a whistle is a resonator. You either have a Helmholtz resonator (like a beer bottle) or a standing wave resonator (like an organ pipe).

Driving the resonator is the variable airflow through the whistle. In most whistles you will have a hole with a blade shaped edge on it. When the edge is blown on, it creates turbulent airflow in the form of vortexes. These vortexes alternate from side to side in what is called a ""vortex street."" There's a good picture of that [here.](http://www.grc.nasa.gov/WWW/Acoustics/code/adpac/sample/CYLINDER_VORTEX_SHEDDING/) The alternating vortexes create a varying positive and negative acoustic pressure, setting up a wave in the resonator. The resonator, as a result, forces the frequency of the vortexes to align with the whistle's natural frequency. In this way the whistle amplifies the normally irregular vortex variations into a sound loud enough to be heard at a distance.

The reason you must blow at the correct angle is that the flow vortexes will depend greatly on how the moving air stream hits the blade. You must hit the wedge shaped part of the whistle fast enough to create unstable flow, otherwise the wave will not be generated.

Hope that helps!",null,0,cdmz9jk,1rg4zj,askscience,new,1
BoxAMu,"The energy of a photon is proportional to frequency, but this energy must match the energy of some transition in the absorbing matter.  The electrons in bonds in glass have transitions in the UV, but not the visible range.

This is the same reason why high energy X-rays are used for imaging: muscle and tissue are mostly transparent to X-rays, while the calcium in bones absorbs them.",null,0,cdmy00a,1rg4zy,askscience,new,4
uberhobo,There is no such thing as relative humidity above the boiling point of water.  It will all stay a gas in any proportion with air.,null,0,cdnb8t8,1rg6ug,askscience,new,3
whatsup4,it depends if there is something for the water to condense on. Basically think of it like cloud formation. Air high in the atmosphere can sometimes be super saturated and achieve higher than 100% rh because it is hard for the water to form droplets without a surface to form on. Given a large enough decrease in temperature you can see cloud formation.,null,1,cdnaiz3,1rg6ug,askscience,new,1
Merrilin,"Anything with a temperature (a.k.a. all matter) is constantly emitting **blackbody radiation**. 

You can think of temperature of an object as being proportional to how much each constituent atom vibrates. The more intense it's vibration, the hotter it is. The short of it is that this vibration causes the release of a photon, which carries with it some energy from the atom, decreasing it's temperature. More on that if I ever get home. 

It so happens that the hotter something is, the higher frequency radiation, on average, it emits. That's why a piece of metal visibly glows when you make it very hot. At room temperature it is emitting light at a range of frequencies, but almost none in the visible light range. As you make the piece of metal hotter, it's blackbody radiation in the visible light range becomes significant enough that a human eye can detect it. 

So, no, matter cannot have temperature without also emitting some frequency of light. And there is no such thing as matter without temperature, so matter is always emitting light. ",null,0,cdmy126,1rg6wj,askscience,new,9
thumbs55,"Excellent question.

First of all what is heat and what is temperature? Are they not the same thing?

[Heat](http://en.wikipedia.org/wiki/Heat) is a measure of thermal energy (measurable in joules like all energies), it can be a measure of the ammount of (highly disordere heat typed energy) energy moving from one body to another.

[Temperature](http://en.wikipedia.org/wiki/Temperature) is a measure of the hottness or coldness of a body, two bodies with the same temp will not exchange any net heat and if one body is hotter than the other then the hotter will give energy to the colder in the form of heat.

&gt;everything I can think of that has heat also has light. Stars, lightbulbs, lava, fire, hot metal,

This type of light is called [black body radiation](http://en.wikipedia.org/wiki/Black-body_radiation).

&gt;Metal only emits light after it heats up past a certain temperature.

While it is true that the light becomes visible after a certain temperature is reached, the metal is actually always emitting invisible ""light"" (electromagnetic radiation) at any finite temperature due to said black body radiation.

All of space has [Cosmic microwave background radiation](http://en.wikipedia.org/wiki/Cosmic_microwave_background) which gives ""empty"" space a temperature. (It is not empty of [real] particles if you include the photons giving it said temperature).

The temperature of space (away from stars and such) is around 3 kelvin, so if you have something hotter than that it in space will get colder and if you have something colder than that it will actually warm up.

The heat energy is often stored in the energy of the jiggling of molecules. But for it to move from one place to another it mainly moves in the form of photons (light) but also phonons (sound). If you engineered a particularly exotic system that only exchanged energy in phonons then this system would emit heat with no light.

Nutrenos are also an example of a form of heat (they carry energy from the sun in a manner that is not work) nutrenos are not light. But many forms of nutreno generation would also produce photons.",null,1,cdmyhyr,1rg6wj,askscience,new,5
AltoidNerd,How about your hands.,null,1,cdn9ibx,1rg6wj,askscience,new,3
Chuk,"Metal only emits light after it heats up past a certain temperature. It can get very hot but still not be glowing. (That is, assuming you are only thinking of visible light.) Living creatures also emit heat without light, as do many other chemical reactions.",null,7,cdmxi6u,1rg6wj,askscience,new,2
CosmicWaffle5,"It's called positional alcohol nystagmus. Basically, there are these things in your ears called semicircular canals that are responsible for your sense of balance. The semicircular canals are supported inside of a fluid that is usually the same density as the semicircular canals, but when you drink alcohol it changes the density of the fluid surrounding the membranes and throws your balance system out of walk. 

http://en.m.wikipedia.org/wiki/Positional_alcohol_nystagmus",null,0,cdn7xz4,1rg8rs,askscience,new,7
Nicked777,"To launch into any orbit the launch site must be directly under the orbital path (ground track). Even though the final orbit is geosynchronous, there will be an intermediate orbit that SpaceX need to hit to get the right path. ",null,3,cdn4y4m,1rg990,askscience,new,3
zelmerszoetrop,"You're right that launch windows usually have to do with the various orbits of the target body and such - eg, there are launch windows to Mars only every 2 years or so because you don't want to launch when Mars and Earth are in the wrong respective positions.

You're also right that to get into any old geostationary orbit, there would be no launch window.  But geostationary satellites are assigned very specific orbits, and have to hold position over very particular spots on the Earth's surface.  Hence, to arrive at the correct spot without a Hohmann transfer from LEO, the satellite must be launched at the right time.",null,1,cdnb28z,1rg990,askscience,new,2
ferociousfuntube,My guess would be that since they use liquid oxygen which is cryogenic and therefore boiling off continuously they may need to add more if it sits for too long. Same goes for the fuel if they are using liquid nitrogen. This is just a guess though and have no idea if this is true.,null,5,cdnbttb,1rg990,askscience,new,2
iorgfeflkd,If the mother and father were half-siblings.,null,3,cdmycqc,1rganp,askscience,new,8
ohheytherewhatsup,"No. Crossover events during Meiosis 1 are required to generate tension in the meiotic apparatus.  Without crossover, division will not occur, and crossovers cause mixing of chromosomes from the grandparents.  Each of your chromosomes is a chimera of your two grandparents DNA.",null,1,cdn9ihk,1rganp,askscience,new,5
laika84,"Although this would not add up to 50%, the child of a mother with Down's syndrome, (men are essentially infertile and women with DS can have a child but they are less fertile than those without DS,) there would be a 50% chance that the child receives the extra chromosome.

Since this chromosome resulted from a non-disjunction event in one of the grandparents, the child would have more than 25% of his/her genetic material from one grandparent.  Again, not 50%, but still interesting.",null,1,cdn6qe0,1rganp,askscience,new,3
null,null,null,0,cdn18w5,1rganp,askscience,new,1
iorgfeflkd,"They're not actually instantaneous, they're just treating them that way because it's much simpler to do so in an intro to physics class. Real objects are made of compressible materials, and when they collide the objects deform.",null,0,cdmy3fx,1rgap2,askscience,new,4
cylon37,"Let's be clear here. Two events that are simultaneous in one frame of reference may not necessarily be simultaneous in another frame ONLY if the two events are separated by some distance. Conversely, if two simultaneous events happen at the same point in space, they are simultaneous in all frames of reference. A collision as described above is a single point in space-time. The two 'events' that you describe, A transferring momentum to B and B transferring momentum to A happen at the same location and are therefore simultaneous in all frames of reference.",null,0,cdn0i76,1rgap2,askscience,new,4
musubk,"Contrary to the other answers, the length of the day decreases at a near constant rate for most of the year. It isn't a sine wave, people! It looks more like a triangle wave with the points lopped off and rounded off. It superficially looks like a sine wave if you view it for lower latitudes because the amplitude is too small to see the shape, but try it for somewhere further north. Fairbanks, AK is a good choice. I just wrote a quick IDL routine to read the daylight hours tables the USNO website gives for a chosen latitude, [here it is for Fairbanks (65 North)](http://i.imgur.com/YMvVGf5.png).

And if you go even further north, like 85 degrees, [you get something silly like this](http://i.imgur.com/bHWfVqK.png).

The point being that the days don't start shortening at a slower rate as you would think for coming over the edge of a sine wave, the rate that days are shortening is actually constant over the majority of the year for a majority of the planet. This is still true at lower latitudes, and if you scale the graphs right you can see that:

[50 degrees latitude](http://i.imgur.com/na9TE3D.png)

[35 degrees latitude](http://i.imgur.com/8BLfwwu.png)

[20 degrees latitude](http://i.imgur.com/PrqgIpq.png)

[5 degrees latitude](http://i.imgur.com/OLRDXDP.png)",null,0,cdnbz0h,1rgcbc,askscience,new,6
iamtheonewhotokes,"As we approach Dec. 21 the days will shorten at a slower and slower rate. Similarly as you approach the summer solstice in June days will get longer at a slower rate the closer you get. And as you approach an equinox (in March or Sept.), the rate increases. 

See chart here: http://cycletourist.com/Miscellany/Length_of_day.html (the slope of the curve is the rate)",null,3,cdn12gy,1rgcbc,askscience,new,6
iorgfeflkd,"Nothing particularly interesting would happen. Light by itself isn't affected by temperature, and if the light is passing through a vacuum then temperature isn't a meaningful quantity. Depending on the medium that the passes through, its temperature can have effects on how the light absorbs it. For example, in an extremely cold dilute gas it is possible for the atoms to absorb light and stay in that configuration for a brief period of time, so the light is in effect trapped. This is the temperature's effect on the medium, however, not the temperature's effect on the light.",null,0,cdmyy7i,1rgcdh,askscience,new,4
stuthulhu,"Another thing to consider, even if the photons *could* be frozen you would not see your display freeze as though stuck in time. You would simply not see your display, since the photons responsible for creating that image are no longer able to *move* to your eye. 

An easy way to simulate what a room would look like if all the photons became frozen in space is to put a box over your head. ",null,0,cdndsia,1rgcdh,askscience,new,2
Das_Mime,"Light won't stop moving, even if it's going through a medium which is at absolute zero.

Temperature is about the thermal motion of particles which have mass, like electrons. The colder you get, the less kinetic energy they have. But light has no mass and its energy is proportional to frequency, so it usually doesn't make a great deal of sense to talk about light having a temperature in the same way that a physical material does (although a spectrum of light can certainly have a characteristic black body temperature, lower energy light doesn't travel any slower than high energy light).

Light, on the other hand, is comprised of massless photons. If they're passing through a medium (like water or air), then they will go somewhat slower than the speed of light in a vacuum. This change in speed can be affected (slightly) by the temperature of the medium, which is why you see effects like heat shimmers. Light can't stop moving, although [certain materials can slow it down to extremely slow speeds](http://www.news.harvard.edu/gazette/1999/02.18/light.html).",null,0,cdmz3om,1rgcdh,askscience,new,2
Das_Mime,"The idea is that another star's gravity will tug the comet farther out at first, and then when the star passes (or just when the comet continues on its newly more-elliptical orbit), the comet falls back inward.

It should be noted that the Oort Cloud is very poorly understood, not really directly detected, and is basically used as an explanation for long-period comets. Most comets are on highly elliptical orbits, so even if several of them are perturbed by the same star, their orbits will be altered in different ways. Even if multiple comets are sent into the inner solar system in this way, they might arrive years or centuries apart.",null,0,cdn86xc,1rgeiw,askscience,new,3
Dyolf_Knip,"East takes you out, out takes you west, **west takes you in**, in takes you east.

The bold one is relevant here.  The star does attract the comet, but does so in a way to slow its velocity relative to the sun.  After the star passes by, the comet assumes an orbit suitable to its new velocity, which means it drops into the inner solar system.",null,0,cdne36w,1rgeiw,askscience,new,1
neverdonebefore,"There is a bit more to it than that.  

In FWD cars, the front wheels are doing both the steering and applying the engine torque to the road.  And RWD, the rear wheels are only applying that torque to the road.  Essentially, your fwd cars are 'pulling' while rwd are 'pushing'.  

As you drive down a straight road, you are applying longitudinal force to the road to propel you forward.  As you enter a curve in the road, you add a rotational component to your travel.  The center of mass of the vehicle has to move laterally through the curve, while the vehicle itself has to rotate about that center of mass in order to be pointed straight as you exit the curve.  With a fwd vehicle, the direction of the force applied to the road by the tires changes as you turn your steering wheel, and the back wheels will follow in that path. Fwd vehicles have a tendency to understeer.  An object in motion wants to stay in motion: the inertia of the car in the longitudinal direction makes it want to keep going straight.  The tires want to follow the path on which they are pointed.  If the lateral acceleration into the curve cannot overcome the forward inertia, the car will understeer, or take a path with a larger radius than the curve.  In a rwd vehicle, the the tendency is to oversteer, or turn at a smaller radius than the curve.  This is because the force on the road by the rear wheels is along the path of the vehicles inertia.  The front wheels will want to follow their path around the curve, but the rear wheels will want to keep going straight.  This means it is easier to rotate about the center of mass.  This is how fish tailing and drifting (and spin outs) occur.
",null,1,cdn5t9m,1rgew2,askscience,new,8
wwarnout,"In a rear-drive car, when you accelerate, the center of gravity shifts toward the rear.  So, if the only consideration was getting good traction during acceleration, this would be the preferable configuration.

However, since most cars have engines in the front, a front-drive car will have better traction is slippery conditions because more of the weight is over the front wheels.",null,1,cdn0w9f,1rgew2,askscience,new,4
AltoidNerd,Take a shopping cart at the grocery store and compare pushing and pulling the cart.  This especially is useful if the back wheels of the cart don't rotate (in analogy to the car).,null,1,cdn9faf,1rgew2,askscience,new,2
5secondstozerotime,"I do not think the rocket is directly launching into Geostationary orbit. Rather, it is going into a geostationary transfer orbit (GTO) that will then allow it to go into a geostationary orbit.

I cannot find a reason why this needs a window, however what you are saying about the rocket is wrong. 

[This article talks exstensivly about it](http://www.americaspace.com/?p=45686).",null,0,cdn8uw4,1rgf3r,askscience,new,4
Nicked777,"A little known consequence of orbital mechanics is that you must be directly under an orbital path to launch into it. SpaceX do not launch from the equator, so they cannot go straight into GEO, they have to start with a transfer orbit, and then do a plane change somewhere. They can only launch into their transfer orbit when this orbital path is directly overhead, which means waiting for the earth to rotate Cape Canaveral into the right spot, thus the launch window troubles. ",null,0,cdn973l,1rgf3r,askscience,new,3
ecopoesis,"Metrics such as temperature describe the behavior of a system that is made up of components.  These types of properties are termed emergent properties because they are derived from the behavior of the system as a whole and are not observable if you were to look only at the components.

So, for your specific question, individual molecules do not have temperature.  They are not ""hot"" or ""cool"" exactly, although they do have energy that is zipping them around their surroundings.  Molecules with more energy will move faster and collide with other matter more frequently and with more force.  It is only when you begin to look at a system of molecules that ideas such as temperature start to be meaningful.  In that sense, a group of molecules with a certain amount of energy will correspond to a certain temperature.  If these molecules are ""hotter"" than other molecules, then they will be moving about much more rapidly and they will be less dense than the latter group of molecules.  Properties such as temperature and density are emergent from the system of molecules interacting with each other and interacting with their surroundings.",null,1,cdn5v46,1rgf8v,askscience,new,7
The_Evil_Within,"&gt;an area of hot air becomes less dense, and so it rises above colder areas of air. 

First, you need to look at it the other way around - hot air doesn't rise, cold air sinks.  As it sinks, it forces the hotter air upwards.

Now, think of a mess (and I do mean 'mess' for the imagery, not 'mass') of cold air, with the molecules fairly still and fairly dense.  Then, something heats up a bit of it near the bottom - what's going to happen?

The molecules of hot air will bounce around a lot more than the cold, and sometimes they're going to bounce up.  When they do, the less active cold air is more likely to fall into the gap than to move in another direction, and now there's nowhere for that hot air molecule to go because it will only bounce off the cold air molecule if it bounces downward again. (Transferring some heat in the process, but we can ignore that for the purposes of this explanation)

Multiply this by unimaginable numbers of interactions, and you end up with a column of hot air rising while all the cold air around it rushes in to fill the gap at the bottom.",null,4,cdn6q0h,1rgf8v,askscience,new,8
AltoidNerd,"&gt;&gt;But what if there was a single constituent molecule from that wood existing in the water? Would it float? It is neither densely nor sparsely aggregated, existing all by itself.

My reaction to this is no not really - a single molecule would have dynamical behavior that isn't familiar like the bouyant force example you gave.  I have no idea how to describe what that situation *would* be like - but I'm positive it would be invalid to treat it like a whole plank of wood floating.",null,1,cdn9dzb,1rgf8v,askscience,new,5
ramk13,"Wanted to add that at the scale of single molecules, static interactions are much more important than buoyant forces. A single molecule of wood will dissolve and behave like another molecule in solution. Even a few molecules of wood together will still be influenced by the hydrogen bonding between water and its external oxygen groups more than the buoyant force on the particle as a whole. All of this applies to your wooden plank example.

To answer your question: And if so, why do they act like an aggregated 'body' with those molecules around them, just because they are at the same temperature?

It's because even in air at atmospheric pressure molecules have a limited mean free path. In air it's [68 nanometers](http://en.wikipedia.org/wiki/Mean_free_path#Mean_free_path_in_kinetic_theory). That is that an oxygen or nitrogen molecule only travels so far before it hits another molecule and they bounce off each other. The molecules collide often enough that they influence each other over that short length. That influence leads to aggregate properties, as each collision redistributes kinetic energy.",null,0,cdnt7nx,1rgf8v,askscience,new,1
dirtpirate,"You seem to be misunderstanding the interaction. Comic book guy is asking for a very high number X, and mister Burns is retorting to Smithers ""Give hime Y"", where Y is much smaller than X. Thus a typical haggling scenario. 

The joke isn't that the two numbers are the same, just that instead of comic book guy saying ""I want a billion"", and Burns replying ""I'll give you a million"", they are instead using physical constants. ",null,0,cdnbei2,1rgi0o,askscience,new,7
iorgfeflkd,"The Faraday constant is the charge of a mole of electrons or protons, measured in Coulombs. Avogadro's number is 6x10^23 and a Coulomb is 6.2x10^19 fundamental charges, and the ratio is 96485 Coulombs per mole.",null,3,cdn0eae,1rgi0o,askscience,new,7
MonadicTraversal,"&gt; Any faster or slower, closer or farther, or difference in direction of travel and the body would de-orbit, spiralling toward the planet or star it's orbiting or flinging off into outer space.

This isn't true. If you smacked a huge meteor into the Earth, you wouldn't knock it into the sun, you'd just change the shape of its orbit a bit. Spiral orbits don't actually exist under inverse-square forces such as gravity; you can show that the only possible orbits are circles, ellipses, parabolas, and hyperbolas. Spiral orbits don't exist except if there's some kind of drag force or whatever dissipating energy from the system; on an interplanetary scale drag doesn't matter. (Note that this is somewhat complicated by the fact that, e.g., Jupiter affects the orbit of the Earth, but in general the perturbations due to planet-planet interactions are small enough to not matter for stability purposes).

&gt; Isn't it difficult for us to keep our own man-made satellites in a stable orbit, requiring periodic adjustments? And yet, the moon is huge and it seems to be in an orbit that will last billions of years with no intervention.

Many man-made satellites are orbiting at an altitude where Earth's atmosphere can still exert some small amount of drag. The moon is so far away from Earth that the drag is essentially negligible. We also want the satellites to be kept in a *predictable* orbit; for a geosynchronous satellite, we want that orbit to be such that it's always above the same spot on the equator. The moon doesn't 'have' to be in any particular orbit, it just orbits wherever it orbits.",null,0,cdn0pyg,1rgi2m,askscience,new,9
iorgfeflkd,"For a circular orbit it has to be that precise, but many more initial configurations will lead to stable elliptical orbits, which are stable due to a balance of gravity and angular momentum. We live in a universe where the force of gravity decays with the square of distance, which is related to the fact that we live in three spatial dimensions. It turns out, there are only two types of forces that can produce stable orbits: inverse square, and linear (harmonic, like a spring). So, basically, we live in a universe where stable orbits can exist. Because of that, the fact that we do see stable orbits is not surprising.",null,0,cdn0bqk,1rgi2m,askscience,new,5
iorgfeflkd,"It's not changing its constant, it's just changing your units. If you use meter-kilogram-seconds unit then hbar is something like 10^-34 m^2 kg /s but if you use Planck units then hbar is 1, G is 1, and c is 1, and you can measure lengths in terms of (hbar G/c^3 )^(1/2), for example. This makes it easier to do theoretical work because you don't have to keep track of all these constants, but you'll have to do more work to get your results in measurable quantities.",null,0,cdn34jk,1rgmvd,askscience,new,14
LoyalSol,"The curve itself, not really.  The function, definitely. There are so many uses it is hard to list them all. ",null,0,cdnph1q,1rgo2l,askscience,new,1
Platypuskeeper,"There cannot be such a thing as a 'non-cohesive liquid'. A liquid is by definition a state where the attraction between the molecules is strong enough that the thermal energy is insufficient to let most of them leave the liquid. But unlike a solid, the molecules are still able to move about. 

If you have no intermolecular forces, you have a gas. 
",null,0,cdn6cb2,1rgslj,askscience,new,3
iorgfeflkd,"Yes, it's both. Just being still in a gravitational field (like we are now, on the Earth) causes time dilation relative to freefall, and orbiting satellites have to take both into account (this is the famous GPS relativity correction).",null,0,cdn3rku,1rgt18,askscience,new,4
mingy,"I think you are right, but it is a minor error that probably got by the editors. I once read the final draft of a textbook written by a renowned expert in optics (long story) and found several errors (mostly units and arithmetic) and I knew maybe 1% of what the author had forgotten. He was grateful nonetheless.

In any event, even the 10 billion bits are wrong. The base pairs are grouped into 3s so you have 64 permutations, however this is not a binary or quaternary system, there are redundant codons and start and stop (http://en.wikipedia.org/wiki/DNA_codon_table) so there are 22 symbols of the 64 permutations.",null,0,cdn4k62,1rgu88,askscience,new,4
selfification,"Yeah that was a mistake in a way.  Each nucleotide base pair carries 2 bits of info.  So 5 billion base pairs carries 10 billion bits of info.

But there is the flip side that you have 2 separate sequences.  Each base pair is 2 codons.  Now you can consider that just 1 letter (because one of them precisely specifies the other) but I guess one could consider them 2 separate letters.  I mean...  2 copies of a file have twice the number of bits, even if the *information content* hasn't increased.  So in that interpretation, each base pair contains 4 bits of info...   and that would make Sagan's calculation make sense.",null,0,cdn4nl4,1rgu88,askscience,new,2
iorgfeflkd,"Each base-4 base can represent 00, 01, 10, or 11. So there is 4 times as much information as just binary.",null,4,cdn3qce,1rgu88,askscience,new,1
Physics_Cat,"In order:

Technically, yes. But the technical definition of temperature isn't what you think it is. More on that in a moment. 

Absolute zero is exactly the same as zero Kelvin. 

Who told you that the temperature of a black hole is absolute zero? That's certainly not correct. In fact, it's not possible for any matter to be at a temperature of exactly zero kelvin, due to the zero-point motion inherent in quantum mechanics. We can get incredibly close in a laboratory (somewhere in the range of hundreds of picoKelvin) but it's not possible to attain exactly zero kelvin. 

As for negative temperature: the colloquial understanding of temperature is something like ""temperature is the average kinetic energy of the constituent particles in a material."" That's a very useful tool for intuitively understanding things like heat capacity, but it's not the ""real"" definition. In thermodynamics, temperature is defined as the partial derivative of internal energy with respect to entropy (not sure how to format that symbolically, so I won't try). There are some kinda-convoluted, not-entirely-realistic examples of physical scenarios with negative temperature. That is, you add a bit of energy to the system, and the entropy goes down. For example, suppose you have N light switches, and each ""quanta"" of energy is represented as turning on one light switch. The entropy of a system is related to the number of configurations (microstates) that lead to the same macroscopic result, so let's say that you have N-1 switches turned on. Then the entropy is, more or less, N (since there are N ways to have N-1 switches turned on in a collection on N switches). Now you add one ""quanta"" of energy and turn on the last light switch. How many microstates are there now? Only one. There's exactly one way to have N out of N light switches turned on. Since we added a unit of energy and saw the entropy decrease, the system could be said to have ""negative temperature"" if you like. There are physical systems that come close to this analogy, but I think the ""light bulb scenario"" is easier to digest.",null,2,cdn51ib,1rgv22,askscience,new,9
fishify,"Absolute zero and 0 K are the same temperature.

Negative absolute temperatures are actually *hotter* than any possible positive temperature.  When you look at the mathematics, at any positive temperature, more energetic states are less likely to be populated than less energetic states (though at higher temperatures, the difference between those likelihoods is not as large as at lower temperatures); what you find is that if you had negative absolute temperature is that it would correspond to a situation in which more energetic states were *more* likely to be populated than less energetic ones.  (Lasers are a place where you might see such population inversions.)

Black holes have positive temperature, inversely proportional to their mass.",null,0,cdn54pg,1rgv22,askscience,new,4
auralucario2,"First, the statement about black holes is completely false.

Now, according to the law of thermodynamics, it is impossible to reach absolute zero, which is the same as zero kelvin. However, quantum mechanics butts its head in here and offers a workaround (kind of). It would be theoretically possible to achieve a temperature of some negative kelvin by having particles achieve a quantum state in which their entropy actually *decreases* as energy is added to the system. Needless to say, this doesn't exactly happen all the time, but it is possible.",null,0,cdnvd3b,1rgv22,askscience,new,1
brickses,"Zero kelvin and absolute zero are the same thing. There is no such thing as negative temperatures except in advanced thermodynamics exams.

Black holes are actually hotter than zero kelvin, like all warm things, they radiate (the same way humans radiate in infrared).",null,3,cdn4wu1,1rgv22,askscience,new,2
owaisofspades,"ACh is your neurotransmitter which triggers a cellular response. In the case of muscles it will cause calcium influx into the cytosol (either from the sarcoplasmic reticulum or from intracellular reservoirs depending on the type of muscle). The summation is a result of excessive Ca2+, which itself is brought about by ACh

Tetanus refers to sustained contraction and is usually a bad thing if it goes on too long. It can be brought about by overstimulation of the muscle cells, and this can happen either through sustained excitation or as a result of acetylcholinesterase inhibitors.

In regards to the intervals, not all the calcium leaves the cytosol immediately after stimulation ends, so if the intervals are close enough together, the residual calcium from each stimulation will begin to add up until your are constantly at a maximally contracted state even in between stimulations, which leads to a tetanic state.

Hope that explained it well enough",null,3,cdn5cpe,1rgx9w,askscience,new,10
RedBeard17,"Tetanus would be a combination of both, really.  A muscle twitch itself consists of the action potential (AP) (from the neuron, stimulating the synaptic cleft), and then the release of Ca2+ from the lateral sacs.  However, the length of an AP is much, much shorter than the length of a single muscle twitch, which is what allows us to sum the twitches, to increase strength, blah blah blah.

When you get to tetanus, the stimulation of the muscle cell is due to a high frequency of AP's coming from the motor neuron.  Because the muscle twitch takes longer (and you've got a continuous, and very high release of Ca from the sarcoplasmic reticulum), you get this prolonged opening of Ca channels, prolonged release of Ca, and thus you get a tetanic contraction.  

So, to summarize what I've tried to say (I ran on a little bit there):

1. Summation occurs because you get a higher frequency of stimulation of the muscles.  This high frequency stimulation causes a lot of Ach release at the NMJ, and a lot of Ca released in the cell to cause the muscles to contract.

2. Tetanus occurs when you've got such high frequencies, and so much Ach release at the NMJ, that the muscle doesn't have anywhere near enough time to relax, that it's constantly contracting, and therefore you have tetanus.
2a. It's not normally because Ach is TRAPPED in the NMJ, it's usually because it's being very quickly released from the high frequency of AP's coming down the motor neuron.

3. Ach is the only one going to be acting on the postsynaptic membrane.  Ca is only involved with troponin/tropomyosin inside of the cell.

4. Ach is going to bind to non-specific ligand gated channels on the End Plate (the ""top"" portion of the muscle sarcolemma), and while Ca is going to enter in through those channels to depolarize the membrane to create your end plate potential.

Does that make any sense?  Let me know if that helps.",null,0,cdpc92e,1rgx9w,askscience,new,2
Dominus_,"When you're wiring your home surround system, no, pretty much not at all. But over long distances like on a concert where some cables run several tenths of meters, sometimes even a hundred meters, the resistance and interference has to be reduced, or else you're going to end up with artifacts and noise. ",null,7,cdnacxc,1rgzbv,askscience,new,46
thegreatgazoo,"For just about anything in your house, lamp cord is an excellent choice of speaker wire. Just make sure one side is marked so you keep the polarity correct. 

Anjou Pear speaker wires (and anything similar) are for delusional people who have too much money. 

",null,1,cdnckf0,1rgzbv,askscience,new,7
littlegreenalien,"yes.. and no. It's not so much the cable that's the problem, rather the interference it can pick up on the way. The longer the cable the more issues come into play (cable resistance, etc… as mentioned already). But at short cable distances it's mostly interference from power cables, and what not.",null,0,cdnb62y,1rgzbv,askscience,new,5
Kriemore,"Computer engineer here: wires are important, but if your question is 'should I spend $90 to get these cables I found at best buy for my home theatre?' Then probably not.

A bad cable will degrade audio quality significantly in addition to causing all manner of other problems with cutting out etc. 

Of course, expensive audio cables were famously compared to a coat hanger with no noticeable difference.


Now, if you're playing a massive theatre... these things start to matter a lot more.",null,0,cdngnf0,1rgzbv,askscience,new,4
jgrun,Ultimately there is always a degradation of signal quality when transmitting over a long distance. But since a digital signal is just binary 0s and 1s and you're only sending it 4 or 6 feet to the TV or stereo it doesn't matter. The receiving end will read the signal very clearly because it's hard to mistake a 1 for a 0.,null,7,cdn7wlm,1rgzbv,askscience,new,9
thisispointlessshit,"Not really. I just like getting cables that don't feel cheap... If that makes sense. The wire itself tends to wear over time if it has cheap shielding when I'm constantly coiling and uncoiling. Something with decent shielding usually lasts longer for me. For home use it might not be as much of an issue, because it's plugged in and never really moved.

In terms of sound quality? No difference.",null,1,cdng5vb,1rgzbv,askscience,new,3
lucaxx85,"You need to distingush three applications: 
1)analog signals in home setups
2)digital signals in home setups
3) live concert signals and similars...

For 1) everything works. Including coat hangers (for the power signals. You need shielding for line ones). The resistence and the impedence of such cables are such that they cannot affect in any way the final signal, which has a very low bandwidth. Only thing to be careful is to have cables large enough for the amp-speaker connection, if you have a very high power system (but I'd guess that you can still forget about this in any practical situation). 

2) Digital signals are more complicated. The bandwidth here is much higher, especially if you're also carrying video. That's why you have maximum lenghts and building them needs lots of care. Still almost any commercial cable is good if you're not trying to do something you shouldn't (e.g.: a 10 meters HDMI connection). In these case of course a 15'000 $ cable made from the finest rhodesian zinc, soldered in a full moon night by an african zoroastrian priest would work as badly as the cheapest one in the store.

3) For concerts and other applications cables can give actual problems. Still not those ""lamented"" by audiophiles. The first thing you look for in a concert cable is the *mechanical* resistance, especially of the connectors. Most of the cables break for a mechanical injury! Those things get torn everywhere. 
Then there is a problem with microphones/guitars signals. They're *extremely* weak. So they're sensitive to interferences. But, like before, those cables that claim to feature platinum in their alloy or even to have a special cristalline structure that favours the signal in a specific direction (how on earth would that work??!?!??!!) won't make *any* difference.   There are other tricks to solve the problem (balanced signals, preamplification before long transmissions etc...)

So you actually need a lot of care and you have a number of problems... But they're so not what the audiophiles claim!",null,0,cdni471,1rgzbv,askscience,new,3
Cyanmonkey,"I find the build of the cable more important than impedance rating, etc.

A properly built cable with Neutrik connectors and strain relief lasts much longer than your cheap Guitar Center POS, but as far as signal goes, as long as your not going over 200' it doesn't make a noticable difference.",null,1,cdnfrd9,1rgzbv,askscience,new,1
EvilHom3r,"For digital (i.e. HDMI), no it does not matter at all. Digital either works or doesn't, there is no in between.

For analog (RCA, speaker wire, TRS wires), you will always get better quality (even if just slightly) with a better wire. However for the average user they will probably never notice the difference, and more likely than not the quality bottleneck is elsewhere in the system.",null,5,cdncqhj,1rgzbv,askscience,new,3
Dyson201,"Not exactly an Audio Engineer, but this isn't a difficult question from a signals standpoint.

Transferring signals through a medium (cable) can pose a variety of challenges that are handled in many different ways.  Without going into extreme detail lets just say that electromagnetic forces could possibly come into play, as well as capacitance to ground producing noise in the circuit, etc. etc.

Long story short, if you're replacing a 3' cable for sound, I highly doubt you'll notice a huge dip in quality between $100 cables and coat hangers.  Both are capable of transferring the signal, and while the expensive cable will transfer the signal with a much greater Signal to Noise ration (SNR), at 3' and with modern noise abatement technology, you would be hard pressed to hear a difference.

Now that being said, I wouldn't wire up your home surround system with soldered together coat hangers, as distance plays a huge factor in the quality of the transmitted signal.  Also, if you buy a cheap ass sound system, expect to hear a big difference in quality between expensive and poor cables, even at 3'.  

Finally, audio quality sound is a very low frequency, and does not travel well over distances with a good SNR.  Quality cables are the only way to increase sound quality over distances (relative term, we're talking meters here not miles).  Technology has come a long way towards discerning the signal from the noise, but any reduction in noise is a huge positive in the quality of the signal.",null,11,cdn6sul,1rgzbv,askscience,new,6
generalelectrix,"This is a very vague question.

For digital signals, yes this does matter.  Digital audio signals require significantly higher bandwidth and run at higher frequencies than the audio content they encode, so transmission line effects become important.  If the characteristic impedance of the cable you use to transmit a digital signal is not matched to the source and destination, you can get partial reflections or standing waves on the cable, which can definitely cause errors in the reconstructed signal at the destination.  This becomes more important with longer cables.

For analog signals, the frequency is low enough that the characteristic impedance isn't really important.  So long as the conductors you're using are low-resistance (copper is great), coat hangers should work just as well as fancy cable.  Shielding in cables is important for line-level interconnects to prevent the cables from picking up noise from the environment, though this usually isn't too big of a problem in a home environment.

The only real exception to this is for speaker cables (carrying post-amplifier level signals) for electrostatic speakers, as the load they present to the driving amplifier is largely capacative.  Then the details of the impedance of the cable driving the speaker become a bit more important.

I'm a physics PhD in quantum electronics with a minor hi-fi addiction.

Edit: I give an in-depth and accurate answer and get a ton of downvotes?  SCIENCE!",null,19,cdn89ly,1rgzbv,askscience,new,5
sever0us,"A meniscus is caused by the ratio of the strength of the cohesive forces of a fluids molecules to each other and the cohesive forces of the fluids molecules to the container wall.

If a fluid has a higher cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a concave meniscus.
If a fluid has a lower cohesive force attracting it to a container wall than the intermolecular forces then the fluid will have a convex meniscus.

Since gels behave is a solid-liquid hybrid way, the presence or absence of a meniscus would most likely depend on the physical properties of the gel. It really depends on weather the cohesive forces described above are enough to deform the gels structure.

TL;DR: It depends on the gel. 'Fluid' gels such as shower gel stand a much greater chance of presenting a meniscus than 'solid' gels like ballistics gel.",null,0,cdn8dfo,1rgzf8,askscience,new,5
StringOfLights,"Yes, it's possible to have multiple ova fertilized by sperm from different men. Sperm can live for several days, and multiple ova can be released over the course of several days in a single ovulation cycle. That means it's possible for more than one ovum to be fertilized and implant, resulting in a pregnancy of multiples with different paternities (I've only ever heard of this happening with twins, but triplets, etc., aren't impossible).

As DNA testing has become more common case reports have come out verifying the different paternities of twins. [Here](http://www.nejm.org/doi/full/10.1056/NEJM197809142991108) is an example from the 1970s, and [here](http://www.fertstert.org/article/S0015-0282%2897%2981456-2/abstract) is one from the 1990s. 

The phenomenon of having two ova fertilized in two seperate coital events is often referred to as ""superfecundation"". It technically refers to any instance in which more than one egg is fertilized in more than one act. Instances where the paternity differs is referred to as ""heteropaternal superfecundation"". [One study estimated](http://www.ncbi.nlm.nih.gov/pubmed/7871943) that 1 in 12 sets of dizygotic twins born to married white women in the US were the result of superfecundation, while 1 in 400 were the result of heteropaternal superfecundation.

Edited for clarity.",null,3,cdn6dbz,1rgzjd,askscience,new,30
claireauriga,"There are definitely equations that can describe what is going on! Heat and mass transfer are an important part of physics and engineering. 

In order to melt, the ice must be raised to its melting point temperature, then given enough energy to melt into liquid. This energy needs to come from somewhere. Heat moves from hotter to colder places, so the warm air will give energy to the ice (and water) until they are the same temperature. 

There are some complications in calculating all this, however. For example, if the air is stagnant then it will get colder as it gives up energy, which means transfer to the sculpture will slow down. If the air is moving, we also have to think about how fast it's going and if it's removing some of the water as vapour too. 

There are many more and less detailed ways of describing what's going on, but in the very simplest terms, the bigger the temperature difference between the air and the ice, the faster energy will transfer. The lower the ice temperature is below its melting point, the more energy needs to be added to make it warm up and melt. ",null,0,cdngq51,1rgzx3,askscience,new,3
RelativisticMechanic,"&gt;Lets say you crush a planet down to mosquito size to form a blackhole.

Alright, we have a black hole of 1 Earth Mass.

&gt;Apparently it would evaporate really fast from your outside frame of reference.

Not really. The lifetime of a Schwarzschild black hole with the mass of the Earth would be about 500 trillion trillion trillion trillion years (as measured by those of us far from the event horizon for the duration).

&gt;But how could any effect pass over the event horizon to reduce the mass of the blackhole?

Nothing necessarily crosses the event horizon; rather, the curvature of spacetime near (but outside) the event horizon produces (nearly) thermal radiation that can be intercepted by those of us far from the black hole. In this process, the spacetime curvature relaxes, manifesting in a decrease in the surface area of the event horizon: the black hole shrinks.

One can, with suitable constructions, model this behavior as a tunneling process whereby particles from inside the event horizon tunnel out; this is analogous to other tunneling behavior wherein particles traverse a classically impenetrable barrier due to quantum mechanical effects.

&gt; I know that things can pass over the EH from their own reference frame - but not from an outside frame.

In fact they *can* cross into the black hole, even in a far-removed frame. The idea that they can't comes from an idealization where you neglect the mass of the infalling object (which we can reasonably assume is very, very small compared to the black hole mass). Even in that approximation, though, if we account for the quantization of light, there will be a final photon emitted from the infalling object. Once that photon is emitted, it will never again be seen by anything outside of the event horizon.",null,0,cdnag5o,1rh0ay,askscience,new,3
Daegs,"The simple version: Because the particle entering the event horizon has negative mass.

When the pair of virtual particles are ""created"", if one sticks around with positive mass, then the other must have a negative mass in order to cancel out (and they must cancel out, no free energy)

So the positive mass one shoots off away from the black hole, and the negative mass one enters the black hole which reduces its overall mass. ",null,0,cdn9pcd,1rh0ay,askscience,new,1
Nicked777,"The Hawking radiation is a deeply quantum mechanical effect, but here is an intuitive way to think about it. The uncertainty principle requires the creation of particle antiparticle pairs, everywhere, all the time. These particles locally violate conservation of energy, which is allowed in QM, as long as it happens on short enough time scales. This means the two particles annihilate very quickly, as if they were never there. 

The point of hawking radiation is if this happens very close to a black hole's event horizon, one of the particles can get sucked in, and the other will escape, albeit very reduced in energy from its trip. Because of this the Hawking radiation is believed to be very weak. A specialist in this topic could explain why it seems to be only the anti particles that fall in, and why we think this admittedly bizarre idea could me true, but I don't know off the top of my head. ",null,1,cdn9rz6,1rh0ay,askscience,new,1
EdwardDeathBlack,"Assuming you use the European convention of having a comma instead of a decimal point, you would get indeed 350,000 people. 

I find the idea of a 6.5 GWh plant weirdly low. A nuclear reactor can easily be a 1GW thermal, assuming 35% conversion efficiency, that's 350 MW electrical. Assuming 90% uptime , that'll be 365 * 24 * 350 * 0.9=~2800GWh. Most nuclear power plants have four or five reactors, so can easily generate 10,000GWh per nuclear plant per year. So a power plant with a total capacity of 6.5GWh per year certainly seems puny by modern energy use. Then again tidal is really not much of an energy source, more of a public relation toy , so maybe it is that puny.",null,1,cdn799s,1rh0eq,askscience,new,3
E_F_F_E_C_T,"So using this site for KWH/capita for china gave me 3,300 KWH/capita -http://data.worldbank.org/indicator/EG.USE.ELEC.KH.PC

Then using this site for the station's output - http://en.wikipedia.org/wiki/Jiangxia_Tidal_Power_Station

The instantaneous power of the station is 3,200KW (we'll ignore the solar stuff).

Multiply this by the amount of hours in a year gives you 28 GWH.

Dividing this by the 3300KWH/capita gives us roughly 8500 people.

Considering the second Wikipedia article states that ""The power station feeds the energy demand of small villages at a 20 km (12 mi) distance, through a 35-kV transmission line."" I feel that this isn't that unreasonable.

Hope this helps.",null,0,cdn7d47,1rh0eq,askscience,new,2
super-zap,"Compared to most other large power plants your favorite tidal power plant is tiny. 

http://en.wikipedia.org/wiki/List_of_largest_power_stations_in_the_world

It has 1000 times less generating capacity than most of the large ones and almost 6000 times less capacity than the largest power plant.

So, overall it is not surprising that it can generate power for only 350 000 people. I believe your math is correct.",null,0,cdn7dqd,1rh0eq,askscience,new,2
battlehawk4,"The sonic boom is happening constantly, and only stops when the plane reduces speed to under the speed of sound. On the ground, you hear one bang. But if you were really close, you would usually hear 2. One for the nose, and another for the tail. The space shuttle was known for this. But by the time the compression wave, aka sonic boom, reached you on the ground the waves are combined into one. 

Anyway, the 'bang' is moving across the Earth with the plane, but slightly behind it. So your friend a mile further away from the plane would hear the bang slightly after you heard it. This is because the shock, and therefore 'bang', takes time to move through the air (at the speed of sound). I which I could draw good diagrams to explain this, hopefully the words work. 

Source: Aerospace Engineer",null,8,cdn74cj,1rh337,askscience,new,50
omardaslayer,"A sonic boom is basically like a wake coming off a boat.  It's a continuous compression of air made by the vehicle moving faster than sound can travel in the medium.  It is in existence the entire time that the object is going faster than sound, stops when it slows back down.  You only hear one boom however because the wave only passes you once.",null,0,cdnfi72,1rh337,askscience,new,5
elbs5000,"The short answer is: it does. The space shuttle creates a ""sonic boom"" as it decelerates below supersonic speed as it's entering the atmosphere. Any time an object moves faster than the speed of sound, it is travelling at ""supersonic"" speed. The boundary of faster or slower than the speed of sound at room temperature (768 mph according to wikipedia) is what creates the ""boom."" Basically you are creating sound but travelling at the same speed as the sound you create; building that sound up around you, until you break the barrier by either moving faster than the sound (basically outrunning it) or moving slower than the sound (letting the sound outrun you). The longer you stay exactly at the speed at which the sound you are generating is travelling the more energetic your ""boom"" would be. When humans were first approaching supersonic flight it was deemed extremely dangerous becuase the accumulated vibrations (all sound is in the end) could potentially shake apart the craft you were in due to the weaker design, materials, and construction techniques they had back then, but also because the crafts could not move past the barrier in a fast enough fashion (without a gravitational assist I must add. Let gravity help you accelerate and it becomes easier). We've since mastered techniques to build crafts that easily reach supersonic speeds and maintain their integrity.",null,0,cdnfcss,1rh337,askscience,new,3
Platypuskeeper,"The color depends on the coordination environment of the Cu(II) ions that are formed. In a concentrated nitric acid solution, the copper ions coordinate to nitrate ions, giving a green/greenish-blue color. If the solution is more dilute (or diluted after oxidizing the copper), then you get a blue solution where the Cu(II) ions are coordinating to water instead.

And on a safety aside: Who the hell are these fools who play around with concentrated HNO3 outside of fume hood? That brown gas is toxic nitrogen dioxide!
",null,1,cdn9hbo,1rh4eg,askscience,new,4
Voerendaalse,"In the ovary of a woman, a lot of eggs are present in an immature state, not ready to be fertilized. So normally during a woman's cycle, a few eggs start maturing. One of them wins and will be released to perhaps be fertilized, the others will die. The process of an egg maturing and then being released is called ovulation.

The hormones of the birth control pill will prevent the maturation process. No eggs will start to mature, no eggs will become mature and be released.

One source: http://en.wikipedia.org/wiki/Combined_oral_contraceptive_pill#Mechanism_of_action",null,23,cdna0r3,1rh4yb,askscience,new,105
vhaaurgh653,"Actually when a woman takes birth control or ""the pill"" she still menstruates. 
There are four ways the pill acts to stop sperm reaching an egg. First, the hormones in the pill try to stop an egg being released from your ovary each month. This is known as the suppression of ovulation. Research has shown that neither the progesterone-only pill nor the combined progesterone-oestrogen formulations always stop ovulation.

Second, all formulations of the pill cause changes to the cervical mucus that your body produces. The cervical mucus may become thicker and more difficult for sperm to fertilize an ovum.

Third, all formulations of the pill cause changes to the lining womb; the lining of the womb doesn’t grow to the proper thickness. This change also means that the womb is not in the right stage of development to allow a fertilized egg to attach properly.

Fourth, the pill causes changes to the movement of the Fallopian tubes. This effect may reduce the possibility of the ovum being fertilised.

So basically the pill does not stop an egg from dropping, it just makes the environment very difficult to conceive in and it is not always 100% preventative. 
",null,25,cdn9zjp,1rh4yb,askscience,new,38
Heal_With_Steel_MD,"To answer you're question:The birth control pill delivers a fixed low dose of progesterone and  usually estrogen to the blood stream.  This  in a way, provides negative feedback on the release of gonadotopins (FSH &amp; LH) by the adenohypophysis (Anterior Pituitary) which prevents the rise and peak of estrogen accumulation. This is the important part because --&gt; No estrogen peak, no LH surge; no LH surge, no ovulation; no ovulation, no pregnancy.  So the eggs that are not being fertilized, regress, they are typically not ""stored"" for future use.
",null,0,cdnv90h,1rh4yb,askscience,new,1
Platypuskeeper,"Two reasons. 1) Most chemical reaction rates increase exponentially with temperature. Water leaching into some stuck food, or something dissolving are chemical reactions. 2) The solubility of most (solid) stuff tends to increase with temperature.
",null,1,cdndlk4,1rh5ch,askscience,new,7
SimpleBen,"The viscosity of fats is dramatically altered by temperature. Think about it. Bacon fat in the package is nearly solid, but at around 200 degrees F it is pretty liquid. Fat changes so much with temperature that it is by far the dominant reason that warm water cleans better than cold (not to mention the fats in the soaps!) ",null,0,cdngody,1rh5ch,askscience,new,4
ThePsuedoMonkey,"Clothes dryers function by evaporating the water in the clothes, and the rate of evaporation of a liquid is directly related to its vapor pressure.  The vapor pressure of water is an [exponential function of temperature](https://en.wikipedia.org/wiki/Vapor_pressure#Boiling_point_of_water), roughly 2.5kPa at room temperature and 101kPa when it boils.  An electrical heating element in a dryer will create heat by electrical resistance, and [Ohms Law](https://en.wikipedia.org/wiki/Resistor#Power_dissipation) states that the power dissipated by a resistor is the product of its resistance and the voltage that is applied to it.

If the amount of water in the clothes were sufficiently small, this would mean that it would be more efficient to dry them at high heat in an enclosed space (do not do this, it is fire hazard).  However, [there is likely](http://www.verber.com/mark/outdoors/gear/clothing-waterabsorption.html) a significant amount of water remaining in the clothes, and based on the room temperature vapor pressure, each kilogram of water will need 44 liters of completely dry air in order to fully evaporate in (which could become a corrosion or electrical hazard when it condenses after the dryer cools).

Because of this, air is vented through the dryer to expel the water-saturated air.  This additional air that must be heated, and there is no guarantee that all of it will be water-saturated by the time it is expelled, but the act of venting air can also help promote evaporation.  The amount of energy lost due to venting is proportional to the dryers temperature and the flow rate of the air, and the amount of energy lost due to thermal radiation is also proportional to the dryers temperature difference with the ambient air due to the first law of thermodynamics.  Reducing the temperature setting would reduce both of theses losses for any given moment, though the drying period would significantly increase due to the associated drop in vapor pressure.  However, without a better understanding of the effects of ambient humidity on evaporation, or of the efficiency of the electronic components at low output I am reluctant to say for certain.",null,18,cdnb7zo,1rh9np,askscience,new,84
BigWiggly1,"There are a lot of factors brought up by ThePsuedoMonkey's comment, and I recommend reading through his comment as well.

I'm going to go with a bit of gut instinct and tell you **no**. I will proceed by first explaining relative humidity, followed by how the dryer is working, and finally returning to the answer of your question.

For water to evaporate, the air it's in contact with needs to be able to hold it. The air's ability to hold it is measured as it's **Relative Humidity (RH)**. As heat is added to the air, its relative humidity decreases and it can hold more water.
Additionally, the lower the RH of the air is, the more quickly the water is able to evaporate. As water vapour saturates the air, the RH goes up and it becomes more and more difficult for that volume of air to pick up more water (Imagine it's arms are full and the more full they get, the more stuff they drop each time they bend over to pick up something new. Eventually it drops water just as fast as it picks it up).

Dryers work by taking air from the outside (ambient air), raising it's temperature, and circulating it through the dryer before sending it back out through the lint screen. The dryer would work, albeit rather slowly, without heating the air. Ambient air is usually not at 100% RH, so it can still hold more water. Lets say you put your clothes in for 60 minutes. At high heat, that's enough to dry them to your liking. At no heat (tumble only), they may still feel damp. Lets rule that out as not an option, because you've got somewhere to be in an hour and your favourite pants just got out of the washer.

By heating the air, the RH of the inlet air to the dryer is lowered as it's temperature rises, giving it the ability to hold more water (I guess it has bigger arms?). This means that for every volume of air that goes through the dryer, more water comes out with it. Additionally this helps to speed up the last bits of drying, where there isn't much water left. Warmer air will also heat the clothes, giving the water some extra energy to boost themselves into the vapour state so it can be carried off. Without heat, the last bits of water are simply too cold to evaporate quickly enough.

To address the energy efficiency:

Heating requires a lot of energy. Any heating process is a fairly inefficient process. Resistor heating elements are good at what they do, but nobody ever claims for them to be efficient. Moving air on the other hand is relatively easy to do (as long as you clean your lint screen). It's much more efficient to pump a volume of fluid (air) than it is to heat that same volume.

In the first stages of drying, there is so much water on the clothes that regardless of how warm the air is, it will saturate with water. There's simply an excess of water. This may make you say ""So lets heat it even more and it'll take more out with each chunk of air right?"" Yes, you are right. *Instead though*, we know it's cheaper to move air than it is to heat it, so let's be patient and let the moving air do it's work. In fact dryers would be more efficient if they increased the air flowrate in the early stages of the drying cycle, and decreased the heating requirements.

As mentioned earlier, in the last stages of drying when there isn't much water, warmer air is able to force water out of it's little microscopic nooks and crannies by giving it more energy. At this point, air circulation isn't as important because there isn't enough water in the clothes to saturate the air that's in there anyways. Now, air circulation is only to prevent overheating that could cause a fire hazard. Still, every bit of air that gets heated and then vented too soon is a waste of energy.

So now that we've covered what is good at the early and late stages of drying, we can make general statements on what the most efficient dryer would do: Start out on low heat with high circulation, followed by a steady increase of heat until finishing while the air flowrate is decreased proportionally to the temperature, but always above a minimum flowrate to prevent overheating.

Since I don't know about any of these fancy dryers on the market, and most people are tempted to use the timed dry options rather than an auto-dry option (which uses an RH or moisture sensor to determine when to stop drying), I will say that it is most efficient to stick to the least amount of heat necessary to get your clothes to a satisfactory level of dryness, because heating is the most inefficient process in your dryer.

As a good compromise between length of cycle and heating required, **use medium heat**. I've noticed that medium heat often doesn't take noticeably longer than a high heat cycle, and does the job well enough.

Alternatively, if you're looking to be the most efficient you could dry on low and manually increase the temperature every 15 minutes or so.

If you're a dryer manufacturer and reading this, consider making the auto-dry cycle adjust airflow and temperature based on the RH leaving the dryer (based on the sensor already in the installation). ",null,2,cdne9bx,1rh9np,askscience,new,13
RabidRabb1t,"This all depends on what you mean by ""efficient.""  Since I can just hang my clothes up and watch them dry (although it takes some time), the application of any extra heat that I then shunt outdoors is clearly wasteful.

If by efficient, you mean in terms of the product your time waiting and cost of drying (economic efficiency), that's a slightly more interesting question.  There are two things to consider: first, that the energy required to vaporize the water in your clothes from room temperature is essentially a constant.  Secondly, the rate of energy transfer is related to the temperature difference between the air and the clothes by an exponential function.  Now, if you keep running hot air over your clothes at a constant rate, relying on the efficiency of energy transfer, we can now figure out the function form of our economic efficiency.
  
Assuming you charge an hourly rate, the opportunity cost to you is simply your rate, R, times the amount of time, time, that it takes to dry your clothes.  The cost to you on your electric bill is the time it takes to dry your clothes multiplied by your power company's rate, P, and the rate of energy usage, E.  Since resistive heating is ~100%, we're going to make the approximation that E proportional to the amount you heated your clothing up.  The only thing left is how much time it takes as a function of temperature (exponential).

So, you're left with:

cost = R * [1/[exp(E)]^2 * P * E

where time is 1/[exp(E)].

Since this function goes to zero very fast, the short answer is that yes, higher temperatures are good for your wallet.  Please note that I did leave out a massive fudge factor, namely that the amount of waste heat is also going to increase in this model since I did not actually bother to take the integral of the exponential; however, the point remains. ",null,0,cdnh7l7,1rh9np,askscience,new,2
LWRellim,"Per [this study (see page 11)](http://www.aceee.org/files/proceedings/2010/data/papers/2206.pdf) a “low heat” setting is more efficient than higher heat settings. 

However, the energy usage difference is not as large as most may think -- what the ""high heat"" setting mostly achieves is apparently just a (slightly) shorter drying time -- and the additional energy expended to heat is offset by the fact that the machine itself (and thus the fan/airflow) runs for a shorter time.

The study includes the following recommendations:

&gt;**Advice to Consumers**

&gt;Consumers can dry clothes with less energy by using (in order of energy savings):    

&gt;1. Outdoor clothes lines get clothes dry using no energy and with no HVAC impacts.    
&gt;2. Indoor drying racks use no direct energy but do have an HVAC impact. The total energy impact is lower than any currently available dryer.    
&gt;3. A natural gas dryer is cheaper to operate and has lower environmental impacts than an
electric dryer.    
&gt;4. High washer spin speeds are more [energy] efficient than evaporating the water in the dryer.    
&gt;5. Drying full loads is more [energy] efficient than a larger number of partial loads.    
&gt;6. A “low heat” setting is more [energy] efficient than higher heat settings.    
&gt;7. A “less dry” setting is more [energy] efficient than “normal” or “more dry.” 

Note that I added the ""[energy]"" in there in a few points, because it is obvious from the context that is what they mean by the use of the word ""efficient"" -- which by itself is otherwise an ambiguous word (i.e. something can said to be more ""efficient"" if it gets the job done faster -- so to a consumer a machine that lets them do 5 full loads within 2 hours will be more ""efficient"" than one that only does 3 loads in the same time period.)

**One of the things that they fail to note -- probably THE easiest way people can reduce laundry energy use -- is to just do LESS laundry!**  Most clothing doesn't need to be tossed into the laundry bin every time you ""touched/wore"" it.
",null,1,cdnislo,1rh9np,askscience,new,3
c8726,"I would say the high heat would be more efficient. 

Drying clothes is just a phase change from liquid to gas. The total energy required to evaporate the water would be the same regardless of the setting. The energy required would just depend on the initial temperature of the clothes, the amount of water in the clothes, the specific heat of water (4.186 kJ/kg K) and the heat of vaporization for water (2260 kJ/kg). 

Lets say we have m=5 kg of water in our clothes close to room temp, To=300K. We need to heat the water to Tb=373K, the boiling point of water at STP, since we are in an open system to the atmosphere. 
We need 4.186 kJ/KgK x m x [Tb-To]=1,527.89 KJ to raise the temp up to boiling point of water. Now we need 2260 kJ/kg x m=11,300 kJ to vaporize the water. In total, 12,827.89 kJ or 35.76 kWh of energy is needed to evaporate the water.

If you assume that the dryer for both cycles is able to heat the water to the boiling point and the  rate of heat absorption to be the same for both cycles, the only thing that matters is the duration of which the motors run to spin the drum and blower. Therefore, the high heat setting would be more efficient.

What really would save energy reducing the amount of water in your clothes. A high speed spin cycle or a centrifugal dryer thats extracts a higher percentage of the water out would save much more energy than selecting a heat setting. ",null,2,cdnev36,1rh9np,askscience,new,2
null,null,null,2,cdnev7c,1rh9np,askscience,new,2
Hiddencamper,"In a nuclear reactor we use the fission process to release energy by splitting the atom. 

For the case of uranium235 the fission process looks roughly as follows

U-235 + n -&gt; Fragment1 + Fragment2 + ~2.4 n + energy

Those fragments are also known as fission products and are somewhat random in size. There is a statistical probability of what you can get. See the image at the top of this Wikipedia page. The fission fragments are where you get all the well known products like xenon, iodine, strontium, cesium, etc

http://en.m.wikipedia.org/wiki/Fission_products_(by_element)

Basically in a nuclear reactor there are fission products that are a result of splitting the atom, and there are also transactinides. What also happens, is the U-238 and U-235 can absorb neutrons but not undergo fission, causing them to become other heavy elements through a series of decay chains. ",null,1,cdnccjy,1rhamu,askscience,new,5
Proxymace,"In the uranium mined from the earth the ""active"" isotope makes up appx 0.7% this is enriched to appx 8-10% depending on the type of plant that will use it so there is a substantial amount of material that will be irradiated and will then decay into different elements to the ""active"" one",null,3,cdnbga6,1rhamu,askscience,new,1
patchgrabber,"This is an unanswerable question. Different organisms mutate at different rates, we don't know exactly when life began (who knows how many different types of microorganisms were around near the beginning), the way a species is distinguished from another is inherently arbitrary, and we have no idea how many species have ever existed.",null,0,cdncaz4,1rhbru,askscience,new,6
biorad17,I've seen  estimates of this.  IIRC you only need one specieation event every million years or so to account for every species.  It's important to note that estimations like this are not necessarily biologically accurate.  They are mathematical models that provide parameters to begin thinking about evolution.,null,0,cdob0of,1rhbru,askscience,new,2
Karnivoris,There is not much change at the bottom of the trench by inspection if you look at the size of the globe in comparison to the depth of the trench.,null,1,cdnrtdc,1rhdel,askscience,new,3
vashoom,"You can use Newton's Universal Law of Gravitation to calculate a decent approximation.  If the average radius from the center of the Earth to the surface is 6,371 km and the trench is 10.9 km deep, simply plug in 6,371 - 10.9 = 6360.1 km.

Crunching the numbers (gravitational constant times mass of earth divided by that radius (6360100 m) squared, gives me 9.8473 m/s^2.  So just a tiny bit above the average gravitational acceleration on the surface.",null,3,cdndqjv,1rhdel,askscience,new,3
null,null,null,5,cdnia0n,1rhdel,askscience,new,5
eebootwo,"As said by 1992^^?, not much different. However, depending on the object underwater, it might accelerate upwards due to buoyancy, or downwards faster than GM/r^2 if it is denser than water: which would be if it were a gas compressed to greater density than 0.998 kgm^-3",null,5,cdnnrfp,1rhdel,askscience,new,1
Christmas_Pirate,"All right my time to shine.  First lets examine exactly how information is stored on a DVD, before we get to how much information can be stored.  As I am sure you know, information is stored by essentially burning little holes in a metallic film in the DVD (not completely accurate, but a reasonable enough description of what is going on).  As technology progressed we have been able to burn smaller and smaller holes, hence the larger storage capacities.  Additionally we have been able to burn different ""types"" of holes I.E. DVD +/- (Dual layer literally means two layers of the metallic film, so double the storage capacity per square inch of DVD), thereby allowing more data to be stored since it could be stored in 3 variations of holes, if you will, instead of two (hole and no hole).  The [Wiki](http://en.wikipedia.org/wiki/DVD) page goes into detail about this, so I wont bother.  

Now, while we have been able to make smaller and smaller lasers, we have not been able to change the laws of physics, one of which is all wavelengths have a [diffraction limit](http://en.wikipedia.org/wiki/Diffraction-limited_system).  Essentially, no matter how good your lens is, you can't focus a beam of light to a point smaller than half it's wavelength, and this is the hurdle consumer products have yet to overcome.  Blueray DVDs can store more information because the laser being used to burn them is blue, which has a shorter wavelength than red or IR (the other commonly used lasers).  The cost of the respective machines has a lot to do with manufacturing of the diodes, but I digress.

Now to the meat of the question; how much data can we store on a DVD?  Well that all depends how small we can make the burns.  Recently technologies have been developed that allow us to make tiny, tiny, burns.  How tiny?  From what I've read they claimed to be able to store 1 petabyte (that's 1,024 terabytes or 1,049,000 gigabytes). [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306).  How did they do this?  Well you're just going to have to do a little bit of reading to find that out.

**TL;DR:** Storage is limited by the size burn we can make with a laser in a thin metallic sheet inside the DVD.  The smallest burn we've made allows us to store roughly 1,000 terabyes or 1,000,000 gigabytes, although the technology to do this hasn't been made available to consumers.  It should be shortly as it doesn't use any novel technology, just a novel way of burning with current technologies.  [Source](https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306)

**Edit**  Added my source to the TL;DR for those of you too lazy to find it in the post.  It's worth a read.",null,29,cdnc6s9,1rhdi6,askscience,new,129
anantha92,"Dual layered DVDs have been around a long time, almost all movies you buy with the extras as well as almost all Xbox 360 games and some PS2 are dual layered DVDs. A single sided dual-layer DVD holds 8.5 GB of data. The 6.1 GB you are referring to is how much of the 8.5 GB is used.",null,18,cdna45d,1rhdi6,askscience,new,85
colin-broderick,"A dual-layered DVD can hold about 8.5 GB.  It says there is zero space remaining, even though it's not full, because the disk has been marked unwritable.  Also, some of the remaining space is generally used for redundant data to protect against physical damage and is not reported in the total, even though the disc may be physically full.  I think (although don't quote me on it) that copy protection data can also be included in this invisible fashion.

Most blank DVDs are single-layered, and hence lower capacity.  4.7 GB is typical.  You can buy dual-layered blank discs but they took far too long to become available and never got cheap enough to be adopted in a big way, so you don't see them often.",null,3,cdnc0md,1rhdi6,askscience,new,27
whosaidmaybe,"I don't think your question has been truly answered yet.

As stated on [wikipedia](http://en.wikipedia.org/wiki/DVD), here are your various sizes for DVD Discs - 
4.7 GB (single-sided, single-layer – common)
8.5–8.7 GB (single-sided, double-layer)
9.4 GB (double-sided, single-layer)
17.08 GB (double-sided, double-layer – rare)

The unit of measurement, however, is in **decimal metric** - which has base units of 1000. 1000 bytes = 1 kilobyte, 1000 kilobytes = 1 megabyte, 1000 megabytes = 1 gigabyte.

Computers count in **binary** and have base units of 1024. So 1024 bytes = 1 kilobyte - so on an so forth.

Therefore, the capacity of a 4.7 gigabyte DVD is 4700000000 bytes in decimal. But when divided by 1024 kilobytes, 1024 megabytes, and 1024 gigabytes, the capacity is ~4.48 gigabytes. Once the disk is formatted you may lose a few more megabytes.

This same principle can be applied to the other sizes of DVD's as well as hard drives - which are sold with the same confusing capacity claims. A 1 terabyte hard drive (1000 gigabytes, 1000000 megabytes, 1000000000 kilobytes, 1000000000000 bytes) as labeled by the manufacturer. A computer will see it as the binary capacity of 976562500 kilobytes, 953674 megabytes, 931.3 gigabytes.

Once you format the hard drive disk you lose a few more kilo/megabytes, but essentially, you have 931 gigabytes.

The reason why your DVD has a capacity of 6.1 gigabytes is because it started off as a 8.5–8.7 GB (single-sided, double-layer) disc, and once the data was burned / copied to the disc - the disc was **mastered / finalized**. This process completes the disc and does not allow any more data to be written to the disc. The DVD will now report to the computer the total size of the data written to the disc.

[edit] typos, grammar, etc.",null,0,cdnf2ri,1rhdi6,askscience,new,8
null,null,null,3,cdn9iux,1rhdi6,askscience,new,3
BastardOPFromHell,Has anyone mentioned double-sided? I have some in my desk. I went to buy double-density because I needed to store a file that was about 5.5GB. But what I got will only hold 4.7GB on a single side. Then you turn it over and write 4.7GB on the other side. Don't really care for them myself because they don't have a label side to write on.,null,0,cdnf8eu,1rhdi6,askscience,new,1
idgarad,"That is subjective at best by data? I can for instance in the following

    1010010101010101011110101001

I could say that is 32 bits of data.

I could also that that in that 32 bits I can have 4 bytes. Ironically though as far as data goes it actually I can get 58 unique bytes of data out of 32 bits. I think you want how much storage in a given unit rather then just ""data"".
",null,0,cdnh1h5,1rhdi6,askscience,new,1
mobchronik,"A company in Australia actually developed a new method for burning data to normal DVD-R discs. See link below:

https://theconversation.com/more-data-storage-heres-how-to-fit-1-000-terabytes-on-a-dvd-15306

The maximum amount of data that is able to be burned onto a DVD-R has more to do with the diameter of the beam from the laser that is burning the data. I believe the current standard beam diameter is 38 nanometers which would limit a regular DVD-R to about 4.7 gigs of storage. But with this new method that has been developed, the beam has been reduced to 9 nanometers increasing the data storage to up to 1000 Terabytes or 1 Petabyte. They have successfully burned 1 Petabyte to a standard DVD-R, and the cost of this new DVD-R burner will actually be close to the same cost of current DVD burners due to the fact that it uses the same technology just slightly modified.",null,0,cdnj5de,1rhdi6,askscience,new,1
kamikaz1_k,"While there are a lot of good answers in this thread, I feel as though many of the simple questions could have been answered by Google instead of posting in this thread and waiting for a reply. 

/rant 

Carry on fine sirs... ",null,0,cdom6w8,1rhdi6,askscience,new,1
ww-shen,"So, lets put this question to an another level.
The technology of early CD-s and  modern bluray is essentally the same. The data is written in the surface of a plastic disk, the difference is the size and denseness of these 'pits' (small holes on the disk). As the technology improves, the precisity of the positioning of writing mechanism and speed of chips makes possible to create disks with more space to store. (blu ray uses two layers instead of one) It could be possible to burn more data on a plastic disk. (the analogy is the same as the hard disks have evolved) if we compare a CD to an early hard disk, and imagine the same amount of advance as it happened ind hard drives, the result could be 100-300 Gb/CD disc. The only couse of nobody invenst in evolving them is that CD has many disadvantages (easly broken) and flash storage has more potentional.",null,10,cdnatd4,1rhdi6,askscience,new,6
Thandius,"People have already covered the sizes of DVD's and the differences between each.

However your initial question is about the maximum amount of data that it's possible to store so lets take an 8.5 GB DVD

We know that due to formatting and a number of other fun things needed to make them work correctly you don't get that full whack.

However you can increase the amount of data stored on this DVD through compression. Most people will be familiar with this as .zip or .rar files which can compress the amount of data into a smaller file size and thus allowing you to store more data on the DVD than before. 

If we are talking about video then we can use a codec (DivX .H264 etc) which effectively does the same thing where it compresses the data into a smaller amount of space allowing you to store a larger amount of Data on the same DVD.

as such this effectively increases ""The maximum amount of data that can be stored on different types of DVDs"".

",null,11,cdndlho,1rhdi6,askscience,new,2
medstudent22,"There are several known benefits to neonatal circumcision. 

- **It prevents penile cancer.** Squamous cell carcinoma of the penis is exceedingly rare in circumcised patients. Circumcision alone may not be the preventative measure. [Phimosis](http://en.wikipedia.org/wiki/Phimosis) (the inability to retract the foreskin) can only occur in non-circumcised individuals and is associated with a higher risk of penile cancer. Phimosis, in many cases, is preventable with adequate hygiene. It should also be noted that penile cancer is extremely rare 1-2 out of 200,000 men per year. Also worthwhile to note that somewhere between 909 and 322,000 circumcisions would need to be performed in order to prevent one case of penile cancer. 

- **It reduces the risk of UTIs in early life and up to 5 years of age.** Uncircumcised males are 20x more likely to develop a UTI during the neonatal period. It should be noted that 111 circumcisions must be performed to prevent one UTI though. Some cost analyses have shown that there is still a cost benefit to performing circumcisions when just considering UTIs though.  
 

There are some claimed benefits of circumcision with varying amounts of evidence. 
 
- **It may reduce the spread of HIV** (to men, in heterosexual relationships). This is based on several large African clinical trials. It was not found to reduce the risk of transmission to women and has not been shown to reduce the risk of transmission in homosexual male couples. 

- **It may reduce the transmission of HPV and herpes (HSV).** In a study of 3393 men (1684 who underwent circumcision), after two years, 7.8% of the circumcised men had HSV-2 antibodies, 10.3% of the uncircumcised group did. In the same study, 18% of the circumcised men had evidence of HPV, 27.9% of uncircumcised men did. ([Study](http://www.nejm.org/doi/full/10.1056/NEJMoa0802556)) It should be noted that this study was performed in Uganda. Also worthwhile to note that most individuals clear HPV spontaneously and also that a [vaccine is available](http://en.wikipedia.org/wiki/Gardasil) for the most common HPV strains. Also worthwhile to note that HPV is associated with penile cancer, but more importantly cervical cancer in women.  

The reason I tried to note the conclusions which were drawn based on African studies is that the underlying prevalence of disease has an effect on the study and these results may not be considered generalizable to other populations.  

Multiple groups have issued statements on neonatal circumcision which may contain more information that may be useful to you. 

[The American Academy of Pediatrics](https://www.google.com/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=1&amp;ved=0CCwQFjAA&amp;url=http%3A%2F%2Fpediatrics.aappublications.org%2Fcontent%2Fearly%2F2012%2F08%2F22%2Fpeds.2012-1989&amp;ei=FrGUUpKsOZHlsATz34HABA&amp;usg=AFQjCNGNikptx2aRUdOftngXQ3JyIFWU5g&amp;sig2=MiBQ3NWjASCvuKD_PjKXHw) states: 
&gt;Evaluation of current evidence indicates that the health benefits of newborn male circumcision outweigh the risks and that the procedure’s benefits justify access to this procedure for families who choose it. 

[The American Urological Association](http://www.auanet.org/about/policy-statements/circumcision.cfm) states: 
&gt; neonatal circumcision has potential medical benefits and advantages as well as disadvantages and risks.",null,4,cdne4ln,1rhdmd,askscience,new,17
redallerd,"Yes. Since there is no limit as to how small fractions can be, there can be an infinite amount between two whole numbers. If you're finding it hard to understand, try adding halving fractions to see if you can get to a whole number for example : 1/2 + 1/4 + 1/8 + 1/16 and etc.",null,0,cdndbyb,1rhegr,askscience,new,10
Captain-Negative,"Yes. For example, consider all finite strings of digits beginning with ""1."" and ending with a varying number of twos.

1.2, 1.22, 1.222, 1.2222, 1.22222, etc.

If you count these numbers one by one, you'll notice that it goes on infinitely long. This is a type of ""countable infinity"" (alternatively, [aleph](http://en.wikipedia.org/wiki/Aleph_number)-zero or [beth](http://en.wikipedia.org/wiki/Beth_number)-zero), because it's an infinity arising from (surprise surprise) a list you can count through.

However, there are even bigger kinds of infinity -- each of which is ""uncountable"" -- one of which describes the number of numbers between 1 and 2. As it turns out, there's no way to come up with a method to list all numbers between 1 and 2 one-by-one simply because there are way-too-fucking-many of them. Perhaps paradoxically, though, you can show that describing all the numbers between 1 and 2 is just as hard as describing those between and 1000000, so the two infinities are actually said to be the same (specifically, it's called the ""beth-one"" infinity).

Very roughly speaking, it is unclear if there is an infinity between beth-zero (countable infinity) and beth-one (the type of incountable infinity we just discussed). Whether or not this ""beth-half"" exists depends on how you decide to model the world, and is the topic of something called ""[the continuum hypothesis](http://en.wikipedia.org/wiki/Continuum_hypothesis)"" in mathematics.",null,0,cdnee15,1rhegr,askscience,new,2
xavier_505,"This is likely an issue with the way the MPEG streams are configured. Generally MPEG-2 encoding (I am not especially familiar with MPEG 4 but I would hazard a guess that it is similar, or at a very minimum has great flexibility in its configuration) uses three types of frames: I, B and P.

- I (intra) frames are full frames of data encoded similarly to JPEG (~7:1 compression). These do not reference any other frames.

- P (predicted) frames only have information describing the difference between the preceding I or P frame (lower size than I frames, ~20:1 compression).

Both I and P frames are called ""reference frames"" since the information they describe can be referenced by other frames.

- B (bidirectional) frames are encoded from interpolation of preceding/following reference frames (even smaller than P frames, ~50:1 compression).

These various frame types are typically sent in the following way:

    I B B P B B I B B P B B I P B B....

Where the distance between I/P frames (in this case it is 3) is configurable as is the distance between I frames (in this case there are 6).

Why am I telling you this!? Well, a P frame cannot be decoded without its referenced I frame, and B frames cannot be decoded without all of their referenced I/P frame. So, the longer the distances mentioned above, the greater affect a lossy channel will have. You would see the behavior you are describing in the following case:

    MPEG2 GOP structure (m=2, n=4): I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B P B I B ....

    MPEG4 GOP structure (m=4, n=16): I B B B P B B B P B B B P B B B I B B B P B B B P B B B P B B B ....

A lost I frame on the lower stream would blow away 16 frames of data, while on the upper frame only 4 frames. Similar for lost P frames.

Edit: Cleaned up description of IBP frames.",null,0,cdnfw8p,1rhf15,askscience,new,3
chucklesMtheThird,"A total stab in the dark here....it could be that MPEG4 channels are sent on higher QAM constellation carriers, which are more susceptible to minute changes in signal quality, and the MPEG2 are sent over lower constellation carriers?

For example, QAM64 receivers have a much higher tolerance range in amplitude and phase angle error than do QAM256 or QAM512 because of the lower symbol density. The higher you go in constellation density, the less room there is for error.",null,1,cdney2o,1rhf15,askscience,new,2
baloo_the_bear,"Behavioral illness do exist in animals, and can be treated with neuroactive compounds. Some causes are organic, such as in prion disease (mad cow) some causes may be structural, and some causes may be chemical. 

Behavior is ultimately a result of how the brain is working (thinking from a completely mechanistic point of view), so any aberration in brain function can lead to an aberration in behavior.",null,3,cdndv4e,1rhi55,askscience,new,32
null,null,null,1,cdnfj49,1rhi55,askscience,new,10
null,null,null,0,cdngnvj,1rhi55,askscience,new,7
Accujack,"There's some interesting stories I remember from college of psychopathic behavior on the part of a chimpanzee duo.  Mother and daughter, they were notable for cooperating in elaborate ways to steal and kill the baby chimpanzees of other mothers.  As I recall, when the mother chimp died, the daughter stopped killing after that point.

I looked, but couldn't find a reference to this unfortunately.  Hopefully someone else can come up with it.

I do also know there's a lot of documentation of chimps waging war against other groups of chimps for resources, and chimps ""murdering"" other chimps for anything from mates to meat.  Here's a video:

http://www.youtube.com/watch?v=CPznMbNcfO8

and an article:
http://phys.org/news196342222.html

Ultimately I think whether these sorts of things are considered ""mental illness"" depends on the point of view of the species involved.  Chimps seem to consider random killings of other chimps as more or less normal.

I think to prove mental illness in any species, we have to have a very good understanding of that species' behavior and/or brain functioning.  We can tell when eg. a pet has issues with abandonment or has self destructive behaviors like biting their own fur off, but we don't have more than a general idea of the mechanism behind them.

The same is true for humans.  If you look at the DSM and the general furor around publication of new versions, you know that we generally define human mental illness by symptoms rather than causes.  No one really knows what causes clinical depression for example, and no one honestly knows with certainty why certain drugs treat it.  We believe we know why they work, but the mechanism is so complicated it's difficult to prove so far.

So I guess the answer to OPs question is that we know that some animals have behavioral issues from our point of view, but the definition of those behaviors as illnesses is subjective.


",null,1,cdni1gq,1rhi55,askscience,new,8
inertia,"Sure they can. It's not uncommon for [military dogs to suffer from PTSD](http://www.bbc.co.uk/news/world-us-canada-10873444), for example",null,1,cdnfar1,1rhi55,askscience,new,6
woody121,"I find this question extremely interesting and am not satisfied with the other answers because they talk about animal specific issues. 

I guess what comes to my mind: is there a bovine equivalent of depression? Could they by treated with psychoactive drugs? Without verbal communication, how would diagnosis look, etc. It seems naive to think that only the human mind would be afflicted with chemically related behavior imbalances. ",null,2,cdnfsh7,1rhi55,askscience,new,5
Gonad-Brained-Gimp,[Parrots' behaviors mirror human mental disorders](https://news.uns.purdue.edu/html4ever/2005/051221.Garner.parrots.html),null,0,cdne66k,1rhi55,askscience,new,1
null,null,null,1,cdng7fo,1rhi55,askscience,new,1
basketoflove,"We would be unable to diagnose an animal with most human mental illnesses because animals do not possess complex verbal repertoires (e.g., the diagnostic criteria for schizophrenia includes bizarre vocalizations and delusions, neither of which could be observed in a nonverbal organism).

Animals can, however, develop incorrect or exaggerated responses to external stimuli.  This misinterpretation could be considered a form of mental illness.

One hypothetical example of this is the development of an anxiety disorder:

A novel stimulus is introduced --&gt; Dog gets ""frightened"" by stimulus (sympathetic nervous system triggered) --&gt; Dog runs away from stimulus (negative reinforcement) + gets comforted by owner (positive reinforcement) --&gt; Dog is now more likely to get ""frightened"" under similar circumstances in the future

If this pattern repeats enough then the dog may learn to generalize their ""fear"" response to a number of different stimuli and environments.  If this generalization becomes broad enough then it could be considered an anxiety disorder.",null,0,cdnnsje,1rhi55,askscience,new,2
Platypuskeeper,"Cling film easily builds up static electricity, the mechanical handling of it causes some electrons to get separated from their atoms, and so there's a charged imbalance causing an electrical force as the negatively charged electrons try to get back to their atoms. Since the cling film is an insulator, they can't just flow through the material. The same static electricity is also responsible for the general 'clingy-ness' of cling film. You may have noticed that cling-film sticks better to insulators like glass and plastic than it does to metal, which is a conductor which allows the static electricity to discharge easily. 

Cling film is pure polyethylene (PE) plastic. It doesn't leave any residue (unless you leave the film itself) and PE itself is non-toxic. 

",null,20,cdndsjx,1rhpc1,askscience,new,75
Osymandius,"It used to be made of PVC with added plasticiser to improve the material qualities. There were fears that the plasticiser could be left on the food - these are complex organic molecules so there were fears that there could be negative health benefits.

Now we use polyethylene mainly. It sticks together by hydrophobic interaction and statics - because the chain is non polar, it repels our mostly aqueous food and sticks to itself. This is why you do get fat adherence to cling film but minimal aqueous adherence. ",null,9,cdndskt,1rhpc1,askscience,new,19
how_hard_could_it_be,"While I only posses very limited knowledge on the subject, I might be able to add something of value to the already great responses in this thread. 

I had the opportunity to work for Glad (makers of Cling-Wrap) and observed that in addition to the various plastics that are added in the extrusion process they add a substance known only to me as ""GMO"" a greasy sort of substance in order to make the film tacky.

I was told this substance was similar in composition to gelatin, but I am not sure how much the production staff knew about the ""GMO"" themselves. 


",null,4,cdnj378,1rhpc1,askscience,new,16
Br0wnch1ckenbrowncow,"Adhesives added to the LDPE cause the sticking, not static electricity. Even a quick look at the Wikipedia article for cling wrap supports this: http://en.m.wikipedia.org/wiki/Plastic_wrap.

The poor adhesion to metal is a result of the surface characteristics, not conductivity. The wrap does not stick as well to smooth, hard, surfaces like metal or ceramic.

Any residue left on food is negligible. Cling wrap is monitored by the FDA and the makers have to prove the components are non-toxic according to ISO 10993 (USP VI in the US) test standards, which includes cytotoxicity, biocompatibility, and extractable testing.",null,4,cdnord3,1rhpc1,askscience,new,15
Osymandius,"You can use immunohistochemistry/immunocytochemistry/flow cytometry like /u/baloo_the_bear says, but tau is present all the time, but you're looking for a specific pathological aggregation state of tau. It's not my specific area, but I believe that you're looking for the hyperphosphorylated tau state. You could ^32 P ATP to see if you can radiolabel your phosphoryl moieties on the protein, or see if you can find a phospho-tau specific antibody.

If you're satisfied with an ex cellular approach, and you can trigger tau polymerisation out of the cellular environment, then you can adsorb tau onto a silica membrane and use AFM to image the fibril formation over time. I've done this with amylin, and to my understanding tau and amyloidal aggregates are very similar.",null,0,cdndz0p,1rhpcf,askscience,new,6
baloo_the_bear,"You could try using a florescent antibody specific to tau protein, and then image. This will give you a good qualitative look at the levels of tau proteins but if you want a quantitative analysis you'll need to do some image processing (imageJ is pretty good for that). I'm not sure how you  would go about capturing the process of tau polymerization, but you could do a kinetic study to look at rates of formation.",null,0,cdndsqq,1rhpcf,askscience,new,4
spiceyone,"It really depends, every way of measuring has some level of artifact so you might want to use 2 or more. Radiolabeling normally has the least effect as osymandius points out. Immunochemistry depends on how good your antibodies are and they may effect the interaction, but this is the quickest and likely cheapest way to set up the experiment. You could also use GFP labeling. This would require making a construct and expressing it. It would take more work, and you would have to validate that this doesn't mess up the proteins of interest, but it would allow you to address the aggregation in much more natural contexts and due to the interaction of the chromophores via FRET/polarization you would likely be able to better quantify multimerization. ",null,0,cdnfwee,1rhpcf,askscience,new,2
ucstruct,"One idea would be to use an antibody selective for oligomerized Tau. Here is one [example](http://www.fasebj.org/content/26/5/1946), but I'd be willing to be there is a labelled commerically available one that you could get your hands on.  ",null,0,cdnrcyl,1rhpcf,askscience,new,1
ozzivcod,"There are universities who have reasearch groups on droplet dynamics, its still an indepent field in thermodynamics and important for jet engines, motors etc. They are detailed simulations on drop behaviour as you have mentioned it. Below is a link to University of Stuttgart in Germany who has a section for droplet dynamics. Surface tension has an impact on the drops via the weber number, im sure if you dig a bit further you can find some info on viscosity as well.

Im just here to tell you droplet dynamics is its own research field. So your interest is not too weird :) People dedicate their scientific life to these questions!

http://www.uni-stuttgart.de/itlr/forschung/tropfen/fs3d/index.php?lang=en&amp;open=t&amp;amp;lang=en

http://en.wikipedia.org/wiki/Weber_number",null,2,cdng2f1,1rhpdk,askscience,new,8
MartinHoltkamp,"I did a decent amount of research into this field, and the most useful piece of information I found was this article.

""Drop Impact Dynamics: Splashing, Spreading, Receding, Bouncing..."" (A.L. Yarin 2006) in the Annual Review of Fluid Mechanics, 38:159-92

This gives a nice overview of research into droplet dynamics. To answer one of your questions, droplet impact response is largely a function of the Weber Number as mentioned in another response. I would recommend reading pieces of this if you would like to know more.",null,0,cdni6se,1rhpdk,askscience,new,4
terminuspostquem,Droplet splash height studies are important for archaeology as a means of relative dating for structures vis a vis drop line patterns that appear in the soil. ,null,0,cdnv3wr,1rhpdk,askscience,new,3
animeturtles,"What you are probably thinking of is a kind of classic setup [like this](http://www.youtube.com/watch?v=QQ37RLXNAgc) with one or two rebound droplets. This setup with all the implied constraints (small droplet, same liquid in pool and droplet, most likely water, medium velocity) is complicated and chaotic enough, and even then it's hard to delimit the cases. A very low velocity droplet of water would rest on the surface and ""rebound"" without creating a real secondary droplet, like [this](http://www.youtube.com/watch?v=ynk4vJa-VaQ). A very high velocity droplet would cause outward splash like an impact crater that could go higher than what you call the secondary droplet (which might instead be a disorderly spray). Not to mention that even at medium velocity, there can be more than one secondary droplet.

Keeping this in mind, consider that your condition ""a droplet or an object"" does not place constraints on the objects, so it's even harder to make a useful statement. What about the shape of the objects for example? A brick will impact differently than a marble, and you could probably optimize the shape to increase splashback as well. 

In the picturebook case the viscosity of the liquid and the speed and size of the droplet ( = total kinetic energy transmitted) should be the decisive factors for the rebound behavior (see [Weber number](http://en.wikipedia.org/wiki/Weber_number)). Extremely high surface tension could inhibit droplet formation, but realistically its impact would be small outside of borderline cases I imagine.

If *any* object can be chosen, I doubt that you can find an optimum mass and velocity, and I would conjecture that, given an infinite pool, the rebound can grow more or less indefinitely with the size of the object. Bigger rocks make bigger splashes, after all (unless you're talking meteorites that will boil away your liquid, but you're getting more non-linear by the minute here, yo).",null,0,cdnf7n8,1rhpdk,askscience,new,2
elerner,Here's some [related work](http://arxiv.org/abs/1111.3630) on how the shape of the impact surface changes the geometry of the secondary droplets. The experiment involved capturing some [really beautiful video](http://www.youtube.com/watch?v=QaxX6nNTZeY) as well. ,null,0,cdniqdg,1rhpdk,askscience,new,2
PaintChem,"Oddly enough I just read this article the other day and work, personally, to invent superhydrophobic coatings.

http://www.bbc.co.uk/news/science-environment-25004942",null,0,cdnei6a,1rhpdk,askscience,new,1
strokeofbrucke,I found [this study](http://www.sciencedirect.com/science/article/pii/S0009250901001750). It's the closest thing I could find. Most studies seem to be on the recoil of a liquid drop off of a solid surface.,null,0,cdnem9w,1rhpdk,askscience,new,1
Oranges4Odin,This might be what you're seeking: http://meetings.aps.org/Meeting/DFD13/Event/202554,null,0,cdnf4a4,1rhpdk,askscience,new,1
The_model_un,"[This paper](http://www.annualreviews.org/doi/pdf/10.1146/annurev.fluid.38.050304.092144) seems a little like what you're looking for, though I admit I didn't read the whole thing to check.",null,0,cdnfdvq,1rhpdk,askscience,new,1
polyphonal,"[DROP IMPACT DYNAMICS: Splashing, Spreading, Receding, Bouncing…
in the 2006 Annual Review of Fluid Mechanics](http://www.annualreviews.org/doi/abs/10.1146/annurev.fluid.38.050304.092144) might be of interest.",null,0,cdnfqz4,1rhpdk,askscience,new,1
Obstinateobfuscator,"Thanks folks, now I'll do some reading. Sometimes it's just a matter of finding which thread to start pulling...
",null,0,cdnpc40,1rhpdk,askscience,new,1
The_Last_Raven,"There apparently is interest. For example, Pietravalle et al have an article entitled ""Modelling of rain splash trajectories and prediction of rain splash height"" (2001).

There's also been studies done on this by variation of the depth of the pools the drops were put into (ie. Harlow and Shannon, ""The Splash of a Liquid Drop"", 1967, Journal of Applied Physics). 

If you do a google scholar search for droplet splashes, you can find a number of articles up to even the current day that are interested in the modeling of droplet splashes. 

I don't know the area much and reading the papers to find answers to all your questions would be a bit much to ask, but it's definitely something that looks like it has been studied a bit. ",null,1,cdnfilz,1rhpdk,askscience,new,2
Platypuskeeper,"It's a quite tangible property, the [Stern-Gerlach](http://en.wikipedia.org/wiki/Stern%E2%80%93Gerlach_experiment) experiment was the first more or less direct observation of particle spin. 

Spin does not imply that the particle is spinning on its own axis, but the name isn't arbitrary - in many ways it _does_ work _as if_ the particle would be spinning on its own axis. It's an intrinsic form of angular momentum. The perhaps most significant or immediate effect is that electrons get a magnetic moment, as you would have classically with a rotating charge.

Spin doesn't actually have any special relationship to entanglement, all the measurable properties about particles can become entangled. Electron spin is just a good example, because it can only take two possible values.

Anyway, the real-world consequences of spin are inestimable, because nearly all matter would behave very very differently if electrons had zero spin and didn't need to obey the Pauli principle. The only chemical bond that would exist in its current form be the simplest molecule of all, H2. Spin and the Pauli principle 'forces' electrons to occupy higher-energy states than they would otherwise, and it's always the highest-energy (valence) electrons that are doing the chemical bonding. 

",null,9,cdndirh,1rhpj0,askscience,new,36
smartass6,"Proton spin is also the basis for NMR (MRI). The proton spins are aligned and anti aligned with the large static magnetic field in the axial direction of the scanner, then RF energy and gradient B fields are used to manipulate the spin directions. Using coils to measure the EM field produced in these processes and changes allows extraction of biological information. 

So yes, spin is very tangible, useful and by exploiting its properties leads to numerous real world applications. ",null,3,cdnfs3t,1rhpj0,askscience,new,14
Pilipili,"To complete what Platypuskeeper said. Magnetism arises from spin. An everyday life use is your computer memories, in which the 0s and 1s are stored in the orientation of tiny magnets, in other words in their spin orientation. If you are interested in this, look into ""Giant Magnetoresistance"". Another interesting kind of devices, that are not commercialized yet but in which there is a ton of research, is spintronics. Basically people are trying to build an analogy to electronics but with waves of spin, not by moving the electrons. 

Source : I'm doing a master's degree in optoelectronics and magnetic quantum devices. ",null,0,cdnf7mk,1rhpj0,askscience,new,11
could_do,"Spin is angular momentum which is intrinsic to a given field, not associated with some particular extra motion. It isn't really something spinning about an axis, but the name has stuck.

Via the spin-statistics theorem of relativistic quantum theory, spin is in fact associated with the distinction between bosons (which don't obey Pauli exclusion), and fermions (which do). This distinction has unimaginably significant consequences - for example, without Pauli exclusion, matter as we think of it could not exist.

Because it is a form of angular momentum, charged particles with non-zero spin give rise to magnetic effects. Ferromagnetism is a familiar example of such.

As an aside, I should say that you might want to reconsider your claim that you have a ""pretty fair grasp of most things [in advanced particle physics]."" If you aren't familiar with spin, then I *strongly* doubt you have the mathematical background to have even a beginner's grasp of quantum field theory, without which any particle physics knowledge is largely superficial and without foundation. I'm not saying this to try make you feel bad, I'm saying it because you seem to think that you might have more of an understanding than you do: Physics is a mathematically formulated subject, and cannot be accurately expressed without (in some cases fairly involved) mathematics. Any non-mathematical understanding of particle physics is fundamentally misleading (hell, even the very idea of a particle falls to pieces in quantum field theory). If you have even a bit of mathematical background (e.g. basic differential equations and linear algebra), there are a few great books I can recommend if you want to try to put together a more thorough understanding.",null,2,cdngi0l,1rhpj0,askscience,new,10
Rastafak,"There are two reasons why spin is important. First spin creates a magnetic moment. Magnetism in most materials is directly caused by electron's spin. There is also whole field which studies the effect of spin in microelectronic devices called spintronics. The other reason is Pauli exclusion principle. As others have stated, this is incredibly important for bonds for example. Solids and molecules would look very differently if electrons had 0 spin. 

I can tell you a bit more about spintronics because this is what I'm doing. Spintronics studies the interplay between electron's charge and spin. In other words we study electronics in which spin plays a role. You most likely actually own a spintronics device: the magnetic sensors in HDD's are based on spintronic effects called [Giant Magnetoresistance](http://en.wikipedia.org/wiki/Giant_magnetoresistance) or [Tunneling Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance). In these sensors, there are two magnetic layers, one of them has fixed direction of magnetic moments, while the other can rotate in external field. Due to spintronic effects, resistivity of this structure depends on the relative orientation of the two layers. If you put it in external magnetic field, the free layer will align with the field and you can then measure its orientation by passing current through the structure. 

You can also make a memory based on these effects, where 0 is represented by the case when the two layers are oriented in the same direction, while 1 is the case, when they have opposite directions. These memories are not very widespread but they are made commercially and there is a lot of development in that area. [Here](http://www.everspin.com/) is one company, which sells them. Apart from these applications, there is a lot of basic research going on in spintronics. It is a very active field and growing field, so there are likely going to be more applications in the future.",null,0,cdnhixq,1rhpj0,askscience,new,4
penisgoatee,"Spin is sort of a big deal.

If it weren't for spin, we wouldn't have hard drives. Electrons can have one of two spin states (spin up or spin down). The different states react to magnetic fields differently, this gives rise to [Tunnel Magnetoresistance](http://en.wikipedia.org/wiki/Tunnel_magnetoresistance), which is used in hard drives. So, yes, spin is quite tangible.

So what *is* spin? It's intrinsic angular momentum. Angular momentum depends on how fast you're spinning relative to an axis. For the spinning earth, the outer surface has angular momentum because it is spinning relative to the poles. For an electron, well, there are no poles. There's not really even a radius. And, yet, the electron still has the same kind of angular momentum as the spinning Earth. That's why we say it is intrinsic - it's just always there. 

Spin has the effect of making it seem like an electron is a little loop of current. The electron has a charge that is ""spinning"". Little loops of current make magnetic fields and interact with them. So spin is the origin of many magnetic phenomena, like permanent magnets and nuclear magnetic resonance (NMR). 

Why is there spin? Well, why is there charge? Why is there mass? As Feynman pointed out, ""Why?"" isn't always a productive question. We could go on a crackpot tangent about how the electron is a nebulous ball of energy which may or may not have some intrinsic rotation, but that's not experimentally verifiable or well accepted by the physics community at large. ",null,0,cdnhsfu,1rhpj0,askscience,new,3
PastryBlender,"As with most things in quantum mechanics, you can't really know exactly what spin is, nor imagine it in your head. There are ways to represent it in classical terms like the vector model ""http://hyperphysics.phy-astr.gsu.edu/hbase/quantum/vecmod.html"" however I myself don't really like this model and take spin to just exist as whatever numerical value it is in my head.

Spin is essentially the magnetic property of a particle (or collection of particles), it's made up of spin angular momentum component, and a magnetic moment component. Magnetic Moment = Gyromagnetic Ratio x Spin Angular Momentum. The spin angular momentum value is derived from quantum mechanics, and the Gyromagnetic ratio is specific for each particle/atom. As the name implies, the magnetic moment is a moment, and if you want help imagining it, think of it as the moment at which the particle/overall atom is actually spinning, and is usually the quantity used for calculations to do with how much things affect/change spin. The spin angular momentum component of this is limited quantum mechanically to a specific number of orientations, depending on the amount of nuclear particles in question. This means that the magnetic properties can have a quantised number of states for an atom, and this is proven in the Stern-Gerlach experiment. This experiment fired nuclei (of something spin 1/2 I think, so with 2 allowed spin orientations), through a magnetic field and onto a detector. Only two spots were detected, implying that the field only had nuclei of two sets of magnetic properties pass through it, with one spot above the altitude at which the nuclei were fired (horizontally) and one below (indicating a positive, and negative spin, both nuclei were displaced by the same amount but in different directions).
 If you put nuclei in a magnetic field and fire electromagnetic waves at them, their overall spin will change when certain frequencies are used. This is the basis of NMR chemistry and the frequency (Larmor frequency) that causes these transitions depends on the magnetic moment of the species in question, the Gyromagnetic ratio, and the strength if the applied magnetic field. Many complicated extra effects arise from doing NMR and it can be used to figure out the chemical structure of many chemicals, using quantitative methods, in both organic and inorganic chemistry. Even now the field is continuously being improved as the sample quantities required to carry out NMR are too high (because of sensitivity issues caused by radio waves used in NMR being of low energy; compared to waves used in other methods of spectroscopy), these low sample qualities mean that biologists studying cells always wine about not being able to NMR the little things they find etc. 

Sorry if its long I got a bit carried away haha",null,0,cdnibzj,1rhpj0,askscience,new,2
DearHormel,"This has always bugged me, and I've never gotten it straight, so let me hijack the thread a little.

There are TWO properties called 'spin'?

1.  Angular momentum
2.  The path of a charged particle curves in a magnetic field

Do I have that right?",null,1,cdnqyy8,1rhpj0,askscience,new,1
Halysites,"Two reasons:

1. When a star is going through [fusion](http://en.wikipedia.org/wiki/Star#Formation_and_evolution), it will combine hydrogen to form heavier and heavier atoms. The basic reaction series would be: 

* hydrogen + hydrogen = helium
* helium + helium = carbon
* carbon + carbon = iron
* there will be other combinations of fusing atoms which would result in the creation of neon, silicon, etc. This is just a very simple list (I'm just a simple geologist and chemistry is not my speciality).

Once a star has burned all it's fuel to generate iron, it will undergo a supernova (or some other process, depending on it's size). Since iron was the last atom to be produced during the fusion process the star will be very rich with iron. If it's goes through a supernova it ends up ejecting most of it's material into the space around it; during this process heavier elements may form as well. So the space around it becomes enriched in a variety of elements, especially iron. This material is what will be used to form planets.

2. Planetary material begins to accrete from the rich stuff spewed out by a dying star. It is very hot and so the material is molten. As the planetoids get larger and larger, gravity becomes a stronger force. Gravity, coupled with a molten states, means that heavier elements are pulled towards the core of the planetary body quickly. Iron, being very heavy, will sink towards the core. Other heavy (metallic) elements will also sink to the core.

Viola, you have an iron-rich core for rocky planets. This process doesn't exactly apply to gas giants like Jupiter (which would be relatively depleted in iron).

Most of this information is from geology textbooks and courses I took in my undergrad. Although I imagine most of the info could be looked up on the internet.",null,1,cdnh1c9,1rhqp5,askscience,new,3
NotAStructrlBiologst,"Water is hydrophilic, milk is a mostly water emulsion with some fats giving it some hydrophobic character. Without knowing every last compound in he mix which can vary, it would be speculation to say. Given that theres chocolate which has dairy fats milk would be more reasonable choice. 

You have a greater chance of powder clumping if you were to dump the powder on top of the liquid. If you were going to prepare it like a chemist who can't leave procedure in the lab, you would add the liquid to the solid. You would add a small amount of liquid and mix, just enough to make a slurry ( a loose paste consistancy ) then bring it up the desired liquid level. 

Dumping powder into the liquid or quickly adding liquid to the powder can cause clumping. The two different mediums flow differently and Van der Waals forces come into play. While there are some powders that you would swear look liquid when poured, most don't. Powder particles are still solid and exhibit more friction upon each other than a liquid. If the liquid is allowed to surround an amount of powder instead of solvating , the water will then be pushing on this clump of powder from all sides. It is still solvating, but only on the surface area of the clump.   ",null,1,cdng8za,1rhtuw,askscience,new,13
Jameslepable,"Only thing I can think would make a difference with the first question is that pouring the hot liquid on the chocolate mix would have a ""natural stir"" from the pouring of the liquid. Where as pouring the powder onto the liquid could result in the powder on top of the liquid and not going straight into the solution.

Dissolving Boric Acid does this if you pour the powder into the liquid.",null,0,cdnfv9y,1rhtuw,askscience,new,1
Trill-Nye,"What do you mean by ""the two angles?"" Electrons will be diffracted by a crystalline material at a number of angles, each corresponding to a certain crystallographic plane with a reflection allowed by the structure factor. So each diffracted beam is due to a different d-spacing. the different n values in the Bragg equation correspond to higher order reflections, but these can generally be ignored. Does this answer your question?",null,0,cdnfqvw,1rhu16,askscience,new,3
Mxlexrd,"In the solar system, all of the planets are on the same plane, but there are lots of smaller objects which have orbits which are at angles to the plane of the planets.

As for the galaxy, it is also roughly flat, and has a diameter about 100 times larger than it's thickness. Within the galaxy, the stars have planetary systems which are aligned randomly at all different angles to the plane of the galaxy.",null,233,cdnhkj4,1rhu7r,askscience,new,1075
santa167,"BA in Astrophysics here.  Your question involves how galaxies and star systems are formed and why they typically stay in the same plane.  Since it seems like no one has answered yet, I'll try and help you out.  To answer, I'm going to do a little background, first on galaxies, then on stars, and then I'll explain why there should not be as much matter above and below the plane of the Milky Way and our Solar System.  

You're correct in assuming that space is infinite, but from the sound of it, you are implicitly also assuming that it is isotropic on any level.  Essentially, the reason flat diagrams are bewildering is because you're thinking of space as completely evenly spread out with stars, planets, and other matter (like Hydrogen clouds and black holes and white dwarfs, etc.) roughly taking up the same spacial distance away from one another.  Space isn't like a 3D grid, however, especially on smaller scales.  

Astronomers recognize that on a [very, very, very large scale](http://upload.wikimedia.org/wikipedia/commons/b/b6/Earth's_Location_in_the_Universe_(JPEG).jpg), above the scale of the local superclusters of galaxies even, the isotropy of the universe can be assumed as true.  As you can see in the picture, this is not true on the scale of our Milky Way Galaxy.  Isotropy means that no matter where you look, everything appears similar and there's no distinguishing point of reference.  In the image, we can see that matter is pretty much equally spread out only on the observable universe level.

That being said, now we should consider how galaxies form.  There are four basic different structures to galaxies: spiral, elliptical, lenticular, and irregular.  These were proposed as a sort of ""evolution"" by Edwin Hubble and called the [Hubble Sequence](http://en.wikipedia.org/wiki/Hubble_sequence).  First, the Hubble Sequence doesn't take into account irregular galaxies, which formed (as you can assume from there name) in a very strange way, mostly in the beginning stages of the universe where matter interactions were really hectic.  

I'm going to put irregular galaxies aside because they aren't really what we're focusing on here, but there's not much more to say about them anyway.  What's left are spiral, elliptical, and lenticular galaxies.  They have different characteristics and form in different conditions.  Long story short, your question only involved star formation and spiral galaxies so I'm going to get into that specifically.  Spoiler: there is a more equal spacing of stars and matter in elliptical galaxies because they formed from galaxies merging together and are shaped, you guessed it, like an ellipse.

Finally!  Onto the good stuff.  Star formation and [spiral galaxies](http://en.wikipedia.org/wiki/Spiral_galaxy#Origin_of_the_spiral_structure)!  Our Milky Way and Solar System.  Both are surprisingly similar actually, so let's get down to it.  First off, spiral galaxies are classified by two things, whether they have a ""bar"" in the middle of them, or not.  This is shown in the Hubble sequence as the fork separating SBa from Sa.  As you can imagine, spiral galaxies are shaped in a spiral way with a group of stars in the middle surrounding the center.  Much like a sprinkler that is shooting water and spinning for a long time, the water or arms in this case appear to be curved due to the rotation of the center.  The spinning of the center is very important and will play a part in answering your question.

Star formation will actually explain both processes so I'm going to jump out of galaxies for a minute.  Imagine a cloud of Hydrogen and other dust just floating around in space.  If the conditions are right, maybe perhaps in the spiral arm of a galaxy where lots of new stars are formed, the cloud might be heated up and have the right pressure to start clumping Hydrogen molecules together.  Obviously, we know that the more mass something has, the more gravitational pull it has.  Even you and I have a slight gravitational pull.  The Hydrogen and other dust starts clumping together at a certain point as more and more matter is pulled toward it.  As more matter is pulled in, the center of the cloud where it's being pulled starts to rotate from being hit with particles.  Fast forward to lots of matter pulled in and gravity of the matter causing immense amounts of pressure down on itself, and you have a cloud with a [protostar](http://en.wikipedia.org/wiki/Protostar)!  

Fast forward some more.  More and more matter is being gravitationally pulled into the protostar and more matter on top means more pressure at the core from matter pushing down on it.  It also means more rotation done by the protostar.  In the cloud, matter starts to orbit around the protostar because it is too far from the protostar to be pulled in and the spinning of the protostar has caused the matter to achieve a tangential velocity creating an orbit.  Now, we're at the point of the cloud looking like a rough haze of particles around a really hot ball.  As the particles in the cloud orbit, they too clump together to form planets, asteroids, comets, meteoroids, etc.  Here's where we get to the crux of your question.  Why do the planets form on a similar ""plane"" of the star system?  The reason is actually because of the spinning protostar.  

The protostar's spin causes the particles of dust and Hydrogen in the cloud to orbit in a specific direction.  That's all well and good, so now everything is orbiting around in the same direction as the protostar is spinning.  Back to another analogy.  If you have a rubber ball and you decide you want to spin it while throwing it in the air straight up, what should happen?  If you spin it like a pizza, the rubber balls top and bottom actually sinks into the middle part because of the spinning acting upon the particles in the rest of the ball.  The top and bottom contract in to the middle plane of the ball where you spun it!  Same concept, but on a much larger scale.  Spin the protostar fast enough, and the particles in the upper and lower parts of the system (not on the same plane as the spin) want to sink down into the plane, forming a sort of CD-like shape with the protostar in the middle and everything else orbiting the same way.  

Eventually, [the star gets big enough, hot enough, and has enough pressure to start Hydrogen fusion in the core](http://en.wikipedia.org/wiki/Star_formation) when it explodes with energy and blows off a lot of the remaining dust and cloud in the system, leaving planets, comets, asteroids, and moons behind.  The planets are still orbiting the star in the same rotational way, also rotating themselves, and their moons as well.  The system looks like a CD and there is little matter above or below the CD plane because of the rotation of the star enacting a force to push and pull everything *into* the plane itself.  You can actually apply the same principal to the formation of a spiral galaxy, although the formation is a little different.  

I hope this answers your question.  Let me know if it doesn't and I'll try and clear it up a little better.  

**TL;DR:** The star/supermassive black hole in the center pushes and pulls matter as the system/spiral galaxy is forming into a disk.  It pulls the matter into the disk by spinning and applying a force into the plane that acts on the matter.  When the matter is in the disk, the rotation/force around the still spinning star/supermassive black hole doesn't allow it to leave.  That's why there's not as much stuff above and below the plane of the system/spiral galaxy.",null,34,cdnfpuh,1rhu7r,askscience,new,203
Hyperchema,"Also on a similar note to this, how did we come to orient ""north"" with being ""up?"" For instance, whenever we view a globe it's always oriented so that antarctica is on the bottom. Is there any scientific reasoning that lead to that orientation?",null,7,cdng9z2,1rhu7r,askscience,new,25
antpuncher,"The solar system sits inside this big bubble of low density gas called the [Local Bubble](http://en.wikipedia.org/wiki/Local_Bubble).  It's a few hundred light years across.

Just outside of that is a ring of clouds called the [Gould Belt](http://imgur.com/1qLC8C7)  In that picture, you can see the plane of the galaxy as the grey target.  The gould belt is about 20 degrees to that plane, and the solar system is about 60 degrees to that plane.  

Moving on out, we sit in the one of these fluffy arms in the galaxy.  [This image shows a reconstruction](http://imgur.com/SEvDs8w) of where we are in the galaxy (though it's sort of difficult to piece together, since we're inside of it.)

If you keep going out, the galaxy sits in a group of galaxies that are all buddies. This is called the [Local Group](http://en.wikipedia.org/wiki/Local_Group). These include Andromeda (M31) which you can see with a telescope, the Large and Small Magellanic clouds, also galaxies, that you can see if you're in the southern hemisphere.   There are a bunch of tiny little galaxies in the local group, as well.  In that map, you can sort out which way the galaxy points by thinking about what you can see from the northern hemisphere (Andromeda) and southern (the SMC and LMC).

If you keep going out, there are more galaxies, and more clusters of galaxies.  Lots and lots. ",null,7,cdnkfb7,1rhu7r,askscience,new,24
spaceman_spiffy,"I know I'm late to the party here but I HIGHLY recommend you download and play with [Space Engine](http://en.spaceengine.org/).  It lets you travel around the universe at super-luminal speeds and is one of the first things I've played with that gave me a sense of scope of it.

  
[From the youtubes.](http://www.youtube.com/watch?v=bqEnCkLPyDQ#t=203)
",null,3,cdng7o4,1rhu7r,askscience,new,15
Frari,"The theory why Planets in our solar system are all in the same plane is due to how they were formed from a [Protoplanetary disk](http://en.wikipedia.org/wiki/Protoplanetary_disk)

What is above and below?  well space and other stars (and galaxies) are?  How far above and below these extend is not really known for sure, but infinity or close to it, is assumed?
",null,2,cdnqm5k,1rhu7r,askscience,new,12
JJrodny,[Download](http://216.231.48.101/celestia/) and play with [Celestia](https://en.wikipedia.org/wiki/Celestia). You'll thank me later.,null,1,cdnni7u,1rhu7r,askscience,new,10
TraderMoes,"The reason the solar system and galaxies are depicted this way is because they largely are flat. All of the planets in our solar system are in the same plane, give or take a few degrees. Pluto isn't, it's orbit has a tilt of 20+ degrees (not sure of the exact figure off the top of my head), and that is one of the reasons it was demoted from being a planet to being merely a member of the Kuiper Belt, a ring of asteroids on the outskirts of the solar system. Even further than the Kuiper Belt is the Oort Cloud, and this is actually spherical and surrounds the entire solar system. 

The reason the main solar system is essentially horizontal though (by main I mean the planets and the sun), has to do with solar system formation. The solar system formed out of a cloud of gas that condensed and heated up. As it did so, due to conservation of angular momentum the gas started to spin faster, and as it spun and gas particles collided their orbits would change, and gradually align into roughly the same plane. That's why later when the sun and planets formed out of that gas, they all occupied the same plane, and all orbit and almost all rotate in the same direction. 

I'm not certain why galaxies are flat-ish as well, that's a good question. But to answer the rest of your question, the universe is actually not infinite, although for our purposes it may as well be since we can never reach or even see the edge. But yes, there are galaxies all around us, in every direction. The galaxies themselves are relatively ""flat,"" but they can be oriented in any direction and be in any direction from us. That is why we have photographs of some galaxies that look like we're looking at them from the top, while others we see only from the edge, and so forth. ",null,0,cdnggm8,1rhu7r,askscience,new,7
atomfullerene,"The local stars are scattered pretty randomly around us, with some above and some below the plane.  They are too far away to be seen in the diagrams of the solar system though.  

Here's a map of the area around the sun, and you can see how stars lie above and below the plane.

http://www.atlasoftheuniverse.com/20lys.html

It's basically the same deal with the galaxy as a whole.  The _galaxy_ lies mostly in a plain, but the things nearby are scattered above and below it

http://www.atlasoftheuniverse.com/localgr.html",null,2,cdnj69w,1rhu7r,askscience,new,7
HappyRectangle,"Most of the planets and asteroids have been spun into the same plane by the forces of gravity and angular momentum. But not entirely -- Mercury is off by about 7 degrees, and Pluto is out of alignment by 17. 

But the ""above"" and ""below"" areas aren't completely empty. [Scattered disc objects](http://en.wikipedia.org/wiki/Scattered_disc) are asteroids that take all kinds of orbits, are often found wildly outside of the plane, and can change their distance to the sun quite a bit as they orbit around it. 

The main problem with having such an off-kilter orbit is that sometimes, you'll come into close quarters with a large planet. While the chances of actually hitting the planet itself are very small (space is just so much bigger than the sizes of the planets), the gravitational pull of the planet will be enough to slightly alter your trajectory and put you into a different orbit. A kind of cosmic natural selection happens: if you can maintain your orbit for a billion years, that means you either have a nice, circular one, or you just happen to have a key position that never gets near a planet.

Pluto is an example of the latter. While Pluto's orbit crosses near Neptune's, it's aligned so that two Pluto orbits take exactly the same amount of time as three Nepture orbits. This ensures they will never get anywhere near each other by accident. (There are other planetoids that have this 2:3 resonance with Neptune too -- we call them *Plutinos*.)

By the way, if the dust cloud that made our solar system settled naturally, there would be much fewer scattered disc objects. The reason we have so many is because at some point a long, long, long time ago, the orbits of the outer planets [""abruptly"" shifted](http://en.wikipedia.org/wiki/Nice_model), and Neptune flew into an outer belt of asteroids, scattering them all over the place with its gravity (I put abruptly in quotes because it actually took millions of years).

If you want to get a hands-on view of what all this looks like now, I'd recommend checking out [Universe Sandbox](http://universesandbox.com/). It has 3d models of the entire solar system as well as models of the nearby stars and galaxies.",null,1,cdnfgcp,1rhu7r,askscience,new,6
SauceBau5,"I have never seen a representation of the relations of the planes of the solar system to the galaxy and our galaxy to other galaxies nearby. It would be an interesting image, even if it was roughly drawn with just lines showing relative angles. Another interesting image would relate our solar system to the planes of nearby solar systems with detected planets. 

Just sayin', if anyone wants to get on that...",null,1,cdnmdx1,1rhu7r,askscience,new,5
mantequillarse,"Also, the Oort cloud, a cloud of comets, debris, and other large chunks of ice, rock, and metal, surrounds the solar system in a sphere. The cloud is the source of a lot of the comets and other things that orbit through the solar system.",null,1,cdnpywc,1rhu7r,askscience,new,5
RantngServer,"http://www.lsw.uni-heidelberg.de/users/mcamenzi/Week_7.html

The dendritic structures in some of the pictures on this page are tendrils made of galaxy clusters clinging together as the universe expands. The author of the page describes the universe's overall appearance as ""sponge-like.""

EDIT: Banana for scale.",null,0,cdnsve8,1rhu7r,askscience,new,4
Thefailingengineer,"[Relevant](http://i.imgur.com/jxSUBYy.gif).  As I understand it, relatively speaking, if you assumed a point in space to be completely still (or not moving) in comparison to the sun, this is a pretty good visualization.  Authors like to put pictures in their science books of our solar system in a 2d plane because it's easier to conceptualize.",null,3,cdnfmxv,1rhu7r,askscience,new,7
herpnderp02,"I have a question similar to this. Let's say you're looking at a picture of the solar system, with the sun on the left, and Mercury, Venus, then Earth to the right. If you were to be looking at North and South America, from that point of view, which direction would you see the Earth's continents in? Would it be with the north on top and south america at the bottom, left to right, reversed, or which way would north and south america be facing?",null,1,cdnguqo,1rhu7r,askscience,new,4
rupert1920,"This is a frequently asked question, so you can check out [this thread](http://www.reddit.com/r/sciencefaqs/comments/fui70/why_do_all_the_planets_in_our_solar_system_rotate/).

You'll also find many other frequently asked questions in /r/sciencefaqs - there's plenty of good reading there. You can also check out the sidebar for other ways of finding answers, under ""Save time with repeat questions! Try..."".",null,3,cdnril0,1rhu7r,askscience,new,6
stickthatarrowupyour,"my smarts are far below par for this thread but i do often silently survey these topics as a great source of intellectual sustenance, but i just wanted to share this video: http://www.youtube.com/watch?v=kGH7zw_puaA for the equally capped. it shows an opinion of the layout from earth to the edge and back again. i would not presume this is accurate but its easy to grasp.",null,1,cdnv3de,1rhu7r,askscience,new,4
SlimeCunt,"There is a program for the phone that lets you see the everything around our planet by looking through the phone. If you point your phone downwards you see whats underneath us and so on. Very cool.


http://www.androidauthority.com/best-astronomy-stargazer-apps-97175/",null,0,cdnvbd6,1rhu7r,askscience,new,3
Nephilius,"Above and below is relative when you are speaking of things larger than our solar system.  There are galaxies all around ours, more or less, and the Sol system sits roughly at a ninety degree inclination in the Milky Way galaxy.  Think of it like a piece of paper sitting on your desk, that's our galaxy.  Now take a quarter and instead of laying flat on the paper, set it on it's edge and that about how our solar system is in our galaxy.  So other stars sit above and below us in our neighborhood, and beyond that sits so much more.

On a smaller scale, most of the planetary bodies sit on the solar plane, given that they all formed from the proto-planetary disk that surrounding the sun while it formed.  There are exceptions, Pluto and the other far-flung planetessimals (is that an accepted word yet?) sit on tilted planes, as well as the Kuiper Belt (where many of these planetessimals orbit and were probably formed.  I've seen models of the solar system (sans the Oort Cloud) that resemble a fuzzy donut of sorts with the Kuiper Belt, but otherwise, yes, the planets sit on pretty much the same ecliptic.",null,0,cdnvzg7,1rhu7r,askscience,new,3
SCM1992,"Think of the sun as a ball of dough at the beginning. As it spins it flattens out, right? The theory of angular momentum carries the remnants into a single plane. Impacts and captured bodies have slightly different planes/orbits than planets created from star leftovers.
Corrections welcome.",null,1,cdnwy1g,1rhu7r,askscience,new,4
dnqxote,"Interesting question.

If you look at the night sky from a place without much light pollution, you can clearly see the milky way forming a 'band' across the sky. If you observe the sky 'above' and 'below' this band - we still see stars.
That means that there are plenty of other galaxies and stars outside the plane of our galaxy.",null,2,cdnjcdd,1rhu7r,askscience,new,6
GhengopelALPHA,"Since other people are focusing on the question of how the solar system is in a plane, I want to answer your hidden question about the difference between space and objects.

You seem to be confusing the term *space* as including all objects in it; the planets in their plane, the galaxy, etc. It is true that the space is (probably) infinite, but the solar system, the Milky Way, etc, are things in space, and are not including everything that is in space. A diagram of the solar system only includes the planets (which orbit in a plane) because those are the larger objects in the space between the Sun and other stars. There are plenty of comets and Kuiper Belt objects that orbit above and below this plane, but they are tiny compared to the planets. Likewise, there are the Large and Small Magellanic clouds which orbit (I think?) the Milky Way on tilted orbits, and of course, there's the Andromeda Galaxy, but each is an entirely separate object from the Milky Way.

So, to answer your deeper question, yes there is as much stuff above and below us. But nearest to the solar system, that stuff is just small ice rocks, not planets. Further out, above and below the galaxy, there are roughly equal amounts of gas and stars, but there is much, **much** less of them near the ""poles"" of the Milky Way than in its plane. Out into intergalactic scales, the universe becomes roughly isotropic, meaning there is an equal amount of ""stuff"" (galaxies and everything in them) in any direction you choose to look in.",null,0,cdnlud9,1rhu7r,askscience,new,2
EvOllj,"solar systems form from clouds condensing. while gas condenses it transfers angular momentum from the inside to the outside where the center has no angular momentum left. angular momentum can not be destroyed, only transferred added and subtracted. but things spin around multiple axis until they cease to rotate around common axes after a collision resulting from rotating differently. The result of condensing gas clouds are a few rings of condensed matter on a plane where the total angular momentum along 2 axes more or less added up to 0, while the angular momentum around the 3rd axis keeps stuff rotating locally around nearly parallel axes. This state has the least collisions and the least ""rotational energy"".",null,0,cdnm1pl,1rhu7r,askscience,new,2
EvOllj,"""below and above"" are other solar systems that formed from other condensing/cooling/compressing gas clouds. The total rotation of the gas cloud determines the most common plane of the planets that form out of it. Gravity causes opposite local rotations to cancel each other out, as far as gravity reaches strongly enough while the gas cloud condenses. But the gas cloud as a whole has one strongest average/shared/total spin that will be visible as its solar systems plane.

Below and above are smaller clouds left over that are still way more spherical, because the gravity of the sun that formed in the center of the gas cloud is too weak on such a long distance to condense the far out gas along the same rotational axis.",null,1,cdnpfal,1rhu7r,askscience,new,3
null,null,null,0,cdns99f,1rhu7r,askscience,new,2
severoon,"I thought you might find this interesting -http://curious.astro.cornell.edu/question.php?number=205 - which basically explains why accretion discs are flat.

The basic idea is: if you release a bunch of particles of matter in empty space and they're all stationary relative to each other, they'll just fall directly toward the center of mass of the whole system and crunch into a sphere. But this never happens. Things are always moving around.

Now you can imagine that if everything is moving directly toward that center of mass of the whole system, they'll all just accelerate and crunch even harder. But once again, this never happens. Things in the universe that get caught up in a system never happen to be flying directly toward the center of mass of the system.

Ok, so they're coming in from all directions. If it's going fast enough, a particle won't get captured by that system, its path will bend, but it will ultimately fly on through. But if it's not going fast enough to escape and it gets trapped, then it will start a spiraling orbit toward the center of mass of the system.

Now we have a bunch of stuff randomly spiraling in toward the center of mass. This still isn't a disc though, so why do we only see discs? Shouldn't it be a big swirling spherical mass? Seems like it should...

But if you think a little more, and give this system a long, long time to settle down into a stable situation, you'll see that it isn't the case. This is because every system has a net angular momentum. In other words, from all these random things falling in, you can add up the linear momentum, and that will tell you how the system as a whole is flying through space (in a straight line). About that point, though, everything is also rotating, and that's the angular momentum.

Over time, all these different things will collide with each other and all the momentum that is moving perpendicular to the accretion disc plane will start to cancel. Furthermore, the gravitational effect of all that mass in that accretion disc plane tends to pull things into it. From there, this matter all starts to compress together into local chunks, and you get planets. You may get a bunch of matter that happens to not settle down before it gets close together and collapses into a local chunk, and you have Neptune (the planet in our system that doesn't fall in our accretion disc, or some theories say it formed and get ejected from some other place and got captured by our sun).

Along comes a meteor and nails a planet hard enough to spew a bit of its molten core into orbit around it, and you have the rings of Saturn.",null,1,cdnsbp7,1rhu7r,askscience,new,3
balkenbrij,i like [this](http://global.fncstatic.com/static/managed/img/Scitech/NASA%20Voyager%20edge%203.jpg) picture of voyager very much. It's taken at the very edge of our solar system and gives a real view of what you would see when you were there. I know it not really answers your question but it might help in visualising the vastness of space.,null,2,cdnt98m,1rhu7r,askscience,new,4
chilehead,"[This lecture](http://atropos.as.arizona.edu/aiz/teaching/nats102/mario/solar_system.html) provides a good example of why the solar system is in the shape of a disk (including a few movies), and it's not a huge stretch to expand that idea to galaxy formation - though that topic is just speculation at this point, since my education didn't extend into galaxy formation.",null,0,cdnof8t,1rhu7r,askscience,new,2
theskyhasbeenfalling,"This is a question that I have tried asking people in the past, and I am still not sure I have an answer, but I feel closer. It is still a bit confusing to me because of the way we are shown things in media, like sci representations of space travel being so planar.

Another part for me is that while thinking along these lines of ""above"" and ""below"", the way we are shown the orientation of the earth is wrong. I think North should actually be ""down"" and south, ""up"". The way the continents are when you look at a map this way, they seem more like the magnetic force is pulling them down like droplets of pitch. Not that gravity and magnetism are the same, it is just that it makes it more apparent that a force is pulling in my mind. I think part of this orientation of maps we are shown has to to do with the eurocentric empires of the past, and Europe needing to be considered top and center...

I don't know, but thanks you for posting the question. Hope I didn't make it worse with my own...",null,1,cdnt8dr,1rhu7r,askscience,new,2
zalo,"Cloth actually becomes more transparent when it gets wet, which is why it looks darker (because there is usually no light source on the other side of the cloth).

Next time you get a piece of cloth wet, hold it up to a light and you will see that more light is able to pass through.",null,63,cdng93y,1rhuln,askscience,new,330
rupert1920,"Check out [all these past threads](http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on) that come up with a simple search.

The short answer is that more light is transmitted into the material, so less light reflects back.",null,31,cdngmzp,1rhuln,askscience,new,95
chrisbaird,"To get to the core of your question, which no seems to have addressed yet:

Many materials (cloth, paper, cement) have a microscopic structure which provides multiple reflecting surfaces. For instance, a solid chunk of ice is mostly transparent, but snow is white. They are both made out of the same substance, but the microscopic structures in the snow flake and not in the ice provide multiple surfaces for light to reflect off of. Optical reflection takes place at the *interface* between one material and another material with different optical properties, such as at the surface separating air and ice. Creating a microscopic structure (scratching up a surface, weaving a fabric, injecting air bubbles) introduces more reflecting surfaces, so the incident light has a higher chance of getting reflected rather than transmitted. A solid, pure chunk of salt is transparent, put a pile of table salt granules is white because of all the reflecting surfaces.

Which brings us to your question. If we get rid of the microscopic structure, we can make white material clear again. Melt pure white sand down and let it harden as a solid piece of glass or quartz and it will be transparent. Melt snow flakes into a homogenous pot of water, and it becomes transparent again. Another way to optically get rid of microscopic structures is to add water. Water behaves optically similar to many materials, such as cloth, ice, glass, or snow. Pour water on a material with microscopic structures and the water will fill most of the cracks, scratches, pores, holes, and bubbles that used to be filled with air. Once this happens, the material now acts optically like a homogenous slab of material without microstucturing. The many reflecting surfaces go away and you are left with a mostly transparent material just by adding water. The index of refraction of water does not exactly match that of cloth (or paper, or cement, etc.), so the effect is not complete. The material only becomes more transparent upon getting wet but it not completely transparent.

As others have mentioned, if there is no light source behind the material, a material that has suddenly become more transparent will look darker.",null,7,cdnix8z,1rhuln,askscience,new,34
NotAStructrlBiologst,"Sight is light photons hitting something and reflected to your eye. White things reflect most of incident light, black things absorb most of the light. Other colored things absorb some of the wavelengths of white light and reflect the rest of the spectrum, this how you see colors. Wavelengths absorbed/reflected are a property of whatever the subject is, when it's wet you've changed the subject.

With the addition of water, you now have a second thing to absorb light. Especially with cloth, water permeates creating a system that allows light to penetrate further and reflect less. The less light reflected the darker it appears.",null,11,cdnfuey,1rhuln,askscience,new,17
aresman71,"[Here's a really good answer](http://www.askamathematician.com/2012/06/q-why-do-wet-stones-look-darker-more-colorful-and-polished/)

It describes the process in enough detail to avoid skipping anything important, but explains everything in a simple enough way that anyone can understand it.",null,0,cdnp4ot,1rhuln,askscience,new,3
ironny,http://www.reddit.com/r/askscience/search?q=darker+when+wet&amp;restrict_sr=on,null,0,cdnfydj,1rhuln,askscience,new,2
egalitaian,"The reason non transparent things become darker when they are wet is because the surface becomes smoother. Table tops, counters, most floors, and plenty of other things have no noticeable difference on their brightness when you get them wet but some things are obviously different. That's because their surfaces are rough compared to the things mentioned above.

The water acts as a layer that helps smooth out this surface and reduces the amount of diffuse reflection that is occuring. If you look at it from the correct angle it should become brighter because the reflection is more ""cohesive"". From other angles than this one it should like dimmer because the diffuse scattering you would see normally is no longer there.",null,0,cdnsr0i,1rhuln,askscience,new,1
null,null,null,32,cdnftrg,1rhuln,askscience,new,29
nomamsir,"The direction of the torque vector is only significant once an arbitrary convention (i.e. the right hand rule) has been chosen.  Really I think it make more sense to think of toques and angular momenta as defined by a plane plus a direction of circulation than it does a vector. However, there's a nice property in three dimensions that each plane has exactly one direction perpendicular to it, and we can define a direction/magnitude of circulation by specifying a given vector along the direction of that normal.

From this point of view the direction (in or out) is just a stand in for the direction of circulation of the plane. In some ways the plane picture is better, however most of the math you would have developed is better at using vectors and since this one to one correspondence between the two exists we can jump back and forth between the two.

that was a bit rushed by I hope its clear.

As for the second question radians are dimensionless so the units of meters/radian are the same as the units of meters.  Radians are the ratio between the arclength (distance around the circumference) and the radius. ratios of two things with the same units are dimensionless. ",null,0,cdngftu,1rhwdb,askscience,new,15
abowow,"the in or out direction comes from the right hand rule, which is where you put your right hand on ""r"" and then curl your fingers towards the direction of the force. so lets say that ""r"" is going to the right and the force is upwards, then the torque would be out of the page. so basically the in our out direction doesnt mean anything all by itself, you have to use the right hand rule to break it down.
i dont really have a good explanation for your second question, but i can say this. radians are kind of weird because they dont really have a unit (the calculation for a radian ends up with a length/length so the units cancel out). 1 radian is the angle that is made from an arc length of 1 radius. thats why there are 2pi radians in a circle, because the circumference of a circle is 2pi",null,1,cdnfssi,1rhwdb,askscience,new,3
jaxxil_,"If you understand the right-hand rule, I don't entirely know what 'significance' you don't understand. Outward pointing of the torque vector means the rotation is accelerated one way. Inward pointing means it is accelerated the other way. There's not much more to understand. Can you elaborate on what you feel you are missing? ",null,2,cdng4a7,1rhwdb,askscience,new,4
Geser,"For the planar problem you described, a disk in the plane of the page, the direction vector specifies the direction of the angular acceleration of that disk. Using your right hand's thumb to point in the direction of the torque vector, your fingers will curl in the direction of the angular acceleration. So for a vector out of the page the disk will accelerate counter-clockwise. 
Related answer: Since r is a distance it's units that of distance so meters. The units of angular acceleration are rad/time^2 , multiplying by a distance will give you (rad * distance)/time^2 . rad * distance is the arc length circumscribed by the radius, r, in angle rad. So the arc length that is circumscribed by the r in 360 deg (2 * pi) is 2 * pi * r which is the circumference of a circle of radius r. ",null,1,cdnfv97,1rhwdb,askscience,new,2
ColinDavies,"The information you need for torque is the plane in which it acts (or equivalently, the normal vector to that plane), and whether it is clockwise or counterclockwise.  But clockwise with respect to what reference?  Using the cross product builds in the point of reference automatically.  Instead of having to describe where you are standing and what you mean by ""clockwise"", that information is incorporated into the direction of the vector.  You just have to choose a convention to say whether clockwise corresponds to the normal or anti-normal direction, and apply that convention consistently all the time (hence the right hand rule).  There's nothing going into or out of the page; the direction is just a sort of translation of ""clockwise or not"" into ""positive or negative"" so you can use it in an equation.",null,0,cdngock,1rhwdb,askscience,new,1
Furrier,"You can spin stuff in a plane two ways. CCW or CW. The direction of the torque vector tells you which direction the angle is accelerating.

Regarding your related question. Radians is not a unit in the same way as mass is not a unit. Mass has in S.I the unit kg. Radians has the unit 1 (no unit). r will thus still be in meters.",null,0,cdnh1gj,1rhwdb,askscience,new,1
rat_poison,"Torque is a cross product, therefore it needs to be perpendicular to the plane of the vectors that produce it. But which way should a cross product point? It's point of origin is on the plane, but its end point can be on either side of the plane. 

We can choose a plane arbitrarily. but because we want our calculations to be consistent with the calculations of others, mathematicians have devised the ""right-hand"" rule, which is a common standard everyone can use in order to judge which way is ""up"" and which way is ""down""

The fact that torque in the clockwise direction points downward and torque in the counterclockwise direction points up is a mathematical convention. We could have been using the left hand rule if we liked. We would just have to be consistent with our choice.

We choose the right hand rule because it resembles a screw. by applying clockwise torque to the screw it goes down and again by applying CCW torque, it goes up. So we have defined torque's vector to point there just because it was an easy thing",null,0,cdnh6tt,1rhwdb,askscience,new,1
Manhigh,"The direction of torque is significant because in many instances you want to know what torque to apply to give an object a certain angular velocity or angular momentum.

Take a spacecraft, for instance.  It's tumbling (spinning) on a certain axis and you want to null out that spin rate.  To do so, you need to apply torque with thrusters or gyros in the appropriate direction.

The direction of the torque vector just tells you whether the applied torque is clockwise or counterclockwise in a given frame of reference.",null,0,cdnjdow,1rhwdb,askscience,new,1
etherteeth,"The significance of the direction of the torque vector is that it indicates the direction of the torque. By an alternate version of the right hand rule, if you point your thumb along the vector, your fingers curl in the direction or torque/rotation. That is, if the torque vector points out of the page, the torque is acting in the counterclockwise direction. Any further ""significance"" than this comes down to convention. 

As for your second question, radians are technically unitless, so multiplying angular acceleration by distance gives rad/sec^2 * m = (m*rad)/sec^2, which is dimensionally the same as just m/sec^2 . ",null,0,cdnlrii,1rhwdb,askscience,new,1
chcampb,"Torques are to force what rotation is to translation. Moment of inertia is to mass what rotation is to translation. They are analogous concepts.

So, what happens when we try to represent the addition of torques like we add forces? This is just vector math. So we need a way to represent torque as a vector. 

You have torque in one direction, which is represented by a vector in a perpendicular direction and whose sign represents the direction of the torque. It turns out that the direction is arbitrary, as long as the sign is consistent. ",null,0,cdnqaas,1rhwdb,askscience,new,1
drzowie,"Torque isn't actually a vector, it just looks like one.  As a cross product, it's an antisymmetric 2-tensor (a linear combination of two vectors), which in three dimensions just happens to have three components.

The direction of the torque vector is the direction of the axis around which the torque is being applied.  It's very important, for example, that when you step on the gas in your car the torque vector applied to the wheels goes off to the left of the car, and that the brakes apply torque that goes off to the right (unless you're reversing).  

The reason torque and rotational ""pseudovectors"" in general are confusing is that you have to combine them with a vector to *get* another vector.  For example, if ""ahead"" is +Z, and ""left"" is +X, then the displacement from the contact patch of the tire to the axle is in the +Y direction.  Since the tire spins around the +X axis, the motion is in the third direction (+Z).  The axis has the nice property that it's perpendicular to *both* of the important directions in the system.

Incidentally, in 2-D cross products are pseudoscalars, since there's only one way to rotate -- and in 4-D cross products have six components, so there's no clean way to represent them other than as the full antisymmetric 2-tensor.  (Just draw a 4x4 matrix and demand that it be antisymmetric.  There are 4 elements in the diagonal; they have to be 0.  That leaves 12 elements, but the symmetry relationship reduces them to 6 independent numbers).
",null,0,cdnypcj,1rhwdb,askscience,new,1
BlazeOrangeDeer,"Think of the direction as the direction of the axis of rotation. So if you're applying torque into the page, you're increasing the angular momentum around that direction (which means clockwise in the plane of the page). For example you can think of the total angular momentum of the Earth as a big arrow along its axis of rotation pointing north, so to slow it down you'd have to apply a torque pointing south (in other words, increase the spin around the south pole, same as decreasing the spin around the north pole).",null,0,cdo0wav,1rhwdb,askscience,new,1
Astrokiwi,"The space between the stars in a galaxy is filled with a very thin gas - the ""interstellar medium"". Even between galaxies there is the ""intergalactic medium"" too. This means stars and galaxies aren't completely isolated in space - there's gas everywhere. This means you'll have matter and antimatter annihilating each other in the ""border regions"" between antimatter and matter galaxies. This would produce a constant stream of ~~\~2~~~1 GeV gamma rays in these border regions, which we'd be able to detect with our gamma ray telescopes. However, we don't see this.",null,2,cdnj4gy,1ri2no,askscience,new,9
rocketgolfer,"Antimatter functionally behaves the same as normal matter, it's just that there's much, much less of it and it annihilates as soon as it contacts normal matter. The parts of antimatter that are ""opposite"" are opposite only by convention (e.g. it doesn't matter whether we treat the electron as being positively charged or negatively charged, but it does matter that the proton has the opposite charge).",null,1,cdnkvjd,1ri2no,askscience,new,2
WhoH8in,"Its completely aritrary. There is no objective way to identify ""up"". We choose the Earth's negative pole as north and assign that to the rest of the solar system and orient our images of other planets to that. If when cartographers started drawing maps they had placed the positive node on top then we would think of Antarctica as being Arctica. The only other way to get bearings in the solar system to orient yourself to the orbits of the planets. If you are looking toward the sun and the planets are going left to right you are oriented ""upward"" if they go right to left you are ""upside down"". But in reality none of this matters.",null,0,cdnj6g9,1ri48m,askscience,new,2
stuthulhu,"&gt;I'm visualizing the Solar System as planets orbiting the Sun in a flat disc. If we imagine that the disc is like a dinner plate, the standard view of Earth is that Antartica is orientated toward the bottom of the dinner plate. Is this actually correct?

Pick one. From a vantage point above the north pole of the Earth, the Earth would appear to revolve in a counterclockwise direction about the Sun. From a vantage point above the south pole, the Earth would appear to revolve in a clockwise direction. ",null,0,cdnjmdd,1ri48m,askscience,new,2
DangerOnion,"The plane itself is tilted pretty severely relative to the plane of the galaxy, making terms like ""up"" kind of meaningless in the first place.  But the simple answer is that you're right. The equators of most planets are roughly aligned with their orbital planes, and we invented astronomy so we get to decide which way is up :)  If we decide that Antarctica is ""down,"" then so is the hemisphere of Jupiter with the GRS in it.  Most pictures of the planets, despite being taken by satellites with no particular orientation, are rotated to be consistent with the way we visualize ""up"" in our solar system.",null,0,cdnv8pl,1ri48m,askscience,new,2
Baloroth,"Putting a fan behind the space heater will produce forced convection, which will cool down the heater and heat up the room as a whole. This is the reason central furnaces have fans in the first place: it spreads the heat around (same for AC, but the reverse principle: you heat up the condenser and cool down the air).

What the net effect over a long period of time will be (i.e. if the room will end up warmer with or without the fan) depends on many factors, but generally I would venture that the room will be warmer overall with the fan than without. But short term, in a cold room, the fan will certainly speed up the process.",null,2,cdnilrg,1ri6wl,askscience,new,13
OlejzMaku,"It depends on the definition of ""heat up the room"". Do you want evenly districubuted heat or warmer area around the heater? How warm do you want the room to be? How big is the room? How well isolated it is? What temperature is outside?

If the room will be too big and/or badly isolated and/or it is very cold outside and/or your desired temperature is too high the fan might be contraproductive.",null,1,cdni8d1,1ri6wl,askscience,new,7
garycarroll,"You are correct that the result will be at least as much heat energy into the room, and more air movement (to a point) will result in more even heat. This may be better, or not. If the room is not sealed for instance, the door is open to the rest of the building) more heat may escape than if you had a warm side of the room away from the door. And as OlejzMaku implied, if the room is too large or cold, the space heater may be unable to make the whole thing comfortable but could heat one corner. 
Also, note that moving air may feel cooler than still air of the same temperature. 
It sounds like you are trying to heat the whole room. If so, the whole room will heat more evenly with better circulation, and this means the guy sitting next to the heater will not get warm as quickly. If I had brought the heater, I might prefer no fan.",null,0,cdnjfua,1ri6wl,askscience,new,3
Richard_Fitzsnuggly,"More information is needed as well as the previous responses.  Is the room a defined sealed space?  If not you will be attempting to heat fresh air instead of re-heated air as it circulates within the space.  The friction of the blades on the air does not impact the heat.  The speed in which the drag coefficient of the air on the blades versus the cooling affect of the ambient air, would need to be astronomically fast.
",null,0,cdnj58v,1ri6wl,askscience,new,2
expertunderachiever,"Ironically it could make the space around the heater hotter than desired as you cycle cool air over the heater basically nullifying the duty cycle [instead of shutting off for a bit it'll always be on].

From experience it will make the room hotter though.  I've used this trick in my basement on really cold days where you just need it to warm up.",null,1,cdniy0a,1ri6wl,askscience,new,2
DangerOnion,"Assuming an enclosed space with no open windows or anything, you'd be right.  The fan is producing a negligible amount of heat energy through friction, but it certainly can't reduce the temperature of the room.  He may be conflating the use of fans with the reason we use fans in the summer, which is that 1) moving air makes our skin feel cooler through evaporative cooling, but doesn't actually reduce the air temperature or 2) by circulating fresh outside air into a stuffy building, which doesn't apply here.  The temperature in one place might rise slower, but it's not reducing the amount of heat added to the room.",null,0,cdnuz1w,1ri6wl,askscience,new,1
OrbitalPete,"Tall mountains are generally a product of continental collision. That occurs when subduction processes close an ocean and collide the continents that  formed its margins.

When that ocean closes lots of the upper sediments get scraped off. They obviously get caught up in the collision zone, and involved in the thrust faulting and deformation that builds the resulting mountain range. Carbonates such as limestone are commonly among these uppermost sediments. Hence the fact you tend to see a lot of limestones at the top of mountain ranges.",null,0,cdnq1t9,1ri7ga,askscience,new,6
DangerOnion,"I don't think they're overrepresented.  The Alps and Rockies are mostly granite, and the Andes are chiefly igneous rock.  Like OrbitalPete says, mountain ranges are usually formed by the collision of tectonic plates, and whatever rock happens to constitute those plates is what gets shoved upwards.",null,0,cdnv4et,1ri7ga,askscience,new,3
bohr_exciton,"Yes, but only if you ionize the gas, manage to extract (at least in part) charge carriers of one type (say electrons), and then manage to somehow isolate the system such that charge neutrality cannot be re-established. In that case there will be a net charge within the gas, and the resulting repulsion would act as an effective increase in the pressure, which like you said could alternatively lead to a larger equilibrium volume if the container is flexible. ",null,0,cdnjcd7,1ri7it,askscience,new,1
TangentialThreat,"The charge will prefer to collect on the outside of the balloon, but the balloon material will repel itself and that may have the desired effect.

Is it cheating if I heat the contents of the balloon to 10,000 K? The rubber will melt, but for a brief moment you will have significantly increased the pressure using ionization.",null,0,cdnkify,1ri7it,askscience,new,1
__Pers,"Yes, but not in the way you think. In plasmas, as in ideal gases, 

P = n k_B T

If you ionize a gas to make a plasma, the density of independent particles n comprises electron density and ion density and the sum is higher than the neutral particle density prior to ionization. Also, in making a plasma from a gas, you generally make the temperature higher. Both will tend to increase the pressure. ",null,0,cdnsyil,1ri7it,askscience,new,1
thetripp,"Your son is basically describing the theory known as ""Tired Light.""  The reason we don't think tired light is true is that we've never been able to come up with a mechanism that would cause energy loss in photons, yet still match our observed data.

For a tired light phenomenon to be true, it would have to:

1) Explain energy loss of photons over long distances, and match the observed redshift.

2) Not scatter photons so much as to induce blurring (since we don't observe significant blurring of distant objects).

3) Also explain the observed time dilation of distant events

4) Cause the same effect in every wavelength band, or in other words photons must ""tire"" in the same way, regardless of their frequency.

The wikipedia page on [Tired Light](http://en.wikipedia.org/wiki/Tired_light) has a nice list of some of the historical proposals related to this theory and why they don't match the observed evidence.",null,1,cdnl44c,1ri89x,askscience,new,13
stimulatedecho,"The distance related red shift is *evidence* of an expanding universe.  There happens to be a mountain of other evidence (search this sub for this question being asked previously to find specifics, mostly related to the cosmic microwave background, I believe), that suggests the same thing, i.e. expanding universe.  So, we don't really *know* that expansion causes the observed red shift, but it is certainly one valid explanation (as you already know), and it also explains other things we observe.  Additionally, and potentially more importantly, we have no experimental evidence to the contrary. 

That said, the requirement of ""dark energy"" energy seems to be a bit of a blemish...there is no doubt something in the recipe we have no understanding of.  I guess we'll find out exactly what as we go!",null,1,cdnlgh3,1ri89x,askscience,new,6
florinandrei,"&gt; ""but what if light just lost energy steadily as it went, wouldn't that look the same?""

No, it would not.

You are talking about an entire class of alternative explanations of redshift, grouped under the umbrella of the ""tired light hypothesis"". The have pretty much been debunked in bulk.

You cannot have light become ""tired"" by magic. There has to be some physical mechanism for photons to lose energy. If so, the energy loss will tweak the properties of those photons a little. As a result, a series of effects would become apparent:

- images from distant objects would become a little blurred, due to the scattering of the photons via energy-sapping interactions

- distant events would be observed to take place at the same rate of time; there would be no time dilation, like in the relativistic redshift models

Other effects would also become observable, depending on the particular ""flavor"" of tired-light theory, none of which have ever been observed.

Bottom line: the expansion of a relativistic universe is the only model that accounts for everything we observe out there.

http://en.wikipedia.org/wiki/Tired_light",null,1,cdnlxim,1ri89x,askscience,new,3
WhoH8in,"Well light does loose energy as it goes, in a sense anyway, every time it doubles its distance its energy is 1/4 what it was because intensity dissipates. This does not affect wavelength though. There are other phenomena that affect wavelength though, like movement. If somehting is moving toward us the light it emits doesn't seem to hit us any ""harder""(b/c light only goes c, no faster, no slower, ever) but that energy is accounted for, the light decreases its wavelength. If it is moving away then the wavelength increases which makes it appear redder.

Now when looking out into the stars hubble noticed that the further an object was the redder it appeared to be compared to what we know that objects [emission spectrum](http://en.wikipedia.org/wiki/Emission_spectrum) *should* look like. Now if the Universe were static then we would expect that, overall, half of all objects must be moving toward us and half be moving away and some tiny minority not moving relative to us at all. It is incredibly unlikely that, in a static universe, all objects would be moving away form us, we would ahve to be a truly uniqe body to observe that, literally the center of the universe. If the universe is expanding though it makes perfect sense because every object percieves every other object as moving away from it (ignoring of course nearby objects).

Use the expanding balloon analogy to understand it. If you have a barely inflated balloon and you draw three dots on it then start to blow it up those dots appear to be moving away form eachother but none fo them are actually moving, the space between them is expanding. This is why we think redshift is caused by expansion.",null,6,cdnjekl,1ri89x,askscience,new,2
s3c7i0n,"It does, were you to look at the sun directly in space, you'd be blinded. That's the point of those gold visors on space suits. The reason it doesn't appear to is that there's very little for the light to strike. Think of shining a flashlight into the air, it has effectively no visible effect. Now if you shine it at a tree, it gets nice and bright. It's the same amount of light, but it doesn't appear so because we can't see it travel. Now if you mean why isn't space blue, like the sky, that's because when sunlight filters through the atmosphere, it's interaction with the various gasses scatter the blue light most, giving us that nice hue. Space, obviously for the most part lacking an atmosphere, doesn't scatter the light, hence the black. ",null,0,cdnj4u9,1riapc,askscience,new,28
alltat,"It *does* make space bright. The only reason space looks black is because it's empty: it's not black because it's dark, but because there's nothing there. If you look at pictures of spaceships and satellites in space, you'll notice that they're all brightly lit with strong shadows. That's because space is bright, as long as you're close to a star.",null,0,cdnjy5p,1riapc,askscience,new,7
VA_guy,"There are two parts.  First, the sun's brightness decreases with the square of the distance.  Meaning when you're twice as far, it is 1/4 as bright.  Four times as far?  4*4=16, 1/16 as bright.  That's because there is a finite amount of light being cast over an ever increasing spherical area.  So the sun would look quite a bit less bright from Mars or Saturn than it would here simply because we're closer.
  
But if you're asking why space isn't glowing, you need to think about what would cause that to occur.  If you have a spotlight on a clear night, it will illuminate a path in front of it but it won't make the entire surrounding area bright, right?  But if you were to shine that same spotlight into a white room, it would do a much better job of making the whole area look bright.  That would be because there are reflections in the second case which case the light to come at you from all angles, appearing to illuminate you from everywhere.  
  
So in space it would be similar to that spotlight.  If you look directly into the sun, it would be very bright (depending on your distance).  But otherwise, there is nothing else out there for the light to reflect off of, so it won't be as if the entire area is glowing or light is coming from you at all directions.
  
Hope that helps.",null,0,cdnj62j,1riapc,askscience,new,6
stuthulhu,"Things are bright because light bounces off those things, and strikes your retina. There's relatively little in space for light to bounce off of, and get redirected towards your retina instead of traveling away. ",null,0,cdnjqys,1riapc,askscience,new,4
Gitsumkikin,"Oh it does! Only specialized cameras can turn towards the sun... If a regular camera were to be facing the sun all you would see is white. Same thing if you were facin it...Kiss yer eye sight goodbye! I think its somethin like 350 degrees if you are in direct sunlight in space, -350 out of. Space is so unimaginably huge that if your back was to the sun,(it better be!) the light probably wouldn't be noticeable at all...nothing for it to reflect off...as I said,mostly educated guesses here. Aside from the temp and the specialized cameras, those are facts, temp may be off one way or another, but, not by much.",null,0,cdnj45h,1riapc,askscience,new,3
Freeoath,"The eraser works in a way that when you rub it, it removes the graphite from the papers surface. The rubber is more ""sticky"" then the paper and thus the graphite preferes the eraser over the paper. Another way some erasers works is the eraser damges the top  layer of the paper effectively removing the graphite that way. 
You can't use an eraser on ink (what a pen leaves behind) because the paper more or less absorbs the ink deeper making the erasers funcion useless. For these you can use ink remover that either changes the chemical compound of the ink removing it from the paper, or dying it white

",null,1,cdnk132,1ricv9,askscience,new,11
stimulatedecho,"Hail forms in the presence of a strong updraft.  In this case, water precipitates, freezes and falls, but is blown back (continuously).  During this cycle more layers of ice form on the previously precipitated particles, until they get too heavy and eventually fall to the ground

If you get the chance, break open a hail piece (bigger the better) and you will find it is layered akin to an everlasting gobstopper.  ",null,0,cdnm2ax,1rid0v,askscience,new,2
ipostjesus,"basically, snow = ice from the beginning to the end of the process of water particles accumulating into larger structures. It doesnt always form hexagonal structures, there are many shapes of snow.

hail = a liquid or partially melted phase in the process, most likely involving a re-freezing event prior to reaching the ground. 

Ive never learnt much about snow formation, but i can tell you about hail. 
In a cumulonimbus cloud, water is cooled below freezing point but it hasnt frozen, called 'supercooled' water. supercooled water will freeze when it comes into contact with something that can start the crystal growth, such as a dust particle or some frozen water. So supercooled water is blowing around in the cloud, being pushed up by updraughts and falling back down when the updraughts cant hold its weight any more. It will cycle around the cloud falling and then riding updraughts back up again, all the while it will be accumulating water (some of it supercooled liquid water, some of it water vapour) until it doesnt find an updraught strong enough to hold it in the cloud and it falls out. Because the supercooled water is liquid and doesnt necessarily freeze instantly, the stone will be wet on the outside, which means stones will stick to each other by touching and then freezing. the sticking together of stones into larger stones makes the irregular surface of the ""random chunks"".
The larger a hail stone, the longer it spent accumulating while blowing around in the cloud. Which generally means larger clouds with stronger updraughts capable of suspending larger particles. ",null,0,cdnmhop,1rid0v,askscience,new,1
Jeffy_Weffy,"Slow chemical reactions are happening. At one end, the reaction wants to take in electrons. On the other end, the reaction wants to give up electrons. The only way for the electrons to flow to complete these reactions is to go through the device you're powering.",null,0,cdnkh2z,1rifop,askscience,new,11
chillichill,"Lithium-ion batteries are common so I'll use those as an example. A battery has 4 major components, a cathode, anode, electrolyte and wires to connect the electrodes and complete the circuit (plus all the other bits that hold them together). This circuit is attached to a power supply during charge (to provide electrons) and a device when discharging (to be powered by movement of electrons).  The cathode and the anode are materials which can hold Lithium, while also allowing it to leave the structure reversibly. The electrolyte is a material which allows lithium to pass through, but not electrons.
When a lithium battery is being charged lithium ions (Li+) move from the cathode to the anode, through the electrolyte. When the Li+ reaches the anode it takes an electron from the power supply to form a (relatively) stable state. The Li is stored in the anode until a device that requires power is connected (discharging the battery). 
When a device that needs power is attached, the Li in the anode releases an electron to form Li+ which moves to the cathode. The released electrons cannot travel through the electrolyte therefore travel around the circuit, powering the device. At the cathode the Li+ recombines with an electron from the circuit. Once all the Li has moved from the anode to the cathode, the battery is completely discharged. ",null,0,cdnoov8,1rifop,askscience,new,1
LuklearFusion,"It really depends on the physical implementation of the qubits, as there are many kinds. An incomplete list of things that people use as qubits are:

1. The electronic structure of ions or atoms, which will be confined to some region of space by some sort of electromagnetic ""trap"".

2. A single electron's spin, where the electron has been trapped in a solid state system; a so called ""quantum dot"".

3. Superconducting circuits which have Josephson junctions can also be used as qubits.

4. Optical qubits use polarization or optical modes in light as qubits.

Each kind of qubit is stored (I'm assuming by stored you mean kept free from noise) and manipulated differently. Is there a particular kind you're interested in?",null,0,cdnnt7i,1rifph,askscience,new,6
DanielSank,"Whenever people ask about this I recommend reading [this post](http://www.reddit.com/r/science/comments/1eg66q/a_15m_computer_that_uses_quantum_physics_effects/c9zzgfp). It refers only to superconducting circuit qubits, but it's definitely worth a read.",null,0,cdnvaq9,1rifph,askscience,new,1
FlavaFlavivirus,"Yes! I work with Alphaviruses; these particles contain a fusion peptide which allows the contents of the capsid to enter the cytoplasm of the cell, by fusing the two membranes together and inducing a conformational change in the structural proteins. 
",null,0,cdnpmde,1rigsa,askscience,new,2
captsuprawesome,"I would not characterize it as ""catalyzing their own import"" but many proteins are capable of penetrating the cell membrane.  HIV-1 Tat is a well studied protein that can do such a thing.  You may be interested in this summary of [cell-penetrating peptides](http://en.wikipedia.org/wiki/Cell-penetrating_peptide).",null,0,cdnsl21,1rigsa,askscience,new,1
bearsnchairs,[Transferrin](http://en.wikipedia.org/wiki/Transferrin) is a neat protein for that. It carries iron into cells. When transferrin binds to its acceptor on the surface of cells it initiates endocytosis and is taken up by the cell. You can attach transferrin to a particle or protein of interest to incorporate it into cells. ,null,0,cdnvctl,1rigsa,askscience,new,1
skleats,"Check out the dog genome sequence - there's lots of great examples of the role of repeated sequences in the selection history of various breeds. For some good sources:

[Here's](http://genomebiology.com/2011/12/2/216) an overview of the dog genome, with some info about repeated sequences.

[Here's](http://www.pnas.org/content/107/3/1160.full) a good rundown of the genomic variety between breeds.

And, for all the money, [here's](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC1356118/) a study showing that the diversity is linked to SINEs.

Evolution gets driven by selection on random variation, but repetitive sequences drive random variation.",null,0,cdnwr9q,1rih74,askscience,new,2
astazangasta,"Most of the genomes that are full of crap like LINEs and retrotransposable elements are higher eukaryotes. Prokaryotic genomes are usually much tighter and filter these sorts of things out. That is probably because higher eukaryotes can tolerate the addition of some extra sequence in the genome - over the course of a large multicellular organism's lifetime, replicating a few extra base-pairs worth of sequence every time a cell divides is not that big a deal. A prokaryote, on the other hand, can optimize over each division, since each replication cycle produces a new generation.

This probably means that repetitive elements in the (e.g.) human genome are mostly harmless crap - they are annoying, but not that important. But if we could get rid of them (like prokaryotes), we probably would.",null,3,cdnuuqn,1rih74,askscience,new,1
Astrokiwi,It's most likely flat. [This post in the FAQ](http://www.reddit.com/r/sciencefaqs/comments/v97po/is_the_universe_infinite/) should be useful.,null,0,cdnkzfg,1riij0,askscience,new,3
Arrogus,"Loud noises and bright lights are damaging because of the energy they carry; scents, on the other hand, are merely particles suspended in the air. Sure, many chemicals could do serious damage to your olfactory receptors if you inhaled them through your nose, but it would be because of their reactivity, not their pungency. In such a scenario, damage to your lungs would probably be your primary concern.",null,38,cdnmjtk,1rim55,askscience,new,199
Zukuto,"/u/Arrogus has it. it isnt that they are too *smelly* that breaks your nose, but that they are comprised of *toxic* fumes; sometimes they are smelly and other times not.

one time i had to clean out a Hair salon next to a business i was working for in a strip mall; i was the only one who posessed a mask that also had eye protection. the salon owner had tried to get a stain off the floor using Bleach and Ammonia. i let all her hoses run onto the floor and pushed the watery mustard gas out of the building before calling the Fire Dept and a HAZMAT team. 

i got a free haircut for my trouble. the salon owner got the shock of her life.",null,0,cdnymdg,1rim55,askscience,new,8
ubcokanagan,"No, you perceive smells when aromatic compounds bind neurons in your nasal passages.  The binding causes these neurons to fire which send a signal to your brain letting you know you just stepped in dog crap.  A very strong smell will innervate many neurons but it wont damage them.

If the odour is present for a long period of time, desensitization of the neurons will occur, and they will be less likely to fire in the absence of an increase in concentration of the aromatic molecules.  This is why smokers don't realize that they smell terrible all the time (well that and any damage caused by the smoke).

If the strong smell is caused by something toxic then yes it can cause damage, but I believe this would be caused by a property of the offending chemical, as opposed to it overstimulating a nerve.",null,0,cdo0u8c,1rim55,askscience,new,3
null,null,null,0,cdo28pj,1rim55,askscience,new,2
Philosophisation,"As with any sensory input it can be damaged via chemicals. Your chemoreceptors in your sinus will not however get destroyed by excessive use. Imagine a bathtub with the plug pulled. This would be ~chemoreceptor being overloaded with scent molecules or similarly shaped molecules at least. Anything that fits through goes through and is registered. But add oil and hey? It doesn't go through for a while. This is one reason for desensitized smell. Another is that the sensory nerve endings present to receive the signal from the sensory organelle(dendrites) fires so often that the brain starts filtering it out as useless signal, same as white noise. So no excessive safe smell will desensitize but only harmful molecules may ruin smell receptors.",null,0,cdo2jpc,1rim55,askscience,new,1
freeze4111,"All odors, indeed anything that gets in your nose, damages it to a very small extent. The strength of the odor isn't necessarily the measure for its destructive capability however; mostly it is corrosive acids or things like smoke which do the real damage: look up tobacco smoking and its damage on the sense of smell to get an idea: http://www.ncbi.nlm.nih.gov/pubmed/22776624. 

Heavy odors, like blue cheese or something like that, may briefly block a number of receptors making your sense of smell not as good as what it could be, but this is very temporary (no more than a few seconds). In addition, your brain can adapt or habituate to odors, making them less noticeable (this is harder to do as the odor gets stronger). You'll probably notice this with your own perfume/deodorant throughout the day. 

Any damage done to your sense of smell is much, much easier to recover from compared to other senses because the neurons involved can turnover and regenerate (these are the only neurons that can). Some environmental experiences can make the turnover slower (as can age) but overall your sense of smell will recover from anything you throw at it. 

Something I find interesting- the neurons that take information from your sense of smell transmit that information to the brain through neurons passing a structure called the cribiform plate via little holes. If these neurons are severed, they can regenerate, but usually can't find their way through the holes again; a case of this unique quality being utterly useless! 

",null,0,cdo3j4q,1rim55,askscience,new,1
paulHarkonen,"I work in the natural gas industry and thus work with odorant (that rotten eggs smell in gas).  Odorant is one of the most powerful smells around but all the health concerns surrounding it involve how your body reacts to strong odors.  Very strong negative smells can pose a nausea risk, along with some breathing concerns because your body expects strong bad smells to also be toxic.  None of the Msds information covers permanent damage to your sense of smell.  (Although there is a short term effect as your nose becomes overwhelmed by the one strong odor and stops caring about other weaker odors).

Its not a super scientific source, but there has been a fair bit of testing to create the MSDS information.",null,0,cdo8mi4,1rim55,askscience,new,2
Dr_JA,"The most important reason strong odors do not damage your sense of smell is because your odorant receptors are replaced regularly - all receptors are replaced every 2 weeks. Thus, even if there were chemicals that would covalently bind to your receptors and render them useless, these receptors will eventually be recycled and you're good to go. 

It is a myth that you destroy your sense of smell by working e.g. in an organic chemistry lab - it is just your brain getting used to it. If you took an extended break and came back, you would smell it like everyone else.

As you get older, your sense of smell diminishes because the bit in the skull where the nerves go through gets clogged, and the nerves can't transmit the signals properly anymore.",null,0,cdoz0ev,1rim55,askscience,new,1
gredders,"**What makes a neutron stick to a proton?**

Neutrons and protons (collectively known as nucleons when discussing their role in nuclear physics) are bound in the nucleus by the strong nuclear force. This interaction is a powerful attractive force which acts over a range of only a few femtometres (which is roughly the diameter of a nucleon). Beyond that the force quickly drops to zero. 

**What determines the number of neutrons and protons?**

As you increase the number of protons in a nucleus you increase the repulsive electrostatic (Coulomb) force due to their positive charge. Now this Coulomb force is weaker than the nuclear force, but it also acts over a long range. That means that as the size of the nucleus increases the strong force 'saturates' while the coulomb repulsion just keeps getting bigger, and causes the nucleus to become unstable.

Adding neutrons, however, contributes to the strong force, but does not increase the coulomb repulsion (since the have neutral charge), therefore heavier nuclei typically need more neutrons than protons for it to remain stable. 

[See on this plot of proton number vs neutron number how the line of stable nuclei tends to bend round to the neutron-rich side of the chart](http://nuclearpowertraining.tpub.com/h1019v1/img/h1019v1_38_4.jpg)


However, it is obvious that this isn't the whole story, since too many neutrons makes a nucleus unstable, too. This is due to the Pauli exclusion principle, which forbids two particles from occupying the same quantum state. Protons and neutrons differ by their isospin quantum number, and can therefore share the same energy state without violating the exclusion principle. Two neutrons (or two protons) are indistinguishable from each other and cannot share the same energy state. Therefore when you add only one kind of nucleon to a nucleus they are forced to 'stack up' more quickly than adding both neutrons and protons, which is to say they have to occupy increasingly higher energy states. In this case, it isn't long until one of the nucleons is forced to occupy a state of higher energy than the binding energy of the nucleus, and it cannot form a bound system. 

In reality, the situation is more complex than this and is very difficult to model. The best so far is the [nuclear shell model](http://hyperphysics.phy-astr.gsu.edu/hbase/nuclear/shell.html). Nuclear structure is an active area of research (and one in which I am doing a PhD), and we do not have a complete picture of what goes on inside the nucleus at this point. ",null,3,cdnrwhp,1rivmw,askscience,new,8
iorgfeflkd,"1. A residual strong nuclear force.

2. Protons repel each other electrically, so as you add more protons, more neutrons are required to keep them apart (that's a simplistic explanation). Nuclei with too few neutrons break apart.",null,1,cdnpwlj,1rivmw,askscience,new,4
Ejb90,"
&gt;- what makes a neutron stick to a proton.

There are four fundamental forces in the universe. One of the more familiar ones is the electromagnetic force. This has two ""possibilities"" - positively charged or negatively charged.Another one, the one we're interested in here, is the ""Strong nuclear force"", which has three ""possibilities"" which we call colour charge (though that's just a name).
Protons and neutrons are a type of particle called hadrons, which means they're made up of three fundamental particles called quarks. These quarks are all oppositely colour charged so they ""stick"" together (analogous to how oppositely charged particles like protons and electrons attract). This force is very strong (hence the name), but acts over a short distance. Inside the nucleus there is some residue from each of the forces inside the protons and neutrons, which leaks out and also causes the protons and neutrons stick together. This is called the residual strong nuclear force.
This is simplified and ignoring some important facts but it's the nuts and bolts of it

For the number of neutrons in different isotopes of elements, this relies on the ""Nuclear Shell Model"" which is analogous to the electron shell model you're familiar with.
This revolves around the fundamental idea of the Fermi exclusion principle - that particles with certain identical properties can't be in the same state.
In the nucleus there are ""shells"" in which the nucleons sit. Sometimes however the exclusion principle stops the nucleons being in the lowest energy state, ah
Md hence that nucleon is unstable, and so those isotopes will get rid of it to become more stable - they will decay. Some however are stable in energy, so won't kick nucleons out.
Hope this helps!",null,1,cdnqo6w,1rivmw,askscience,new,5
paperanch0r,"You will experience half of the pain relief for the entire duration. This is an effect referred to as ""dose/response curve"", which is basically a comparison of the amount of substance to the strength of its effect. The half-life(time it takes a substance to break down in the body) of a substance remains the same regardless of dose, but its effects will change with dose.
 
Edit: grammar",null,5,cdnqx4y,1riwul,askscience,new,6
Dannei,"I'm almost certain 65mph would be more efficient for any car. Generally, it seems cars are most fuel efficient around 40-50mph, although there's a lot of conflicting results out there...

As you go at higher speeds, you have to start fighting air resistance more, which reduces your fuel economy for obvious reasons. As this increases with the square of speed, it's a much more important factor at high speed than low - 85mph has twice the air resistance of 60mph.

At the lower end, I believe you're less fuel efficient due to not being in top gear (I'd appreciate it if someone could find a decent source on that!). Once you're in that, your engine is at the minimum RPMs possible for your speed, meaning that you reduce losses within the engine itself - the faster the engine is turning, the more energy is lost to friction.

Therefore, you want to aim for a relatively low speed whilst in top gear, which is the minimum of the combination of mechanical losses and air resistance.",null,1,cdnrpcv,1rixux,askscience,new,11
wmeredith,"There's not really enough information to answer this. Energy efficiency is going to depend on how the speed was achieved and then sustained much more so than what the actual velocity is. This would be a combination of mechanical factors of gearing, engine type, thrust delivery, etc...",null,0,cdnptnn,1rixux,askscience,new,2
baldeagleNL,"I always learned that the most efficient way to drive is in the highest gear with the least amount of rotations per minute. In that way, the least energy is spilled on useless movement. However, the actual optimal speed is probably a bit more complicated. You'll have to find out at what RPM your engine has the best (*is there even an English word for* **rendement**).",null,1,cdnpw0p,1rixux,askscience,new,3
sweaterhead,"Assuming that the lower speed has more mpg's, and you are traveling the same distance, driving at the lower speed will be more efficient, because it's miles per gallon that defines fuel efficiency specifically, not miles per hour. ",null,1,cdnqi8y,1rixux,askscience,new,3
xavier_505,"If you are in any modern passenger vehicle, 65mph will be  more efficient than 80. The engine will be running for longer however the energy required to overcome drag increases with the cube of velocity, and consumer vehicles are not designed to have their peak efficiency at 80 mph.",null,0,cdns0vb,1rixux,askscience,new,1
DanielSank,"This is one of my favorite questions.

&gt; Why do mirrors flip along a vertical axis, while still preserving up/down orientation?

They don't! This is a result of your brain doing extra work and fooling itself. Think about this: when you look at another person face to face, you are used to seeing their left side on the right side of your field of vision. Of course this is true when you look at anything that has a ""front."" You always see what would be the left side in the object's own frame of reference on your visual field's right side. Because of this your brain is accustomed to mentally register the switch.

The vertical direction isn't flipped because normally you and the person (or object) you're looking at are both oriented upward from the ground (I guess you could say this is because we're all subject to the ground being on the same side).

When you look in a mirror, *nothing is flipped*. It's a plain old mirror image. It *looks* flipped because you're *not* seeing the side reversal that you're used to seeing when you face another person.

A good way to visualize this is as follows. Imagine standing in a room facing in one direction. Now imagine you make a copy of yourself right where you're standing. Now you take a step forward and to the side, and then turn around and face your copy. If you visualize this in action you'll see that your right eye is directly opposite your copy's left eye. However, your feet and your copy's feet are both on the ground. This is the reversal that you're used to seeing when you face other people and objects, and this reversal does *not* happen when you look in a mirror.

EDIT: grammar",null,7,cdnqa8v,1rixyo,askscience,new,29
IX-103,"The mirror is not flipping along the vertical axis, but instead in the front/back axis. You can see this by noting that if you point your hand out toward the mirror the reflection of your hand appears to be getting closer (movement away is changed to movement toward). You don't usually see people flipped front/back, so you perceive the person of your reflection as having their left and right sides swapped.",null,1,cdnxgtn,1rixyo,askscience,new,14
Dorcus0,"Take a picture of yourself. Go to a mirror, and compare the picture of yourself with your mirror image. Or, as a thought experiment, put gloves on your right hand, then look in the mirror.

A mirror image is more accurately thought of as you pushing yourself through the mirror, not a 180 degree rotation.",null,0,cdo4jr4,1rixyo,askscience,new,1
SMURGwastaken,"glucose doesn't require insulin to be metabolised; rather insulin causes cells to absorb glucose. It's a significant distinction since fructose and glucose metabolism are similar in that the same kind of chemistry occurs, albeit with different enzymes.

The short answer is that the small differences between fructose and glucose are enough for the body to differentiate them. IIRC, glucose has an aldehyde functional group whereas fructose has a ketone. 

It's more complicated than that however because insulin *does* affect fructose uptake in skeletal muscle cells, just not in the liver. This is because fructose is not normally metabolised in all cells and is processed by the liver first, so presumably it's beneficial for all fructose to enter the liver so that it can be phosphorylated into a form which can be used elsewhere.",null,1,cdnqmo2,1riyww,askscience,new,10
fartprince,"Just because chemicals are similar in structure doesn't mean they are necessarily processed the same way. Indeed, even differences in enantiomers (ie same exact chemical structure but just mirror-images of each other) can lead to very different effects. The classic case is thalidomide, where one enantiomer was teratogenic while the other wasn't. 

As for glucose, insulin's active site contains residues that can directly bind/interact with the chemical structure of glucose but probably not fructose. Why it evolved this way, one can only speculate. There are many other examples where similar chemical structures can have very different effects (and also very different chemical structures can elicit very similar effects).",null,0,cdnqqds,1riyww,askscience,new,5
mutatron,"What SMURGwastaken said. Molecules and the molecules that operate on them are like keys and locks. If you live in my apartment complex, my door key may have the same general shape as yours, but it won't unlock your door. It's only a small difference, but it's enough.

There are [a number of sugar molecule transport proteins](http://en.wikipedia.org/wiki/Glucose_transporter), which are embedded into the walls of cells and often only admit sugar molecules at the request of a third signaling molecule, or even a signaling complex of molecules. Also, the number of transporters in a cell's walls is not fixed, but can be regulated by other molecules.",null,0,cdnqw7e,1riyww,askscience,new,2
threegigs,"But, it doesn't *require* insulin to be metabolized. Insulin simply causes more GLUT4 expression, resulting in *increased* uptake.",null,0,cdnqfge,1riyww,askscience,new,1
Criticalist,"Yes, there is a wealth of evidence from animal and human studies that links cholesterol to heart disease.

In animals that have genetic defects that lead to high LDL cholesterol, tend to develop atherosclerosis, while those species that have low levels don’t. In humans, there is the [Framingham Heart Study](http://www.ncbi.nlm.nih.gov/pubmed/?term=9603539), [The Multiple Risk factor Intervention Trial](http://www.ncbi.nlm.nih.gov/pubmed/?term=3773199) and the [Lipid Research Clinics Trial](http://jama.jamanetwork.com/article.aspx?articleid=391065) which are large scale trials demonstrating a clear association between levels of LDL cholesterol and the development of heart disease. The higher the level, the greater the risk.

In people who have a genetic defect that leads to very high blood cholesterol levels there is a very early onset of heart disease, even if they have no other risk factors.

When LDL cholesterol is reduced in clinical trials there is a clear benefit, and this occurs with cholesterol lowering drugs other than statins, and lifestyle interventions. These include the [STARS trial](http://www.ncbi.nlm.nih.gov/pubmed/1347091), the [Lifestyle Heart Trial](http://www.ncbi.nlm.nih.gov/pubmed/?term=1973470) and the [NHLBI type II study](http://www.ncbi.nlm.nih.gov/pubmed/6360415).

LDL cholesterol has been comprehensively studied and the results are very strongly suggestive that it is an important, modifiable risk factor for heart disease. 

",null,0,cdo50t1,1rj0sn,askscience,new,9
Philosophisation,"Yes there are. Let me give the scientific basis for why ""cholesterol causes heart diseases"". Cholesterol is a steroid: it goes into cells and tells them what to make. There are two common types of cholesterol. Low density lipoprotein cholesterol or LDL-Cholesterol and High Density Lipoprotein Cholesterol or HDL-Cholesterol. Low density cholesterol is bad for you because your body has a harder time absorbing it and it will over time accumulate as plaque on your artery walls. Think of this process like a pipe getting clogged by kitchen fat. If you flush the fat in small chunks then the chance that it sticks to the sides is lower due to lower surface area. But if you do it as a bucket of fry fat then it will coat the sides and definitely leave some residue.So really you do not need a study to understand why and how LDL cholesterol impacts you. Here is a tip to maintain healthy cardiovascular system or ""heart"" as you put it: Eat foods low in both LDL and Mono-Sodium Glutamate or MSG which helps the cholesterol aglutamate (stick together).edit to pedants: Yes this is a simplification.",null,3,cdo2ocu,1rj0sn,askscience,new,1
_NW_,"Diamonds are the hardest material, but are brittle.  They can be cleaved with a hammer and chisel.  To get it to the final shape, they are ground on a wheel covered in oil and diamond dust.  Diamonds can also be cut with a steel blade lubricated with oil and diamond dust.",null,0,cdogkuv,1rj19e,askscience,new,2
Freeoath,"I saw noone had commented so I though I might explain it with what I know.

The index finger is the prefered finger amongs most people, some use the middle finger.
Why we use the index finger more than the other fingers is most likely due to the fact that it is more dextorious and flexible than any of our other finger. The index finger is also the most sensitive, giving back the best response to touch. 
Using the index finger to point or press does not strain the hand or any of the other fingers, like thr other fingers do.
The middle finger can be used but is universally seen as a insult. 
Even from the age of 1-2 kids prefer to use their index finger to point over their other fingers.

I have not studied human anathomy but this is what I know.
Sorry for spelling errors, on a phone and english is not my main language ",null,0,cdo3h5x,1rj19j,askscience,new,3
SMURGwastaken,"To elaborate on some of what /u/Freeoath said, the index finger is (aside from the thumb) the only one of the digits on our hand capable of wholly independent action. For example, try moving your little finger without moving your ring finger. You can't (or you shouldn't be able to anyway). Now make a fist and try extending your ring finger without moving you middle finger. Again, you can't (or shouldn't be able to). Now open your hand again so all your fingers are fully extended and attempt to touch your palm with your middle finger whilst keeping the ring finger fully extended. You can't (or shouldn't be able to). 

This is basically down to the fact that the tendons in your forearm control multiple finger functions, leaving the index finger as the only one which can operate independently of the others. This makes it the most flexible and best for precise operation, which is probably why it has evolved to be the most sensitive and responsive, and why we tend to use it above all others. The middle finger is the next best thing, which is why some people use that one too.",null,0,cdo55sf,1rj19j,askscience,new,2
SMURGwastaken,"I'm not sure I understand what you're asking... Plants do not conduct nitrogen fixation, rather bacteria do it for them (forming colonies in root nodules grown specially by the plant in the case of things like legumes). As for the 'advantage' of doing so, it simply comes down to the fact that plants (like all complex life) require proteins and are (generally) autotrophic organisms. Since proteins require Nitrogen and the only source of non-organic Nitrogen is the N2 in the air, it is beneficial for a plant to culture its own colonies of Nitrogen-fixing bacteria rather than relying entirely on those free in the soil.

I think what you mean by ""Nitrogen-fixing plants"" are plants that grow root nodules in which Nitrogen-fixing bacteria then grow. It only helps competing plants if the host plant dies, at which point it doesn't care - and in the mean time it effectively has its own nitrate factory.",null,0,cdnqz4c,1rj1hy,askscience,new,2
cladocerans,"First question: Yes and no. Plants that have symbiotic colonies of nitrogen-fixing bacteria primarily promote their own growth. The nitrogen-fixers are located in their roots, and they get first access to that resource. They do, however, facilitate the growth of competitors--some nitrogen is lost to the soil, and becomes available to other nearby plants. Also, more nitrogen is available to the community as the symbiotic plants are consumed and decomposed. It's basically a win-win situation--the symbiotic plant gets most of the N (minus the loss of some photosynthate for maintaining the bacteria), and the other plants get a fertilizer boost tangentially.

As for arising in homogenous or austere environments--I don't really understand what you are asking. Nitrogen is limiting in most terrestrial ecosystems (i.e. it is the one nutrient plants need the most relative to how common it is). The habitat doesn't have to be homogeneous or stressed for a N-fixing symbiosis to be advantageous.",null,0,cdnsl1l,1rj1hy,askscience,new,1
South_park_fan,"Look at the stability of the intermediate carbocation compared with the effects of steric hinderance, 1˚ (primary) carbons will form unstable carbocations, whilst 3˚ carbons form much more stable carbocations. This means that Sn1 is not favoured for 1˚ carbons, but is favoured for 3˚ carbons. Remember, Sn1 requires a stable intermediate carbocation. Conversely, 3˚ carbons have high steric hinderance and thus are not easily attacked by nucleophiles, whilst 1˚ carbons are easily attacked by nucleophiles as there is space for the nucleophile to come into contact with the carbon. So Sn2 is favoured by 1˚ carbons.

A solvent is not termed as a reactant as it is just what is used in order to make the reactants come into contact with each other.

Nucleophiles tend to be strong bases and will usually carry a negative charge. They always have a lone pair of electrons. ie OH-

You can find a substrate by looking for leaving groups, leaving groups are  weak conjugate bases (Cl-,Br-) ect. Your substrate will be some kind of organic chain and will always be fully saturated at the carbon at which the substitution is taking place.",null,0,cdo48c0,1rj285,askscience,new,4
__Pers,"I'm not sure what specific chaotic pendulum system you're describing, but let's consider a damped pendulum subject to a sinusoidal forcing, a chaotic pendulum yielding chaotic orbits. 

Orbits of the chaotic pendulum's dynamics will be 1D trajectories (curves) in some sort of smooth, abstract space (a ""manifold"" in the mathematical parlance). In the case above, the three ""axes"" of this space could be chosen to be phase, angular frequency, and phase of the driver, for example, with the understanding that phase and driver phase wrap back on themselves every 2 pi. A Poincare map is made by looking at the structure of the crossings of an orbit through a lower-dimensional surface embedded in the 3D. For instance, you could make a 2D map whose coordinates are the phase and angular frequency of the pendulum every time the driver's phase reaches 2 pi.

A reason one might make such a Poincare map (besides the fact that they are rather pretty) are that properties of the map such as behavior around fixed points reveal insight into the underlying dynamics. Also, they tend to be fractal sets whose dimensionality and other properties can reveal something of the chaotic nature of the underlying dynamics.",null,0,cdo80qu,1rj2ho,askscience,new,1
zmbbmz,Yes this happens all over the world.  This is from the clouds insolating the Earth.  The energy from the sun comes to Earth in the form of short wave energy being that the molecules are excited from the heat of the sun and move faster.  The gasses in the atmosphere allow these short wave energies to pass through.  When the short wave comes into contact with the ground the energy is taken in and released as long wave energy.  The gasses in the atmosphere do not allow this long wave energy to travel through as readily and hold most of it in.  So when it is cloudy there are more of these gasses in that area preventing more of the long wave energy getting out which in turn heats up the area.  This is the cause known as the 'greenhouse effect'.,null,1,cdnzgvg,1rj31i,askscience,new,1
RelativisticMechanic,"I've never seen a good one, but it's important to realize that even such a ""3D representation"" would fail to show the ever-so-important *time* dimension and the curvature thereof.",null,0,cdo280s,1rj34y,askscience,new,5
iorgfeflkd,"http://cdn-static.zdnet.com/i/story/50/00/018934/18935.jpg

http://d1jqu7g1y74ds1.cloudfront.net/wp-content/uploads/2008/02/noexcision_full_7.png

https://www.nikhef.nl/uploads/pics/Colliding_black_holes.jpg

But really, see /u/reativisticmechanic's reply.",null,0,cdo8wk8,1rj34y,askscience,new,2
uncleawesome,Gravity isn't something you can see. It is an invisible force. Those pictures represent gravity as a low spot in a plane. If you roll a ball on the plane it will roll to a low spot. If the ball is going faster it might roll thru part of the indent and move off in a new direction. It could also orbit around the low for a while until its speed slowed enough for it to fall down into the hole.,null,1,cdo2blz,1rj34y,askscience,new,1
iorgfeflkd,"The light-analogue of a conducting  (or insulating, semiconducting, etc) material is called a photonic crystal. There is a type of photonic crystal called a Whispering Gallery Resonator where the light can constantly be reflected around in a circle, being totally internally reflected from the edges. They don't last forever because if inherent imperfections in their construction.",null,1,cdo8qh4,1rj6t4,askscience,new,2
Osymandius,"I may have misinterpreted /u/Philosophisation's answer but I don't quite agree with all of it. He's right in that first we metabolise carbohydrates, then lipids, then protein, but some of the details don't quite sit right with me.

Initially we metabolise free blood glucose - this is plentiful after a meal, but insulin secretion rapidly triggers glycogenesis and blood glucose falls. We normally have enough glycogen to last us about 12-24 hours of starvation. By the time breakfast rolls around, you might /just/ be beginning to burn fat. 

Now we start mobilising adipose reserves. Stored fat (triacylglycerides) is mobilised by breaking it down into free fatty acids (FFAs) and glycerol. Glycerol is easily metabolised. FFAs are transported to the target tissue and imported into the mitochondria and undergo b-oxidation where they are broken down, 2 carbon chunks at a time, into acetyl CoA. This can be fed directly into the Krebs cycle and respiration proceeds as normal.

Eventually (now you're really starving) we start metabolising protein. This is bad for a number of reasons. Firstly and most importantly - we don't store energy as protein. Any protein in the body is there because it needs to be there, either structurally or functionally. Secondly, protein is full of nitrogen. We have to deaminate amino acids in order to metabolise them. This is energetically expensive and leads to conditions like hyperuricemia and other unpleasant ways to go.

Ketone bodies are also very important. The majority of your tissues are happy to burn anything more or less. The brain is considerably more picky. It will only burn glucose (either free blood glucose straight from food, or that stored in glycogen and remobilised) or ketone bodies. These are produced in the liver from fatty acid metabolism and circulated back into the blood. Their synthesis begins more or less simultaneously with the start of fat metabolism, albeit at a slow rate. As starvation proceeds, levels of ketone bodies continue to rise.

",null,0,cdo4e12,1rj98n,askscience,new,4
Philosophisation,Think of the order of magnitude for carbohydrates and also recall that monosaccharides are all that your cells eat. The bigger things are broken down in order to be consumed.Order of consumption:1Monosaccharides:GlucoseGalactoseFructose2Disaccharides:Sucrose= glu+fruLactose=gal+gluMaltose=glu+glu3Polysaccharides4Lipids5ProteinGetting to 5 is almost impossible and only occurs when there is extreme malnourishment as protein does not yield a large amount of net energy. ,null,0,cdo2rpi,1rj98n,askscience,new,2
medikit,"*Bordetella pertussis* is super infectious and was a significant cause of infant mortality. Tetanus is ubiquitous in the environment and a danger to all who are not vaccinated. Diphtheria used to be a significant cause of morbidity and mortality.

We try to combine vaccines when we can. It is much more efficient and also minimizes the amount of shots that people receive which improves the likelihood that people will comply with vaccination. There is not currently a single acellular pertussis vaccine separated from tetanus and diphtheria.

There are other formulations that include *haemophilus influenzae* type b (Hib), Polio, and even Hepatitis B. I assume you are familiar with Polio but Hib was quite deadly and has all but disappeared in the US after vaccination. Hepatitis B is more likely to cause chronic disease if acquired in infancy which can lead to Liver failure and/or Liver cancer.

Because pertussis (and measles) are so incredibly infectious (research this) you could make the best case for mandatory vaccination of these diseases. It would be possible to create a pertussis vaccine separate from Diphtheria and Tetanus so I wouldn't worry too much about this issue. You may want to know the difference precise difference between Tdap and DTaP (Hint: it has to do with concentration of each component).

This article will be helpful: http://www.npr.org/blogs/health/2013/09/25/226147147/vaccine-refusals-fueled-californias-whooping-cough-epidemic

Also be aware that we changed the pertussis vaccine to its current acellular version to avoid unwanted side effects but there is concern that it may not be as effective. Here is some very recent info about this: http://news.sciencemag.org/health/2013/11/whooping-cough-vaccine-does-not-stop-spread-disease-lab-animals",null,1,cdo05hx,1rj9fp,askscience,new,5
contactinhibition,"[DTaP](http://www.cdc.gov/vaccinesafety/vaccines/dtap/dtapindex.html) is short for Diptheria Tetanus acellular pertussis, which is composed of the Diptheria toxoid, Tetanus toxoid, and a protein formulated to mimic the pertussis bacteria. It replaced DTP, which used toxoids but whole pertussis bacteria that had been lysed (broken apart). Part of the effectiveness of the vaccine is how it is formulated-the toxoids are linked to the pertussis protein to increase effectiveness and provide protection. ",null,1,cdnyvq8,1rj9fp,askscience,new,3
Henipah,"Diphtheria and tetanus are also horrific diseases. Having single injections is going to reduce the likelihood of being covered for them. For instance after the pointless controversy over the MMR disease after Andrew Wakefield's fraudulent Lancet paper people tried getting separate measles, mumps and rubella vaccinations. This increases the chances of people missing the necessary jabs, e.g. rubella... which parents may be less worried about immediately for their children but with important public health implications. ",null,1,cdo33cf,1rj9fp,askscience,new,2
jamesinphilly,"&gt; Are there shots available on request to only give the pertussis vaccine?

Back in the old days when the vaccines were killed cells of the pathogen (called 'inactivated' vaccines), you could get the shots separate. This is because people often had allergies associated with them (really, many were just side effects, but that's how they got classified), so you'd get the vaccines a la carte. As we have moved to the acellular vaccines, you can't get the pertussis vaccine by itself. This is because the manufacturing process is much more expensive, there's no demand for it by itself, and there's no harm in getting an extra booster. And if you combine it with other vaccines (as other people have pointed out), it also means less shots and more compliance with doctor's visits. Also, storage is a pain, you have to chill these things or they denature easy. It's obviously much easier and cheaper to transport and keep chilled 1 vial vs 3. If our hospital had the option of a pertussis-only vaccine we would not buy it for this very reason.

Good luck on your essay",null,0,cdoz19a,1rj9fp,askscience,new,2
expandedthots,"You should think of schizophrenia as the typical psychotic disorder, as any other psychotic disorder is going to have a similar spectrum of symptoms as schizophrenia (bipolar with psychosis, psychotic disorder now etc). So the dopamine hypothesis says that some disruption in dopamine processing leads to dysfunctional signaling between multiple locations in the brain. The benefits of this that it does serve to decrease positive symptoms (hallucinations, delusions) and newer atypicals also are somewhat active in reducing negative symptoms (lack of speech, not caring about anything, no energy to do anything). The downside of this is that it disrupts dopamine signaling globally across the brain, and causes movement problems when the drug reaches high enough concentrations in these areas. Look at tardive dyskinesia for a picture into how debilitating these problems can be. 

Now, there are other theories about whats truly occurring in psychotic brains. Some people argue that dopamine antagonists are merely acting as symptomatic relief, meaning it isn't really targeting the mechanism of whats going wrong, just acting at a downstream site where the symptoms can be limited. I'd argue this could be true, because of the success of a specific atypical, clozapine, in treating treatment resistant schizophrenia. 

Clozapine is an antagonist of basically every receptor in your brain. Some people, such as Herbert Meltzer, argue that its actually an effect of clozapine on serotonin receptors that increases its efficacy over other antipsychotics. He has substantial evidence to back this up, but it again becomes a matter of mere receptor pharmacology. 

I'd suggest looking into the phospholipid theory of schizophrenia. It discusses how the neuronal composition of susceptible brains has a different composition of phosphoplipids, which can have significant downstream effects as far as inflammation and disrupted immune signaling and oxidative stress handling. Its probably a piece to the puzzle along with the receptor pharmacology. The reason I support it is because clozapine is the only antipsychotic that interacts truly with this system.

http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3478065/
",null,0,cdpllb0,1rj9sv,askscience,new,1
syvelior,"Well, actually, the example you cite is a case of misperception in both directions.

Japanese doesn't have an /r/ sound - they have an alveolar tap /ɾ/ which is similar to the sound produced in most varieties of American English in the word butter (Tsujimura, 1996).

Bradlow et al. (1997) showed that extensive training on perceiving the /r/ - /l/ distinction with native Japanese speakers resulted in better production distinctions, strongly suggesting that incorrect productions stem from failing to perceive the differences between these sounds (and in fact, putting them in a category that is incorrect for both).


**References**:

Bradlow, A. R., Pisoni, D. B., Akahane-Yamada, R., &amp; Tohkura, Y. I. (1997). Training Japanese listeners to identify English /r/ and /l/: Some effects of perceptual learning on speech production. *The Journal of the Acoustical Society of America*, 101(4), 2299.

Tsujimura, N. (1996). *An Introduction to Japanese Linguistics.* Basil Blackwell: Oxford.",null,0,cdo3hnd,1rjast,askscience,new,6
paolog,"1. The sound doesn't exist in the speaker's language. For example, speakers of Romance languages may not be able to hear a difference between English /æ/ and /ʌ/.

2. Making a particular sound involves positioning the tongue, lips, etc in an unfamiliar way that a speaker, unlike a native speaker, has not been practising since childhood, so a more familiar sound that approximates it is substituted. English speakers learning French will often use /u/ for /y/ and /nj/ for /ɲ/.",null,0,cdo3suv,1rjast,askscience,new,1
shavera,"the positron's the only anti particle with its own special name. In diagrams though we'll just label it with an e^+ . It's just an old naming convention that's stuck around.

Mesons were so named because early particles seemed to fall into 3 categories, based on mass. Light ones were ""leptons,"" medium ones were ""mesons,"" and heavy ones were ""baryons,"" coming from the greek for light, middle, and heavy respectively. The muon was originally lumped in with the mesons because it has a mass similar to a pion.

Later on, after the discovery of the quark model, we realized that baryons had 3 quarks, mesons had a quark and an anti-quark, and leptons had no quarks (which reclassified the muon back into the lepton family where it belongs). 

Recently, we've also discovered 4-quark (2 quarks, 2 anti-quarks) bound states, but we've been rather dull and called them tetraquarks",null,0,cdnxn58,1rjc6k,askscience,new,3
MCMXCII,"A quark/antiquark pair is not necessarily bound. And a meson is not necessarily a quark/antiquark pair. A meson is a quark and an antiquark, but they don't have to be the same flavor.

What do you mean by ""an antiquark is not an anti particle like say an anti electron is a positron?"" Antiquarks *are* antiparticles just like positrons....",null,0,cdnxgkf,1rjc6k,askscience,new,2
blueyedlvrx01,"Ok, I think I understand your question. You are wondering if there is any gravitational pull on our atmosphere other than the gravitational pull of earth. The short answer is no. If there is such a thing, then the forcing is so weak that it is not considered in the governing equations we use to understand and forecast weather. There are terms that we do throw out due to them being relatively small in scale and complicated to calculate. Even with that, there is no term for what you are describing here. I

A few quick things to note that you may find interesting:

The atmosphere is deepest in the tropics due not only to the very warm temperatures experienced here, but also because of centripetal acceleration due to angular momentum from the rotation of the earth. Angular momentum is largely conserved by atmospheric motions, especially in the upper atmosphere. This is the reason for mid-latitude extratropical synoptic systems (aka cold-front/warm-front systems). 

I could also get into gravitational forces and buoyancy. A parcel of air (think of like a small cluster of air - I tend to think of a bubble of air for all intents and purposes) can become positively buoyant for many reasons. When this parcel becomes positively buoyant, it accelerates upward to try to become neutrally buoyant. It will overshoot this level of neutral buoyancy and then it will experience a downward acceleration. It will still overshoot this level it's trying to get to (the level where it will be neutrally buoyant) so it will  experience vertical motions over time much like a sine wave. Ever heard of gravity wave clouds? This is what is happening here. The downward acceleration it experiences is due to gravity. 

Edit:
Source: BS in meteorology and currently an atmospheric science graduate student. ",null,4,cdo3piw,1rjceb,askscience,new,4
MichaelHRender,"No, you can not sing a chord. A chord being a harmonic set of three or more notes.

But you can sing an interval. [Tuvan throat singers](http://en.wikipedia.org/wiki/Tuvan_throat_singing) are a classic example. When you create a tone with your larynx, it has a rich harmonic content. Meaning it is made up of a whole series of smaller waves of different frequencies. You can adjust your throat and mouth to amplify those frequencies (sometimes called partials) to make them distinct enough to be heard as an interval of the main frequency.",null,0,cdo4ony,1rjchg,askscience,new,1
mrmayo26,"Well this is not the case for all transmittable infections, but at least for HIV and things like bird flu, the idea is that at some point the infectious agent does something (like say change the structure or functioning of its outer membrane or the receptors on it which it uses to gain access into a cell) which allows it to start to infect another species. 

On a similar note there are some infections which have intermediate hosts which they infect but don't cause harm to (like mosquitos and malaria, and ducks with several other parasites) which a parasite can evolve to be part of it's life cycle where it matures. This also has the advantage that if all the final host species in an area die off, the parasite species can still stay alive in the intermediate host. I believe the general term for these is [Zoonosis](http://en.wikipedia.org/wiki/Zoonosis)

But as for your specific example of HIV, it came about after a particular form of SIV (simian immunodeficiency virus) which infects certain primates (simians being a classification of ""higher"" primates) found its way into the blood and/or body of a human and managed to somehow do its virus thing.

One thing to remember is that viruses, especially retroviruses like HIV and SIV, are very sloppy when it comes to replicating themselves and make a lot of uncorrected errors. Luckily for HIV/SIV and unluckily for us, if you increase the mutation /error rate you also increase the rate of adaptation / evolution which helped make the jump to humans. 

tl;dr  viruses are sloppy at replicating their genome, leads to more variety, one variety happens to be able to infect a new species ",null,0,cdnxm8y,1rjcxp,askscience,new,2
medikit,I think you might enjoy this radiolab: http://www.radiolab.org/story/169879-patient-zero/,null,1,cdnzn3t,1rjcxp,askscience,new,3
mzyos,"Just to add to this, they believe the transmission was via the preparation of ""bushmeat"", which can be different kinds of primates depending on the region it comes from. What is thought to have occurred is that a women preparing the meat underwent blood to blood contact with an infected primate. This may have happened quite a few times until the virus was able to transfer from human to human after a mutation. 

   This is the reason everyone was getting seriously worried about bird flu. It could infect someone who had contact with birds, but it couldn't pass from human to human. This will be a serious problem if it ever mutates and gains the ability to do so.",null,0,cdoj1ox,1rjcxp,askscience,new,1
abstrusey,"Viruses (IMO) offer the most interesting examples of infectious diseases that act this way (the majority of clinically relevant bacteria, fungi, and parasites tend to also exist elsewhere in the world, outside of humans, or typically exist in/on humans without causing disease). You have to remember that viruses are just little self-copying machines. They copy copy copy, and then copy some more. Like with a Xerox copier, sometimes the quality degrades, and it's hard to read the message. In the case of these viruses, it can sometimes be a copying alteration/error (aka mutation) that results in different abilities/limitations of the virus (e.g. now it's more infectious but it is also less likely to travel outside of the host species). Over time, the virus can become so ""host-specific"" that it is only transmitted between that species. Diseases that are too weak will be wiped out by the immune system, or they will not be able to replicate enough/fast enough to cause the disease to spread. On the other hand, infectious diseases that are too aggressive can kill the host before they are able to pass it on to others (especially before the modern era of the global community).

There is pretty convincing evidence that HIV comes from Central African chimps (it is transmitted through infected body fluids, like blood, which would be a likely original source if someone fought with or ate a chimp). 

Hepatitis C Virus is an interesting case study for your question, because the answer is still unknown. An interesting [paper on HCV](http://www.ncbi.nlm.nih.gov/pubmed/23463195) states: 

&gt;Going back a final step to the actual source of HCV infection in these endemic areas, non-human primates have been long suspected as harbouring viruses related to HCV with potential cross-species transmission of variants corresponding to the 7 main genotypes into humans. Although there is tempting analogy between this and the clearly zoonotic origin of HIV-1 from chimpanzees in Central Africa, no published evidence to date has been obtained for infection of HCV-like viruses in either apes or Old World monkey species. Indeed, a radical re-think of both the host range and host-specificity of hepaciviruses is now required following the very recent findings of a non-primate hepacivirus (NPHV) in horses and potentially in dogs. Further research on a much wider range of mammals is needed to better understand the true genetic diversity of HCV-like viruses and their host ranges in the search for the ultimate origin of HCV in humans.",null,0,cdpej1s,1rjcxp,askscience,new,1
Mn2,"&gt; I recently was reading about the late Stephen Jay Gould's objection to sociobiology (that is to say, that a relationship exists between human social behavior and evolution by natural selection), and how scientists like Richard Dawkins firmly advocate the theory and the correlation it suggests, but I must say that I don't fully understand the counterargument to it (as suggested by Gould and others).

To my knowledge, Gouldt mostly argues that many of the higher functions we have are by-products/indirect consequences of natural selection rather than something that was directly selected for. This is not the same as to say that natural selection has not have a major impact on human behavior. He did approve the theories of kin selection, for instance. 

I'm coming from the field of biology and within our field a lot  -  I would almost dare say majority -  of people are skeptical of sociobiology simply because you have to be very careful and thorough with what you do. It is way too easy for us to make unfounded assumptions (often mirroring our society and values) of how things were x number of years ago, to conclude that this lead to natural selection and that's why things are like they are today. We already know from first hand experience, how much our own values and expectations influence science (e.g. only recently did we start to acknowledge that homosexual behavior in animals is common and that females are in general pretty promiscuous). Hence, it often feels like sociobiology is repeating the same mistakes as ethology did.

There is also a lot of very poor quality sociobiology out there... It is often very simplistic (does not discuss what genes might be responsible, what the mechanism might be and if this is influenced by the environment) and non-critical (disregards a lot of cognitive psychology, neuroscience etc in how our brains interpret and react to stimuli, often forgets or ignores to take in count the cultural influence on our behavior and our assumptions, clumps several different biological functions into one and does not take in count all what we still don't know about biology). Unfortunately this really flies well within media because it is easy to understand and related to and it doesn't require any actual understanding of biology. Dawkins does, unfortunately, sometimes fall into this category.

However, absolutely not all of it is bad. For one, Robert M. Sapolsky has written high-quality stuff relating to sociobiology.

",null,0,cdo47wl,1rje0t,askscience,new,1
Jabra,"To see if a drug may be addictive, it is tested on animals. Rats for example. A strain of rats which has a high propensity toward addiction is typically used. Basically, these rats are put in a [Skinner box](http://en.wikipedia.org/wiki/Operant_conditioning_chamber). They receive a dose of the drug each time they press a lever. If the rats keep pressing the level to get a new dose, the drug might cause addiction. One can compare the drug to a placebo to make sure it is in fact the drug and not the rat's anticipation causing it to press the lever.

The animal model itself is validated by using known addictive substances such as cocaine and comparing that to a placebo. Rats with a propensity towards drug seeking behavoir will keep pressing the lever as long as it results in a new dose of cocaine, but will stop when the receiving placebo a few times.

On a related, unscientific note: A colleague of mine used to work with these rat models. He was in the business of validating these tests. In order to study other possible behavoiral effects of drugs they would put a rat on a large table. Normally, a rat will skirt around the edges of the table to see what is up. Once it gets bored, it will scoot back to the centre of the table. There it waits to be picked up and brought back to its cage. A rat on cocaine is something else. It starts skirting the edges of the table with lighting speed. It keeps running, faster and faster, until... it cannot make the turn at the end of the table and flies off. Then you have a rat on cocaine loose in your lab. Try catching one of those. Fun times...",null,0,cdo5t0l,1rjfqu,askscience,new,3
chuck10470,"The tires are very hard when cold. F1 speeds would melt normal tires, so they use tires that get soft and ""grippy"" at high speeds. Conversely, when the car is going slow, the hard tires have little grip, and are more akin to hockey pucks than tires. So, in an F1 car, if you go slow, you spin. ",null,1,cdo027b,1rjhk1,askscience,new,4
Ermagerd_cerpcerk,"Also F1 cars are designed to generate lots of downward force. The wings and aerodynamics of the car use the air whipping passed to force the car onto the ground. I can't verify, but I was told that if an F1 car was topped out, it would be generating enough downwards force to be able to drive upside down ( like in Men in Black when they drive on the the tunnel ceiling). So the faster you go, the more force the tires have on the road, resulting in higher friction/grip ",null,0,cdoawat,1rjhk1,askscience,new,2
fortunecooki,"F1 tires, when heated, soften and ""mould"" to the surface. The track is not completely smooth and every little tiny crevice in the track is a decrease in surface area for the tire to grip to. By melting/ softening the tires, the tire rubber moulds into these crevices, thereby increasing the surface area on which a friction force can act on. This increases the coefficient of friction. here is a nice pic:
http://insideracingtechnology.com/Resources/mechkey.gif
The more surface area, the more ""grip"" a tire has. 
As a side note, if you look at the normal racing tires, they are completely smooth compared to the ones we normally drive with day-to-day. The have more grip but are in serious trouble if there is a touch of water. there is very low coefficient of friction between water and rubber so the tires will have little to no grip. Hence, they swap to tires that have grooves in them to expel the excess water. Because we don't like changing our tires all the time, we always have grooved tires. This can also been seen on your dress shoes compared to your running shoes",null,0,cdoqhkq,1rjhk1,askscience,new,1
ControllerInShadows,"They do a variety of things depending on the species... Many bugs go into a dormant stage as adults or larvae. Ants for example will be dormant deep below the surface in very cold climates, while other bugs may find refuge in rotten logs or bark. Other bugs (such as the Praying Mantis) will lay eggs which survive the winter and 'hatch' in the spring. In such cases the adults will typically die.

If you find a rotten log in the winter and open it up, you'll likely see many slow moving bugs (mostly larvae) calling the log home for the winter.",null,1,cdo3bke,1rji2i,askscience,new,8
Platypuskeeper,"Well, there's no consensus on what 'atomic radius' means in the first place! Atoms have a diffuse cloud of electrons around them, and it's fairly arbitrary where you consider the 'end' of to be. E.g. the Bohr radius is the most probable radius for the electrons (note: not the same thing as the most probable _location_) Another possible definition (but seldom used) would be the radius that encloses a certain percent of the electron density. Or you could take half the bond distance when two identical atoms have a single bond (the covalent radius). Then there's the 'ionic radius' which exists in two different definitions by Shannon and Pauling. Or, you could use the 'effective radius' as determined from their interactions in the gas phase (read: the van der Waals gas law), which is the vdW radius.

Now since noble gases don't bond, you can't really use ionic or covalent radii. Van der Waals radii work well, because noble gases have close to ideal-gas behavior, but the vdW radii for anything is substantially larger than other measures, so you can't make a comparison that way. 

But the most concrete measure here would probably be the Bohr radius, as it's more directly related to the electron density. And from that perspective, adding electrons does increase repulsion, but it doesn't do so by an extra amount when you complete the shell. The effect of electron-electron repulsion on the radius is actually pretty small. This is illustrated pretty well by comparing hydrogen to helium. Hydrogen has a Bohr radius of 1 Bohr. Since helium has twice the charge, it would have exactly half of that - if the electrons didn't repel each other at all. The [actual radial density](http://www.rsc.org/ej/CP/2009/b901402k/b901402k-f5.gif) (upper curve) has its peak only a slight above that. In fact, it's hard to tell the difference in the radial density distribution between reality and [the situation where they don't repel at all](http://wiki.chemeddl.org/mediawiki/images/0/05/H_and_He_orbitals.gif) (leftmost curve, squashed a bit by a different horizontal scale).

This is not as dramatic for heavier elements where the relative increase in nuclear charge isn't as large, but it still tends to outweigh the increased e-e repulsion. By just about any measure related to the electron density, the noble gases will have a radius somewhat smaller than the preceding halogen.

The exception to this (and perhaps the reason why some people think the radius is bigger), is the van der Waals radius, which for noble gases is indeed larger than for the preceding halogen. But this goes to how the vdW radius is defined by the effective interactions, it's more a result of noble gases having much less mutual attraction than any other atoms. 
",null,0,cdo18bs,1rji79,askscience,new,4
Henipah,"If someone has a head injury they can potentially develop [bleeding](http://www.hakeem-sy.com/main/files/images/Location%20of%20epidural,%20subdural%20%20hematomas.JPG) inside the skull, for example an epi/extradural haematoma. This particular injury can be associated with a ""lucid interval"" where people seem fine, but then as pressure builds up they become confused and progress to coma and ultimately death. 

It used to be advised that people not go to sleep because this would prevent proper observation. These days generally people would not be sent home if they were thought to be high risk. ",null,0,cdo2son,1rjiho,askscience,new,4
SirGoo,"when you look at something, there is a split second lag between the light hitting your eye and your brain interpreting the image. Also, when you stare a light source for too long, and then look at a blank wall, you can see a spot that clouds your vision slightly. I assume that when you look at these images above, your eye is not holding perfectly still. Even when you try to focus on the exact center of a bar, your eye is slightly correcting the angle every once in a while. this causes a negative image of what you are looking at to flash for brief moments, superimposed behind the actual image you are trying to see. you have to realize all these ""trail"" phenomena are taking place in your head, not on the image, or in your eye. ",null,3,cdo1znz,1rjir4,askscience,new,6
chrisbaird,"Your eyes are always jittering around slightly without you noticing. Your eyes do this to compensate for the blind spot where the optic nerve exits through the retina. If your eyes did not jitter, you would always see an ugly blind spot in your field of vision. When you look at one bright color, it saturates the cone cells specific to that color so that they briefly don't work as well. If you then look at a different color, that color is perceived as skewed because of underperforming receptors. The eyes' jitters makes the receptors the are detecting the part of the image that is close to the border between two bright colors to jump back and forth across the border, getting saturated and skewed in the process.

The bottom line is that the effect is caused by the ways your eyes work and not by the way the computer screen works or the way your brain works.",null,0,cdoblgs,1rjir4,askscience,new,3
WhiteLightMods,"Try this for fun. Make 4 squares, one each of red, blue, green, yellow. Arrange them in a 2x2 arrangement with a small gap between, kind of like the Windows logo, on a white background. Hold your head still and stare at the center between the squares for 45 seconds. Shift your eyes over to a white sheet of paper. There will be an opposite image in your view.",null,0,cdoil25,1rjir4,askscience,new,1
paolog,"Because the yarn passes round the edges of the square in a curve rather than a sharp angle. This means that the yarn does not flatten out until it is a small distance in from the edge of the square. The next winding will lie on top of this curve and will form a looser curve and the yarn will only flatten out a bit further still from the edge of the curve. If you were winding in one direction only around the square, eventually these curves at the top and bottom edges of the square would become so wide that they would meet in the middle, in other word, the yarn would no longer lie flat between the two edges. The wound yarn would now resemble a cylinder. Winding in random directions around the square forms curves in all directions, which eventually become so large that they form a shape resembling a sphere.",null,0,cdo3vcp,1rjivn,askscience,new,2
FoolsShip,"The amount of dissociation that occurs is proportional to the ratio of dissociated product in the mixture. The more of the dissociated product is in the mixture, the less likely dissociation will occur. 

When burning hydrocarbons the dissociated products are obviously a result of combustion as well. Therefore they will be present during combustion. This means that there will always be more dissociation in a leaner mixture. In a slightly rich mixture, there will be a maximum ratio of dissociated products to mixture, and so further dissociation will not be favored by the mixture, and there will ultimately be more methane to oxidize.

TL;DR: More methane = less dissociation per mole of O2, meaning that in practice this will basically be the ""true"" stoichiometrically balanced system.",null,0,cdo4cjd,1rjjez,askscience,new,1
RelativisticMechanic,"To say that a differential equation is linear means that the differential operator defined by the (homogeneous part of) the differential equation is linear in the sense of linear algebra. This is also the sense in which the Laplace transform is linear. Loosely, ""being linear"" is the statement that you can ""split it across sums"" and ""pull out constants"", as they say.

More formally, given a vector space, V an operator L on that space is said to be linear if it satisfies two conditions:

1. It is additive. That is, L(v + w) = L(v) + L(w) for all vectors v and w;
2. It is homogeneous (of degree 1). That is, L(cv) = cL(v) for all scalars c and vectors v.

Note that under this definition, a function f(x) = mx + b will be linear if an only if b = 0 (the case for nonzero b is called ""affine"").

Now, let's look at your differential equations. The ""vectors"" here are twice-differentiable functions (assuming we're talking about a second order equation) and the ""linear operator"" is the differentiation operator. For example, consider the equation

y'' + 3y = 0.

If you define the operator L(y) = y'' + 3y, then this says

L(y) = 0.

Note that if we compute L(y + v) for two functions y and v, we get

L(y + v) = (y + v)'' + 3(y + v) = y'' + 3y + v'' + 3v = L(y) + L(v),

and if we compute L(cy) for some number c and function y, we get

L(cy) = (cy)'' + 3(cy) = c(y'' + 3y) = cL(y).

Thus, L is a linear operator on the space of twice-differentiable functions.

On the other hand, consider the differential equation

y'' + y^(2) = 0.

The differential operator here is L(y) = y'' + y^(2). What happens if we consider L(cy)? We get

L(cy) = (cy)'' + (cy)^2 = cy'' + c^(2)y^(2) = c(y'' + cy^(2)).

Note that this does *not* satisfy L(cy) = cL(y), so we conclude that this is *not* a linear operator.",null,1,cdo1x7y,1rjjua,askscience,new,9
MCMXCII,"Linear in the context of differential equations means that the variable of interest and all of its derivatives appear to the first (or zeroth) power in the equation. If you notice in the equation y = mx + b, the terms on the RHS of the equation contain x to the first and zeroth power respectively. So you can see why the term ""linear"" applies in both situations.

But what's so special about linear differential equations? For starters linear systems can't exhibit chaos. Chaos arises from systems described by nonlinear differential equations (although not all nonlinear differential equations exhibit chaos). Also linear differential equations obey the principle of superposition. That means if f(x) and g(x) are both solutions to a linear differential equation, any linear combination of f and g is also a solution. So f(x) + g(x) is a solution as well. Finally, and possibly most importantly, linear differential equations are relatively easy to solve. There are all kinds of techniques for solving linear differential equations. Nonlinear differential equations are in general much harder to solve.",null,3,cdo1udy,1rjjua,askscience,new,6
Dinstruction,"Linear is a term used to describe functions that satisfy f(x * y) = f(x) * f(y). Note that x and y could represent not only numbers, but functions as well. The operation * could also represent different binary operations such as addition, multiplication, or things like matrix addition, etc. Think of integration, differentiation, and the Laplace transform as functions on functions.

These functions behave rather nicely and show up often. For example, differentiation is linear. This is because d/dx (f(x) + g(x)) = d/dx f(x) + d/dx g(x). The Laplace transform is linear because L(f(x) + g(x)) = L(f(x)) + L(g(x)). You probably saw a rule like that when they were first introduced. Similar rules for scalar multiplication hold. i.e. If k is a constant, then L(kf(x)) = k L(f(x)).

Oddly enough, the familiar function f(x) = mx + b is only linear (as described above) when b = 0. When b = 0, and u and v are arbitrary real numbers, we have that f(u + v) = m(u + v) = mu + mv = f(u) + f(v). The function f(x) = 2x + 1 is not linear. As a counterexample, f(2+3) = f(5) = 11, yet f(2) + f(3) = 5 + 7 = 12. 

In more abstract mathematics, this concept is known as homomorphism. Such functions are said to ""preserve operations.""",null,1,cdo1v06,1rjjua,askscience,new,3
LoyalSol,"Since others have already defined what being linear is defined as,  I'll add on why we care.   The major reason it is called linear is that for equations or operators which satisfy linearity,  you can use a simple change of variables to generate a linear curve.  For instance,

     x+x^2

is one of the simplest examples.  But let's say we introduce a variable called y and let it be set to y=x^2 .   The above equation is reduced to

   x+y

So if you graphed the values of x+x^2 against x and y you will get a curve that falls on the 3D plane (3D version of a line) x+y.   Why do we care?  Well there are a host of techniques which work for linear systems and usually they are significantly easier than their non-linear counterparts. So if we can take advantage of any linearity it's a massive help.  Conversely if you had an equation like

   f(x)=exp(x+x^2 ) 

Even with a change of variables there no way to reform this equation to make a line while keeping the two variables separate.  Because even if you define y = exp(x) and z=exp(x^2 ) you will simply get

   f(y,z)=yz

Which is still not linear unless you plot with respect to yz.  But this may not actually be useful. ",null,0,cdo77yo,1rjjua,askscience,new,2
ACStellar,"I'm sure someone more qualified will come around and answer this but since nobody has at this time I'll give you the simple explanation.

It basically comes down to angular momentum. When the solar system first formed from a collapsing cloud of gas and dust it started spinning as material was pulled toward the center. As such the spinning caused the materials to flatten into a disk orbiting the star. It was from this disk that all the large planetary bodies formed, as such they're all generally contained within the same plane as a consequence of the nature of their formation. 

As far as I know the galaxy is similar. The rotation causes the galaxy to flatten into a roughly disk shape just as it did in the solar system.",null,0,cdnxthr,1rjk9l,askscience,new,2
Das_Mime,"See the FAQ

http://www.reddit.com/r/askscience/wiki/astronomy

They formed out of a rotating gas cloud, and when such an object collapses, it forms a rotating disk.",null,0,cdny2av,1rjk9l,askscience,new,2
chocapix,"No. They would hit the ground almost as hard.

This is because right before it hits the ground, the car isn't stationary.
Let's say you can get to about 10mph upward when jumping.
If you jump off a car that's falling towards the earth at a 100mph, you're now falling at 90mph.

",null,0,cdo3fle,1rjlcq,askscience,new,6
stephenhauskins,"Both the car and the person are accelerating in free fall.  Jumping off might gain you a very little change in downward acceleration but it would be insignificant.

All objects fall at the same rate in a gravitational field",null,0,cdobnp5,1rjlcq,askscience,new,1
DangerOnion,"Nope.  It's the same as the urban myth about jumping in a falling elevator.  Though your speed relative to the car is changing, your speed relative to the ground is still very high, and in a bad way.  As chocapix mentioned, unless you can take almost all of the speed of your fall off by jumping, you're going to hit almost as hard.",null,0,cdockbg,1rjlcq,askscience,new,1
SpaceEnthusiast,"There are two things we need to decide on before we can answer this question. First, what are we looking at the stars with? To make things easier, let's just consider stars we can view with the naked eye.Typical human eyes can see stars as dim as magnitude 6.5 – 7 ([Apparent Magnitude](http://en.wikipedia.org/wiki/Apparent_magnitude)). You are in luck because there is the [Yale Bright Star Catalog](http://en.wikipedia.org/wiki/Bright_Star_Catalogue) which contains the information on the 9110 objects of stellar magnitude 6.5 or higher. Now, 9096 of these objects are stars and I found [this spreadsheet]( http://handprint.com/ASTRO/XLSX/Yale_BSC.xlsx) (Warning: it’s an excel spreadsheet) that contains all of them. 

Second, we need to decide on the locations we are viewing the stars from. If you are at the equator you can see all the stars but not all of them at the same time (due to Earth's rotation). It'll be most useful to consider the [celestial sphere](http://en.wikipedia.org/wiki/Celestial_sphere). From the page ""the celestial sphere is an imaginary sphere of arbitrarily large radius, concentric with Earth. All objects in the observer's sky can be thought of as projected upon the inside surface of the celestial sphere, as if it were the underside of a dome or a hemispherical screen"". This makes things easy for us. The stars in the North are the ones that are on the Northern celestial hemisphere. The ones in the South are in the Southern celestial hemisphere. This is good because this information is contained in the spreadsheet I found. The parameter we need is the stars' [declination](http://en.wikipedia.org/wiki/Declination). A positive number indicates north and vice versa.

If we sort the numbers and just count the entries with plus or with a minus we get that there are 4428 stars with positive declination and 4668 stars with negative declination. That is, there are slightly more stars visible in the southern sky than in the northern sky (51.3% of the catalog’s stars have negative declination). So you can see a bit more stars in the southern sky.

We can also take a look at the [50 brightest stars](http://astropixels.com/stars/brightstars.html) and count that there are 22 stars with positive declination and 28 with negative declination. The ratio here is 56% of the 50 brightest stars are in the southern hemisphere. That’s not all though. The 3 brightest stars in the night sky all have negative declination.

Conclusion: The southern night sky wins by a small margin.",null,0,cdo3kku,1rjlhn,askscience,new,4
do_od,"Much of our understanding of the universe rests on the assumption that it is homogenous on large scales, meaning that it looks pretty much the same in all directions. However the [center of the Milky Way](http://en.wikipedia.org/wiki/Sagittarius_A*) is located at a declination of -30^o which means that we have a better view of the Milky Way from the southern hemisphere. ",null,1,cdo3ifu,1rjlhn,askscience,new,3
baloo_the_bear,"The damage to the retina caused by the sources you mention have to do with the **intensity** of the incoming light. The eye mechanism works to focus the incoming image on a specific area of the retina called the fovea, and if the intensity of the light is very high, it can cause damage. The best analogy I can give is that it's like using a magnifying glass to light something on fire: by focusing the sunlight down to a point, the light is intense enough to cause combustion. Now imaging that light being focused on the back of your eye. ",null,0,cdo57m1,1rjlv4,askscience,new,1
Platypuskeeper,"You're dealing with the equilibrium H2O &lt;--&gt; H^+ + OH^- so in pure water, the concentrations of H^+ and OH^- will always be the same. pH and pOH are the negative logarithms of the concentrations (actually activities but you don't need to know that yet), so they're equal too. 

Ka is the product of the two concentrations Ka = [H^(+)][OH^(-)], and pKw is the negative logarithm of that, so pKa = -log10([H^(+)][OH^(-)]) = -log10([H^(+)]) - log10([OH^(-)]) = pH + pOH.

pKw decreases with temperature (at least up to the boiling point at atmospheric pressure), meaning _more_ water molecules are breaking apart into ions. (just as low pH = more H^+ , remember it's the negative logarithm) 

So the pH decreases with temperature, but _so does the pOH_. The solution does not become more acidic or basic, rather it's the neutral point that's shifted.  
",null,0,cdo1gwl,1rjmlf,askscience,new,3
afranius,"A pixel is not a little square: http://www.cs.princeton.edu/courses/archive/spr06/cos426/papers/smith95b.pdf

That paper might be a little dense, so a simpler explanation is this: a real image is continuous, it is not composed of little boxes. But computers and digital TVs can't work with continuous signals very easily, so the continuous true image is discretized into samples. These samples are called pixels. For a (gross) oversimplification, you can think of these samples as ""probes"" that test the color of the underlying image at evenly spaced locations (in reality, they actually average the color of the image over a small area, but let's not worry about that for now).

Now the job of a TV or monitor is to take these samples (pixels) and reconstruct something that looks as close as possible to the original image. What mechanism the TV or monitor uses depends on the type: a CRT will excite a phosphorus coating by means of an electron gun, an LCD will change the opaqueness of the liquid crystal layer, etc. Some LCDs indeed have square or rectangular regions that change opacity to match the corresponding pixel, but their arrangement need not line up perfectly. So long as the final image faithfully reproduces the original samples.

For some images to illustrate what I mean, start on p19 of this presentation: https://graphics.stanford.edu/wikis/cs148-11-summer/FrontPage?action=AttachFile&amp;do=get&amp;target=148-1.pdf

In answer to your original question, 1080 just refers to the format of the input image that the TV can handle. How it reconstructs an image with 1080 samples on a side is up to the TV, and the number of physical pixels need not be 1080 (and they need not be square!).",null,0,cdo29o8,1rjplu,askscience,new,3
king_of_the_universe,"I don't understand the question.

    ...................
    ....#######........
    ....#     #........
    ....#     #........
    ....#######........
    ...................

The object is a square even though the aspect ratio of the screen is not square. And if you change the magnification in your browser (akin to changing the font size), you get a different screen size with an identical amount of pixels. (So yes, if a screen is bigger and has the same amount of pixels, the pixels are bigger.)",null,0,cdo47nd,1rjplu,askscience,new,2
Larsor,"A black hole is created in a supernova. A star that has a mass of about 20 times the mass of our sun can create a black hole when it ""dies"". Supernovas are really big explosions that occur when an star has run out of ""fuel"". The gravitational force of the star is creating an inward pressure. This pressure is canceled out by the energy released in the star. When the energy release is stopped the gravitational force wins and the star collapses.",null,0,cdo352o,1rjpt6,askscience,new,2
cowboysauce,"&gt; Do we have any evidence for a ""super galaxy""

There are numerous examples of galaxies merging together.

* [Anetnnae galaxies](http://en.wikipedia.org/wiki/Antennae_Galaxies)

* [Mice galaxies](http://en.wikipedia.org/wiki/Mice_Galaxies)

* [NGC 7318](http://en.wikipedia.org/wiki/NGC_7318)

* [NGC 2207](http://en.wikipedia.org/wiki/NGC_2207_and_IC_2163)

* [NGC 520](http://en.wikipedia.org/wiki/NGC_520)

Hell, the milky way is currently in the process of merging with at least one galaxy.

&gt;and what creates a black hole?

There's no consensus on how supermassive black holes form, but stellar mass black holes are formed by the death of large stars.",null,0,cdo32sp,1rjpt6,askscience,new,1
Philosophisation,No. Light is simultaneously a wave and a ray. It goes in one direction until absorbed by an electron if it is the right wavelength i.e. energy. It can also be re-emitted if the electron falls to a lower quantum state (energy level). The amount of light in one direction is not affected by any factor other than the amount emitted in that direction and whether it is absorbed. Advanced: Light will not be emitted if a ray is diametrically opposed to it. Rather the enrgy will be released as heat at the source. Other conditions prohibiting the normal release of light in a straight line: A singularity blocking its path or bending it. If an electron has no spin is spin neutral such as if it hawking radiation it will not absorb or emit light. Shining light on said electron will add energy until it disintegrates into multiple decay products including new photons. These photons will be released not upon wavelengths meaning they will not be interfereable with until they are normalized by absorption into another electron.,null,0,cdo2yav,1rjqg5,askscience,new,1
Osymandius,"If you're aware of how flu nomenclature works then excuse the patronising explanation. This is something I wrote for a post a while back:

&gt;Let's take flu as it's a virus we know a lot, if not the most, about. The influenza virus enters your cells by binding to sialic acid on your cell membrane. This is enabled by haemagglutinin, a surface protein on the viral capsid. Upon replication, the virions then need to get out your cell, and employ a sialic acid cleaver, neuraminidase. This is where the HXNX notation comes from for flu (H1N1, H3N1 and so on).

&gt;Small mutations (i.e. single amino acid substitutions) in the sequence is what is known as drift - this occurs continually - but the viral phenotype is unchanged. Large charges are rarer and are denoted by a change in nomenclature - H1N1 shifts to H9N1 for example. It is these large shifts that represent a sudden drop in immunity - and occasionally the beginning of a pandemic: Shifting to H1N1 brought about the Spanish Flu and the 08/09 pandemic.

&gt;Drift makes existing vaccines weaker (hence the annual requirement of flu vaccines compared to others lasting a lifetime) and memory T cells less effective as the previously stimulated and clonally expanded naive cells are no longer as specific for the correct epitope. However, it is no where near as drastic as shift which can render any previous cellular immunity totally useless.

So here we're looking at Influenza A and Influenza B - Influenza A is more virulent and seems to shift and drift faster than B. For any given year we normally get 2 strains of A and 1 strain of B knocking about - at any rate that's what the WHO recommend we vaccinate for!

The names are just the site of first identification - normally somewhere around the Pacific Rim - Japan, Malaysia, Australia, West Coast US. Lineage not determined is sort of what you've said, it could be either Yamagata, or Victoria, or any of the other classification sites. For example, in 2006 we were warned about:

A/New Caledonia/20/99 (H1N1)-like virus

A/Wisconsin/67/2005 (H3N2)-like virus (A/Wisconsin/67/2005 and A/Hiroshima/52/2005 strains)

B/Malaysia/2506/2004-like virus from B/Malaysia/2506/2004 and B/Ohio/1/2005 strains which are of B/Victoria/2/87 lineage

Influenza type/Origin/Strain type/Year of isolation/HXNX typing followed by any lineage data.

Does that help?",null,0,cdo4jbp,1rjqqf,askscience,new,1
Platypuskeeper,"Because it's in the next period, silicon has larger and more diffuse valence orbitals. The overlap between its p orbitals and those of the oxygen atom is less than in carbon, and so it forms a weaker pi (double) bond. The lower energy state for it then is to form four single bonds to oxygen atoms, and create this network rather than have double bonds to two oxygen atoms as in CO2. 

",null,1,cdo1mof,1rjryk,askscience,new,4
cowboysauce,"&gt;What kind of radiation is emitted?

Beta and gamma.

&gt;Why does it destroy the thyroid, but doesn't harm much else?

The thyroid actively takes up iodine, as it's required to synthesis thyroid hormones, removing it from general circulation and preventing too much damage to other parts of the body.

&gt;How exactly does radiation kill cells?

Ultimately, radiation damages DNA and if DNA is damaged too severely, cells cannot preform their necessary functions and die. ",null,0,cdo2w34,1rjsmy,askscience,new,4
duckdoodoo,"So, the important thing to know here is that the thyroid gland produces hormones which regulate metabolism in the body. The vital thing about these hormones is that they contain lots of iodine. When we take iodine up into the body, it concentrates in the thyroid gland because this is where the majority of iodine is used in the body.

Radioactive iodine will concentrate in the thyroid gland but will not concentrate anywhere else, sparing the rest of the body from damage.

Once the radioactive iodine concentrates in the thyroid gland, it releases beta radiation within the gland which attacks cells within a small radius. Only cells within the thyroid gland are killed because beta radiation cannot penetrate through our tissues well enough to damage any neighbouring structures. It also releases some gamma radiation too.

Radiation kills cells by damaging the DNA molecules. If enough of the DNA is damaged, the cell will no longer be able to function and will die.

I hope this helps!",null,0,cdox42g,1rjsmy,askscience,new,1
E13ven,"That all pretty much has to do with skull characteristics and things like that. We really can't know *exactly* how things sounded, but looking back through evolutionary history and comparing fossil samples of the past with current skulls and things that we have for extant species of the same lineage we can make a good educated guess on what kind of sounds things would have made. ",null,0,cdo1bir,1rjtgm,askscience,new,3
atomfullerene,"Mostly we don't.  But given that crocodylians and birds both vocalize, it's quite likely that dinosaurs did as well.  But with a few exceptions based on models of hadrosaur skulls, we have no idea at all what they sounded like.  

But having no sound is almost certainly even more wrong (for some dinosaurs, anyway)  

It's like color...we don't know what color dinosaurs were, but they certainly _weren't_ all some neutral gray color.  So you kind of have to add in something to give the right impression.",null,0,cdo7lz5,1rjtgm,askscience,new,1
King_Dynamo,"A person has multiple vocal cords that can vibrate to produce sound, but they can't be independently controlled, so no, not really.  The closest a single person can get to singing a chord is two notes at once using properties of overtones and mouth resonance to throat sing.",null,0,cdo1lc3,1rjub7,askscience,new,3
synchrony_in_entropy,"In the end, all memories are just representations that our brains make to learn about the world and every time you think about a memory you make it malleable again. This malleability is a large part of why repressed memories are so controversial and mostly considered to be pseudo-science. This malleability has been shown in a number of studies. Rat research has shown that you can actually erase memories if you inject the right compound when somebody is reflecting on a memory, while human researchers have shown that you can make people change their memories by describing elements of the memory that were never actually initially encoded. So, essentially, repressed memories are never pure and are probably not trustworthy.",null,0,cdo2rzk,1rjuzo,askscience,new,13
cowboysauce,"At perigee, the moon is ~ 363,000 km away, amateur telescopes have a a resolving power of about 0.5 arc seconds. Assuming that the telescope is perfectly calibrated and when the moon is closest to Earth, you could see details as small as 2 km.",null,0,cdo35om,1rjw6z,askscience,new,6
king_of_the_universe,"I can't answer that, just want to give this impression of the Moon's real distance - it's often imagined that the Moon circles the Earth at a distance of 5 Earth diameters or something like that, but the truth is much different:

http://upload.wikimedia.org/wikipedia/en/archive/7/73/20070429012601!Distance_From_Earth_to_Moon_In_Light_Seconds.gif",null,1,cdo2zfj,1rjw6z,askscience,new,5
null,null,null,95,cdoedmo,1rjwyn,askscience,new,376
GritsVids,"Not really. Most of the defining characteristics of a face are bone and cartilage. As a human, you're wired to notice mostly the eyes, nose shape, and mouth. These are the key areas that don't change, and why you're able to still recognize someone who's lost a lot of weight, or aged. 

From a silhouette perspective, fat is the only real augmenter, and even then it takes a significant change before morphing someone's features. As we age, those fatty deposits sag, adding to a look of jowls, or baggy eyes. But even then, since most of the key features are reliant on bony structures,  you are still able to recognize the person.

Purpose: Facial muscles aren't really designed to be ""load bearing"" aside from 2: The masseter and the temporalis. Both are used to help close the jaw, and both are laid very low and ""inside"" against the skull, behind your cheek bones a bit. (Look at the bridge behind the cheeks on a skull) Aside from those, the rest are pretty much designed around communication, both verbal and non-verbal. They need to move quickly, and rather precisely.

And lastly... perception. Human being are hard-wired to recognize faces and what they are communicating to us. Our social development decided to go a visual, instead of a smell-based recognition. Your brain has areas just for it. Even if you were able to cause some measurable change, it might just flat out go unnoticed.


TL;DR: No, the recognizable parts of the face are bone and cartilage, and face muscles don't work that way.  

Creds: facial animator for several years. 


edit: spelling",null,54,cdodbrm,1rjwyn,askscience,new,229
ren5311,"Hello, as a friendly reminder, please keep all comments civil, on topic, scientific and free from speculation, otherwise they will be removed in line with our [community determined guidelines!](http://www.reddit.com/r/askscience/wiki/index#wiki_answering_askscience)

If you have any questions or comments on AskScience policies, please message the moderators via the link on the sidebar as all off-topic conversation in this thread will be removed.",moderator,59,cdo4bf9,1rjwyn,askscience,new,149
null,null,null,9,cdo3lo3,1rjwyn,askscience,new,67
null,null,null,21,cdo36rm,1rjwyn,askscience,new,61
SpaceEnthusiast,"Sometimes the jaw muscle grow for no known reason at all. Take a look at [this paper](http://dmfr.birjournals.org/content/36/5/296.full.pdf) and [this page](http://www.ncbi.nlm.nih.gov/pmc/articles/PMC3275871/). The condition is called Idiopathic Masseter Muscle Hypertrophy. Sometimes it happens on [both sides](http://www.msrdc.ac.in/files/Dento_Quest/02_Dr_Ravleen_Oral_Med.pdf). It seems that Botulinum toxin A [helps](http://www.sciencedirect.com/science/article/pii/S0278239104012686) in reducing the effects of this conditions


",null,14,cdo4bgh,1rjwyn,askscience,new,39
null,null,null,10,cdo6n93,1rjwyn,askscience,new,21
null,null,null,6,cdo9bbf,1rjwyn,askscience,new,17
null,null,null,3,cdohvhz,1rjwyn,askscience,new,6
prestonhh,"The answer is a simple yes, provided you are interested in the perception of said facial appearance. The tone of given musculature, irritability of fibers is determined to a significant degree by the representation within the CNS, this of course is greatly shaped by dominant motor behavior. If a person has a cheerful disposition, they are connected to and express things on their face effectively we will perceive a subjectively different appearance. The face may appear more perky.

The levers in the face and the orientation of the musculature doesn't make for the ability to apply significant resistance very easy. There are therapeutic devices and modes of e-stim to encourage effective recruitment but the usage of such devices diminishes after a certain point.

Like the top comment said the biggest contributor to appearance are the fatty deposits and the bone structure. My hero Jack Lalanne was famous for his facial exercises and he has some nice ones on youtube. They are quite invigorating and are a great idea when you first wake up.

Source: I massage people with TMJ among other things for a living",null,3,cdo6ujg,1rjwyn,askscience,new,5
null,null,null,4,cdody38,1rjwyn,askscience,new,6
TheCrazyOrange,"Nothing. Humans are just about the dumbest mammals on the face of the earth, when it comes to innate sense of direction.

The only animals springing to mind are things like the panda (dumb as a rock), sheep (also incredibly stupid), though not a mammal, the Kakapo. 

If someone says they have a good sense of direction, most likely they are just better able to visualize the route they took from a known location, and translate that into terms of their current location.


But if you drop a human and a panda in a maze, knowing only that the exit is to the north, the human will begin just as lost as the panda.",null,2,cdo16xv,1rjwyp,askscience,new,3
ITRAINEDYOURMONKEY,"I've heard that some researchers have found a few people (I think in some isolated tribe?) who do in fact have neurons that fire when the person is facing north.

This of course doesn't mean that they have a biological direction sensor. What's more likely is they've trained themselves so heavily to navigate, that part of their brain (conscious or subconscious) always keeps track of the direction they're facing, re-calibrating when it sees a reference point. A number of animals are thought to do this kind of positioning (though they'll keep track of both position and direction) where they estimate current position by integrating the movement from the last reference point. People who have a good sense of direction are usually good at using landmarks and their own previous movement to infer their current direction, but it's not an innate sense in the same way as touch/smell/etc.

If you want to see some impressive stuff, look up research on ants' navigation systems. It's likely they have very finely tuned pedometers and directional sense, though it's not settled what all the mechanisms at work are or how big of a role they play. Some of it is thought to be visual flow, and some is based on using the polarization of sunlight as a compass of sorts (they can see the direction in which light is polarized). They're very good at doing a long, wandering search for food, and then returning to the nest in a straight line after they find the food source.

I don't know how many different animals this applies to, but at least bats and rats can build a map of a region they've explored. After they've acclimated to an enclosed environment, they'll have specific neurons that will fire when they're in certain locations within their enclosure (look up ""place cells""). Not sure, but I believe they also have neurons firing based on the direction they're facing relative to the enclosure.",null,0,cdp0l55,1rjwyp,askscience,new,1
fartprince,"From Wikipedia:
""However, most spiders that lurk on flowers, webs, and other fixed locations waiting for prey tend to have very poor eyesight; instead they possess an extreme sensitivity to vibrations, which aids in prey capture. Vibration sensitive spiders can sense vibrations from such various mediums as the water surface, the soil or their silk threads. Changes in the air pressure can also be detected in search of prey.""
http://en.wikipedia.org/wiki/Spider_anatomy#Eyes.2C_vision.2C_and_sense_organs

They ""hear"" through their legs by the way. What we think of as the sense of hearing is basically a measurement in air pressure fluctuations. Spiders have sensory systems in place on their legs that detect similar vibrations in the air that, say, perhaps a moth might make while flapping its wings.

If you want further, non-wikipedia reading, here you go:
http://www.european-arachnology.org/proceedings/11th/010_Barth.pdf
",null,0,cdo8i4r,1rjxui,askscience,new,2
uncleawesome,Doubtful they sense them. They just make webs where they are luckier at finding bugs. You don't see webs where no bugs are because the spiders moved to a different location or died from starvation.  They move around until they find a spot that is satisfactory.,null,4,cdo2990,1rjxui,askscience,new,1
EvergreenStateofMind,"I don't believe so. The smells coming from your clothing may be caused by your normal microbial flora found on your skin. Although most bacteria are anaerobic, the ones flourishing on your skin are aerobic. This would mean that the oxygen present in the atmosphere would not kill these bacteria. I assume that once the nutrients that are on your soiled clothing are depleted the remnants of the bacteria could still cause your clothes to be dirty. I am but a measly microm student so please correct me if i am wrong.",null,0,cdo34yw,1rjz4b,askscience,new,5
FatSquirrels,"This very much depends on the type of soil you are talking about and where you hang it up.

If you are trying to remove stains or grime from your clothes then hanging them up won't do much.  Hanging the clothes in the sun and wind will help break up some of the chemicals that make up this stain/grime, but they are likely oils and other large molecules that aren't going to evaporate and won't break down enough to do any good in UV light.

If you have stuff growing on your clothes then hanging them in the sun could help.  UV radiation is pretty good at killing small organisms, especially once you have dried out their living environment.  It might not get rid of the oils or whatever they were feeding on, but it could kill the bacteria and temporarily remove the odors they were producing.",null,0,cdobvoh,1rjz4b,askscience,new,1
sharp12180,"Buoyancy is independent of the Earth's rotation. Gravity, however, is necessary for the buoyant force to exist. The buoyant force arises from the fact that, in a fluid, pressure increases as depth increases and this pressure exerts a force in all directions. This means that if an object were submerged there would be more pressure on the bottom of the object than at the top which causes the object to accelerate upward. If there were no gravity then there would be no pressure in the fluid and thus no buoyant force. This link shows a pretty clear diagram.
http://hyperphysics.phy-astr.gsu.edu/hbase/pbuoy.html",null,0,cdo19v6,1rk09y,askscience,new,2
chrisbaird,"The problem with time travel to the past is that it violates conservation of energy/mass. Say you time travel back three days and appear in your room. In the local reference frame, you were not there one second and then you are there the next second. Mass/energy has therefore been created out of nothing, which is not allowed. You hang out for three days and then time travel back again alongside your original self. You can see where this leads: you end up with an infinite number of you appearing out of no where, which is nonsense. This is not a psychological effect. Send back in time a rock and conservation of mass is still broken, and a runaway infinity is still possible. ",null,1,cdocv11,1rk11y,askscience,new,4
tagaragawa,"I recommend Sean Carroll's [From Eternity to Here](http://preposterousuniverse.com/eternitytohere/). He has a whole chapter on whether and how time travel would fit into our current understanding of physics. Basic outcome: probably not. Also: paradoxes do not happen, any model that would lead to a paradox is necessarily false.",null,0,cdomnzj,1rk11y,askscience,new,1
PHYS101,"As far as I know, General Relativity doesn't forbid travelling backwards in time, I am yet to take a course on it besides some reading in my own time.  

However, there is a problem with causality. If you go back in time, you can cause something to happen from before you were born. And then you can run into paradoxes. I have heard a quote that a paradox just means that your model is wrong, nature doesn't allow paradoxes.

My answer's vague and I should feel bad.",null,9,cdo44cu,1rk11y,askscience,new,5
sharp12180,"There are reasons to believe that if an object were to travel faster than the speed of light it would then travel back in time, but as far as we know objects cannot travel faster than the speed of light. However, it has been shown (by Richard Feynman) that antiparticles are regular particles traveling backwards in time. Cool Stuff",null,12,cdo1bh0,1rk11y,askscience,new,2
professor__doom,"It is called the Valsalva maneuver.  It does have effects on blood pressure and heart rate: http://en.wikipedia.org/wiki/Valsalva_maneuver

When my dad was in the Navy ROTC, he was told to perform it as part of the process of abandoning ship to both prepare for the rigors of the jump and prevent ear injury through equalization of pressure.",null,4,cdo1fiv,1rk2i6,askscience,new,27
lennardi,"The main action of holding your breath (after taking in a deep breath) is that it increases intra-abdominal pressure. This is important, especially when lifting, because it is harder to bend your abdomen forward when you have a full breath. It's the same reason that many weightlifters use a weightlifting belt - helps to keep your abdomen straight and can prevent injuries such as hernia which usually arise when you strain with a bent back.

The reason it increases blood pressure is because your torso is filled up with air, then your muscles contract, similar to squeezing a bottle of soda. Obviously, the more air the bottle has in it, the easier it is to squeeze. If the bottle is full of only soda (liquid, like most organs), then it is very hard to squeeze.
",null,5,cdo2sgu,1rk2i6,askscience,new,20
steamtrol1er69,"We hold our breath while straining/struggerling while we do complete an obsidious task due to our nervous system connecting and controlling the influences of our raspiratory system to incerase the output of our strength influxes.

source: studying PhD in Bio-Medical Sciences",null,7,cdo6tjw,1rk2i6,askscience,new,2
null,null,null,10,cdo1xwv,1rk2i6,askscience,new,1
wazoheat,"The only thing that governs whether something floats or sinks in something else is the density. Almost all substances, liquid water included, have a positive [coefficient of thermal expansion](http://www.engineeringtoolbox.com/linear-expansion-coefficients-d_95.html), meaning as they warm up they get less dense, and vice versa. Warm water ""floats"" above cold water because the denser cold water pushes the warm water up.

Ice is a special case, however. During the phase transition from liquid to solid water, individual water molecules arrange themselves [in a hexagonal lattice](http://media.web.britannica.com/eb-media/90/62690-004-1FB5CC40.gif); an arrangement which is *less dense* than liquid water. Because ice is less dense than liquid water, it will float on it.",null,2,cdo30ss,1rk606,askscience,new,10
whatthefat,"Both are possible, and it's not entirely clear what causes one to happen rather than the other.

By ""sleep cycle"", I'm assuming you mean the cycle between rapid eye movement (REM) and non-REM (NREM) sleep that occurs during sleep. This cycle tends to occur with a period of ~90 minutes, although the period shortens slightly across the night, and the amount of REM sleep in each cycle increases across the night. However, the [textbook image](http://upload.wikimedia.org/wikipedia/commons/3/3e/Sleep_Hypnogram.svg) of a night of sleep is a gross simplification. In reality, the cycles are often fragmented, highly variable in period, and often difficult to even discern. It's not possible to predict when a cycle will start or end to a high level of accuracy.

A long awakening can restart the cycle, whereas a very short awakening generally does not. Most people briefly awaken ~20 times per night and then fall back to sleep. These brief awakenings seem not to have much effect on the overall structure of sleep. Many of these awakenings also naturally occur near the end of a NREM/REM sleep cycle.

For longer awakenings, things are more interesting. In [this study](http://www.sciencedirect.com/science/article/pii/0013469489901892), people were awoken 20 minutes into a NREM sleep period, and then kept awake for 10-90 minutes. When they went back to sleep, one of two things generally happened. Either they started a new cycle and went into REM sleep about an hour after they fell back to sleep, or they finished their cycle and went into REM sleep fairly quickly (within about 25 minutes). The latter outcome occurred about one third of the time, but it wasn't possible to predict which outcome would occur. Curiously, no relationship was detected between the outcome and the length of the sleep interruption.",null,0,cdodwpo,1rk6qj,askscience,new,4
DangerOnion,"Essentially, yes.  The reason your foot doesn't go through the ground is because of the electromagnetic interactions between matter.  The fact that they can't collide means that the kinetic energy of your shoe is transferred.  Some goes into the ground as heat, some goes back into your foot as heat and as reverberations in your muscles, and some goes into the air.  When that energy is applied to the air, it creates a compression wave in the form of sound.

TL;DR — You're not literally hearing the sounds of electrons colliding, but the interactions between them are somewhat responsible for the sound.",null,1,cdoch0t,1rk7f0,askscience,new,4
shavera,"Let's say a station is broadcasting at a full Megawatt of power (the strongest, to my knowledge are usually in the half-megawatt range). Let's say they have a dish (or array of dishes with equivalent collection area equal to) of *d* meters in radius. Their collection area is pi *d*^2 , but our signal is spread out over the sphere of 4.24 light years, that's about 4 x 10^16 meters. And the surface area of a sphere is given by 4 pi r^2 , so 6.4 x 10^33 pi meters. So the signal strength is approximately their collection area divided by the surface area of the sphere, so about d^2 / 6.4 x 10^33 . 

So there's a factor of 1.5625 x 10^-34 reduction in signal strength. A megawatt is 10^6 , so that leaves us 10^-28 . d^2 / 10000 Yoctowatts of power (we don't even have an SI prefix small enough to properly label it). 

So now, looking at wiki's orders of magnitudes of power, the Galileo satellite was giving a 70 m pickup about 10^-21 (zeptowatt) of power. So let's say that the alien SETI program needs to pick up .1 zeptowatt (to make the math easy). So it needs 10^3 yoctowatts, so d^2 = 10^8 , so d = 10^4 , they'd need something like a 10 kilometer dish to pick up our signal. Given all the above approximations, their dish maybe needs to be between a kilometer and 100 kilometers, just to be safe. 

So not *impossible* but seems like just barely possible. Certainly more distant objects are going to have even more trouble (remember again that signal decreases with square of distance)",null,2,cdo7e79,1rk7o8,askscience,new,19
Mortis7432,"Yep (http://news.bbc.co.uk/1/hi/7544915.stm)

Here is a chart to see what they would be watching. 

http://www.chartgeek.com/tv-signals-in-space/",null,2,cdo63pl,1rk7o8,askscience,new,4
TildeAleph,"Not an expert, and I think I might be being a bit too speculative, but here goes: Yes, you *could* detect radio signals, but not the average random stuff we produce on a day to day basis. Alpha Centauri is ~[4.3 light years away](http://en.wikipedia.org/wiki/Alpha_Centauri), so they would be listening to what we were broadcasting in 2009. Radio communication has only gotten more efficient over the decades, with less and less signal escaping into space. If someone near AC had something like the [Very Large Array](http://www.vla.nrao.edu/) and pointed it at earth they probably [wouldn't](http://zidbits.com/2011/07/how-far-have-radio-signals-traveled-from-earth/) be able to pick anything out of the background radiation. *But*, If we we tried broadcasting a message using something like the [Arecibo Radio Telescope](http://en.wikipedia.org/wiki/Arecibo_Observatory#The_Arecibo_message), and they were using their version of the Very Large Array to listen, then they could easily read us five-by-five.",null,1,cdo72go,1rk7o8,askscience,new,2
pspinler,"According to the SETI institute's FAQ on this question, most of our radio signals are too weak to be detected on interstellar distances, but there are some exceptions.  In particular, some military radars, and some deliberate broadcasts.  Here: http://www.seti.org/faq#obs12

",null,0,cdogc33,1rk7o8,askscience,new,2
null,null,null,3,cdo6n4a,1rk7o8,askscience,new,1
AK-Arby,"Because radio signals travel at the speed of light, in 4.367 years.

The only variable would be the transmission power, but the answer is almost assuredly yes.",null,5,cdo4w9l,1rk7o8,askscience,new,2
albasri,"You may be interested in my comment to a similar question:
http://www.reddit.com/r/askscience/comments/1phyv1/has_science_determined_the_most_effective_way_to/",null,0,cdobkit,1rk829,askscience,new,3
king_of_the_universe,"Follow-up question - Has the following hypothesis been scientifically verified/falsified?:

""If a person *wants* to learn a certain fact (e.g. 'Hm. Let me just Google that.'), the data is more likely to be stored in the person's head.""",null,0,cdov4l8,1rk829,askscience,new,1
__Pers,"I'm not sure what your specific question is, to be honest. The following is from Glasstone's book *The Effects of Nuclear Weapons* (my copy is the 1962 version). 

We have data on what would happen from an underwater burst from the Bikini BAKER event. As in air, a blast wave is created at the point of detonation, though the decrease in overpressure doesn't fall off as fast as in air. Also, because the burst is in water, the overpressure is much greater--peak overpressure at 3k feet from a 100 kt explosion is 2700 psi, compared with only a few psi from a surface or atmospheric burst. Shock waves, if the detonation is close enough to a surface (like a sea floor or the interface between sea and air), can reflect from the surface and surface waves can travel along the surface. Ships near the blast site in Bikini BAKER had superstructures damaged by these surface waves. Bikini BAKER excavated a crater about 2k feet across and (at maximum) about 30 feet deep on the bottom of the sea floor. 

An underwater blast releases a large bubble of superheated gas. In Bikini BAKER, this release led to the production of a large (100 ft. tall) solitary wave on the surface. Several smaller, subsequent waves were observed from the sloshing around as the void filled with water.",null,0,cdo5mko,1rk94s,askscience,new,3
c_programmer,"What you're talking about is similar to symbolic linking which is extensively done in both databases and file systems. The bottom line is that the whole data block has to be stored somewhere so you aren't compressing anything. You can arbitarially decide that the byte 01011001 is a 20gb file but that is worthless unless you always have that 20gb file on hand somewhere. In database systems you'll store the big file on some massive file system and store a link taking up a few hundred bytes to point to the actual file. 

Actual compression can be done though things like [Huffman Encoding](http://en.wikipedia.org/wiki/Huffman_coding) but it won't always work well. The bottom line is that there isn't always a way to reduce the amount of information used without losing some of it. ",null,0,cdofdgo,1rk9xa,askscience,new,4
lithiumdeuteride,"Since the data you receive is unspecified and could be absolutely anything (or it could be encrypted and thus indistinguishable from random data), every possible chunk could be encountered.

Suppose your chunks are 64 bits each.  The number of possible unique chunks is 2^64 = 18446744073709551616, so you need 2^64 different 'symbols' to represent all of the possible chunks.

How will these symbols be stored in the dictionary?  Well, you'd be crazy to use anything except binary data for the symbol names.  The number of bits needed to uniquely name 2^64 different symbols names is 64, so each symbol name will itself be a 64-bit number.  That means if you get a random 1024-bit file, it will take 16 symbols to encode it.  Each of those symbols itself requires 64 bits to identify, which brings the encoded file size to 1024 bits.  Note that this operation saved zero space.

Space can only be saved when you can break data into chunks without accounting for every possible chunk of that size (thus allowing the symbol names to be shorter than the data they represent).  For this to work, the data must be structured/repetitive, not random-looking.",null,0,cdolrm6,1rk9xa,askscience,new,2
MonadicTraversal,"The problem with this idea is that you'd need to decide what chunks of data you put in the database; even if you decide that 'k' expands to a PDF of War and Peace, that means that anybody whose PDF copy has a single bit change now has to store the entire text. Plus, it means that anybody who wants to expand their data has to have *the entire database*, which will be many times bigger than the data they wanted to compress in the first place.",null,0,cdo9qnk,1rk9xa,askscience,new,1
Smashfigs,"Things from cranberry juice are absorbed into the bloodstream, you just can't tell because they don't get you drunk!

Things you eat/drink enter the bloodstream through the intestinal wall. The molecules of the things you eat and drink are absorbed directly through the cells of your intestine and enter the bloodstream that way (in a process called transcytosis). Some molecules, such as complex proteins, need to be broken down a bit first as they are quite large. Ethanol is a component molecule of alcoholic drinks, and it's this that makes you drunk when it enters the bloodstream along with all the other molecules present in the drink.",null,0,cdoapz4,1rkatz,askscience,new,3
MCMXCII,"&gt;I assume it has something to do with the strong force and the structure of things in the nucleus.

Exactly. Unfortunately it's very complicated. But the nucleus (and everything in nature) wants to exist in the lowest energy state it possibly can. So for certain nuclei there exists a more energetically favorable configuration. If the nucleus can naturally reach the more stable state, it will (probably) do so. The rate at which this occurs can be expressed as a probability per unit time (the decay constant), or a time that it takes for half to decay (half life), or the time it takes for the sample to decay it 1/e times its original size (mean lifetime), among other less useful ways.",null,0,cdoairw,1rkb76,askscience,new,3
niceasimov,"yes, all the time. One cool example is the Lyme disease pathogen, *Borrelia burgdorferi*.  Bacteria in the genus *Borrelia* express a protein which elicits an antigenic response in vertebrate hosts (we mount an immune response), called [vlsE] (http://www.ncbi.nlm.nih.gov/pmc/articles/PMC165742/).  Once our immune system mounts an effective response against this antigen, the Borrelia circulating in our system undergo recombination at the vlsE locus resulting in a new antigenic ""strain"" of the bacteria which we no longer recognize, resulting in a persistent infection.  *Borrelia* have a number of unexpressed silent cassettes which recombine into the vlsE locus for just this purpose.    
You might also be interested in reading about the [Red Queen Hypothesis](http://www.sciencemag.org/content/333/6039/216.abstract) in relation to host-pathogen interactions.  ",null,0,cdo9z0k,1rkbad,askscience,new,4
stuthulhu,"No. The local cluster is gravitationally bound to the Milky Way, and other clusters of galaxies may be gravitationally bound together in a similar fashion, which is how we have events such as the likely eventual merger of the Milky Way and Andromeda. However, at larger scales the expansion of the universe dominates, and distant objects will recede from one another forever. ",null,0,cdo5fz6,1rke6g,askscience,new,2
SMURGwastaken,"Helium only floats upwards when surrounded by heavier particles; if you put a helium balloon in a container full of hydrogen, the balloon would sink to the floor because helium is heavier than hydrogen.

Consequently, in the absence of any other matter helium behaves like any other gas - i.e. it would expand to fill the container as far as was possible.",null,0,cdo51wu,1rkej7,askscience,new,8
Platypuskeeper,"Like any gas, it would expand to fill the container it's in.
",null,3,cdo4nt1,1rkej7,askscience,new,8
MustafaBei,"If the meteor is massive enough (this is also highly dependant on the meteor's elemental composition), it may hit the ground. But if it burns up like in your case, again, dependant on its elemental composition, it may totally evaporate, or pieces that are very small may still fall down scattered. Although the first and the latest rarely happen. Meteors usually totally evaporate before they can hit the earth. 

They cannot go back to space because of the earth's gravity (or whichever body's it is approaching to). In rare cases, and if it is not going directly towards the celestial body in question (i.e. it is grazing the celestial body), it may start to orbit the source of gravity.",null,1,cdo54em,1rkfch,askscience,new,3
DangerOnion,"Depends what it's made of.  Most of it will sprinkle to the ground very slowly, basically as dust.  If it has a component that's ice or frozen gases, then that would evaporate and be incorporated into the atmosphere.  There's really no way it could go back to space.",null,0,cdoc88y,1rkfch,askscience,new,2
KRoNlC,"Well actually depending on a few factors like composition and part of the atmosphere where evaporation occurs, i.e. thermosphere, some of the gas could escape back to space via photodissociation. But like I said, this would only occur with the elements light enough to freely escape (H, He). ",null,0,cdosms9,1rkfch,askscience,new,2
DrVentureWasRight,"As with everything in engineering, reliability and cost.

People are relatively cheap and replacing an injured stoker is easier than fixing a clogged hopper.  Your loading mechanism will also need to be able to withstand the heat, smoke and soot of an active boiler.

The next benefit is level loading.  For a hopper you need your fuel sufficiently above your boiler to give enough force to load into the boiler.  This will be quite tough on a train due to hight limits and potentially dangeous on a ship as you want your mass as low in the water as possible.

It is possible to use a mechanically driven loader, but that's one more thing that can break.  It also makes starting up the boiler that much harder since your machine as to be producing useful power to load in more fuel.

This systems are just expensive and complicate the design of an already complicated machine.  In the end, particularily at the time steam power was big, a person loading coal into a boiler is just cheaper than the equipment to do it automatically.

",null,1,cdoa63k,1rkfgd,askscience,new,8
apostate_of_Poincare,"**yes**

-Piano keys are weighted, and ""reset"" by gravity when you release the key, so that would be annoying (probably like having sticky keys).  

-There are some exotic water-based instruments that it would obviously affect.  Especially once the vessel started vibrating (might make some neat harmonics before the water escapes though)

---------------------
**No**

-Stringed instruments are based largely on the string's properties and perturbations on it.  

-Similarly, brass and reed instruments rely on perturbing instrument components with air, same with an accordion.

-Also similarly, drums are based on perturbing a stretched or solid material that vibrates.

---------------------

(if you have any more instruments in mind, feel free to ask).",null,0,cdoky85,1rkg9b,askscience,new,3
jakkes12,Unless you're in a sealed tank filled with air you wouldn't be able to hear what you're playing.As of the effects on the instrument while playing I'd say there is no. Perhaps trumpet since there's no air to blow,null,8,cdohmcl,1rkg9b,askscience,new,3
Catsplorer,"I don't believe we've reached a point where we can 'revive' tissue after cryopreservation. In the case of gametes and embryos, special freezing media is required to protect them from cold shock and freezing damage. Cryoprotectants such as low density lipoproteins and glycerol stabilise the plasma membrane during chilling and replace intracellular fluid to prevent ice crystals forming within the cell. As the follicular fluid does not have these properties, I doubt the oocytes would survive being frozen within the ovary itself.",null,0,cdoks61,1rkgwb,askscience,new,2
adamhstevens,"Well, for a start Venus is much closer to the Earth in size (0.95 x the radius) and Mass (0.815 x Earth mass). 

But obviously Venus still has a thick atmosphere. This is partially due to the runaway greenhouse, which has driven the carbonate-silicate cycle to an extreme, produced massive amounts of Carbon dioxide.

The other reason is that Mars' magnetosphere died a long time ago, allowing the solar wind to strip away the atmosphere.

In fact, Venus doesn't have a substantial magnetosphere either, but it's probable that it had one at some time in the past.",null,1,cdo63xe,1rkh30,askscience,new,7
DangerOnion,"They're not really of comparable size.  Mars is two-thirds the diameter and only an eighth of the mass of Venus.  Much of the Earth's atmosphere is maintained by several factors.  First, volcanoes erupt huge amounts of gas into the atmosphere, compensating for the atmosphere that we lose.  Second, we have a strong enough gravity to keep the atmosphere close, making it denser.  And third, we have a magnetosphere that diverts most of the solar wind around the atmosphere, protecting it from being torn away.

Mars has none of those things.  It cooled long ago, leaving no volcanic activity to replenish its atmosphere.  It has no global magnetic field, though it has very strong surface fields and it appears as though it might have had a global field in the past.  Regardless, what atmosphere it has is offered no protection.  And its gravity is only a third of that of Earth.

Venus, on the other hand, is large, has a sort of fake magnetic field created by the magnetic field of the Sun wrapping around it, and is hot enough to be replenishing its own gas supply.",null,0,cdobhtf,1rkh30,askscience,new,3
Javi2639,"Alkali ions like sodium and potassium serve incredibly important functions in the body. Since they are reactive,  they're usually found in salt form. As these salts are rare in nature, our bodies give us positive feedback when it is ingested, therefore it tastes good. ",null,0,cdoke1e,1rkizt,askscience,new,1
Henipah,"About 10% of the time you have a bowel movement and often when you floss your teeth you get a transient bacteraemia, i.e. bacteria circulating in your blood. Your immune system manages to clear them which is why we don't all die of sepsis. ",null,0,cdojl3m,1rkj5o,askscience,new,8
expandedthots,"The bacteria in your GI tract have become part of your normal flora, meaning they live and survive in your gut, without being attacked by your immune system, and actually help to digest food and in some ways regulate your immune system. 

So basically, your immune system doesn't fully recognize them as a threat in the same way it recognizes other bacteria, because the bacteria that make up your normal flora don't have the virulent versions of things like endotoxin and other toxins, which activate your immune system. Hence, your immune system recognizes these less threatening versions of bacteria and learn not to attack them like they would an infection. It also allows your body to recognize when something is bad more easily, because it has all this ""data"" on what isn't bad. 

Gets out of whack at times, so you see e. coli sepsis in really sick patients. 

Also, you don't get blood on toilet paper from wiping too hard. You probably have a hemorrhoid.",null,9,cdoa6e5,1rkj5o,askscience,new,12
Natolx,"&gt; Why don't all the pathogens which can cause disease get into the bloodstream and if so, why don't they effect us as much as they would elsewhere?

OP, please see my reply to one of the other posts in this thread, your question is addressed by the edit(supported by the information above)

http://www.reddit.com/r/askscience/comments/1rkj5o/why_dont_you_get_infected_when_you_wipe_too_hard/cdoq0eh


",null,0,cdoqfdz,1rkj5o,askscience,new,1
jyaron,"Your rectum, though external, is essentially meshed with the mucus membranes of your lower GI tract. As with essentially all mucus membranes in the body, the rectum is rich in immune cells representing the first line of defense against invading pathogens. In particular, the immune cells in these sorts of mucus membranes are especially good at tagging invading pathogens with Immunoglobulin A (IgA), which signal to professional phagocytes of the immune system that ""this is a bug that you should destroy."" As such, though we may bleed in the area, the bacteria that make their way into that particular wound have a hell of a time surviving for very long, as they are quickly destroyed by these ready-to-fight immune cells.

Search ""mucosal immunity"" for more info.",null,0,cdpra42,1rkj5o,askscience,new,1
albasri,"This question has been asked several times before. Here a few relevant links that you might find interesting:

http://www.reddit.com/r/askscience/comments/1hwivf/is_there_a_documented_explanation_as_to_why_the/
http://www.reddit.com/r/askscience/comments/10kocv/why_do_neurons_decussate/
http://www.reddit.com/r/askscience/comments/1ljq6x/is_there_an_adaptive_purpose_to_the_different/
http://www.reddit.com/r/askscience/comments/1c2uqz/why_do_nerves_decussate_in_the_spinal_cord/
http://www.reddit.com/r/askscience/comments/14pgpx/why_do_nerve_pathways_to_and_from_the_brain_cross/
http://www.reddit.com/r/askscience/comments/lxb2t/is_there_an_evolutionary_biological_or_other/
http://www.reddit.com/r/askscience/comments/j2i6z/why_does_our_brain_cross_nerve_signals_ie_the/
http://www.reddit.com/r/askscience/comments/gqxid/why_does_the_left_hemisphere_of_the_brain_control/

Sometimes, a lot of really interesting questions and answers get buried because of the high volume (and long history!) of /r/askscience. The search bar can be a great way to search for answers to some of your questions (although getting just the right search terms in can sometimes be a bit tricky) and explore /r/askscience in general!",null,0,cdobfym,1rkj7a,askscience,new,1
_NW_,"The products of combustion are CO2 and water.  When the exhaust pipe is cold, the water vapor is condenced into water droplets and exits as a fog.  Once the exhaust pipe gets hot enough, the water stops condencing and exits the pipe as a clear water vapor instead of a fog.  The same amount of water is being produced in both cases.  It's just the state of the water that determines if it is visible or not.",null,1,cdode72,1rkjot,askscience,new,7
Sterlz,"The 'steam' that you're talking about is a mix of water vapor and other nitrogen compounds and carbon compounds as well as mostly carbon dioxide. The reason you see more exhaust when you start up is because the catalytic converter on your exhaust system hasn't been brought up to operating temperature. When the converter reaches operating temperature it is more efficient at turning the nitrogen and carbon compounds from the cylinders into water and CO2. 
The compounds that come out of a cold exhaust are more visible in the cold air than water vapor, or CO2 (essentially invisible).",null,0,cdodffx,1rkjot,askscience,new,2
Thermodynamicist,"When you first start the engine, the exhaust system is cold, so the exhaust is colder when it first hits the atmosphere, and its velocity is less because its volume is less (PV = mRT). 

[The saturation vapour pressure of water is a strong function of temperature.](http://en.wikipedia.org/wiki/Vapour_pressure_of_water)

Therefore the combination of improved mixing due to increased exhaust gas velocity, and the increased saturation vapour pressure due to increased exhaust  gas temperature mean that once the engine warms up, the exhaust diffuses out into the atmosphere fast enough that it doesn't ever intercept the saturation line, and no condensation occurs. ",null,0,cdp6l0v,1rkjot,askscience,new,2
flamoutan,"Yes. [According to their website](http://tellspec.com/howitworks/), TellSpec is in essence just a handheld Raman spectrometer. [Raman spectroscopy](http://en.wikipedia.org/wiki/Raman_spectroscopy) can be used to identify different molecules based on their unique vibrational spectra (thus allowing for identification of various ingredients by matching patterns with their online database). Various types of handheld spectrometers already exist on the market (you can see this through a quick search), so this is certainly technologically feasible.",null,0,cdoftym,1rkk8j,askscience,new,2
Jyesss,"rhEPO does not have a different number of amino acids than does human EPO. rhEPO is made by transfecting the human gene into yeast and expressing it. The fact that human EPO and rhEPO are genetically identical is what made separating them so difficult. They managed to find a way of separating them by detecting charge differences on the amino acids. rhEPO is glycosylated differently than human EPO because it is made in yeast and not humans. This allows the protein to be isolated by 2d visualization based on charge and molecular weight.

Check out this figure from Lasne et al:
http://imgur.com/RLF5Omn",null,0,cdoefap,1rklhj,askscience,new,2
expandedthots,"My guess is that when you draw labs looking for doping, you don't really need to analyze the structure of the hormone to see if its recombinant, you just need to measure the amount. If its sky high, then they're doping unless they have an EPO secreting tumor in their kidney. The big dopers obviously are aware of this, so they cycle before they have to be tested but this habit is decreasing because testing is becoming more random. In my mind, you could just look at a hematocrit as well, even if their EPO levels are normal, although this can be naturally elevated by training at high altitudes etc. 

Except for the most high profile cases and levels of competition, I doubt any lab would analyze the amino acid profile of EPO harvested from thousands of athletes unless they were paid very very well.",null,1,cdo9y7q,1rklhj,askscience,new,2
selfification,"You might want to retag this with Biology or Neuro.  It'll probably get the attention of folks who know more about action potentials and the electrical properties of peripheral nerves in general.  The question you're essentially asking is ""Is there a difference between having a positive voltage along a nerve and a negative one?"" and the doctors will probably have a better answer to that.",null,0,cdoak5j,1rko77,askscience,new,7
shavera,"First, omfg do I hate that video. Like you have no idea. It's so so so wrong.

To answer your question properly, no, it's not. We can get into semantics about what it means to be ""moving"" through time, but that's all a bit of philosophy about what the theory of relativity implies about the metaphysical nature of reality (ie beyond our capacity to experiment and thus describe scientifically). 

In particle physics, we end up with some really awful infinite sums of integrals. One ingenious way of solving these integrals was to rearrange them in a specific order and then rewrite them in a different mathematical ""language."" It turns out that new ""language"" is one of diagrams of particles moving and colliding and whatnot. Again though, these are just graphical representations of mathematical representations of physical behaviour, not necessarily ""reality itself."" (again more metaphysics here)

Well it turns out that in these *representations* of reality, antimatter can be treated *as if* it is matter travelling backwards in time in these diagrams. 

But what is often more wondered about the ""arrow of time"" is really the ""arrow of causality."" Where it seems like actions earlier in time *cause* actions at further points in time, but not in the reverse. However, causality isn't a rigorous scientific description of reality. It's another approximate representation of reality. And it's an approximation that doesn't really help when we're talking about particle physics. What matters is that at time **a** we observe particles {*A*} with energy and momentum and position and at time **b** we observe particles {*B*} with energy and momentum and position. ",null,2,cdo7ssl,1rkprv,askscience,new,38
chrisbaird,"The answer to this question is really one of philosophy and not physics. Let me explain why. There are certain symmetries in particle physics that make it possible to describe antimatter as backward-in-time traveling regular matter and still get the right answer in the end (if you are careful and do it right). *But*... antimatter still obeys all the usual conservation laws and obeys causality. All the interesting stuff in science fiction that becomes possible with time travel to the past ends up breaking conservation laws (conservation of energy, charge, etc.). Since antimatter obeys these laws, it can't be used to do anything interesting typically associated with time travel. You can't use antimatter to shoot your grandpa as a kid. So with regards to conservation laws and causality, antimatter acts like regular forward-in-time matter. With regards to the symmetry in particle interactions, it *can* act like backwards-in-time matter. Which one is it *really*? That type of question is better suited to philosophers and linguists. There is no *really* in science. There is just layers upon layers of observations.",null,0,cdobzxi,1rkprv,askscience,new,8
Alphaetus_Prime,"What that really means is that if you take the equations that govern the behavior of matter and you reverse both the charge and the direction of travel through time, the equations are still valid. An electron annihilating with a positron can be interpreted as an electron that spontaneously flips its charge and starts traveling backwards in time, giving off lots of energy in the process.",null,0,cdobkue,1rkprv,askscience,new,3
Smoothened,"Our skin cells are not as exposed to physical stress as bacteria are. The outermost layer of our skin, the stratum corneum, acts as a barrier against many types of damage, including that inflected by mechanical stress such as the nano-spikes in this material. The stratum corneum is composed of biologically dead but still active cells called corneocytes which are interlocked forming a thick hydrophobic matrix. Besides mechanical stress, it also protects the body against dehydration, infection, burns, chemicals, etc. Bacteria, on the other hand, are only protected by their cell envelope. 

Edit: Also bear in mind that the stratum corneum is composed of up to 20 layers of cells and its thickness is between 10 and 40 micrometers. That's at least 20 to 80 times thicker than the 500 nanometers spikes in this material. And even if the cells in the outermost layers are damaged, these cells are constantly being replaced. ",null,1,cdoc4k9,1rkqh7,askscience,new,3
chrisbaird,"The ultimate physical constraint on telescopes is diffraction. Due to diffraction, the best resolution is proportional to the ratio of wavelength observed to aperture size. Note that this just places limits on individual telescopes depending on aperture size and does not mean that there is one fixed limit for all telescopes. In principle, you could get ever better resolution if you built ever bigger telescopes. While a telescope the size of our solar system could in principle see with far better resolution than we currently have, such a feat is currently technologically unattainable.",null,0,cdociv9,1rkrla,askscience,new,5
selfification,"Yep:
http://en.wikipedia.org/wiki/Diffraction-limited_system
http://en.wikipedia.org/wiki/Rayleigh_criterion#Explanation

Light has a certain wavelength and at some point, you cannot extract more resolution out of it.  It would be like trying to map out where ships in the ocean are based on the tides...  the wavelength would simply be too high to resolve the image.",null,0,cdo9hj7,1rkrla,askscience,new,3
iorgfeflkd,"Their path is curved when they move perpendicular to strong gravitational fields, and their energy is shifted up or down if they move towards or away from a gravitational source.",null,0,cdo954c,1rksxv,askscience,new,4
patchgrabber,"There is no average, this question isn't answerable. The pace of evolution is mainly dictated by selection, which is constantly changing. Add in other factors like environmental changes such as formation of mountain ranges, not to mention the sheer fact that it takes thousands upon thousands of years for most organisms our size to progress enough to a point where we deem it (arbitrarily) to be another species, something that we cannot observe directly, and you can see why something like this cannot have a simple number value placed on it.",null,0,cdo8wg8,1rkt46,askscience,new,4
snusmumrikan,"A way that may help you visualise it is to think of species in the same way as you think of generations in your family. If you look at a father and a child you can confidently say they are of two different generations. However if you lined up 100 people all aged from 1-100, you wouldn't be able to pick exact points in which one generation ends and another begins.

Evolution gives rise to new species through small accumulative changes in the genome of an organism. It is only by looking back that we see that now an organism is incompatible with regards to mating with the ancestor.",null,0,cdow5gv,1rkt46,askscience,new,1
julesjacobs,"No, it would appear just as fuzzy to them as looking directly at the text (slightly fuzzier in fact, since now you also add the distance from the eyes to the mirror and back the the equation). Optometrists use the same effect for the opposite means. If you want to test the right glasses for a person who can't see far away objects, you would need to place objects really far away (like 10 meters). Usually an optometrist's office is too small for that, so what they do is place a mirror on one side of the room, and let the patient look via the mirror to the opposite side of the room. This way you only need a 5 meter room to display on object 10 meters away for the purposes of the measurement.",null,3,cdoa7nk,1rktky,askscience,new,21
Arladerus,"First, let's examine what is actually happening in the eye of a short-sighted person.  The function of the lens in the eye is to take light rays from objects around you and bend them so that they converge on your retina to form an image.  However, in a myopic eye, this image is actually formed in front of the retina, causing blurriness.

Now, let's discuss your scenario.  When looking through a mirror (we are assuming a typical, flat mirror), we see objects behind the mirror, which is called a virtual image.  Without going too deep into the matter, basically the light rays act as if they were really behind the mirror.  So, if we have a mirror one foot in front of us, and we see an object 20 feet behind us, the object would appear to be a total of 22 feet away from us in the mirror.  The important question is whether the light rays behave as if were from an object 22 feet away.  Because the mirror is flat, the answer is yes.  

To summarize, no, the object would not appear clear.",null,1,cdoace3,1rktky,askscience,new,6
stuthulhu,"&gt;if a short sighted person were to try to read some text 20 ft away using a reflection from a mirror a foot away from their face,

You see by the photons reflecting from the object into your retina. In this case, the photons reflect from the object, hit the mirror, bounce off, then hit your retina. Now the object will appear as though it is 21 feet away, because the photons have to travel all the way from the object to the mirror, then from the mirror to your eye. This would actually make the problem worse.

Think about it this way, if you go outside during a full moon, and you hold up a mirror a foot away from your face, would you expect to see the footprints on the moon?",null,1,cdoajn3,1rktky,askscience,new,3
yeast_problem,"I read the question as using a ""close up"" mirror, i.e a convex mirror usually used to enlarge the view of your face. If your eye was further from the mirror than its focal point, surely a short sighted person could see an inverted but corrected image? A diverging concave mirror would probably be more useful, creating a virtual image behind the mirror.",null,1,cdobslm,1rktky,askscience,new,2
Ermagerd_cerpcerk,"I'm not 100%, but I believe that bad breath is caused in the mouth, not the lungs. Salivating cleans your mouth. This is why if you wake up with bad breath, then eat something, you won't have the morning dragon breath afterwards. While you sleep you don't salivate as much so your mouth gets ""stale"", causing bad breath. If you don't exhale from your mouth, the air coming from your lungs won't pick up the odors from your mouth",null,0,cdo9emq,1rktnc,askscience,new,1
Sterlz,"Oil is made up of much larger carbon chains approaching eight carbons (octane), while natural gas is usually composed of one carbon (methane).  Different underground pressures and differences in the surrounding chemical composition are most likely the reason that the carbon chains produced different lengths. ",null,0,cdod844,1rkumr,askscience,new,2
LuckyThursdays,"That biomass is made up of compounds (hydrocarbons) with varying chain lengths. The longer the chain, the more intermolecular forces form between the compounds which means there are more 'bonds' to break, requiring more energy and therefore the compounds have a lower boiling point. These are the ones which are liquid (or solid).
Smaller chains (eg. methane) have very low boiling points, below room temperature, so are naturally found as gases.
Longer chains tend to have formed under much higher pressure and temperatures as the atoms are forced together. Essentially, many methane molecules are 'squashed' to form longer hydrocarbon chains.",null,1,cdodaj4,1rkumr,askscience,new,3
albasri,"It used to be thought that chess masters had extraordinary memories, but this was shown to be untrue. Shown a board position for a very brief period of time, a chess master can reconstruct it from memory with much greater accuracy than a novice. However, if pieces are randomly arranged on the board, then experts are just as bad as novices. This was taken to indicate that the difference between experts and novices is that experts are able to rapidly extract features and detect patterns or structure in arrangements of chess positions, forming simpler representational units that can be encoded faster and with greater accuracy. These are typical characteristics of perceptual learning and exist in many domains of expertise, e.g. bird watchers, radiologists, etc. 

See Chase and Simon's chapter in *Visual information processing* (1973) and *Thought and choice in chess* by de Groot (1946/1978). The latter work showed that there was no difference between masters and controls in general cognitive abilities, only this memory difference for board positions. Another useful resource could be *The psychology of chess skill* by Holding (1985). 

(as an aside - I feel that this question is more appropriate for psychology than neuroscience…)

edit: grammar x2",null,22,cdoaul2,1rkutv,askscience,new,146
darkheritage,"There has been some buzz the last few years of research being done on chess' ability to increase cognitive ability, more specifically on how it relates to (oddly enough) Alzheimer's.  Studies have shown that playing chess can increase cognitive ability and help keep the disease in check.
Here is a brief overview:[From Chessbase](https://chessbase.com/post/checkmating-alzheimers-disease-210513)

Edit:  I know little about the subject, I am just an amateur chess player who has read some of the articles over the last few years.",null,6,cdobpox,1rkutv,askscience,new,16
honkycat1,"I don't think whether or not chess masters performance are due to nature of nurture quite answers your original question. 

Recent research has shown that cognitive games (those mental exercises, e.g. puzzles, mind games, etc) don't really help general mental ability. They only help one's ability to play those games in a very context specific way. This is a pretty sensitive topic, as a lot of companies have made a ton of money selling mind-exercises because of the argument that, like a muscle, you need to train the mind. While the general premise of this is true, empirical evidence does not show that mind games improve anything other than performance on those mind games. (Unfortunately, I don't have reference off the top of my head, but either way, I'm sure there are plenty of support on both sides of this argument) Therefore, I don't think playing chess will make someone smarter, in anything other than chess. But again, this is a hotly debated area, there is also contradicting evidence.

As for expert performance in chess. Hambrick is a colleague of mine so I am somewhat familiar with his work. He is on the camp of ""nature"" in the nature vs nurture debate. He has shown that working memory is related to expert performance of piano players even at the highest level, which indicate that regardless of practice, basic cognitive abilities is related to performance. My personal research is in job performance and research show that general mental ability can predict job performance even at the highest level of performance. Bottom line is, practice aside, your disposition will always play a role. 

When you ask ""In other words, it seems that I have a better chance of reaching a more ""beneficial"" conclusion if I could process 5 units of information with a superior ability to synthesize them, than if I could process 10 units of information with a poor ability to synthesize them. "" Of course, working memory alone doesn't predict your performance in chess. There are other factors, and there is also random noise. But holding other variables constant, someone higher in working memory will ON AVERAGE out perform in chess than someone who is lower. ",null,0,cdonux9,1rkutv,askscience,new,2
vincentrevelations,"As long as you're *learning* chess and studying moves you'll enhance your cognitive abilities. Once the game, or rather the opponents you encounter, begin to lack challenge the only thing you're training is your skill in recognizing chess set ups faster. 

Studies like the one in [this recent post](http://www.reddit.com/r/science/comments/1qdfvn/a_study_shows_that_older_adults_mean_age_72/) support this. I link the Reddit post because the discussion diverges to questions like yours.",null,1,cdogmyo,1rkutv,askscience,new,2
yeast_problem,"Newtons law and the Heat Equation do apply to a hot surface in contact with air, at any instant in time.

But this is a complex thing to model mathematically when you add gravity and buoyancy. With no air movement the temperature difference would continue to reduce and the rate of heat transfer fall until the temperatures equalise, like any other heat transfer, in the absence of any further boundary conditions.  But with buoyancy the air will move away from the hot surface bringing in cold air again, so the rate of transfer is going to depend on how fast the buoyancy works, which is temperature dependent.

So Newtons law wont work for the heat transfer between the hot surface and the bulk of the air it is in contact with over a range of temperature differences. But is does apply if you were to integrate the the heat equation across the surface and the boundary layer of the air taking account of the buoyancy and viscosity. At moderate temperature differences the flow becomes turbulent making solutions harder.

Thermal modelling in buildings often assumes that Newtons law applies for simplicity, but more complex models will use a temperature dependent function to calculate the convection rate.
",null,0,cdockqu,1rkw0i,askscience,new,2
newoldwave,"Convective rate of heat transfer depends upon the coefficient of heat transfer of the two materials and the temperature difference between them. For a fluid moving across a solid, the temperature difference fluctuates of course. Coolant flowing through a radiator for instance.  ",null,0,cdo9db0,1rkw0i,askscience,new,1
therationalpi,"The two slit experiment is based on wave physics, so it works with sound waves, light waves, water gravity waves, etc. It's most noticeable when the wavelength is on the same order as the distance between the slits, and the slits are small with respect to the wavelength. For example, if I wanted to do this experiment with sound with a 1 m wavelength, then I'd want the slits no more than 10 meters apart, and I'd like the slits to be less than 10 cm.",null,1,cdo9vwh,1rky8r,askscience,new,4
fortunecooki," As the double slit experiment is based on a wave model, sound waves and even ocean waves show this too. Even in nature, you see the same effect when a wave goes through two rocks:
http://bsbh.wikispaces.com/file/view/diffraction.jpg/214660796/377x392/diffraction.jpg",null,1,cdodpvb,1rky8r,askscience,new,4
iorgfeflkd,"Yes it does. Any type wave, like surface waves in a lake, etc. There's nothing mystical or spooky about it.",null,1,cdo9tg2,1rky8r,askscience,new,2
Armadyll,"Firstly, you need to understand what 1440p means. the 1440 refers to the number of pixels in the vertical axis. The normal aspect ratio of HD screens is 16:9 making the horizontal axis 2560 pixels. This is a total of 3,686,400 pixels which is much higher than the 2,073,600 pixels of a 1080p screen. 
Our eyes can resolve (tell the difference between) things about 1' (1/60 of a degree) apart. The angle between the pixels depends, of course, upon the distance between the pixels and your eye. To test this look at your monitor from 10m away and see if you can resolve any of the pixels. You can't. Now try it with your face pressed against the screen. You probably can. The only thing that has changed was the distance between your eye and the pixels. What does all of this mean? You can make 1440p screens much larger than their 1080p counterparts without being able to resolve any of the pixels, and this is vital to maintaining a good picture quality. To answer your question, the limit to how many pixels per inch we can resolve depends on the distance at which the screen is viewed. ",null,1,cdofh1w,1rkyt0,askscience,new,15
dakami,"I've held in my hands a 20"" 4k tablet; images on it look like printed photographs (more accurately, they look like photographs printed onto plastic and backlit).

So size and distance really do matter.

Where things get interesting is when we get up to around 32K to 64K displays; those start getting dense enough that you can have pixels emitting at only certain angles.  That lets you actually have light field displays, i.e. monitors that have true 3D and appear as windows instead of surfaces.  I've seen early versions of these; they're amazing.",null,1,cdoi55u,1rkyt0,askscience,new,7
bunjay,"The detail we can see is always going to depend on pixel density and distance to the image vs your eyesight.

I think you might be trying to ask what resolution it would take for the individual pixels to be indistinguishable from one another at even the closest distance. Regular eyesight can distinguish high contrast line pairs about 0.003 inches apart at a distance of 10 inches. So if you had a display with about 700 lines per inch or more it could display high contrast detail at or above the resolving power of the average person's eyes even at the closest distance they can focus. This would shrink a 1080p display down to an inch and a half wide, and possibly as little as half that size would be required for someone with exceptional vision.",null,0,cdojmr5,1rkyt0,askscience,new,5
littlemisfit,"If your TV is less than 50 inchs and you don't sit closer than 10 feet from your screen then you won't even be able tell the difference between 720p and 1080p, so getting a 4000k TV is a complete waste of money unless you have a massive screen or sit really close to the TV.  [This site](http://www.digitaltrends.com/home-theater/720p-vs-1080p-can-you-tell-the-difference-between-hdtv-resolutions/) has a chart that shows what resolution is best based on the TV size and viewing distance.

Source: http://www.audioholics.com/hdtv-formats/1080p-and-the-acuity-of-human-vision",null,10,cdoffaa,1rkyt0,askscience,new,12
expertunderachiever,"Lot of good answers but honestly where things are lacking isn't in the DPI area but in the PNSR area.  It's not hard to see MPEG artifacts on a 40"" TV even at 10-14ft away and that will always trigger a ""this is a fake image"" response from your brain.

",null,0,cdovwel,1rkyt0,askscience,new,3
ITRAINEDYOURMONKEY,"Back in my undergrad Optics course (physics optics, not physiology), we did a kind of back-of-the-envelope approximation. Based only on the pupil size (not retina), we came up with about 5MP at arm's length. This is only based on treating the eye as a lens system, not as a CCD.

I read a post on here a couple months ago claiming much higher resolution based on the retina's physiology, but I'm afraid I can remember neither the numbers they gave nor the exact reasoning process to get there.",null,0,cdp04bn,1rkyt0,askscience,new,2
outerspacepotatoman9,"Light does slow as it passes through a gas. It slows down as it passes through any medium with an index of refraction greater than one. Air, for example, has an index of refraction of 1.000293. So, the speed of light in air is .9997 times the speed of light in a vacuum.",null,0,cdohvxr,1rl04b,askscience,new,2
MasterPatricko,"A refractive index/slowing of light arises because the EM wave interacts with the charged parts of atoms. Gases, unlike liquids and solids, have a very low density of atoms so will always have a refractive index close to the vacuum -- *significant* slowing of light is unlikely.",null,0,cdowsbh,1rl04b,askscience,new,1
mc2222,"Recall that when we say ""polarization"" we're talking about the direction of the electric field of the EM wave.

A few observations/relevant comments:

* The double slit experiment is independent of polarization.

* Bragg diffraction is the molecular/atomic version of double (or multiple) slit interference, where the space between crystal planes act like a slit structure.  *The gaps are so small that they don't diffract visible light*, but rather diffract x-rays.  Hence the field of ""X-Ray Crystallography"".

Polarizing films are, as you indicate, sometimes made of long strings of molecules with gaps between them.  The spacing between molecules is so small the light won't be diffracted.  However, if the polarization of light is appropriate, the electric field will exert a force on the valence electrons in the molecule, driving them up and down along the molecular chain.  Since the optical energy is being converted to energy to drive the electrons back and forth, the EM wave is absorbed, and thus blocked.  

The component of polarization in the direction perpendicular to these long chains is unable to drive valence electrons back and forth (since there is no chain like structure for them to travel along), and thus, this component of the electric field is transmitted through the polarizing sheet.

It's worth mentioning that these polarizers work for things like sunglasses and inexpensive polarizers, but scientific grade polarizers rely on different methods of achieving good polarization.",null,0,cdoff2r,1rl2n8,askscience,new,6
failuer101,"that's a really good question. i think it's because the way the polarized lenses are designed which causes only one pattern of light through. the light that is let through will act like a wave but only in one direction so it doesn't interfere with the other beams of light that make it through.

[here](http://www.microscopyu.com/articles/polarized/images/polarizedlightfigure1.jpg) is an image of polarized light. now if you can imagine many sets of these vertical waves side by side, they wouldn't overlap, therefore they wouldn't interfere with each other. i think the double slit experiment doesn't polarize the light.

edit: just watch this video [sixty symbols video](https://www.youtube.com/watch?v=KM2TkM0hzW8)

",null,2,cdof8hl,1rl2n8,askscience,new,1
Gplads,"I may be looking in the wrong places, but the only stuff I've been able to find talks about protective instinct specifically in new mothers (and even then it's about mice.) Altruistic behavior in general though could be an example of costly signaling theory, in which an individual exposes themselves to risk or sacrifices resources (time, energy, food) in exchange for benefits such as social recognition or mating preference. I could imagine protective instinct as being an extension of this, in which an individual puts himself at risk to keep another member of their group alive, with the assumption that it would be done for them, if needed. I would really like to hear a more informed answer though.

Edit: here is an article proposing that altruistic behavior in humans is a costly signal for general intelligence.

https://lirias.kuleuven.be/bitstream/123456789/101194/1/Millet
",null,0,cdof0b1,1rl2qw,askscience,new,1
stuthulhu,"Planets are pulled into spherical shapes by the force of gravity. One of the defining features of a planet is that its mass is substantial enough that it overcomes the [compressive strength](http://en.wikipedia.org/wiki/Compressive_strength) of its material, forcing it into a spherical shape that is balanced by [hydrostatic equilibrium](http://en.wikipedia.org/wiki/Hydrostatic_equilibrium#Planetary_geology). ",null,0,cdoe7ut,1rl37t,askscience,new,11
hovissimo,"Well, planets **by definition** are spherical (or very close).

http://en.wikipedia.org/wiki/Definition_of_planet#Hydrostatic_equilibrium

http://en.wikipedia.org/wiki/IAU_definition_of_planet

There are plenty of objects orbiting the sun that definitely aren't spherical, but we don't call them planets.  There are even some spherical objects (like [Ceres](http://en.wikipedia.org/wiki/Ceres_%28dwarf_planet%29)) that we don't consider planets because they don't meet other planetary criteria.



___

To answer the question I think you intended to ask, it's because of gravitational attraction.  The largest bodies in our solar system have enough mass (enough *stuff*) in them that their mutual gravitation ""pulls"" them into a mostly spherical shape.

A good analogy would be a water droplet on a piece of glass, the water drop also ""pulls"" itself into a spherical-ish shape because the molecules of water mutually attract each other.  (Note: This is not a good example, because it's not gravity that makes water bead like this, but [Cohesion](http://en.wikipedia.org/wiki/Cohesion_%28chemistry%29).)

  That cohesion article on wikipedia has a picture of some water floating in microgravity in space, and you can see that the water is much closer to a perfect sphere than it gets here on the surface of Earth.

Edit: newlines",null,0,cdoeiri,1rl37t,askscience,new,5
natty_dread,"Because a sphere is the energetically most desirable state.

Gravity is the ""glue"" that holds the gas together, but there are some other things to consider:

The surface of a body (no matter if it is solid or a fluid) is on an energetically higher level than the particles inside the body.

Since a system always wants to be in the state of least potential energy, the system wants to have a surface as small as possible.

On the other hand, there is a pressure inside the sphere, due to the kinetic energy of the particles. This pressure wants the body to take a shape of maximum volume.

These two factors are working against each other, hence the system will choose a shape that has maximum volume while sustaining minimum surface.

The shape of the highest volume to surface ratio is the sphere. That is why Planets - or soap bubbles for that matter - take the shape of spheres.
",null,1,cdoiky2,1rl37t,askscience,new,2
null,null,null,7,cdody0j,1rl37t,askscience,new,2
dalgeek,"In an ideal world the wireless client associates with the AP with the loudest signal.  A ""perfect"" signal is around -35dBm (if you're directly next to the AP) and most wireless cards disconnect around -85dBm.  This range  is reduced if lower data rates are disabled; the lower the data rate, the further away you can be, but this reduces the performance of all clients.

This gets tricky when you mix 2.4GHz and 5GHz.  The 5GHz radios operate at half the power of 2.4GHz radios and the signal attenuates (degrades) faster than 2.4GHz, BUT even with a supposedly weaker signal a client can get faster speeds from 802.11a.

Roaming is also up to the client.  If the client can see two APs at -55dBm then it's pretty much a 50/50 chance on which it will pick.  The client will associate to one and only attempt the other if the signal level drops too far.  Some devices will maintain a death grip on an AP before roaming, while others will hop between APs like a jack rabbit.

A lot of this depends on the driver implementation.  For example, iDevices up until iOS 4 or 5 preferred 2.4GHz radios over 5GHz radios even if the 5GHz could give a higher data rate.  After that point the developers swapped the preference.  The same thing happened with Dell Inspiron wireless drivers.  The drivers also determine how the signal strength is calculated.

EDIT: Quick guide to frequencies and speeds (physical data rate)

* 802.11b - 2.4GHz - 1, 2, 5.5, 11 (Mbps)
* 802.11g - 2.4GHz - 6, 9, 12, 18, 24, 36, 48, 54 (Mbps)
* 802.11a - 5GHz - 6, 9, 12, 18, 24, 36, 48, 54
* 802.11a/n - 5GHz - 15, 30, 45, 60, 90, 120, 135, 150
* 802.11b/g/n - 2.4GHz - 7.2, 14.4, 21.7, 28.9, 43.3, 57.8, 65, 72.2

Note that ""n"" works on both 2.4 and 5GHz.  This is why it is important to look for a/n because a lot of consumer devices that advertise 802.11n are actually running at 2.4GHz.",null,0,cdoejwh,1rl3px,askscience,new,23
iBeReese,"Funfact: This is a great question that I would love to know the answer too, but is isn't actually a computer science question. CS is the field devoted to software that straddles the line of applied math and engineering. This question is in the realm of computer engineering and information technology more than CS.",null,0,cdomvyi,1rl3px,askscience,new,5
adamsolomon,"The energy was lost to the expansion of the Universe. Energy isn't actually conserved as the Universe expands, because energy is only conserved when the laws of physics don't depend on time, whereas an expanding Universe clearly does change with time.

The Universe cools down as it expands because radiation (like light) *redshifts*, meaning the expansion of the Universe stretches light waves to longer and longer wavelengths. Those longer wavelengths correspond to lower energies, and energy corresponds to temperature, so the temperature of the radiation cools down. Similarly, the expansion slows down matter particles running around, so their kinetic energy and therefore their temperature also decreases with the expansion.",null,1,cdoh4h3,1rl406,askscience,new,7
Slave_to_Logic,"The ""white"" light coming from your flashlight is leaving the bulb and traveling towards the red plastic.  White light is of course all colors of light traveling together.  

When this white light hits the red plastic, the part of the light that is red continues through, but all of the other colors of the rainbow are blocked and absorbed into the plastic.

",null,0,cdodi6e,1rl4gp,askscience,new,13
ramk13,"To add to what /u/Slave_to_Logic said, the plastic is absorbing the non-red colors. The same is true for other colors of translucent/transparent materials. Here's a plot of what gets absorbed for different types of dyes (food coloring in this case):
http://www.nsta.org/images/news/legacy/jcst/0705/FavDemoFig5.jpg

The bar at the bottom is the color of the light at that wavelength. So red dyes absorb green light strongly and some blue light too. What's left over (what you see coming out of the flashlight) is the red light not absorbed.",null,0,cdodsy9,1rl4gp,askscience,new,3
wazoheat,"""Radiation"" is a scary word for the public because they only know of it in the context of nuclear bombs and nuclear power plants. In reality, ""radiation"" is just a word that means something emitted from something else. In most scientific contexts, the word ""radiation"" is referring to ""electromagnetic radiation"", which is the generic term for waves given off in the form of alternating electric and magnetic fields.

""Electromagnetic radiation"" actually covers a broad spectrum of things you experience in your day-to-day life. [X-rays, UV light, visible light, microwaves, and radio waves are all just electromagnetic waves at different energies](http://www2.chemistry.msu.edu/faculty/reusch/VirtTxtJml/Spectrpy/Images/emspec.gif): in reality they are all the same thing. The only difference is the wavelength of these electromagnetic waves: x-rays have the shortest wavelength (most energy), radio waves have the longest wavelength (least energy).

All objects in the universe emit electromagnetic radiation at all times; the energy of this radiation is directly related to the temperature of that object. On earth, this radiation is typically at [infrared](https://en.wikipedia.org/wiki/Infrared) wavelengths, which are less energetic than visible light. Hotter objects will emit higher-energy wavelengths, which our eyes can detect as ""visible light"". This is the reason why objects glow when you heat them up.

Electromagnetic radiation *can* be dangerous, but *only if* the frequency of the waves is high enough. X-rays and UV-light can be bad for you in high doses because at high energies, this radiation can break sensitive molecular bonds in your cells, particularly in your DNA. And while your body has mechanisms for repairing this damage, if you receive too much high-energy radiation in a short period of time the repair mechanisms can not keep up, leading to permanent damage, and possibly cancer if the DNA is damaged enough. **But they key point here is that this damage can only occur from high-energy electromagnetic radiation**: lower-energy EM radiation, such as visible light, microwaves, and radio waves, *do not have enough energy to break molecular bonds*, and so can not cause ill effects in this manner.

The ""radiation"" that electronic devices emit work in a wide range of wavelengths. Remote controls use signals in the infrared. Cell phones and other wireless devices tend to use microwave wavelengths. Broadcast TV and radio use radio waves. But **all of these signals are at lower energy than visible light**, and so can not cause the same ill effects that higher-energy EM radiation can.

**In summation: electronics emit electromagnetic radiation at lower energies than the radiation you get from everyday normal objects, and can not hurt living tissue.**",null,0,cdofa5v,1rl8a4,askscience,new,6
stefvonb,"There is always a danger with regards to radiation. These are called stochastic effects (the probability of the effect increases with the amount of exposure, or *dose*).

However, electronics usually give off radiation with low frequencies, such as visible light, microwaves, infrared, or even radio waves. These types of radiation are not primarily what we call *ionizing radiation*. It is ionizing radiation which has the power to strip the electrons off of the molecules in our bodies, like those present in DNA. Ionizing radiation is primarily responsible for the risk of cancer, as far as radiation is concerned. Electronics don't give off much ionizing radiation.

Yes, there is a lot of speculation, but electronics do not present much of a risk.

EDIT: This is more of a physics question. Hope my answer is satisfactory.",null,0,cdodzr2,1rl8a4,askscience,new,1
iamdelf,"These sorts of clinical trials are actually interesting to design.  http://www.ncbi.nlm.nih.gov/pubmed/9708750  Has the abstract of one of the original clinical trials.  Basically someone comes into a clinic and asks for emergency contraceptive.  The clinic asks the person if they are interested in participating in a clinical trial of a new medication.  They collect the results both as far as efficacy(pregnant/not pregnant) as well as side effects to compare it to currently available contraceptives.

The compounds used aren't new, its the same chemicals which are already approved for daily contraception.  It is a new indication trial and you compare it to the standard accepted treatment to see if it is any better than what is available.",null,9,cdoiopu,1rl9fc,askscience,new,67
null,null,null,21,cdohhj0,1rl9fc,askscience,new,52
takeandbake,"I am going to respond to the part of your question about efficacy of emergency contraception (EC) in women over 80kg.

Researchers were looking at the data sets for 2 randomized controlled clinical trials, each one about a different medication used as EC.  The purpose of the meta-analysis was to find risk factors for EC failure, which would be pregnancy.  They found that obese women, as defined by a body weight over 80kg, were more likely than non-obese women to become pregnant despite using EC.

Also note that it's not that it ""doesn't work"" for women over 80kg, it is less likely to be effective.",null,1,cdomofz,1rl9fc,askscience,new,6
housebrickstocking,"Just remember some ""testing"" in science is in fact mathematics, chemistry, and physics extrapolated with some real world observations to support the outcomes. A lot of people imagine testing as needing to be a lab experiment rather than just a mathematical proof.

Note: This is not really relevant to the topic, excepting that the recent news about Plan B comes from more or less the above method.",null,2,cdor76y,1rl9fc,askscience,new,3
null,null,null,4,cdoj2n1,1rl9fc,askscience,new,2
null,null,null,8,cdoi4z7,1rl9fc,askscience,new,1
null,null,null,20,cdoh6qd,1rl9fc,askscience,new,4
SyNNeR6x3,"The capacity of the human stomach varies, on average the stomach can expand to hold up to 4 L (4.2 qts.) of food, expanding to about 50 times its volume when empty. When it is completely filled, the human bladder can hold on average 680 ml of urine; the urge to urinate however can start at about 250 ml.

TL; DR: Stomach: 4L (4.2 qts) Bladder: 680 ml
",null,2,cdoijew,1rl9yg,askscience,new,21
seroevo,"Are you asking whether it can in general, or whether it does for you specifically if you play driving games?

It's been shown to make a difference for people already, where those with extensive game time in Forza or GT can drive a track for the first time in RL and put up a significantly better time and display of skill than a person otherwise totally inexperienced. ",null,7,cdoem4e,1rla91,askscience,new,34
rtechnix,"Short answer, [yes](http://www.popularmechanics.com/cars/news/industry/can-you-learn-real-racing-skills-from-gran-turismo-16055980). Long answer, it's arguable that such people were bound to be good drivers naturally, but just as pilots train in simulators, a sufficiently realistic game should test/build up your reactions/reflexes that can translate to actual driving. In more realistic games like the Gran Turismo series, you will learn much more about how to pick a racing line as well and in general get a better grasp on how cars react to steering/braking/etc and changes to the traction. Of course, there will be a few real life things that simulators won't be able to emulate.

That and many racers and other with hefty experience in real racecars have often spoken highly of the realism of some games like [GT](http://en.wikipedia.org/wiki/Gran_Turismo_4#Reception) and [iRacing](http://www.iracing.com/testimonials/). In fact iRacing claims some racers even use it sometimes to get a feel of a track before they can actually drive there (though take it with a grain of salt like most promotional statements).

IIRC I think the biggest challenge from that one guy who went from winning the GT competition to an actual race in a real race car was that he wasn't ready for the forces you can experience and his neck hurt from the lateral g's.",null,4,cdok7j4,1rla91,askscience,new,15
null,null,null,0,cdogn4z,1rla91,askscience,new,2
null,null,null,4,cdoi33r,1rla91,askscience,new,8
null,null,null,2,cdoizca,1rla91,askscience,new,4
null,null,null,1,cdopck7,1rla91,askscience,new,4
null,null,null,2,cdop1vt,1rla91,askscience,new,2
null,null,null,1,cdom4yw,1rla91,askscience,new,1
Cyanotical,"playing a racing sim will not improve them per say as you can not get a proper feel for g-force and tire traction, but there has been at least one study that showed playing video games in general makes you a better driver due to increased situational awareness, reduced reaction time, and the ability to make faster on-the-fly decisions

I can't seem to find the study atm, [there are plenty of news articles,](http://www.theregister.co.uk/2010/09/14/action_games_make_you_a_finer_human_being/) but direct links are a 404",null,2,cdoo46q,1rla91,askscience,new,1
null,null,null,9,cdogowl,1rla91,askscience,new,4
null,null,null,11,cdoed4d,1rla91,askscience,new,5
null,null,null,10,cdoere2,1rla91,askscience,new,3
Smoothened,"Decussation, or cross-wiring in neural circuits is thought to confer some form of functional advantage because it's prevalent across animal taxa. The exact advantage is not entirely understood, but research suggests that decussation prevents wiring errors in complex 3D networks. [This paper](http://onlinelibrary.wiley.com/doi/10.1002/ar.20731/pdf), for example, shows that decussation makes complex wiring networks more robust and offers a mathematical explanation. As a molecular neuroscientist, trying to read the paper gave me a headache, but you might find it useful.  ",null,2,cdofogt,1rlc83,askscience,new,37
null,null,null,1,cdojhgk,1rlc83,askscience,new,5
howlin,"It is stranger than that.  Not only are our left and right crossed, but our front and back are also crossed.  We do sensory processing in the back of our brain (farthest away from the eyes), and our motor planning in the from of our brain (farthest away from our spinal cord).

It's believed this was an evolutionary accident that occurred when vertebrates and arthropods diverged. The arthropod nervous system runs along the inside of their bodies, while our nervous system runs along our back. Anatomically, the best way to explain this is that at some point, vertebrates flipped their body plan 180 degrees, but the brain remained stationary.  

You can read a little more about this in  ""The Upright Ape: A New Origin of the Species"" By Aaron G. Filler if you want a high-level review

",null,2,cdok66w,1rlc83,askscience,new,5
null,null,null,2,cdofdzn,1rlc83,askscience,new,1
null,null,null,3,cdou7rp,1rlc83,askscience,new,1
xtxylophone,"Right now, best we can say is we dont really know. We can't get observational evidence of them having planets so we must rely on models and those arent quite perfect either or suited for those conditions.

So to throw a stab at an answer. Probably not 'planets' but there may well have been smaller failed stars orbiting other stars.",null,2,cdojsvn,1rlf3l,askscience,new,4
GeoGeoGeoGeo,"To build a planet you need lots of rubble and that means lots of heavy elements – stuff more massive than atoms of hydrogen and helium. The prevailing wisdom had been that the magic of stellar alchemy didn’t produce enough useful “star-stuff” to build terrestrial worlds until at least six or seven billion years after the Big Bang. In this sense a host stars [metallicity](http://en.wikipedia.org/wiki/Metallicity) has been previously assumed to be directly correlated to what kinds of planets it can be host to which was backed up by studies of exoplanets, frequently finding worlds around stars with a “metallicity” (i.e. a heavy element abundance) equal to or greater than our Sun. This leads to the conclusion then that the 1st generation stars simply were not able to be host to terrestrial (rocky) or gas-giant type planets (also with rocky [metallic] cores. However, the correlation that a stars metallicity is a strong indicator for planets, has been weakened, at least for smaller, terrestrial planets - **relative to our suns metallicity**, a pop I star. 

&gt;We find that planets with radii less than four Earth radii form around host stars with a wide range of metallicities (but on average a metallicity close to that of the Sun), whereas large planets preferentially form around stars with higher metallicities. - [Nature](http://www.nature.com/nature/journal/v486/n7403/full/nature11121.html)

Surveys do detect a decrease in the number of planet-hosting stars with decreasing metallicity, but this drop is much shallower for terrestrial planets than it is for gas giants. According to our current understanding the very 1st stars, termed [population III stars](http://astronomy.swin.edu.au/cosmos/P/Population+III) would have had no planets; pop II stars have low-metallicity *relative to our sun* and created all the other elements in the periodic table (except the more unstable ones). Though some older pop II stars have been found with [gas-giants orbiting them](http://arxiv.org/abs/1011.4938), they are believed to 'adopted' planets. Pop I stars (such as our sun) are metal-rich and can therefore be host to rocky planets. 

In order to make planets you clearly need metals, and therefore it is extremely unlikely to have planets formed alongside their metal deficient or even metal-poor host star. ",null,0,cdos4nj,1rlf3l,askscience,new,3
EvanRWT,"&gt;There weren't any heavy elements to speak of, so there obviously couldn't have been rocky planets like our own. But what about gas giants?

Our current models of planet formation (through accretion) require the presence of heavy elements. Even for gas giants. All of the gas giants in our own solar system have cores which consist of heavy elements. Though they may small in comparison to the mass of the whole planet, they are needed for the planet to begin forming.

This was the status until a few years ago. Most astronomers would have said that planets were unlikely among very old stars, because there was a scarcity of heavy elements.

However, more recently some very old extrasolar planets have been discovered. For example, the [Methuselah Planet](http://en.wikipedia.org/wiki/PSR_B1620-26_b) was discovered by Hubble near the core of the globular cluster M4. This [planet](http://hubblesite.org/newscenter/archive/releases/2003/19) is a gas giant, about 2.5 times the size of Jupiter, and about 12.7 billion years old. So it was formed about a billion years after the big bang. The star it orbits is of very low metallicity, and is now a white dwarf.

So right now there is a bit of a confusion. Apparently, planets could form very very early in the universe, as early as a billion years after the big bang. The ratio of heavy elements was very low at this time, so our present accretion models of planet formation don't really explain how they were formed. This is an active area of research. Right now, nobody knows how they were formed, but we have good evidence of very old planets.

But even this doesn't go back to the **very first** generation of stars (population III). We really can't speculate much about them because so far population III is hypothetical; no population III stars have ever been found.

The idea is that population III stars were very large, about 100-150 solar masses. Stars this size have very short lifetimes, so they burned out quickly and supernova'd, producing the first heavy elements. Given what we suspect about them, it seems highly unlikely that they had any planets.",null,0,cdphhs1,1rlf3l,askscience,new,2
camCut,"There was only hydrogen and helium in the early universe, where first generation stars ""lived"" in, no heavier elements like carbon, nitrogenium  or silicium. and I am not sure if the first gen stars lived long enough to allow planets to form around them.
edit: im in a train, to lazy to go deeper  ;)",null,3,cdofps8,1rlf3l,askscience,new,3
iCookBaconShirtless,"The problem with addressing this question is that there is no natural and intrinsic way to decide whether a non-zero number is ""close to zero"" or ""far from zero.""  The constants you listed are closer to zero than 10^10 but further from zero than 10^-10 .  Who is to say where we should place to cutoff between numbers that are ""close to"" or ""far from"" from zero?

They may strike you as being close to zero, but this is a question of psychology not mathematics.  Perhaps we deal with numbers that are further from zero than pi more often than numbers that are closer to zero than pi, so pi seems close to zero?

I'll add that the precise values of those constants might not be as fundamental as you think.  The ratio of the surface area to the diameter of a sphere in n dimensions, for example, is only pi when n=2.  However, it can be written in terms of pi for other dimensions.  In fact, many people argue that tau=2*pi should be thought of as more fundamental than pi.  I personally believe that any rational multiple of pi is equally ""fundamental"", whatever that means.  We just settled on one of them out of convenience and for historical reasons.

As for your second question, the reason that small primes are denser than large primes is fairly well understood.  See [the prime number theorem](http://en.wikipedia.org/wiki/Prime_number_theorem).

Edit: Think about it this way. OP is intrigued that so many constants are smaller than 5 in magnitude. But any finite list of numbers has an upper bound.  The list given by OP happens to have 5 as an upper bound.  So what is so surprising about 5 versus some other upper bound? It's tempting to answer that a number smaller than 5 is unlikely to occur if you uniformly draw a random positive number. But there is actually no mathematical way to make sense of drawing a real number uniformly. The lack of scale is the real culprit. You cannot claim that a number is intrinsically ""large"" or ""small"" in any meaningful way without choosing something to compare that number to.


**EDIT 2: To anyone who still thinks that the list of constants given by OP (or any other finite list of numbers) is intrinsically small in magnitude, please provide a list of number that you think is not small in magnitude.**

I am spending a lot of time responding to commenters that might be glossing over some subtleties of my argument.  If you wish to debate my comment, please first answer the question I posed in EDIT 2.  I think the doomed effort to answer this question will reveal some of the subtleties of the point I am making.

EDIT 3: I am in no way trying to discredit OP's question.  I agree with OP that the fact that so many named constants are less than 5 is surprising in a psychological sense.  But I contend that there is no way to answer the question of whether it is surprising mathematically.  The reasons for this are actually a bit more subtle than they first appear.  The problem is that *every* finite list of numbers has an upper bound.  So how surprising is it that there are upper bounds on OP's list that are less than 5?  Answering this would require defining some sense of a probability distribution on the positive real numbers.  But every probability distribution on the positive real numbers artificially imposes a scale because there is no uniform distribution on the positive reals.  So **you can only really ask whether a list of numbers is surprisingly close to zero with respect to some arbitrarily chosen scale.** 

",null,508,cdooolp,1rlfdu,askscience,new,1622
Vietoris,"I will try to give a different point of view on this question. It will be more geometrical and not exactly answering the question but that might still give some insight.

I will talk about [Constructible numbers](http://en.wikipedia.org/wiki/Constructible_number). The principle is the following. You start with the two points at distance 1 on a plane, and you have a compass and a straightedge. At each step of the process you will get new points as follows :

Imagine you are at step n and you have already constructed N points. You draw all possible lines passing through two of your N points.  You also draw all the circles centered at one of your N points with a radius equal to the distance between two of your N points. You mark all the intersections created between lines, circles or both and add all these points to your list of N points. 

For convenience I will see the points in the plane as complex numbers. A number is constructible if it can be obtained by this process. 

Let's give the first examples : 

* From the first two points 0 and 1, I can draw the line passing though these two points, the two circles of radius 1 centered on 0 and 1 respectively. This gives me 4 new points : -1, 2 , e^ipi/6 , e^-ipi/6

* From this new set of six points, I can draw 10 different lines. And I have four possible radius for my circles (1,2,3,sqrt(3)) and 6 possible centers which gives me more or less 24 new circles. So I get more than 100 new points (see the [picture](http://imgur.com/bE3dlfG) )

* And so on ... the number of points explodes very quickly.

Ok what was the point of all this ? Well it is interesting to note that all the (non-zero) numbers we can construct in 2 steps all have norm between 0.2 and 5. And more importantly, most (like 95% may be, I didn't compute it) have norm between 0.5 and 4. (see the [picture](http://imgur.com/bE3dlfG) )

When I go on with the steps I see that there is always a lower bound and an upper bound (obvious), and that most of the numbers have norm between 0.5 and 4. (less obvious but it's because, when you draw all the circles around your existing points, most of them will stay in the same range ...).

Now that we understand that, what next ? Well, suppose we could see the property of being ""interesting"" as a random occurence among numbers (this is my hypothesis, I am not exactly proving that this means anything). And to construct interesting numbers we have to do a certain number of steps. Equivalently, I construct all possible points in n steps and pick randomly some of them  (to say that they are interesting). With the arguments above, this would prove that :

* a majority (I have no idea how to exactly compute it) of the interesting numbers constructible in n steps lie in a rather restricted range between 0.5 and 5. 

* a few will be larger or smaller than that ... 

* And no interesting number constructible in n steps will have norm smaller than 10^-n or be bigger than 10^n (because no  number constructible in n steps can be that big)

&gt; Tl; DR : If I randomly pick numbers that could be defined in a certain number of steps, then most of the numbers I picked will be in the [0.5 , 5] range

Now, back to mathematical constant. How are they defined ? Well, we have to do several steps. Much more complicated steps than for constructible numbers, but still a certain number of steps. (I could look at the numbers that can be defined in a certain number of words, to get an idea, even if this definition is problematic).

So I would guess (and this is wishful thinking) that the principle stay the same. Of all the numbers that can be defined in a certain number of steps, most of them should in the zone around 0.5 and 5. So the same should be true for interesting numbers ...

Thank you for your attention. I hope this was not too confusing.

EDIT : I changed the phrasing of the sentence defining constructible numbers.",null,42,cdolc72,1rlfdu,askscience,new,195
ReyJavikVI,"First, I wanted to link [this math.stackexchange post](http://math.stackexchange.com/questions/120780/why-are-all-the-interesting-constants-so-small), which discusses the topic.

There's a constant that no one has mentioned: the order of the monster group, which is 808017424794512875886459904961710757005754368000000000 (seriously). I don't know much of the math behind it, but as far as I know the monster group is the largest of the sporadic finite simple groups, and this is the number of elements in it. Surely not close to zero!",null,18,cdog7cz,1rlfdu,askscience,new,57
penorio,"A lot of times you are working with ratios that are not the actual constant, but a multiple of the constant. For example the ratio between radius and circumference is 2*pi, not pi. So any other multiple would work the same, but I guess it's more useful to choose a small one.

For why there are bigger gaps for primes as numbers grow the [Sieve of Eratosthenes](http://en.wikipedia.org/wiki/Sieve_of_Eratosthenes) is a really nice way to generate prime numbers and visualize it. There's an animation of the method in that wikipedia link.",null,14,cdohd6r,1rlfdu,askscience,new,46
null,null,null,12,cdops2y,1rlfdu,askscience,new,47
SpaceEnthusiast,"This is probably going to get buried deeply but I need to give an example as why some constants tend to be small. Take the Gamma function Γ(x) for example. It is defined recursively as Γ(x+1) = xΓ(x) so for example take

Γ(3/2) = (1/2)Γ(1/2) = pi^(1/2)/2

Or take Γ(5/2) = (3/2)Γ(3/2) = (3/2)(1/2)Γ(1/2)

In a similar manner

Γ(m/n) = qΓ(1/n)

where q is a rational number obtained by the process above. In essence, when it comes to the Gamma function, you only need to know the values of the function for x between 0 and 1 to know all other real values. When it comes to the Gamma function, the smallness of the constants comes from the recursive nature of the Gamma function. 

I would argue that a lot of small constants come from an anthropometric perspective - the objects we usually consider are somewhat idealized, small, less complicated things. There is only so far you can go with a limited application of the basic operations. 

I would also argue that if you are given a larger constant, more often then not you'll be able to break it down into simpler/smaller constants.",null,9,cdoky4q,1rlfdu,askscience,new,38
null,null,null,7,cdopotb,1rlfdu,askscience,new,29
NemoKozeba,"I think I know the answer to your question. Or answers, because there are two.

First, whether you realize it or not, all of your examples are ratios. Even pi, in a way. It's a question of scale and balance. Nature tends toward balance. Larger ratio's tend to be unstable. For example imagine a rectangle crystal which grew to a ratio of 22:7062. Which would be a constant 321. It would take a very odd molecule to form such a crystal. And should the molecule exist, nature would have to provide space an materials 321 times longer than wide. That's just not how nature tends to work. 1:1 is most common. Any shift from that becomes less and less likely. 

For the second reason you should read about [Benford's law](http://en.wikipedia.org/wiki/Benford%27s_law). Which basically states that lower numbers are more common than higher numbers. And goes even farther to show that even in larger numbers, lower digits are many times more common. There are many theories as to why. My belief is simple. One comes before two. Two is twice as hard to reach as one so one is twice as common. Two is twice as common as four. Etc... 

",null,17,cdom8rd,1rlfdu,askscience,new,37
disconcision,"'the set of important mathematical constants' does not appear to be well-defined. all of the lists provided seem to amount to lists of numbers which particular humans find particularly interesting. is there even a single number on these lists whose membership is uncontroversial? why pi as opposed to tau? if both, why not all rational multiples of pi, eliminating the perceived bias? if you ask a random person to list numbers they find interesting i'd guess that the list would be similarly biased.",null,4,cdoglzb,1rlfdu,askscience,new,21
null,null,null,13,cdon75p,1rlfdu,askscience,new,23
antonfire,"One somewhat reasonable answer (or at least a relevant comment) is that a lot of these constants have have some interpretations as probabilities, and they are probabilities of events that are not all that unlikely. For example, 1/e is roughly the probability that a random permutation of the set {1, ..., n} will not fix an element. 6/pi^(2) is roughly the probability that two random numbers in the set {1, ..., n} are relatively prime. 1/phi is the probability that the sum of a random number in the set [0,1] and its square is less than 1. I suspect you can relate Feigenbaum's constants to some event relating to periods of a dynamical system as well.",null,4,cdolwdp,1rlfdu,askscience,new,13
monstertofu,"If we look at pi, well that's the ratio of the circumference of a circle to its diameter.... in Euclidean space.  It's certainly not the case in a highly negatively curved geometry.  In fact, in a very weird negatively curved geometry, the circumference-diameter ratio could be absurdly large.  Now why don't we live in a world where we see that to be the case?

Now we're running into something that seems like the anthropic principle.  Namely, if the ratio were absurdly large, how would the ancients have discovered this relationship?  Remember even counting up to a large number requires a lot of mathematical sophistication.  One could argue that if the ratio were not around 3 (as with pi) then the relationship would either not have been discovered or it would not have been considered useful enough to merit noting.  

One could argue the same way with e.  So e arose as the limiting value  as one continuously compounds interest on $1 (at a 100% annual rate).  If compound interest behaved in such a weird way that the $1 would become a gazillion dollars under continuous compounding, then it's unlikely continuous compounding would have become a focus of interest to us.  ",null,0,cdoo5xa,1rlfdu,askscience,new,8
ENORD,"These numbers are close to 1 because they are *defined* as ratios of one (in most cases). A ratio is not a number but an equivalence class over pairs of numbers that satisfy the constraints in question (i.e. 1 to pi / 2 to tau are the ratio of the circumference of a circle to its radius/diameter).

A number close to one is chosen because we are fond of multiplying and any ratio represented as some ratio of one (1) multiplied with any number will yield a number that satisfies the constraints of the equivalence class of the ratio in question in a way that is often easy to observe.

Tl;Dr: Numbes are arbitrary.",null,5,cdooypi,1rlfdu,askscience,new,12
nihilaeternumest,"There's a lot of good responses here and this will undoubtedly remain buried forever, but I feel like adding my two cents.

It makes sense that constants would be close to zero or one because those numbers are unique. Zero and one are non-random* numbers. They are the additive and multiplicative identities, respectively. Constants are non-random by definition, so it intuitively makes sense for them to be close to other non-random numbers (ie. 0 and 1).

Obviously, this idea does not hold up to mathematical or scientific scrutiny, but it doesn't need to. ""Close"" is not a mathematical term, so it makes sense for the answer to be non-mathematical. We're talking about how these numbers ""feel"" to us, after all.

\*Because this term is difficult to actually define, I'm cheating a little bit and defining a random number as any number that is a reasonable\** multiple of a mathematical constant.

** Yes, I realize I am defining non-randomness using the concept of non-randomness. This is why this is hard to define.

edit: making \* not make things italic",null,3,cdovupd,1rlfdu,askscience,new,8
adequate_potato,"iCookBaconShirtless' answer is spot-on in terms of us not being able to say that a number is either ""close to"" or ""far from"" zero.

That said, I think your question comes more from these being numbers in the range of 10^0, which are numbers similar in scale to those we deal with on a day-to-day basis. 

The reason so many constants are like this is because they describe relationships between things we deal with - the diameter and circumference of the same circle, for instance. Most of these numbers are small as a result of the components we use to describe them being similar in scale because that's how we've defined them to be.

In fact, the ones not like this tend to be very large or small in magnitude compared to constants on the scale of 10^0. Consider the gravitational constant (10^-11), the speed of light (10^8), Boltzmann Constant (10^-23), and many, many others, all of which relate things of very small scale with things of very large scale. ",null,0,cdok579,1rlfdu,askscience,new,6
null,null,null,1,cdotvlr,1rlfdu,askscience,new,7
fleetingshadow,"I think you're spot on with your hunch actually - 0 and 1 are the most important numbers.  
 
Sqrt(2)? Comes from the hypotenuse of a triangle with short sides both 1 unit in length.  
  
Pi comes from the use of the unit circle as the way that pi is defined - its recipe already has parameters normalized to 1 and 0. (origin 0, radius 1).  
  
In fact, you can view 2*pi as the number of different triangles you can create with a hypotenuse of 1 in two dimensions.  
  
e? Similarly defined. 1+(1/n!)^n  
    
Out of my depth on the Feigenbaum constants...  
  
As for other (typically physical constants), we'vegone through a process over the centuries of using the normalized versions as much a possible - you set what you think the most fundamental constant is to 1, and scale the related ones. Why? It makes the math easier.  
  
So while it doesn't hold for all constants, there is a relationship there - even if we've deliberately created one - and it's all because of normalization.
",null,1,cdojh84,1rlfdu,askscience,new,5
Parametrize,"To add to other people's answers:

How do you define 'close'? We are using the standard metric - the norm on R. But I'll ask you a few questions: Why is it fair to say that 0.01 is just as close from 5 as 9.99 is? Sure, they are the same 'distance' apart... but 0.01 and 5 are orders of magnitude apart while 5 and 9.99 are on the same order of magnitude... (This is why we use the geometric as well as the arithmetic mean). 

On another note:
Some of the constants on the wikipedia page are chosen arbitrarily. Why are we choosing a certain constant over a different constant? And you can find plenty of functions which will give you arbitrarily large constants. (Look up Ramsey theory for a few of them, if you want).",null,2,cdoqdyn,1rlfdu,askscience,new,6
bigbad486,"I may not really be qualified to answer this, but it seems like the people who are more so are being purposely obtuse about the word ""close"".  I think a simple answer might be that many of the important constants are the most basic possible part of a pattern, a common piece of a certain type of equation.  That is to say, they are as small as it is possible to make them, because it makes more sense to multiply them to reach larger values as needed than to divide a larger number for smaller ones.  Like the OP I'm sure that I'm not phrasing this in an ideal way.",null,0,cdokvz9,1rlfdu,askscience,new,3
long-shots,"I think it is because the differentiation among lower numbers (those ""closer to zero"") seems more significant.  Like the distance from one to two is double in quantity, but the difference between one billion vs. One billion and one is almost negligible.  So, finding constants around zero might not be surprising because numbers closer to zero offer greater ""number power"", and by that I mean quantitative significance.   From the standpoint of a philosopher who doesn't do much math, I can't see a better explanation; be warned I am not nearly an expert theoretician in the art of numbryng and, so, these words may be explosive. ",null,1,cdoqbo7,1rlfdu,askscience,new,4
Onechrisn,"It seems like a lot of people are shooting around, but not actually hitting upon, the idea of [Bedford's Law](http://en.wikipedia.org/wiki/Benford's_law). 
Bedford's Law: 
&gt; refers to the frequency distribution of digits in many (but not all) real-life sources of data. In this distribution, the number 1 occurs as the leading digit about 30% of the time, while larger numbers occur in that position less frequently.

Basically, one would expect constants and a lot of other numbers to cluster around 1 and drop off logarithmically from there. In fact, studying logarithms is [how Bedford's Law was discovered.](http://www.rexswain.com/benford.html)",null,2,cdoqo9n,1rlfdu,askscience,new,4
NicknameAvailable,"Disregarding the point of trying to define ""close to zero"" on an infinite scale and taking it for what you seemed to intend:

Because the vast majority of people aren't smart enough to come up with relations that would equate to something larger and those that are have no need to do so as of yet.  Something like a circle's circumferene divided by it's diameter, though useful, is completely arbitrary.  We could have used the radius and it would have been twice the size, we could have used 1/nth the radius and it would be that much larger.  We could do the same sort of thing for multidimensional geometric shapes and the number again would be larger - we simply don't have the need to do so.  You could take the planck length vs the diameter of the universe if you wanted a really big number.",null,2,cdorjy3,1rlfdu,askscience,new,3
professor__doom,"It can be proven, stemming from axiomatic definitions not related to numeric value, that e^pi*i = -1 .  This is called Euler's Identity.

Instinct leads me to believe that in order for this equality to hold to within a given tolerance, the constants e and pi must have values within a certain neighborhood--either relative to zero, or they must at least be reasonably close to each other.  I have not verified this, but I would be willing to bet that this can be shown via numeric methods.

Somebody better versed than I am in numeric methods, please jump in and try it, I'd love to see your findings.

The ""golden ratio"" phi also has an absolute numerical definition in terms of another irrational number: phi=1+sqrt(5)/2.  Using the extremely intuitive inequality 1&lt;sqrt(x)&lt;x for x&gt;1, this definition gives us an obvious set of possible bounds for phi: 1&lt;phi&lt;3

Now there ARE some dimensionless constants with ""real world"" applications that are enormous numbers.  For example, the Eddington Number is the number of protons in the universe--just a number, not expressed in terms of meters or miles or anything like that--and is approximately 137*2^256 .",null,2,cdovit7,1rlfdu,askscience,new,4
ShadowKing94,"A lot of them are ratios (golden ratio, pi).
Ratios tend to be ""smaller"" numbers because they describe the ratio between two things that are related to each other and thus wont be hugely different.

Fe the radius and circumference of a circle have pi as a ratio and not a number in the thousands because they cant differ that much. 

May not be the most scientific answer, but I think its a very valid one. ",null,0,cdowej3,1rlfdu,askscience,new,2
turtle889,"Most people on this thread refer to an arbitrary definition of close as an explanation for OP's question, but I find this unsatisfying. From a mathematical perspective, while the real numbers are uncountable, and so 'closeness' *is* somewhat arbitrary, it doesn't address the fact that 1.618, for example, is a value that explains how much longer one line is than another. These numbers express a relative scaling between two things, and so rescaling these values to a different number system would only change our impression of them, not the absolute ratio. 

For the constants that you mentioned, I suspect their small values result from our **tendency to compare things on the same order of magnitude**. It would be ridiculous, for example, for us to compare area of a circle to a millionth of its diameter. This would in fact change our 'fundamental' value for pi, but it would make no sense. So we choose to compare things that we can relate to each other visually or conceptually - a circle's area to the line that bisects it, for example.",null,0,cdoxczg,1rlfdu,askscience,new,2
SigaVa,"I'll give a perspective from physics:

Dimensionless constants are ratios.  To observe a ratio you have to measure, in some sense, the relative effect of two ""similar"" things; similar meaning having the same units.

Due to our historic inability to measure things of hugely varying magnitudes we tend to measure stuff of around the same size ... stuff on the size scale we can directly observe.  When you then take a ratio of two such measurements, you're going to get a number around 1.

If you look at constants with dimension, there's a huge variation in size.  In our normal units, the speed of light is ~10^8, the charge of a proton is ~10^-19, the mass of an electron is ~10^-30, Planck's constant is ~10^-33",null,0,cdonrj5,1rlfdu,askscience,new,2
kenhill,"I think the question would be better phrased a question of maximizing a function. Given a set I of ""important mathematical constants"" in C and a ball B in C centered at point P of radius r define the density of important constants as the number of elements of I that are also in B divided by the volume of B.  For a given set of constants I what point P and radius r yields the greatest density.  We could also ask, for each element i of I what is the largest ball centered on i that contains no other elements I.  We could also ask, what is the smallest ball that contains all elements of I or any non-empty subset of I.  Lots more questions.  Keep in mind there are a lot of undefined issues still in my questions (what distance metric do we use to define a ball, what function do we use to define volume, etc.)
",null,0,cdonug8,1rlfdu,askscience,new,1
CuriousMetaphor,"The ""number line"" is a way to graph the real numbers while highlighting their properties as a field with the operation of addition.  It goes from negative infinity to positive infinity.  0 is the additive identity, so it is in the middle.  Each number and its additive inverse are equal distances from 0.

But there are other ways to do it.  You could graph the positive real numbers while emphasizing their properties as a field with the operation of multiplication.  This alternate number line would go from 0 to infinity.  The number 1, being the multiplicative identity, would be in the middle, and each number and its multiplicative inverse would be equal distances from 1.

Neither of these number lines is more ""correct"" than the other.  They are both graphical representations of an abstract concept.

If graphed on the second number line, all the constants you mention are on the right side so are ""closer"" to infinity than they are to 0.

Usually, there is some latitude when choosing a mathematical constant.  The constants you mentioned could all be replaced by their multiplicative inverses without becoming less ""fundamental"".  For example, 3.142 is the ratio between a circle's circumference and its diameter, but 0.3183 is the ratio between a circle's diameter and its circumference.  The reason we choose numbers greater than 1 rather than numbers less than 1 as constants probably has to do with the fact that most of the numbers we are most familiar with, the natural numbers, are greater than 1.  Also, positive numbers are easier to work with than negative numbers.

Another point: ""close"" doesn't mean anything unless you are comparing it to something else.  Number lines are one-dimensional objects and so they look exactly the same when zoomed in or out.  If you draw an (additive) number line with tick marks every 10 units, then yes those constants seem close to 0.  But if you draw the same number line with tick marks every 0.01 units, those constants suddenly seem very far from 0.  

There are equal numbers of real numbers in any two segments of the number line.  You can draw a one-to-one correspondence between them.  For example, there are exactly the same number of real numbers between 0.2 and 0.3 as there are between 1 and infinity.",null,1,cdoosdu,1rlfdu,askscience,new,2
Nantosuelta,"Also, female [Eclectus Parrots] (http://en.wikipedia.org/wiki/Eclectus_Parrot) are scarlet and purple, while males are a uniform green. There are many species of birds where females are larger than males (especially among birds of prey, shorebirds, and some flightless birds), but phalaropes and eclectus parrots are the only ones I can think of where the females are *brighter*.",null,0,cdoik0f,1rlg2l,askscience,new,5
monster_cookie,Yes. In the genus *Phalaropus* females are bigger and more brightly colored. This is because they are polyandrous (one female mates with multiple males) and males provide most parental care. So females have to look good while for males is better to not be detected by predators. ,null,1,cdogslb,1rlg2l,askscience,new,5
99trumpets,"One other example (in addition to the phalaropes and parrots already mentioned) is the belted kingfisher. That's the kingfisher that's commonly seen zipping around suburban lakes in North America, the one with the rattling call. [Females](http://www.shutterpoint.com/photos/T/566377-Female-Belted-Kingfisher_view.jpg) have a red belly band, [males](http://www.birdwatchersdigest.com/site/resizedImages/20070521113133_kingfisher.jpg) do not.
",null,0,cdopb6u,1rlg2l,askscience,new,2
iCookBaconShirtless,"Mandelbrot sets are not actually very mathematically significant in the sense that they don't help us solve many (if any) mathematical or physical problems that arise naturally.  [EDIT: Experts in fractal geometry, please correct me if this is wrong.]

They are mostly famous for historical reasons and because they're really cool: they were some of the first examples of sets that are defined by very simple rules, but have very complex properties.  The discovery of such sets (and also functions with similar properties) spawned the study of chaos theory and fractal dimensions, which have since grown to be extremely significant fields of mathematics (especially the former).  Many similarly complicated sets and functions which can be defined by simple rules have been discovered and intensely studied since the discovery of Mandelbrot sets.  Some of these have important applications/consequences in other fields of math and in the sciences.",null,9,cdogzh2,1rlhst,askscience,new,37
KerSan,"I always find this kind of question interesting. It is a question of the form ""X mathematical concept is cool and all, but why does anyone care?"" I find this interesting because no one ever asks this about art, or television, or even astronomy. Everyone seems to think that mathematics is supposed to have some purpose outside of pure enjoyment, even though they do not apply those same rules to all sorts of other human endeavours. I have yet to come up with a good explanation for why this is the case.

Nevertheless, it just so happens that the Mandelbrot set is extremely significant in a way that no one in this discussion seems to have realized. As /u/AsterJ [points out](http://www.reddit.com/r/askscience/comments/1rlhst/why_are_mandelbrot_sets_so_significant_what_can/cdop10f), the Mandelbrot set is the collection of points in the complex plane whose associated Julia set is connected.  This statement needs a bit of explanation before I can discuss its significance.

A Julia set is, to put it almost too simply, the collection of chaotic points of a given rational function of complex numbers. It's best to understand this by example. [This famous image](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) shows the Julia set for the function f(z) = z^3 - 1. The idea is that the non-coloured points bounce around in a chaotic fashion under iteration of this map. The Mandelbrot set is the set of points c such that the Julia set for f(z) = z^2 + c is connected (a technical term that means roughly what you think it means).

I don't know your level of education so I'm not explaining further because I don't want to condescend. This information is on Wikipedia and in books, so you can take the time to understand these concepts a little better if you're interested. You didn't ask for an explanation of the Mandelbrot set, after all, so I'm assuming that you've thought a bit about it. I'm here to answer to the significance of the concept.

[This image I linked before](http://en.wikipedia.org/wiki/File:Julia-set_N_z3-1.png) is significant for the validity of the Newton's method for finding the roots of a polynomial equation -- specifically, z^3 = 1. When you seed Newton's method with an initial guess, which root will the method eventually approximate? The answer depends, of course, on where you started. What is surprising, at least at first, is that the root you find might be quite far away from your initial point, even when there are other perfectly reasonable roots much nearer. In other words, this Julia set is telling you about when and how to use Newton's method, and what kind of things you should worry about. The Mandelbrot set is an example of how to classify Julia sets, which in turn helps you extract useful information from a given Julia set.

More generally, the value of so-called chaos theory (for which the Mandelbrot set is rather foundational) is, to me, in analyzing the performance of numerical algorithms. Describing the real-world uses of numerical algorithms is about as difficult as describing the real-world uses of writing.

Edit: some grammar and other minor fixes",null,1,cdoykpe,1rlhst,askscience,new,7
EvOllj,"fractals and recursive functions allow a line to have near unlimited length within a limited area, the length only limited only by the possible resolution and possible accuracy to display/measure. They are pretty uniform where they are recursive. There are some geometrical and symmetrical properties interesting for some math and physics.",null,7,cdojbqs,1rlhst,askscience,new,5
cowboysauce,"Iron doesn't kill stars, it's just that stars cannot use iron as fuel because the fusion of iron doesn't release energy. When iron accumulates in the core of a star, it means that the star has run out of fuel.

It's a similar relation to ashes and a fire. Ashes aren't what kill a fire.",null,1,cdoga2p,1rljcb,askscience,new,37
drzowie,"The Sun has a significant amount of iron in it already! [The solar corona is about 0.01% iron by number](http://arxiv.org/pdf/astro-ph/0004007v1.pdf) (about 0.5% by mass), and the deep interior is within a factor of 4 of that number.

In other words, the Sun may already contain as much as 10^28 kg of iron.  That's a Hell of a lot of iron.  (Consider that the Earth only has 6x10^24 kg of stuff total -- so the Sun contains literally thousands of Earth-masses of iron).

But the Sun keeps chugging along just fine.

The fact that there's so much iron in the Sun, but it isn't even close to out of fuel, implies that it's a second-generation star:  the iron must have been present in the material from which the Sun was made, because it's hard to make a lot of iron in a star that is still 'burning' hydrogen and helium -- the core just doesn't get hot enough to drive the fusion reactions that make iron.  

We can also tell that the Sun is second-generation, because we have lots of iron, enough gold to use it for jewelry, and small quantities of exotic fissile materials right here on Earth.  Those heavy elements are only produced in large quantities by endothermic fusion in supernovae or by neutron capture in very metal rich stars.  Hence the accretion disk that became the solar system must have *already* been processed through the interior of a large, metal-rich star.  

So the facts that your car is made of steel (because iron is plentiful and therefore cheap), your wedding band is made of gold (because gold can be found in small but non-microscopic quantities), and we have a nuclear proliferation problem (because there are fissile elements on Earth) all stem from the same root cause -- as Carl Sagan pointed out, ""We're all starstuff"" that must have at one time been in the middle of a supernova.

But the titanic quantity of iron already in the Sun isn't quenching fusion, because there's still plenty of energy to be had by fusing light elements together.  The core is thought to be about halfway through its initial hydrogen load.  ",null,0,cdopgyd,1rljcb,askscience,new,10
inventor226,"There are several factors here.

1) The mass of the star

2) The age of the star

3) The mass of the iron (to have any effect it needs to be a significant fraction of the stars mass)


But if we are taking about pieces of iron small enough to be controlled by any traditional scifi civilization it will have no effect. Stars produced today are not pure H/He, they are some fraction metal (in astronomy if it isn't H/He it is a metal). They have something called a [metallicity](http://en.wikipedia.org/wiki/Metallicity), a ratio of some element to another (usually Fe to H).",null,1,cdogg18,1rljcb,askscience,new,8
pigeon768,"If you were to inject a [1.44 solar mass](https://en.wikipedia.org/wiki/Chandrasekhar_limit) slug of iron into a star's core, and were able to displace anything else that might be potential fuel, it would die. The iron would collapse into a neutron star, and the rest of the star would likely go supernova. The entire thing would either collapse into a black hole or add to the mass of the ""seed"" neutron star.

Injecting 1.44 solar masses of iron directly into a star's core is a difficult engineering problem.",null,2,cdolehb,1rljcb,askscience,new,8
Henipah,"Plastics are polymers of very long molecules bound by covalent bonds. Ionising radiation can break these bonds. Metals have a completely different molecular structure, a bunch of nuclei in a sea of electrons. If you bash a metal it just deforms but stays together. ",null,0,cdojqjl,1rlu8n,askscience,new,5
Platypuskeeper,"Metals don't degrade because of the nature of how metals work. The valence electrons (the outmost ones who do bonding) are free to move about within the metal. In plastic, the stuff is made up of molecules, and the electrons aren't bound to their places(bonds) within the molecule. A UV excitation with break the bond, and then the molecule comes apart. 

In a metal, the atoms are packed much more densely in a crystal lattice. And since the electrons are are free to move about, if an electron is excited by UV (even to the extent of leaving the metal), then other electrons can quickly take their place. Removing a few electrons temporarily doesn't necessarily break any bonds, and the atoms can't move even if you do temporarily break a bond to it, as it's tightly packed to it's neighbors. 

If you break the C-C bond of a carbon chain in a polymer though, there's smaller chance of the two atoms finding each other and managing to bond again, as they're free to move about.
",null,0,cdorfor,1rlu8n,askscience,new,3
joca63,"From what I've read in the comments, the simplest answer is touched upon, but is largely missing.

Rubbers and plastics are made of very long carbon chains. This is actually a fairly unfavoured state for the system, entropy would prefer short chains. So when they are hit eith enough energy to break bonds they will tend to make smaller chains. It happens that UV light has enough energy to do this, leading to slow degradation of the plastic.

Metals on the other hand can't really be though of as having bonds. At least not covalent bonds like the ones in rubber and plastic. So first of all there is no real bond that can break. Secondly metals exist as a lattice. This is the lowest energy state for an elemental metal at room temperature and presssure (save mercury), so there is no more favoured state for the metal to be in.

Another effect that happens particularly with aluminum is the creation of a passivation layer. With the plastic, the molecules are exposed directly with air, allowing reaction with the air itself. (in theory if you leave plast long enough in air it would turn to carbon dioxide. it would just take an insanely long time because of the activation barrier) The instant a metal such as aluminum is exposed to air it forms a very thin layer of aluminum oxide which prevents further reaction. ",null,0,cdq1obi,1rlu8n,askscience,new,2
NotFreeAdvice,"I would like to add to this one point that is missing so far.  

Metals (because they are great electrical conductors) do not allow for deep penetration of light into their bulk.  Thus, the interaction of metals with light (such as UV) usually take place at the surface.  

What this means is that, if there were to be any light-initiated chemistry, it would also need to take place at the surface.  However, most metals are pretty reactive, actually, and their surface will already be coated with a number of molecules (or -- even more likely -- will have formed an oxide layer).  These molecules then protect the surface of the metal from further reaction (most of the time).  

At any rate, this is not *universally* true, and the other explanations below provide more fundamental reasons why metals are not reactive under UV-light, but I thought it would be interesting to point out a more practical reason as well.  ",null,1,cdowsew,1rlu8n,askscience,new,2
bipnoodooshup,"It is my uneducated understanding that any organic substance is susceptible to ultraviolet rays if it does not use them to produce food (ie photosynthesis). That's why we get skin cancer, plastics get weaker, pigments lose their vibrancy, etc. Meanwhile metals seem to only be susceptible to other elements. Mercury eats aluminum",null,6,cdoj942,1rlu8n,askscience,new,3
afcagroo,"When video is slowed down, you just see fewer still frames per second.  Each frame is imaged the same way as before. The motion has been sampled, essentially.   
  
Music is slowed down for real (in the analog domain), so a given pitch changes at a slower number of cycles per second (Hertz). ",null,0,cdojpxg,1rluan,askscience,new,5
selfification,"Color does change when you change speeds.  It's called relativistic doppler shifting.  It just works in a different order of magnitude and also slightly differently because sound works with a fixed medium.  We can actually tell how fast things are moving towards or away from us based on how much their colors are shifted from what their supposed to be.  Hubble's law is based off of such measurements and the age of the universe is derived for this principle.

Oh wait, you meant when a video is slowed down...  well in that case /u/afcagroo has the right answer.  Also note that if you did digital music playback where the music is actually not time-stretched but instead chopped up and replayed at regular speed, you'll hear no pitch change.  You'll instead here very choppy audio of the right frequency instead (which is what you're seeing in video - more choppiness).",null,0,cdozyme,1rluan,askscience,new,4
iorgfeflkd,"You'd start accelerating until you reached the center, then start decelerating until you reach the other side. You'd get to about 8 km/s in the middle, and the whole trip would take about 38 minutes. 

If you controlled your descent so that you're weren't accelerating the whole time, and there was some kind of hollowed out cave in the center, you could conceivably float there.

You can read more about the details [here](http://arxiv.org/pdf/1308.1342.pdf)",null,3,cdol1lz,1rm1ho,askscience,new,18
LibertasEtSerenitas,"Whoa, hold on here.  I'm having trouble with some air resistance calculations. If you were to assume the air in the hole is not affected by the heat of the earth, e.g. insulated adiabatic tunnel walls, isn't it still super hot because of the compression from the weight of the air above it?  Not only is it high pressure air, but also high temperature?",null,1,cdopqex,1rm1ho,askscience,new,1
listens_to_galaxies,"The calculation of electron speeds in a conductor is a pretty classic exercise in electromagnetism.  It depends on the current (charge per second) passing through your conductor/cable and the cross section (thickness) of the cable.  More current requires the electrons to move faster, while thicker cables hold more electrons so they don't need to move as fast.  The formula, and a sample calculation, can be found [here on Wikipedia](http://en.wikipedia.org/wiki/Drift_velocity).  Bear in mind that this assumes DC current; for an AC current the electrons will oscillate back and forth.

For typical currents of a few amperes, and typical wire thicknesses, electron speeds are generally on the order of millimetres per second.  So it would take on the order of hours (1000s of seconds) to travel distances of ~6 feet (a few meters).

This may seem really weird, because when you flip a light switch or plug in a device, it works instantly rather than taking hours to turn on.  This is because the signal is carried by the electric field that moves the electrons.  The electrons themselves act more like carriers of the signal/power, rather than the source, just like sound waves can propagate through air or water much faster than the individual air/water atoms.  The electric field propagates at the speed of light, so there isn't a noticeable delay between plugging something in and it working.",null,1,cdou7vk,1rmgbh,askscience,new,8
fizixx,"It takes a LONG time. Many people think it travels at the speed of light, which is not the case. I did a calculation many years ago, based on my guess at how far (how much wiring) extended from a light switch to a fluorescent light in the room. It came out to almost 40 hours for an electron leaving the switch and traveling through all the wiring to the light.",null,0,cdpl7aa,1rmgbh,askscience,new,1
paolog,"Things that happen inside the eye as seen by the owner of the eye, such as [floaters](http://en.wikipedia.org/wiki/Floater), for example. Of course, these could be captured by a camera by viewing the inside of the eye, but they aren't captured as seen by the person.",null,0,cdougep,1rmgp4,askscience,new,7
FriendlyCraig,"There are certain optical illusions which are lost when photographed, although it's more the brain seeing than the eye. If you ever look at a picture of the moon near the horizon, it looks, awfully small while in person it's huge.",null,1,cdp2tu5,1rmgp4,askscience,new,3
samloveshummus,"Both dinosaurs and mammals are examples of clades, groups of organisms defined by a common ancestor, so that much is correct. As to your specific example, Stegosaurus belongs to the family Stegosauridae while Velociraptor belongs to the family Dromaeosauridae while both humans and chimps belong to the family Hominidae, so we're much closer than a typical pair of mammal species. Moreover, while chimpanzees and us are contemporaries, Stegosauruses were as ancient to Velociraptors (another 75 million years) as Velociraptors are to us (75 million years old).",null,0,cdovp8g,1rmgph,askscience,new,4
stuthulhu,"Dinosaurs have distinguishing physical characteristics and common ancestors as you can see [here](http://en.wikipedia.org/wiki/Dinosaur#Definition)

Interestingly, this includes birds, and excludes other things commonly thought of as dinosaurs, for instance [plesiosaurs](http://en.wikipedia.org/wiki/Plesiosauria), [icthyosaurs](http://en.wikipedia.org/wiki/Ichthyosaurs) and other marine reptiles, flying reptiles such as [pterosaurs](http://en.wikipedia.org/wiki/Pterodactyls), or animals such as the [dimetrodon](http://en.wikipedia.org/wiki/Dimetrodon) which were actually more closely related to mammals.",null,0,cdoz6xi,1rmgph,askscience,new,2
patchgrabber,"If you're asking can wind make water freeze, the answer is no, unless the air temperature is below freezing. For inanimate objects and things like water, the only thing wind does is cool its present temperature to ambient air temperature. It cannot cool anything more than the ambient temperature.",null,0,cdoyf5b,1rmgtz,askscience,new,1
TangentialThreat,"Your basic fan can move heat around in a room, but it won't produce temperatures below ambient unless you happen to have chosen to point it at something like a sweaty human or a bowl of water AND the humidity is not 100%.

Evaporation will produce a cooling effect. The most energetic water molecules get kicked out, leaving behind low-energy ones. Near standard conditions this is mild and will not freeze a bowl of water in a warm room, but [here's a paper](http://www.sciencedirect.com/science/article/pii/S0017931003000723) saying you could produce ice this way by drying incoming air with desiccant. Taken to its limit and using a fluid other than water, this is also close to an adsorption heat pump.
",null,0,cdozk2p,1rmgtz,askscience,new,1
fizixx,"I think you have to have a certain initial set of conditions, but.....yes, you can. 

High speed air moving pas a vessel of water can reduce its temperature to freezing. The fast air reduces the pressure in its vicinity. Based on the Idea Gas law, that means the temperature will also go down.",null,0,cdpleq7,1rmgtz,askscience,new,1
TITS_ME_UR_PM_PLS,"Well, [Chile](http://earthquake.usgs.gov/earthquakes/world/events/1960_05_22.php) had a magnitude 9.5 in 1960.

Here's the thing: Richter magnitude [was originally developed](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Development) for a tiny part of California, on a specific type of instrument (a Wood-Anderson torsion seismograph). It's been adapted for use elsewhere, using other instrumentation, based on total energy release.

Note from [here](http://en.wikipedia.org/wiki/Richter_magnitude_scale#Examples) that the highest recorded incident- the 9.5 in Chile- released 11 EJ (exojoules) of energy, equivalent to the release of 2.7 gigatons of TNT.

A 10.0 on that scale would be 63 EJ, or 15 gigatons of TNT, and that one has ""Never [been] recorded, equivalent to an earthquake rupturing a very large, lengthy fault, or an extremely rare/impossible mega-earthquake, shown in science fiction.""

Anyway- depth is a big consideration here, as a fault that is close to or at the surface will release energy more efficiently to us surface-dwellers than a very deep one, [such as this 8.3 recorded at 378 miles below the seafloor.](http://www.livescience.com/34671-russian-earthquake-deepest-ever.html)",null,1,cdp49vq,1rmj44,askscience,new,6
dakami,"At a certain point, the earth can't move any more energy, and so Richter intensity starts referring to time.  So, maybe an M7 that goes on for minutes and minutes?  (I live through the SF quake of 89, that 7.1 lasted for but 18 seconds.)",null,2,cdozu2t,1rmj44,askscience,new,5
chuck10470,"The Richter Scale is exponential. A ""1"" level increase,  say from 7 to 8 is actually 10 times more powerful in terms of total energy release. A 9 would be 100 times as strong as a 7. A 10 on the scale is a staggering amount of energy. But this isn't solely what makes earthquakes dangerous. Focus depth, the distance beneath the surface that the epicenter is located is just as important. Shallow quakes deliver more of that energy to the surface in a concentrated area,  so it is possible to have more damage from a weaker quake. ",null,0,cdpeuf8,1rmj44,askscience,new,2
MasterPatricko,"It's not the fact there are electrons moving that's important, it's that *electromagnetic charge* is moving. Any moving/changing charge (electrons, holes, ions, protons...) is a current and is associated with electric and magnetic fields.

There are other types of charge - colour, weak force stuff - but those forces have a different structure (not inverse square law) so there is no analogy with electromagnetic forces. 

There is a particular view of gravity (which is inverse square law) which allows you to write it analogously to EM -- where mass is the charge -- and there you can talk about gravitoelectric and gravitomagnetic fields. If I remember correctly that's only an approximation to full GR though.",null,0,cdow50o,1rmjmp,askscience,new,5
Dannei,"Any charged particle is affected by the laws of electromagnetism, and so can form electric currents and will be affected by (or generate) magnetic fields.

(However, you can't just start throwing protons down the wires into your lightbulb - it's a bit more complicated than that!)",null,0,cdouxgs,1rmjmp,askscience,new,2
iorgfeflkd,"There are ionic currents based on charged atoms. In your body, nerve signals are conducted by having ions move in and out of cells. This produces an extremely small magnetic field that can be detected in magnetoencephalography.",null,0,cdoxke2,1rmjmp,askscience,new,2
MasterPatricko,"Jello is a [gel](http://en.wikipedia.org/wiki/Gel), a substance which is a fluid trapped in a network of bonds.

Basically it's a solid until you put a force on it, when it flows like a liquid.

A similar concept is a paste.

There is no requirement that mixtures/suspensions like gels and pastes fall neatly into 'solid' or 'liquid'.",null,0,cdowkbh,1rmloo,askscience,new,2
fizixx,Depending on the concentrations I would say yes. The sugar needs proximity to the water molecules to dissolve. If there are other particles in the water (salt) the sugar would not have the frequent/constant contact with the water as it would otherwise.,null,0,cdplrgf,1rmn9e,askscience,new,1
joca63,"In general, no it wouldn't. For the concentration of one solute to effect another they must have a common ion. Since salt only gives sodium and chloride ions it wouldnt have any effect on the ability for sugar to dissolve. There may be some supersaturated case where this is not true, but I don't know of any.",null,0,cdq0p5u,1rmn9e,askscience,new,1
CosmotheSloth,"A lot of biochemists nowadays turn to computational chemistry methods to obtain potential molecules for drug uses.

Large libraries of different molecules are searched by different methods to give results that best match your requirements. You can search via different criteria; by looking at current drugs that are in use or by looking for specific functional groups or shapes of molecules, for example. The main methods of molecular modelling are docking or pharmacophores.

Pharmacophores are computational descriptions of molecules that specify the positions relative to one another of the main functional groups in space.

Docking is a method use to see if the various orientations of a known molecule have a preferred fit with a desired receptor molecule, much like the lock and key model of enzymes and substrates in biochemistry.

More simulations are then used to determine whether the synthesis and testing of the molecule (drug) is viable financially (as essentially all computational chemistry and pharmaceuticals comes down to money!).

Source: 3rd Year Degree Level Chemist remembering notes from last years Cheminformatics module.",null,0,cdovhni,1rmnfn,askscience,new,6
rupert1920,"An example of drug development is this:

We have a known site we want to bind to - usually it's a receptor, sometimes it's a protein binding site. To find a drug candidate, one uses computational chemistry: DOCKing studies uses a huge library of potential drug candidates, and examples how well they bind to the site we're interested in. Based on this data, we have a list of molecules that bind the strongest.

The next step is where medicinal chemists spend most of their time: to make a drug candidate into a successful drug, one must make sure that it makes the transition from ""the computer says it binds strongly"" to ""it will make it through your body's metabolism and actually do its job"". Often time, many drug candidates work well _in vitro_, but fails _in vivo_. Even if they seem to work well in animal models, most failures occur in Phase II clinical trials, when the the candidate was shown to be no better than placebo.

So the medicinal chemist is tasked with modifying your drug candidate to survive the journey to the site of action. This is often done by replacing functional groups we know will undergo chemical changes - for example, an orally administered drug shouldn't contain ester groups, as they are easily hydrolyzed in the stomach. They will replace this with a non-hydrolyzable group, such as a straight chain ketone, which retains the carbonyl group but does away with the other oxygen.

Other replacement strategies are more complex. Peptides, for example, are easily hydrolyzed by peptidases, but depending on the conformation, the backbone can be completely replaced. One strategy is using aryl chains as a mimetic for alpha helixes.

In short, there are many considerations just for addressing the pharmacokinetics of a drug. A general rule for a drug is [Lipinski's rule of five](http://en.wikipedia.org/wiki/Lipinski%27s_rule_of_five), which lists some general characteristics that'll make a drug lipophilic enough to be absorbed easily, and hydrophilic enough to be distributed easily.

There can be other changes one makes to attempt to increase binding with the site. For example, knowing the landscape of the receptor, one make attempt to introduce further interactions by tagging on extra groups onto your drug candidate. For example, if a binding site has an adjacent pocket formed by amino acids with hydrogen bond acceptors, you can add hydrogen bond donors onto your molecule to attempt to increase the interaction.

To address your last part question: no, not all drugs are synthetic copies of a biologically active chemical. There are many drugs that have their own, unique interaction with the body. For example, the classic chemotherapy drug [cisplatin](http://en.wikipedia.org/wiki/Cisplatin) doesn't work by mimicking a molecule in the body - it works by chemical reaction with DNA.

That said, from the sample description of a drug development above, you can see how the strategy would _lead_ to a compound being chemically close to the natural compound it's attempting to mimic.",null,0,cdowocn,1rmnfn,askscience,new,3
System09,"Things vibrate differently due to 3 things: their shape, material they are made of and forces acting on them.

 Calculating these modes of vibration isn't easy and even simple shapes, like a sphere, can vibrate in many complicated ways. The sound you hear is a combination of these modes, with primary being most important, as it has the highest amplitude. 

Easiest thing to imagine is a string on a guitar. Different thicknesses give different sounds. Using your finger, to shorten the cord, also gives a different sound. You can also tune the strings by adjusting the tension in the head of the guitar. ",null,0,cdovo20,1rmnul,askscience,new,3
natty_dread,"&gt; What would I be looking at? 

The thing is, that the limit is not a matter of precision of the microscope. It is inherently impossible to ""see"" atoms.

The reason for this is, that the wavelength of visible light is ~400-700 nm = 4 x 10^-7 - 7 x 10^-7 m.  
Atoms have roughly the size of ~1Å = 1 x 10^-10 m. 

As you can clearly see, the shortest wavelength of visible light is still more than 3 orders of magnitudes bigger than an atom. Thus, visible light will never be able to represent Atoms.

What we can do, and what we are already doing, is using particles of shorter wavelength than visible light, like electrons.

This will result in an representation of Atoms in visible light.

&gt; If I zoomed into an electron when would I be able to tell it is made up of quarks? For that matter what would the inside of an electron really look like if I dissected it.. 

Electrons are not made up of quarks. In fact, as far as we know, electrons are not ""made up"" of anything. They are fundamental particles.  
According to the standard Model of Particles there are three kinds of fundamental particles:  

*  Quarks  - Those are the particles that make up protons and neutrons

* Leptons - The most famous of which is the electron. There are also neutrinos etc...

* Gauge Bosons - Those are force carriers

(And then there's the Higgs Boson which is a whole other story)",null,0,cdow6ua,1rmo1g,askscience,new,5
chrisbaird,"First, of all, atoms are too small to see using visible light. The wavelength of visible light is simply too large to interact with single atoms in a predictable, individual way. But that does not mean we can't ""see"" them.

Secondly, atoms are fuzzy quantum clouds of wavefunctions and don't have hard boundaries, so you can't see them in the sense of seeing a sharp silhouette or a distinct surface. It's more like seeing a cirrus cloud in the sky. You can see where the cloud is and where the cloud isn't, but the *size* and *shape* of the cloud depends somewhat on how you define the border of the cloud, how you tweak the contrast in your image, and how you calibrate your camera.

With that said, we can see atoms by bouncing electrons off of them instead of light:

http://www.research.ibm.com/articles/madewithatoms.shtml#fbid=i02OkJ97gWy

http://researcher.ibm.com/researcher/view_project_subpage.php?id=4251

http://education.mrsec.wisc.edu/background/STM/images/stm17.jpg

http://images.iop.org/objects/phw/news/13/8/19/afm1.jpg

Also, electrons are fundamamental according to our current knowledge. They are points particles composed of nothing else and can't be dissected.",null,0,cdpovwy,1rmo1g,askscience,new,1
yeast_problem,"The answer is resistance to metal fatigue. A single strand could have a defect  in the crystal structure causing a point of weakness which will spread, while the probability of multiple strands having a defect in the same location is very low.

Stranded cables are more flexible for the same diameter.
",null,1,cdow0m8,1rmpm7,askscience,new,13
Benginieur,"Tensile strength in engineering is one-directional and means there is no bending influence. It depends on the material and is proportional to the area of the cable. So a group of thin cables twisted together will be weaker, because there will be small gaps between them that don't add to the tensile strength.

Now for cutting foam the force from pressing the wire against it will result in a change in force distribution inside the cable with compression on the side of the foam and tension on the other side of the cable with a neutral fibre in the center (http://toolboxes.flexiblelearning.net.au/demosites/series10/10_01/content/bcgbc4010a/04_struct_members/01_beams/page_003.htm).

A single strand will be more stiff as the diameter factors in heavily (Try bending a ruler along both axis and you'll see the effect. Same area but very different flexibility.) So, in my opinion, the reason to use many twisted cables would be if the bending of the single strand would plastically deform it near the edge, where forces are strongest, and thus destroy it over time. Thin twisted cables should distribute the forces more evenly, but they won't be stronger in general with the same diameter.

",null,0,cdp7q90,1rmpm7,askscience,new,4
WorldIsImagination,"I would think a single strand would be stronger in theory and here's why:

If you were to set two metal rods down on a piece of concrete, one twisted and one regular, and you set a rock on both of varying weights until one broke, the spiral would break first. The weight of the object bearing down on the spiral would crack one of the thinner strands before the single strand. ",null,0,cdpeqfx,1rmpm7,askscience,new,1
walexj,"If you consider a bundle of untwisted small diameter wires which together form the same overall diameter of a single wire of the exact same material, they're strength will be equivalent. The reason that things like suspension bridges and other cables with tensile loads are made of several strands is due to the likelihood of failure. A single wire design with a safety factor of 1.5 may have a material defect that can propogate through the material and cause a catastrophic failure. The multi-wire cable of the same overall diameter may have a single strand with a material failure that causes the single strand to fail, but then you still have 99/100 other strands to take the load. Because your safety factor was 1.5, your cable is still safe by a factor of around 1.49.

Things get a little more complicated when you twist, or braid, the wires as that introduces things like bending stress. But overall, they're pretty close to being similarly strong and can handle the same tensile stresses. The difference is that multi-wire cables are tougher and more reliable. They're also way easier to manufacture as well, so win-win.",null,0,cdpq4ey,1rmpm7,askscience,new,1
arble,"Combustion requires a source of heat. In an engine, there's easily enough heat for everything to combust but there often isn't quite enough oxygen. Outside of the engine, there's plenty more oxygen but there's no longer the heat, so the partially combusted products remain as they are thereafter.",null,0,cdotbcr,1rmpo9,askscience,new,5
arble,"Ozone is made in the upper atmosphere by the interaction of normal diatomic oxygen (O*_2_*) with ultraviolet light. The light splits apart an oxygen molecule to form two highly reactive oxygen radicals which can attach themselves to an O*_2_* molecule to form O*_3_*.

In the lab, ozone can be made a number of ways, but the most common is usually exposing oxygen to a strong electrical discharge.",null,0,cdotlk7,1rmug0,askscience,new,3
rocketsocks,"Ozone is the product of almost any energetic reaction involving Oxygen. A common way of producing ozone is to simply run a spark of electricity through the air. Ozone is also a common byproduct of ordinary combustion, as in an automobile, though catalytic converters and electric engine control reduce ozone production substantially.",null,0,cdotpqq,1rmug0,askscience,new,2
patchgrabber,"I'm not sure about a chemical reaction, but a tesla coil produces a decent amount of ozone, you can even use it to decontaminate water (lots of water treatment is done with ozone). Laser printers also produce smaller quantities of ozone through those electrical voltages as well.",null,0,cdovql2,1rmug0,askscience,new,1
NeverQuiteEnough,"followup question, at my school we have a physics club and we were operating our tesla coil as part of a demonstration for some kids.

after a few hours it stopped working.  I suspected it had reacted with all of the nearby oxygen, turning it into ozone.  Told our captain, put a fan on it and it worked when we turned it back on.  Was my guess likely to be right?",null,0,cdowyg5,1rmug0,askscience,new,1
Platypuskeeper,"This guy copy-and-pasted this [from here](http://www.syhdee.com/repositoryx-24-103-1.html) and his submissions history consists entirely of links to that website.

",null,0,cdp8dao,1rmvtm,askscience,new,4
rohrspatz,"The main problem is that MHC signaling also involves a *lot* of ""co-stimulation"". The T cell has to be able to bind to a lot of different cell surface receptors at the same time in order to be properly activated. ""Loose"" MHC floating around in the intercellular spaces wouldn't provide the necessary stimulation. 

Additionally, CD8 T cells trigger apoptosis by releasing chemicals directly onto the surface of the cell they're bound to - so even if loose MHC could stimulate them, they'd cause a lot more local damage than normal, because they would just be indiscriminately releasing toxins into the environment instead of ""targeting"" tumor cells. You'd basically end up with some kind of big ulcerated hole, not a neat little swiss-cheese with all the cancer gone and the normal stuff preserved. They would also be unable to seek out tumor cells that weren't in the local environment where the MHC had been injected, so the efficacy would still depend on the doctor being able to find and manually target every last cancerous cell.

It really wouldn't be any more effective than just cutting out the tumor mass or injecting drugs into it, because all of the limitations that make tumors ""inoperable"" would also apply to the hypothetical MHC injection treatment.",null,0,cdp430s,1rmw8k,askscience,new,2
SquirrelSoul,"I assume you are referring to the fact that birds/reptiles, and amphibians expel fecal matter and urine through a single opening - the cloaca, while mammals, with the exception of the monotremes and some marsupials, have separate openings. (Interestingly, the cloaca is also used for fertilization during sexual reproduction in most birds).

In biology we distinguish between ""elimination"" which is for substances that never cross a membrane and are expelled as feces, and ""excretion"" which is for substances that do cross a membrane and are expelled as a nitrogenous waste such as urine.

Reptiles (which includes birds) actually do have an excretory system. They have kidneys which direct uric acid to the intestinal tract via ureters. That waste then exits through the cloaca along with the feces - so effectively, reptiles practice elimination and excretion simultaneously.

Why do they do this instead of the way we do it? 

Essentially, the answer is evolution. Mutations for the creation of two exits occurred by chance in mammals, were passed on to offspring, and were selected for. The cloaca still worked just fine for birds and reptiles though so it wouldn't disappear entirely.",null,2,cdp5cr4,1rmyt8,askscience,new,5
dukwon,"It's Lorentz force at its simplest.

Charges moving perpendicular to magnetic fields (that little silver thing at the bottom is a magnet) will experience a force perpendicular to both the motion of the charge and the direction of the field.

Let's say that the magnetic field lines go parallel to the battery, then the current flowing through the straight sections of the wire at the bottom is perpendicular to the field, so the moving electrons experience a horizontal force perpendicular to the wire, causing it to rotate.

There will be some magnetic field induced by the rotation, but this will oppose motion",null,0,cdp3qlq,1rn09r,askscience,new,9
Platypuskeeper,"The activation energy is a [saddle-point](http://www.chm.bris.ac.uk/pt/harvey/elstruct/pics/pot_surface.jpeg). It's the highest point of the total energy when going from reactant to product through the path that requires the least energy. That includes all forms of energy in the system, but not all forms of energy in the system will necessarily change with the reaction coordinate. 

In practice, the 'reaction coordinate' is an abstracted form of the actual spatial coordinates of the atoms as they move from the reactant structure to the transition state to the product. So the form of energy required to overcome a transition state barrier is the kinetic energy of the atoms. 
",null,0,cdp6ak7,1rn24f,askscience,new,3
lokim,"I wrote a reentry simulator in javascript some years ago. It mainly deals with the ballistic decent in an atmosphere, http://mdj.dk/project/atmospheric-reentry-simulator/


I just realised that the angle of decent is not a configurable variable in the simulator, I might add it if you want to play around with it to see why survival depends on the right decent.

In practice there are many factors that can determine the landing point, have a look at http://www.dtic.mil/cgi-bin/GetTRDoc?AD=ADA483248&amp;Location=U2&amp;doc=GetTRDoc.pdf for instance.

",null,0,cdowyc1,1rn2y4,askscience,new,18
Julian_Berryman,"[Check this out](http://www.faa.gov/other_visit/aviation_industry/designees_delegations/designee_types/ame/media/Section%20III.4.1.7%20Returning%20from%20Space.pdf).

It seems the main factors to consider are deceleration of the craft, heating due to atmospheric friction and the precision of your desired landing.",null,0,cdoz46a,1rn2y4,askscience,new,4
rohrspatz,"I unfortunately can't remember where I heard this, but one of my favorite explanations is: ""Evolution is not an engineering process"".

In other words, there's nobody sitting there evaluating things and going, ""hm, it would be better to do that, so let's make it happen"". A population of insects has no way of knowing that it would be better off with stripes and then achieving that effect. Traits emerge completely by chance, and if they *happen* to be beneficial, they persist.

But which traits emerge depends on the genetics of the organism and what traits it currently has. Let's imagine an insect that synthesizes a black pigment via a three step reaction: Chemical A is colorless, Chemical B is yellow, and Chemical C is black. A single mutation that disables the B-&gt;C enzyme will cause that reaction chain to stop at B, and all the pigment it produces will be yellow. That's definitely a trait that could emerge and then stick around. But then, let's imagine an insect that produces C directly from A. In order to produce B, it would have to somehow acquire the entire gene for the A-&gt;B enzyme out of thin air, and for the yellow pigment to be visible and not just masked by the black, it would also have to mutate/disable the A-&gt;C enzyme. That's really, really, really unlikely to happen spontaneously.

Although it's hard to shoehorn into this example, there are also lots of examples of traits that would require multiple mutations to arrive at the final ideal state, but the intermediate stages are so disadvantageous that all the mutations would have to happen simultaneously (the odds of which are seriously miniscule).

**TL;DR: basically, a more ideal genetic composition for an organism may exist in theory, but the actual range of possibilities for how it will evolve in real life is very much limited by the fact that it has to ""get where it's going"" through spontaneous mutations of the genes it already has.**",null,0,cdp31vs,1rn39a,askscience,new,4
polistes,"Apart from what rohrspatz has already typed, that evolution does not always lead to the optimal solution, there is an answer for this specific question as well. This is a strategy that does not work if ALL insects/animals have it. 

First of all, lets talk about why warning symbols work. Wasps can sting and have venom, and by also displaying strong colors, predators learn to associate these strong colors with the nasty sting. This association means they will leave the wasps alone, which is good for the wasps. This is the same for many species of colourful caterpillars or for example ladybirds; they taste disgusting so by displaying warning colours predators will not even try to eat them. 

Now, imagine you are a syrphid fly. You are not venomous and don't sting, but you have mimicked the wasps' colours. This means that the predators that associate your colours with the wasps will also leave you alone, which is great for you too!

But, imagine that ALL species of insects have these colours. Predators will now no longer learn to associate the colours with danger/disgust, because most of the prey they eat with those colours are not dangerous or disgusting at all. This makes the use of these warning colours obsolete. 

In fact, this has been studied a lot for poison dart frogs. There are many species of them, of which a part is poisonous and a part is just mimicking the poisonous ones. In areas in which the majority is members of the poisonous species, birds quickly learn not to eat the colourful frogs because eating them makes them very sick. They will avoid these frogs, and the mimic also thrives. Now imagine that in another area the number of mimics far exceeds the number of poisoned frogs. The bird will eat a few mimics and learns that it is good food and will eat more of them. The warning strategy does not work anymore, not even for the poisonous frog.

Tl;dr : If all insects display warning colours, predators no longer associate those colours with danger and will eat you regardless of them.  ",null,0,cdpnzut,1rn39a,askscience,new,3
GOD_Over_Djinn,"The multiplicative inverse of a real number n is the unique real number m such that nm=1. Hence, if you want a multiplicative inverse for ∞, you're going to need to define multiplication by ∞. You'll run into a hundred million problems here.

The fundamental issue is that the familiar operation of division is defined for real numbers, and real numbers have the [Archimedean property](http://en.wikipedia.org/wiki/Archimedean_property). Roughly speaking this says that for any real number x, there is a strictly bigger real number y. Since this wouldn't be true if we put ∞ into our real number system, trying to put ∞ in breaks everything.

It is often convenient to write 1/∞=0 as a shorthand for lim 1/n = 0 with the limit is as n-&gt;∞. This is a nice way to save paper, but that's about it.",null,1,cdp4ogo,1rn3pg,askscience,new,7
gababa,"Ok, . I hope this isn't too much, I kinda got carried away with this answer, but I just can't stop myself when I'm writing about math :P


First I would like to point out, that infinity is not a number, it is a concept used in math to describe mathematical objects that are ascending without an upper limit. For example the following series of numbers (1,2,3, ...) is ascending, and it has no upper limit, because for any number you may choose (lets say m), i can find one in this series that is bigger (lets say m rounded up + 1) and as such it is defined that its limit is infinity.

Notice now, that properties you listed of infinity are actually derived from it's definition. For example it is not changed because of multiplication by a finite numbers because, if you take the series from before (1,2,3, ...) and divide each number by a finite number (lets call it r), it still holds that for any number(m) you can through at me I can still find a number in the series that is bigger (now its m rounded up + 1 divided by r) and as such the limit remains the same. And because this is true for any such mathematical object with no upper limit, we say that infinity divided by any number remains infinity.

Now, although infinity is not a number, it has an intuitive concept for humans as something very very large, and this intuitive concept is on par with the mathematical definition for example - The universe is infinite because no matter how far you go, you can still go farther. (Just like in the series where for any number you choose, you can find a bigger one in the series.)

To proceed let us explore now what 1/inf actually means. If we take our example of the ever ascending series (1,2,3, ...), we said that it's limit is infinity, so 1/inf is what limits the series - (1/1, 1/2, 1/3, ...). This series is limited by 0, because, for any number you can give me, be it close to zero as you'd like, by going down the series, I can find a number in this series that is closer to zero. 

Notice, how 1/inf isn't actually zero, this thing we just calculated is not 'nothing at all', and it is not exactly a number. Just like infinity, we calculated a concept of something very very small, this is called an [infinitesimal](http://en.wikipedia.org/wiki/Infinitesimal). (Or at least it's close enough). It is the concept of something so small it can not be measured, so small that no matter what number you give me, it is always smaller, but still not nothing. And this infinitesimal displays some of the same properties infinity does, because they are the same kind of concept, and are mathematically defined similarly.",null,0,cdpg8yw,1rn3pg,askscience,new,3
sufferingplanet,"Because ""infinite"" is not actually a number. This is why you cannot have ""infinity plus one"" or ""infinite minus one"" as they are still infinite, regardless of how you modify them. An ""inverted infinite"" would remain infinite.",null,2,cdp1r7l,1rn3pg,askscience,new,3
MWVaughn,"Infinity isn't a number; it's a concept. This allows it to do its freaky stuff where certain ""infinities"" can be larger than others (for example, the number of numbers between 0 and 1 is infinite, but the number of numbers between 0 and 2 is also infinite, but twice as large too.) Because infinity isn't a number, it has no inverse. You can say the same for 0. 1/0=NaN, and 1/∞=NaN.",null,0,cdpzu4g,1rn3pg,askscience,new,1
SquirrelSoul,"Technically, insomnia could be caused by any chemical that when ingested acts as a [stimulant](http://en.wikipedia.org/wiki/Stimulant). Stimulants are substances that increase physiological arousal and often do this by acting on the sympathetic nervous system - by increasing the levels or activity of norepinephrine or dopamine.

The illicit drugs that increase arousal are amphetamines like methamphetamines (""meth"") or MDMA (""ecstasy""), or norepinephrine reuptake inhibitors like cocaine.

The milder stimulants that you are more likely to encounter are caffeine, nicotine, or pseudophedrine (Sudafed).",null,0,cdp5oi8,1rn4ev,askscience,new,2
kooksies,"Basically you use the distance the band travelled down the gel (Rm), to estimate the size of the DNA fragments in base pairs (bp) by comparing it with known standards; a marker ladder.  

You can then use the fragment sizes from each lane to determine what restriction enzymes (RE) were used by comparing the Rm values with a table of known values. 

When using agarose gel electrophoresis with RE analysis, the bands always represent size because RE linearise circular DNA. Unless the RE had no activity for it.   
However, you can use electrophoresis to determine the species of the DNA too, i.e. open circular, linear, covalently closed circular etc..   

So you should be aware that plasmid DNA of the same size can move at different speeds down the gel depending on what species it is and what buffer is being used. ",null,0,cdp3zfk,1rn4h9,askscience,new,4
Pombologist,"While your gel will show you the sizes of the DNA fragments in your digest, what is more important in this context is the information you can deduce from your results.

What were you trying to determine when you did these digests? For example, were you looking for the presented of a cloned piece of DNA that had been ligated into the vector? How do your results give you this information?",null,0,cdpah1v,1rn4h9,askscience,new,2
cladocerans,"Yes, basically. What you are doing is sorting pieces of DNA by weight.  Since DNA is a sequence of bases, sorting by weight means you get longer pieces at the bottom and shorter sequences at the top. You are pulling DNA through gel--longer pieces are going to move more slowly and end up at the top (where you put the product in), while short sequences can move more quickly.

Also, intensity of the bands indicates roughly how much of that size you have. (more intense = more DNA). This is important in PCR, when you are aiming to replicate many copies of one particular part of the genome.",null,1,cdp20mx,1rn4h9,askscience,new,1
McMillan_Astro,"Images [like this one](http://www.spitzer.caltech.edu/images/1925-ssc2008-10b-A-Roadmap-to-the-Milky-Way-Annotated-) are built from a mixture of things.

First and foremost they are drawn under the assumption that the Milky Way is a lot like other nearby galaxies, so how they look is a useful guide to what the Milky Way would look like from outside.

Then different known parts of the Milky Way are put in. We know there's a [fat bulge-like bar component of our Galaxy](http://adsabs.harvard.edu/abs/1998ApJ...492..495F), and we know its rough size and alignment so that gets put in. We know that there's a [longer, thinner bar](http://adsabs.harvard.edu/abs/2007A%26A...465..825C) at a similar angle as well, so that gets put in.

We know that the bulk of the stars are in a disc, and we know that it has spiral arms. We have some information about where those are from [a variety of sources](http://adsabs.harvard.edu/abs/2008AJ....135.1301V). Measurements of gas velocities give us indirect information, as does observed Milky Way star counts at infrared wavelengths (IR is less blocked by dust) as we look at different lines of sight through the Galaxy. The most exciting source of information is from objects known as [maser sources](http://www3.mpifr-bonn.mpg.de/staff/abrunthaler/BeSSeL/index.shtml) which can be viewed to such high accuracy by radio telescopes that we can determine the distance to them very accurately from their parallaxes. These masers are expected to lie mostly within spiral arms.

Putting all of this information together can give the artist information about where the spiral arms might lie. This information is mostly about the spiral arms on the Sun's side of the Milky Way - on the far side a lot of extrapolation is required.

The details of what these spiral arms then look like (as opposed to where they are) is almost entirely artistic license, guided by observations of other galaxies to give them an idea what to expect.",null,55,cdozjrd,1rn5lm,askscience,new,343
Samply,"This was my the first question on Reddit. Great answers and nice discussion -thanks.However more pressure on accuracy part: What are uncertainties on scales or distances? +-light years?-""We have plenty to do on this planet too, but we really should work more on these star maps.""",null,5,cdp51sl,1rn5lm,askscience,new,19
xenophonf,"If I recall correctly, astronomers measured the redshift of the hydrogen line in the (radio) spectrum of stars in the Milky Way.  They even figured out that we live in a barred spiral.  If I weren't so full of Thanksgiving turkey (tryptophan poisoning FTW!), I'd chase down the citations for you, but you should be able to find them yourself on Wikipedia.  Start with the articles on the Milky Way and the hydrogen emissions spectrum.",null,0,cdpax2v,1rn5lm,askscience,new,3
null,null,null,0,cdp34t3,1rn5lm,askscience,new,1
null,null,null,11,cdoxv4q,1rn5lm,askscience,new,5
rohrspatz,"In broad terms, heart performance is controlled by contrasting/opposing input from the sympathetic (""fight or flight"") and parasympathetic (""rest and digest"") nerves. Sympathetic input increases the heart rate and makes the heart muscles contract more strongly, while parasympathetic input decreases the heart rate. If sympathetic signaling is too strong/intense, or if it's even moderately strong in someone with certain kinds of heart disease, it can cause the heart muscles to work so hard that they consume oxygen faster than it can be replenished through the coronary arteries.

Normally, this effect is observed due to physical activity - exercise increases sympathetic stimulation of the heart. But from what I can see about mental stress ischemia, the effect is pretty much exactly analogous to physical stress ischemia. Strong emotions also tend to increase the activity of the sympathetic nervous system, and if the person has some kind of predisposition, apparently mental stress ischemia is the result.

However, there really are just these two ends of the scale. The neurons of the brain interact in much more complex ways to differentiate dozens of emotions, but the signals that get sent into the rest of the body ""feed into"" relatively few pathways - the nerves that control various parts of the body typically only release one of a few activating or inhibitory signals that have specific physiological effects. Ever noticed that ""happy"" anticipation and ""fearful"" anticipation have a lot of the same physical signs, even though you can clearly tell the difference between the two emotions? That's why.

I seriously doubt that different emotions have different effects on heart function, except possibly to the extent that certain emotions could activate sympathetic signaling more strongly or weakly than others. But I'd also bet that that effect is hugely variable among different people, and given our current understanding of the precise neurological signaling mechanisms that result in precise emotional states (i.e. basically none at all), it's probably not a good topic to study.",null,0,cdp3ng7,1rn5n6,askscience,new,2
Dr_JA,"It depends. There are some plants that you can grow under 24h light condition and they'll just be fine, whereas others will get stressed and not so happy. In the polar circle there are enough plant species growing, and they seem to be doing fine with 24h periods of light during the summers there - they will most certainly be photosynthesizing all day, although probably not at a constant rate.

In principle, there's nothing against perpetual photosynthesis - the enzymes that do this whole process will need to be turned-over obviously, but that is normally an ongoing process.

The only thing that is probably an influence in this, is the ability to deal with reactive oxygen species, or ROS. ROS are generated in the photosynthesis process, and need to be cleaned-up by certain enzymes. I guess this normally happens in the dark, so these ROS can be cleaned-up. Continues photosynthesis will probably require that ROS scavenging is increased, which plants should have the plasticity for, depending a bit on the species.
Hope this made things a little clear.",null,0,cdpa4az,1rn6m6,askscience,new,3
MarineLife42,"Plants do need, and take, rest.  
Depending on species and location, plants will only photosynthesize between 8 and 14 hours every day, independent on daylight. Outside that time, their metabolism will switch from photosynthesis, producing sugar and oxygen, to breathing just like animals, i.e. using sugar and producing CO2.  
Only the small amount of sugar that has been permanently built into the plant structure represents a net withdrawal of CO2 from the atmosphere and a net gain of Oxygen. ",null,1,cdpxiys,1rn6m6,askscience,new,1
naijizaknayr,"They probably mean dont do things like take it from the oven and put it in the sink with water.  The dish will heat up slowly in the oven because air is a poor conductor of heat...but if you take it from the oven and run water over it, water is a much better conductor so the dish will cool rapidly and could crack.",null,4,cdp426u,1rn7aa,askscience,new,28
Apollo_Felix,"Pyrex today is made of tempered glass. Pyrex used to be a low thermal expansion borosilicate glass, according to Wikipedia. Now glass subjected to a fast change in temperature breaks because glass is a thermal insulator. If you cool a surface of the glass quickly, you induce a large variation of temperature along the inside of the glass, which for a dish can be pretty large. The cool side will shrink, but the hot side won't. This will cause stress inside the glass dish, which can be enough to break the glass. Since original Pyrex had a low thermal expansion, it would induce lower stresses and not break. Tempered glass is a resistant glass that is already stressed in a way to make it more resistant. It also breaks into very small pieces when shattered. However it still expands or contracts enough to break when subjected to sudden large temperature changes.",null,0,cdp7hco,1rn7aa,askscience,new,8
PostGradWarrior,"In general amorphous materials such as glass don't deal well with rapid temperature changes. They are insulators, so they don't transfer heat very well. If the surface is suddenly cooled by water, the outer surface will want to contract and the inner surface will want to stay the same. The lack of a crystalline structure (Think of it like a solid trapped as a liquid), makes it very hard to resist shear stress. A contraction of the top surface due to a temperature difference will create a shear stress and this can cause it to crack.",null,0,cdpfxjh,1rn7aa,askscience,new,3
FieryPianos,"As was mentioned before, Pyrex is currently made of tempered glass. Tempered glass is manufactured in high temperatures, in which the glass suffers compression forces on its surface and traction forces in its interior. It is then cooled very quickly, so as to mantain those forces and also keep its glass transition temperature (the temperature above which a glass starts behaving a lot like a liquid) very high. The result of these forces is that superficial fissures on tempered glass rarely ever spread and cause it to break, making it incredibly resistant. Before the change to tempered glass, Pyrex was made of borossilicate glass. The reason for the resistance of that material is that the boric oxyde that is added occupies empty spaces in the glass and helps keep the silicate, aluminum oxyde and sodium oxyde tight together. It is also created at very high temperatures, again to keep its glass transition temperature high.",null,0,cdpitsv,1rn7aa,askscience,new,2
null,null,null,0,cdpygwf,1rn7aa,askscience,new,2
null,null,null,3,cdp1twh,1rn7aa,askscience,new,1
SheepShepherd01,"The mass of an animal is a function of the mean density of its tissues times its volume. When increasing an animals size, its weight will grow as a cubic function. The weight comes into play when determining the momentum of the animal just before it hits the ground - a heavy animal dropped from the same height as a smaller animal will, while reaching the same velocity as the lightweight animal, have a much larger momentum. However, the force of the impact is just applied to the area of the animals bottom side, which will behave like a quadratic function when going from small to big animals.

So, the larger an animal gets, the the higher gets the ratio weight/area of its bottom side. In a larger animal, more energy is applied to a relativly smaller area when it falls to the ground, so it will get hurt more than a smaller animal.",null,0,cdp3gqu,1rn7k4,askscience,new,5
Platypuskeeper,"I think 'all the above' would work as an answer here. 

DNA bases (deoxyribonucleotides) are synthesized (by [Ribonucleotide reductase](http://en.wikipedia.org/wiki/Ribonucleotide_reductase)) from ribonucleotides, those used as bases in RNA. Which is one of the reasons the dominant theory is that [RNA evolved first](http://en.wikipedia.org/wiki/RNA_world_hypothesis). 

Unlike DNA, RNA is single-stranded. But it can still form a double helix, if part of the strand has a set of complimentary bases, it can loop back and form a helix with itself, which it does in [tRNA](http://en.wikipedia.org/wiki/Transfer_RNA) for example. I would say (or speculate) that this is mainly for structural and chemical stability there. But note that RNA that carry genetic information (mRNA) don't usually (or at least not necessarily) form helices as far as I know. 

Why bother with DNA in the first place? RNA can already carry genes (and e.g. [some viruses](http://en.wikipedia.org/wiki/RNA_virus) only have RNA), so it might seem redundant. It also costs energy to produce DNA bases from RNA ones. The apparent answer to this would be that DNA is more stable (both chemically and against mutations), which is something that's gets increasingly important as the amount of genetic material increases. From that perspective, it also makes sense why we'd still retain RNA for [transcribing](http://en.wikipedia.org/wiki/Transcription_%28genetics%29) genes, since they're just short fragments of genetic material - there's no need for the extra stability, so it's not worth the cost.

So, given that DNA is double-stranded (I'll get to that), it's sort-of a consequence of it's structure that it would be able to form helices like RNA does when it has a complementary strand. (However, there's a bit of difference in that RNA only forms 'A-type' helices, while DNA is usually 'B-type'. There are at least 3 types of DNA helices - [A](http://en.wikipedia.org/wiki/Z-DNA), [B](http://en.wikipedia.org/wiki/Z-DNA) and [Z](http://en.wikipedia.org/wiki/Z-DNA), so in order to not get bogged down in detail, I'll just leave it at 'forming a helix' here)

Now, the complementary strand, which is a key to helix-forming here, is a big difference to RNA. It does provide stability both against mutations and chemical reactions by having the bases inwards, facing each other, shielding them a bit from the environment. But there's more than just that, the redundancy allows for '[proofreading](http://en.wikipedia.org/wiki/Proofreading_%28biology%29)' of the DNA, helping correct errors during replication.

So there are benefits to having a complementary strand, and having a helix likely helps with the aforementioned shielding of the bases. But in a way it's also side-effect of the structure. I don't think you could possibly 'flatten' it into a ladder-like structure without putting an implausible amount of strain on the chemical bonds. 

",null,1,cdp883i,1rn8mb,askscience,new,5
darkgrenchler,"A couple reasons: Size, chemical protection, and readability by proteins.

[DNA is actually coiled multiple times](http://i.imgur.com/sOjvbBS.png). this supercoiling allows for space conservation inside cells. A fully uncoiled human genome in a cell would be a meter or two in length. This isn't directly associated with the double-helix nature, but helps nonetheless.

the double helix nature of DNA helps protect DNA from being damaged, while still being able to be unzipped by proteins who *need* to access the DNA for some reason. Each (single) strand is built by covalent bonds, but the two strands in a helix use hydrogen bonds to keep eachother attached. H-bonds are weak alone, but a billion H-bonds acting at once are strong enough to keep a DNA strand intact. The twisting nature of the double helix also puts the hydrogen bonds on the ""inside"" of the helix, increasing its protection.

[The spacing in the coils](http://i.imgur.com/Yckvfgp.jpg) are only a few angstroms in width, so there aren't many things that could get in there and mess around.",null,0,cdp87ur,1rn8mb,askscience,new,1
andrewjkwhite,"Since there has been no answer posted.
As a layman if I had to guess, residue on the window. Outside of the wiper streak probably had wax or some other residue that was a poor surface for crystallization but inside the streak that will have been worn off and the crystals could form directly on the glass. 

I know I'm breaking the rules and I would love to know the real answer.",null,0,cdp2ocl,1rn8r3,askscience,new,4
princetonwu,"Pathologically speaking, the cells of the urinary tract epithelium (ie the lining of the bladder walls) do not contain the same properties as the cells lining the colon. Secondly, the purpose of the bladder is to hold the metabolic wastes after urine is filtered from the kidney; so it would serve no purpose to additionally reabsorb more water when the kidney is already efficient enough to reabsorp water. Thirdly, if a person is severely dehydrated, the body acts to minimize fluid loss through the reabsorption mechanisms of the kidney (and not the bladder). ",null,1,cdp34qm,1rn9nv,askscience,new,8
Sterlz,"The melting point of a material is independent of how the material is shaped. The only thing that effects the melting point is pressure. The minute change in crystalline structure from machining might have a negligible effect, but nothing close to a noticeable change.",null,0,cdp5zdd,1rncb0,askscience,new,2
columbium,"Yes.  If you have a material which splits into multiple phases upon cooling, the rate of cooling can dictate how much of a given element diffuses into each phase.  The resulting variation in compositions will change the melting point of each phase.

Also, the cooling rate of a polymer will affect its glass transition temperature.  Cool faster and the glass transition temperature increases.  This isn't a melting point, but for practical use, you probably wouldn't want to use a polymer above its glassy transition temperature for structural applications. ",null,0,cdpnksz,1rncb0,askscience,new,1
NHsniper5689,"The answer is simple. It does not make the water boil faster, actually it does just the opposite... the boiling temperature of the water is raised so that the pasta (or whatever you're cooking) cooks faster due to the water temperature being higher). Sorry for the not so sciency answer.",null,1,cdp1lv6,1rncfh,askscience,new,5
TheNextDoctorWho,"Took me some time to find the english word, but the term to look up is the [boiling-point elevation](http://en.wikipedia.org/wiki/Boiling_point_elevation) (In german, it's ""molale Siedepunktserhöhung"".

This property is inherent to ANY solution with solved particles in it. The nature of the particles is irrelevant, only the number counts - hence the term ""molale"" in german, because it is dependet on the molality of any solute.

A liquid is boiling if its vapor pressure is equal to the surrounding atmospheric pressure. If you have a couple of non-solvent molecules in the solution, the concentration of the solvent molecules gets diluted, therefore the vapor pressure of the solvent is lower. That in turn means that you have to bring the solution to a higher temperature than usual, so that the 'lower' vapor pressure can catch up to the atmospheric pressure. 



",null,0,cdp5n05,1rncfh,askscience,new,2
Alexis_deTokeville,"By adding salt to the water, you increase its entropy by diluting the water particles and therefore decrease the *change* in entropy needed to change phases from a liquid to a gas. Smaller change in entropy means less heat/temperature is required. I believe the entropy effect also corresponds to a lower vapor pressure.",null,3,cdpbgu1,1rncfh,askscience,new,1
SingleMonad,"&gt;..., so if they are experiencing acceleration ...

Yes, *if*.  They are not.  That's the reason they do not radiate.

More broadly, classical radiation such as the kind generate by an accelerating charge is proportional to the square of the acceleration.  The expectation value of the electron's position is constant (this is a feature of all energy eigenstates, and the reason they are also called *stationary states*).  Related to this is the fact that an atom in its ground state has no classical dipole moment, and no way to generate one in a transition to a lower state of energy (because no lower state exists).  

It seems odd, since there is generally a nonzero probability to find the electron throughout the atom.  But no classical acceleration is at work, i.e., the second derivative of the expectation value of position is zero.


You might wonder what's different about an atom in (say, just for concreteness) its *first excited state*.  You know that decays, and emits radiation.  What is different about it?  The answer is subtle (and beautiful):  the atom in its excited state is **not** in an eigenstate of the coulomb Hamiltonian (from the nucleus).  It's in an eigenstate of the *total* Hamiltonian of the system.  The total Hamiltonian includes the electromagnetic field.  The true energy eigenstates are *superpositions* of atomic states.  That excited state I mentioned is really a super position of (mostly) the excited state plus a little bit of the ground state.  This superposition state does possess a dipole moment, and hence radiates when the atom falls down to the ground state.

Your question is what kept the pressure on to find a better atomic theory than the Bohr model.

Edit: it's-its stuff",null,0,cdp5h3u,1rng96,askscience,new,6
serweet,"The best way to think of this in classical terms is in electron shells, which are a set of allowed state for an electron to be in. These shells are based on intrinsic quantum numbers of the electrons (such as angular momentum and spin), with each further shell increasing in energy. Due to the Pauli exclusion principle, only one electron may occupy each state.

Now, to get onto your question, if an electron were to radiate energy, then it must be an exact quantity to move it from a higher shell to a lower, but if the electrons are all in the ground state (the state of lowest energy), then they cannot physically move to a lower state, thus radiating photons. Electrons tend to sit in their ground state configuration, but do absorb photons of exact energies to move up into higher states, and then radiate photons as they drop down the shells.",null,2,cdp2vfv,1rng96,askscience,new,4
mc2222,"&gt;They are charged particles, so if they are experiencing acceleration they should

Well, they do.  When an electron looses energy, it emits light as an EM wave (or photon depending on your philosophical bent).  

&gt;I understand that in quantum physics, due to Schroedinger equation, electrons are static fields of probability but that seems to only take into account their wave properies, completely neglecting the fact that they are also particles.

It's not that the *electron* is a static field of probability, it's that the electron's *location* can be described in terms of the probability of where *you will find it*.  It's the electron's *location* that is described in terms of probability.

",null,2,cdp2y5t,1rng96,askscience,new,3
RetraRoyale,"There doesn't need to be a classical explanation because classical physics doesn't apply at that scale. It's only true at larger scales as an approximation.

&gt;I understand that in quantum physics, due to Schroedinger equation, electrons are static fields of probability around the nucleus and so are their charges, but that seems to only take into account their wave properies, completely neglecting the fact that they are also particles.

~~First, electrons are not 'static' fields of probability, they're quite dynamic. In fact, the quantum mechanical notion of energy typically manifests a sort of complex cycling-of-the-wave effect, much like a vibrating string.~~ (This is probably not a useful thing to say.)

Second, electrons don't have 'wave properties' and 'particle properties', they *only* have wave properties (or tautologically, 'electron properties'.) To call it a particle means that the EM field has quantized, localized features, not that these features are *actually* a single indivisible entity.
",null,3,cdp2w8d,1rng96,askscience,new,3
RelativisticMechanic,"[TL;DR]

It depends on just what you mean, but the usual definition of black hole temperature comes from Hawking radiation, in which case if we express our mass in multiples of solar masses (so the sun would have M = 1, a star ten times as massive would have M = 10), the temperature is

T = (0.0000000617/M) Kelvin.

So, for example, a black hole the mass of the sun would have a ""temperature"" of approximately 0.0000000617 Kelvin. One that was ten times as massive would have one tenth the temperature.

[Comment Proper]

When dealing with curved spacetimes, you have to be careful with the definition of things like ""temperature"". When dealing with black holes in particular, you also have to be careful about how you interpret questions that you expect naïvely to depend on the behavior of stuff inside the event horizon.

One of the difficulties here is that we can't speak to the microscopic behavior of matter inside of the black hole. We do not have a consistent model for describing quantum systems in such spacetime regions, and any proper microscopic description is going to require a quantum theory. Thus, we can't really talk about temperature in the sense of ""motion of particles"".

However, we *can* apply quantum field theory in a blackhole spacetime provided, roughly, that we stay away from the singularity and our field doesn't carry too much energy. In particular, if we're interested in describing events outside of the event horizon, we can use these techniques as a good first approximation. We can also apply quantum statistical mechanics/thermodynamics to this process. When you do this, you find that if an observer falling across the event horizon sees themselves in a vacuum, an observer far from the black hole must see a flow of thermal radiation coming from the black hole. This radiation has the same spectrum as a blackbody source of some definite temperature, so we can associate it with a temperature. It turns out that, for a black hole of mass M, the temperature T associated with this radiation is

T = (ħc^(3) / [8πGk_*B*_])/M,

where ħ is the reduced Planck's constant, c is the speed of light, G is Newton's gravitational constant, and k_*B*_ is Boltzmann's constant. That factor is just a constant that we can evaluate, and it comes out to roughly

T = 1.227\*10^(23) kg K / M = (0.0000000617/M) Kelvin,

where the second form comes about by expressing M as a multiple of the solar mass.

",null,18,cdp2hll,1rngpn,askscience,new,157
dumb_,"Simplified answer:

*The temperature of a black hole is determined by the 'black body radiation temperature' of the radiation which comes from it. (e.g., If something is hot enough to give off bright blue light, it is hotter than something that is merely a dim red hot.)*

*For black holes the mass of our Sun, the radiation coming from it is so weak and so cool that the temperature is only one ten-millionth of a degree above absolute zero. This is colder than scientists could make things on Earth up until just a few years ago (and the invention of a way to get things that cold won the Nobel prize this year). Some black holes are thought to weigh a billion times as much as the Sun, and they would be a billion times colder, far colder than what scientists have achieved on Earth.*

*However, even though these things are very cold, they can be surrounded by extremely hot material. As they pull gas and stars down into their gravity wells, the material rubs against itself at a good fraction of the speed of light. This heats it up to hundreds of millions of degrees. The radiation from this hot, infalling material is what high-energy astronomers study.* 

[^Via: ^NASA's ^Ask ^An ^Astrophysicist](http://imagine.gsfc.nasa.gov/docs/ask_astro/answers/971111e.html)",null,1,cdp3rrp,1rngpn,askscience,new,25
cladocerans,"Yes, this is a real problem if you are not protecting the soil from erosion and also doing nothing to build up topsoil (adding compost, leaving standing crop, etc). The US is losing approximately 6.9 billion tons of topsoil every year: http://anrcatalog.ucdavis.edu/pdf/8196.pdf",null,1,cdp2608,1rnhwj,askscience,new,1
McMillan_Astro,"If Hubble's constant (H0) were actually constant that would be correct - the relative velocity of two objects a distance s apart is v = H0 * s, so using some maths (apologies if you don't know calculus):

v = ds/dt, so ds/dt = H0 * s. This is a differential equation with solution s = const * exp(H0 * t).

That means that the universe would be exponentially expanding, as you suspected, and that never goes to zero size however far back in time you go.

**However**, Hubble's ""constant"" changes with time! We believe it is constant in space (it is the same whichever direction we look), but it changes over time. The evolution of Hubble's ""parameter"" (which has the value known as Hubble's constant today) is governed by the [Friedmann equations](http://en.wikipedia.org/wiki/Friedmann_equations).

Broadly speaking these mean that when the universe is dominated by matter, so it's density and pressure decreases over time, then the Hubble parameter decreases over time - this has been the case for most of the history of the universe. In this scenario we can get back to everything in the universe being at the same point long ago (the big bang).
",null,2,cdp3c3z,1rni8w,askscience,new,15
Dendrimer14,"For every million parsecs of distance from the observer, the rate of expansion increases by about 74 kilometers per second.  This is equivalent to ~2.4 femtometers/sec/meter (if the expansion rate is linear over distance)   2 meters would expand twice as fast as 1 meter.  1 meter would take about 26.4 million years to double in size.
",null,0,cdp43kg,1rni8w,askscience,new,7
nohoxe,"Let's think of the factors that effect tire ware. Heat will effect how soft the tire is and how quickly it wares. Rain would cool the tire making it much less elastic and more durable. Though it might be that the vulcanized rubber on the tire wouldn't change it's elasticity much at even the highest temperatures that tires are subject to on the rode. 
",null,0,cdp1yqh,1rnkz1,askscience,new,2
elsjaako,"You need to define what ""acting like a sphere"" means.

A coin can only flip over if it bounces, a six sided die already sort of rolls over a hard surface, a twenty sided dice rolls (I don't feel like what they do needs a qualifier). A 100 sided die moves almost like a smooth ball, but it will land on one of 100 discrete points.",null,127,cdp5a68,1rnn07,askscience,new,721
thepsyborg,"Your question presumes that we can answer a binary question (""Does it act like a sphere?"" Yes/no) about a given shape. Without specifying an arbitrary threshold for spheroid behavior, the most we can say is that behavior approaches that of a sphere as shape approaches that of a sphere.

The largest number of sides a single brand-new die could have while still rolling fairly is limited only by the precision &amp; accuracy of its manufacturing process. The upper limit on number of facets for a _usable_ fair-rolling die would also rely on the resistance of its material to wear and deformation, if we expect it to remain essentially fair for a reasonable number of rolls.",null,30,cdp7es9,1rnn07,askscience,new,164
dracho,"The answer is obvious.  None.  

If an object has 1,000,000,000,000,000,000 sides, it still doesn't act like a sphere.  It's acts *more* like a sphere than a cube, depending on what metrics you're measuring, but it's still not a sphere.

The only thing that ""acts like a sphere"" is a sphere.

There is some discussion about molecules, and how in a real-world scenario, a sphere made of ordinary matter isn't truly a sphere... I'm ignoring this and focusing on the pure math.",null,30,cdp6v7x,1rnn07,askscience,new,108
sperho,"OP's post reminds me of the [MythBuster's segment](http://dsc.discovery.com/tv-shows/mythbusters/videos/square-wheels-angle-2.htm) on putting square wheels on a truck and seeing if driven at high speeds they would ""act like a"" round wheel and produce a smooth ride. spoiler alert:  not really.",null,6,cdp1ngu,1rnn07,askscience,new,37
null,null,null,7,cdp6586,1rnn07,askscience,new,30
Hokararu,"To clarify what I mean by 'act like a sphere' I am referring to the point at which an object will maintain its current position when the angle of the surface it rests on is adjusted.

For example, if I roll a die onto a table and then lift that table off the ground at one end, the die may slide down the table but is unlikely to change the side it rests on. A sphere however, would roll down the table making it useless as a die in this situation.

Math me...",null,11,cdp5j36,1rnn07,askscience,new,26
AdmiralTroll,"Not an expert on this, but my understanding is that everything, aside from 'the perfect sphere' holds a number of angles, even if it is down to an molecular level. Therefore every smooth rubber ball has a finite number of sides and so technically is a die in its own right?

On another note: surely then a perfect sphere that had no angled side to land on would; on a completely flat smooth surface, continue rolling without needing more force applied?",null,3,cdp6q8p,1rnn07,askscience,new,13
John_Duh,"If you only consider a Dice one of the Platonic Solids you can only go as high as 20 sided. They are the only polyhedron that has equal sized sides, so the question is not really possible to answer.

You could though take a sphere and cut of parts of it to create surfaces but it would not be considered a Platonic Solid.",null,3,cdp7li9,1rnn07,askscience,new,12
Pitboyx,"Since you probably won't be rolling on a perfectly clean 100% smooth surface, the die would be considered uselessly inaccurate between 8-11 thousand sides (estimated through blender icosphere) because at that point, the die will be tipped on an edge or corner instead of a face by particles of dust, grooves, and bumps in the surface as if you were spinning a regular 6-sided die on a corner.

From an even more practical viewpoint, it would be near impossible to decide which face is facing upwards with the naked eye after 500-700 faces. 

A 80 sided icosphere would probably be the ideal size for having the most results per die. The more reasonable choice would be to simply use more dice.",null,7,cdpalpy,1rnn07,askscience,new,16
vortik,"As others have mentioned, the term ""act like a sphere"" is a bit vague.  So when I read this, I started wondering if there was a way that we could quantify this in terms of three-dimensional packing densities.

A cube can be arranged to fill a three-dimensional space completely.  But as you ""subdivide"" the sides, on your way toward turning it into a sphere, your maximum packing density decreases.  For example:

| Solid | Maximum known packing density |
| ------ | ------------------- |
| octahedra | 0.947003 |
| dodecahedra | 0.904002 |
| icosahedra | 0.836315 |

[Wiki reference](http://en.wikipedia.org/wiki/Packing_problem#Packings_of_Platonic_solids_in_three_dimensions)

As these objects become more sphere-like, the maximum known packing density is conjectured to approach that of the [Kepler conjecture](http://en.wikipedia.org/wiki/Kepler_conjecture) density of ~0.74 for best-possible sphere packing in 3d.  My understanding is that much of these ""best known"" values have been computed via computer simulation, and no rigorous proofs of this transition have been forthcoming.

If we assume that this trend is correct (frankly, I think it is, but I don't think it has been formally proven), then what I was thinking was that the OP may be able to quantify (to some extent) what ""close to a sphere"" means.  For example, he could take the given icosahedron considered in [this paper](http://www.nature.com/nature/journal/v460/n7257/full/nature08239.html), and say that it is ""0.83/0.74 ~= 12%"" away from being a ""sphere"", and this is what it looks like, and that is how well it packs.  

I do not know if the above would answer the OP's question.  A simpler answer would probably be:  ""a 20-sided die rolls pretty well"", but others have tried to tackle this.  I suspect that you'd have to consider the effects of friction between the sides of the die against the contact surface (compared to the infinite case for a sphere?), but that's out of my purview.


*Edit: Formatting.",null,0,cdp2oz3,1rnn07,askscience,new,7
null,null,null,1,cdpbqy3,1rnn07,askscience,new,6
Jackeddaniels69,"Hi Hokararu, if we assume that each side is a congruent regular polygon then you can actually find the spherical deviation (relatively how close it is to being a sphere) for a Polyhedra (Basically a symmetrical 3D Object)!
If we take a 3 sided object connected by equilateral triangles we get a tetrahedron. Use the formula δ(v) = 360 - (sum interior angles surrounding a single vertex) which will be, δ(v) = 360 - (180). 180 was derived by each of the three interior angles of the equilateral triangle 3(60). 

δ(v) = 180 = Spherical Deviation. The lower the number the ""more spherical"". 0 would be a sphere. Unfortunately you can only have a certain number of side that are regular and congruent fitting the description of a dice.

The largest number of sides to still be considered a 'dice' and be as close to 'spherical' as possible is 20 Sides.
http://en.wikipedia.org/wiki/Icosahedron 
Devils advocate though, these are a thing.
http://en.wikipedia.org/wiki/Zocchihedron

Edit&gt; Also a dices vertices connect edges all with the same interior angle. so Catalan or Semi-regular polyhedra would not count in my opinion. So it really depends on your perspective of what a dice really is.",null,0,cdp7hl7,1rnn07,askscience,new,5
OneTwoTriangle,"This is more a physical answer, than a mathematical one. I am assuming a dice in our reality.

When the surfaces of the sides are a couple of molecules big the dice will start to behave like a sphere. It is basically a ball. To calculate the amount of sides you calculate the surface of a sphere with (almost) the same diameter as your dice. Divide this number by the surface area of your dice side. This gives the amount of sides it can have.

~~A problem however is the regularity of your dice. A twenty sided dice is the biggest dice you can make on which all sides have equal probability. After that you need different kinds of shapes to create the dice. This follows by the fact that there are only five platonic solids.~~ 

EDIT: That last part is wrong see the comment by eli5--answer. 

",null,2,cdpe2r1,1rnn07,askscience,new,4
88888888333,"Technically, nothing is a perfect sphere. In fact, all perfect geometric shapes are only mathematical concepts. You cannot create a perfect sphere out of atoms. So the answer to your question is that what you perceive to ""act like a sphere"" is actually similar to a die with *many* sides, since there is no such thing as a perfect sphere. ",null,0,cdpezbt,1rnn07,askscience,new,3
gubasaurus,"To me (an engineer, not a mathematician), it's an issue of stability. Will the dice ever reach a position in which it is stable? When perturbed, will it want to go back to its original state? To 'behave like a sphere' the smallest angular perturbation will cause the sphere to move and keep moving. A dice with sides of equal surface areas that is perturbed by a small enough will go back to its original position. So my answer to your question would be that, to behave like a sphere, an object cannot have sides, and so cannot be a dice.",null,0,cdp7zu5,1rnn07,askscience,new,3
bangsecks,"I think OP is asking how many sides could be added to a die before it no longer would settle and rest on a discrete surface, not how many facets are possible to have on one.  This comes down in part to the volume and mass of the die, as the die gets larger and heavier, whatever inertia from the roll acting on its mass will pull it in one direction or another such that it doesn't find a resting position, at least in a reasonable amount of time continuing to move about like a ball does.  The greater the mass the fewer surfaces possible before it just rolls.

Before we get the volume and mass to certain quantities such that we can fit a lot on there and still have it behave as a die we'd probably run into the problem of not being able to clearly tell what surface is the value surface, or the side that's telling you what number you have both because it would be hard to tell just which is directly at the top when they're so small and because as there are more and more sides the numbers will have to get to digit strings so large they'd be difficult to fit on shrinking facets.",null,0,cdp8bd3,1rnn07,askscience,new,2
huggybear0132,"This is a 3D version of making a polygon into a circle. You could consider a circle to be a regular polygon with an infinite number of sides, and likewise a sphere a regular prism with an infinite number of sides. That is the theoretical side. Now physically, what you mean by ""act like"" is ambiguous. Do spheres not roll and then eventually rest on a ""side"" (the point tangent to the surface they are resting on)? In that sense, all dice act like a sphere already.  I am going to go ahead and say the number of sides n has to tend towards infinity.",null,0,cdpe3en,1rnn07,askscience,new,2
ImAVibration,"Here's a crazy idea, a perfectly smooth sphere has an infinite number of sides it can rest on. But if you increase its diameter, it will still have an infinite number of 'sides', but it will also have infinitely more than the smaller sphere.",null,0,cdpfg2x,1rnn07,askscience,new,2
bloonail,"Neutron stars be damned -0- the question is about dice and ""acting like a sphere"".

That question seems three parts:
1) what makes a dice act like a dice
2) when is a dice no longer acting like a dice
3) at what level do facets make an object non spherical.

A dice provides evenly random results based on the number of facets of the dice.

Dice no longer act like dice when the randomness is predicable or favors a group of facets. Six sided dice are well understood. They are hard to roll in a predictable way.. No one really trusts the ones with more sides. There are too many ways to mess with the spin and bowling-like parameters of the many sided dice.

However a regularly multifaceted object would likely be considered dice-like even if the number of facets were very large. A comet with 90 near regular sides would be dice-like. ",null,0,cdpkpgh,1rnn07,askscience,new,2
broofa,"tl;dr: **1,876 faces for a die to behave like a sphere on a smooth wooden surface** *

Here's the rational for this ...

As others have noted the behavior of a die depends on the surface it is rolled on. Intuitively we know that rolling a die on a granite countertop is very different than rolling it on a shag carpet. Thus, to get to a specific answer we have to make some assumptions.

For starters, from [this size chart](http://dicegamedepot.com/dice-sizes/) we see that the most common die is 16mm across.  And to avoid some nasty dynamic analysis, let's just say the die will ""begin to act like a sphere"" when the size of a face is &lt;= 10x the imperfections in the surface.  [This is rather arbitrary but this is a vague enough question that if we can get to within +/- 10x we probably have a ""good enough"" answer.]

For the surface, let's assume we're on a finished wood counter top with [surface irregularities on the order of ~0.05mm](http://www.nyme.hu/fileadmin/dokumentumok/fmk/acta_silvatica/cikkek/Vol04-2008/08_magoss_p.pdf).

We can approximate our die as a sphere of radius = 8mm, with faces of radius = 0.25mm.  To get our answer, we then calculate how many faces can be packed into the surface area provided by our sphere.  To simplify this problem, we use the fact the face size is small relative to the die size, meaning we can just treat the sphere surface like a flat plane.  Thus, our equation for determing the # of faces is this:

    # of faces = AS / AF * D

Where ...

* AS = Area of sphere = 4 * pi * 8mm ^2 = 804mm^2
* AF = Area of die face  = 2 * pi * 0.25mm^2 = 0.39mm^2
* D = [Packing density of circles](http://en.wikipedia.org/wiki/Circle_packing#Packings_in_the_plane) = 0.91

Plug those in and you get the answer at top of post.

(*) Your Mileage May Vary, depending on size of die, material and finish of surface, and extent to which you actually give a damn about accuracy of this answer.

[Edit: move answer to top]",null,0,cdp8o78,1rnn07,askscience,new,3
abz_eng,"I think this is question of angles between sides 

A cube 6 sided has 90 degrees between sides whereas a sphere essentially has 180 - (1/infinity)? 

Now as the number of sides increases the angle increases and the angle at which the dice flips decreases (centres of mass and gravity) - cube flips at 45 degrees whereas a sphere is just over zero. You need to determine subjectively in each situation the angle you require.

(I've ME/CFS guys so this is the best I can do)",null,0,cdp9c65,1rnn07,askscience,new,1
Larrysbirds,"Think about the changes between a triangle to a square to a pentagon to a hexagon, etc. The number of sides increase by 1, the theoretical maximum amount of sides (or infinite) would be shaped as a circle. To answer your question: infinite in theory.
I speak in 2D, but it translates into 3D the same.",null,0,cdp9cz1,1rnn07,askscience,new,1
godless117,"technically with no limit specified on the surface area of each facet, there is no limit, so long as you have the manufacturing means you can make one giant die with 1,000,000,000 facets. Perhaps a better question would be what is the minimum surface area, or surface area of facet to number of facet ratio, that a facet of a die must possess in order to still give specific rolls?",null,0,cdpb4vt,1rnn07,askscience,new,1
mao_zedonk,"Figured I'd take a quick stab at a ""back-of-the-napkin"" calculation for this one. One can imagine that a polyhedron with edge lengths one atom apart would be indistinguishable from the best physical approximation we could possibly make of a sphere. I made some assumptions here: 
-all bond lengths are an average of 135 picometers (mid-range for carbon-carbon bonds you would find in a plastic die).
-a polyhedron can be constructed from ""n"" approximately equilateral triangles (would love some input on this from the mathematicians out there)

The surface area of a standard 6-sided die with 16mm edges is 1.536E-3 square meters. The area of an equilateral triangle with edge lengths of 135pm is 3.490E-12 square meters. So.... to construct a n-sided polyhedron composed of approximately equilateral triangular sides and having the same surface area as a standard die, we would need... 4.40E17 sides.

That is, 440,000,000,000,000,000 sides. Granted, some of my assumptions may have over-reached, thoughts anyone?",null,0,cdpbihm,1rnn07,askscience,new,1
ademnus,"Whatever it is, it must be 6 or higher.

No one needs a 1 sided die. I can predict what it would land on with 100% certainty anyway, if you could somehow possess one. 2 sided dice are otherwise known as coins and they toss or flip better than they roll, unless you wish to chase one under the vending machine. I have tried oddly-shaped 3-5 sided dice and they do not roll well. They're best shaken in closed, cupped hands and tossed down. Once you hit a 6 sider, however, it's all downhill from there.
",null,0,cdpctvm,1rnn07,askscience,new,1
KillingSloth,"I think what OP wants to know is this : http://www.toplessrobot.com/2009/04/the_10_most_shameful_rpg_dice.php 

And from experience, 100 gives you pretty much a sphere (a ball if you want), you could say 1000 sided die for maximum crazyness (I'm sure it will come out one day with 3D printing) ",null,0,cdpczox,1rnn07,askscience,new,1
kobescoresagain,"Something to take into consideration as well is the size of the sphere like object.  As size increases the amount of sides required to approach sphere like properties would increase unless you also changed how the outside interacted with the sphere like object.  For instance if you had a 20 sided die similar to ones used in card games, you could easily roll it like a sphere.  However of it were much larger it would become increasingly more difficult to use it like a sphere.  ",null,0,cdpdmh9,1rnn07,askscience,new,1
GBBerg88,"Technically speaking that answer to this must be that the dice would have to be able to land on a single square, but also be readable. 

The size in itself could technically be very large, but not infinite because at some point you have to account for the hardness of the material and make sure you don't have any kind of failiure either in the ground or in the dice itself. But then you'd go in to extremes and you couldn't actually throw it, only roll it over and see what square it landed on. 

So limiting the size to something reasonable, such as a ""handful"" at max, an adult sized handball for instance. And then accounting for readability and it landing reasonably on one square. I'd say something like 350 sides would be possible. 

But for that to work properly you need a perfect surface. ",null,0,cdpek01,1rnn07,askscience,new,1
WorldIsImagination,"I don't know the exact formula, but I think it would be something like""

A constantly changing formula based on the speed of growth of our universe. 

Meaning the largest dice would be slightly smaller than the exact dimension of our (spherical??) universe. As the universe expands the answer to the question would change. ",null,0,cdpgxp6,1rnn07,askscience,new,1
king_of_the_universe,"Well, there would certainly be a number of sides that makes it impossible to easily determine what number the dice throw resulted in. From this perspective, ""acting like a sphere"" might already have begun with a 100-sided die.",null,0,cdphwbd,1rnn07,askscience,new,1
TITAN_,"Having played a lot of percentile dice based RPG systems, the closest I've seen a dice come is the d100, a 100 sided dice. It rolls nearly like a ball, so easy to roll off the table. Which is why you usually opt for 2d10 instead where one die is the tens and the other the single numbers.",null,0,cdpiix3,1rnn07,askscience,new,1
ohmz2,"The determining factors here are the surface finish and geometrical integrity of the die and the surface it's being rolled on. We can use microfabrication techniques such as photolithography and micromachining, which can produce structures with tolerances at the micron level (1/1000 of a mm, ~0.000039 inches). You can make a die with an innumerable amount of super flat sides, as well as super-flat surface to roll it on. Dust and other finite particles would interfere with our microfabricated die (dust particles range from ~5-1000 microns), so we'd enclose it in a vacuum chamber. The super-flatness of our surface would mean that the die would be subjected to minimal frictional/damping forces, so it would roll around and slide for a while. Potential and kinetic energy will eventually dissipate as the die hits the inner walls of our vacuum chamber. Ultimately, the die will reach mechanical equilibrium, and the most elevated side of our die from the super-flat surface (the side should also be parallel with the right number of sides and geometry) will indicate the number we rolled.   

Keep in mind, just because it's microfabricated, doesn't mean the die as a whole has to be microscopic. The sides themselves will be on micron scale, but the whole die itself can be as big as a basketball, or larger. But let's stick to the former for example's sake: Let's make each side of our die a hexagon with sides 10 microns in length, and let's take a regulation basketball having a surface area of roughly 0.184838 m^2. Dividing the basketball's surface area by the surface area of our hexagon means we'd have 1,422,927,945 possible sides to roll.   


Totally possible. Incredibly unfeasible. I can't think of all the problems such a project would present, but the first thing that comes to mind is material requirements which are problematic. We'd need a material that could withstand the impacts of bouncing around without shattering or compromising the integrity of the die's incredibly finite geometry.   

Hope I managed to provide some insight into that.",null,0,cdp504i,1rnn07,askscience,new,1
Astrokiwi,"Most things in the solar system go at 10s of km/s. Earth goes at about 0.01% of the speed of light, or about 30 km/s. Even Mercury is going at less than 60 km/s. So if ISON was going at 0.1% of the speed of light, it's going at 300 km/s, which is more than 5 times faster than the next fastest thing.

This speed basically comes from the Sun's gravity: the comet has ""dropped"" towards the Sun and picked up a lot of speed. The greatest speed you can get from falling downwards (assuming you start pretty stationary) is the same as the escape velocity from the Sun's surface - a bit over 600 km/s. So you'll not likely see anything natural in the solar system going at 600 km/s or faster.",null,5,cdp33ut,1rnn7b,askscience,new,35
null,null,null,34,cdp4gsf,1rnn7b,askscience,new,5
Psygnosis911,"The proteins in the egg denature and congeal when heated, that is what causes them to solidify. It's the same process that causes meat to firm up when cooked.

[Wikipedia page on protein denaturation](http://en.wikipedia.org/wiki/Denaturation_%28biochemistry%29)",null,1,cdp1wop,1rnokt,askscience,new,8
UpboatOrNoBoat,"Well, for starters, you're trying to compare an egg to a homogenous mixture, which has phase states and transitions that are clearly defined.

Eggs are a vast mixture of proteins, lipids, and other molecules. The general solidification is due to the denaturization and reassembly of the vast amount of proteins in the egg that cause it to solidify. Basically the proteins in the liquid form break down due to heating or chemical reasons and as they break, they form strong bonds with surrounding proteins. Eventually, these protein bonds force water molecules out and the egg starts to solidify.",null,0,cdp1wzl,1rnokt,askscience,new,4
Trying_2help,"The main difference between gas giants and rocky planets is that rocky planets are primarily composed of solid materials and gas giants are not, even though the atmosphere of a planet like Venus is very dense it's still only a small part of the total mass. 

A planet like Venus has a solid surface but a gas giant doesn't have a surface at all (at least not a solid one), the gas just gets denser as you get closer to the center. A gas giant can still have a molten core or have that core dispersed throughout the planet, but that only makes up a small part of the total mass.",null,0,cdp2901,1rnpru,askscience,new,3
UpboatOrNoBoat,"Gas giants are more similar to stars than they are planets like Venus or Earth. They are basically failed stars, they've accumulated such massive amounts of gas that they have a very hot and active core, but their mass is so large that any heavy elements don't survive long enough to accumulate due to the intense heat. Either they disperse through the atmosphere of the planet or they stay at the molten core.


You are wrong about their cores, they are completely molten. Gas giants are almost completely comprised of Hydrogen and Helium, and completely lack a surface. Rather, strata are distinguished by the density of the atmosphere.",null,2,cdp21vq,1rnpru,askscience,new,4
dukwon,"Neutrinos are absolutely affected by gravity. In fact, even photons are.

According to General Relativity, all particles will follow the curvature of spacetime: i.e. they are affected by gravity whether they have mass or not.

If an approximately massless neutrino travelling at approximately the speed of light approaches the Earth tangentially to its surface: assuming it doesn't interact, it will be deflected through an angle of about 0.16 millionths of a degree. (Using the gravitational lensing formula).

This is an infinitesimally small angle, and I've chosen the case where it's largest.
The angle is zero if the neutrino passes through the exact centre of mass of the Earth.

Edit: A note about neutrino beams: it is very hard to control the angle at which neutrinos are emitted. To a good approximation, all processes that emit neutrinos do so isotropically (in all directions) in the rest frame of that process. We can use such devices as magnetic focusing horns to direct particles that are likely to give off neutrinos (e.g. charged pions and muons) towards the detector. You don't even need (or sometimes want) to point the beam directly at the detector, for example T2K uses a 2.5 degree offset in order to understand well the range of energies that the neutrinos have.",null,0,cdp2sst,1rnq5s,askscience,new,5
dukwon,"Without the immense gravity, the neutrons would beta decay into protons (and electrons and antineutrinos).

The question really is how much gravitational energy per proton was originally required to force them to electron-capture in the first place. I'm not sure how to calculate it, but it should give you the initial temperature of the resulting matter.

Presumably it's hydrogen, but I guess it's dense enough for fusion to occur, if there's enough energy

edit: Apparently neutron star material is already incredibly hot, so enjoy your explosion.",null,1,cdp6etq,1rnqya,askscience,new,5
super-zap,"The thing that keeps it so compact, is immense gravity, once that is gone it should expand very very rapidly. At the same time the chunk of neutron star is so dense that it should be able to push through any material (if it stayed compact ).",null,0,cdp26i0,1rnqya,askscience,new,3
RelativisticMechanic,"Cosmological expansion only causes things that are already sufficiently far apart to expand away from one another. For things that are relatively close to one another, like the galaxies in the Local Group, the behavior is much more closely approximated by the standard picture of attractive gravity.

In other words, gravitationally bound systems (like galaxy groups) don't experience cosmological expansion. It's only when you consider widely separated galaxies that you find the distance between them increasing due to expansion.",null,13,cdp2yr1,1rnrll,askscience,new,90
yosata,"Basically the gravitational force is much stronger than the expansion force of the universe. Only if the distance between two objects is sufficiently large, then the objects will move away from each other.

In billions of years our cluster of galaxies will be the only thing visible to us in the sky. Without a telescope there wouldn't be much different as the stars keep are kept together by gravity.
The galaxies that are far away will have moved so far away that we can't see them anymore.",null,9,cdp6c1r,1rnrll,askscience,new,22
Thermodynamicist,"At the most basic level, explosions happen because of rapid exothermic processes, which produce extremely large transient power densities. 

Extremely high power densities imply high temperatures, which means that the primary initial method of energy transfer is going to be radiation, because [radiative power varies as T^4](http://en.wikipedia.org/wiki/Radiative_heat_transfer#Radiative_power) and therefore it tends to dominate when T is large. 

If you detonate a nuke in space, you get a big flash of gamma rays, which for practical purposes come from a point source.

If you detonate a nuke in the atmosphere, the gamma rays get absorbed by the atmosphere, heating it up, and then re-radiated at a slightly longer wavelength. Eventually, the wavelength hits an atmospheric window and the flash ""escapes"". What you actually see, if you look at the high-speed footage, is a flash of light which comes from a sphere of finite radius and is composed of a variety of wavelengths of light approximating a Boltzmann distribution (obviously it's going to be missing sundry spectral absorption lines).

This sphere of really hot air then obviously expands violently, due to the gas law, and produces a [*wave of abrupt disturbance*](http://homepages.abdn.ac.uk/h.tan/pages/teaching/explosion-engineering/Rankine.pdf), which is responsible for the blast effects. 

At a high level of abstraction, what's happening is that the really low entropy energy from the explosion is being converted into higher entropy energy by interaction with the medium through which it is being transmitted. 

What this does is to localize the effects of the explosion to a greater degree than would be expected from the inverse square law of [radiation flux](http://en.wikipedia.org/wiki/Radiation_flux).

Shrapnel released in the atmosphere is subject to aerodynamic drag, which converts its kinetic energy into heat. 

Shrapnel released in space just keeps on going until it hits something. 

What this means is that explosions in space can cause radiative damage (e.g. to sensors) at very long distances, but will generally be much less likely to cause physical damage at short to medium ranges because there are no localizing effects. 

It also means that there is a small probability of devastating shrapnel impact out to arbitrary range. 

Overall, the effects are more stochastic, because the absence of localized interactions with a coupling medium such as a fluid or a solid mean that all the work is being done by a smaller number of higher energy particles. ",null,36,cdp6d37,1rnt7q,askscience,new,189
xtxylophone,"There would be no shockwave, all the energy would be in electromagnetic radiation of varying wavelengths i.e. visible light and infrared. Also it would accelerate any part of the bomb left over.

So the only way an explosion would do damage is if it were actually touching the target on detonation, it was hit by shrapnel from the blast or the radiation is intense enough to damage the target.

EDIT: Also as /u/super-zap said, expanding gas",null,10,cdp43xx,1rnt7q,askscience,new,25
Ferociousaurus,"Semi-related question. I've often heard there would be no fire in an explosion in space, and movies often get this wrong. Let's use the Death Star as an example. If the Death Star is presumably mostly sealed and full of oxygen, wouldn't there briefly be some fire in an explosion, albeit quickly snuffed? Same with destroyed fighter ships on a smaller scale? Or would the vacuum disperse anything that could catch fire too quickly?",null,2,cdp5pr1,1rnt7q,askscience,new,10
TangentialThreat,"The shrapnel usually has a larger kill radius than the overpressure.

With no atmosphere and in microgravity, shrapnel would *never slow down*. Only a real dick would set off a pipe bomb in orbit, and if you did it would work perfectly because the expanding hot gas is confined.

Armor-piercing shaped charges should also function properly as the expanding gas does not have to go very far.",null,1,cdp4r42,1rnt7q,askscience,new,6
Your_ish_granted,An explosion and the resulting shock wave is a rapid energy release being propagated through mass (air or water). So if there is no mass (in a vacuum) the energy is transfered in the fragments of the bomb (hot gasses etc) and as energy in wavelength form (ultraviolet etc). ,null,2,cdp4en0,1rnt7q,askscience,new,4
0hLaVache,"Since we're talking space explostions... 

Say we're trying to deflect an incoming earth-bound asteroid, and for whatever reasons have decided to go with an explosive deflection.  

What kind of explosion would be best to push the asteroid?  Atomic?  Thermonuclear? Conventional?  

Given the relative weakness of space explosions described in this thread, what is the biggest asteroid at what distance which could reasonably be nudged away from our poor little planet?  

Thanks!",null,0,cdp9lez,1rnt7q,askscience,new,1
null,null,null,9,cdp4hz3,1rnt7q,askscience,new,1
mrrp,"Who says they're not flying towards the moon (or otherwise using the moon for navigation?)

If you're out in a field and you want to walk in a straight line you might adopt a strategy of keeping the moon on your right. If you replace the moon with a lamp you're going to be circling the lamp. If you keep the lamp at 45 degrees rather than 90 you're going to quickly spiral right into it.",null,2,cdpdd3z,1ro0cm,askscience,new,3
piantado,"I believe that the concensus is that they fly at a constant angle to the brightest light, which happens to be the moon in their natural environment. Because the moon is so far away, this keeps them flying in a generally straight line. But, when artificial lights are around, they try to fly at a constant angle to them too (mistaking them for the moon), and end up spiraling inwards.",null,0,cdpzamw,1ro0cm,askscience,new,1
Das_Mime,"The planets of the solar system formed out of an accretion disk of gas and dust. These disks are usually flared, meaning that they get thicker as you go outward from the center. Thus there is a larger amount of material at larger radii.

The heaviest elements were preferentially sorted toward the inner parts of the disk, while radiation pressure tended to push the light elements outward. This is what resulted in the inner planets being low-mass objects comprised of heavier elements while the outer planets are higher mass with lighter elements.",null,0,cdpgnc8,1ro139,askscience,new,1
null,null,null,4,cdpglmr,1ro139,askscience,new,1
zeug,"Very rarely proton-proton (or proton-antiproton) collisions at a high enough energy will produce top quarks. Typically these come in a top and antitop pair. Some even more rare collisions will result in a single top quark without a corresponding antitop. These are the ""single top quark"" events originally observed at Fermilab.

In either case, the top quark, or top-antitop pair, is not directly observed. What is seen in the detector is a spray of hadrons called a ""jet"". The very brief existence of a top quark is inferred from the patterns of jets and other observed particles produced in certain collision events. ",null,1,cdpe9v2,1ro2zy,askscience,new,5
cppdev,"Combining multiple flash units together is actually already how SSDs are made. For example, if you buy a 128 GB SSD, chances are it has 8 16 GB NAND chips wired together. 

As for why you can't combine thousands of little SSD chips to get very fast speeds, the answer is somewhat technical. In digital circuits, each time you have what's called ""fan in"", or multiple wires converging together there is associated with it some input capacitance which is proportional to the number of wires that are converging. So if you have 1000 wires from 1000 flash chips converging to one output (e.g., your SATA cable) you're going to have a very high input capacitance. However, the speed at which you can operate a circuit is inversely proportional to the input capacitance. 

TLDR: You could connect 1000 SATA chips in parallel, but it would end up being much slower than you'd expect.",null,0,cdph1pe,1ro8bz,askscience,new,2
TangentialThreat,"You may be looking for [this paper](http://grad.physics.sunysb.edu/~amarch/Walborn.pdf).

The single-photon experiments are just creepy and feel very intuitively wrong; there's no way to interpret it without a particle being in more than one place at once. The worst of them all is probably the [delayed-choice version of this experiment](http://www.youtube.com/watch?v=P2cMFrXqbYM) wherein the which-path information is erased *after* the photon has hit the screen.

",null,0,cdpfccj,1rodeo,askscience,new,6
Platypuskeeper,"Yes, you see that interference patterns you see in Young's double-slit experiment or a Michelson interferometer are still there even when you work with individual photons. 

",null,1,cdph2jb,1rodeo,askscience,new,5
The-Grim-Reefer,"If you do this your fish will most likely die. Marine fish have evolved to produce very concentrated urine while absorbing most of the water they take. While some marine fish may be able to tolerate a freshwater environment (called euryhaline fish), most will absorb too much water, creating a hypotonic solution within the fish, effectively killing it. My advice to you is to do some research on your particular fish and see if they can tolerate living in a freshwater environment. If they can be sure not to place them directly in freshwater because it will shock and kill them.",null,0,cdpef1o,1roeuz,askscience,new,4
patchgrabber,"It depends on the fish. Some fish are euryhaline, meaning they can live in a differing concentration of salt in water. Salmon are one such example; they are born in freshwater rivers but spend most of their lives in the sea, returning to spawn. But they have to go through an acclimation period where they slowly lower the concentration of salt in water. I wouldn't try this with your aquarium fish, though.",null,0,cdpj5ib,1roeuz,askscience,new,3
yankee333,"Probably not....

This depends on the species of fish, but most have physiologically adapted to the salinity of their environment. They have specialized ion channels in their cells that regulate osmotic pressure. When you change those gradients the cells can swell or burst, basically killing the fish.",null,1,cdpr6bz,1roeuz,askscience,new,1
NotFreeAdvice,"The answer has to do with the idea of bubble *nucleation*.

So, it turns out that the release of carbonation (ie. the bubbles) is thermodynamcally favorable.  This means that the CO2 would ""like"" to exit as bubbles.  

There is just one problem: to get a bubble, you must first form a bubble.  The *start* of bubble formation (what we call nucleation) is *not* thermodyanmically favorable.

The reason is entropic.  There is much more ordering at the surface of a bubble than inside the bulk bubble or the bulk water.  This presents a barrier to bubble formation that is related to the surface area of the bubble.

At the same time, there is a enthalpic reason to form bubbles, and this related to the *volume* of the bubble. 

The end result is this: when the bubbles are quite small, the effects from the surface area of the bubble, dominate the effects from the bulk of the bubble.  And the bubble will not form -- or forms very slowly.  

Once a bubble forms, then it will grow more easily.  So the bottleneck, if you will, is the initial formation of the bubble.  

By shaking the beverage, you introduce a bunch of gas bubbles into the beverage, and this provides sites for bubble growth -- and an increase in released carbonation.  Essentially, you are providing the mean by which to skip the slow ""nucleation"" step.

Hope that makes sense!",null,0,cdppqxy,1rofgc,askscience,new,2
Das_Mime,"Yes, there are stars that are very close to each other. In environments like the nucleus of our galaxy and globular clusters, stars can be packed extremely tight. Globular cluster cores can have dozens of stars within a cubic light year. By contrast, the nearest star to us (Proxima Centauri) is 4.2 light years away.",null,0,cdpcsvj,1roh5n,askscience,new,4
trebuday,"Yes, you can see storm clouds from the International Space Station!  See [this](http://i.huffpost.com/gen/1231373/thumbs/o-STORM-CLOUDS-900.jpg?6) cool photo, from July 4, 2013, showing storm clouds over the Atlantic ocean.  This was shot with a 50mm lens, which approximates the width of view of human vision quite well.

However, while you are seeing the bottom of a storm cloud, the ISS astronauts only see the top!  [This](http://www.youtube.com/watch?v=KaOC9danxNo) music video by former Commander Chris Hadfield of the ISS has some nice shots of the view of Earth from the ISS.

According to [wikipedia](http://en.wikipedia.org/wiki/Visual_acuity#Normal_vision), 20/20 vision is defined as being able to see an object 1 arc-minute wide.  This means a person with 20/20 vision can see something a little over an inch tall at 100 meters away.  The ISS orbits at 370km, which, according to [wolframalpha.com](http://www.wolframalpha.com/input/?i=one+arc-minute+at+370km), means someone looking down from ISS could see something as small as 107.6m wide, or about the size of an American Football field. 

",null,0,cdpdlnu,1roi9d,askscience,new,3
iorgfeflkd,"Yeah, the Earth gets hit all the time by cosmic rays which are mostly high energy protons. Older TV and computer monitors use beams of electrons to produce images on the screen. Free neutrons come from radioactive decay and nuclear reactors.",null,0,cdpdpya,1roisb,askscience,new,8
null,null,null,0,cdpfvyd,1roisb,askscience,new,3
Muh-Freedoms,"A loose proton flying around is called Hydrogen.
Neutrons may exist outside of a nucleus, but they are unstable and [decay](https://en.wikipedia.org/wiki/Free_neutron#Free_neutron_decay).
Electrons may be free as well; cathode ray tubs in your old TVs shoot electrons, for example.

",null,2,cdpdt9k,1roisb,askscience,new,3
joca63,"Neutrons are produced by nuclear decay in bombs, and reactors.
Electrons are produced in a similar manner (termed beta decay).
The bare helium nucleus is produced by alpha decay, again in nuclear reactions.
Protons are a bit different, they can also be expelled in radioactive decay, but they are also seen frequently in acid base reactions. To be clear, if a chemist uses H+ they are referring to the reactive equilivent. An example would be HCL + H2O --&gt; H+  + CL-  What actually happens would be more accurately described as HCL + H2O --&gt; H3O+  + CL-  With the proton being bound to the oxygen in a water molecule. The reason it is so frequently described as just H+ is that the speed at which the protons bounce from solvent molecule to solvent molecule is insanely fast. That proton doesnt have a ""home"" oxygen that it sticks around so it can be described as a free proton in the solvent
",null,0,cdq15pt,1roisb,askscience,new,1
remarcsd,"Seebeck

The materials with the greatest difference in Seebeck effect are the ones from which the greatest energy can be harvested. Of the current standard thermocouples, Type E (chromel constantan) gives the greatest output",null,0,cdpd38b,1roj3s,askscience,new,1
LukeSkyWRx,"The two primary material properties that can be manipulated for thermoelectric are thermal and electrical conductivity. Ideally for a given material you would want a low thermal conductivity and a high electrical conductivity. An easy way to decrease thermal conductivity is reducing the grain size of the material but that also typically hurts electrical conductivity so it depends on which property changes fastest.

As to what material is best, that depends on the temperature you want to operate at. The best material at one temperature will not be the best at a higher or lower temp so your heat source temp plays a big role.",null,0,cdpue22,1roj3s,askscience,new,1
gingerkid1234,"I don't know if that shape has a name.  However, shapes whose volume formula is unknown can have it calculated by integrating over the volume of the shape.  All that's required is knowing equations that describe the boundaries of that shape.  It's worth mentioning that this is how the volume equations of shapes you're familiar with are generally derived.  The following may be a bit confusing if you don't know calculus.  In case you don't, the basic principle of integrating a function is that you add the infinitesimally small heights under a curve to get an area, or the areas under a curve to get a volume.

I'll use the xy plane for the cross section, with the flat surface along x, and z for the length dimension of the shape, and assume that the rate of flattening is linear, and that it tapers to a point.  The easiest way to integrate it, it seems to me, is to break it up into triangles in the yz plane, and integrate along z.

I'll use r as the radius at the un-squished end, and L as the length.  I'll also put the center of the un-squished cross-section at the origin.  Now, on the xy plane, a y coordinate along the boundary can be expressed as y=sqrt(r^2 - x^2 ).  If the squishing of the shape is linear, the yz sections will be triangles with height 2 * sqrt(r^2 - x^2 ) and length L.  The area of each will be L * sqrt(r^2 - x^2 ).  A brief sanity check shows that the areas are 0 at x=r, where the height tapers from zero to zero.

By integrating these areas along x the volume can be determined.  The integral in question is the integral of L * sqrt(r^2 - x^2 ) with respect to x as x goes from -r to r, which is equal to V.

This integral isn't terribly easy to solve, but there's a way around actually doing it out.  By inspection, sqrt(r^2 - x^2) is just the y coordinate of a circle.  The integral of this from -r to r should just be the area of a semicircle, pi * r^2 /2.  And plugging the definite integral into wolframalpha, it is!

Plugging that back in and cancelling the 2, V = 1/2 * L * pi * r^2, or half the area of a cylinder.  And intuitively, that makes sense--each cross-section of it is a triangle with the same base and height as full rectangle.  And that shape made of rectangles is a cylinder.  So the volume will be half that.

An important practical note is that squishing a real-life cylinder will result in something significantly different, because just squishing the end won't produce a constant taper throughout the cylinder.

tl;dr it can be determined by summing all the arbitrarily thin cross-sectional volumes of the shape, each of which is an area times an arbitrarily small increment in the direction we're slicing.  By doing out this calculus, the area is A = 1/2 * L * pi * r^2 , or half the volume of a cylinder.  This makes sense when you think about how a cylinder and this shape differ.",null,36,cdpdfg9,1rokpj,askscience,new,176
bzishi,"Don't get drawn in, don't get drawn in. Damn.

The key point nobody is discussing is that when you pinch a cylinder, the circumference of the cross section remains the same. You can't simply pretend it is an ellipse where you vary the semi-minor axis from r to 0 while the semi-major axis stays the same. On a plus note, you don't have to use the pain-in-the-ass ellipse circumference equation for substitution, since you know the end points and can write a linear function. The flat 'ellipse' (or line) at the end will simply have a half-length of r*pi/2. The semi-major axis (x counting up from the pinch) will then be 

a = (r-r * pi/2) * x / L + r * pi/2. 

Let q = r * pi/2 =&gt; 

a = (r-q) * x/L + q =  rx/L -q(x-L)/L. 

The semi-minor axis will be 

b = rx/L

The area is then 

A(x)= pi * a * b = pi * (rx/L - q(x-L)/L)(rx/L) = pi/L^2 * (r^2 x^2 -qrx(x-L)) = pi/L^2 * (r^2 x^2 -qrx^2 + qrxL)

Integrating:

V = pi/L^2 * (1/3 * r^2 * L^3 -1/3qrL^3 + 1/2 * qrL^3 ) = pi * L * (1/3 * (r^2 - qr)+qr/2) = pi * L * (1/3 * r^2 + 1/6 * qr) = pi * L * (1/3 * r^2 +1 /12 * pi * r^2 ) = pi * r^2 * L * (1/3 +pi/12) =

V = 0.595 * pi * r^2 * L (my best estimate)

Check my work, because I might have had some Thanksgiving rum and we all know not to drink and derive.",null,4,cdpik0n,1rokpj,askscience,new,48
yeti_manetti,"For a ""tube"" of length L where the radius of the circular end is R, I get:
V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

So its the volume of the cylinder multiplied by approximately 0.595.

My approach differs from gingerkid1234. We make the same assumptions but the expression of y and z are simpler I believe. 

Lets orient the shape  with the end of the tube that is a line lying on the y axis centred at 0, and the tube going out in the positive x direction. 

The xy plane cross-section should be a quadrilateral through the points (0, (pi * R)/2), (L, R), (L, -R), (0, -(pi * R)/2). So the line from (0, (pi * R)/2) to (L, R) defines the semi-major axis of the elipses that make up zy cross sections of the tube. The line defined here is: y = (R/L) * (1 - pi/2) * x + (pi * R)/2.

In the xz plane, we have a triangle defined by (0,0), (L, R), (L, -R). So we see the semi-minor axis of the eliptical cross-sections is defined by z = (R/L) * x.

Using the formula for the area of an elipse A = pi * x * y where x is the length of the semi-major axis and y is the length of the semi-minor axis. [ a = b for a circle hence A = pi * r^2 ]

If we plug in the formulas for the lines defining the semi-major and semi-minor axes and integrate over x from 0 to L, we get:

V = pi * R^2  * L * [1/3 - pi/6 + pi/4]

EDIT: Since I see gingerkid got something different, I looked it over again and found some differences. I'm open to critiques!",null,2,cdpdiqc,1rokpj,askscience,new,14
anon5005,"I am thinking that if the flattening is happening linearly in one dimension, then the area of the cross section each proportion r of the way from the squished end to the ordinary end has r times the area of the ordinary end.

Meaning, the cross section halfway up has half the area, and so-on. 

As always, the volume is the height times the average cross sectional area. Here, since the area goes linearly it is the same as the height times the area when it is cut at the half way point.

So the squishing cuts the volume in half.

You can see an example if you do it to other shapes, like a 1x1x1 cube.  Then you get what looks from the side like a triangle of area 1/2 thickened up to have thickness 1. So the volume is 1/2 x 1 = 1/2.",null,1,cdpejjk,1rokpj,askscience,new,3
brewsan,"Here's my take.. 

The cross-section is an ellipse (the very bottom being a circle and the very top being an ellipse with the y radius being 0 i.e. a line).  Let's put that on the XY plane. Note that x never changes from the radius(R) at the bottom, regardless of where along z we are. 

So the area of the ellipse is A=pi * x * y = pi * R * y

y changes as we travel up the z axis linearly from R at the bottom(z=0) to 0 at the top (Z=H) so y=R-(R/H)z = R * H/H -R/H * z = R/H(H-z)

Subbing in the A=pi * R * (R/H(H-z)) = pi * R²/H(H-z)
Now to get the volume we need to integrate over z (think of this as adding up all the cross sections of ellipses).

V = Int[z=0 to H]pi*R²/H(H-z)dz 

= pi * R²/H * Int[z=0 to H](H-z)dz 

= pi * R²/H (H * z - 1/2z²) |z=0 to H

= pi * R²/H (H² - 1/2*H²) 

= pi * R² * 1/2 * H

= 1/2 * pi * H * R²
",null,0,cdpfaz9,1rokpj,askscience,new,3
YouDoNotWantToKnow,"O      --

Orient your axis at the center of the circle at the bottom of the cylinder. The circle is radius r and let it exist in the x-y plane, so cylinder extends into the z plane. Let the line at the other end of the shape, the ""squish"" point, also be described by y=0, z=h where h is the height of the cylinder.

Now look at the y-z plane in slices for x in {-r,r}. For any given x in this range, the y-z plane contains a triangle. That triangle has a height of h and a base of 2\*sqrt(r^2 - x^2 ).

Now you have a simple calculus integral.

Integrate [sqrt(r^2 - x^2 ) \* h]/2 for x from 0 to r and multiply that by 2 (and to make that simpler, there's a /2 in the integral so just pull that out and divide it out right away).

There's probably a more elegant way of going from here, but to brute force it you can look up (or derive using trig substitution) that the indefinite integral of [A - x^2]^(1/2) is I = 1/2(x(A-x^2)^(1/2) + A tan^(-1)(x(A-x^2)^(-1/2)

Looks messy, but just plug in the integration points. A = r^2 btw, so the definite integral from 0 to r is I(r) - I(0). I(0) is easy, there are x's in both numerators, so I(0) = 0.

I(r) causes the first time to drop out, but the second term inside the inverse tangent goes to infinity. tan^(-1) of infinity is pi/2.

So the integral comes out to just A\*pi/2, don't forget the height constant we left out earlier and that A=r^2.. the final solution is:

V = h\*pi\*r^(2)/2.

Does that make sense? It's half the volume of a full cylinder. So that is the right direction (less). Is it correct? Well, do you think we could form the same shape out of the volume we took out? Think about each slice we made at the beginning. Initially they would have been rectangles instead of triangles right? What part of each rectangle is missing? The area of the triangle is b*h/2. The area of the rectangle is b*h. So the missing area is bh - bh/2 = bh/2. Half the rectangle. So it's pretty clear this makes sense and is the correct result.


And now I read the other top reply and it has a similar derivation and the same solution. Sweet.",null,0,cdpfd5k,1rokpj,askscience,new,3
CoorsLightNTits,"You have two sections here that you can effectively add together if you cannot come up with equations and integrate.  The shape is a cylinder sitting beneath a cone, if I am to understand your description accurately.  This is simple enough a manner to describe the volume. 

Cylinder volume = Vc = .25*(pi*hd^2)

Cone volume = V▲ = bh▲/3

This assumes straight sides with the pinched end. Introducing curvature maked this approximation less accurate, but it's useful enough to get close for most any application. 

Furthermore, to simplify, b = d/2, thus V▲ = dh▲/6

TL; DR Approximately V = .25*(pi*hd^2) + dh▲/6",null,0,cdpgvp7,1rokpj,askscience,new,3
null,null,null,0,cdpeg4o,1rokpj,askscience,new,1
Diogenes_Laertius,"Would anyone be able to get the equation for a 3D Plot of this shape? I would prefer it to be in the program ""Grapher"" on Macintosh. It will be used in a philosophy class to demonstrate how perspective changes conclusions. If you look at it from one direction, you see a disc. If you look from another, you see a triangle, and from a third angle you see a square. ",null,0,cdpgxnt,1rokpj,askscience,new,1
null,null,null,1,cdpg49w,1rokpj,askscience,new,1
U235EU,"Yes, it is known as the polar route. Virgin Airlines has been cleared to fly almost directly over the North Pole for a UK to Fiji flight. See more information here: 

http://www.dailymail.co.uk/news/article-2078301/Mind-sleigh-Airlines-given-permission-fly-North-Pole-time-slashing-hours-exotic-destinations.html",null,4,cdpcf1j,1rom21,askscience,new,13
howardcoombs,"Here is an old discussion thread showing various routes over the poles 
http://www.airliners.net/aviation-forums/general_aviation/read.main/1918010/",null,0,cdpewwa,1rom21,askscience,new,1
trebuday,"Nope!  Take a looks at wikipedia's article on [elevation](http://en.wikipedia.org/wiki/Elevation), which describes how the average elevation of all of the land is around 0.8km, whereas the average depth of the oceans is 3.7km.  Since the oceans comprise about 71% of the Earth's surface, (approximate 2.4:1 ratio of ocean to land), we can estimate that if all the land above sea level were put move to the bottom of the ocean, the average ocean depth would decrease by only 0.33km (to ~3.4km).  

I am now really interested in what the average elevation of the entire planet is...  ",null,1,cdpcq2c,1romjn,askscience,new,3
chicken_slaad,"Most water has various gases dissolved in it, like oxygen or carbon dioxide.  The warmer the water is, the less gas it is able to hold--that's why your soda foams up so much more when it's hot than when cold.  Also, the less pressure the water is under, the less gas it's able to hold--that's why your soda foams up when you open the can and release the pressure.

Water comes out of the tap, where it was cold and under pressure, and starts to warm up.  These gases come out of solution, forming bubbles.  The bubbles formed at the wall of the glass tend to stick there until they get big enough to break away, which sometimes never happens.",null,2,cdpd5xy,1ronfo,askscience,new,14
sammmmmmmmmm,"The air bubbles stick to the 'dirt' on your glass. Try drinking from that glass a couple of times, letting it dry inbetween, you should notice more and more bubbles forming as your glass gets 'dirtier'. These bubbles should mostly dissapear after properly washing the glass. ",null,11,cdpend5,1ronfo,askscience,new,3
null,null,null,110,cdpdcox,1ronxr,askscience,new,513
yogurtsurprise,"&gt;why don’t your teeth heal on your own like other bones in your body?

The crown of teeth (the part that is visible) are made of two primary materials, enamel on the outer surface and dentin which makes up most of the tooth structure (with pulp in the centre which contains blood vessels, lymph and nerves). When the crown forming there are made of two major cells types. Ameloblasts and odontoblasts. First the dentin in formed by the odontoblasts at the centre of the tooth and dentin is formed outwards. The ameloblasts start from the DEJ (dentinoenamel junction where enamel and dentin meet) and move towards the outside of the crown. They are lost during tooth eruption (when the tooth comes through the gums). 
The tooth is unable to heal since it cannot produce enamel after eruption because there are no cells to make new enamel.

Dentin on the other hand can and does form after the tooth is already in the mouth (secondary dentin forms after the tooth is in the mouth and more dentin is made towards the centre of the tooth; tertiary dentin is made in response to inflammation of the pulp often due to cavities). So in one sense the tooth is healing. However, it doesn't heal like bones since often a tooth fractures in the middle of the tooth and the odontoblasts cannot move through already formed dentin to make new tooth structure on top of old tooth structure (and as the ameloblasts are gone, no new enamel can be produced). 

While I'm not as familiar with bone healing, bone is a vascularized (blood containing) cellular tissue. Osteoblasts (which synthesis new bone), and osteoclasts (which eat old bone) are always working even when there is no damage. So when the bone is broken, these cells can work to remodel and repair the bone. 

I hope this helps answer your question",null,29,cdpf0fv,1ronxr,askscience,new,164
Swoondoc,"Saliva is there to help protect your teeth and even has some immunoglobulin A in it along with enzymes.  So you can make the argument that your immune system DOES fight off bacteria that causes cavities.

The constant flow of saliva helps clean food debris from your teeth, and the keep the pH levels slighly basic to cancel out any acids that can wear away at the enamel.
",null,22,cdpf1ca,1ronxr,askscience,new,107
null,null,null,11,cdpeut3,1ronxr,askscience,new,34
handle8,"Your body does have some minimal defenses against bacteria. Saliva has some antibodies, specifically IgA antibodies which help prevent bacterial absorption, but mostly through mucous membranes rather than bone.

Saliva also has defensins or proteins that have anti-bacterial activity.

However, the mouth is an extremely unsterile area. There are a myriad of bacteria that grow in the mouth naturally. On teeth, bacteria can form biofilms, or collections of bacteria that become thick and have special secretions that prevent binding of antibodies or other natural host defenses. This is a problem even with decent blood supply to provide these host defenses to the inside of teeth.",null,4,cdpeyob,1ronxr,askscience,new,23
El_Dentistador,"So far all the other comments are spot on.  The cells that form the enamel and dentin in teeth are ameleoblasts and odontoblasts respectively.  The ameleoblasts work from the DEJ out while the odontoblasts work from the DEJ in.  The cell bodies of the odontoblasts are mostly at the edge of the pulp only extending processes through tubules within the dentin, and your ameleoblasts are long gone once they finish forming the enamel. This leaves enamel and dentin without any significant intrinsic method of healing, and your immune system has no access to these two tissues either as they are avascular.
By the time your immune system gets to see the infection the bacteria have breached the pulp chamber. Much like the brain, pulpal tissue  cannot handle inflammation well since it's completely encased by hard tissue.  Your own inflammatory response increases the intrapulpal pressure dramatically, and it hurts.  The pulpal tissue will necrose and the bacteria now have an easy path through the canals to the periapical tissues, which can lead to more serious infections like cellulitis and osteomyelitis.  There have actually been cases where odontogenic infections have led to fatalities, which should never happen.  The most famous case is Deamonte Driver. 
",null,1,cdpgs6c,1ronxr,askscience,new,13
mkav8,"Also the teeth have an outer layer called the pellicle. The pellicle is where plaque formation first starts and eventually can become calculus. A question I thought of when first beginning cariology (study of cavities) was why don't antibiotics stop caries? And the answer is because bacteria in plaque and calculus form a biofilm. This biofilm forms a protective layer and also has channels between adjacent biofilms. These channels allow the transfer of genes which can cause some minor resistance in these strains. 

Enamel is formed by ameloblasts which deposit enamel in an inside to outside direction. After the enamel is deposited the ameloblasts either die off or can form a cuticle layer. Dentin however is formed by odontoblasts and can be deposited through a lifetime in an inward direction. So in a way teeth can heal to a small degree because if there is insult or resorption of dentin possibly due to inflammation or trauma, the tooth can lay down tertiary dentin in order to repair it as long as it is on the pulpal side. ",null,0,cdph1sv,1ronxr,askscience,new,7
We_have_no_future,"A reason why we get cavities is because several thousands of years ago our diet changed. Evolutionary we were not designed to base our diet on carbs (such as rice, beans, corn, and other cereals). 

http://www.abc.net.au/science/articles/2013/02/18/3691558.htm
&gt;Mesolithic hunter-gatherers living on a meat-dominated, grain-free diet had much healthier mouths that we have today, with almost no cavities and gum disease-associated bacteria, a genetic study of ancient dental plaque has revealed.",null,10,cdpfj24,1ronxr,askscience,new,16
ThatInternetGuy,"&gt; Why doesn’t your immune system fight off the bacteria that causes cavities?

Saliva contain enzymes such as Lysozyme that kill bacteria. Saliva also contains mucus which contains IgA antibodies. While antibodies don't kill bacteria, they serve the first line defend by finding and marking the bacteria as unsafe foreign bodies so that the white blood cells can finish them off as soon as possible *if* the bacteria or virus from our mouth enter our body (through gum for instance).

Cavities are caused by acid in your mouth, consumed by you (phosphoric acid in sodas) and converted from sugar to acid by some bacteria in your mouth.

&gt; why don’t your teeth heal on your own like other bones in your body? 

Tooth enamel do heal. It's called remineralization. The flouride ions in toothpaste together with minerals in your saliva help regenerate teeth enamel. [More info](http://health.howstuffworks.com/wellness/oral-care/procedures/remineralization-of-teeth.htm)",null,2,cdpfy0z,1ronxr,askscience,new,6
jgrun,"Just to be clear its not the bacteria in your mouth itself that directly causes cavities. Rather its the byproduct of their metabolic processes, which are acidic, that wear away at your enamel. Some people are more prone to cavities than others because their saliva can neutralize that acid less effectively.",null,0,cdph40n,1ronxr,askscience,new,4
Dymethyltryptamine,"I need to mention that tooth decay is not only the result of bacteria and acidity. You can have a really good dental hygiene but still suffer from tooth decay because of dietary deficiencies. You need certain minerals such as calcium and phosphorous along with vitamins and other minerals that regulate the absorption of these. If lacking, your body will leech minerals from your teeth to deposit into your skeleton. ",null,0,cdpivip,1ronxr,askscience,new,4
crankfair,"We did not evolve to eat so much refined grain, which sticks between teeth.  The worst I have ever had is corn flakes or frosted flakes- they leave a layer ON your molars, not just in between.

That becomes a food source for the bacteria.  So our immune systems cant handle it because they have not been selected to since that was not a factor in our traditional environment of evolutionary adaptation.",null,0,cdpkbn4,1ronxr,askscience,new,3
flansiro2zero,"I have never had a cavity and im now into my 4th decade on this earth. I always wondered about the science behind this. I brush my teeth once a day if I remember and that's about the height of my dental hygiene routine.
A dentist told me a few years ago that I was part of a small proportion of the worlds population that was ""immune"" to cavities. Any credence to this theory or am I just a lucky guy?",null,0,cdpmxql,1ronxr,askscience,new,1
PumPumPumpkin,"When enamel gets damaged/broken, the stuff that makes it gets damaged/broken too. But it also starts to get harder over time as soon as it starts to get damaged, so that's pretty cool I guess. 

And our immune system can't exactly reach the outside of our teeth. Unless you would want us to suddenly start oozing some sort of immune-system goo out of our teeth/tongue to clean them, that's why we have tooth-paste, like a friend for your immune system. Saliva helps too, but not a lot in the long run.

It's not as sciencey as some other answers, but just felt like chipping in.",null,2,cdpgwrw,1ronxr,askscience,new,1
wampa-stompa,"Your teeth will rebuild enamel as long as your saliva has not become acidic (due to too much sugar being broken down) and fluoride is present. Also, there can't be a hard layer of plaque in the way for obvious reasons. This is why brushing and flossing is so important, and why water supplies are treated with fluoride (not mind control). It pays to keep this in mind.",null,1,cdplkyv,1ronxr,askscience,new,1
InfinitePS,"No one here has pointed out the importance of the oral microflora. It seems that ""good"" bacteria that coevolved with humans and reside in our mouths actually have the power to fight off other ""bad"" bacteria. Of course, fluoride in our toothpaste kills off all bacteria whether good or bad. In fact, a particular strain of bacterium Lactobacillus reuteri has been scientifically shown to reduce tooth decay. 
This strain is patented and I have no personal interest in promoting it (One can assume that there are a lot of other bacteria that behave similarly). Keep in mind, we live in a capitalist society; I may be a little cynical, but the American dentist association really has no particular interest in finding real strategies to fight off tooth decay. But it seems that the oral microflora may definitely play a huge role.",null,1,cdpm2vh,1ronxr,askscience,new,1
null,null,null,9,cdpgd9k,1ronxr,askscience,new,5
zadzad,"Here's whats really going on.

Look up phytic acid and cavities in google. Mostly its bread and nuts that cause cavities as it eats enamel. Once the enamel is eroded sugar will quickly eat to the pulp (sugar is also acidic but much weaker than those found in oatmeal, wholewheat bread etc.). 

Conversely, if you have tartar thats a more alkaline condition and has different health problems. Not cavities though more gingivitis.

The answers in my post will pretty much reverse any tooth sensitivity. You can try eating oatmeal frequently and see how your teeth feel. Mind you milk is alkaline so it will slow the effect but not stop it.

",null,18,cdpf8fc,1ronxr,askscience,new,5
remarcsd,"Providing that the heat is not lost quicker than it can be created--so a really well insulated container may be required.

Note: centrifugal pumps--really common--have an impeller in them. If there is no liquid flow through the pump, and the impeller remains spinning in the same volume of liquid, it may only take a few minutes until the liquid is dangerously hot.",null,0,cdpcjv2,1ronyp,askscience,new,6
wesramm,"Immediately.  Adding work is thermodynamically equivalent to adding heat in increasing the internal energy of a fluid.  Since heat transfer takes time, the excess heat will escape to the environment and reach equilibrium, but instantaneously, the temperature will certainly increase.  A whole slew of Victorian era scientists in the US, UK and Germany performed careful experiments with paddle-wheels immersed in water tanks.  The paddle wheels were powered by falling weights, and to within a very high degree of correlation, it was found that the amount of temperature increase was proportional to the work added to the system following the first law of Thermodynamics.  Here is a link for you that talks about it quite nicely:
[Joule](http://en.wikipedia.org/wiki/James_Prescott_Joule)",null,0,cdpjng1,1ronyp,askscience,new,4
leftoveroxygen,"Yes.

Source:

I once made a camp shower with a 250-watt submersible pump in a 5-gallon bucket.

To my surprise, the pump became a *very effective* 250-watt water heater if I let the water circulate for about 20 minutes. All that energy has to go somewhere, and it ends up as heat.",null,1,cdpffco,1ronyp,askscience,new,2
darthmunkeys,"It will approach and equilibrium temperature assuming it is not insulated or perfectly insulated. Basically it will only get to room temperature. the shaking you do to it will not noticeably change it.

If the water starts cold then it will tend toward room temp, in a non insulated canteen. In a perfect insulated canteen the air and water would reach an equilibrium temp for just the inside of the perfect canteen. But in the real world, the outside air transfers energy into the canteen wall which is transferred to the water on the surface of the canteen. By mixing the water, through shaking the canteen, the water more quickly gains the energy from the canteen wall, thus bring the temperature up. 

In you could shake it extremely fast, then the energy you supply by shaking becomes more and more of a factor in the overall temperature of the water. If you could shake it at near lightspeed, which would mean lots of acceleration changes, then the water might turn into a gas then plasma as you shake the molecule apart into ions, at this point though shaking anything would do that. 


TL;DR shaking the canteen will make cold water reach room temp faster than letting it sit. It is a convection thing. ignore my last paragraph, im tired. ",null,2,cdpcq3y,1ronyp,askscience,new,3
MarineLife42,"When they say things in documentaries like 'sharks can smell blood for five kilometres' then this is at least misleading.  
Smelling is a chemical interaction so to do that, the molecules that make up the odor must be present in the olfactory organ - at zero distance. 
However, sharks have a very acute sense of smell and can smell blood that has been greatly diluted in water. Thus they can follow a trace of blood for several kilometres that will (hopefully) lead them to a meal.  
Like you and me only being able to smell something downwind from a source, sharks can only pick up a smell that a current brought to them, or was artificially laid out by people on a moving boat throwing fish remains into the water. ",null,0,cdpxd5q,1roywu,askscience,new,1
Entropius,"Not to the best of my knowledge.  I've seen different sources and textbooks give slightly different ranges for even common visible colors.  They'd disagree on exactly where green or blue begin &amp;amp;amp;amp; end.  But not by much, mind you.


You might think this would be a problem for scientists since they might disagree on their definitions for x-Ray ranges but in practice there's no problem since you're going to be referring to light by it's exact numeral wavelength, not broad terms like ""x-Ray"".",null,0,cdpr0t4,1royxd,askscience,new,2
Manzikert,"There's no fundamental difference between different categories of EM radiation. Energy increases proportionally with frequency/inversely with wavelength, regardless of what arbitrary category it slots into. ",null,0,cdpvfj0,1royxd,askscience,new,1
The-Grim-Reefer,"The dinosaurs never really went extinct. Almost all of today's modern birds are a descendant of the dinosaur *Archaeopteryx*. They just look much different now. Certain qualities allow organisms to live when the environment changes. If a species or even a select few members of a species can deal with a changing environment and reproduce they will live on. Most dinosaurs could not do this in the environment presented to them so they died off. Some lived on and genetic mutations throughout members of these species have changed their phenotypic qualities. In order for a species  to ""re-evolve"" after it has died off would require an organism to go through the same genetic mutations that originally brought that particular species to life. 
The chances of this actually happening are slim to none. If the environment is the reason for extinction, the qualities possessed by that species are not advantageous to qualities held by other organisms, therefore the other species will live on and continue to evolve with different phenotypic qualities than the ones held by the dinosaurs",null,0,cdpnejy,1royyw,askscience,new,3
GenL,"Dinosaurs evolved from an ancient reptile that no longer exists. Current reptiles might superficially resemble those dino ancestors, but they are very different. The starting point that you're imagining revisiting doesn't exist any more.

Could a new family of giants descended from reptiles arise again? Yes. But they wouldn't be dinosaurs. They would end up being different in many ways.",null,0,cdpqq81,1royyw,askscience,new,3
The-Grim-Reefer,No it actually helps to moisten the turkey meat. The salt from the solution gets absorbed by the muscle tissue which loosens up the fiber proteins. This creates more space within the meat causing it to swell with the juices of the solution that are pulled into the swelling meat.,null,0,cdpl4i2,1rozc0,askscience,new,3
yankee333,"Cells have many ion channels -&gt; the sodium/chloride diffuse as well, into the cells, bringing water with them osmotically.

Theoretically, if you blocked all sodium or chloride channels first, with drugs, you could dehydrate your turkey very well.",null,1,cdpr4rl,1rozc0,askscience,new,1
patchgrabber,"This happens more often than you'd think. Consider naked mole rats' eyes: they live underground in complete darkness nearly their entire lives, but they have non-functional (or extremely limited functional) eyes rather than no eyes.",null,0,cdpk4o2,1rozdg,askscience,new,3
yankee333,"No such thing as ""devolution"". Things are selected for and then not selected for. Removal of physical features over time indicates that their presence is not being selected for. This isn't devolution, but rather another form of evolution.",null,2,cdpr3h4,1rozdg,askscience,new,3
haysoos2,"There are many examples of traits that were advantageous for an organism being selected against later in the phylogeny, and the population losing that trait.

One of the more visible ones is flightless birds.  There are numerous lineages of birds - penguins, ratites, and many island species - who have secondarily lost the ability to fly that their ancestors had.  

In the case of flight, the metabolic costs to maintain that ability can be quickly outweighed if the advantages this gives (especially predator avoidance) are no longer required.

In our own lineage, the massive teeth and powerful jaw muscles that some of our ancestors had were no longer an advantage, and were selected against.  ",null,0,cdpkeed,1rozdg,askscience,new,1
anthropophobe,"My answer is going to be off topic.  Here are three examples of regression of technology:

1. Supersonic passenger aircraft (the Concorde) no longer exist.

2. There is still not a plane in regular service that is as fast as the SR-71 was.

3. The space shuttle was a financial sink hole and is now gone.",null,1,cdpv54v,1rozgj,askscience,new,1
wazoheat,"I can think of a few problems meteorologically:

* [The Mediterranean Sea is an important source of atmospheric moisture for southern Europe](http://www.atmos-chem-phys.net/10/5089/2010/acp-10-5089-2010.html). With it removed, not only would the resulting land be desert, but much of Europe's existing arable land would become desert as well.
* Not only would the resulting area be dry, it would be incredibly hot. [Much of the Mediterranean is over 1000 m (3300 ft) deep](http://www.geologie.ens.fr/spiplabocnrs/IMG/gif/CarteEastmed.gif), some of it more than 5000 m deep. With the sea removed, there would now be an incredibly large valley as much as 5000 m (16400 ft) deep. As wind blew down into this newly formed, incredibly large valley, it would heat up due to compression according to the [dry adiabatic lapse rate](https://en.wikipedia.org/wiki/Lapse_rate#Dry_adiabatic_lapse_rate), which is about 10°C (18°F) per 1000m. That means that the air temperature in the deepest parts of the valley could potentially reach 70-80°C (160-180°F) on hot days!",null,0,cdpgko7,1rozo5,askscience,new,8
Gargatua13013,"Sure, it is even believed it has happened in the past, a mere 6 or 7 MY ago (see: http://en.wikipedia.org/wiki/Messinian_salinity_crisis).

What you have to remember is that the mediterranean basin is, overall, evaporitive. This means that the annual sum of the water from all the streams and rivers draining into it is less than the amount lost annually to evaporation. The difference is made up by seawater coming in through the Gibraltar straights.

So all you'd have to do is dam up the Gibraltar straights and the med basin would evaporate down to the seafloor; exception made of a few residual ""Dead Sea"" type hypersaline lakes. 

But would you want to?

wazoheat brings up some excellent meteorological caveats.

Geologically, you've got to wonder just how fertile that ground would be. All the current salt load of the Med would precipitate, leaving behind thick layers of evaporite minerals (Salt, gypsum and anhydrite). And given the omnipresence of Messinian evaporites, I doubt you'd have much joy hoping for a usefefull aquifer.

So you'd basically get an ultrahot hypersaline desert bordering a rapidly desertifying southern Europe.",null,0,cdpk57n,1rozo5,askscience,new,4
Mightycoz,"TL;DR: Yes. Satellites in LEO and MEO don't see this as a major perturbation since Earth's masstrons and atmospheric drag dominate their orbits, but satellites in GEO must correct their orbits or their inclination gets amplified, requiring north-south station keeping maneuvers. 

Since the Moon is not orbiting Earth's equator, it drags at satellites either north or south, depending on its relative position as the Earth rotates. Over time this can become significant, moving the satellite ""off station"" and out of its assigned ""box"", which would require repointing of ground satellite dishes. There can also be east-west perturbation, which combines with other orbital effects to drag satellites off station. Uncontrolled GEO satellites are considered a Bad Thing, since that requires everyone else's satellites to predict conjunctions and take avoidance steps.",null,0,cdpkmhc,1rp09t,askscience,new,7
Davecasa,"20/20 vision is an angular resolution of one arc minute. A human pupil runs into the diffraction limit at 15-20 arc seconds, and it's impossible to do better without having a larger eye. 15-20 arc seconds is equivalent to 20/5 to 20/7.",null,0,cdprtiy,1rp0s1,askscience,new,3
The-Grim-Reefer,"There really is no limit to how good someone's eyesight can be. The 20/X scale means that the person being tested can see things at 20 feet that an average person can see at X feet. So someone with 20/10 can see something at 20 feet that the average person can see at 10 feet. 20/10 is the best vision on the visual acuity scale that we use. Although Theoretically someone could have 20/1 vision, but I have never heard of an instance where someone exceeded 20/10",null,0,cdpl0k0,1rp0s1,askscience,new,1
jayman419,"You might want to try /r/asksciencefiction. There are theories for using diamagnetism to generate a field that may simulate gravity, but it's probably unsafe for humans, definitely unsafe for any ferromagnetic materials. Or some kind of sci-fi gravity generator/controller.

But mass and/or motion are the only two that are really solid and you've already ruled them out.

Why not have the mission parameters begin with a constant acceleration, then some sort of accident or collision which knocks the ship into the eccentric orbit and also takes out the gravity. Then they can use like magnetic boots or something after they flail around for a bit.

edit: That way you can work in the set pieces you have in mind specifically taking place in gravity, explain the orbit and keep the ship in the solar system, still have the characters able to move around the environment throughout the game, and not have to go too far from the hard sci-fi approach you want to use.",null,0,cdpjzp2,1rp0ya,askscience,new,1
in4real,"Acceleration and gravity are equivalent from the subject's point of view.

Linear acceleration and centripetal force are both forms of acceleration (the later is a change in the velocity vector which by definition is acceleration).

Any change in velocity vector is going to simulate acceleration.

Acceleration aside I suppose you could recreate gravity on the ship with a captured black hole or maybe some neutron star material.  You'd have to make up some way to contain it.  Magnetic field or some such thing.",null,1,cdpjztw,1rp0ya,askscience,new,1
OnceReturned,"Monsanto started a company called Beelogics to developed genetic technologies to protect bees, mostly with an eye towards Colony Collapse Disorder and Israeli Acute Paralysis Virus, which pesticides likely play a roll in but which seem to involve a complex interplay of a variety of factors.  Their website is here:

http://www.beeologics.com/

They're working primarily on gene expression manipulation through RNA interference.  So, different than one what you're talking about, but the closest thing to it going on today as far as I know.

We simply don't yet understand the problem well enough to stick a gene into the bees and wait for it to proliferate, and it may very well turn out that a less heavy handed approach could deliver satisfactory results.

Edit: Monsanto actually bought them in 2011, they didn't start the company.",null,0,cdpssa2,1rp1tp,askscience,new,1
Philsutty,"Assuming you were lifting with both arms equidistant from the centre they'd be lifting half each, if you were to place your hands at different distances it would be different.  Imagine lifting one arm in the centre and one at an end, the bar would be balanced around the centre, so that arm would be taking all the weight and you could theoretically take the hand at the end off. If you had one arm  at position x from the centre and the other at 2x from the centre in the opposite direction the first arm would be lifting twice the weight of the second, totalling the overall weight",null,0,cdpl9nt,1rp2hf,askscience,new,2
chrisbaird,There is a very easy way to test this. Try lifting the bar with two hands and then try lifting the exact same bar with one hand in the center. Is it any harder? Does it feel any heavier? ,null,0,cdpobjr,1rp2hf,askscience,new,1
OnceReturned,"You could get into some deep water here thinking about what randomness really is, and randomness vs. unpredictability.  

I think in conventional usage though, and in the sense that you seem to be using it here, things can be random without all possible outcomes having equal probabilities.  If it was 1's on five sides and a 6 on the last, the outcome would be random with rolling a six having a probability of 1/6 and rolling a 1 having a probability of 5/6.  A better example might be picking a fruit out of a barrel with 99 pears and single apple.  Your selection could be random, even though one outcome is much more likely than another.

On the other hand, on any given roll of the die, if you knew enough about the conditions of the roll and properties of the die itself, the surface you rolled it on, and the air you tossed it through, there are quite a few people who would argue that you could determine the outcome with certainty beforehand, in which case it wouldn't be random. ",null,0,cdpvhnv,1rp2py,askscience,new,2
paolog,"The outcome of rolling a die many times would still be random, but the probabilities of getting each of the numbers from 1 to 6 may be affected by the indentations on the sides. In particular, the die's centre of gravity will be slightly closer to the 1 face than the 6 face, which would make throwing a 6 a tiny bit more likely than throwing a 1.",null,0,cdpkfo6,1rp2py,askscience,new,1
xxx_yyy,"* Geology: One can look at old stuff (e.g., rocks) on Earth to see if chemical processes were the same then.  I'm not very familiar with geology.

* Astrophysics: One can look at distant galaxies (""looking back in time"") to see if atomic properties (e.g., transition energies) are the same as now.  Also, the abundances of the light elements (hydrogen, helium, lithium, beryllium) created in the first few minutes after the big bang depend on nuclear processes that we can measure.  Agreement with modern measurements is an indication that the laws haven't changed.",null,0,cdpkb6l,1rp33e,askscience,new,4
adamsolomon,"&gt;And since science doesn't allow for assumptions

That's definitely not true! We make assumptions all the time - the point is then to figure out how to test them.

As xxx_yyy said, there are a few ways of testing the variation of constants, and none of these tests (with a few controversial exceptions) have turned up statistically significant evidence that any constants of nature have varied. These tests span a fairly wide range of places and times, all the way back to a mere few seconds after the Big Bang. But of course, that's only within present experimental precision: there's always the possibility that there is some smaller variation of some presumed constant.",null,0,cdpkjp5,1rp33e,askscience,new,2
chrisbaird,"You seem to be misunderstanding how science works. If we discover that something was different a million years ago, we don't say that the rules of physics has changed. We say that our model was wrong. We intentionally create a new model to account for the differing situation a million years ago such that the model is the same for all periods of time. Science is always evolving to account for all observations, and not the other way around.

For instance, let's say that we discovered that a million years ago the gravity created by a unit mass was twice as strong as the gravity created by a unit mass today. We would not say that, ""The rules of physics have changed themselves! Our assumption has been wrong all along!"". We would simply say, ""We misunderstood the rules of physics in thinking the relationship between gravity and mass is unchanging. We now know that this relationship is not fixed, so here is a more accurate, *unchanging* rule that describes how gravity has evolved through the history of the universe."" We would not have two separate rules of physics; one for a million years ago and one for today. We would instead create a better, single, theory that accounts for all ages.",null,0,cdpo6f9,1rp33e,askscience,new,2
aggasalk,"this is a complicated question. closest to your meaning, I think, is the issue of filling-in of surface color - the [Cornsweet illusion] (http://en.wikipedia.org/wiki/Cornsweet_illusion) is usually taken as one piece of evidence that the visual system uses boundary properties to infer properties of surfaces. the [watercolor illusion] (http://en.wikipedia.org/wiki/Watercolor_illusion) is another example at the same level. these illusions can be taken to demonstrate the fact that surface features are at least in part inferred from non-surface properties. there's a whole system of 'lightness and brightness' illusions that show this in different ways. [color constancy] (http://en.wikipedia.org/wiki/Color_constancy) is also at a similar level, showing that what you see as a particular color depends on other structure in the same scene. 

but more broadly speaking, *objects* are also perceptual inferences. 'dog', 'pillow', 'chair' - these are mental constructs. there is a physical reality, but it doesn't contain identities or meanings or clear-cut distinctions between clumps of matter. it's the brain that decides *this* clump and *this* clump are different (pillow vs dog), while *this* clump and *that* clump are similar (pillow vs another pillow).

so under this theory, you can see any clump of stuff as any object that is *known* to you, and while you'll usually get it right, you will often make mental mistakes. so you might see a clump and interpret it as 'dog', and then shift your interpretation to 'pillow' - what has happened is that the identity of objects is something that is filled in by context and features, just as is the color of surfaces, etc; and if the context strongly suggests 'dog place', you might be inclined to mistake a non-dog object for a dog, just as the border in the Cornsweet illusion inclines you to mistake the two sides of the display as being different colors. the difference is maybe that the dog/pillow has internal structure, as well as contextual relations, that help you to make further disambiguations upon a little more analysis.",null,1,cdpqiqf,1rp418,askscience,new,2
jowofoto,"Each person has a 'blind spot' due to the optic nerve. In terms of percentage, the area of the human retina is 1,094 mm^2 (average). A typical sized optic nerve is about 1.86mm x 1.75mm, so 3.255 mm^2 . To give a percentage of the optic nerve 'blind spot' would be something like 0.298%. However, the distance between each rod and cone as well as how large the retina is is also something to consider and your brain also 'fills in' those tiny gaps as well. ",null,0,cdpmcv5,1rp418,askscience,new,1
paashpointo,"They will not be able to for the same reason we dont know if other universes are out there.

If there is a thing that is impossible to observe(interact with in any way ever) no matter what then it effectively doesnt exist in your universe.


For such a universe that has expanded such that no light will ever reach the next galaxy, you could extrapolate backwards only if there was interaction. So it would be theoretically possible with a perfect grasp of gravity to prove that a long time ago other things that dont currently appear anywhere in the universe to have been there interacting. But to calculate with this sort of precision would require more memory than exists in the known universe. 

Fyi I am just a layman that likes stuff so my details might not be perfect or technically correct but that is how I understand it.",null,0,cdpkc61,1rp4aw,askscience,new,2
Entropius,"The Big Bang wasn't like a supernova.  A supernova is just an explosion *within*  space.  That would imply it was just throwing matter and energy through an already existing void.

The Big Bang was an explosion that created space &amp;amp; time (well maybe, it's up for interpretation).

The theory was derived from a catholic priest who also happened to be a physicist, [Georges Lemaître](http://en.wikipedia.org/wiki/Georges_Lema%C3%AEtre). From Hubble's observations of galaxies expanding away from one another, and Einstein's General Theory of Relativity, Lemaître figured if that's all true then all matter and energy had to have originally been at a single position some time in the past.

",null,0,cdprsmc,1rp4cp,askscience,new,1
calibri00010,"We don't know what happened at the EXACT moment of the big bang, but we are confident we know what happened .000000000000000000000000000000000000000000001 seconds after the ""bang"". Scientists currently believe that the Big Bang was the moment gravity broke away from the other unified forces (nuclear strong, nuclear weak, electromagnetic) causing the physics of our observable universe to ""lock"" in to place. As soon as the basic forces of nature were locked in to place; an immediate expansion of space caused intense heat and light (photons). The particles were so excited(hot) that they combined in to matter. Eventually the universe began to cool, at this point the universe was made of 75% hydrogen and 25% helium. For one billion years nothing existed but gas, but at some point gas clouds became so heavy they collapsed and ignited the first stars. Stars clustered to become galaxies etc. ",null,0,cdpxw9v,1rp4cp,askscience,new,1
haysoos2,"Some of the latest exoplanet discoveries have included huge gas giants orbiting much closer to their suns than we previously thought possible.

So at the moment, it appears our models of how planets form and arrange in a solar system are incorrect and need to be modified.

In short:  We have no idea.",null,0,cdpkseh,1rp4n7,askscience,new,1
adamsolomon,"Good question! There are two parts to this answer.

First, gravity is not always attractive. And it turns out that on the largest cosmic scales, it's repulsive: the expansion of the Universe isn't slowing down, as you'd expect, but is speeding up. This means that either the Universe is filled with some exotic stuff (""dark energy"") which has repulsive gravity, or the laws of gravity differ from what we thought they were, in such a way that at large distances/late times, gravity between normal objects becomes repulsive. If the Universe were to one day stop expanding and collapse on itself, it would first have to be expanding but with an expansion that's slowing down. The fact that the expansion isn't slowing down means that collapse is likely not in our future.

But in fact, before we knew about this bizarre speeding-up expansion, we still weren't sure there would be a Big Crunch. The reason is that even if gravity is always attractive, it doesn't necessarily make everything collapse. Think about it in terms of a rocket ship: if it's launched with an initial speed greater than some value (the escape velocity, neglecting air resistance), then the Earth's gravitational pull will constantly slow the rocket down, but never slow it down so much that it stops going up and falls back down.

Similarly, if galaxies were moving away from each other at sufficiently high speeds, then even ignoring this weird repulsive gravity stuff, they still wouldn't ever start moving towards each other. The expansion would slow down but never quite stop. This was considered a good candidate for our own Universe before we discovered the accelerating expansion, which put a Big Crunch way out of reach.",null,1,cdpkdng,1rp4yk,askscience,new,5
Pombologist,"These genes aren't binary. It's not a matter of having this gene will make you an alcoholic, not having this gene means you can't ever become alcoholic. Rather, it means that these genes give you a certain predisposition to those conditions. You are *more likely* to develop them if you have that genetic marker.

In terms of implications, it means that the patient can become aware of the potential for that problem before they get it, and therefore be aware of early warning signs or have better detection for it. For example, if you have a genetic marker associated with colon cancer, your doctor may recommend that you get a colonoscopy more often. Genomic is quickly becoming a more important part of medicine.",null,0,cdpnqxw,1rp54b,askscience,new,4
snusmumrikan,"One of the biggest misconceptions with the human genome project and genetics in general is that people think that finding out what gene(s) are responsible for something means we can fix it. That's not true, it might help develop treatments in the future, but on it's own it gives no real advantage. 

It's like having a boat with a leak - if it is a small leak you might be able to treat the symptoms of the leak (water in the boat) with a bucket twice a day and leave it at that. If the leak is more serious you will want to know why, however finding out that 'there's a flaming massive hole in the bottom of the boat cap'n!' makes no difference if your only tool is still one bucket. 

We have limited gene-therapy treatments as they need a way to counteract the latent problem in all of the cells affected in your body. For cystic fibrosis, which is most commonly caused by a single gene mutation in an ion channel in the lungs, they have tried to insert the correct gene with viral transmission vectors with [limited success](http://www.ncbi.nlm.nih.gov/pubmed/16296753)

There is also promising work with [non-coding DNA](http://en.wikipedia.org/wiki/Morpholino) injections for [Duchenne Muscular Dystrophy treatment](http://www.ncbi.nlm.nih.gov/pubmed/19288467) - another single gene disease. 

Other than those kinds of approaches the most promising future seems to be systemic treatments like bone marrow transplants and stem-cell therapies, however both of those are relatively infant fields. 

P.S. sorry for the awful boat analogy. ",null,0,cdpwni2,1rp54b,askscience,new,2
expertunderachiever,"Signals in a wire can only travel at most that fast (in reality it's much slower).

So your 4GHz CPU means that a signal can travel 74mm per clock period ( http://www.wolframalpha.com/input/?i=%28light+second+%2F+4*10^9%29+in+mm ) but nothing in your CPU is a straight line and not all gates react evenly to signaling [there is slack/skew/etc].  So to meet timing the distance between latched pieces of logic must be shorter than the distance light can travel in that period.

Ignoring other electrical properties at the very least to get to 8GHz you'd have to have half as much length per latch otherwise your circuit won't meet timing.",null,1,cdpmcyy,1rp6wm,askscience,new,2
wesramm,"Yes, paint ""drying"" is the release of the volatile solvent from the paint.  An analogous term is ""off-gassing"".  On earth, if you want to ""off-gas"" a solvent, you put the part or article containing the solvent into a vacuum chamber.  The lower pressure causes any volatile material to (whether it is a petrochemical or water) boil off at a much lower temperature, leaving only the solids.  Its the same with paint.  So, in fact, the process should be much faster in space, but I'd expect the process to be nearly explosive, resulting in spotty coverage, as the boiling would be extremely violent...  ;)
",null,5,cdpjlhm,1rp7i5,askscience,new,29
ashary,No. Paints contain unsaturated oils which react with oxygen to form a cross linked polymer. As space doesn't have any oxygen the polymer will not form and hence the paint will not dry,null,1,cdpik9n,1rp7i5,askscience,new,6
Dominus_,"Because of the near total vacuum of space, [the boiling point of any liquid is much lower](http://www.youtube.com/watch?v=rM04U5BO3Ug), causing water or other liquids in paint to evaporate much more quickly. 

So yes, paint would probably dry almost instantly, in a violent boiling fashion.",null,0,cdpizgy,1rp7i5,askscience,new,5
thegreatgazoo,"It has been a while but an LED has a voltage that is applied to it (around .7 volts) that raises electrons from one energy level to another. When the electron 'falls back down' it releases a photon corresponding to the wavelength/distance that it 'fell down'. The wavelength depends on the material used which is why some are red, green, yellow, or blue. 



",null,0,cdpi9ju,1rp7zv,askscience,new,4
Ejb90,"thegreatgazoo gave a good answer, but to expand:

Light can be considered as waves of electromagnetic radiation OR a particle, dependent on what it's doing. This duality is counterintuitive, but it's a fundamental consequence of quantum mechanics.
The issue is that light can behave with different properties we consider as ""particulate"" or ""wave-like"" depending on what it's doing. Hence we just say it's a duality between the two, and pick what we need for the situation.

In a light bulb the electrons pass through a filament. However the filament has some resistance to the electrons passing, as it's full of atoms the electrons need to push their way through. The actual mechanics of how this works is complex (it requires a bit of band theory and quantum condensed matter), but you can think of it as the electrons pushing each other into higher energy levels whilst trying to get through. These electrons are a bit unstable in the higher energies, so they emit some energy to drop down into a more stable shell. This energy it releases is in a packet a ""quanta"" of energy, which we call a photon. Hence it produces light. The differences between the energy levels is different for different materials, and it determines the colour of the light emitted.

The beta-decay process is a bit different to what you describe. the reaction path is:
(up, down, down) -&gt; (up, up, down) + electron + electron antineutrino + photon
neutron -&gt; proton + electron + electron antineutrino + photon

The mass lost from the neutron to the proton is (sort of) the mass of the antineutrino and electron.
There is also beta+ decay, which you can look up.
You can think about the products of these reactions by making sure several quantities are unchanged, such as the energy, angular momentum, charge, lepton number etc.
Hence here the charge is conserved, as is the lepton number (the number of leptons in a system, funnily enough).

Your point about the positive and negative charges producing a stream of photons is slightly misguided, but I think you're talking about virtual particles. It's (in a way) true that there is some photons that produce the electromagnetic force, called virtual photons, but these aren't physical, tangible photons we can observe. It's more of a mechanism for dealing with subatomic particles.
",null,0,cdpiham,1rp7zv,askscience,new,4
Spirko,"In an incandescent light bulb, the electrons that form the current can basically flow where they want, but they keep running into things.  This stops their progress along the wire and heats up the wire.  Some energy of the charges that form the current is given to the atoms in the wire, and they vibrate around randomly.  Any object emits [Blackbody Radiation](http://en.wikipedia.org/wiki/Black-body_radiation), which is what we see from ""regular"" light bulbs.

In an LED, the electrons in the semiconductor (the business part of the LED) are confined to particular orbitals.  Because there are so many atoms and the orbitals overlap, they are called bands.  (Imagine drawing the same line across a page many times.  Eventually, you just have a big stripe of ink.)  At one point, the material changes, which changes the energies of the bands, and the electrons must find new bands to occupy to cross that point.  They must lose some energy.  The energy released is given off in the form of photons that have that amount of energy.  Because this is a microscopic process with conservation of momentum and energy, the energy basically all has to go to one photon, and the color comes from the amount of energy.  The simplest case to think about is a single atom's [Spontaneous Emission](http://en.wikipedia.org/wiki/Spontaneous_emission), but the LED's case isn't that different.

How does it happen?  It's probably best to start thinking about electric and magnetic fields, go through the ideas of [Displacement Current](http://en.wikipedia.org/wiki/Displacement_current) and [Electromagnetic Induction](http://en.wikipedia.org/wiki/Electromagnetic_induction), and understand [Maxwell's Equations](http://en.wikipedia.org/wiki/Maxwell%27s_equations).  Any time-varying electric or magnetic field will stimulate the other, and they will oscillate together into the distance as an EM wave.  Classically, any accelerating charge should emit EM radiation (i.e. light) and that includes the ones in the LED.  As an electron changes speed, its electric field changes, etc.  The restrictions of the energies of the electrons and the photons (i.e. electron bands and photon energies) come from quantum mechanics.",null,0,cdpkjif,1rp7zv,askscience,new,2
PRBLM2,"Technically, a shadow is an area where direct light from a light source cannot reach due to obstruction by an object.  So, anything that blocks visible light and creates a shadow would also have to be visible.

However, in the spirit of your question, it is possible to use a lens (while not completely invisible) to create a ""shadow"".  If you've ever used an incandescent flashlight, you've experienced this type of ""shadowing"".  [Here's a picture to illustrate my point.](http://www.spcmarketing.co.uk/acatalog/dead_spot_beams.jpg) I don't know if there's a name for this type of dark spot, but it isn't technically a shadow since the light is being refracted and not obstructed.  ",null,2,cdpi7gc,1rp86x,askscience,new,14
Fyrefli,"Carbon dioxide can cast a shadow, but not because it blocks the light.  Rather, due to its higher density (relative to nitrogen), the carbon dioxide refracts light, causing darker patches (i.e. a  shadow).

Edit: at about the 0:40 mark of [this video](http://www.bbc.co.uk/learningzone/clips/an-introduction-to-carbon-dioxide/1614.html), you can see the shadow.",null,0,cdpittk,1rp86x,askscience,new,2
jayman419,"The Wardenclyffe Tower's main job was to be a wireless telegraph. It was his entry in the race with Marconi. A secondary purpose was a large scale proof-of-concept for the wireless transmission of energy, something Tesla had already demonstrated in smaller scale experiments.

But the version he had up and running wasn't necessarily unlimited or free. It was a method of power *transmission* ... not power *generation*.

http://www.teslauniverse.com/nikola-tesla-article-teslas-wireless-light

The ""World Wireless System"", had he been able to work on it with continued iterations, might have someday been something functional. It might have taken mile-high antennas and mile-deep geothermal pits to generate a strong enough field to cover a city, but who knows what he had in mind for the final version?

Wireless energy is something they're working on today. They use microwaves for longer-distance transmission though, because there is less loss due to atmospheric variability and water vapor. And his basic theories for the wireless transmission of energy were sound, for the most part. 

While not a great source, [the wiki](http://en.wikipedia.org/wiki/Wireless_power#Electric_energy_transfer) goes over some of the different ways it's been proven to work, and can go over the basic terminology to give you some better search terms.",null,0,cdpkfrx,1rp9gl,askscience,new,2
adamsolomon,"As SilentCast said, it's mass - not physical size - that determines gravity. But it's also mass which, to an extent, determines whether what you have is a star or a planet. If you have something as massive as a star, its gravity is going to be strong enough to force hydrogen atoms in its core to fuse into helium, producing light. That's pretty much the definition of a star. So you wouldn't have a star-planet system, but a binary star system.",null,0,cdpll5a,1rpabd,askscience,new,7
iorgfeflkd,"It is possible for a planet to have a larger radius than its star (if the star is a white dwarf or neutron star), but not more massive. Anything that's more massive than a star will become a star.",null,0,cdpmj08,1rpabd,askscience,new,6
SilentCastHD,"Well, the thing that makes gravity work is not size, but mass.

Just to be clear about that.

radius:

1 | Jupiter | 69173 km

2 | Saturn | 57316 km

3 | Uranus | 25266 km

4 | Neptune | 24553 km

5 | Earth | 6367.5 km

6 | Venus | 6051.9 km

7 | Mars | 3386 km

8 | Mercury | 2439.7 km

Mass:

1 | Jupiter | 1.8988×10^27 kg

2 | Saturn | 5.685×10^26 kg

3 | Neptune | 1.0278×10^26 kg

4 | Uranus | 8.6625×10^25 kg

5 | Earth | 5.9721986×10^24 kg

6 | Venus | 4.869×10^24 kg

7 | Mars | 6.4191×10^23 kg

8 | Mercury | 3.3022×10^23 kg

So for exampel as you see, uranus is bigger that neptune (chuckle) but still has less mass, since the density is way off:

1 | Earth | 5.515 g/cm^3

2 | Mercury | 5.43 g/cm^3

3 | Venus | 5.24 g/cm^3

4 | Mars | 3.94 g/cm^3

5 | Neptune | 1.76 g/cm^3

6 | Jupiter | 1.33 g/cm^3

7 | Uranus | 1.3 g/cm^3

8 | Saturn | 0.7 g/cm^3

So uranus would gravitate around neptune (not really, since even planets make our sun wobble around, since they really gravitate around each other but neptune would move a little less around uranus than uranus around neptune)

So with that out of the way: 

Yes, planets can be bigger (not heavier) than stars:

There are for exsample a Neutron star has a radius of only 12 km but have a density of 3.7×10^17 to 5.9×10^17 kg/m3

So wikipedia states: This density is approximately equivalent to the mass of a Boeing 747 compressed to the size of a small grain of sand.

So as you see, every planet in our solar system would be bigger than this Neutron star, but they could still gravitate around it, since this star is around 2.6×10^14 to 4.1×10^14 times the density of the Sun at 1/60000 of the radius.",null,0,cdpl75m,1rpabd,askscience,new,3
iorgfeflkd,"It's possible to use an electromagnetic field to vaporize a solid. Powerful lasers can do this (I personally have used a ~1 Watt 1064 nm laser to melt ice). However, the mechanism of melting or vaporization isn't as you described.",null,0,cdpja5b,1rpaf7,askscience,new,4
__Pers,"This is routinely done with ultra-intense (intensity greater than 10^18 W/cm^2 for ~1-micron wavelength light) lasers through a process called [Coulomb explosion](http://en.wikipedia.org/wiki/Coulomb_explosion). Typically, the samples exploded in this way are rather small (~10s to 100s of nm across) and the lasers are rather short in duration (picosecond to tens of femtoseconds).  

There are also other, related means of particle acceleration from lasers that operate on similar principles, yet in different parameter regimes; target normal sheath acceleration is one such example. 

Source: I work in this field. ",null,0,cdpkvdp,1rpaf7,askscience,new,2
NAG3LT,"Well, changing electric and magnetic fields produce electromagnetic waves. Light is also an electromagnetic wave and can be used to vaporise solid. Also, you almost never strip out all electrons from the more charged nuclei, but stripping few electrons out of outer shell is usually enough to break bonds between atoms. When you shine light at a material and it is absorbed, most of its energy first goes into electrons, and then redistributed to the nuclei. The kinetic energy of nuclei manifests itself as a heat and acts the same. 


To put a lot of energy in a form of light into a solid, you can use laser. If you're using a laser that shines continuously, you can melt and then vaporise some part of a molten metal. Depending on the reflectance, thickness and focusing few Watts of laser power may be enough for cutting. However, when you use a non-pulsed laser, even 50 kW lasers will still leave some parts molten, as the absorbed heat has time to diffuse around the spot you shine at. 


There is a way to vaporise metal with minimal melting, however. Going straight from solid to a vapour is called ablation, and it can be achieved with a very short laser pulses. It is currently possible to generate pulses on the order of picoseconds (10^(-12) s) and less. The energy you put into the pulse is absorbed so quickly that the thermal diffusion into surrounding area cannot move it away fast enough. One of the values I've found was done with a 100 fs (100*10^(-15) s) [laser pulse](http://link.springer.com/article/10.1007%2Fs00339-010-5766-1). The flux required to ablate copper was 1.7 J/cm^2, which means a peak irradiance of 17 TW/cm^2. The peak electric field strength is 11 GV/m (Gigavolts per meter). That's a lot, as making an electric spark go through the air requires ~1000 times weaker field. 


So, if you want to vaporise material from a solid state, you're looking at very strong electric fields. However, that won't strip all electrons, only the least bound ones on the outer shell. Removing all the electrons gets harder the further you go along the periodic table. Unfortunately I can't give you any numbers for those.",null,0,cdpjek4,1rpaf7,askscience,new,1
rupert1920,"[Mass spectrometry](http://en.wikipedia.org/wiki/Mass_spectrometry) is a technique where gas-phase ions are separated by their mass-to-charge ratio, so both things you mentioned are employed: moving the sample into the gas phase, and making it into an ion.

One way to do this is basically what you're describing - using a strong electric field (at the kilovolt range) to both ionize the sample and to expel it into the gas phase. This ionization technique is called [field desorption](http://en.wikipedia.org/wiki/Field_desorption).

As mentioned by others, there are other ways involving actual _electromagnetic_ fields (i.e., light). You can use a laser to ablate - vaporizing a solid - your sample embedded in a matrix. The laser also acts to ionize the matrix, which then ionizes your sample. See [matrix-assisted laser desorption/ionization (MALDI)](http://en.wikipedia.org/wiki/Maldi).

As a final note, neither of these methods will strip _all_ of the electrons from a solid. It's unstable to have a huge excess of charge - and if that occurs, the molecule will fragment instead.",null,0,cdpjvb3,1rpaf7,askscience,new,1
jakkes12,Imagine yourself the zipper has a few points it will produce a sound on whenever you cross it. Each time you cross one of these marks it'll produce one sound wave.Thus the frequency will be determined by how many marks you cross each second. Higher speed means more marks crossed per second equals more sound waves per second (higher frequency) which means higher pitch.,null,4,cdpiz4t,1rpbvb,askscience,new,28
expertunderachiever,"To put it in simple algebraic terms, suppose computing 1/x was a hard problem but somehow I had an algorithm to do it efficiently.  You could know me as the ""guy who can compute modular inverses"" if I take a string and give you the inverse of it ... e.g.

    r = hash(msg)

then I hand you 
 
    s = 1/r

Which you can verify by comparing s * hash(msg) == 1.  You could encrypt a message m to me by picking a random r and sending me c = rm and r.  Since 1/r is hard to compute [in this scenario...] I can easily compute 1/r multiply against c and get m back.

In this case you're dealing with problems like RSA where you have a modulus n = pq where the private key owner knows the factors.  Because they know the factors they can compute two exponents

    e = something like 17 or 65537

and

    d = 1/e modulo the order of the multiplicative group mod n

The order of the group is given by lcm(p-1,q-1).

So now we have 

    m^ed mod n == m^1 == m

So if I send you

    c = m^e mod n

you can compute

    m = c^d mod n

since

    c^d == ( m^e )^d == m^ed == m^1 == m

Since nobody knows d but the private key owner they can also easily compute
 
    s = hash(msg)^d mod n

And we can verify with

    hash(msg) == s^e mod n

Computing the order from n = pq is a hard problem believed to be as hard as factoring, so is computing the e'th root.

[note: messages are padded in RSA to avoid other forms of non-random attacks on the operation...]
",null,0,cdpkgma,1rpcqt,askscience,new,3
paashpointo,"What it means is if I have a plain text of something I want the world to know and I put whatappears to be gibberish at the end that has been generated using my private key, then anyone with my public key can prove that my private key was used and thus it has to be me.

So for example if you and I were doing business and I just said in plain language hey this is paashpointo send me my money at account number and so on, you would have no way of verifying it was me and not a hacker. 

But instead I could send the same message and at the end attach paashpointorulesandreallyistheguyyouaretalkingtotrustme after running it through my private key and it would look like gibberish but then running it through the public key would restore it. So it is only good at verifying to the public that they are talking to who they say they are. Hope this helps.",null,0,cdpkgp8,1rpcqt,askscience,new,2
blackhattrick,"In asymetric encription, usually RSA, you have a pair of keys: the private and the public key. You can encrypt with the public key and decrypt with the private key and viceversa. When you encrypt something with a private key, which no one have, certifies at the moment of decryption with the public key that this message was encrypted by you. If someone sends a message with a different key saying it is you and someone else tries to decrypt it with his public key, he will get garbage data. So the person trying to decrypt this message would know this isn't you.",null,0,cdprplv,1rpcqt,askscience,new,2
linozeros,"Somebody asked a similar question and already got it answered:
[Did viruses that are lethal to humans evolve to kill us, or is that a side effect of their existence?](http://www.reddit.com/r/askscience/comments/17h29d/did_viruses_that_are_lethal_to_humans_evolve_to/)

Hope this is what you were looking for",null,0,cdpl0qo,1rpcs5,askscience,new,7
WhenTheRvlutionComes,"A microorganisms success isn't determined by whether or not it kills someone. We just tend to think that way since harmful microorganisms are typically the only ones of interest to us - such that, in the common lexicon, ""virus"" essentially only means something harmful. But there are plenty of benign or beneficial microorganisms that are very successful as species. Killing the host is just a side effect, undesirable in its own right, sure, but tolerable in evolutionary terms if it's a necessary part of some overall strategy that leads to new hosts and new chances for reproduction.",null,0,cdpqr73,1rpcs5,askscience,new,2
yankee333,"Many of these pathogens have large environmental reservoirs. While not a virus, vibrio cholerae is a good example. There is little need for the vibrio to maintain a chronic infection in a host because it already has an environmental reservoir to maintain itself. This is very much an indirect process, as all evolution is.",null,1,cdpqxy6,1rpcs5,askscience,new,1
casonthemason,"It depends how you define 'success.'  If the goals of a virus are to infect, replicate, and spread, then viruses which quickly kill their hosts would actually be regarded as unsuccessful.  Generally speaking, the viruses that are most lethal to humans tend to come from non-human reservoirs (for example: ebola, influenza).",null,0,cdpsfcg,1rpcs5,askscience,new,1
ucstruct,"Not really, even these two have somewhat indirect effects on mineral absorption. Vitamin C acts as an [anti-oxidant](http://en.wikipedia.org/wiki/Vitamin_C) that helps keep iron in a more soluble form and [vitamin D](http://en.wikipedia.org/wiki/Vitamin_D) stimulates production of an ion channel that can facilitate calcium absorption. 

Vitamins generally function by being cofactors in enzymatic reactions or signaling molecules (except for D, which signals, and [E](http://en.wikipedia.org/wiki/Vitamin_E) which mainly is an anti-oxidant) by performing chemistry difficult to perform with just amino acids. Mineral absorption is handled by transporters and channels located at the cell cell surface that are specific to each mineral. These include phosphate, calcium (upregulated by D), manganese, magnesium, zinc, iron, as well as all of the nutrients we require like amino acids or sugars.  ",null,0,cdpu3xh,1rpdnd,askscience,new,1
MrQuiver,"The light we observe from it has been traveling towards us for 2500 years. We are observing it right now as it was 2500 years ago. We do not have to send it anything to observe it, nor to determine its composition. We are passively receiving the light that has been radiating out from it. This light contains the information that tells us of its composition.

There is no difference between this and how you observe, say, an airplane in the sky above you. You do not have to travel to the plane to observe it. You just have to observe the light that is arriving at your eyes from the plane. The only difference in your example is that the thing you are observing is much further away, such that the light that is arriving at your eyes was emitted a long time ago.",null,0,cdplcfr,1rpf7x,askscience,new,4
DanielSank,"This excellent question is at the root of *many* other questions that show up on /r/askscience.

There are aspects of this question that nobody really understands yet. However, we understand a lot more than popular science and even some physicists will lead you to believe. This will be a somewhat long post but possibly worth your time to read.

Before I say anything else, I want to say that your statement

&gt; How does it ""know"" it was observed? By its interaction with surrounding particles, of course.

is absolutely essential. I hope to explain why below.

**Structure of physical theory**

Any theory of physics has two parts.

* (1) The first part is a representation of the natural system under consideration. For example, if you are making a theory of balls rolling down hills you represent each ball with three numbers (x,y,z) indicating the ball's position in space. You may also have (Vx, Vy, Vz) representing its velocity in all three directions.

* (2) The second part is some way of predicting how this representation changes as time goes by [1]. For the case of rolling balls this could just be [Newton's laws of motion](http://en.wikipedia.org/wiki/Newton%27s_laws_of_motion).

In classical physics these two parts were all we needed to go happily about our scientific lives. However, there's actually a crucially important third part:

* (3) We have to have a prescription for how the representation put forth in part (1) relates to our personal/human/cognitive observations of the natural system. In other words, ""what does a group of numbers (x,y,z) have to do with my observation of the system?""

In the case of rolling balls this is pretty simple. I might use Newton's laws to compute that my ball should be at position

(1 meter, 0.5 meters, 0 meters) at time=1.3 minutes

and we all just *know* that this means that we should see a ball at the specified location and time. This works just fine and indeed in classical mechanics the link between part (1) and (3) is usually pretty straightforward. Note that we never mentioned anything about what we meant by ""see a ball"", because in classical physics we implicitly assume that it doesn't matter. In quantum mechanics it does matter and I'll now try to explain why.

**Quantum mechanics**

The first two parts of quantum mechanics are 

* (1) We represent the natural world with a [wave function](http://en.wikipedia.org/wiki/Wave_function) (or [state vector](http://en.wikipedia.org/wiki/Quantum_state)).

* (2) The wave function evolves according to [Schrodinger's equation](http://en.wikipedia.org/wiki/Schr%C3%B6dinger_equation) (or [Heisenberg's equation](http://en.wikipedia.org/wiki/Heisenberg_picture)).

These two together are just as good as (x,y,z) coordinates and Newton's laws. The place where everyone gets confused is in what the wave function means in terms of what we observe in experiments. To answer this question you hear people say things like ""when you make a measurement the wave function collapses into a definite position, chosen at random, and that's what you see."" The crucial thing to understand is that this statement modifies part (1). It says that Schrodinger's equation determines the evolution until some vaguely defined ""observation"" happens, at which time Schrodinger's equation stops being correct, the state collapses probabilistically, and then Schrodinger's equation goes back into effect. This breaks the theory's self-consistency. Suppose I make a measurement and say that the atom's state collapsed. Now suppose that me and my experiment are in a box, which is actually the experiment of some greater alien. For the theory to be true for the alien, there can't be any state collapse until *he* does the observation. Thus we see that state collapse as stated above can't work.

The issue is that as stated above, state collapse modifies part (1) of the theory, whereas we *should* have been looking for a part (3) type statement. It turns out that if we *leave part (1) alone* and make an extra statement like this:

* (3) When a human's brain collects information about a physical system that information will be randomly distributed according to the square of the wave function, and that system will henceforth appear to have collapsed in accordance with the result of the human's information.

then we have a self-consistent theory. I emphasize that it is both falsifiable and self-consistent and therefore must be admissible as a theory of Nature. That said, its not very satisfying. because of this bizarrely anthropocentric rule (3) which seems much more weird than the sort of obvious equivalent rule (3) in classical physics [2].

**Decoherence**

Amazingly we can *almost* get the quantum rule (3) to be as *obvious* as the classical one. The trick is to really consider what happens when we make ""measurements"". In fact whenever you measure something you are connecting that thing to many extra physical elements. If I measure the position of an atom with a CCD camera, I am coupling the wave function of the atom to an enormous number of electrons etc. in the camera's inner workings. Certainly I do not know the wave functions of all those electrons. **This is the crux of the entire story**: It turns out that if you actually compute what happens to the wave function of the atom in the case that it couples to a bunch of extra stuff whose wave functions you *don't know*, then from your perspective the wave function of the atom becomes a classical probability distribution. I emphasize that this comes about *entirely from Schrodinger's equation and nothing else*. From the perspective in which the wave functions of the electrons are known, the atom's wave function *does not collapse*. Therefore, collapse can be understood as an apparent phenomenon, predicted by pure quantum mechanics, that shows up with only part of the information in a system's wave function is available. This is always the applicable case for humans because our measurement apparatuses contain many uncontrollable degrees of freedom.

When the wave function appears to have collapsed for one observer, it may not appear that way for another one who has more information.

This is really interesting because it gets us really really close to being able to explain rule (3) via rule (1). We know that when only some information of a system is available, that information appears probabilistic. What we haven't explained is why humans see only one of those possible results. Nobody knows the answer to that.

**Answer to the original question**

So, how does a particle ""know"" when it has definite position? It doesn't. It never does. Definite position is just an apparent property of things that comes up when we don't have complete information about the system.

**Conclusion**

We have a self-consistent theory of quantum mechanics using wave functions and Schrodinger's equation. Part of the theory is the statement that when humans have access to the prabability distributions furnished by partial access to a complete system wave function, they perceive one particular value, for some reason that we don't know.

[1] For the physicists out there I'd say more generally equations of motion are constraints on how the representation at one point in space-time is related to the representation at other points in space-time.

[2] However I challenge anyone to state a truly self-consistent theory of physics without such a rule.

EDIT: For down voters: If you could express what you think is unscientific, wrong or irrelevant about this post I'd appreciate it.

EDIT: Formatting",null,1,cdppbdo,1rpid6,askscience,new,13
DanielSank,"Since my other post was so long:

&gt; If none of the particles around it have definite positions either, how does any particle in the universe ""know"" for sure that it was observed?

They don't. Particles don't do anything special when they're observed. We can propose a self-consistent theory of quantum mechanics in which the wave functions don't actually collapse and take on a definite position. The *appearance* of definite position only happens for in cases where only subsets of the system's information is available, which happens to be the case relevant when humans are involved. From the perspective of someone with fuller information about the system, the collapse that *you* see may not happen.

The falsifiability of this theory (described more fully in my other post) is definitely open for discussion. I'd love to have that discussion with anyone else who's interested.",null,0,cdppng9,1rpid6,askscience,new,5
RetraRoyale,"DanielSank's description is very good, so you probably want to read it over carefully. 

Essentially, ""measurement"" and ""observation"" are poor words for describing what happens. An experiment with observed particle is *fundamentally* different from an experiment with an unobserved particle because the observer is made out of a huge number of particles.

If you have a train barreling down the track in one experiment, and in another you build a huge wall in the middle of the track, it's silly to ask how the train ""knows"" to derail it'self when you build a wall there. *You built a huge wall on the track*! That's how it 'knows' to derail itself.",null,0,cdpricr,1rpid6,askscience,new,3
foreveraloneirl,Depends on what quality footage you would find acceptable. Smaller lens/sensor = poorer quality. This is why professional photographers despite many years of technological advancements still need to haul around massive lenses and cameras.,null,0,cdpm3xx,1rpiq9,askscience,new,10
asdfWriter,It all depends on the amount of pixels you want do have. Let's say the minimum resolution you accept is 1000px times 1000px. According to [this paper](http://isl.stanford.edu/groups/elgamal/abbas_publications/C072.pdf) the optimal (and minimal) pixel width in a CMOS-sensor is 6.5um -- that would result in a minimum sensor-width of 1000px x 6.5um/px = 6500um = 6.5mm. Of course future technoligies might be able to shrink the pixel width.,null,1,cdpnkpq,1rpiq9,askscience,new,5
mcnubbin,"This question reminds me of the semi-autobiographical Phillip Dick novel, A Scanner Darkly.  


""What does a scanner see? Into the head? Down into the heart? Does it see into me? Into us? Clearly or darkly? I hope it sees clearly because I can't any longer see into myself. I see only murk. I hope for everyone's sake the scanners do better, because if the scanner sees only darkly the way I do, then I'm cursed and cursed again.""",null,11,cdpokzn,1rpiq9,askscience,new,1
From_Ashes_Rize,"The answer is sort of. You can not create an extremely small flame from a source such as a traditional candle. But there is a way to create micro flames.  This is called microcombustion and they are flames smaller than the width of a millimeter.  They are typically made in small hard ceramic combustors (such as alumina oxide) and they burn at temperatures of up to 2000 C. The combustors have small channels through which the fuel flows and the flame burns. They are typically fueled by hydrocarbons and oxygen. Methane, butane, propane, etc.

The two inlet flows of the fuels meet and combine at some point and then you have your combustible gas. The point at which the two flows meet exhibits turbulent flow and this is the limiting factor at ignition for the combustor due to wall conditions and fluid dynamics not worth explaining here. Turbulent flow tends towards extinction for a flame. Typically a large candle type flame will be seen at the end of the exhaust channel.

As the combustor ramps up to its max temp (steady state) the walls of the exhaust channel heat up and begin to provide the thermal energy necessary for the gases within the channel to ignite. You will hear loud pops as ignition events begin to occur within the channel. As the temperature of the wall increases due to the heat seen from the exhaust flame, the ignition events begin occurring closer to the mixing points of the 2 fuels.

Finally at steady state the walls of the combustor at the mixing point of the fuels is so hot that the gases ignite immediately upon mixing. Due to the turbulent nature of the flow, they go out almost immediately.  They are also almost immediately replaced by fresh uncombusted fuel which them ignites. This cycle repeats itself many times per second and you end up with a sustained micro flame. The ratio of gases determines how many times per second ignition occurs and you can actually say that the flame has a frequency (I.e. You can hear the flame buzz while it is lit).

So in short, yes micro flames are possible, just not from a candle.


Tl;dr
Teeny candles don't work. Burn hydrocarbons in a ceramic combustor for super hot, buzzing micro flames. Science is awesome.",null,106,cdplrw5,1rpive,askscience,new,596
Freemantic,"How sustainable do you want it?

Because technically little miniature ""fires"" are happening all the time at the molecular level. So if you count those, the smallest fire would be that.

The sustainable fire is different, and would change depending on what planet you were on and depending where on Earth you were because pressure changes. The limiting factors being volume, scale, and pressure.

You can scale a flame down indefinitely, but there will be a certain point where it can't sustain a flame. The amount of heat a flame produces/loses isn't a 1:1 ratio, and as it shrinks it loses more heat than it produces. As far as an exact size? Honestly, you're not going to get much smaller than a match. 2-3mm is probably the smallest ""real"" flame without cheating.
",null,1,cdplpdz,1rpive,askscience,new,35
dizekat,"If we are speaking of a steady, hot flame in the air at room temperature and atmospheric pressure, there is a limit because the energy loss at a given temperature would be proportional to the surface area (size squared) while the output power is proportional to the volume. 

Consequently, very small flames lose more heat at the combustion temperature than they produce. Unless of course you use a gas mix that combusts at a very low temperature, in which case the flame could be arbitrarily small (but it will be cold and it is not clear if you should call that a flame). 

Personally, the smallest actual in-air flame that I have seen was smaller than 1 mm, at the end of a hypodermic needle feed oxygen and hydrogen mixture. You can probably go somewhat smaller with hydrocarbons and oxygen (e.g. acetylene and oxygen). Maybe smaller still with chlorine trifluoride or something similar as an oxidizer, but it would be difficult to make a nozzle that can withstand it.

It should be possible to make a smaller flame at higher temperature, or in a less thermally conductive environment than air.",null,0,cdpqfsu,1rpive,askscience,new,12
LakeSolon,"Some good comments so far but to put it simply:

Surface area and volume do not scale the same. When a flame gets small enough it doesn't have enough combustion volume to overcome the heat loss, so it cools too fast and dies out.

That's most of the answer for most of the situations.",null,0,cdpp37h,1rpive,askscience,new,9
borthuria,"Well you have to know that a flame is also a plasma (the fourth state of the matter (solid liquid and gazeous being the other three)) wich consist of a cloud of electron et ion (atoms that lost their electron).

It would be possible to create a plasma as small as you want to do, as long as you can control it.  Frome here on, we would need an experienced plasma physicist.  You can control a plasma with a magnetic field (since it's only electron and Ions, it's ""pretty"" easy thing to do (we know how to do it and we can do it)).

I'm a physic teacher and in a microfabrication lab, we create a plasma in a vaccuum chamber to clean a silicium chip.",null,0,cdpt5kj,1rpive,askscience,new,5
leftylouis,"well it depends on what you would call a flame but in theorie you can let just a few atoms reacts and they will release light and heat doing this (a flame). so id say that it is possible at an atomic level. 

Edit: But a candle ofourse has a lot of limiting factors. it needs the right amount of energy to get the reaction started and to keep it going. if the flame gets to small then it cant supply itself of enough parrafine and it will stop burning. if the fuse is too short then the surface area to burn with enough energy to sustain. 
but then also you got the temprature of the enviroment and the  amount of oxygen and the pressure etc.. 

Oh and about the reguar/birthday candle flame size. the size of the candle doesnt really matter it is the fuel and oxygen supply that matter. 
",null,1,cdpk6ay,1rpive,askscience,new,4
Conotor,"Viscous effects scale with 1/VL (part of Reynolds Number) where V is a typical speed in your scenario, and L is a typical length.  Viscosity tends to mix and split up flows, so at very small scales your flame would become less smooth and steady.",null,0,cdpr74b,1rpive,askscience,new,3
jowofoto,"Just thought I'd put this out there.. Cavitation is something that we've all seen, heard and even felt. It causes a microscopic 'light show' and temperatures yielding thousands of degrees kelvin. It is a big reason why water pumps and propellers fail. 
[Cavitation](http://en.wikipedia.org/wiki/Cavitation)
[Cool video](http://www.youtube.com/watch?v=G5D0F0tDYZQ)",null,0,cdpm10s,1rpive,askscience,new,2
WillNotBeAttending,"I always used to wonder about this as a kid, because in books like The Borrowers series the characters would have a tiny campfire made out of twigs. It never made sense to me because twigs burn really quickly, it didn't seems possible to have a small fire without controlling the flow of oxygen to the flame to slow down combustion.",null,0,cdpotjs,1rpive,askscience,new,3
abt137,"Yes, if you follow the Formula 1 this is a common issue. Drivers in pursuit of another tend to have issues with engine temperature when they are not able to overtake the rival. The more laps they stay behind another car the bigger the overheating risk, and this overheating also applies to other car systems like tires and brake discs (tip: F1 car have air cooled engines). This is referred as being in the ""dirty air"" or ""suck dirty air"", this overheating apart from represent a risk for the engine tends to manifest itself faster in tires and breaks causing unexpected degradation in the tires and malfunctioning of the brakes and also alter the aerodynamic flow so your wings, spoilers etc will not create the desired grip force pushing the car down against the ground.
When the situation above happens you could see drivers getting out of the drag of the car in ahead not trying to overtake but simply to suck fresh air.
This refers to F1 specifically and i can't say about common street cars but I'd say is unlikely to happen since they are liquid cooled; if you are in the mid of a traffic jam the fan starts and cools down the engine temp. Cannot say about other Motorsports as I do not follow them.",null,1,cdpkp45,1rpiyx,askscience,new,5
patchgrabber,"This question is a little vague; there are a great deal of hormones affected by light, and ""affect a plant cell on a molecular level"" is incredibly broad, and has different implications depending on the age of the plant in question, not to mention differences between taxa.",null,0,cdpmnxr,1rpj0x,askscience,new,3
yankee333,"Like patchgrabber said, very broad question. Generally though, light intensity is actually the number of photons, i.e. quanta. So increasing light intensity would activate more light-sensitive proteins, and lead to a stronger physiological effect as more photons activate more light-sensitive proteins through biological conversion of the photon to an electron.",null,2,cdpqz6p,1rpj0x,askscience,new,1
Davecasa,"The space elevator concept involves putting something really heavy in geostationary orbit around Earth (a bit beyond this, actually, so that it generates a centrifugal force to hold up the cable). Geostationary orbit means that you're still orbiting (and still in free fall), you just happen to be falling around Earth once every time the planet rotates, so you stay above the same point on the surface. The altitude you need to be in for your orbit to have this property is 35,786 km. The ISS is only at 416 km, so it moves very quickly relative to Earth's surface.

In terms of orbital mechanics, there's nothing special about a geostationary orbit. You would still experience weightlessness.",null,2,cdpkwrp,1rpj3t,askscience,new,8
Fringe_Worthy,"Looking at [http://en.wikipedia.org/wiki/Space_elevator](http://en.wikipedia.org/wiki/Space_elevator) you want your space elevator to be balanced just past the Geostationary orbit. 

It looks like if you're connected to the elevator, you'd be feeling gravity opposed by centrifugal force.  It would go from 1g towards earth at the base, decreasing to 0g at the geostationary orbit (35,786 km) and then increasing, away from earth as you keep on going upwards past there.

A space elevator with a center of mass at ISS level would be either be in the final process of falling down, or have dangly bits moving at ISS orbital velocity ( 7km/s? ) That seems like it would generate excessive amounts of drag, debris, and/or self destruction.",null,0,cdpm2aq,1rpj3t,askscience,new,5
stuthulhu,"Zero gravity and weightlessness are very different things. People in space appear weightless because they are in free fall. They still are experiencing gravity. If the gravity of the Earth did not reach as high as orbit, things would not orbit. Ergo, the moon would fly away.",null,2,cdpl33k,1rpj3t,askscience,new,3
nucleomancer,"Basically No. Remember that Felix BaumThingies. He was way up there and he still fell like a brick. (Proof number one) (There is a small reduction in gravity as you go up, but that is near to nothing.)

Secondly. Weightlessness in space is only relative to the shuttle/station. And is caused by its high velocity. As the shuttle passed overhead, gravity is pulling it in under just the right force to make it go in circles.

To be more precise the shuttle is traveling at just the right speed where the pull of gravity causes it to go in circles.

Same thing goes for the moon by the way. All the way out there. THAT heavy and still going neatly in a ""circle"" around the earth.

The moon goes round every month, the shuttle every couple of hours.

The idea with an elevator is that it is forced to go at the speed of the earths rotation, even though it's distance would allow for a lower speed, thereby pulling the wire taught and allowing ""small"" objects to travel up. When the object gets all the way up there it travels at the same speed and can be released into orbit.",null,7,cdpk4vx,1rpj3t,askscience,new,2
Broes,"Gravity is every, even if you would be 1.000.000.000 miles from the sun, it would still exert a pull on you. The pull will however be small.
If you would mean weightless, you can become 'weightless' just by jumping of something. During your fall you will be weightless... until you hit something. To be permanetly weightless, all that is required is that you somehow avoid hitting something that will stop you from falling. ",null,7,cdplp6i,1rpj3t,askscience,new,2
MotoEnduro,"Yes, warm early spring conditions can cause buds on trees to open prematurely. If the buds open and begin to develop and then there is a hard frost the buds can die. The results of this can be damaging. For example if flower buds freeze there will be no flowers to fertilize, and therefore no fruit. With leaf buds, the bud may die, but dormant buds will usually awake and sprout new leaves, but this takes a serious toll on the vigor of the tree as It has already used its stored energy from last season to produce this seasons leaves. This happened in north America 2 springs ago, where there was a week in march with 70 degree days, followed by a hard freeze, and resulted in a 50% decrease in apple production in many areas.
Source: I am a forester.",null,7,cdplsrs,1rpkur,askscience,new,64
LetsSmokeWeedAboutIt,"According to this site,http://northernwoodlands.org/outside_story/article/how-do-trees-know-when-to-leaf-out-in-the-spring, trees have to go through a warming phase and then a refreezing phase before bud break occurs.  So they've basically developed a system to account for thawing in late winter so they aren't fooled into budding before another frost.  Of course some plants and trees will get tricked because weather can vary a lot, there can be multiple thaws and refreezes. Those plants might not die necessarily, but they waisted however much energy it took them to make the buds they lost.  But obviously most species seem to be pretty good or we wouldn't have as many plants as we do in northern climates.  

I'm sure theres more to it even that, like hormones, circadian rhythms, and light cycles stuff like that.  Plant's are just way more awesome then we give them credit for too.  If you have the time take a botany class at your local college, I'm in one right now and I get my mind blown to pieces all the time its great.",null,2,cdpm3kz,1rpkur,askscience,new,6
MrQuiver,"The gaps in the cage just have to be significantly smaller than the wavelength of the radiation. You know that metal screen that covers the front ""window"" of your microwave? Faraday cage. Microwave radiation has a wavelength of about 1 cm. Visible light /starts/ at 0.0005 mm! That's a big difference, so you can definitely block microwaves and let in visible light. Make a cage with a reasonably thick conductor (more than one layer of tin-foil) and gaps of 0.5 cm or less.",null,0,cdpl0o9,1rpl5q,askscience,new,7
feynmanwithtwosticks,"The body has 3 main opiood receptors, Mu, Kappa, and Delta. Each the receptors control the transmission of pain signals from the body to the brain. Opioids work by blocking (primarily) Mu opioid receptors, though each drug impacts the receptors in a unique combination (morphine acts almost entirely on Mu receptors while methadone is much more active on Kappa opioid receptors)

So they work by blocking the transmission of pain signals to the central nervous system. They have a ton of other effects as well, such as inhibiting the cough reflex, but that's outside of the scope here.",null,0,cdpy1qd,1rpl7a,askscience,new,3
yankee333,"picture 1: both

picture 2: both

drawing option 1 is correct

Fibronectin does not embed itself inside plasma membranes, but binds to integrins that themselves traverse the membranes. So it would be on the outside of the membrane, connected to a protein that is itself embedded in the lipid bilayer.

The reason why both pictures could indicate both is because basement membranes contain collagen.",null,1,cdpqwe0,1rpq89,askscience,new,2
walexj,"When the ball falls off as it's stationary, you notice that it tips to one side.

The same thing happens when the ball is spinning. It begins to tip to one side. But wait! The ball is spinning, so the side that it was tipping toward is now on the opposite side of your finger, so the ball starts 'falling' back toward the axis of spin. So, basically, so long as the ball is spinning fast enough, it will right itself every time it begins to tip. If it's right on the cusp of spinning too slowly to correct itself in time then it begins to wobble. When it's too slow, it falls off your finger as expected.

This is one of the features of angular momentum and the gyroscopic effect.

And the Earth's rotation has nothing to do with it, but if the Earth stopped spinning, we'd observe ~1000 km/h winds if we weren't also thrown into the nearest wall at ~1000 km/h ourselves, while every building (save for the few near the poles) would get torn off their foundation.",null,0,cdppoyr,1rpvm6,askscience,new,3
rcs_thruster,"For the earth question: The Earth would start to spin again, because of the Moon is revolving around the Earth. At the same time, the Moon would lose orbital velocity and eventually fall to the Earth. Feel free to correct me if I'm wrong.",null,0,cdpt270,1rpvm6,askscience,new,1
_ask_,"When a ball spins, centripetal force occurs, and centripetal force always points towards the center of the object it is emmiting from. If your finger is on the center of the ball, and if the ball is spinning, the ball will attempt to stay on your finger.",null,2,cdprpqs,1rpvm6,askscience,new,2
MrQuiver,"The water level stays the same. As soon as something is floating on a liquid, it will displace a volume of that liquid equal to the weight of itself and everything in it. This means it raises the water level by the same amount as if one had added an amount of liquid equal to its weight.

The ice is inside the boat, so the water level has already raised up to account for an amount of liquid water equal to the weight of the ice. When the ice is dumped overboard it is still floating in the water, so nothing changes. When the ice melts, it turns into an amount of water equal to its own weight (obviously, no mass can be lost in the melting process). So, no change to the water level.",null,1,cdpod85,1rpwbu,askscience,new,6
null,null,null,1,cdpop5d,1rpwbu,askscience,new,2
0ndem,"When the block goes into the water the level will go up.

When the block melts it will go down but should still be above the original level because you did add material.

You can test this with a glass of water and an ice cube.


This is assuming the block has a large enough size to impart a noticeable difference and the water that was frozen was the same as that which is in the pool.

",null,13,cdpobyf,1rpwbu,askscience,new,2
erath_droid,"Half of your genes will come from your father, half will come from your mother. (What actually happens is a bit more complex than that, but this is close enough for this discussion.) This means that any single parent/child combination will have half of the same genes.

When it comes to siblings (from the same parents) you can have as many as 0% of the same inherited genes or as much as 100% of the same inherited genes.

For example, say for one particular allele the father has type Aa and the mother has type Bb. This means that the child can have one of four allele combinations: AB, Ab, Ba and ab. It's entirely possible for one child to inherit the combination AB and the other inherit ab. This would be a case of zero shared genes for that particular trait.

However, in the real world, due to the large number of genes that are inherited siblings will share close to 50% of the same (inheritable) genes making them in general as genetically similar to each other as they are to their parents.",null,4,cdpp17m,1rpxrq,askscience,new,33
null,null,null,4,cdpok54,1rpxrq,askscience,new,25
Xinlitik,"You are 50% related to mom and dad--this is basically static, although de novo changes and certain recombination events do account for a small deviation. Your non-identical siblings are theoretically 0-100% related to you, but since the genome is so huge, it's going to be close to 50%. That is, you could somehow get the exact 50% of dad's genome that your brother did not get, but the chance is extremely low, just like how flipping a coin 1000 times is probably not going to give you 1000 heads.",null,1,cdpq1t9,1rpxrq,askscience,new,5
OnceReturned,"In the average, oversimplified, model case, you will share 50% of your variable genes with both your siblings and your parents, making you equally similar to both, though the 50% that you share with each is not likely to be the same 50%.

However, you could think about the question with an eye towards probabilities and find that the two cases are not quite as similar as they seem.  Specifically, if the question is, ""Are you more likely to find a sibling pair that is &lt; 50% identical than you are to find a parent-child pair that is &lt; 50% identical,""  the answer is yes.  Here's why:

The rule that we're using to figure this out is that each parent contributes ~50% of their genes to their children.  

Because the genome is large, there are many, many different combinations of contributions from each parent that satisfy this rule of thumb of each parent having provided 50% - ranging from the same 50% from each parent in each sibling (making the siblings identical), to the opposite 50% from each parent to each sibling (making the siblings have 0% of their variable genes in common).  

In the case of your own child, no matter which 50% of your variables genes you contribute to their genome, you share 50% of variable genes with them.  

Therefore, as long as the rule of thumb of 50% parental contribution holds true, you are guaranteed to be 50% similar to your offspring.  While under the same circumstances, you may be anywhere from 0-100% similar to your siblings.  In other words, there is far more variability in the similarity between siblings than there is between parents and their children, though all that variability centers around the same point that the parent-child pairs exhibit consistently.",null,0,cdpsa4f,1rpxrq,askscience,new,2
LietKynes62,"On average, you're slightly more related to your parents. Since the creation of your siblings involved an entirely separate meiosis and fertilization process, they have had an additional opportunity for random mutation to occur. Meaning, they have twice as many mutations different from you than your parents would. Also taking into account sex chromosome differences in males in females, there's other variables involved. Practically speaking, though, you share approx 50% DNA with your parents AND your siblings.

To simplify it -- you are guaranteed to get 50% of your DNA from either parent. You may have less in common with your sibling depending on mutations and gender(since the X chromosome complicates things).",null,0,cdq3vfp,1rpxrq,askscience,new,1
GProteins,"They developed them separately. Current knowledge is that birds evolved from small dinosaurs, who had normal toothed jaws. This theoretically happened around the [mesozoic era](http://en.wikipedia.org/wiki/Mesozoic), which started around 250 million years ago.

Squids and octopi, on the other hand, are evolved from earlier forms of mollusks, which separated themselves out from other life forms in the some time during the [cambrian era](http://www.jakobvinther.com/Mollusc_evolution.html), which ended about 450 million years ago.",null,0,cdppwko,1rpxux,askscience,new,30
LietKynes62,"They developed seperately, which would be an example of analogous structures. This means they have similar appearance and function, but developed independently. A common example involved the octopus is their eye compared to the mammalian eye. We know, because of the organization of its layers, that the octopus eye developed independently from the mammalian eye. However, they share similar appearance and function.

Another example would be comparing an insect's wings to a bird's wings -- they look similar, are both used for flying, but have different structures and evolved separately.",null,0,cdq3rnj,1rpxux,askscience,new,2
iyaerP,"The human body is quite good at healing itself, and bandages don't accelerate the healing process so much as they prevent interference by outside forces. Without a bandage, a wound is vulnerable to infection by foreign bacteria. By keeping it isolated, you allow the body to heal on its own without also having to have the immune system wage a constant war to keep the healing process from being contaminated. All of the energy can go to growth. This is the same reason that Vaseline is so helpful, because it forms an air-tight barrier to keep contamination out of the wound.",null,0,cdpx74r,1rpzt2,askscience,new,8
veracosa,"Keep in mind that a bandage does not always mean healing will be faster or better. While a bandage is good at keeping stuff out, it also keeps stuff in. 

So if, for example, you have a shallow paper cut, the physical barrier of the skin has been broken and makes that site susceptible to bacterial (or less commonly fungal, viral, parasitic) invasion. By placing the typical band-aid, you are simply restoring a physical barrier, keeping bacteria as well as general dirt and lemon juice out of the wound. 

Now say you stepped on a nail. This kind of wound is deep, and covering the opening would keep debris from getting in, but the trouble is, the nail already shoved debris into the wound. So that wound needs the body's immune system to call white blood cells to the site through chemical signaling (really cool stuff!), and and those cells will deal with bacteria, and the resultant debris and dead white cells (pus) need to drain out, otherwise you get an abscess (buildup of pus and bacteria) or infection can spread. Then you wind up on r/popping.

Bandage management of serious wounds (areas of tissue loss or burns, etc) gets very detailed and quite involved, but it really neat stuff.

So, bandages can be good, but also can be trouble!

At a glance around the internet, here are some links for more info.

[Basic Info about neutrophils](http://www.britannica.com/EBchecked/topic/410999/neutrophil#ref1114809), the most common white blood cell in people. 
[Nursing school guide for wound classification and management](http://www.tphc.com.au/Brochures/Module%204.pdf), there are pics of real wounds in there so not for squeamish sort of folks, but is pretty basic",null,0,cdq07uk,1rpzt2,askscience,new,6
Apeirohedron,"It depends on the manufacturer and type of display.  On most screens, the components that handle color and those that handle lighting are separate.  The color is, as you said, red, green, and blue subpixels, and the brightness is just a backlight.

On some newer displays, however, these two components are being merged and black pixels are truly off.  A great example of this is an AMOLED smartphone such as the Samsung Galaxy S4's screen.  You can see the difference when scrolling past truly black content - the pixels take longer to turn back on than change color.",null,2,cdpu4bj,1rq1ho,askscience,new,5
IHTFPhD,"There is the Xenon problem, which is actually the opposite of what you asked for - there is a much higher concentration of Xenon found in asteroids and elsewhere in the universe than we find it here on earth. There are quite a few recent discoveries that may reveal the mechanism behind this discrepancy though, several research groups have found that Xenon may be soluble in rocks under very high pressures:

http://www.nature.com/nature/journal/v490/n7421/full/nature11506.html

http://www.nature.com/nchem/journal/v5/n1/full/nchem.1497.html",null,0,cdpueyn,1rq4i9,askscience,new,1
TangentialThreat,"Water is relatively rare. There's some in the outer solar system but Earth is really wet for where it is.

We also seem to have a monopoly on molecular oxygen, and many minerals formed by the interplay of water, oxygen, active plate tectonics and living things. Martian colonists will be forced to import marble countertops from Earth.

",null,0,cdpxgn4,1rq4i9,askscience,new,1
OnceReturned,"A big problem with much of his work is that it's hard to test it in a scientifically rigorous way.  He had a lot to say about the subconscious/unconscious - that many of our actions have deep seeded motives found therein, and that the landscape of that part of our mind developed in such and such a way.  The problem is that, while behaviors can be observed, the mind cannot.  There's no way to see if he's right about why you did something because no one has access to your motives as they exist in your mind, in many cases not even you.  It is widely accepted that scientific theories/hypotheses have to make testable (in principal) predictions, and that if a theory doesn't do this, it's not really a theory.  Freud gets a lot of criticism based on this, because he has his ideas about how the mind works and then uses them to explain observed behavior in an ad hoc way.  How could you ever test whether or not all of our actions arise from our sexual desires?  There's no way to falsify that claim even in principal - it's untestable - so it's not a scientific claim.  It's more of a philosophical choice to interpret things in a certain way.  You could just as easily say that all of our actions are motivated by our fear of death.  How could we find out which position is correct when, according to Freud, we ourselves don't even always have access to our own motives at the deepest level?",null,0,cdpuz6w,1rq4ni,askscience,new,4
Venarius,"While interesting to ponder, most any respected psychologist today wouldn't call himself Freudian...  And if they do its with a HEAVY dose of modern psych to boot.  Hypnoses has been proven all but useless (too easy to force-create false memories/too difficult to substantiate pieced-together memories).  

His view on hysteria was actually well shared in the 1800's.  Doctors are the ones who invented the Dildo.  

His work was based on mostly case-studies, and while a fascinating journey into personal depths of the human psyche and it's churnings, it lacks the hard hitting truth-i-ness of being of predictive or analytical in most any way...  and was really just ad-hoc explanations...

",null,2,cdpyqcc,1rq4ni,askscience,new,3
rocketgolfer,"The answer to this is that air has viscosity. One of the simplest explanations for how wings generate lift is that they are ""magical air deflectors"". In other words, wings are able to generate lift because they can change the direction of the airflow and take advantage of momentum conservation. For an ""ideal"" fluid, there is no viscosity and the notion that maximum lift would occur at a higher angle of attack would be correct (though there is still debate over whether any lift at all is possible in the absence of viscosity, but this is beyond the scope of this discussion). Unfortuately, air is not ideal and the viscous stresses, both laminar and turbulent, resist the ability of the pressure field to turn the flow and generate the lift. When the air is no longer able to turn along the surface of the wing, the flow separates and decreases the lift (i.e. it's not turned as much as it could be). At some point, typically in the angle of attack range you describe, the increase in lift due to increasing angle of attack (the ideal flow behavior) is exactly canceled by the decrease due to flow separation (the viscous behavior). Further increases in angle of attack tend to cause viscosity to win out even more, decreasing the lift.

Source: I'm a soon-to-be PhD aerodynamicist (currently holding an M.S.) at a major research university.",null,0,cdpzrle,1rq4wf,askscience,new,5
Longwaytofall,"On my phone on a family trip, so I'll keep this short.

There's two things at play here. Laminar flow and induced drag. Laminar flow is a flow of air around the airfoil of the wing which remains smooth across the chord. There is a critical angle at which a given wing will stall ( that is to say the air delaminates and become turbulent). When this happens the bernulian lift (vacuum on top, high pressure beneath) is largely destroyed.

If we were speaking about strictly Newtonian lift (an angled plane pushes air down, and thus itself up), then your 45 degree assumption might be more realistic. Because an airfoil uses both Newtonian and bernulian lift, that critical angle of attack depends on how quickly laminar flow is lost.

The last factor is induced drag. Planes experience two types of drag, parasitic and induced.  Parasitic drag is from the whole plane traveling through the air, and as you would expect increases with speed. Induced drag however, is negatively correlated to speed. Induced drag comes from angle of attack. As an airplane slows down, it must increase angle of attack to maintain level flight. Extra angle of attack presents more wing area to the relative wind, and produces extra drag. So it would make sense that as speed increases, angle of attack decreases, which decreases induced drag.

There is the sweet spot where all forces come together for maximum lift with minimal drag. This happens to land around 20 degrees... ( there's a huge variation between aircraft designs on this number).
",null,4,cdpv62b,1rq4wf,askscience,new,5
erros_,"The key word you are looking for is [seasonal lag.](http://en.wikipedia.org/wiki/Seasonal_lag)

Mostly due to the oceans, which can store a large amount of latent heat, the change in (air-) temperature induced by insolation is delayed. The oceans release latent heat for some time in the early winter and later regain it in early summer.

",null,1,cdpuslp,1rq53q,askscience,new,7
Platypuskeeper,"Two atoms (or more) that are bound to each other can vibrate back and forth because of the chemical bond, which is due to the energy and motion of the electrons. The two nuclei have an equilibrium point which minimizes the electronic energy, so pushing them farther together or pulling them apart from that position will result in a restoring force pulling the nuclei back. They act a bit like two balls connected by a spring, where the nuclei are the balls and the electrons are what make up the 'spring'. If you pull hard enough the spring (chemical bond) will break though.

To give a simplified rationale for why this is, you can imagine that a chemical bond is a situation where you have electrons spending time between the nuclei of the atoms. By pulling the nuclei apart, the electrons will be farther away from one or both of the positively-charged nuclei, which increases their potential energy. But if you push the atoms too closely together, the electrical repulsion between the two nuclei increase. 

From this perspective it's a classical and not a quantum mechanical thing, even if the electron's behavior giving rise to bonding is quantum-mechanical. Quantum mechanics does manifest itself in how the vibrations work though: First in the fact that the atoms can only vibrate with certain specific frequencies, and second in that the lowest-energy state is not entirely stationary. They're always vibrating at least a little bit, while a pair of classical balls with a spring have no problems being stationary. 

A single atom can't vibrate any more than a single hand can't clap, an object in motion will remain in motion, an atom can't spontaneously reverse its direction. There needs to be two atoms attracting each other (read: chemically bonded). In fact the [technical definition](http://goldbook.iupac.org/M04002.html) of what constitutes a molecule is that two or more atoms attract strongly enough to be able to vibrate.

When it comes to heat, all forms of energy in the system are actually included - such as the translational (linear motion) and rotational kinetic energy of the molecules, in addition to the vibrational energy. The electronic energy and other things can be included as well, but at temperatures around room temperature, more or less all the thermal energy is bound up in the kinetic energy, and for atoms in solids which cannot rotate or fly off, all the kinetic energy is bound up in the vibrations. Which is why one often encounters a bit over-simplified statements saying ""heat is the (vibrational) motion of atoms"". 
",null,0,cdptid6,1rq9vh,askscience,new,4
null,null,null,1,cdpvml7,1rq9wj,askscience,new,3
Ursus_misanthropicus,"While not necessarily a cause, individuals with PTSD seem to show a reduced amount of GABA in the insula -- [source](http://onlinelibrary.wiley.com/doi/10.1002/da.22155/abstract;jsessionid=F24E1AEE7D4E056CC27DD8FBD46873E6.f04t02). The insula, among many other things, is used heavily in emotional responses and is associated with a wide array of ""disgust"" reactions, both to real sensory input (sight, smell, etc.) as well as imagined events -- [source](http://www.cell.com/neuron/retrieve/pii/S0896627303006792) and [source](http://www.plosone.org/article/info%3Adoi%2F10.1371%2Fjournal.pone.0002939).

So how does it matter?

If you've ever imbibed with alcohol, you're probably well aware of GABA's role as neurological depressant. It slows things down. When you have a higher-than-normal release or availability of GABA, it makes it generally harder for neurons to fire and communicate, slowing down processes. GABA is the depressant counterpart to glutamate, which is the main excitatory neurotransmitter in the brain. If there were no GABA, the brain would essentially die from excitotoxicity (over-firing of neurons; death via electricity). 

So, given a reduced presence or function of GABA in the insula, you might expect to see less suppression of the emotional/disgust response that the region is known for. This could result in disgust/fear being triggered in situations that probably might not be triggered in a normally functioning person, especially given the past history of traumatic events. 

As for why some people get PTSD and others do not, the verdict is still out. There appears to be a genetic component, and twin studies have shown if one twin gets PTSD after a traumatic event, then their twin is also at an increased risk for getting PTSD after a traumatic event. Genetic factors may account for 30-40% of PTSD heritability -- [source](http://www.ncbi.nlm.nih.gov/pubmed/24103155).",null,0,cdq10g6,1rq9wj,askscience,new,2
veul,"I am a veteran and read a prominent book on the topic called On Killing.  There are a bunch if factors, such as Officers experience PTSD less.  This could mean they see less action, more education helps, being an issue orderer lessens the burden.

Another are medics typically experience the brunt of the aftermath.  However they get PTSD much less likely because they are trained that this is what they will see.  The other is even if its an unjust war medics can justify that they are only helping heal their brethren as opposed to injure others.

I'm on my phone but I can elaborate on a few points if still interested.",null,0,cdq3yzn,1rq9wj,askscience,new,2
Smoothened,"They shared a recent common ancestor and were genetically very similar. All three species belong to the genus *Homo* within the great apes. It's generally required for interbreeding that the species share a recent common ancestor because factors that determine whether they are compatible for reproduction (such has having the same number of chromosomes) evolve independently from functional traits that make them visually or behaviorally similar. [Convergent evolution](http://en.wikipedia.org/wiki/Convergent_evolution) can lead species to evolve similar traits to match their comparable niches, but if they come from different evolutionary tracks their reproductive systems would typically remain incompatible. A good example is the thylacine, or Tasmanian tiger, which looked and behaved a lot like a feline, but as a marsupial there is no way it could have interbred with cats which are plancental. 
",null,0,cdptnvm,1rqan1,askscience,new,4
RugglesIV,"The energy stored in batteries is stored as chemical potential energy. ""Energy"" is not a thing, rather it is a property that things can have, and can be understood as the ability to make other stuff happen. The energy in batteries being chemical potential energy means that there is a chemical reaction that occurs in the battery to produce a voltage. The reaction only goes forward if energy is drawn from the battery by means of drawing current from the voltage. So, if you don't use a battery, the voltage difference between terminals goes (almost entirely) unexploited and no (or very, very little) energy is drawn from the battery.",null,0,cdptml0,1rqao3,askscience,new,2
arumbar,"This is a rather complicated aspect of neuroscience, but a good starting point would be the two separate neural pathways for eye movement - [saccadic](http://en.wikipedia.org/wiki/Saccades) and [smooth pursuit](http://en.wikipedia.org/wiki/Smooth_pursuit).  The former describes the jerky eye motion for controlled gaze shifts, and is mediated by the [paramedian pontine reticular formation](http://en.wikipedia.org/wiki/PPRF), frontal eye fields, superior colliculus, and parietal cortex.  The last region is notable because that is where much of your attention and orientation responses are based.  The latter describes your smooth eye motion following a moving object, and is mediated by regions of the temporal cortex of the brain, which project to the frontal eye fields and the flocculus and vermis of the [cerebellum](http://en.wikipedia.org/wiki/Cerebellum).  [More general reading here](http://en.wikipedia.org/wiki/Eye_movement_%28sensory%29).",null,1,cdpwk5g,1rqaqp,askscience,new,8
justsoican,"Look off into the distance and imagine following a line thats wraps around the room you're in.  Now follow it slowly.  You can make your eyes move without the jerking, the jerking is a matter of your eyes actually wanting to focus on the next interesting thing in the path, thus moving quicker to get there.",null,1,cdq1hfe,1rqaqp,askscience,new,3
snusmumrikan,"Our brains are only responsive enough to process images moving across our eyes at a few degrees [per second maximum](http://www.researchgate.net/publication/22013675_Visual_acuity_in_the_presence_of_retinal-image_motion) so we need to basically look at a stable field of view. That's why your eyes will spin quickly to a new viewpoint instead of slowly panning when you conciously move your eyes that way. 

If you move your head and concentrate on one spot, say your computer screen right now, your eyes are moving completely smoothly at whatever rate you choose aren't they? It's the stability of the field of view that matters, not the speed of the eye movement itself, as our brains have evolved to maintain the focus of an image on the [fovea](http://en.wikipedia.org/wiki/Fovea) which is the area with the highest visual acuity and covers only a few degrees of vision in the centre of the field of view.

The reflex action to keep the eyes focused on one point is more accurate than our ability to track a moving object, try this: wave hand 1ft in front of your face and speed it up to the point it becomes a blur. Then hold it still and try to move your head faster and faster as you look at the hand - you won't be able to do it fast enough to lose focus on the hand as the muscles counteracting the head movement act instinctively and automatically. ",null,5,cdpvy1q,1rqaqp,askscience,new,5
jeggy,"One of them is [cosmological inflation](https://en.wikipedia.org/wiki/Inflation_(cosmology\)). This is the idea that just after the Big Bang, the universe underwent a brief period of exponential expansion. Inflation supposedly solves a number of problems with the conventional Big Bang model: the fact that the bulk properties of the universe (e.g., the average amount of matter per cubic meter) appear to be the same everywhere, the fact that the universe has an overall Euclidean geometry, and the fact that there are no magnetic monopoles.

Part of the reason people are paying attention to this is because it can be tested: a lot of models of inflation predict that there are large-scale swirl patterns (""B-modes"") in the [cosmic microwave background](https://en.wikipedia.org/wiki/Cosmic_microwave_background) (CMB). CMB experiments have been focusing on polarization measurements for the last decade or so, and are now starting to get to the sensitivity levels required to see B-modes. The South Pole Telescope found small-scale B-modes [a few months ago](http://www.nature.com/news/polarization-detected-in-big-bang-s-echo-1.13441), but these are known to be generated predominantly by gravitational lensing as the CMB travels through galaxy clusters. The hope is that in a few years, CMB experiments will be able to make a statement about the presence or absence of large-scale B-modes.",null,0,cdpycpy,1rqat6,askscience,new,2
Platypuskeeper,"The (Real) square root is defined for positive numbers. So sqrt((x+2)(x-3)) is undefined for -2 &lt; x &lt; 3, since that's where the (x+2)(x-3) is negative. 

When you split that into a product of two square roots, then you run into the issue that the product of an undefined thing is undefined. Sqrt(x-3) is undefined for all x &lt; 3, so the product is undefined for all x &lt; 3. (Sqrt(x+2) is undefined for all x &lt; -2, but if x &lt; -2 then x &lt; 3 of course)

An even simpler example is f(x) = 1, which is defined for all x, and g(x) = x/x , which isn't defined for x = 0. So f(x) = g(x) only if x ≠ 0. You do have to pay attention to the domain when making these kinds of equalities. A classic example are the many ['proofs'](http://www.math.toronto.edu/mathnet/falseProofs/first1eq2.html) that 1 = 2 (or 1 = 0 sometimes).

",null,1,cdpstsr,1rqbs4,askscience,new,7
trebuday,"Life as we know it requires water to survive.  So, when we look at planets around other stars, we look to see if they are not so far that all the water is frozen, or that it's not so close that all the water is boiled away.  

It is true that life may exist without the same conditions, but at that point, you're opening up a door to the question of defining what life itself is, and that leads to too many different options. 

So, for now, we look for life that looks like Earth.  The minute we find something ""living"" in conditions completely un-Earth-like, then we can broaden our search!",null,0,cdq3qgn,1rqd1e,askscience,new,1
TheNextDoctorWho,"Light can be absorbed by the molecule if it has the right energy corresponding to an electron transfer between two molecular orbitals.
Usually, if you don't have any functional groups, this transition energy between two molecular orbitals, for example the [HOMO and the LUMO](http://en.wikipedia.org/wiki/HOMO/LUMO) is relatively high..
Light with shorter wavelengths, like UV-light, has higher energy so most normal substances absorb at a certain wavelength in the UV-region of the electromagnetic spectrum. If only light in the UV-region is absorbed, we normally don't see special color from this compound.

If there are delocalized pi-electron-systems, like in aromatic compunds, then the energy difference between HOMO and LUMO gets smaller. Therefore, less energy is needed to excite this transition and therefore light with longer wavelength, like visible light, can excite the transition. If for example red light gets absorbed, the [complementary color](http://en.wikipedia.org/wiki/Complementary_color) would be green and we would perceive the substance as green (Hopefully I'm right here, difficult as an almost colorblind person).

If you look at most dyes, they contain large aromatic systems and most often additional functional groups which create ""push-pull-systems"", thereby increasing the resonance and lower the HOMO-LUMO-gap even further. Look for example at [malachite green](http://en.wikipedia.org/wiki/Malachite_green), which is excited by red light, absorbs it and is perceived as green. By adding another functional group, the color properties are changed because of the changed electronic distribution in the dye and you get [crystal violet](http://en.wikipedia.org/wiki/Crystal_violet) (the situation is a bit more complex in malachite green since you have two absorptions with different wavelengths, but in theory it works that way).

During my studies, the dye-expert of our university explained the theory of different dyes by a concept propagated by two relatively unknown chemists, König and Ismialsky. It makes a lot of sense to me, but unfortunately there is not much literature out there, besides [one old german paper from 1926](http://onlinelibrary.wiley.com/doi/10.1002/prac.19261120101/abstract) and the script of my prof.
In principle, for most substances to have color (therefore, absorb a portion of the visible light), they ideally have to contain this system: Donor-substituent ---pi-system---acceptor-substitutent---pi-system---donor-substituent. The reverse case is also possible. The better the system is contained in the molecule, the lower the required energy for an HOMO/LUMO-transition.

If you want I can go into more detail and explain this on some dye molecules. I hope this short excursion into dye molecules gave you some insight into why molecules absorb certain wavelengths of light.

TL;DR: It's all about the HOMO/LUMO-gap and therefore, as almost always in organic chemistry, about molecular orbitals, which are influenced by the bonds and groups contained in the molecule.
",null,0,cdpyugy,1rqd7b,askscience,new,3
joca63,"There is only one way to absorb light. That is for a molecule to absorb the energy and for one of its energy levels to increase.

To deal with all wavelengths of light, not just visible light which already has an answer, one must examine all energy systems of the molecule.

There are three main energy systems that are a molecule can absorb with. Vibrational energy, rotational energy, and electronic energy. 

The rotational energy of a molecule can be increased by specific increments which are calculable, and the size of the increments depends mainly on the mass and the radius of the molecule in question. These energy levels tend to be very closely spaced and do not take very much energy to jump from one level to the next. These are generally probed at the far IR wavelengths.

Vibrational energy levels depend on the strength of the bond, and the masses of the atoms. Again there is not a continous spectrum of allowed states. So the difference in energy of the allowed states is measureable. These are probed in the near IR. In organic chemistry IR is useful for determining functional groups such as aromatics or carbonyls as they have a characteristic absorbance.

Electronic energy levels are more largely spaced and have to be probed in the visible or UV spectrum. Molecules usually only absorb in this area if they are either a transition metal complex, or are a conjugated pi system. Drawing molecular orbitals isnt an easy process to explain, but the general idea is the same as the previous systems. There are only a limited number of allowed states and the difference between these states is what is being probed. A key difference is that with the previous examples a more obvious property was changing. Either the molecule spun faster or vibrated faster. Easy enough to imagine. With electronic absorbance it is the promotion of an electron from one orbital to another that causes the absorbance.

No proper molecule (salts and metals can) absorbs past the near UV range. This would corresond still to an electronic absorption, however the larger HOMO/LUMO gaps are associated with moving an electron into an antibonding orbital, and so in the far UV to x-ray and beyond, the tendancy is for molecules to just simply break apart or ""ionize""",null,0,cdq24y8,1rqd7b,askscience,new,1
OnceReturned,"A comprehensive answer to that question would have many components, but in animals there are a few dominant factors:

DNA - Cell division is not perfect, and DNA damage occurs naturally due to environmental factors and normal cellular processes.  Genomic robustness and DNA repair capacity is a huge factor in lifespan determination.  DNA damage is generally considered the leading cause of aging.  Species with more robust genomes and better DNA repair mechanisms tend to have greater lifespans.

Physical fragility - There are many potentially fatal events which individuals have some probability of experiencing in any given time period.  These include things like harsh environmental conditions (i.e. a bad winter), infectious diseases, and encounters with predators.  Every organism has some set of such events which are likely to be fatal, and which occur with some probably, such that the more time passes, the more likely the organism is to have been exposed to such an event.  The more sensitive an organism is to these things, the shorter its lifespan is expected to be.  For example, small animals are particularly susceptible to predation, so the odds that they have a fatal encounter with a predator in any time period are higher than the those same odds for larger animals that are higher on the food chain.  

Biological wear and tear:  Tissue and organs get worn out through use.  Small animals generally have higher metabolisms, which amounts to something similar to more ""friction"" or ""wear and tear"" on all of their systems (i.e. more heartbeats per minute).  So, they don't typically live as long as typical larger creatures with slower metabolisms. 

Generally, larger animals live longer.  Dogs are something of an exception.  It's not exactly clear why large dogs don't live as long, but it likely has something to do with the fact that domesticated dogs are highly inbred.  This means they have genetic risk factors that are much less common in wild animals.  This manifests in a variety of ways.  For example, large dogs are more likely to get cancer, and this is likely because they simply grow more cells throughout their lives.",null,0,cdptcay,1rqeob,askscience,new,2
trebuday,"&gt;1) Can we estimate the pH of the world's oceans at defined points circa 55mya~1mya to hundredths of pH units?

The acidity of the ocean appears to be defined by the amount of CO2 in the oceans and atmosphere (since the two mix).  In this way, ocean acidity is closely linked to atmopheric temperatures.  When CO2 is introduced to seawater, it combines with H2O and CO3+ to produce two bicarbonate (HCO3-) molecules.  This process lowers the pH by removing the carbonate. 

These effects are most readily seen in the shells of Benthic Foraminifera, but can also be measured roughly in changing effects of CO2 weathering on land.

According to [Rae, et. al., 2010](http://www.academia.edu/1464312/Boron_isotopes_and_B_Ca_in_benthic_foraminifera_Proxies_for_the_deep_ocean_carbonate_system),  use of Boron isotopes in Benthic Foraminifera can be used to accurately determine pH within 0.03.  Benthic forams can be used as accurate proxies at least as far as 1 mya, but I am having difficulty finding anything detailing the loss of accuracy the older the forams are. 

&gt;2) How temporally far apart is each pH read? Could the alkalinity of the ocean have changed significantly and re-normalized between each read?

It appears the sampling interval for benthic forams is about 1,000 years [(Lisiecki, L. E., and M. E. Raymo, 2005)](http://www.lorraine-lisiecki.com/stack.html).  It could be possible for the composition of the ocean to suddenly oscillate, but we don't have any way of telling it for sure.  However, the current idea is that if the ocean did change so drastically in so sort a time span, it would take much longer to recover than 1000 years. 

An interesting and related event is the [Paleocene–Eocene Thermal Maximum](http://en.wikipedia.org/wiki/Paleocene%E2%80%93Eocene_Thermal_Maximum), where global temperatures and ocean acidity changed drastically in a very short time span, about 22 million years ago.

&gt;3) If pH is determined geologically: does the method used account for the chemical state of the ocean at each time point - dissolved gases / electrolyte concentration differences?

As I said before, geologic methods of determining atmospheric CO2 concentrations (and by extension oceanic CO2, and oceanic pH) are very rough, since they are very slow processes. Benthic forams respond much more quickly to composition changes.  I doubt they'd be able to tell the difference in regards to specific composition changes, and I'm having difficulty finding papers that say otherwise.

&gt;4) If pH is determined paleontologically: does the method used account for undefined phenotypic traits of fossilized organisms? What are the habitable extremes for informative fossils?

It appears [foraminifera](http://en.wikipedia.org/wiki/Foraminifera) are around wherever there is ocean, and the methods of determining compositional differences over time involve isotope ratios, which are not specific to phenotype of the forams.  ",null,0,cdq3ngr,1rqeuc,askscience,new,2
RetraRoyale,"According to General Relativity (as I understand it), and in particular, [Mach's Principle](http://en.wikipedia.org/wiki/Mach's_principle), A rotating universe would be indistinguishable from a non-rotating one in which you yourself were spinning. That is, if the universe were spinning, it would emit gravitational waves in just the right way that made you think you were spinning and the universe wasn't.

As for everything else you've said, it's not very clear what you're talking about because our description of viscosity and friction don't really work for General Relativity, so you're inappropriately generalizing those concepts.",null,3,cdpykkm,1rqeus,askscience,new,19
Sexy_Philistine,"Godel had some interesting solutions to the field equations that implied a rotating universe (and time travel!), though they don't describe our universe.  Still interesting in that it's a consistent description of a hypothetical universe 

http://en.wikipedia.org/wiki/G%C3%B6del_metric#Cosmological_interpretation",null,0,cdq2pgn,1rqeus,askscience,new,2
null,null,null,1,cdpwy9i,1rqeus,askscience,new,2
UnkeptPorpoise,"What you are describing with matter bending space time is, I believe part of special relativity? Not sure how that relates to the universe as a whole spinning. The question would be spinning in relation to what? Maybe I just misunderstood your question. I am by no means an expert though.",null,9,cdpwzj7,1rqeus,askscience,new,5
DrRagnarok,"It depends on what type of monogamy you mean.

Birds tend to be socially monogamous, which means they find a mate, have children, and raise them with the same partner. However, birds typically aren't sexually monogamous. For example, the scissor tail flycatcher has a ""cheating"" rate as high as 70% some seasons. This is due to seeking out the most ""fit"" mates, so one can have better offspring. 

As for being socially monogamous, the theories I've read believe it has to do with birds requiring more help early on compared to mammals. Where most mammals are walking within hours of being born, it takes birds several weeks before they're flying. Since breeding is all about continuing on your genetic line, it would make sense for bird fathers to hang around to ensure their success.",null,0,cdptta2,1rqgkj,askscience,new,4
KarlOskar12,"The performance of athletes today is far superior to what it was back in the ancient olympics partly because of the use of steroids (a *big* part) and also better training, nutrition, equipment, etc. For example - the swimming events were done in the sea, then they were done in cold pools, now they're in heated pools with very specific chemical compositions and the temperature is highly regulated for maximum performance.",null,0,cdq1auf,1rqgtw,askscience,new,1
