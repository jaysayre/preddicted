<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="">
    <meta name="author" content="">

    <title>Preddict It! By Serguei, Jay, and Sebastian</title>
    {% load staticfiles %}
    <!-- Bootstrap core CSS -->
    <link rel="stylesheet" type="text/css" href="{% static 'visualization/css/bootstrap.css' %}" rel="stylesheet">

    <!-- Add custom CSS here -->
    <link rel="stylesheet" type="text/css" href="{% static 'visualization/css/simple-sidebar.css' %}" rel="stylesheet">
    <link rel="stylesheet" type="text/css" href="{% static '/visualization/font-awesome/css/font-awesome.min.css' %}" rel="stylesheet">

  </head>

  <body>
  
    <div id="wrapper">
      
      <!-- Sidebar -->
      <div id="sidebar-wrapper">
        <ul class="sidebar-nav">
          <img id = "logo" src="http://www.redditstatic.com/about/assets/reddit-alien.png">
          <li class="sidebar-brand"><a href="/">Preddict It!</a></li>
          <li><a href="collect">Collecting Data</a></li>
          <li><a href="visualize">Visualizing the Data</a></li>
          <li><a href="build">Building the Model</a></li>
          <li><a href="findings">Findings</a></li>
          <li><a href="about">About</a></li>
        </ul>
      </div>
           <!-- Page content -->
      <div id="page-content-wrapper">
        <div class="content-header">
          <div>

          </div>
          <h1>
            <a id="menu-toggle" href="#" class="btn btn-default"><i class="icon-reorder"></i></a>
            Collecting the Data
          </h1>
        </div>
        <!-- Keep all page content within the page-content inset div! -->
        <div class="page-content inset">
          <div class="row">
            <div class="col-md-12">
              <p class="lead">We begin with constructing a mental model of Reddit and considering the most important elements that we will need to collect before proceeding with statistical analysis. There are more then 278,000 subreddits on Reddit and each has its own rules and community which means that each requires a different approach to predict a post's success. Depending on the size of the community, a different score or number of comments will count as success. For instance, a subreddit dedicated to jokes will have much different indicators of success than a subreddit dedicated to science.</p>
              <p class="lead">We wanted to compile a list of roughly 10 largely-text based subreddits that have a large community and cover diverse topics. This is our list:</p>
              <ul class="well">
                <li><b>explainlikeimfive</b>, a subreddit where people ask questions and get answers accessible to laypeople
                <li><b>AskReddit</b>, a subreddit where people ask questions about anything. This is probably the largest and most actice subreddit on reddit
                <li><b>TalesFromTechsupport</b>, a subreddit where people working in tech support tell stories from their job
                <li><b>talesFromRetail</b>, which originated from talesfromtechsupport. People there work in retail and tell stories from their job.
                <li><b>pettyrevenge</b>, where people tell stories about revenges they got
                <li><b>askhistorians</b>, where you can ask historians about anything related to history
                <li><b>askscience</b>, where you can ask scientists about anything related to science
                <li><b>tifu</b>, which is short for "today i f***ed up" and where people tell stories how they did
                <li><b>nosleep</b>, where people tell scary stories
                <li><b>jokes</b>, a subreddit dedicated to jokes
                <li><b>atheism</b>, where people who are atheists discuss religious topics
                <li><b>politics</b>, a subreddit about political news and discussions
              </ul>
              <h1>
              Getting data
              </h1>
              <p class="lead">Reddit offers an API to access their site which can be found here. We used it to download all possible data sets for our list of subreddits. Reddit allows to access the top 1,000 posts for each of the following lists: Top (all / week / day), new and hot.</p>
              <p class="lead">At first we used a wrapper for the API called praw to download our data. We found it to be very slow, and which was problematic given the sheer amount of data we required. For this reason, instead we wrote our own functions to access the reddit API. Ultimately, using our scraping algorithms, downloading the posts for all subreddits gave us a list of a total of 44,000 submissions. Reddit's API has some issues that we ran into, unfortunately, as it can only serve up 25-100 posts on every call, and they require that each request is spaced at least 2 seconds apart.</p>
              <h2>
              The Big Concern
              </h2>
              <p class="lead">We were hoping initially that we could take all of the entries for a given subreddit in the "top all" section, referring to the top scoring posts (where score=upvotesâˆ’downvotes) in the subreddit for all time. However, we discovered that Reddit's server will only deliver the top 1000 results for any section, which introduced some problems. Moreover, Reddit will not serve more than this limit on the website, making manually scraping the data not an option either. To overcome this, we scraped all the available posts in each section provided by the API, consisting of "top day", "top week", "top all", "hot", and "new". The algorithm used to classify hot posts was discussed in class, but new simply gives us the newest 1000 (or less) posts in the subreddit. This method of scraping from differenet was employed in order to try to see both the high and low scoring posts. However, ultimately, this method is admittedly imperfect, and the "correct" way to do this would be to examine new posts over a long (few months or longer) period of time, in order to do a time series of how the scores of each post changes.</p>
              <h2>
              Cleaning the data
              </h2>
              <p class="lead">After spending much time trying to perform some rudimentary data analysis and visualization, we noticed that we were getting numerous errors for unexplainable reasons. After trying to clean out offending elements from our dataset by hand, we ultimately decided that it would be best to systematically clean all data. Although the reddit API worked fine most of the time, sometimes titles were stored as data types other than strings and scores were not stored as ints or floats. We had to make sure to drop all offending posts from our dataset and convert the incorrect types to the appropriate ones before proceeding.</p>

              <p class="lead">Additionally, we did not want to look at moderator posts because they were not community-driven and did not behave in the same way as normal posts. They stood out among the others and were more successful by nature. There was no reason to let those posts influence our function.</p>
            </div>
          </div>
        </div>