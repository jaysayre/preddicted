{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import praw\n",
      "import pandas as pd\n",
      "import json\n",
      "import os\n",
      "import time\n",
      "import urllib\n",
      "import urllib2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 42
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")\n",
      "api_call_limit = 100 # Leave this at 100 or smaller\n",
      "r = praw.Reddit(user_agent=user_agent)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def subreddit_json_parser(data, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt):\n",
      "        for post in data['data']['children']:\n",
      "            if post['kind'] == 't3':\n",
      "                for key, value in post['data'].items():\n",
      "                    if key == \"title\":\n",
      "                        title.append(value)\n",
      "                    elif key == \"id\":\n",
      "                        ids.append(str(value))\n",
      "                    elif key == \"ups\":\n",
      "                        upvts.append(value)\n",
      "                    elif key == \"downs\":\n",
      "                        downvts.append(value)\n",
      "                    elif key == \"author\":\n",
      "                        authors.append(value)\n",
      "                    elif key == \"num_comments\":\n",
      "                        comments.append(value)\n",
      "                    elif key == \"score\":\n",
      "                        score.append(value)\n",
      "                    elif key == \"media\":\n",
      "                        media.append(value)\n",
      "                    elif key == \"distinguished\":\n",
      "                        distin.append(value)\n",
      "                    elif key == \"selftext\":\n",
      "                        selftxt.append(value)\n",
      "                    else:\n",
      "                        pass"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 45
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_subreddit_df(subreddit, sort_call, user_agent, api_call_limit, n):\n",
      "    #Builds a subreddit dataframe\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/%s.json?' % (subreddit, sort_call)\n",
      "    headers = {'User-agent': user_agent}\n",
      "    ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt = [], [], [], [], [], [], [], [], [], []\n",
      "    \n",
      "    if float(n)%float(api_call_limit) != 0.0:\n",
      "        num = float(n)/float(api_call_limit)\n",
      "        num = str(num).split('.')[0]\n",
      "        num = int(num) +1\n",
      "    else:\n",
      "        num = n/api_call_limit\n",
      "        \n",
      "    for i in range(num):\n",
      "        if i == 0:\n",
      "            post_params = {'limit': api_call_limit, 't' : 'all'} #'t' can be week, hour, day, year, etc.\n",
      "            form = urllib.urlencode(post_params)\n",
      "            reddit_url = reddit_base+form\n",
      "            request = urllib2.Request(reddit_url, headers=headers)\n",
      "            jsondata = json.loads(urllib2.urlopen(request).read())\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            print \"Downloaded %s posts...\" % len(ids)\n",
      "        else: \n",
      "            post_params = {'limit': api_call_limit, 't' : 'all', 'after': tostartfrom} #Indicates the post after we wish to call from\n",
      "            form = urllib.urlencode(post_params)\n",
      "            reddit_url = reddit_base+form\n",
      "            request = urllib2.Request(reddit_url, headers=headers)\n",
      "            jsondata = json.loads(urllib2.urlopen(request).read())\n",
      "            tostartfrom = jsondata['data']['after']\n",
      "            subreddit_json_parser(jsondata, ids, title, upvts, downvts, authors, comments, score, media, distin, selftxt)\n",
      "            print \"Downloaded %s posts...\" % len(ids)\n",
      "    \n",
      "    return pd.DataFrame({'id': ids, 'title': title, 'upvotes': upvts,\\\n",
      "         'downvts': downvts, 'comments': comments, 'score': score, 'author': authors}) #, 'karma':athrkarma})\n",
      "\n",
      "def add_karma(df):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "Not sure why this step takes so long, but unfortunately it does.\n",
      "    '''\n",
      "    authorkarma = []\n",
      "    for author in df['author']:\n",
      "        try:\n",
      "            redditauthor = r.get_redditor(author)\n",
      "            authorkarma.append(redditauthor.comment_karma)\n",
      "        except:\n",
      "            authorkarma.append(0)\n",
      "    df['karma'] = authorkarma\n",
      "    return df\n",
      "\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 53
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eli5top = get_subreddit_df('explainlikeimfive', 'top', user_agent, api_call_limit, 300)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "Downloaded 100 posts...\n",
        "Downloaded 200 posts..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "Downloaded 300 posts..."
       ]
      },
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Add karma if you want....\n",
      "eli5top = add_karma(eli5top)"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"n\" in the following function is the number of top level comments you want to get. By default, reddit only shows the first 200, you can expand it (look at the docs for praw). \n",
      "\n",
      "Another idea is to include comment karma per post or based on the subreddit instead of just total karma \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(postid, n=5):\n",
      "    submission = r.get_submission(submission_id=postid)\n",
      "    # Flattens comments and replies \n",
      "    #flat_comments = praw.helpers.flatten_tree(submission.comments) \n",
      "    comments, ids, scores, authors, authorkarma = [], [], [], [], []\n",
      "    \n",
      "    for i in range(n):\n",
      "        try:\n",
      "            #print ' \\n Comment: ASD\"\"' + submission.comments[i].body + '\"\"'\n",
      "            comments.append(submission.comments[i].body)\n",
      "            #print \"Submission Id:\", submission.comments[i].id\n",
      "            ids.append(submission.comments[i].id)\n",
      "            #print \"Score:\", submission.comments[i].score\n",
      "            scores.append(submission.comments[i].score)\n",
      "            #print \"Author:\", submission.comments[i].author\n",
      "            authors.append(submission.comments[i].author)\n",
      "            #This part can take a while... may be wiser to fetch in seperate step\n",
      "            #print \"Author Karma:\", submission.comments[i].author.comment_karma\n",
      "            authorkarma.append(submission.comments[i].author.comment_karma)\n",
      "        except:\n",
      "            pass\n",
      "    datadict = {'comment': comments, 'id': ids, 'score': scores, 'author': authors, 'authorkarma:': authorkarma}\n",
      "    return pd.DataFrame(datadict)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(eli5top['id'])\n",
      "    \n",
      "comments = get_comments(topids[2], 2)\n",
      "\n",
      "comment_texts = list(comments['comment'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comments.score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "0    2882\n",
        "1     254\n",
        "Name: score, dtype: int64"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The alchemy SDK sucks so I rewrote things. In my humble opinion, this is much cleaner. We may want to implement this as a class, but it doesn't really matter.\n",
      "\n",
      "ENTITITES refers to indivuduals mentioned in the text. RELEVANCE is the relevance in the context. sentiment is the impact the individual has on the meaning of the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "#apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alchemybase = \"http://access.alchemyapi.com/calls/text/\"\n",
      "textkeywords = \"TextGetRankedKeywords\"\n",
      "textcategory = \"TextGetCategory\"\n",
      "textconcepts = \"TextGetRankedConcepts\"\n",
      "textsentiment = \"TextGetTextSentiment\"\n",
      "textentities = \"TextGetRankedNamedEntities\"\n",
      "#texttargetedsent = \"TextGetTargetedSentiment\"\n",
      "\n",
      "postparams = {\n",
      "    'apikey' : apikey,\n",
      "    'text' : comment_texts,\n",
      "    'outputMode' : 'json',\n",
      "    #'maxRetrieve' : 8, #If you want to limit responses\n",
      "    'showSourceText' : 0, # I believe this turns it off IIRC\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_json(url):\n",
      "    jsonrsp = urllib2.urlopen(url)\n",
      "    data = json.load(jsonrsp)\n",
      "    return data\n",
      "\n",
      "def get_url(postparams, whichtoget, otherparams=None):\n",
      "    form = urllib.urlencode(postparams)\n",
      "    keywordsurl = alchemybase + whichtoget + \"?\" + form\n",
      "    if otherparams != None:\n",
      "        otherform = urllib.urlencode(postparams)\n",
      "        keywordsurl = alchemybase + whichtoget + \"?\" + form + otherform \n",
      "    data = get_json(keywordsurl)\n",
      "    return data\n",
      "        \n",
      "def text_concepts(data):\n",
      "    for item in data['concepts']: \n",
      "        print item['dbpedia']\n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        \n",
      "def text_category(data):\n",
      "    print data['category']\n",
      "    print data['language']\n",
      "    print data['score']\n",
      "    print data['status']\n",
      "    \n",
      "def text_keywords(data):\n",
      "    for item in data['keywords']: \n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        \n",
      "def text_sentiment(data):\n",
      "    print data['docSentiment']['mixed']\n",
      "    print data['docSentiment']['score']\n",
      "    print data['docSentiment']['type']\n",
      "\n",
      "def text_entities(data):\n",
      "    for item in data['entities']:\n",
      "        print item['count']\n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        print item['type']\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#text_concepts(get_url(postparams, textconcepts, {'linkedData': 1}))\n",
      "#text_keywords(get_url(postparams, textkeywords, {'keywordExtractMode' : 'strict'})) \n",
      "#text_category(get_url(postparams, textcategory))\n",
      "#text_sentiment(get_url(postparams, textsentiment))\n",
      "#text_entities(get_url(postparams, textentities))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 35
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}