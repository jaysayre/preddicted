{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import praw\n",
      "import pandas as pd\n",
      "#from __future__ import print_function\n",
      "from alchemyapi import AlchemyAPI\n",
      "import json\n",
      "import os\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for class 0.01\" #Should probably link later\n",
      "              \"/u/Valedra\"\n",
      "              \"https://github.com/jaysayre/intelligentdolphins\") # Reddit API encourages having a descriptive name, preferably with /u/..\n",
      "r = praw.Reddit(user_agent=user_agent)\n",
      "api_call_limit = 10 # Reddit API limits to 1000\n",
      "subreddit = r.get_subreddit('explainlikeimfive', fetch=True)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 2
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The API is limited to 1000 calls at a time. We need to figure out how to get the \"next\" 1000."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "\n",
      "sub = subreddit.get_top_from_all(limit=api_call_limit)\n",
      "\n",
      "# Fancy list comprehensions don't seem to work here, unfortunately, without recalling line above\n",
      "ids = []\n",
      "title = []\n",
      "upvts = []\n",
      "downvts = []\n",
      "authors = []\n",
      "athrkarma = []\n",
      "comments = []\n",
      "score = []\n",
      "for post in sub:\n",
      "    if post.distinguished == None: #Removes moderator comments, which we presumably don't want.\n",
      "        ids.append(post.id)\n",
      "        title.append(post.title)\n",
      "        upvts.append(post.ups)\n",
      "        downvts.append(post.downs)\n",
      "        authors.append(str(post.author))\n",
      "        comments.append(post.num_comments)\n",
      "        score.append(post.score)\n",
      "        #This part can take a while... may be wiser to fetch in seperate step\n",
      "        #try:\n",
      "        #    athrkarma.append(post.author.comment_karma)\n",
      "        #except AttributeError:\n",
      "        #    athrkarma.append(0)\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 3
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The following code will create the dataframe right away but it is way slower. Pandas is not made for adding rows one by one that's why we should use the code above"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "eli5top2 = pd.DataFrame(columns=('id', 'post_title', 'upvotes', 'downvts', 'comments', 'score', 'author'),)\n",
      "#eli5top2.setIndex('id')\n",
      "sub = subreddit.get_top_from_all(limit=api_call_limit)\n",
      "\n",
      "for post in sub:\n",
      "    if post.distinguished == None:\n",
      "        inputdict = dict()\n",
      "        inputdict['id'] = post.id\n",
      "        inputdict['post_title'] = post.title\n",
      "        inputdict['upvotes'] = post.ups\n",
      "        inputdict['downvts'] = post.downs\n",
      "        inputdict['comments'] = post.author\n",
      "        inputdict['score'] = post.num_comments\n",
      "        inputdict['author'] = post.score\n",
      "        eli5top2 = eli5top2.append(pd.Series(inputdict), ignore_index=True)\n",
      "'''"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 38,
       "text": [
        "\"\\neli5top2 = pd.DataFrame(columns=('id', 'post_title', 'upvotes', 'downvts', 'comments', 'score', 'author'),)\\n#eli5top2.setIndex('id')\\nsub = subreddit.get_top_from_all(limit=api_call_limit)\\n\\nfor post in sub:\\n    if post.distinguished == None:\\n        inputdict = dict()\\n        inputdict['id'] = post.id\\n        inputdict['post_title'] = post.title\\n        inputdict['upvotes'] = post.ups\\n        inputdict['downvts'] = post.downs\\n        inputdict['comments'] = post.author\\n        inputdict['score'] = post.num_comments\\n        inputdict['author'] = post.score\\n        #eli5top2 = eli5top2.append(pd.Series(inputdict), ignore_index=True)\\n        eli5top2 = pd.merge(eli5top2, pd.DataFrame(inputdict))\\n        #eli5top2.loc[post.id] = pd.Series(inputdict)\\n\""
       ]
      }
     ],
     "prompt_number": 38
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eli5top = pd.DataFrame({'id': ids, 'post_title': title, 'upvotes': upvts, 'downvts':downvts, 'comments':comments, 'score':score, 'authors':authors})#, 'karma':athrkarma})#"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"n\" in the following function is the number of top level comments you want to get. By default, reddit only shows the first 200, you can expand it (look at the docs for praw). \n",
      "\n",
      "Another idea is to include comment karma per post or based on the subreddit instead of just total karma \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(postid, n=5):\n",
      "    submission = r.get_submission(submission_id=postid)\n",
      "    # Flattens comments and replies \n",
      "    #flat_comments = praw.helpers.flatten_tree(submission.comments) \n",
      "    comments = []\n",
      "    ids = []\n",
      "    scores = []\n",
      "    authors = []\n",
      "    authorkarma = []\n",
      "    \n",
      "    for i in range(0, n):\n",
      "        try:\n",
      "            #print ' \\n Comment: \"\"' + submission.comments[i].body + '\"\"'\n",
      "            comments.append(submission.comments[i].body)\n",
      "            #print \"Submission Id:\", submission.comments[i].id\n",
      "            ids.append(submission.comments[i].id)\n",
      "            #print \"Score:\", submission.comments[i].score\n",
      "            scores.append(submission.comments[i].score)\n",
      "            #print \"Author:\", submission.comments[i].author\n",
      "            authors.append(submission.comments[i].author)\n",
      "            #This part can take a while... may be wiser to fetch in seperate step\n",
      "            #print \"Author Karma:\", submission.comments[i].author.comment_karma\n",
      "            authorkarma.append(submission.comments[i].author.comment_karma)\n",
      "        except:\n",
      "            pass\n",
      "    datadict = {'comment': comments, 'id': ids, 'score': scores, 'author': authors, 'authorkarma:': authorkarma}\n",
      "    return pd.DataFrame(datadict)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 48
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(eli5top['id'])\n",
      "\n",
      "#for i in range(len(topids)):\n",
      "#    topids[i] = str(topids[i])\n",
      "    \n",
      "comment = get_comments(topids[2], 2)\n",
      "\n",
      "comment_texts = list(comment['comment'])\n",
      "#the alchemyAPI works with lists as input. We can use the line above to generate the input for the following cells"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 55
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We currently have to write api_key.txt in order to use the sdk of alchemyAPI. We may want to get rid of this in the future and just their API and get the results as json (not necessary but we need to include their code otherwise)\n",
      "\n",
      "\n",
      "ENTITITES refers to indivuduals mentioned in the text. RELEVANCE is the relevance in the context. sentiment is the impact the individual has on the meaning of the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "#apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "with open('api_key.txt', 'w') as keytxt:\n",
      "    keytxt.write(apikey)\n",
      "    \n",
      "# Create the AlchemyAPI object \n",
      "alchemyapi = AlchemyAPI()\n",
      "\n",
      "response = alchemyapi.entities('text', comment_texts, { 'sentiment':1 })\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "    #Entity Extraction\n",
      "\tprint('## Entities ##')\n",
      "\tfor entity in response['entities']:\n",
      "\t\tprint('text: ', entity['text'].encode('utf-8'))\n",
      "\t\tprint('type: ', entity['type'])\n",
      "\t\tprint('relevance: ', entity['relevance'])\n",
      "\t\tprint('sentiment: ', entity['sentiment']['type'])\n",
      "\t\tif 'score' in entity['sentiment']:\n",
      "\t\t\tprint('sentiment score: ' + entity['sentiment']['score']) \n",
      "else:\n",
      "\tprint('Error in entity extraction call: ', response['statusInfo'])\n",
      "    \n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "## Entities ##\n",
        "('text: ', 'palpatine')\n",
        "('type: ', u'Person')\n",
        "('relevance: ', u'0.772481')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.25927\n",
        "('text: ', 'DR Survival')\n",
        "('type: ', u'Person')\n",
        "('relevance: ', u'0.592679')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.177819\n",
        "('text: ', 'Bane')\n",
        "('type: ', u'Person')\n",
        "('relevance: ', u'0.446542')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.161501\n"
       ]
      }
     ],
     "prompt_number": 54
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Keyword Extraction\n",
      "\n",
      "response = alchemyapi.keywords('text',comment_texts, { 'sentiment':1 })\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "\tprint('## Keywords ##')\n",
      "\tfor keyword in response['keywords']:\n",
      "\t\tprint('text: ', keyword['text'].encode('utf-8'))\n",
      "\t\tprint('relevance: ', keyword['relevance'])\n",
      "\t\tprint('sentiment: ', keyword['sentiment']['type']) \n",
      "\t\tif 'score' in keyword['sentiment']:\n",
      "\t\t\tprint('sentiment score: ' + keyword['sentiment']['score'])\n",
      "\n",
      "else:\n",
      "\tprint('Error in keyword extaction call: ', response['statusInfo'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "## Keywords ##\n",
        "('text: ', 'sith')\n",
        "('relevance: ', u'0.993939')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.0413518\n",
        "('text: ', 'apprentice strong eneugh')\n",
        "('relevance: ', u'0.887739')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.0902359\n",
        "('text: ', 'darth bane trilogy')\n",
        "('relevance: ', u'0.862751')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.00819054\n",
        "('text: ', 'sith fighting')\n",
        "('relevance: ', u'0.861336')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.13595\n",
        "('text: ', 'Sith lord')\n",
        "('relevance: ', u'0.857081')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.0965022\n",
        "('text: ', 'Sith master')\n",
        "('relevance: ', u'0.831508')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.0161541\n",
        "('text: ', 'lesser weak people')\n",
        "('relevance: ', u'0.817885')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.169405\n",
        "('text: ', 'huge array')\n",
        "('relevance: ', u'0.635137')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.0845457\n",
        "('text: ', 'dark side.')\n",
        "('relevance: ', u'0.633508')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.240285\n",
        "('text: ', 'true Sith.\\\\nBecause')\n",
        "('relevance: ', u'0.63187')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.21023\n",
        "('text: ', 'jedi stagnate')\n",
        "('relevance: ', u'0.627358')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.258307\n",
        "('text: ', 'various times')\n",
        "('relevance: ', u'0.620097')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.130498\n",
        "('text: ', 'power base')\n",
        "('relevance: ', u'0.616819')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.455828\n",
        "('text: ', 'DR Survival')\n",
        "('relevance: ', u'0.599605')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.0798678\n",
        "('text: ', '\\\\n\\\\nA Master')\n",
        "('relevance: ', u'0.578214')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.15585\n",
        "('text: ', 'blade')\n",
        "('relevance: ', u'0.566333')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.25687\n",
        "('text: ', 'palpatine')\n",
        "('relevance: ', u'0.536044')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.101799\n",
        "('text: ', 'way')\n",
        "('relevance: ', u'0.494137')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.568849\n",
        "('text: ', 'knowledge')\n",
        "('relevance: ', u'0.492889')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.145057\n",
        "('text: ', 'existance')\n",
        "('relevance: ', u'0.46629')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.0228855\n",
        "('text: ', 'starters')\n",
        "('relevance: ', u'0.453983')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.617006\n",
        "('text: ', 'immortality')\n",
        "('relevance: ', u'0.452285')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.00729056\n",
        "('text: ', 'cautions')\n",
        "('relevance: ', u'0.451786')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.296049\n",
        "('text: ', 'patience')\n",
        "('relevance: ', u'0.447296')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.250007\n",
        "('text: ', 'books')\n",
        "('relevance: ', u'0.44637')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.200201\n",
        "('text: ', 'nature')\n",
        "('relevance: ', u'0.445715')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.208223\n",
        "('text: ', 'rule')\n",
        "('relevance: ', u'0.444933')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.442223\n",
        "('text: ', 'throat')\n",
        "('relevance: ', u'0.443932')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.213577\n",
        "('text: ', 'successor')\n",
        "('relevance: ', u'0.443567')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.389027\n",
        "('text: ', 'concept')\n",
        "('relevance: ', u'0.443391')\n",
        "('sentiment: ', u'positive')\n",
        "sentiment score: 0.339643\n",
        "('text: ', 'death')\n",
        "('relevance: ', u'0.443149')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.146666\n",
        "('text: ', 'hungrier')\n",
        "('relevance: ', u'0.442003')\n",
        "('sentiment: ', u'negative')\n",
        "sentiment score: -0.17655\n"
       ]
      }
     ],
     "prompt_number": 56
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Concept Tagging\n",
      "\n",
      "response = alchemyapi.concepts('text',comment_texts)\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "\tprint('\\n## Concepts ##')\n",
      "\tfor concept in response['concepts']:\n",
      "\t\tprint('text: ', concept['text'])\n",
      "\t\tprint('relevance: ', concept['relevance'])\n",
      "else:\n",
      "\tprint('Error in concept tagging call: ', response['statusInfo'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "## Concepts ##\n",
        "('text: ', u'Sith')\n",
        "('relevance: ', u'0.986381')\n",
        "('text: ', u'Jedi')\n",
        "('relevance: ', u'0.672537')\n",
        "('text: ', u'Darth Vader')\n",
        "('relevance: ', u'0.664654')\n",
        "('text: ', u'Palpatine')\n",
        "('relevance: ', u'0.632988')\n",
        "('text: ', u'Force')\n",
        "('relevance: ', u'0.631518')\n",
        "('text: ', u'Anakin Skywalker')\n",
        "('relevance: ', u'0.60888')\n",
        "('text: ', u'Star Wars Episode I: The Phantom Menace')\n",
        "('relevance: ', u'0.586302')\n",
        "('text: ', u'Star Wars')\n",
        "('relevance: ', u'0.583846')\n"
       ]
      }
     ],
     "prompt_number": 57
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Relation Extraction\n",
      "\n",
      "response = alchemyapi.relations('text', comment_texts)\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "\tprint('\\n ## Relations ##')\n",
      "\tfor relation in response['relations']:\n",
      "\t\tif 'subject' in relation:\n",
      "\t\t\tprint('Subject: ', relation['subject']['text'].encode('utf-8'))\n",
      "\t\tif 'action' in relation:\n",
      "\t\t\tprint('Action: ', relation['action']['text'].encode('utf-8'))\n",
      "\t\tif 'object' in relation:\n",
      "\t\t\tprint('Object: ', relation['object']['text'].encode('utf-8'))\n",
      "\t\t\n",
      "else:\n",
      "\tprint('Error in relation extaction call: ', response['statusInfo'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " ## Relations ##\n",
        "('Subject: ', \"[u'The darth bane trilogy of books\")\n",
        "('Action: ', 'explains')\n",
        "('Object: ', 'the concept of the rule of two very well,\\\\nfor starters there are only two sith because that way the power base is focused, there is not a huge array of sith fighting each other for power, as is the nature of the dark side. and so it is harder for the jedi to confirm their existance, thus ensuring the continued survival of the true Sith.\\\\nBecause there are only two sith, a Sith lord MUST take an apprentice, for the power and knowledge of the sith must not be allowed to die out')\n",
        "('Subject: ', 'the power base')\n",
        "('Action: ', 'is focused')\n",
        "('Object: ', 'as is the nature of the dark side. and so it is harder for the jedi to confirm their existance')\n",
        "('Subject: ', 'not a huge array')\n",
        "('Action: ', 'is')\n",
        "('Object: ', 'of sith fighting each other for power')\n",
        "('Subject: ', '\\\\nfor starters')\n",
        "('Action: ', 'ensuring')\n",
        "('Object: ', 'the continued survival of the true Sith.\\\\nBecause there')\n",
        "('Subject: ', 'he')\n",
        "('Action: ', 'killed')\n",
        "('Object: ', 'his master')\n",
        "('Subject: ', 'it')\n",
        "('Action: ', 'means')\n",
        "('Object: ', 'he has grown weak, while his apprentice is strong, in this way the Sith ALWAYS grow stronger, while the jedi stagnate')\n",
        "('Subject: ', 'the Sith')\n",
        "('Action: ', 'grow')\n",
        "('Object: ', 'stronger')\n",
        "('Subject: ', 'lesser weak people who should not be allowed to survive.\\\\n\\\\nthere')\n",
        "('Action: ', 'are')\n",
        "('Object: ', 'Sith who strive for immortality')\n",
        "('Subject: ', 'Sith')\n",
        "('Action: ', 'strive')\n",
        "('Object: ', 'for immortality')\n",
        "('Subject: ', 'they')\n",
        "('Action: ', 'find')\n",
        "('Object: ', 'an apprentice strong eneugh to succeed them')\n",
        "('Subject: ', 'palpatine')\n",
        "('Action: ', 'has')\n",
        "('Object: ', 'tried various times')\n",
        "('Subject: ', 'palpatine')\n",
        "('Action: ', 'has tried')\n",
        "('Object: ', 'various times')\n",
        "('Subject: ', 'by Bane')\n",
        "('Action: ', 'was instituted')\n",
        "('Object: ', 'two')\n",
        "('Subject: ', 'u')\n",
        "('Action: ', 'Choose')\n",
        "('Object: ', 'someone')\n",
        "('Subject: ', 'you')\n",
        "('Action: ', 'be')\n",
        "('Object: ', 'succeeded')\n",
        "('Subject: ', 'you')\n",
        "('Action: ', 'dodge')\n",
        "('Object: ', 'the blade')\n",
        "('Subject: ', 'you')\n",
        "('Action: ', 'block')\n",
        "('Object: ', 'the blade')\n",
        "('Subject: ', 'you')\n",
        "('Action: ', 'hold')\n",
        "('Object: ', 'the blade')\n",
        "('Subject: ', 'the blade')\n",
        "('Action: ', 'kills')\n",
        "('Object: ', 'you')\n",
        "('Subject: ', 'you')\n",
        "('Action: ', 'know')\n",
        "('Object: ', 'your end')\n",
        "('Subject: ', 'an apprentice')\n",
        "('Action: ', 'is')\n",
        "('Object: ', 'essential')\n",
        "('Subject: ', '\\\\n\\\\nA Master without an apprentice')\n",
        "('Action: ', 'is')\n",
        "('Object: ', 'a Master of nothing.\\\\n\\\\n-Darth Sidious\"]')\n"
       ]
      }
     ],
     "prompt_number": 58
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Text Categorization\n",
      "response = alchemyapi.category('text', comment_texts)\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "\tprint('\\n ## Category ##')\n",
      "\tprint('text: ', response['category'])\n",
      "\tprint('score: ', response['score'])\n",
      "else:\n",
      "\tprint('Error in text categorization call: ', response['statusInfo'])\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        " ## Category ##\n",
        "('text: ', u'arts_entertainment')\n",
        "('score: ', u'0.549265')\n"
       ]
      }
     ],
     "prompt_number": 59
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Language Detection\n",
      "response = alchemyapi.language('text',comment_texts)\n",
      "\n",
      "if response['status'] == 'OK':\n",
      "\tprint('\\n## Language ##')\n",
      "\tprint('language: ', response['language'])\n",
      "else:\n",
      "\tprint('Error in language detection call: ', response['statusInfo'])"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "\n",
        "## Language ##\n",
        "('language: ', u'english')\n"
       ]
      }
     ],
     "prompt_number": 60
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Removes api key so we all aren't pushing different keys\n",
      "os.remove('api_key.txt')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 61
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}