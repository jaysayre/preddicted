{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import praw\n",
      "import pandas as pd\n",
      "#from __future__ import print_function\n",
      "from alchemyapi import AlchemyAPI\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 7
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "user_agent = (\"Project for class v1.0\" #Should probably link later\n",
      "              \"/u/Valedra\"\n",
      "              \"https://github.com/jaysayre/intelligentdolphins\") # Reddit API encourages having a descriptive name, preferably with /u/..\n",
      "r = praw.Reddit(user_agent=user_agent)\n",
      "api_call_limit = 1000 # Reddit API limits to 1000"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 8
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The API is limited to 1000 calls at a time. We need to figure out how to get the \"next\" 1000."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "'''\n",
      "Function to get the DataFrame of a subreddit\n",
      "subreddit : str - title of subreddit\n",
      "n : int - nuber of rows required in DF\n",
      "'''\n",
      "def get_subreddit_df(subreddit, n=None, limit=100):\n",
      "    ids, title, upvts, downvts, authors, athrkarma, comments, score = [], [], [], [], [], [], [], []\n",
      "    subreddit = r.get_subreddit('explainlikeimfive', fetch=True)\n",
      "    sub = subreddit.get_top_from_all(limit=n)\n",
      "    calltracker = 0\n",
      "    for post in sub:\n",
      "        if calltracker < limit:\n",
      "            if post.distinguished == None: #Removes moderator comments, which we presumably don't want.\n",
      "                ids.append(post.id)\n",
      "                title.append(post.title)\n",
      "                upvts.append(post.ups)\n",
      "                downvts.append(post.downs)\n",
      "                authors.append(str(post.author))\n",
      "                comments.append(post.num_comments)\n",
      "                score.append(post.score)\n",
      "                calltracker += 1\n",
      "                #This part can take a while... may be wiser to fetch in seperate step\n",
      "                #try:\n",
      "                #    athrkarma.append(post.author.comment_karma)\n",
      "                #except AttributeError:\n",
      "                #    athrkarma.append(0)          \n",
      "    d = pd.DataFrame({'id': ids, 'post_title': title, 'upvotes': upvts,\\\n",
      "         'downvts': downvts, 'comments': comments, 'score': score, 'author': authors}) #, 'karma':athrkarma})\n",
      "    return d"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 9
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eli5top = get_subreddit_df('explainlikeimfive')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "eli5top"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "html": [
        "<pre>\n",
        "&ltclass 'pandas.core.frame.DataFrame'&gt\n",
        "Int64Index: 100 entries, 0 to 99\n",
        "Data columns (total 7 columns):\n",
        "author        100  non-null values\n",
        "comments      100  non-null values\n",
        "downvts       100  non-null values\n",
        "id            100  non-null values\n",
        "post_title    100  non-null values\n",
        "score         100  non-null values\n",
        "upvotes       100  non-null values\n",
        "dtypes: int64(4), object(3)\n",
        "</pre>"
       ],
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 11,
       "text": [
        "<class 'pandas.core.frame.DataFrame'>\n",
        "Int64Index: 100 entries, 0 to 99\n",
        "Data columns (total 7 columns):\n",
        "author        100  non-null values\n",
        "comments      100  non-null values\n",
        "downvts       100  non-null values\n",
        "id            100  non-null values\n",
        "post_title    100  non-null values\n",
        "score         100  non-null values\n",
        "upvotes       100  non-null values\n",
        "dtypes: int64(4), object(3)"
       ]
      }
     ],
     "prompt_number": 11
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The \"n\" in the following function is the number of top level comments you want to get. By default, reddit only shows the first 200, you can expand it (look at the docs for praw). \n",
      "\n",
      "Another idea is to include comment karma per post or based on the subreddit instead of just total karma \n"
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(postid, n=5):\n",
      "    submission = r.get_submission(submission_id=postid)\n",
      "    # Flattens comments and replies \n",
      "    #flat_comments = praw.helpers.flatten_tree(submission.comments) \n",
      "    comments, ids, scores, authors, authorkarma = [], [], [], [], []\n",
      "    \n",
      "    for i in range(n):\n",
      "        try:\n",
      "            #print ' \\n Comment: ASD\"\"' + submission.comments[i].body + '\"\"'\n",
      "            comments.append(submission.comments[i].body)\n",
      "            #print \"Submission Id:\", submission.comments[i].id\n",
      "            ids.append(submission.comments[i].id)\n",
      "            #print \"Score:\", submission.comments[i].score\n",
      "            scores.append(submission.comments[i].score)\n",
      "            #print \"Author:\", submission.comments[i].author\n",
      "            authors.append(submission.comments[i].author)\n",
      "            #This part can take a while... may be wiser to fetch in seperate step\n",
      "            #print \"Author Karma:\", submission.comments[i].author.comment_karma\n",
      "            authorkarma.append(submission.comments[i].author.comment_karma)\n",
      "        except:\n",
      "            pass\n",
      "    datadict = {'comment': comments, 'id': ids, 'score': scores, 'author': authors, 'authorkarma:': authorkarma}\n",
      "    return pd.DataFrame(datadict)\n",
      "        "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 12
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "topids = list(eli5top['id'])\n",
      "\n",
      "#for i in range(len(topids)):\n",
      "#    topids[i] = str(topids[i])\n",
      "    \n",
      "comments = get_comments(topids[2], 2)\n",
      "\n",
      "comment_texts = list(comments['comment'])\n",
      "#the alchemyAPI works with lists as input. We can use the line above to generate the input for the following cells"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 13
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "comments.score"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 32,
       "text": [
        "0    2882\n",
        "1     254\n",
        "Name: score, dtype: int64"
       ]
      }
     ],
     "prompt_number": 32
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "The alchemy SDK sucks so I rewrote things. In my humble opinion, this is much cleaner. We may want to implement this as a class, but it doesn't really matter.\n",
      "\n",
      "ENTITITES refers to indivuduals mentioned in the text. RELEVANCE is the relevance in the context. sentiment is the impact the individual has on the meaning of the text."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "# Make sure to use your own API key\n",
      "#apikey = \"dcac82649daaa2627ee783b25779cfaed4af0067\" #Jay's key\n",
      "apikey = \"e945cef59338f9e8e7bc962badde170e623fb7e5\" #Basti's key\n",
      "#apikey = \"cb736ca44e57cd6764b70ec86886f4fce8f6a68d\" #Serguei's Key"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 105
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "alchemybase = \"http://access.alchemyapi.com/calls/text/\"\n",
      "textkeywords = \"TextGetRankedKeywords\"\n",
      "textcategory = \"TextGetCategory\"\n",
      "textconcepts = \"TextGetRankedConcepts\"\n",
      "textsentiment = \"TextGetTextSentiment\"\n",
      "textentities = \"TextGetRankedNamedEntities\"\n",
      "#texttargetedsent = \"TextGetTargetedSentiment\"\n",
      "\n",
      "postparams = {\n",
      "    'apikey' : apikey,\n",
      "    'text' : comment_texts,\n",
      "    'outputMode' : 'json',\n",
      "    #'maxRetrieve' : 8, #If you want to limit responses\n",
      "    'showSourceText' : 0,\n",
      "}"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 106
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_json(url):\n",
      "    jsonrsp = urllib2.urlopen(url)\n",
      "    data = json.load(jsonrsp)\n",
      "    return data\n",
      "\n",
      "def get_url(postparams, whichtoget, otherparams=None):\n",
      "    form = urllib.urlencode(postparams)\n",
      "    keywordsurl = alchemybase + whichtoget + \"?\" + form\n",
      "    if otherparams != None:\n",
      "        otherform = urllib.urlencode(postparams)\n",
      "        keywordsurl = alchemybase + whichtoget + \"?\" + form + otherform \n",
      "    data = get_json(keywordsurl)\n",
      "    return data\n",
      "        \n",
      "def text_concepts(data):\n",
      "    for item in data['concepts']: \n",
      "        print item['dbpedia']\n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        \n",
      "def text_category(data):\n",
      "    print data['category']\n",
      "    print data['language']\n",
      "    print data['score']\n",
      "    print data['status']\n",
      "    \n",
      "def text_keywords(data):\n",
      "    for item in data['keywords']: \n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        \n",
      "def text_sentiment(data):\n",
      "    print data['docSentiment']['mixed']\n",
      "    print data['docSentiment']['score']\n",
      "    print data['docSentiment']['type']\n",
      "\n",
      "def text_entities(data):\n",
      "    for item in data['entities']:\n",
      "        print item['count']\n",
      "        print item['relevance']\n",
      "        print item['text']\n",
      "        print item['type']\n",
      "    "
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 107
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#text_concepts(get_url(postparams, textconcepts, {'linkedData': 1}))\n",
      "#text_keywords(get_url(postparams, textkeywords, {'keywordExtractMode' : 'strict'})) \n",
      "text_category(get_url(postparams, textcategory))\n",
      "#text_sentiment(get_url(postparams, textsentiment))\n",
      "#text_entities(get_url(postparams, textentities))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "arts_entertainment\n",
        "english\n",
        "0.549265\n",
        "OK\n"
       ]
      }
     ],
     "prompt_number": 110
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}