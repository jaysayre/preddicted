{
 "metadata": {
  "name": ""
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "import time\n",
      "import pandas as pd\n",
      "import numpy as np\n",
      "import json\n",
      "import os\n",
      "import urllib\n",
      "import urllib2\n",
      "import datetime\n",
      "\n",
      "\n",
      "user_agent = (\"Project for Data Science class v1.0\" \" /u/Valedra\" \" https://github.com/jaysayre/intelligentdolphins\")"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've downloaded all of the data we need using Redditscraping.ipynb, we now need to assemble it and download other pieces of information. First, lets combine all of the data frames."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "file_dir = \"Data/New/\"\n",
      "\n",
      "path, dirs, files = os.walk(file_dir).next()\n",
      "csvfiles = [file_dir + i for i in files if \".csv\" in i ] #Builds a list with .csv files\n",
      "csvfiles.sort()\n",
      "\n",
      "csvfiles"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 2,
       "text": [
        "['Data/big.csv']"
       ]
      }
     ],
     "prompt_number": 2
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def filemerge(csvfiles):\n",
      "    if len(csvfiles) >= 2:\n",
      "        df = pd.DataFrame()\n",
      "        for csvfile in csvfiles:\n",
      "            dfnew = pd.read_csv(csvfile, encoding='utf-8')\n",
      "            df = df.append(dfnew)\n",
      "        return df\n",
      "    else:\n",
      "        print 'Not enough files'"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#filemerge(csvfiles).to_csv('Data/combined.csv', index=False, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Next, let's add something here that will download a seperate dataframe containing the total karma each poster in our new, large data frame has."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def add_karma(df, user_agent, display=20, start=0, end=len(df)):\n",
      "    '''\n",
      "Adds in the karma score for every author, and returns the same df except with the new information.\n",
      "This part takes a bit longer, as we have to make a get request for every user.\n",
      "\n",
      "Parameters --\n",
      "    df: input dataframe\n",
      "    user_agent: same as before\n",
      "    display: How often one wants status updates on the download progress\n",
      "    \n",
      "Returns --\n",
      "    A pandas dataframe with the same information, in addition to overall and link karma for the post author\n",
      "    '''\n",
      "    \n",
      "    count = 0\n",
      "    dfidlist = list(df.index)\n",
      "    dfidlist = dfidlist[start : end]\n",
      "    df2 = pd.DataFrame({'karma':df['karma'], 'link_karma':df['link_karma'], 'author':df['author']})\n",
      "    for i in dfidlist:\n",
      "        try:\n",
      "            reddit_url = 'http://www.reddit.com/user/%s/about.json' % df['author'][i]\n",
      "            headers = {'User-agent': user_agent}\n",
      "            df2['author'][i] = df['author'][i]\n",
      "            jsondata = json_extract(reddit_url, headers)\n",
      "            df2['karma'][i] = jsondata['data']['comment_karma']\n",
      "            try:\n",
      "                df2['link_karma'][i] = jsondata['data']['link_karma']\n",
      "            except:\n",
      "                df2['link_karma'][i] = 0\n",
      "            count += 1\n",
      "        except:\n",
      "            df2['karma'][i] = 0\n",
      "            df2['link_karma'][i] = 0\n",
      "            count += 1\n",
      "        if count%int(display) == 0:\n",
      "            print \"Retrieved karma for %s users\" % count          \n",
      "    return df2\n",
      "\n",
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 6
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "#Add in karma... Be careful as this is really quite slow\n",
      "df = pd.read_csv('Data/combined.csv', encoding='utf-8')\n",
      "df2 = add_karma(df, user_agent, display=40) #Change the start and end\n",
      "df2.to_csv('Data/karma.csv', index=False, encoding='utf-8')\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 1
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we've downloaded the karma information, we can now merge it into our original data set."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "fulldf = df.merge(df2, left_on='author', right_on='author', how='inner')\n",
      "fulldf = fulldf.drop_duplicates() #For some reason we need to recheck for duplicates\n",
      "fulldf.to_csv('Data/full.csv', index=False, encoding='utf-8')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Finally, we need to download one more thing... all of the comments related to a given post! Well, not all, but just the top comments for a given post."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def get_comments(subreddit, postid, sort_call, subtype, user_agent):\n",
      "    '''\n",
      "    Parameters --\n",
      "    subreddit: subreddit title\n",
      "    postid: 6 digit id corresponding to the post\n",
      "    sort_call: one of confidence, top, new, hot, controversial, old, random\n",
      "    user_agent: same as before\n",
      "    \n",
      "    Returns --\n",
      "    '''\n",
      "    reddit_base = 'http://www.reddit.com/r/%s/comments/%s.json?' % (subreddit, postid) \n",
      "    headers = {'User-agent': user_agent}\n",
      "    post_params = {'sort': sort_call}\n",
      "    jsondata = json_extract(reddit_base, headers, post_params)\n",
      "    comments, ids, ups, downs, authors, distin = [], [], [], [], [], []\n",
      "    for item in jsondata[1]['data']['children']:\n",
      "        for key, value in item['data'].items():\n",
      "            if key == \"author\":\n",
      "                if value == None:\n",
      "                    authors.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    authors.append('null')\n",
      "                else:\n",
      "                    authors.append(value)\n",
      "                    \n",
      "            elif key == \"id\":\n",
      "                if value == None:\n",
      "                    ids.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ids.append('null')\n",
      "                else:\n",
      "                    ids.append(str(value))\n",
      "            elif key == \"body\":\n",
      "                if value == None:\n",
      "                    comments.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    comments.append('null')\n",
      "                else:\n",
      "                    comments.append(value)#.replace('\\n', ''))\n",
      "            elif key == \"ups\":\n",
      "                if value == None:\n",
      "                    ups.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    ups.append('null')\n",
      "                else:\n",
      "                    ups.append(value)\n",
      "            elif key == \"downs\":\n",
      "                if value == None:\n",
      "                    downs.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    downs.append('null')\n",
      "                else:\n",
      "                    downs.append(value)\n",
      "            elif key == \"distinguished\":\n",
      "                if value == None:\n",
      "                    distin.append('null')\n",
      "                elif value == '[deleted]':\n",
      "                    distin.append('null')\n",
      "                else:\n",
      "                    distin.append(value)\n",
      "            else:\n",
      "                pass\n",
      "    \n",
      "    try:\n",
      "        return pd.DataFrame({'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, \n",
      "                    'distinguished': distin, 'subreddit': subreddit, 'post': postid, 'type': subtype})\n",
      "    except:\n",
      "        #print len(comments), len(ids), len(ups), len(downs), len(authors)\n",
      "        ids.pop(0) #Might need to be more formulaic\n",
      "        return pd.DataFrame({'comment': comments, 'id': ids, 'ups': ups, 'downs': downs, 'author': authors, \n",
      "                    'distinguished': distin, 'subreddit': subreddit, 'post': postid, 'type': subtype})\n",
      "    \n",
      "\n",
      "def json_extract(baseurl, headrs=None, params=None, extraparam=None):\n",
      "    '''\n",
      "    Helper function to download and read json data. Takes in explanatory headers and returns json dict.\n",
      "    '''\n",
      "    if params != None:\n",
      "        if extraparam != None:\n",
      "                params['t'] = extraparam\n",
      "        form = urllib.urlencode(params)\n",
      "        url = baseurl+form\n",
      "    else:\n",
      "        url = baseurl\n",
      "    if headrs != None:\n",
      "        request = urllib2.Request(url, headers=headrs)\n",
      "    else: \n",
      "        request = urllib2.Request(url)\n",
      "    return json.loads(urllib2.urlopen(request).read())\n"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "bigdf = pd.read_csv(df, encoding='utf-8')\n",
      "subs = list(bigdf['subreddit'].unique())[:-1] #Gets rid of NaN\n",
      "types = list(bigdf['type'].unique())"
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "This will download a seperate dataframe with the comments for each subreddit type. It will take a while."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "for sub in subs:\n",
      "    print sub\n",
      "    for typ in types:\n",
      "        print typ\n",
      "        df = bigdf[bigdf['subreddit'] == sub]\n",
      "        df = df[df['type'] == typ]\n",
      "        dfidlist = list(df.index)\n",
      "        comments = pd.DataFrame()\n",
      "        for i in dfidlist:\n",
      "            try:\n",
      "                comments = comments.append(get_comments(df['subreddit'][i], df['id'][i], 'top', df['type'][i], user_agent))\n",
      "            except:\n",
      "                print i\n",
      "                with open('Data/Comments/missingcomments.txt', 'a') as the_file:\n",
      "                    the_file.write(df['id'][i])\n",
      "                    the_file.write(' ' + sub)\n",
      "                    the_file.write(' ' + typ + '\\n')\n",
      "        filename = 'Data/Comments/' + sub + typ + '.csv'\n",
      "        comments.to_csv(filename, index=False, encoding='utf-8')\n",
      "                "
     ],
     "language": "python",
     "metadata": {},
     "outputs": []
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [],
     "language": "python",
     "metadata": {},
     "outputs": []
    }
   ],
   "metadata": {}
  }
 ]
}